{"url": "https://en.wikipedia.org/wiki?curid=18844072", "text": "Synthetic lethality\n\nSynthetic lethality arises when a combination of deficiencies in the expression of two or more genes leads to cell death, whereas a deficiency in only one of these genes does not. The deficiencies can arise through mutations, epigenetic alterations or inhibitors of one of the genes. In a synthetic lethal genetic screen, it is necessary to begin with a mutation that does not kill the cell, although may confer a phenotype (for example, slow growth), and then systematically test other mutations at additional loci to determine which confer lethality. Synthetic lethality has utility for purposes of molecular targeted cancer therapy, with the first example of a molecular targeted therapeutic exploiting a synthetic lethal exposed by an inactivated tumor suppressor gene (BRCA1 and 2) receiving FDA approval in 2016 (PARP inhibitor). A sub-case of synthetic lethality, where vulnerabilities are exposed by the deletion of passenger genes rather than tumor suppressor is the so-called \"collateral lethality\".\n\nThe phenomenon of synthetic lethality was first described by Calvin Bridges in 1922, who noticed that some combinations of mutations in the model organism \"Drosophila melanogaster\" confer lethality. Theodore Dobzhansky coined the term \"synthetic lethality\" in 1946 to describe the same type of genetic interaction in wildtype populations of \"Drosophila\". If the combination of genetic events results in a non-lethal reduction in fitness, the interaction is called synthetic sickness. Although in classical genetics the term synthetic lethality refers to the interaction between two genetic perturbations, synthetic lethality can also apply to cases in which the combination of a mutation and the action of a chemical compound causes lethality, whereas the mutation or compound alone are non-lethal.\n\nSynthetic lethality is a consequence of the tendency of organisms to maintain buffering schemes that allow phenotypic stability despite genetic variation, environmental changes and random events such as mutations. This genetic robustness is the result of parallel redundant pathways and \"capacitor\" proteins that camouflage the effects of mutations so that important cellular processes do not depend on any individual component. Synthetic lethality can help identify these buffering relationships, and what type of disease or malfunction that may occur when these relationships break down, through the identification of gene interactions that function in either the same biochemical process or pathways that appear to be unrelated.\n\nSynthetic lethality may be explored in a variety of model organisms, including \"Drosophila melanogaster\" and \"Saccharomyces cerevisiae\". Since synthetic lethal mutations are inherently inviable, common approaches are to employ temperature sensitive mutations or put mutations under the control of a regulated promoter to allow exploration of the phenotype without leading to death. Some synthetic lethal pairs are detected while attempting to elucidate molecular mechanisms of fundamental biological processes without the use of high-throughput screens. For instance, the synthetic lethality of Parkin and MTF1 in \"Drosophila\" was discovered when examining the relationship between oxidative stress and metal homeostasis in the pathogenesis of Parkinson's disease.\n\nHowever, high-throughput synthetic lethal screens may help illuminate questions about how cellular processes work without previous knowledge of gene function or interaction. Screening strategy must take into account the organism used for screening, the mode of genetic perturbation, and whether the screen is forward or reverse. Many of the first synthetic lethal screens were performed in S. cerevisiae. Budding yeast has many experimental advantages in screens, including a small genome, fast doubling time, both haploid and diploid states, and ease of genetic manipulation. Gene ablation can be performed using a PCR-based strategy and complete libraries of knockout collections for all annotated yeast genes are publicly available. Synthetic genetic array (SGA), synthetic lethality by microarray (SLAM), and genetic interaction mapping (GIM) are three high-throughput methods for analyzing synthetic lethality in yeast. A genome scale genetic interaction map was created by SGA analysis in \"S. cerevisiae\" that comprises about 75% of all yeast genes. By examining 5.4 million gene-gene pairs for synthetic lethality, an unbiased network of functional connections between genetic interactions was constructed.\n\nHigh-throughput synthetic lethality screens are also performed in metazoans, but a major challenge is efficient gene perturbation. In the nematode C. elegans, RNA-interference can be combined with a query strain loss-of-function mutation. While RNA-interference is more experimentally demanding in \"Drosophila\", living cell microarrays allow knockdown of two genes simultaneously. RNA-interference is also feasible in mammalian cells, and chemical screens in mammalian cell lines is important for identifying pharmacological targets of drugs.\n\nA synthetic lethal approach to cancer therapy is currently being explored as a means of developing therapies that reduce off-target effects of chemotherapies and chemopreventative drugs. Cancer cells are marked by genetic instability, errors in DNA repair, and uncontrolled transcription, which create new synthetic lethal partners in cancer cells. Because a drug effect targeting a specific gene product resembles the phenotype caused by a mutation in that gene, a cancer-related mutation can sensitize cancer cells to chemotherapeutics that target its synthetic lethal partner. Consequently, drugs that target synthetic lethal partners of mutations in cancer cells may not be toxic to normal cells, which could avoid off-target side effects of chemotherapeutics.\n\nSynthetic lethal analysis can be used to elucidate mechanisms of known chemotherapeutic drugs by identifying genes whose function is necessary for drug function. For example, BRCA1 and BRCA2 are important for repairing double-strand breaks in DNA, and mutations in these genes predispose individuals to breast cancer and ovarian cancer. The enzyme PARP1 is involved in repairing single-strand breaks, and the inhibition of PARP1 in a BRCA mutant background is selectively lethal to tumors because cancer cells accumulate DNA lesions that they cannot repair. Synthetic lethality is also useful for screening libraries of molecules to detect drugs that selectively inhibit cancer cells. In a recent chemical-genetic screen, one compound of 3200 screened molecules was a synthetic lethal inhibitor of pancreatic cancer KRAS gain-of-function cells, which suggests a potential treatment for this cancer type.\n\nAs pointed out by Gao et al., the stability and integrity of the human genome are maintained by the DNA damage repair (DDR) system. In the presence of a defect in DDR, DNA damages will accumulate. Un-repaired DNA damage is a major cause of mutations that drive carcinogenesis. Such excess DNA damage can increase mutational errors during DNA replication due to error-prone translesion synthesis. Excess DNA damage can also increase epigenetic alterations due to errors during DNA repair. Such mutations and epigenetic alterations are the source of the mut-drivers and epi-drivers that cause progression to cancer.\n\nAbout 3 or 4 driver mutations and 60 passenger mutations occur in the exome (protein coding region) of a cancer. However, a much larger number of mutations occur in the non-protein-coding regions of DNA in a cancer. The average number of DNA sequence mutations in the entire genome of a breast cancer tissue sample is much higher, about 20,000. In an average melanoma tissue sample the total number of DNA sequence mutations is about 80,000.\n\nEarly epigenetic or mutational alterations in DDR genes are likely the source of the genetic instability characteristic of cancers. While a mutation or epimutation in a DDR gene, itself, would not confer a selective advantage, such a repair defect may be carried along as a passenger in a cell when the cell acquires an additional mutation/epimutation that does provide a proliferative advantage. Such cells, with both proliferative advantages and one or more DNA repair defects (causing a very high mutation rate), likely give rise to the 20,000 to 80,000 total genome mutations frequently seen in cancers. Thus a defect in a DDR gene is likely to be present in cancers (see, e.g. frequencies of epimutations in DNA repair genes in cancers). Synthetic lethality with an identified DNA repair defect in a cancer could thus be a frequent effective method for therapeutic attack on the cancer.\n\nCollateral lethality is a sub-case of synthetic lethality in personalized cancer therapy, where vulnerabilities are exposed by the deletion of passenger genes rather than tumor suppressor genes, which are deleted by virtue of chromosomal proximity to major deleted tumor suppressor loci.\n\nHomozygous deletion of the 1p36 locus, which contains several candidate tumor suppressor genes, occurs in approximately 5% of glioblastoma (GBM), hepatocellular carcinoma (HCC) and cholangiocarcinoma. Deletion of this region often includes deletion of several passenger genes, including \"ENO1\", which has been identified to confer collateral lethality with subsequent inhibition of its redundant paralogue \"ENO2\". \"ENO1\" encodes the major enolase isoform, accounting for >90% of cellular enolase activity. Enolase is responsible for conversion of 2-phosphoglycerate (2-PGA) to phosphoenolpyruvate (PEP) in glycolysis. Cancer cells could be especially sensitized to inhibition of this pathway due to their frequent shift toward glycolysis, known as the 'Warburg effect'. Knockdown of ENO2 by shRNA inhibited growth, survival, and tumorigenic potential of \"ENO1\"-null cell lines over \"ENO1\"-intact cells. Additionally, the enolase inhibitor PhAH, with half-maximal inhibitory (IC) concentrations around 20 nM, proved to be selectively toxic to \"ENO1\"-null cells over \"ENO1\"-intact cells or normal human astrocytes. Although this inhibitor did not show selective inhibition of the ENO1 isoform over ENO2, higher toxicity toward \"ENO1\"-null cells is explained by the decreased overall enolase expression in these cell lines, which only retain about 10% of their normal enolase activity. SF2312, a natural antibiotic produced by \"Micromonospora\" and active against bacteria under anaerobic conditions, was also shown to be a potent inhibitor of enolase, with (IC) concentrations ranging from 10-50 nM. This inhibitor exhibited increased potency toward ENO2 over ENO1 at concentrations above its IC. Further, a strong selective toxicity of SF2312 toward multiple \"ENO1\"-deleted cell lines was observed; these effects were not observed in \"ENO1\"-rescued cells, were increased after SF2312 treatment compared to PhAH treatment, and were more potent during hypoxia. Cell death and inhibition of proliferation in SF2312 treated \"ENO1\"-deleted cells were preceded by a decrease in ATP and other high-energy phosphates, and the conversion of glucose to lactate was inhibited. These effects are consistent with the impacts of enolase inhibition upon glycolysis, and were reversible by ENO1 re-expression or ENO2 overexpression. Thus, pharmacological targeting of ENO2 offers a promising therapeutic target for cancers harboring \"ENO1\" deletions, due to the redundancy and cell-essential function of these genes.\n\nA considerable fraction (~30%) of pancreatic ductal adenocarcinoma (PDAC) cases are characterized by frequent homozygous deletion of the SMAD4 tumor suppressor gene along with a concomitant loss of neighbouring housekeeping genes (18qso). Among these passenger genes that are deleted due to their close proximity with the tumor suppressor gene SMAD4, malic enzyme 2 (ME2) has been identified to confer collateral lethality, with ME3 as its collateral lethal partner (ref &&). ME2 and ME3 are mitochondrial oxidative decarboxylases involved in the catalytic conversion of malate to pyruvate with the concomitant generation of NAD(P)H. ME1 and ME2 have redundant, but important roles in NADPH regeneration and ROS homeostasis. Consistent with the framework of collateral lethality, shRNA knockdown of ME3 in a panel of PDAC cell lines only results in selective killing of ME2-deleted but not intact PDAC cells. ShRNA knockdown of ME3 in ME2 null cells causes ROS accumulation and AMPK activation, which phosphorylates SREBP1 and inhibits its activity and nuclear translocation. SREBP1 regulates the transcription of BCAT2 which mediates BCAA catabolism and glutamine synthesis, which is critical for de novo nucleotide synthesis. In the absence of two mitochondrial malic enzymes, an impaired NADPH production and ROS accumulation ensue, that render a cell apoptotic. Collectively, the cell-essential and functional redundancy in the expression of collateral lethal genes ME2 and ME3 indicate a promising therapeutic strategy in the treatment of PDAC patients which can be achieved by pharmacologically targeting the redundant paralog ME3 of the deleted passenger gene ME2.\n\nMost human cancers are characterized by the inactivation of the tumor suppressor gene TP53 that encodes p53, either by mutation or loss of the gene. Genomic deletion of 17p13.1 TP53 locus also involves co-deletion of neighbouring genes such as POLR2A. POLR2A encodes the catalytic subunit of RNA POLII complex and is susceptible to inhibition by alpha-Amanitin. In colorectal cancer with hemizygous deletion of TP53 and POLR2A, inhibition of POLR2A leads to a selective inhibition of proliferation of the cancer cells independent of p53. POLR2A inhibition in POLR2A intact cells exerts no significant effect on cell proliferation, survival and tumorigenicity, but the inhibition is dramatic on POLR2A hemizygous cells. This suggests that collateral and therapeutic vulnerability exposed by TP53 and POLR2A hemizygous deletion in colorectal cancer can be exploited to treat TP53 deleted colorectal cancers using POLR2A inhibitors.\n\nMutations in genes employed in DNA mismatch repair (MMR) cause a high mutation rate. In tumors, such frequent subsequent mutations often generate \"non-self\" immunogenic antigens. A human Phase II clinical trial, with 41 patients, evaluated one synthetic lethal approach for tumors with or without MMR defects. In the case of sporadic tumors evaluated, the majority would be deficient in MMR due to epigenetic repression of an MMR gene (see DNA mismatch repair). The product of gene \"PD-1\" ordinarily represses cytotoxic immune responses. Inhibition of this gene allows a greater immune response. In this Phase II clinical trial with 47 patients, when cancer patients with a defect in MMR in their tumors were exposed to an inhibitor of PD-1, 67% - 78% of patients experienced immune-related progression-free survival. In contrast, for patients without defective MMR, addition of PD-1 inhibitor generated only 11% of patients with immune-related progression-free survival. Thus inhibition of PD-1 is primarily synthetically lethal with MMR defects.\n\nThe analysis of 630 human primary tumors in 11 tissues shows that \"WRN\" promoter hypermethylation (with loss of expression of WRN protein) is a common event in tumorigenesis. The \"WRN\" gene promoter is hypermethylated in about 38% of colorectal cancers and non-small-cell lung carcinomas and in about 20% or so of stomach cancers, prostate cancers, breast cancers, non-Hodgkin lymphomas and chondrosarcomas, plus at significant levels in the other cancers evaluated. The WRN helicase protein is important in homologous recombinational DNA repair and also has roles in non-homologous end joining DNA repair and base excision DNA repair.\n\nTopoisomerase inhibitors are frequently used as chemotherapy for different cancers, though they cause bone marrow suppression, are cardiotoxic and have variable effectiveness. A 2006 retrospective study, with long clinical follow-up, was made of colon cancer patients treated with the topoisomerase inhibitor irinotecan. In this study, 45 patients had hypermethylated \"WRN\" gene promoters and 43 patients had unmethylated \"WRN\" gene promoters. Irinitecan was more strongly beneficial for patients with hypermethylated \"WRN\" promoters (39.4 months survival) than for those with unmethylated \"WRN\" promoters (20.7 months survival). Thus, a topoisomerase inhibitor appeared to be synthetically lethal with deficient expression of \"WRN\". Further evaluations have also indicated synthetic lethality of deficient expression of \"WRN\" and topoisomerase inhibitors.\n\nAs reviewed by Murata et al., five different PARP1 inhibitors are now undergoing Phase I, II and III clinical trials, to determine if particular PARP1 inhibitors are synthetically lethal in a large variety of cancers, including those in the prostate, pancreas, non-small-cell lung tumors, lymphoma, multiple myeloma, and Ewing sarcoma. In addition, in preclinical studies using cells in culture or within mice, PARP1 inhibitors are being tested for synthetic lethality against epigenetic and mutational deficiencies in about 20 DNA repair defects beyond BRCA1/2 deficiencies. These include deficiencies in \"PALB2\", \"FANCD2\", \"RAD51\", \"ATM\", \"MRE11\", \"p53\", \"XRCC1\" and \"LSD1\".\n\nARID1A, a chromatin modifier, is required for non-homologous end joining, a major pathway that repairs double-strand breaks in DNA, and also has transcription regulatory roles. \"ARID1A\" mutations are one of the 12 most common carcinogenic mutations. Mutation or epigenetically decreased expression of \"ARID1A\" has been found in 17 types of cancer. Pre-clinical studies in cells and in mice show that synthetic lethality for deficient \"ARID1A\" expression occurs by either inhibition of the methyltransferase activity of EZH2, or with addition of the kinase inhibitor dasatinib.\n\nThere are two pathways for homologous recombinational repair of double-strand breaks. The major pathway depends on BRCA1, PALB2 and BRCA2 while an alternative pathway depends on RAD52. Pre-clinical studies, involving epigenetically reduced or mutated \"BRCA\"-deficient cells (in culture or injected into mice), show that inhibition of RAD52 is synthetically lethal with \"BRCA\"-deficiency.\n\nAlthough treatments using synthetic lethality can stop or slow progression of cancers and prolong survival, each of the synthetic lethal treatments has some adverse side effects. For example, more than 20% of patients treated with an inhibitor of PD-1 encounter fatigue, rash, pruritus, cough, diarrhea, decreased appetite, constipation or arthralgia. Thus, it is important to determine which DDR deficiency is present, so that only an effective synthetic lethal treatment can be applied, and not unnecessarily subject patients to adverse side effects without a direct benefit.\n\n\n", "id": "18844072", "title": "Synthetic lethality"}
{"url": "https://en.wikipedia.org/wiki?curid=18844051", "text": "Synthetic genetic array\n\nSynthetic genetic array analysis (SGA) is a high-throughput technique for exploring synthetic lethal and synthetic sick genetic interactions (SSL). SGA allows for the systematic construction of double mutants using a combination of recombinant genetic techniques, mating and selection steps. Using SGA methodology a query gene deletion mutant can be crossed to an entire genome deletion set to identify any SSL interactions, yielding functional information of the query gene and the genes it interacts with. A large-scale application of SGA in which ~130 query genes were crossed to the set of ~5000 viable deletion mutants in yeast revealed a genetic network containing ~1000 genes and ~4000 SSL interactions. The results of this study showed that genes with similar function tend to interact with one another and genes with similar patterns of genetic interactions often encode products that tend to work in the same pathway or complex. Synthetic Genetic Array analysis was initially developed using the model organism \"S. cerevisiae\". This method has since been extended to cover 30% of the \"S. cerevisiae\" genome. Methodology has since been developed to allow SGA analysis in \"S.pombe\" and \"E. coli\".\n\nSynthetic genetic array analysis was initially developed by Tong et al. in 2001 and has since been used by many groups working in a wide range of biomedical fields. SGA utilizes the entire genome yeast knock-out set created by the yeast genome deletion project.\n\nSynthetic genetic array analysis is generally conducted using colony arrays on petriplates at standard densities (96, 384, 768, 1536). To perform a SGA analysis in \"S.cerevisae\", the query gene deletion is crossed systematically with a deletion mutant array (DMA) containing every viable knockout ORF of the yeast genome (currently 4786 strains). The resulting diploids are then sporulated by transferring to a media containing reduced nitrogen. The haploid progeny are then put through a series of selection platings and incubations to select for double mutants. The double mutants are screened for SSL interactions visually or using imaging software by assessing the size of the resulting colonies.\n\nDue to the large number of precise replication steps in SGA analysis, robots are widely used to perform the colony manipulations. There are a few systems specifically designed for SGA analysis, which greatly decrease the time to analyse a query gene. Generally these have a series of pins which are used to transfer cells to and from plates, with one system utilizing disposable pads of pins to eliminate washing cycles. Computer programs can be used to analyze the colony sizes from images of the plates thus automating the SGA scoring and chemical-genetics profiling.\n\n", "id": "18844051", "title": "Synthetic genetic array"}
{"url": "https://en.wikipedia.org/wiki?curid=4021188", "text": "Chromosome microdissection\n\nChromosome microdissection is a technique that physically removes a large section of DNA from a complete chromosome. The smallest portion of DNA that can be isolated using this method comprises 10 million base pairs - hundreds or thousands of individual genes.\n\nScientists who study chromosomes are known as cytogeneticists. They are able to identify each chromosome based on its unique pattern of dark and light bands. Certain abnormalities, however, cause chromosomes to have unusual banding patterns. For example, one chromosome may have a piece of another chromosome inserted within it, creating extra bands. Or, a portion of a chromosome may be repeated over and over again, resulting in an unusually wide, dark band (known as a homogeneously staining region). Some chromosomal aberrations have been linked to cancer and inherited genetic disorders, and the chromosomes of many tumor cells exhibit irregular bands. To understand more about what causes these conditions, scientists hope to determine which genes and DNA sequences are located near these unusual bands. Chromosome microdissection is a specialized way of isolating these regions by removing the DNA from the band and making that DNA available for further study.\n\nTo prepare cells for chromosome microdissection, a scientist first treats them with a chemical that forces them into metaphase: a phase of the cell's life-cycle where the chromosomes are tightly coiled and highly visible. Next, the cells are dropped onto a microscope slide so that the nucleus, which holds all of the genetic material together, breaks apart and releases the chromosomes onto the slide. Then, under a microscope, the scientist locates the specific band of interest, and, using a very fine needle, tears that band away from the rest of the chromosome. The researcher next produces multiple copies of the isolated DNA using a procedure called PCR (polymerase chain reaction). The scientist uses these copies to study the DNA from the unusual region of the chromosome in question.\n\n\n", "id": "4021188", "title": "Chromosome microdissection"}
{"url": "https://en.wikipedia.org/wiki?curid=19269660", "text": "Tandem chimerism\n\nTandem chimerism is the phenomenon where two consecutive genes are transcribed into a single RNA molecule. The translation (after splicing) of such RNAs can lead to a new, fused protein, having parts from both original proteins. \n\n\n", "id": "19269660", "title": "Tandem chimerism"}
{"url": "https://en.wikipedia.org/wiki?curid=1179383", "text": "Overdominance\n\nOverdominance is a condition in genetics where the phenotype of the heterozygote lies outside the phenotypical range of both homozygous parents. Overdominance can also be described as heterozygote advantage, wherein heterozygous individuals have a higher fitness than homozygous individuals.\n\nAn example in humans is sickle cell anemia. This condition is determined by a single polymorphism. Possessors of the deleterious allele have lower life expectancy, with homozygotes rarely reaching 50 years of age. However, this allele also yields some resistance to malaria. Thus in regions where malaria exerts or has exerted a strong selective pressure, sickle cell anemia has been selected for its conferred partial resistance to the disease. While homozygotes will have either no protection from malaria or a dramatic propensity to sickle cell anemia, heterozygotes have fewer physiological effects and a partial resistance to malaria.\n\nPopulation Geneticist John H. Gillespie established the following model:\n\nWhere h is the heterozygote effect and s is the recessive allele effect.\nThus given a value for s (i.e.: 0<s<1), h can yield the following information:\nFor the case of sickle cell anemia the situation corresponds to the case h<0 in the Gillespie Model .\n\n", "id": "1179383", "title": "Overdominance"}
{"url": "https://en.wikipedia.org/wiki?curid=19651509", "text": "Fixed allele\n\nA fixed allele is an allele that is the only variant that exists for that gene in all the population. A fixed allele is homozygous for all members of the population. The term \"allele\" normally refers to one variant gene out of several possible for a particular locus in the DNA. When all but one allele go extinct and only one remains, that allele is said to be fixed.\n\nThere are only two ways in which a fixed allele can become un-fixed. This can happen through random mutations that lead to the development of a new allele. Or this can happen through immigration.\nFixed alleles were first defined by Motoo Kimura in 1962. He discussed how fixed alleles could arise within populations, and was the first to generalize the topic. He credits the works of Haldane in 1927 and Fisher in 1922 as being important in providing foundational information that allowed him to come to his conclusion. Kimura's later works were pivotal in the foundation of evolutionary and population genetics. Kimura is responsible for the development of the neutral theory of molecular evolution, which discusses how most of the variation and evolution within species is caused by the random fluctuation of neutral allele frequencies, so by genetic drift, rather than natural selection. More recent studies have confirmed the works of these early evolutionary biologists, showing that rates of extinction decrease with increasing beneficial alleles, that extinction of deleterious alleles occurs faster than that of beneficial alleles, and that the process of adaptation can become very complex.\nTo illustrate what a fixed allele, lets imagine a population of rabbits where there are three alleles for fur color brown, gray or white. In this initial population, there is no fixed allele. Then an event, such as a forest fire, causes the elimination of one of the alleles from the population. Lets assume all the gray rabbits were killed in a forest fire, and now all that remains in the population are the white and brown alleles. This happens in the summer, so there is no snow, and the white rabbits fall prey to owls more often than brown rabbits. Eventually the only remaining allele in the population is the brown allele, this allele is now a fixed allele.\n\nFixed alleles are a very important aspect of evolutionary biology. Low genetic diversity, which is seen with allele fixation, is dangerous as it can lead to mass extinctions. If there is little genetic variability within a population and the genetically similar individuals all are susceptible to a certain pathogen, the population will likely cease to exist. This is why we see examples of populations with fixed alleles becoming threatened or endangered.\n\nA great example of why fixed alleles matter is the US agriculture supply and the threat of bioterrorism. Many crops grown in the US are genetically similar, and allows for the possibility of devastating bioterrorism. Should a pathogen be developed targets certain crop supplies, such as corn, which are pivotal to the US food supply and therefore vital to the US's economic state, disastrous events could occur as the food supply could be depleted very quickly.\n\nFixation is the process through which an allele becomes a fixed allele within a population. There are many ways for an allele to become fixed, but most often it is through the action of multiple processes working together. The key driving force behind fixation is natural selection and genetic drift. Natural selection was postulated by Darwin and encompasses many processes that lead to the differential survival of organisms due to genetic or phenotypic differences. Genetic drift is the process by which allele frequencies fluctuate within populations. Natural selection and genetic drift propel evolution forward, and through evolution alleles can become fixed. \nThe natural selection processes such as sexual, convergent, divergent or stabilizing selection pave the way for allele fixation. One way some of these natural selection processes cause fixation is through one specific genotype or phenotype being favored, which leads to the convergence of the variability until one allele become fixed. Natural selection can work the other way, where two alleles become fixed through two specific genotypes or phenotypes being favored, leading to divergence within the population until the populations become so separate that they are now two species each with their own fixed allele.\n\nSelective pressures can favor certain genotypes or phenotypes. A commonly known example of this is the process of antibiotic resistance within bacterial populations. As antibiotics are used to kill bacteria, a small number of them with favorable mutations can survive and repopulate in an environment that is now free of competition. The allele for antibiotic resistance then becomes a fixed allele within the surviving and future populations. This is an example of the bottleneck effect. A bottleneck occurs when a population is put under strong selective pressure, and only certain individuals survive. These surviving individuals have a decreased number of alleles present within their population than were present in the initial population, however these remaining alleles are the only ones left in future populations assuming no mutation or migration. This bottleneck effect can also be seen in natural disaster, as shown in the rabbit example above. \n\nSimilar to the bottleneck effect, the founder's effect can also cause allele fixation. The founder effect occurs when a small founding population is moved a new area and propagates the future population. This can be seen in the \"Alces alces\" moose population in Newfoundland, Canada. Moose are not native to Newfoundland, and in 1878 and 1904 six total moose were introduced to the island. The six founding moose propagated the current population of an estimated 4000-6000 moose. This has had dramatic effects on the offspring of the founding moose and has led to a great decrease in genetic variability within the Newfoundland moose population as compared to the mainland population. \nOther random processes such as genetic drift can lead to fixation. Through these random processes, some random individuals or alleles are removed from the population. These random fluctuations within the allele frequencies can lead to the fixation or loss of certain alleles within a population. To the right is an image that shows thorough successive generations; the allele frequencies fluctuate randomly within a population. The smaller your population size, the faster fixation or loss of alleles will occur. However, it should be noted that all populations are driven to allele fixation and it is inevitable, it just takes varying amounts of time for this to occur due to population size. \nSome other causes of allele fixation are inbreeding, as this decreases the genetic variability of the population and therefore decreases the effective population size. This allows genetic drift to cause fixation faster than anticipated. \nIsolation can also cause fixation, as it prevents the influx of new variable alleles into the population. This can often be seen on island populations, where the populations have a limited set of alleles. The only variability that can be added to these populations is through mutations.\n\nOne example of a fixed allele is the DGAT-1 exon 8 in Anatolian buffalo. A non-conservative mutation in the DGAT-1 allele, which produces a protein with a lysine at position 232 instead of an alanine. This mutation produces a protein different from the wild type protein. This mutation in cattle has an effect in milk production. Investigation into three water buffalo populations revealed four different haplotypes each having a single nucleotide polymorphism (SNP), however all of these SNPs were conservative mutations, causing no change in protein production. All populations of Anatolian buffalo studied had the non-conservative lysine mutation at 232, leading to the conclusion that this DGAT-1 allele mutation is fixed within the populations.\n\nThe \"Parnassius apollo\" butterfly is classified as a threatened species, having many disjointed populations in Western Palaearctic region. The population in the Mosel Valley of Germany has been genetically characterized, and has shown to have six long-term monomorphic microsatellites. Six microsatellites were examined looking at the current population in 2008 as well as museum samples from 1895 to 1989. One of the microsatellite alleles examined has fixed within the population prior to 1895. For the current population all six microsatellites as well as all sixteen alloenzymes analyzed were fixed.\n\nFixed alleles can often be deleterious to populations, especially when there is a small population size and low genetic variability. For example, the California Channel Island Fox (\"Urocyon littoralis\") has the most monomorphic population ever reported for a sexually reproducing animal. During the 1990s the Island Fox experienced disastrous population decline, leading to near extinction. This population decline was in part caused by the canine distemper virus, the foxes were susceptible to this virus, and due to their genetic similarity many were killed. The introduction of a predator, the golden eagle, also attributed to this population decline. With current conservation efforts the population is in recovery.\n\nMeiotic drive\n\n", "id": "19651509", "title": "Fixed allele"}
{"url": "https://en.wikipedia.org/wiki?curid=5902061", "text": "Outcrossing\n\nOut-crossing or out-breeding is the practice of introducing unrelated genetic material into a breeding line. It increases genetic diversity, thus reducing the probability of an individual being subject to disease or genetic abnormalities.\n\nOutcrossing is now the norm of most purposeful animal breeding, contrary to what is commonly believed. The outcrossing breeder intends to remove the traits by using \"new blood\". With dominant traits, one can still see the expression of the traits and can remove those traits whether one outcrosses, line breeds or inbreds. With recessive traits, outcrossing allows for the recessive traits to migrate across a population. The outcrossing breeder then may have individuals that have many deleterious genes that may be expressed by subsequent inbreeding. There is now a gamut of deleterious genes within each individual in many dog breeds.\n\nIncreasing the variation of genes or alleles within the gene pool may protect against extinction by stressors from the environment. For example, in this context, a recent veterinary medicine study tried to determine the genetic diversity within cat breeds.\n\nOutcrossing is believed to be the \"norm\" in the wild. Outcrossing in plants is usually enforced by self-incompatibility.\n\nBreeders inbreed within their genetic pool, attempting to maintain desirable traits and to cull those traits that are undesirable. When undesirable traits begin to appear, mates are selected to determine if a trait is recessive or dominant. Removal of the trait is accomplished by breeding two individuals known not to carry it.\n\nGregor Mendel used outcrossing in his experiments with flowers. He then used the resulting offspring to chart inheritance patterns, using the crossing of siblings, and backcrossing to parents to determine how inheritance functioned.\n\nCharles Darwin, in his book \"The Effects of Cross and Self-Fertilization in the Vegetable Kingdom\", came to clear and definite conclusions concerning the adaptive benefit of outcrossing. For example, he stated (on page 462) that \"the offspring from the union of two distinct individuals, especially if their progenitors have been subjected to very different conditions, have an immense advantage in height, weight, constitutional vigor and fertility over the self-fertilizing offspring from either one of the same parents\". He thought that this observation was amply sufficient to account for outcrossing sexual reproduction. The disadvantages of self-fertilized offspring (inbreeding depression) are now thought to be largely due to the homozygous expression of deleterious recessive mutations; and the fitness advantages of outcrossed offspring are thought to be largely due to the heterozygous masking of such deleterious mutations.\n\n", "id": "5902061", "title": "Outcrossing"}
{"url": "https://en.wikipedia.org/wiki?curid=10174380", "text": "Enhancer trap\n\nAn enhancer trap is a method in molecular biology that allows hijacking of an enhancer from another gene, and so, identification of enhancers. The enhancer trap construct contains a transposable element and a reporter gene. The first is necessary for (random) insertion in the genome, the latter is necessary for identification of the spatial regulation by the enhancer. On top of this, the construct usually includes a genetic marker, e.g., the white gene producing red-colored eyes in \"Drosophila\", or ampicillin resistance in \"E. coli\".\n\nThe most common and basic enhancer traps are: P<nowiki>[</nowiki>lacZ<nowiki>]</nowiki> from the bacterium \"E. coli\" and P<nowiki>[</nowiki>GAL4<nowiki>]</nowiki> from yeast. There exists a large number of fly stocks containing GAL4 insertions and an equally large number of fly stocks containing an UAS DNA sequence followed by a gene of interest, which permits the expression of a large number of genes with different GAL4 \"drivers\". Rather than generating transgenic flies with the enhancer linked directly to the gene of interest (which takes about a year, if you are starting without the appropriate DNA construct), you simply mate (cross) one transgenic fly with another transgenic fly.\n\n", "id": "10174380", "title": "Enhancer trap"}
{"url": "https://en.wikipedia.org/wiki?curid=1944711", "text": "Indel\n\nIndel is a molecular biology term for an insertion or deletion of bases in the genome of an organism. It is classified among small genetic variations, measuring from 1 to 10 000 base pairs in length, including insertion and deletion events that may be separated by many years, and may not be related to each other in any way.\nA microindel is defined as an indel that results in a net change of 1 to 50 nucleotides.\n\nIn coding regions of the genome, unless the length of an indel is a multiple of 3, it will produce a frameshift mutation. For example, a common microindel which results in a frameshift causes Bloom syndrome in the Jewish or Japanese population. Indels can be contrasted with a point mutation. An indel inserts and deletes nucleotides from a sequence, while a point mutation is a form of substitution that \"replaces\" one of the nucleotides without changing the overall number in the DNA. Indels can also be contrasted with Tandem Base Mutations (TBM), which may result from fundamentally different mechanisms. A TBM is defined as a substitution at adjacent nucleotides (primarily substitutions at two adjacent nucleotides, but substitutions at three adjacent nucleotides have been observed.\n\nIndels, being either insertions, or deletions, can be used as genetic markers in natural populations, especially in phylogenetic studies. It has been shown that genomic regions with multiple indels can also be used for species-identification procedures.\n\nAn indel change of a single base pair in the coding part of an mRNA results in a frameshift during mRNA translation that could lead to an inappropriate (premature) stop codon in a different frame. Indels that are not multiples of 3 are particularly uncommon in coding regions but relatively common in non-coding regions. There are approximately 192-280 frameshifting indels in each person. Indels are likely to represent between 16% and 25% of all sequence polymorphisms in humans. In fact, in most known genomes, including humans, indel frequency tends to be markedly lower than that of single nucleotide polymorphisms (SNP), except near highly repetitive regions, including homopolymers and microsatellites.\n\nThe term \"indel\" has been co-opted in recent years by genome scientists for use in the sense described above. This is a change from its original use and meaning, which arose from systematics. In systematics, researchers could find differences between sequences, such as from two different species. But it was impossible to infer if one species lost the sequence or the other species gained it. For example, species \"A\" has a run of 4 G nucleotides at a locus and species \"B\" has 5 G's at the same locus. If the mode of selection is unknown, one can not tell if species A lost one G (a \"deletion\" event\") or species B gained one G (an \"insertion\" event). When one cannot infer the phylogenetic direction of the sequence change, the sequence change event is referred to as an \"indel\".\n\n", "id": "1944711", "title": "Indel"}
{"url": "https://en.wikipedia.org/wiki?curid=20667047", "text": "Geneticism\n\nGeneticism is a school of thought that holds that genetics determines all human characteristics, particularly psychological characteristics. This concept is primarily associated with the works of Francis Galton.\n", "id": "20667047", "title": "Geneticism"}
{"url": "https://en.wikipedia.org/wiki?curid=5706520", "text": "Trinucleotide repeat expansion\n\nTrinucleotide repeat expansion, also known as triplet repeat expansion, is the DNA mutation responsible for causing any type of disorder categorized as a trinucleotide repeat disorder. These are labelled in dynamical genetics as dynamic mutations. Triplet expansion is caused by \"slippage\" during DNA replication, also known as \"copy choice\" DNA replication. Due to the repetitive nature of the DNA sequence in these regions, 'loop out' structures may form during DNA replication while maintaining complementary base pairing between the parent strand and daughter strand being synthesized. If the loop out structure is formed from the sequence on the daughter strand this will result in an increase in the number of repeats. However, if the loop out structure is formed on the parent strand, a decrease in the number of repeats occurs. It appears that expansion of these repeats is more common than reduction. Generally, the larger the expansion the more likely they are to cause disease or increase the severity of disease. Other proposed mechanisms for expansion and reduction involve the interaction of RNA and DNA molecules.\n\nThe number of trinucleotide repeats appears to predict the progression, severity, and age of onset of Huntington's disease and similar trinucleotide repeat disorders.\n", "id": "5706520", "title": "Trinucleotide repeat expansion"}
{"url": "https://en.wikipedia.org/wiki?curid=6763", "text": "Cistron\n\nA cistron is an alternative term to a gene. The term cistron is used to emphasize that genes exhibit a specific behavior in a cis-trans test; distinct positions (or loci) within a genome are cistronic.\n\nThe words \"cistron\" and \"gene\" were coined before the advancing state of biology made it clear that the concepts they refer to are practically equivalent. The same historical naming practices are responsible for many of the synonyms in the life sciences.\n\nThe term cistron was coined by Seymour Benzer in an article entitled \"The elementary units of heredity\". The cistron was defined by an operational test applicable to most organisms that is sometimes referred to as a cis-trans test, but more often as a complementation test.\n\nFor example, suppose a mutation at a chromosome position formula_1 is responsible for a recessive trait in a diploid organism (where chromosomes come in pairs). We say that the mutation is recessive because the organism will exhibit the wild type phenotype (ordinary trait) unless both chromosomes of a pair have the mutation (homozygous mutation). Similarly, suppose a mutation at another position, formula_2, is responsible for the same recessive trait. The positions formula_1 and formula_2 are said to be within the same cistron when an organism that has the mutation at formula_1 on one chromosome and has the mutation at position formula_2 on the paired chromosome exhibits the recessive trait even though the organism is not homozygous for either mutation. When instead the wild type trait is expressed, the positions are said to belong to distinct cistrons / genes. It's important to know that cistron localization has been studied for several years, and, depending of the organism, this gene can be found in different places. For example, the gene can be found in RNA phage and is related to the cistron coat in some particular organisms. Some examples of the organisms that have been study are Tobacco, grapevine.\n\nFor example, an operon is a stretch of DNA that is transcribed to create a contiguous segment of RNA, but contains more than one cistron / gene. The operon is said to be polycistronic, whereas ordinary genes are said to be monocistronic.\n", "id": "6763", "title": "Cistron"}
{"url": "https://en.wikipedia.org/wiki?curid=20854585", "text": "Direct repeat\n\nDirect repeats are a type of genetic sequence that consists of two or more repeats of a specific sequence. In other words, the direct repeats are nucleotide sequences present in multiple copies in the genome. Generally, a direct repeat occurs when a sequence is repeated with the same pattern downstream. There is no inversion and no reverse complement associated with a direct repeat. It may or may not have intervening nucleotides. The nucleotide sequence written in bold characters signifies the repeated sequence. \nLinguistically, a typical direct repeat is comparable to saying “bye-bye”.\n\nThere are several types of repeated sequences :\n\n", "id": "20854585", "title": "Direct repeat"}
{"url": "https://en.wikipedia.org/wiki?curid=20191599", "text": "Simple sequence length polymorphism\n\nSimple Sequence Length Polymorphisms (SSLPs) are used as genetic markers with polymerase chain reaction (PCR). An SSLP is a type of polymorphism: a difference in DNA sequence amongst individuals. SSLPs are repeated sequences over varying base lengths in intergenic regions of deoxyribonucleic acid (DNA). Variance in the length of SSLPs can be used to understand genetic variation between two individuals in a certain species.\n\nAn example of the usage of SSLPs (microsatellites) is seen in a study by Rosenberg et al., where SSLPs were used to cluster different continental races. The study was critical to Nicholas Wade's New York Times Bestseller, \"\".\n\nRosenberg studied 377 SSLPs in 1000 people in 52 different regions of the world. By using PCR and cluster analysis, Rosenberg was able to group individuals that had the same SSLPs . These SSLPs were extremely useful to the experiment because they do not affect the phenotypes of the individuals, thus being unaffected by natural selection.\n", "id": "20191599", "title": "Simple sequence length polymorphism"}
{"url": "https://en.wikipedia.org/wiki?curid=21003747", "text": "High Resolution Melt\n\nHigh Resolution Melt (HRM) analysis is a powerful technique in molecular biology for the detection of mutations, polymorphisms and epigenetic differences in double-stranded DNA samples. It was discovered and developed by Idaho Technology and the University of Utah. It has advantages over other genotyping technologies, namely:\n\nHRM analysis is performed on double stranded DNA samples. Typically the user will use polymerase chain reaction (PCR) prior to HRM analysis to amplify the DNA region in which their mutation of interest lies. In the sample tube there are now many copies of the DNA region of interest. This region that is amplified is known as the amplicon.\nAfter the PCR process the HRM analysis begins. The process is simply a precise warming of the amplicon DNA from around 50 ˚C up to around 95 ˚C. At some point during this process, the melting temperature of the amplicon is reached and the two strands of DNA separate or \"melt\" apart.\n\nThe secret of HRM is to monitor this process happening in real-time. This is achieved by using a fluorescent dye. The dyes that are used for HRM are known as intercalating dyes and have a unique property. They bind specifically to double-stranded DNA and when they are bound they fluoresce brightly. In the absence of double stranded DNA they have nothing to bind to and they only fluoresce at a low level. \nAt the beginning of the HRM analysis there is a high level of fluorescence in the sample because of the billions of copies of the amplicon. But as the sample is heated up and the two strands of the DNA melt apart, presence of double stranded DNA decreases and thus fluorescence is reduced. The HRM machine has a camera that watches this process by measuring the fluorescence. The machine then simply plots this data as a graph known as a melt curve, showing the level of fluorescence vs the temperature:\n\nThe melting temperature of the amplicon at which the two DNA strands come apart is entirely predictable. It is dependent on the sequence of the DNA bases. If you are comparing two samples from two different people, they should give exactly the same shaped melt curve. However, if one person has a mutation in the DNA region you have amplified, then this will alter the temperature at which the DNA strands melt apart. So now the two melt curves appear different. The difference may only be tiny, perhaps a fraction of a degree, but because the HRM machine has the ability to monitor this process in \"high resolution\", it is possible to accurately document these changes and therefore identify if a mutation is present or not.\n\nThings become slightly more complicated than this because organisms contain two (or more) copies of each gene, known as the two alleles. So, if a sample is taken from a patient and amplified using PCR both copies of the region of DNA (alleles) of interest are amplified. So if we are looking for mutation there are now three possibilities:\n\n\nThese three scenarios are known as \"Wild–type\", \"Heterozygote\" or \"Homozygote\" respectively. Each gives a melt curve that is slightly different. With a high quality HRM assay it is possible to distinguish between all three of these scenarios.\n\nHomozygous allelic variants may be characterised by a temperature shift on the resulting melt curve produced by HRM analysis. In comparison, heterozygotes are characterised by changes in melt curve shape. This is due to base-pair mismatching generated as a result of destabilised heteroduplex annealing between wild-type and variant strands. These differences can be easily seen on the resulting melt curve and the melt profile differences between the different genotypes can be amplified visually via generating a difference curve \n\nConventional SNP typing methods are typically time consuming and expensive, requiring several probe based assays to be multiplexed together or the use of DNA microarrays. HRM is more cost effective and reduces the need to design multiple pairs of primers and the need to purchase expensive probes. The HRM method has been successfully used to detect a single G to A substitution in the gene Vssc (Voltage Sensitive Sodium Channel) which confers resistance to the acaricide permethrin in Scabies mite. This mutation results in a coding change in the protein (G1535D). The analysis of scabies mites collected from suspected permethrin susceptible and tolerant populations by HRM showed distinct melting profiles. The amplicons from the sensitive mites were observed to have a higher melting temperature relative to the tolerant mites, as expected from the higher thermostability of the GC base pair \n\nIn a field more relevant to clinical diagnostics, HRM has been shown to be suitable in principle for the detection of mutations in the breast cancer susceptibility genes BRCA1 and BRCA2. More than 400 mutations have been identified in these genes.The sequencing of genes is the gold standard for identifying mutations. Sequencing is time consuming and labour-intensive and is often preceded by techniques used to identify heteroduplex DNA, which then further amplify these issues. HRM offers a faster and more convenient closed-tube method of assessing the presence of mutations and gives a result which can be further investigated if it is of interest. In a study carried out by Scott et al. in 2006, 3 cell lines harbouring different BRCA mutations were used to assess the HRM methodology. It was found that the melting profiles of the resulting PCR products could be used to distinguish the presence or absence of a mutation in the amplicon. Similarly in 2007 Krypuy et al. showed that the careful design of HRM assays (with regards to primer placement) could be successfully employed to detect mutations in the TP53 gene, which encodes the tumour suppressor protein p53 in clinical samples of breast and ovarian cancer. Both these studies highlighted the fact that changes in the melting profile can be in the form of a shift in the melting temperature or an obvious difference in the shape of the melt curve. Both of these parameters are a function of the amplicon sequence.\nThe consensus is that HRM is a cost efficient method that can be employed as an initial screen for samples suspected of harbouring polymorphisms or mutations. This would reduce the number of samples which need to be investigated further using more conventional methods.\n\nCurrently there are many methods used to determine the zygosity status of a gene at a particular locus. These methods include the use of PCR with specifically designed probes to detect the variants of the genes (SNP typing is the simplest case). In cases where longer stretches of variation is implicated, post PCR analysis of the amplicons may be required. Changes in enzyme restriction, electrophoretic and chromatographic profiles can be measured. These methods are usually more time consuming and increase the risk of amplicon contamination in the laboratory, due to the need to work with high concentrations of amplicons in the lab post-PCR. The use of HRM reduces the time required for analysis and the risk of contamination. HRM is a more cost effective solution and the high resolution element not only allows the determination of homo and heterozygosity, it also resolves information about the type of homo and heterozygosity, with different gene variants giving rise to differing melt curve shapes. A study by Gundry et al. 2003, showed that fluorescent labelling of one primer (in the pair) has been shown to be favourable over using an intercalating dye such as SYBR green I. However, progress has been made in the development and use of improved intercalating dyes which reduce the issue of PCR inhibition and concerns over non-saturating intercalation of the dye.\n\nThe HRM methodology has also been exploited to provide a reliable analysis of the methylation status of DNA. This is of significance since changes to the methylation status of tumour suppressor genes, genes that regulate apoptosis and DNA repair, are characteristics of cancers and also have implications for responses to chemotherapy. For example, cancer patients can be more sensitive to treatment with DNA alkylating agents if the promoter of the DNA repair gene \"MGMT\" of the patient is methylated. In a study which tested the methylation status of the \"MGMT\" promoter on 19 colorectal samples, 8 samples were found to be methylated. Another study compared the predictive power of \"MGMT\" promoter methylation in 83 high grade glioma patients obtained by either MSP, pyrosequencing, and HRM. The HRM method was found to be at least equivalent to pyrosequencing in quantifying the methylation level .\n\nMethylated DNA can be treated by bi-sulphite modification, which converts non-methylated cytosines to uracil. Therefore, PCR products resulting from a template that was originally unmethylated will have a lower melting point than those derived from a methylated template. HRM also offers the possibility of determining the proportion of methylation in a given sample, by comparing it to a standard curve which is generated by mixing different ratios of methylated and non-methylated DNA together. This can offer information regarding the degree of methylation that a tumour may have and thus give an indication of the character of the tumour and how far it deviates from what is \"normal\".\n\nHRM also is practically advantageous for use in diagnostics, due to its capacity to be adapted to high throughput screening testing, and again it minimises the possibility of amplicon spread and contamination within a laboratory, owing to its closed-tube format.\n\nTo follow the transition of dsDNA (double-stranded) to ssDNA (single-stranded), intercalating dyes are employed. These dyes show differential fluorescence emission dependent on their association with double-stranded or single-stranded DNA. SYBR Green I is a first generation dye for HRM. It fluoresces when intercalated into dsDNA and not ssDNA. Because it may inhibit PCR at high concentrations, it is used at sub-saturating concentrations. Recently, some researchers have discouraged the use of SYBR Green I for HRM, claiming that substantial protocol modifications are required. This is because it is suggested that the lack of accuracy may result from \"dye jumping\", where dye from a melted duplex may get reincorporated into regions of dsDNA which had not yet melted. New saturating dyes such as LC Green and LC Green Plus, ResoLight, EvaGreen, Chromofy and SYTO 9 are available on the market and have been used successfully for HRM. However, some groups have successfully used SYBR Green I for HRM with the Corbett Rotorgene instruments and advocate the use of SYBR Green I for HRM applications.\n\nHigh resolution melting assays typically involve qPCR amplification followed by a melting curve collected using a fluorescent dye. Due to the sensitivity of high-resolution melting analysis, it is necessary to carefully consider PCR cycling conditions, template DNA quality, and melting curve parameters. For accurate and repeatable results, PCR thermal cycling conditions must be optimized to ensure that the desired DNA region is amplified with high specificity and minimal bias between sequence variants. The melting curve is typically performed across a broad range of temperatures in small (~0.3 °C) increments that are long enough (~10 seconds) for the DNA to reach equilibrium at each temperature step.\n\nIn addition to typical primer design considerations, the design of primers for high-resolution melting assays involves maximizing the thermodynamic differences between PCR products belonging to different genotypes. Smaller amplicons generally yield greater melting temperature variation than longer amplicons, but the variability cannot be predicted by eye. For this reason, it is critical to accurately predict the melting curve of PCR products when designing primers that will distinguish sequence variants. Specialty software, such as uMelt and DesignSignatures, are available to help design primers that will maximize melting curve variability specifically for high-resolution melting assays.\n\n\n", "id": "21003747", "title": "High Resolution Melt"}
{"url": "https://en.wikipedia.org/wiki?curid=9129848", "text": "CRT (genetics)\n\nCRT is the gene cluster responsible for the biosynthesis of carotenoids. Those genes are found in eubacteria, in algae and are cryptic in \"Streptomyces griseus\".\n", "id": "9129848", "title": "CRT (genetics)"}
{"url": "https://en.wikipedia.org/wiki?curid=16725900", "text": "MEGAN\n\nMEGAN (\"MEtaGenome ANalyzer\") is a computer program that allows optimized analysis of large metagenomic datasets.\n\nMetagenomics is the analysis of the genomic sequences from a usually uncultured environmental sample. A large term goal of most metagenomics is to inventory and measure the extent and the role of microbial biodiversity in the ecosystem due to discoveries that the diversity of microbial organisms and viral agents in the environment is far greater than previously estimated. Tools that allow the investigation of very large data sets from environmental samples using shotgun sequencing techniques in particular, such as MEGAN, are designed to sample and investigate the unknown biodiversity of environmental samples where more precise techniques with smaller, better known samples, cannot be used.\n\nFragments of DNA from an metagenomics sample, such as ocean waters or soil, are compared against databases of known DNA sequences using BLAST or another sequence comparison tool to assemble the segments into discrete comparable sequences. MEGAN is then used to compare the resulting sequences with gene sequences from GenBank in NCBI. The program was used to investigate the DNA of a mammoth recovered from the Siberian permafrost and Sargasso Sea data set.\n\nMetagenomics is the study of genomic content of samples from same habitat, which is designed to determine the role and the extent of species diversity. Targeted or random sequencing are widely used with comparisons against sequence databases. Recent developments in sequencing technology increased the number of metagenomics samples. MEGAN is an easy to use tool for analysing such metagenomics data. First version of MEGAN was released in 2007 and the most recent version is MEGAN6. First version is capable of analysing taxonomic content of a single dataset while the latest version can analyse multiple datasets including new features (query different databases, new algorithm etc.).\n\nMEGAN analysis starts with collecting reads from any shotgun platform. Then, the reads are compared with sequence databases using BLAST or similar. Third, MEGAN assigns a taxon ID to processed read results based on NCBI taxonomy which creates a MEGAN file that contains required information for statistical and graphical analysis. Lastly, lowest common ancestor (LCA) algorithm can be run to inspect assignments, to analyze data and to create summaries of data based on different NCBI taxonomy levels. LCA algorithm simply finds the lowest common ancestor of different species.\n\nLatest version of MEGAN can be downloaded here. It is available in Windows, MAC and Unix platforms. The Community edition is open-source and free to use, the Ultimate edition with command-line support is licensed by Computomics.\n\nMEGAN can be used to explore taxonomic diversification of the dataset which could be collected from any type of metagenomic project or sequencing platform. In pre-processing step, the set of DNA reads is compared with sequence databases which can be computationally exhaustive and computationally complex for a standard user. MEGAN makes such a task easy and data analyses can be made on a workstation after completing sequence comparison on a computer cluster. In addition to that, functional analysis using SEED, functional analysis using KEGG and functional analysis using COG/EGGNOG is possible. Principal coordinate analysis (PCoA) is also available in the latest version for taxonomy and functional profiles. Comparative visualization options also provides extra functionality to display and present data.\n", "id": "16725900", "title": "MEGAN"}
{"url": "https://en.wikipedia.org/wiki?curid=21677056", "text": "Niche adaptation\n\nNiche adaptation refers to the ability of some organisms to adapt to changing environments, or niches. Genetic mechanisms of this adaptation include horizontal gene transfer, gene duplication, and gene shuffling. Adaptations are the result of Evolution and allow an organism to live in new environments. All organisms must fill some ecological niche and, at some point, species must adapt to fill newer niches if they are to survive in a changing world. Mechanisms involved in evolving new abilities to exploit new environments happen on the genetic scale. Genes mix and match in processes like horizontal gene transfer and gene or genome duplications. Transposable elements and plasmid or phage introduced genomic islands also provide organisms with the genetic tools they need to adapt to newer environments.\n\nAdaptations can result in obviously visible morphological differences or increased metabolic pathways that are not as easily detectable and require genetic analysis to confirm. For pathogenic organisms, this usually means increasing virulence. In the case of wilt-fungi like Verticillium dahliae and Verticillium albo-atrum, the increased virulence is dependent on homologous genes picked up from some incidence of horizontal gene transfer with a bacterium early in its evolutionary history. For ocean dwelling cyanobacteria, adapting to colonize new territories has led to clades of Prochlorococcus becoming specialized to thrive in the low light and colder temperatures of the deeper depths in the ocean. Additionally, resistance to harsh compounds is also important for adaptation. The aerobic ammonia oxidizing bacteria Nitrosomonas eutropha C91 contains genomic islands that house genes important in providing resistance to heavy metals and oxidizing nitrogen. As a result, the species occupies environments with elevated N concentrations. Some of these genes are unique to this species meaning no other ammonia oxidizing bacteria could survive in the climates that N. eutropha C91 does, giving it a significant fitness advantage.\n\nThe most notable example of niche adaptation comes from the original subject of the topic of evolution, the finches of the Galapagos Islands. The different species of finches that inhabit the different islands were famously one of the main sources of evidence Charles Darwin used when writing his book, On the Origin of Species. Darwin noted that the birds varied in beak size and shape, and that the sizes and shapes of the beaks were perfectly suited to match the source of seeds on the specific islands the birds inhabited. In a recent study by Lamichhaney et. all, the genetic source of this difference was discovered in the form of haplotypes of the ALX1 gene. Using this gene to highlight beak diversity, along with whole genome sequencing of 120 individuals from the different islands, the researchers concluded that individual finches of different species had more similar beak shapes and sizes with birds inhabiting the same islands, and thus feeding from the same sources of food, than with birds of the same species that lived on different islands.\n\nHorizontal Gene Transfer (HGT) has long been known to contribute to genetic recombination and variation for prokaryotic organisms like bacteria but, the extent to which it contributes has been disputed. Recently, experiments have indicated that HGT might be more responsible for more genetic variation than simple accumulation of mutations. Additionally, HGT can occur between any combination of organisms as distantly related as animals, bacteria, archaea, fungi, and plants. Gene cassettes, plasmids, transposons, and viruses are all known methods of the transfer of genes between individuals, strains, and species of organisms.\n\nBacteria can incorporate DNA from many different species into their own genetic libraries. Two of the most common methods involve plasmids and bacteriophages. Incorporation of DNA from plasmids between two bacterial cells usually involves the creation of a pilus. One bacteria, the donor cell, initiates transmition by extending the pilus to connect with the acceptor cell. The DNA from the plasmid is cut by an endonuclease and a single strand is transferred to the donor cell. Both cells replicate the single strand into a double strand. Plasmids can become incorporated into the full bacterial chromosome. The restriction sites still exist, meaning various lengths of DNA can be transposed from one cell to another directly from the chromosome. In this way non plasmid bacterial genes neighboring the plasmid genes can also migrate from cell to cell.\n\nBacteriophages can cause the chromosomes of their infected hosts to break up into segments small enough to fit into the newly constructed viral particles. If some of these bacterial genes are caught up into the phage, they can be released into the next infected individual. This new host can incorporate the DNA from the first bacteria into its own chromosome.\n\nMany animals use their adaptations not just to survive in their environments, but to modify their surroundings to maximize their abilities to thrive. Humans, earthworms and beavers are good examples.\n\nHuman beings have constructed large civilizations in environments we are not perfectly suited for. Insulated homes, extensive piping networks, and air conditioning/ heating units are examples of how cities like Phoenix, Arizona and Fairbanks, Alaska can support human populations year round. Earthworms generally inhabit the topsoil layer of the ground and their lifestyle and metabolic processes work to increase the size of the topsoil layer and improve the quality of the soil they inhabit. This in turn increases the amount of plant matter available to eventually die and decompose and provide the food for the earthworms. Beavers create dams and lodges that dramatically change the ecosystems they inhabit. Families will maintain these changed ecosystems for generations. In all three cases, the construction and maintenance of ecological niches helps drive the continued selection of traditional human, worm, and beaver genes which influences the evolution of the organisms for generations.\n\n", "id": "21677056", "title": "Niche adaptation"}
{"url": "https://en.wikipedia.org/wiki?curid=21879316", "text": "Low copy repeats\n\nLow copy repeats (LCRs), also known as segmental duplications (SDs), are highly homologous sequence elements within the eukaryotic genome. They are typically 10–300 kb in length, and bear greater than 95% sequence identity. Though rare in most mammals, LCRs comprise a large portion of the human genome owing to a significant expansion during primate evolution.\n\nMisalignment of LCRs during non-allelic homologous recombination (NAHR) is an important mechanism underlying the chromosomal microdeletion disorders as well as their reciprocal duplication partners. Many LCRs are concentrated in \"hotspots\", such as the 17p11-12 region, 27% of which is composed of LCR sequence. NAHR and non-homologous end joining (NHEJ) within this region are responsible for a wide range of disorders, including Charcot–Marie–Tooth syndrome type 1A, hereditary neuropathy with liability to pressure palsies, Smith–Magenis syndrome, and Potocki–Lupski syndrome.\n\n", "id": "21879316", "title": "Low copy repeats"}
{"url": "https://en.wikipedia.org/wiki?curid=21893031", "text": "Homeotic gene\n\nIn evolutionary developmental biology, homeotic genes are genes which regulate the development of anatomical structures in various organisms such as echinoderms, insects, mammals, and plants. This regulation is done via the programming of various transcription factors by the homeotic genes, and these factors affect genes through regulatory genetic pathways.\n\nMutations in homeotic genes cause displaced body parts, such as antennae growing at the posterior of the fly instead of at the head. Mutations that lead to such ectopic placements are usually lethal.\n\nThere are several subsets of homeotic genes. They include many of the Hox and ParaHox genes that are important for segmentation. Hox genes are found in bilaterian animals, including \"Drosophila\" (in which they were first discovered) and humans. Hox genes are a subset of the homeobox genes. The Hox genes are often conserved across species, so some of the Hox genes of \"Drosophila\" are homologous to those in humans. In general, Hox genes play a role of regulating expression of genes as well as aiding in development and assignment of specific structures during embryonic growth. This can range from segmentation in \"Drosophila\" to central nervous system (CNS) development in vertebrates.\n\nThey also include the MADS-box-containing genes involved in the ABC model of flower development. Besides flower-producing plants, the MADS-box motif is also present in other organisms such as insects, yeasts, and mammals. They have various functions depending on the organism including flower development, proto-oncogene transcription, and gene regulation in specific cells (such as muscle cells).\n\nDespite the terms being commonly interchanged, not all homeotic genes are Hox genes; the MADS-box genes are homeotic but not Hox genes. Thus, the Hox genes are a subset of homeotic genes.\n\nOne of the most commonly studied model organisms in regards to homeotic genes is the fruit fly \"Drosophila melanogaster\". Its homeotic genes occur in either the Antennapedia complex (ANT-C) or the Bithorax complex (BX-C) discovered by Edward B. Lewis. Each of the complexes focuses on a different area of development. The antennapedia complex consists of five genes, including proboscipedia, and is involved in the development of the front of the embryo, forming the segments of the head and thorax. The bithorax complex consists of three main genes and is involved in the development of the back of the embryo, namely the abdomen and the posterior segments of the thorax.\n\nDuring development (starting at the blastoderm stage of the embryo), these genes are constantly expressed to assign structures and roles to the different segments of the fly's body. For \"Drosophila\", these genes can be analyzed using the Flybase database.\n\nMuch research has been done on homeotic genes in different organisms, ranging from basic understanding of how the molecules work to mutations to how homeotic genes affect the human body. Changing the expression levels of homeotic genes can negatively impact the organism. For example, in one study, a pathogenic phytoplasma caused homeotic genes in a flowering plant to either be significantly upregulated or downregulated. This led to severe phenotypic changes including dwarfing, defects in the pistils, hypopigmentation, and the development of leaf-like structures on most floral organs. In another study, it was found that the homeotic gene \"Cdx2\" acts as a tumor suppressor. In normal expression levels, the gene prevents tumorgenesis and colorectal cancer when exposed to carcinogens; however, when \"Cdx2\" was not well expressed, carcinogens caused tumor development. These studies, along with many others, show the importance of homeotic genes even after development.\n\n\n", "id": "21893031", "title": "Homeotic gene"}
{"url": "https://en.wikipedia.org/wiki?curid=21904452", "text": "Imitation SWI\n\nThe protein ISWI or imitation SWI of \"drosophila melanogaster\" (common fruit fly), is the first ATPase subunit which has been isolated in the ISWI chromatin remodeling family. This protein presents high level of similarity to the SWI/SNF chromatin remodeling family in the ATPase domain. Outside the ATPase domain ISWI loses the similarity with the member of the SWI/SNF family, possessing a SANT domain instead of the bromodomain. The protein ISWI can interact with several proteins giving three different chromatin-remodeling complexes in \"Drosophila melanogaster\": NURF(nucleosome remodeling factor), CHRAC (chromatin remodeling and assembly complex) and ACF (ATP-utilising chromatin remodeling and assembly Factor).\n\n\"In vitro\", the ISWI protein alone can assemble nucleosomes on linear DNA and it can move nucleosomes on linear DNA from the center to the extremities. Inside the CHRAC complex, ISWI catalyzes the inverse reaction, moving nucleosomes from the extremities to the center.\n\nA single molecule study using atomic force microscopy (AFM) and tethered particle motion (TPM) has observed that ISWI can bound naked DNA in absence ATP wrapping DNA around the protein. In presence of ATP, the protein generates DNA loops while simultaneously generating negative supercoils in the template.<ref name=\"DOI 10.1002/jbio.200810027 \"></ref> The first figure in this paper shows three AFM images from where single DNA interacting with ISWI was deposed on mica surfaces. On the center, a single ISWI is bound near the end of a dsDNA template. The right image shows two DNA loops generated by ISWI. These loops contains supercoils.\nThe TPM study instead showed that the duration of loop formed by ISWI was ATP-dependent.\n", "id": "21904452", "title": "Imitation SWI"}
{"url": "https://en.wikipedia.org/wiki?curid=21526715", "text": "Syntelic\n\nSyntelic attachment occurs when both sister chromosomes are attached to a single spindle pole.\n\nNormal cell division distributes the genome equally between two daughter cells, with each chromosome attaching to an ovoid structure called the spindle. During the division process, errors commonly occur in attaching the chromosomes to the spindle, estimated to affect 86 to 90 percent of chromosomes.\n\nSuch attachment errors are common during the early stages of spindle formation, but they are mostly corrected before the start of anaphase.\nSuccessful cell division requires identification and correction of any dangerous errors before the cell splits in two.\nIf the syntelic attachment continues, it causes both sister chromatids to be segregated to a single daughter cell.\n", "id": "21526715", "title": "Syntelic"}
{"url": "https://en.wikipedia.org/wiki?curid=22074838", "text": "Fusion transcript\n\nFusion transcript is a chimeric RNA encoded by a fusion gene or by two different genes by subsequent trans-splicing. Certain fusion transcripts are commonly produced by cancer cells, and detection of fusion transcripts is part of routine diagnostics of certain cancer types.\n", "id": "22074838", "title": "Fusion transcript"}
{"url": "https://en.wikipedia.org/wiki?curid=22359140", "text": "Phred base calling\n\nPhred base-calling is a computer program for identifying a base (nucleobase) sequence from a fluorescence \"trace\" data generated by an automated DNA sequencer that uses electrophoresis and 4-fluorescent dye method. When originally developed, Phred produced significantly fewer errors in the data sets examined than other methods, averaging 40–50% fewer errors. Phred quality scores have become widely accepted to characterize the quality of DNA sequences, and can be used to compare the efficacy of different sequencing methods.\n\nThe fluorescent-dye DNA sequencing is a molecular biology technique that involves labeling single-strand DNA sequences of varied length with 4 fluorescent dyes (corresponding to 4 different bases used in DNA) and subsequently separating the DNA sequences by \"slab gel\"- or capillary-electrophoresis method (see DNA Sequencing). The electrophoresis run is monitored by a CCD on the DNA sequencer and this produces a time \"trace\" data (or \"chromatogram\") of the fluorescent \"peaks\" that passed the CCD point. Examining the fluorescence peaks in the trace data, we can determine the order of individual bases (nucleobase) in the DNA. Since the intensity, shape and the location of a fluorescence peak are not always consistent or unambiguous, however, sometimes it is difficult or time-consuming to determine (or \"call\") the correct bases for the peaks accurately if it is done manually.\n\nAutomated DNA sequencing techniques have revolutionized the field of molecular biology – generating vast amounts of DNA sequence data. However, the sequence data is produced at a significantly higher rate than can be manually processed (i.e. interpreting the trace data to produce the sequence data), thereby creating a bottleneck. To remove the bottleneck, both automated software that can speed up the processing with improved accuracy and a reliable measure of the accuracy are needed. To meet this need, many software programs have been developed. One such program is Phred.\n\nPhred was originally conceived in the early 1990s by Phil Green, then a professor at Washington University in St. Louis. LaDeana Hillier, Michael Wendl, David Ficenec, Tim Gleeson, Alan Blanchard, and Richard Mott also contributed to the codebase and algorithm. Green moved to University of Washington in the mid 1990s, after which development was primarily managed by himself and Brent Ewing. Phred played a notable role in the Human Genome Project, where large amounts of sequence data were processed by automated scripts. It is currently the most widely used base-calling software program by both academic and commercial DNA sequencing laboratories because of its high base calling accuracy. Phred is distributed commercially by CodonCode Corporation, and used to perform the \"Call bases\" function in the program CodonCode Aligner. It is also used by the MacVector plugin Assembler.\n\nPhred uses a four-phase procedure as outlined by Ewing \"et al.\" to determine a sequence of base calls from the processed DNA sequence tracing: \nThe entire procedure is rapid, usually taking less than half a second per trace.\n\nPhred is often used together with another software program called Phrap, which is a program for DNA sequence assembly. Phrap was routinely used in some of the largest sequencing projects in the Human Genome Sequencing Project and is currently one of the most widely used DNA sequence assembly programs in the biotech industry. Phrap uses Phred quality scores to determine highly accurate consensus sequences and to estimate the quality of the consensus sequences. Phrap also uses Phred quality scores to estimate whether discrepancies between two overlapping sequences are more likely to arise from random errors, or from different copies of a repeated sequence.\n\n", "id": "22359140", "title": "Phred base calling"}
{"url": "https://en.wikipedia.org/wiki?curid=8476844", "text": "Common disease-common variant\n\nThe common disease-common variant (often abbreviated CD-CV) hypothesis predicts that common disease-causing alleles, or variants, will be found in all human populations which manifest a given disease. Common variants (not necessarily disease-causing) are known to exist in coding and regulatory sequences of genes. According to the CD-CV hypothesis, some of those variants lead to susceptibility to complex polygenic diseases. Each variant at each gene influencing a complex disease will have a small additive or multiplicative effect on the disease phenotype. These diseases, or traits, are evolutionarily neutral in part because so many genes influence the traits. The hypothesis has held true in the case of putative causal variants in apolipoprotein E, including \"APOE\" ε4, associated with Alzheimer's disease. IL23R has been found to be associated with Crohn's disease; the at-risk allele has a frequency 93% in the general population .\n\nOne common form of variation across human genomes is called a single nucleotide polymorphism (SNP). As indicated by the name, SNPs are single base changes in the DNA. SNP variants tend to be common in different human populations. These polymorphisms have been valuable as genomic signposts, or \"markers\", in the search for common variants that influence susceptibility to common diseases. Recently published research has linked common SNPs to diseases such as type 2 diabetes, Alzheimer's, schizophrenia and hypertension.\n\n", "id": "8476844", "title": "Common disease-common variant"}
{"url": "https://en.wikipedia.org/wiki?curid=9893901", "text": "AMELX\n\nAmelogenin, X isoform is a protein that in humans is encoded by the \"AMELX\" (amelogenin, X isoform) gene.\n\nThe protein Amelogenin, X isoform is an isoform of amelogenin that comes from the X chromosome. The protein Amelogenin is a type of extracellular matrix protein, and is involved in the progcess of amelogenesis, the formation of enamel on teeth. Amelogenin X is a member of the amelogenin family of extracellular matrix proteins. When alternative splicing occurs, it results in multiple transcript variants encoding different isoforms, which in humans results in amelogenin genes on both the X and Y chromosomes.\n\nAMELX is involved in biomineralization during tooth enamel development. The AMELX gene encodes for the structural modeling protein, amelogenin, which works with other amelogenesis-related proteins to direct the mineralisation of enamel. This process involves the organization of enamel rods, the basic unit of tooth enamel, as well as the inclusion and growth of hydroxyapatite crystals.\n\nMutations in the AMELX gene can result in amelogenesis imperfecta, which refers to the collection of enamel defects resulting from either genetic or environmental causes. It has been shown that mice with a knocked-out AMELX gene will present disorganized and hypoplastic enamel.\n\n\n", "id": "9893901", "title": "AMELX"}
{"url": "https://en.wikipedia.org/wiki?curid=9263973", "text": "AMELY\n\nAmelogenin, Y isoform is a protein that in humans is encoded by the \"AMELY\" (amelogenin, Y-linked) gene.\n\nAMELY is a gene which encodes a form of amelogenin found on the Y chromosome. Amelogenin is a member of a family of extracellular matrix proteins. They are involved in biomineralization during tooth enamel development. Mutations in the related AMELX gene on the X chromosome cause X-linked amelogenesis imperfecta.\n\n\n", "id": "9263973", "title": "AMELY"}
{"url": "https://en.wikipedia.org/wiki?curid=22784732", "text": "Chimera (EST)\n\nIn genetics, a chimera is a single cDNA sequence originating from two transcripts. It is usually considered to be a contaminant in transcript and expressed sequence tag (EST) databases. It is estimated that approximately 1% of all transcripts in the National Center for Biotechnology Information's Unigene database contain a \"chimeric sequence\".\n\nMethods have been devised to detect them, such as the Ribosomal Database Project's CHECK_CHIMERA program. However, in protein engineering, \"chimeragenesis (forming chimeras between proteins that are encoded by homologous cDNAs)\" is one of the \"two major techniques used to manipulate cDNA sequences\".\n\n", "id": "22784732", "title": "Chimera (EST)"}
{"url": "https://en.wikipedia.org/wiki?curid=4145940", "text": "Haplogroup Z\n\nIn human mitochondrial genetics, Haplogroup Z is a human mitochondrial DNA (mtDNA) haplogroup.\nHaplogroup Z is believed to have arisen in Central Asia, and is a descendant of haplogroup CZ.\n\nIts greatest clade diversity is found in Korea, northern China, and Central Asia. However, its greatest frequency appears in some peoples of Russia and among the Saami people of northern Scandinavia.\n\nThis phylogenetic tree of haplogroup Z subclades is based on the paper by Mannis van Oven and Manfred Kayser \"Updated comprehensive phylogenetic tree of global human mitochondrial DNA variation\" and subsequent published research.\n\n\n\n", "id": "4145940", "title": "Haplogroup Z"}
{"url": "https://en.wikipedia.org/wiki?curid=26214", "text": "Reverse transcriptase\n\nA reverse transcriptase (RT) is an enzyme used to generate complementary DNA (cDNA) from an RNA template, a process termed \"reverse transcription\". It is mainly associated with retroviruses. However, non-retroviruses also use RT (for example, the hepatitis B virus, a member of the Hepadnaviridae, which are dsDNA-RT viruses, while retroviruses are ssRNA viruses). RT inhibitors are widely used as antiretroviral drugs. RT activities are also associated with the replication of chromosome ends (telomerase) and some mobile genetic elements (retrotransposons).\n\nRetroviral RT has three sequential biochemical activities: \nThese activities are used by the retrovirus to convert single-stranded genomic RNA into double-stranded cDNA which can integrate into the host genome, potentially generating a long-term infection that can be very difficult to eradicate. The same sequence of reactions is widely used in the laboratory to convert RNA to DNA for use in molecular cloning, RNA sequencing, polymerase chain reaction (PCR), or genome analysis.\n\nWell studied reverse transcriptases include:\n\nReverse transcriptases were discovered by Howard Temin at the University of Wisconsin–Madison in RSV virions and independently isolated by David Baltimore in 1970 at MIT from two RNA tumour viruses: R-MLV and again RSV. For their achievements, both shared the 1975 Nobel Prize in Physiology or Medicine (with Renato Dulbecco).\n\nThe idea of reverse transcription was very unpopular at first, as it contradicted the central dogma of molecular biology, which states that DNA is transcribed into RNA, which is then translated into proteins. However, in 1970, when the scientists Howard Temin and David Baltimore both independently discovered the enzyme responsible for reverse transcription, named reverse transcriptase, the possibility that genetic information could be passed on in this manner was finally accepted.\n\nThe enzymes are encoded and used by reverse transcribing viruses, which use the enzyme during the process of replication. Reverse-transcribing RNA viruses, such as retroviruses, use the enzyme to reverse-transcribe their RNA genomes into DNA, which is then integrated into the host genome and replicated along with it. Reverse-transcribing DNA viruses, such as the hepadnaviruses, can allow RNA to serve as a template in assembling and making DNA strands. HIV infects humans with the use of this enzyme. Without reverse transcriptase, the viral genome would not be able to incorporate into the host cell, resulting in failure to replicate.\n\nReverse transcriptase creates double-stranded DNA from an RNA template.\n\nIn virus species with reverse transcriptase lacking DNA-dependent DNA polymerase activity, creation of double-stranded DNA can possibly be done by host-encoded DNA polymerase δ, mistaking the viral DNA-RNA for a primer and synthesizing a double-stranded DNA by similar mechanism as in primer removal, where the newly synthesized DNA displaces the original RNA template.\n\nThe process of reverse transcription is extremely error-prone, and it is during this step that mutations may occur. Such mutations may cause drug resistance.\n\nRetroviruses, also referred to as class VI ssRNA-RT viruses, are RNA reverse-transcribing viruses with a DNA intermediate. Their genomes consist of two molecules of positive-sense single-stranded RNA with a 5' cap and 3' polyadenylated tail. Examples of retroviruses include the human immunodeficiency virus (HIV) and the human T-lymphotropic virus (HTLV). Creation of double-stranded DNA occurs in the cytosol as a series of these steps:\n\nCreation of double-stranded DNA also involves \"strand transfer\", in which there is a translocation of short DNA product from initial RNA-dependent DNA synthesis to acceptor template regions at the other end of the genome, which are later reached and processed by the reverse transcriptase for its DNA-dependent DNA activity.\n\nRetroviral RNA is arranged in 5’ terminus to 3’ terminus. The site where the primer is annealed to viral RNA is called the primer-binding site (PBS). The RNA 5’end to the PBS site is called U5, and the RNA 3’ end to the PBS is called the leader. The tRNA primer is unwound between 14 and 22 nucleotides and forms a base-paired duplex with the viral RNA at PBS. The fact that the PBS is located near the 5’ terminus of viral RNA is unusual because reverse transcriptase synthesize DNA from 3’ end of the primer in the 5’ to 3’ direction (with respect to the RNA template). Therefore, the primer and reverse transcriptase must be relocated to 3’ end of viral RNA. In order to accomplish this reposition, multiple steps and various enzymes including DNA polymerase, ribonuclease H(RNase H) and polynucleotide unwinding are needed.\n\nThe HIV reverse transcriptase also has ribonuclease activity that degrades the viral RNA during the synthesis of cDNA, as well as DNA-dependent DNA polymerase activity that copies the sense cDNA strand into an \"antisense\" DNA to form a double-stranded viral DNA intermediate (vDNA).\n\nSelf-replicating stretches of eukaryotic genomes known as retrotransposons utilize reverse transcriptase to move from one position in the genome to another via an RNA intermediate. They are found abundantly in the genomes of plants and animals. Telomerase is another reverse transcriptase found in many eukaryotes, including humans, which carries its own RNA template; this RNA is used as a template for DNA replication.\n\nInitial reports of reverse transcriptase in prokaryotes came as far back as 1971 (Beljanski et al., 1971a, 1972). These have since been broadly described as part of bacterial Retrons, distinct sequences that code for reverse transcriptase, and are used in the synthesis of msDNA. In order to initiate synthesis of DNA, a primer is needed. In bacteria, the primer is synthesized during replication.\n\nValerian Dolja of Oregon State argues that viruses, due to their diversity, have played an evolutionary role in the development of cellular life, with reverse transcriptase playing a central role.\n\nReverse transcriptase enzymes include an RNA-dependent DNA polymerase and a DNA-dependent DNA polymerase, which work together to perform reverse transcription. In addition to the transcription function, retroviral reverse transcriptases have a domain belonging to the RNase H family, which is vital to their replication.\n\nThere are three different replication systems during the life cycle of a retrovirus. First of all, the reverse transcriptase synthesizes viral DNA from viral RNA, and then from newly made complementary DNA strand. The second replication process occurs when host cellular DNA polymerase replicates the integrated viral DNA. Lastly, RNA polymerase II transcribes the proviral DNA into RNA, which will be packed into virions. Therefore, mutation can occur during one or all of these replication steps.\n\nReverse transcriptase has a high error rate when transcribing RNA into DNA since, unlike most other DNA polymerases, it has no proofreading ability. This high error rate allows mutations to accumulate at an accelerated rate relative to proofread forms of replication. The commercially available reverse transcriptases produced by Promega are quoted by their manuals as having error rates in the range of 1 in 17,000 bases for AMV and 1 in 30,000 bases for M-MLV.\n\nOther than creating single-nucleotide polymorphisms, reverse transcriptases have also been shown to be involved in processes such as transcript fusions, exon shuffling and creating artificial antisense transcripts. It has been speculated that this template switching activity of reverse transcriptase, which can be demonstrated completely \"in vivo\", may have been one of the causes for finding several thousand unannotated transcripts in the genomes of model organisms.\n\nAs HIV uses reverse transcriptase to copy its genetic material and generate new viruses (part of a retrovirus proliferation circle), specific drugs have been designed to disrupt the process and thereby suppress its growth. Collectively, these drugs are known as reverse-transcriptase inhibitors and include the nucleoside and nucleotide analogues zidovudine (trade name Retrovir), lamivudine (Epivir) and tenofovir (Viread), as well as non-nucleoside inhibitors, such as nevirapine (Viramune).\n\nReverse transcriptase is commonly used in research to apply the polymerase chain reaction technique to RNA in a technique called reverse transcription polymerase chain reaction (RT-PCR). The classical PCR technique can be applied only to DNA strands, but, with the help of reverse transcriptase, RNA can be transcribed into DNA, thus making PCR analysis of RNA molecules possible. Reverse transcriptase is used also to create cDNA libraries from mRNA. The commercial availability of reverse transcriptase greatly improved knowledge in the area of molecular biology, as, along with other enzymes, it allowed scientists to clone, sequence, and characterise RNA.\n\nReverse transcriptase has also been employed in insulin production. By inserting eukaryotic mRNA for insulin production along with reverse transcriptase into bacteria, the mRNA could be inserted into the prokaryote's genome. Large amounts of insulin can then be created, sidestepping the need to harvest pig pancreas and other such traditional sources. Directly inserting eukaryotic DNA into bacteria would not work because it carries introns, so would not translate successfully using the bacterial ribosomes. Processing in the eukaryotic cell during mRNA production removes these introns to provide a suitable template. Reverse transcriptase converted this edited RNA back into DNA so it could be incorporated in the genome.\n\n\n \n", "id": "26214", "title": "Reverse transcriptase"}
{"url": "https://en.wikipedia.org/wiki?curid=2524792", "text": "Marker gene\n\nIn biology, a marker gene may have several meanings. In nuclear biology and molecular biology, a marker gene is a gene used to determine if a nucleic acid sequence has been successfully inserted into an organism's DNA. In particular, there are two sub-types of these marker genes: a selectable marker and a marker for screening. In metagenomics and phylogenetics, a marker gene is an orthologous gene group which can be used to delineate between taxonomic lineages.\n\nA selectable marker protects the organism from a \"selective agent\" that would normally kill it or prevent its growth. In a transformation reaction, depending on the transformation efficiency, only one in a several million to billion cells may take up DNA. Rather than checking every single cell, scientists use a selective agent to kill all cells that do not contain the foreign DNA, leaving only the desired ones.\n\nAntibiotics are the most common selective agents. In bacteria, antibiotics are used almost exclusively. In plants, antibiotics that kill the chloroplast are often used as well, although tolerance to salts and growth-inhibiting hormones is becoming more popular. In mammals, resistance to antibiotics that would kill the mitochondria is used as a selectable marker.\n\nA screenable marker will make cells containing the gene look different. There are three types of screening commonly used:\n\n\n", "id": "2524792", "title": "Marker gene"}
{"url": "https://en.wikipedia.org/wiki?curid=6154524", "text": "Position-effect variegation\n\nPosition-effect variegation (PEV) is a variegation caused by the silencing of a gene in some cells through its abnormal juxtaposition with heterochromatin via rearrangement or transposition. It is also associated with changes in chromatin conformation.\n\nThe classical example is the \"Drosophila\" w (speak white-mottled-4) translocation. In this mutation, an inversion on the X chromosome placed the \"white\" gene next to pericentric heterochromatin, or a sequence of repeats that becomes heterochromatic. Normally, the \"white\" gene is expressed in every cell of the adult \"Drosophila\" eye resulting in a red-eye phenotype. In the w[m4] mutant, the eye color was variegated (red-white mosaic colored) where the \"white\" gene was expressed in some cells in the eyes and not in others. The mutation was described first by Hermann Muller in 1930. PEV is a \"heterochromatin-induced gene inactivation\". Gene silencing phenomena similar to this have also been observed in \"S. cerevisiae\" and \"S. pombe.\"\n\nTypically, the barrier DNA sequences prevent the heterochromatic region from spreading into the euchromatin but they are no longer present in the flies that inherit certain chromosomal rearrangements.\n\nPEV is a position effect because the change in position of a gene from its original position to somewhere near a heterochromatic region has an effect on its expression. The effect is the variegation in a particular phenotype i.e., the appearance of irregular patches of different colour(s), due to the expression of the original wild-type gene in some cells of the tissue but not in others, as seen in the eye of mutated \"Drosophila melanogaster.\"\n\nHowever, it is possible that the effect of the silenced gene is not phenotypically visible in some cases. PEV was observed first in \"Drosophila\" because it was one of the very first organisms on which X-ray irradiation was used as a mutation inducer. X-rays can cause chromosomal rearrangements that can result in PEV.\n\nAmong a number of models, two epigenetic models are popular. One is the cis\"-spreading of the heterochromatin past the rearrangement breakpoint. The trans\"-interactions come in when the \"cis-\"spreading model is unable to explain certain phenomena.\n\nAccording to this model, the heterochromatin forces an altered chromatin conformation on the euchromatic region. Due to this, the transcriptional machinery cannot access the gene which leads to the inhibition of transcription. In other words, the heterochromatin spreads and causes gene silencing by packaging the normally euchromatic region. But this model fails to explain some aspects of PEV. For example, variegation can be induced in a gene located several megabases from the heterochromatin-euchromatin breakpoint due to rearrangements in that breakpoint. Also, the austerity of the variegated phenotype can be altered by the distance of the heterochromatic region from the breakpoint.\n\nThis suggests that \"trans\"-interactions are crucial for PEV.\n\nThese are interactions between the different heterochromatic regions and the global chromosomal organisation in the interphase nucleus. The rearrangements due to PEV places the reporter gene in a new compartment of the nucleus where the transcriptional machinery required is not available, thus silencing the gene and modifying the chromatin structure.\n\nThese two mechanisms affect each other as well. Which mechanism dominates to influence the phenotype depends upon the type of heterochromatin and the intricacy of the rearrangement.\n\nThe mutations in \"mus\" genes are the candidates as PEV modifiers, as these genes are involved in chromosome maintenance and repair. Chromosome structure in the vicinity of the breakpoint appears to be an important determinant of the gene inactivation process. Six second chromosomal \"mus\" mutations were isolated with w. A copy of wild-type white gene was placed adjascent to heterochromatin. The different \"mus\" mutants that were taken were: \"mus\"201, \"mus\"205, \"mus\"208, \"mus\"209, \"mus\"210, \"mus\"211. A stock was constructed with the replacement of standard X-chromosome with w. It was observed that the suppression of PEV is not a characteristic of \"mus\" mutations in general. Only for homozygous \"mus\"209, the variegation was significantly suppressed. Also, when homozygous, 2735 and D-1368 and all heteroallelic combinations of its Pcna mutations strongly suppress PEV.\n\nIn mouse, variegating coat colour has been observed. When an autosomal region carrying a fur color gene is inserted onto the X chromosome, variable silencing of the allele is seen. Variegation is, however, observed only in the female having this insertion along with a homozygous mutation in the original coat color gene. The wild-type allele gets inactivated due to heterochromatinization.\n\nIn plants, PEV has been observed in \"Oenothera blandina\". The silencing of euchromatic genes occurs when the genes get placed into a new heterochromatic neighborhood.\n\n\n\n", "id": "6154524", "title": "Position-effect variegation"}
{"url": "https://en.wikipedia.org/wiki?curid=9192883", "text": "Compound heterozygosity\n\nCompound heterozygosity in medical genetics is the condition of having two heterogeneous recessive alleles at a particular locus that can cause genetic disease in a heterozygous state. That is, an organism is a compound heterozygote when it has two recessive alleles for the same gene, but with those two alleles being different from each other (for example, both alleles might be mutated but at different locations). Compound heterozygosity reflects the diversity of the mutation base for many autosomal recessive genetic disorders; mutations in most disease-causing genes have arisen many times. This means that many cases of disease arise in individuals who have two unrelated alleles, who technically are heterozygotes, but both the alleles are defective.\n\nThese disorders are often best known in some classic form, such as the homozygous recessive case of a particular mutation that is widespread in some population. In its compound heterozygous forms, the disease may have lower penetrance, because the mutations involved are often less deleterious in combination than for a homozygous individual with the classic symptoms of the disease. As a result, compound heterozygotes often become ill later in life, with less severe symptoms. Although compound heterozygosity as a cause of genetic disease had been suspected much earlier, widespread confirmation of the phenomenon was not feasible until the 1980s, when polymerase chain reaction techniques for amplification of DNA made it cost-effective to sequence genes and identify polymorphic alleles.\n\nCompound heterozygosity is one of the causes of variation in genetic disease. The diagnosis and nomenclature for such disorders sometimes reflects history, because most diseases were first observed and classified based on biochemistry and pathophysiology before genetic diagnosis was available. Some genetic disorders are really a family of related disorders that occur in the same metabolic pathway, or in related pathways. Naming conventions for the disease became established before precise molecular diagnosis was possible.\n\nFor example, hemochromatosis is the name given to several different heritable diseases with the same outcome, excess absorption of iron. These variants all reflect a failure in a metabolic pathway associated with iron metabolism, however mutations that cause hemochromatosis can occur at different gene loci. Mutations have occurred at each locus many times, and a few such mutations have become widespread in some population. The fact that multiple loci are involved is the primary cause for the variant forms of hemochromatosis and its outcome. This variation is caused not by compound heterozygosity, but rather by the fact that several different enzyme defects can cause the disease. Clinically, most cases of hemochromatosis are found in homozygotes for the most common mutation in the HFE gene. But at each gene locus associated with the disease, there is the possibility of compound heterozygosity, often caused by inheritance of two unrelated alleles, of which one is a common or classic mutation, while the other is a rare or even novel one.\n\nFor some genetic diseases, environmental cofactors are an important determinant of variation and outcome. In the case of hemochromatosis, penetrance is incomplete, even for the classic HFE mutation, and is affected by gender, diet, and behaviors such as alcohol consumption. Compound heterozygotes are often observed only through subclinical symptoms such as excess iron. Disease is rarely observed in such compound heterozygotes unless other causal factors (such as alcoholism) are present. As a result, compound heterozygosity for hemochromatosis may be more common than diagnosis based on pathology would suggest.\n\nSome genetic diseases are named more precisely, and represent a single point of failure on a metabolic pathway. For example, Tay-Sachs disease, GM2-gangliosidosis, AB variant, and Sandhoff disease might easily have been defined together as a single disease, because the three disorders are associated with failure of the same enzyme and have the same outcome. However, the three were discovered and named separately, and each represents a distinct molecular point of failure in a subunit that is required for activation of the enzyme. For all three disorders, compound heterozygosity is responsible for variant forms. For example, both TSD and Sandhoff disease have a more common infantile form and several late-onset variants. Post-infantile forms, which are rare, are generally caused by the inheritance of two unrelated alleles, of which one is usually a classic mutation, while the other is a rare or even novel one.\n\n", "id": "9192883", "title": "Compound heterozygosity"}
{"url": "https://en.wikipedia.org/wiki?curid=21777359", "text": "Virtual karyotype\n\nVirtual karyotype is the digital information reflecting a karyotype, resulting from the analysis of short sequences of DNA from specific loci all over the genome, which are isolated and enumerated. It detects genomic copy number variations at a higher resolution level than conventional karyotyping or chromosome-based comparative genomic hybridization (CGH). The main methods used for creating virtual karyotypes are array-comparative genomic hybridization and SNP arrays.\n\nA karyotype (Fig 1) is the characteristic chromosome complement of a eukaryote species. A karyotype is typically presented as an image of the chromosomes from a single cell arranged from largest (chromosome 1) to smallest (chromosome 22), with the sex chromosomes (X and Y) shown last. Historically, karyotypes have been obtained by staining cells after they have been chemically arrested during cell division. Karyotypes have been used for several decades to identify chromosomal abnormalities in both germline and cancer cells. Conventional karyotypes can assess the entire genome for changes in chromosome structure and number, but the resolution is relatively coarse, with a detection limit of 5-10Mb.\nRecently, platforms for generating high-resolution karyotypes \"in silico\" from disrupted DNA have emerged, such as array comparative genomic hybridization (arrayCGH) and SNP arrays. Conceptually, the arrays are composed of hundreds to millions of probes which are complementary to a region of interest in the genome. The disrupted DNA from the test sample is fragmented, labeled, and hybridized to the array. The hybridization signal intensities for each probe are used by specialized software to generate a log2ratio of test/normal for each probe on the array. Knowing the address of each probe on the array and the address of each probe in the genome, the software lines up the probes in chromosomal order and reconstructs the genome \"in silico\" (Fig 2 and 3).\n\nVirtual karyotypes have dramatically higher resolution than conventional cytogenetics. The actual resolution will depend on the density of probes on the array. Currently, the Affymetrix SNP6.0 is the highest density commercially available array for virtual karyotyping applications. It contains 1.8 million polymorphic and non-polymorphic markers for a practical resolution of 10-20kb—about the size of a gene. This is approximately 1000-fold greater resolution than karyotypes obtained from conventional cytogenetics.\n\nVirtual karyotypes can be performed on germline samples for constitutional disorders, and clinical testing is available from dozens of CLIA certified laboratories (genetests.org). Virtual karyotyping can also be done on fresh or formalin-fixed paraffin-embedded tumors. CLIA-certified laboratories offering testing on tumors include Creighton Medical Laboratories (fresh and paraffin embedded tumor samples) and CombiMatrix Molecular Diagnostics (fresh tumor samples).\n\nArray-based karyotyping can be done with several different platforms, both laboratory-developed and commercial. The arrays themselves can be genome-wide (probes distributed over the entire genome) or targeted (probes for genomic regions known to be involved in a specific disease) or a combination of both. Further, arrays used for karyotyping may use non-polymorphic probes, polymorphic probes (i.e., SNP-containing), or a combination of both. Non-polymorphic probes can provide only copy number information, while SNP arrays can provide both copy number and loss-of-heterozygosity (LOH) status in one assay. The probe types used for non-polymorphic arrays include cDNA, BAC clones (e.g., BlueGnome), and oligonucleotides (e.g., Agilent, Santa Clara, CA, USA or Nimblegen, Madison, WI, USA). Commercially available oligonucleotide SNP arrays can be solid phase (Affymetrix, Santa Clara, CA, USA) or bead-based (Illumina, SanDiego, CA, USA). Despite the diversity of platforms, ultimately they all use genomic DNA from disrupted cells to recreate a high resolution karyotype \"in silico\". The end product does not yet have a consistent name, and has been called virtual karyotyping, digital karyotyping, molecular allelokaryotyping, and molecular karyotyping. Other terms used to describe the arrays used for karyotyping include SOMA (SNP oligonucleotide microarrays) and CMA (chromosome microarray). Some consider all platforms to be a type of array comparative genomic hybridization (arrayCGH), while others reserve that term for two-dye methods, and still others segregate SNP arrays because they generate more and different information than two-dye arrayCGH methods.\n\nCopy number changes can be seen in both germline and tumor samples. Copy number changes can be detected by arrays with non-polymorphic probes, such as arrayCGH, and by SNP-based arrays. Human beings are diploid, so a normal copy number is always two for the non-sex chromosomes.\n\nAutozygous segments and uniparental disomy (UPD) are diploid/'copy neutral' genetic findings and therefore are only detectable by SNP-based arrays. Both autozygous segments and UPD will show loss of heterozygosity (LOH) with a copy number of two by SNP array karyotyping. The term Runs of Homozgygosity (ROH), is a generic term that can be used for either autozygous segments or UPD.\nFigure 7 is a SNP array virtual karyotype from a colorectal carcinoma demonstrating deletions, gains, amplifications, and acquired UPD (copy neutral LOH).\n\nA virtual karyotype can be generated from nearly any tumor, but the clinical meaning of the genomic aberrations identified are different for each tumor type. Clinical utility varies and appropriateness is best determined by an oncologist or pathologist in consultation with the laboratory director of the lab performing the virtual karyotype. Below are examples of types of cancers where the clinical implications of specific genomic aberrations are well established. This list is representative, not exhaustive. The web site for the Cytogenetics Laboratory at Wisconsin State Laboratory of Hygiene has additional examples of clinically relevant genetic changes that are readily detectable by virtual karyotyping.\n\nBased on a series of 493 neuroblastoma samples, it has been reported that overall genomic pattern, as tested by array-based karyotyping, is a predictor of outcome in neuroblastoma:\n\nEarlier publications categorized neuroblastomas into three major subtypes based on cytogenetic profiles:\n\nTumor-specific loss-of-heterozygosity (LOH) for chromosomes 1p and 16q identifies a subset of Wilms' tumor patients who have a significantly increased risk of relapse and death. LOH for these chromosomal regions can now be used as an independent prognostic factor together with disease stage to target intensity of treatment to risk of treatment failure.\n\nRenal epithelial neoplasms have characteristic cytogenetic aberrations that can aid in classification. See also Atlas of Genetics and Cytogenetics in Oncology and Haematology. \n\nArray-based karyotyping can be used to identify characteristic chromosomal aberrations in renal tumors with challenging morphology. Array-based karyotyping performs well on paraffin embedded tumors and is amenable to routine clinical use.\n\nIn addition, recent literature indicates that certain chromosomal aberrations are associated with outcome in specific subtypes of renal epithelial tumors.Clear cell renal carcinoma: del 9p and del 14q are poor prognostic indicators.Papillary renal cell carcinoma: duplication of 1q marks fatal progression.\n\nArray-based karyotyping is a cost-effective alternative to FISH for detecting chromosomal abnormalities in chronic lymphocytic leukemia (CLL). Several clinical validation studies have shown >95% concordance with the standard CLL FISH panel. In addition, many studies using array-based karyotyping have identified 'atypical deletions' missed by the standard FISH probes and acquired uniparental disomy at key loci for prognostic risk in CLL.\n\nFour main genetic aberrations are recognized in CLL cells that have a major impact on disease behavior.\n\nAvet-Loiseau, et al. in Journal of Clinical Oncology, used SNP array karyotyping of 192 multiple myeloma (MM) samples to identify genetic lesions associated with prognosis, which were then validated in a separate cohort (n = 273). In MM, lack of a proliferative clone makes conventional cytogenetics informative in only ~30% of cases. FISH panels are useful in MM, but standard panels would not detect several key genetic abnormalities reported in this study. \n\nArray-based karytyping cannot detect balanced translocations, such as t(4;14) seen in ~15% of MM. Therefore, FISH for this translocation should also be performed if using SNP arrays to detect genome-wide copy number alterations of prognostic significance in MM.\n\nArray-based karyotyping of 260 medulloblastomas by Pfister S, et al. resulted in the following clinical subgroups based on cytogenetic profiles:\n\nThe 1p/19q co-deletion is considered a \"genetic signature\" of oligodendroglioma. Allelic losses on 1p and 19q, either separately or combined, are more common in classic oligodendrogliomas than in either astrocytomas or oligoastrocytomas. In one study, classic oligodendrogliomas showed 1p loss in 35 of 42 (83%) cases, 19q loss in 28 of 39 (72%), and these were combined in 27 of 39 (69%) cases; there was no significant difference in 1p/19q loss of heterozygosity status between low-grade and anaplastic oligodendrogliomas. 1p/19q co-deletion has been correlated with both chemosensitivity and improved prognosis in oligodendrogliomas. Most larger cancer treatment centers routinely check for the deletion of 1p/19q as part of the pathology report for oligodendrogliomas. The status of the 1p/19q loci can be detected by FISH or virtual karyotyping. Virtual karyotyping has the advantage of assessing the entire genome in one assay, as well as the 1p/19q loci. This allows assessment of other key loci in glial tumors, such as EGFR and TP53 copy number status.\n\nWhereas the prognostic relevance of 1p and 19q deletions is well established for anaplastic oligodendrogliomas and mixed oligoastrocytomas, the prognostic relevance of the deletions for low-grade gliomas is more controversial. In terms of low-grade gliomas, a recent study also suggests that 1p/19q co-deletion may be associated with a (1;19)(q10;p10) translocation which, like the combined 1p/19q deletion, is associated with superior overall survival and progression-free survival in low-grade glioma patients. Oligodendrogliomas show only rarely mutations in the p53 gene, which is in contrast to other gliomas. Epidermal growth factor receptor amplification and whole 1p/19q codeletion are mutually exclusive and predictive of completely different outcomes, with EGFR amplification predicting poor prognosis.\n\nYin et al. studied 55 glioblastoma and 6 GBM cell lines using SNP array karyotyping. Acquired UPD was identified at 17p in 13/61 cases. A significantly shortened survival time was found in patients with 13q14 (RB) deletion or 17p13.1 (p53) deletion/acquired UPD. Taken together, these results suggest that this technique is a rapid, robust, and inexpensive method to profile genome-wide abnormalities in GBM. Because SNP array karyotyping can be performed on paraffin embedded tumors, it is an attractive option when tumor cells fail to grow in culture for metaphase cytogenetics or when the desire for karyotyping arises after the specimen has been formalin fixed.\n\nThe importance of detecting acquired UPD (copy neutral LOH) in glioblastoma:\n\nIn addition, in cases with uncertain grade by morphology, genomic profiling can assist in diagnosis.\n\nCytogenetics, the study of characteristic large changes in the chromosomes of cancer cells, has been increasingly recognized as an important predictor of outcome in acute lymphoblastic leukemia (ALL).NB: Balanced translocations cannot be detected by array-based karyotyping (see Limitations below).\n\nSome cytogenetic subtypes have a worse prognosis than others. These include:\n\n\nCorrelation of prognosis with bone marrow cytogenetic finding in acute lymphoblastic leukemia\nUnclassified ALL is considered to have an intermediate prognosis.\n\nMyelodysplastic syndrome (MDS) has remarkable clinical, morphological, and genetic heterogeneity. Cytogenetics play a decisive role in the World Health Organization's classification-based International Prognostic Scoring System (IPSS) for MDS. \n\nIn a comparison of metaphase cytogenetics, FISH panel, and SNP array karyotyping for MDS, it was found that each technique provided a similar diagnostic yield. No single method detected all defects, and detection rates improved by ~5% when all three methods were used.\n\nAcquired UPD, which is not detectable by FISH or cytogenetics, has been reported at several key loci in MDS using SNP array karyotyping, including deletion of 7/7q.\n\nPhiladelphia chromosome–negative myeloproliferative neoplasms (MPNs) including polycythemia vera, essential thrombocythemia, and primary myelofibrosis show an inherent tendency for transformation into leukemia (MPN-blast phase), which is accompanied by acquisition of additional genomic lesions.\nIn a study of 159 cases, SNP-array analysis was able to capture practically all cytogenetic abnormalities and to uncover additional lesions with potentially important clinical implications.\n\n\nIdentification of biomarkers in colorectal cancer is particularly important for patients with stage II disease, where less than 20% have tumor recurrence. 18q LOH is an established biomarker associated with high risk of tumor recurrence in stage II colon cancer. Figure 7 shows a SNP array karyotype of a colorectal carcinoma (whole genome view).\n\nColorectal cancers are classified into specific tumor phenotypes based on molecular profiles which can be integrated with the results of other ancillary tests, such as microsatellite instability testing, IHC, and KRAS mutation status:\n\nMalignant rhabdoid tumors are rare, highly aggressive neoplasms found most commonly in infants and young children. Due to their heterogenous histologic features, diagnosis can often be difficult and misclassifications can occur. In these tumors, the INI1 gene (SMARCB1)on chromosome 22q functions as a classic tumor suppressor gene. Inactivation of INI1 can occur via deletion, mutation, or acquired UPD.\n\nIn a recent study, SNP array karyotyping identified deletions or LOH of 22q in 49/51 rhabdoid tumors. Of these, 14 were copy neutral LOH (or acquired UPD), which is detectable by SNP array karyotyping, but not by FISH, cytogenetics, or arrayCGH. MLPA detected a single exon homozygous deletion in one sample that was below the resolution of the SNP array.\n\nSNP array karyotyping can be used to distinguish, for example, a medulloblastoma with an isochromosome 17q from a primary rhabdoid tumor with loss of 22q11.2. When indicated, molecular analysis of INI1 using MLPA and direct sequencing may then be employed. Once the tumor-associated changes are found, an analysis of germline DNA from the patient and the parents can be done to rule out an inherited or de novo germline mutation or deletion of INI1, so that appropriate recurrence risk assessments can be made.\n\nThe most important genetic alteration associated with poor prognosis in uveal melanoma is loss of an entire copy of Chromosome 3 (Monosomy 3), which is strongly correlated with metastatic spread. Gains on chromosomes 6 and 8 are often used to refine the predictive value of the Monosomy 3 screen, with gain of 6p indicating a better prognosis and gain of 8q indicating a worse prognosis in disomy 3 tumors. In rare instances, monosomy 3 tumors may duplicate the remaining copy of the chromosome to return to a disomic state referred to as isodisomy. Isodisomy 3 is prognostically equivalent to monosomy 3, and both can be detected by tests for chromosome 3 loss of heterozygosity.\n\nUnlike karyotypes obtained from conventional cytogenetics, virtual karyotypes are reconstructed by computer programs using signals obtained from \"disrupted\" DNA. In essence, the computer program will correct translocations when it lines up the signals in chromosomal order. Therefore, virtual karyotypes cannot detect balanced translocations and inversions. They also can only detect genetic aberrations in regions of the genome that are represented by probes on the array. In addition, virtual karyotypes generate a \"relative\" copy number normalized against a diploid genome, so tetraploid genomes will be condensed into a diploid space unless renormalization is performed. Renormalization requires an ancillary cell-based assay, such as FISH, if one is using arrayCGH. For karyotypes obtained from SNP-based arrays, tetraploidy can often be inferred from the maintenance of heterozygosity within a region of apparent copy number loss. Low-level mosaicism or small subclones may not be detected by virtual karyotypes because the presence of normal cells in the sample will dampen the signal from the abnormal clone. The exact point of failure, in terms of the minimal percentage of neoplastic cells, will depend on the particular platform and algorithms used. Many copy number analysis software programs used to generate array-based karyotypes will falter with less than 25–30% tumor/abnormal cells in the sample. However, in oncology applications this limitation can be minimized by tumor enrichment strategies and software optimized for use with oncology samples. The analysis algorithms are evolving rapidly, and some are even designed to thrive on ‘normal clone contamination’, so it is anticipated that this limitation will continue to dissipate.\n\n", "id": "21777359", "title": "Virtual karyotype"}
{"url": "https://en.wikipedia.org/wiki?curid=20722644", "text": "Genopolitics\n\nGenopolitics is the study of the genetic basis of political behavior and attitudes. It combines behavior genetics, psychology, and political science and it is closely related to the emerging fields of neuropolitics (the study of the neural basis of political behavior and attitudes) and political physiology (the study of biophysical correlates of political attitudes and behavior).\n\n\"The Chronicle of Higher Education\" recently reported on the increase in academicians' recognition of and engagement in genopolitics as a discrete field of study, and \"New York Times Magazine\" included genopolitics in its \"Eighth Annual Year in Ideas,\" noting that the term was originally coined by James Fowler.\n\nPsychologists and behavior geneticists began using twin studies in the 1980s to study variation in social attitudes, and these studies suggested that both genes and environment played a role. In particular, Nick Martin and his colleagues published an influential twin study of social attitudes in \"Proceedings of the National Academy of Sciences\" in 1986.\n\nHowever, this early work did not specifically analyze whether or not political orientations were heritable, and political scientists remained mostly unaware of the heritability of social attitudes until 2005. In that year, the \"American Political Science Review\" published a reanalysis of political questions on Martin's social attitude survey of twins in that the suggested liberal and conservative ideology is heritable. The article sparked considerable debate between critics, the authors and their defenders.\n\nInitial twin studies suggested that predispositions toward espousal of certain political ideas are heritable, but they said little about political behavior (patterns of voting and/or activism) or predispositions toward it. A 2008 article published in the \"American Political Science Review\" matched publicly available voter registration records to a twin registry in Los Angeles, analyzed self-reported voter turnout in the National Longitudinal Study of Adolescent Health (Add Health), and studied other forms of political participation. In all three cases, both genes and environment contributed significantly to variation in political behavior.\n\nAdditional studies showed that genes did not play a direct role in the choice of a political party, supporting a core finding in the study of American politics that the choice to be a Democrat or a Republican is largely shaped by parental socialization. However, other studies showed that the decision to affiliate with any political party and the strength of this attachment are significantly influenced by genes.\n\nScholars therefore recently turned their attention to specific genes that might be associated with political behaviors and attitudes. In the first-ever research to link specific genes to political phenotypes, a direct association was established between voter turnout and monoamine oxidase A (MAO-A) and a gene–environment interaction between turnout and the serotonin transporter (5HTT) gene among those who frequently participated in religious activities. In other research scholars have also found an association between voter turnout and a dopamine receptor (DRD2) gene that is mediated by a significant association between that gene and the tendency to affiliate with a political party. More recent studies show an interaction between friendships and the dopamine receptor (DRD4) gene that is associated with political ideology. Although this work is preliminary and needs replication, it suggests that neurotransmitter function has an important effect on political behavior.\n\nThe candidate genes approach to genopolitics received substantial criticism in a 2012 article, published in the \"American Political Science Review\", which argued that many of the candidate genes identified in the above research are associated with innumerable traits and behaviors. The degree to which these genes are associated with so many outcomes thus undermines the apparent important of evidence linking a gene to any particular outcome.\n\nEmploying a more general approach, researchers used genome-wide linkage analysis to identify chromosomal regions associated with political attitudes assessed using scores on a liberalism-conservativism scale. Their analysis identified several significant linkage peaks and the associated chromosomal regions implicate a possible role for NMDA and glutamate related receptors in forming political attitudes. However, this role is speculative as linkage analysis cannot identify the effect of individual genes.\n\nAssociations between genetic markers and political behavior are often assumed to predict a causal connection between the two. Scholars have little incentive to be skeptical of this presumed causal link. Yet it is possible that a confounding factor exists which makes the genetic relationship with politics purely correlative. For instance work on Irish parties, which shows some evidence of a genetic basis for the otherwise inexplicable distinction between the historically two main parties there, is also and more easily explained by socialization.\n\n\n", "id": "20722644", "title": "Genopolitics"}
{"url": "https://en.wikipedia.org/wiki?curid=23577988", "text": "Chi site\n\nA Chi site or Chi sequence is a short stretch of DNA in the genome of a bacterium near which homologous recombination is more likely than expected to occur. Chi sites serve as stimulators of DNA double-strand break repair in bacteria, which can arise from radiation or chemical treatments, or result from replication fork breakage during DNA replication. The sequence of the Chi site is unique to each group of closely related organisms; in \"E. coli\" and other enteric bacteria, such as Salmonella, the sequence is 5'-GCTGGTGG-3'. The existence of Chi sites was originally discovered in the genome of bacteriophage lambda, a virus that infects \"E. coli\", but is now known to occur about 1000 times in the E. coli genome.\n\nThe Chi sequence serves as a signal to the RecBCD helicase-exonuclease that triggers a major change in the activities of this enzyme. Upon encountering the Chi sequence as it unwinds DNA, RecBCD cuts the DNA a few nucleotides to the 3’ side of Chi; depending on the reaction conditions, this cut is either a simple nick on one strand or the change of nuclease activity from cutting the 3’-ended strand to cutting the 5’ended strand. In either case the resulting 3’ single-stranded DNA (ssDNA) is bound by multiple molecules of RecA protein that facilitate \"strand invasion,\" in which one strand of a homologous double-stranded DNA is displaced by the RecA-associated ssDNA. Strand invasion forms a joint DNA molecule called a D-loop. Resolution of the D-loop is thought to occur by replication primed by the 3’ end generated at Chi (in the D-loop). Alternatively, the D-loop may be converted into a Holliday junction by cutting of the D-loop and a second exchange of DNA strands; the Holliday junction can be converted into linear duplex DNA by cutting of the Holliday junction and ligation of the resultant nicks. Either type of resolution can generate recombinant DNA molecules if the two interacting DNAs are genetically different, as well as repair the initially broken DNA.\n\nChi sites are sometimes referred to as \"recombination hot spots\". The name \"Chi\" is an abbreviation of \"c\"rossover \"h\"otspot \"i\"nstigator. The term is sometimes written as \"χ site\", using the Greek letter chi.\n\n\n", "id": "23577988", "title": "Chi site"}
{"url": "https://en.wikipedia.org/wiki?curid=18690851", "text": "Genetic correlation\n\nIn multivariate behavioral & quantitative genetics, a genetic correlation (denoted formula_1 or formula_2) is the proportion of variance that two traits share due to genetic causes, the correlation between the genetic influences on a trait and the genetic influences on a different trait estimating the degree of pleiotropy or causal overlap. A genetic correlation of 0 implies that the genetic effects on one trait are independent of the other, while a correlation of 1 implies that all of the genetic influences on the two traits are identical. The bivariate genetic correlation can be generalized to inferring genetic latent variable factors across >2 traits using factor analysis. Genetic correlation models were introduced into behavioral genetics in the 1970s-1980s.\n\nGenetic correlations have applications in validation of GWAS results, breeding, prediction of traits, and discovering the etiology of traits & diseases.\n\nThey can be estimated using twin studies and molecular genetics. Genetic correlations have been found to be common in non-human genetics and to be broadly similar to their respective phenotypic correlations, and also found extensively in human traits.\n\nThis finding of widespread pleiotropy has implications for artificial selection in agriculture, interpretation of phenotypic correlations, social inequality, attempts to use Mendelian randomization in causal inference, the understanding of the biological origins of complex traits, and the design of GWASes.\n\nA genetic correlations is to be contrasted with environment correlations between the environments affecting two traits (e.g. if poor nutrition in a household caused both lower IQ and height); a genetic correlation between two traits can contribute to the observed (phenotypic) correlation between two traits, but genetic correlations can also be opposite observed phenotypic correlations if the environment correlation is sufficiently strong in the other direction, perhaps due to tradeoffs or specialization; for example, in the UK Biobank, of 118 continuous human traits, 29% of their intercorrelations have opposite signs between environmental & genetic correlations.\n\nGenetic correlations are not the same as heritability, as it is about the overlap between the two sets of influences and not their absolute magnitude; two traits could be both highly heritable but not be genetically correlated or have small heritabilities and be completely correlated (as long as the heritabilities are non-zero).\n\nFor example, consider two traits - dark skin and black hair. These two traits may individually have a very high heritability (most of the population-level variation in the trait due to genetic differences, or in simpler terms, genetics contributes significantly to these two traits), however, they may still have a very low genetic correlation if, for instance, these two traits were being controlled by different, non-overlapping, non-linked genetic loci.\n\nA genetic correlation between two traits will tend to produce phenotypic correlations - e.g. the genetic correlation between intelligence and SES or education and family SES implies that intelligence/SES will also correlate phenotypically. The phenotypic correlation will be limited by the degree of genetic correlation and also by the heritability of each trait. The expected phenotypic correlation is the \"bivariate heritability\"' and can be calculated as the square roots of the heritabilities multiplied by the genetic correlation. (Using a Plomin example, for two traits with heritabilities of 0.60 & 0.23, formula_3, and phenotypic correlation of \"r\"=0.45 the bivariate heritability would be formula_4, so of the observed phenotypic correlation, 0.28/0.45 = 62% of it is due to genetics.)\n\nGenetic correlations can arise due to:\n\n\nGenetic correlations are scientifically useful because genetic correlations can be analyzed over time within an individual longitudinally (e.g. intelligence is stable over a lifetime, due to the same genetic influences - childhood genetically correlates formula_5 with old age), or across studies or populations or ethnic groups/races, or across diagnoses, allowing discovery of whether different genes influence a trait over a lifetime (typically, they do not), whether different genes influence a trait in different populations due to differing local environments, whether there is disease heterogeneity across times or places or sex (particularly in psychiatric diagnoses there is uncertainty whether 1 country's 'autism' or 'schizophrenia' is the same as another's or whether diagnostic categories have shifted over time/place leading to different levels of ascertainment bias), and to what degree traits like autoimmune or psychiatric disorders or cognitive functioning meaningfully cluster due sharing a biological basis and genetic architecture (for example, reading & mathematics disability genetically correlate, consistent with the Generalist Genes Hypothesis, and these genetic correlations explain the observed phenotypic correlations or 'co-morbidity'; IQ and specific measures of cognitive performance such as verbal, spatial, and memory tasks, reaction time, long-term memory, executive function etc. all show high genetic correlations as do neuroanatomical measurements, and the correlations may increase with age, with implications for the etiology & nature of intelligence). This can be an important constraint on conceptualizations of the two traits: traits which seem different phenotypically but which share a common genetic basis require an explanation for how these genes can influence both traits.\n\nGenetic correlations can be used in GWASes by using polygenic scores or genome-wide hits for one (often more easily measured) trait to increase the prior probability of variants for a second trait; for example, since intelligence and years of education are highly genetically correlated, a GWAS for education will inherently also be a GWAS for intelligence and be able to predict variance in intelligence as well and the strongest SNP candidates can be used to increase the statistical power of a smaller GWAS, a combined analysis on the latent trait done where each measured genetically-correlated trait helps reduce measurement error and boosts the GWAS's power considerably (e.g. Krapohl et al. 2017, using elastic net and multiple polygenic scores, improving intelligence prediction from 3.6% of variance to 4.8%; Hill et al. 2017b uses MTAG to combine 3 \"g\"-loaded traits of education, household income, and a cognitive test score to find 107 hits & doubles predictive power of intelligence) or one could do a GWAS for multiple traits jointly.\n\nGenetic correlations can also quantify the contribution of correlations <1 across datasets which might create a false \"missing heritability\", by estimating the extent to which differing measurement methods, racial influences, or environments create only partially overlapping sets of relevant genetic variants.\n\nGenetic correlations are also useful in applied contexts such as plant/animal breeding by allowing substitution of more easily measured but highly genetically correlated characteristics (particularly in the case of sex-linked or binary traits under the liability-threshold model, where differences in the phenotype can rarely be observed but another highly correlated measure, perhaps an endophenotype, is available in all individuals), compensating for different environments than the breeding was carried out in, making more accurate predictions of breeding value using the multivariate breeder's equation as compared to predictions based on the univariate breeder's equation using only per-trait heritability & assuming independence of traits, and avoiding unexpected consequences by taking into consideration that artificial selection for/against trait \"X\" will also increase/decrease all traits which positively/negatively correlate with \"X\". The limits to selection set by the inter-correlation of traits, and the possibility for genetic correlations to change over long-term breeding programs, lead to Haldane's dilemma limiting the intensity of selection and thus progress.\n\nBreeding experiments on genetically correlated traits can measure the extent to which correlated traits are inherently developmentally linked & response is constrained, and which can be dissociated. Some traits, such as the size of eyespots on the butterfly \"Bicyclus anynana\" can be dissociated in breeding, but other pairs, such as eyespot colors, have resisted efforts.\n\nGenetic correlations require a genetically informative sample. They can be estimated by using breeding experiments on two traits of known heritability and selecting on one trait to measure the change in the other trait (allowing inferring the genetic correlation), family/adoption/twin studies (analyzed using SEMs or DeFries-Fulker extremes analysis), molecular estimation of relatedness such as GCTA, methods employing polygenic scores like LD score regression, BOLT-REML, CPBayes, or HESS, comparison of genome-wide SNP hits in GWASes (as a loose lower bound), and phenotypic correlations of populations with at least some related individuals.\nThe methods are related to Haseman-Elston regression & PCGC regression. Such methods are typically genome-wide, but it is also possible to estimate genetic correlations for specific variants or genome regions.\n\nOne way to consider it is using trait X in twin 1 to predict trait Y in twin 2 for monozygotic and dizygotic twins (i.e. using twin 1's IQ to predict twin 2's brain volume); if this cross-correlation is larger for the more genetically-similar monozygotic twins than for the dizygotic twins, the similarity indicates that the traits are not genetically independent and there is some common genetics influencing both IQ and brain volume. (Statistical power can be boosted by using siblings as well.)\n\nGenetic correlations are affected by methodological concerns; underestimation of heritability, such as due to assortative mating, will lead to overestimates of longitudinal genetic correlation, and moderate levels of misdiagnoses can create pseudo correlations.\nAs they are affected by heritabilities of both traits, genetic correlations have low statistical power, especially in the presence of measurement errors biasing heritability downwards, because \"estimates of genetic correlations are usually subject to rather large sampling errors and therefore seldom very precise\": the standard error of an estimate formula_1 is formula_7. (Larger genetic correlations & heritabilities will be estimated more precisely.) However, inclusion of genetic correlations in an analysis of a pleiotropic trait can boost power for the same reason that multivariate regressions are more powerful than separate univariate regressions.\n\nTwin methods have the advantage of being usable without detailed biological data, with human genetic correlations calculated as far back as the 1970s and animal/plant genetic correlations calculated in the 1930s, and require sample sizes in the hundreds for being well-powered, but they have the disadvantage of making assumptions which have been criticized, and in the case of rare traits like anorexia nervosa it may be difficult to find enough twins with a diagnosis to make meaningful cross-twin comparisons, and can only be estimated with access to the twin data; molecular genetic methods like GCTA or LD score regression have the advantage of not requiring specific degrees of relatedness and so can easily study rare traits using case-control designs, which also reduces the number of assumptions they rely on, but those methods could not be run until recently, require large sample sizes in the thousands or hundreds of thousands (to obtain precise SNP heritability estimates, see the standard error formula), may require individual-level genetic data (in the case of GCTA but not LD score regression)\n\nGiven a genetic covariance matrix, the genetic correlation is computed by standardizing this, i.e., by converting the covariance matrix to a correlation matrix. For example, if two traits, say height and weight have the following additive genetic variance-covariance matrix:\nThen the genetic correlation is .55, as seen is the standardized matrix below:\n\nIn practice, structural equation modeling applications such as Mx or OpenMx (and before that, historically, LISREL) are used to calculate both the genetic covariance matrix and its standardized form. In R, cov2cor() will standardize the matrix.\n\nTypically, published reports will provide genetic variance components that have been standardized as a proportion of total variance (for instance in an ACE twin study model standardised as a proportion of V-total = A+C+E). In this case, the metric for computing the genetic covariance (the variance within the genetic covariance matrix) is lost (because of the standardizing process), so you cannot readily estimate the genetic correlation of two traits from such published models. Multivariate models (such as the Cholesky decomposition) will, however, allow the viewer to see shared genetic effects (as opposed to the genetic correlation) by following path rules. It is important therefore to provide the unstandardised path coefficients in publications.\n\nGenetic correlations, both positive and negative, have been measured for a wide variety of human traits using primarily molecular genetic methods but also, historically, twins.\nBelow are genetic correlations which have been reported in the literature as statistically significant (direction is not indicated but generally diseases positively correlate with other diseases and negatively correlate with traits like intelligence).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "18690851", "title": "Genetic correlation"}
{"url": "https://en.wikipedia.org/wiki?curid=19224530", "text": "Gene redundancy\n\nGene redundancy is the existence of multiple genes in the genome of an organism that perform the same function. This is the case for many sets of paralogous genes. When an individual gene in such a set is disrupted by mutation or targeted knockout, there can be little effect on phenotype as a result of gene redundancy, whereas the effect is large for the knockout of a gene with only one copy.\n\nGene redundancy most often results from Gene duplication. When a gene is duplicated within a genome, the two copies are initially functionally redundant. Three of the more common mechanisms of gene duplication are retroposition, unequal crossing over, and non-homologous segmental duplication. Retroposition is when the mRNA transcript of a gene is reverse transcribed back into DNA and inserted into the genome at a different location. During unequal crossing over, homologous chromosomes exchange uneven portions of their DNA. This can lead to the transfer of one chromosome's gene to the other chromosome, leaving two of the same gene on one chromosome, and no copies of the gene on the other chromosome. Non-homologous duplications result from replication errors that shift the gene of interest into a new position. A tandem duplication then occurs, creating a chromosome with two copies of the same gene. Figure 1 to the left provides a good visual explanation of these three mechanisms. As the genome is replicated over many generations, the redundant gene's function will most likely evolve due to Genetic drift. This means that the gene will acquire mutations that change the overall function. The three mechanisms of functional differentiation in genes are nonfunctionalization (or gene loss), neofunctionalization and subfunctionalization.\n\nDuring nonfunctionalization, or degeneration/gene loss, one copy of the duplicated gene acquires mutations that render it inactive or silent. At this time, the gene has no function and is called a pseudogene. Pseudogenes can be lost over time due to genetic mutations. Neofunctionalization occurs when one copy of the gene accumulates mutations that give the gene a new, beneficial function that is different than the original function. Subfunctionalization occurs when both copies of the redundant gene acquire mutations. Each copy becomes only partially active; two of these partial copies then act as one normal copy of the original gene. Figure 2 to the right provides a visual explanation.\n\nThe evolution and origin of redundant genes remain unknown, largely because evolution happens over such a long period of time. Theoretically, a gene can not be maintained without mutation unless it has a selective pressure acting on it. Gene redundancy, therefore, would allow both copies of the gene to accumulate mutations as long as the other was still able to perform its function. This means that all redundant genes should theoretically become a pseudogene and eventually be lost. Scientists have devised two hypothesis as to why redundant genes can remain in the genome: the backup hypothesis and the piggyback hypothesis.\n\nThe backup hypothesis proposes that redundant genes remain in the genome as a sort of \"back-up plan\". If the original gene loses its function, the redundant gene is there to take over and keep the cell alive. The piggyback hypothesis states that two paralogs in the genome have some kind of non-overlapping function as well as the redundant function. In this case, the redundant part of the gene remains in the genome due to the proximity to the area that codes for the unique function. The reason redundant genes remain in the genome is an ongoing question and gene redundancy is being studied by researchers everywhere. There are many hypotheses in addition to the backup and piggyback models. For example, at the University of Michigan, a study provides the theory that redundant genes are maintained in the genome by reduced expression.\n\nResearchers often use the history of redundant genes in the form of gene families to learn about the phylogeny of a species. It takes time for redundant genes to undergo functional diversification; the degree of diversification between orthologs tells us how closely related the two genomes are. Gene duplication events can also be detected by looking at increases in gene duplicates.\n\nA good example of using gene redundancy in evolutionary studies is the Evolution of the KCS gene family in plants. This paper studies how one KCS gene evolved into an entire gene family via duplication events. The number of redundant genes in the species allows researchers to determine when duplication events took place and how closely related species are.\n\nCurrently, there are three ways to detect paralogs in a known genomic sequence: simple homology (FASTA), gene family evolution (TreeFam) and orthology (eggNOG v3). Since the Human Genome Project's completion, researchers are able to annotate the human genome much more easily. Using online databases like the Genome Browser at UCSC, researchers can look for homology in the sequence of their gene of interest.\n\nThe Human Olfactory Receptor (OR) gene family contains 339 intact genes and 297 pseudogenes. These genes are found in different locations throughout the genome, but only about 13% are on different chromosomes or on distantly spaced loci. 172 subfamilies of OR genes have been found in humans, each at its own loci. Because the genes in each of these subfamilies are structurally and functionally similar, and in close proximity to each other, it is hypothesized that each evolved from single genes undergoing duplication events. The high number of subfamilies in humans explains why we are able to recognize so many odors.\n\nHuman OR genes have homologues in other mammals, such as mice, that demonstrate the evolution of Olfactory Receptor genes. One particular family that is involved in the initial event of odor perception has been found to be highly conserved throughout all of vertebrate evolution.\n\nDuplication events and redundant genes have often been thought to have a role in some human diseases. Large scale whole genome duplication events that occurred early in vertebrate evolution may be the reason that human monogenic disease genes often contain a high number of redundant genes. Chen et al. hypothesizes that the functionally redundant paralogs in human monogenic disease genes mask the effects of dominant deletorious mutations, thereby maintaining the disease gene in the human genome.\n\nWhole genome duplications may be a leading cause of retention of some tumor causing genes in the human genome. For example, Strout et al. have shown that tandem duplication events, likely via homologous recombination, are linked to acute myeloid leukemia. The partial duplication of the \"ALL1\" (\"MLL\") gene is a genetic defect has been found in patients with acute myeloid leukemia.\n\n", "id": "19224530", "title": "Gene redundancy"}
{"url": "https://en.wikipedia.org/wiki?curid=23875217", "text": "Public Population Project in Genomics\n\nPG (Public Population Project in Genomics and Society) is a not-for-profit international consortium dedicated to facilitating collaboration between researchers and biobanks working in the area of human population genomics.\nPG is member-based and composed of experts from the different disciplines in the areas of and related to genomics, including epidemiology, law, ethics, technology, biomolecular science, etc.\nPG and its members are committed to a philosophy of information sharing with the goal of supporting researchers working in areas that will improve the health of people around the world.\n\nPG is a not-for-profit organization with members from over 40 countries. Membership falls under two different categories: Institutional and Individual. Institutional members have the right to elect and vote the board of directors of P³G, and all members are eligible for office.\n\nPG is headquartered in the McGill University Genome Quebec Innovation Centre, Montreal (Canada).\n\nPG works with its members and other experts to develop tools, methods and resources designed to optimize and harmonize the infrastructures of biobanks and research projects in the areas of population genomics, epidemiology and the environment. The PG site is completely free and accessible, and all documents, web sites and tools included on the site are non-commercial and open source. Such tools include:\n\nToolkit - provides the epidemiological, ethical, statistical and IT instruments necessary to the access and use of data. The aim of this platform is to create a one-stop location and open access environment, where key documents are accessible to the research community. The TOOLKIT currently contains over 80 tools across five categories (Epidemiology and Biostatistics, Sample collection and processing, Data collection and processing, GE3LS, other tools).\n\nLifespan - an open access web platform offering users a step-by-step approach for the development and maintenance of a biobank or a research project within a definitive funding timeline.\n\nHUB - provides an online agora for all those interested in genetic and genomic research, allowing for ongoing discussion, exchange and collaboration on research projects.\n\nTraining - provides researchers in the biobanking field with pertinent and up-to-date training opportunities\n\nCatalogs - currently provide information on large population-based biobanks. Over 160 studies, spanning all continents, are presently listed in the database, and this index is regularly updated and curated to include relevant studies.\n\nIn addition, PG bring its members together at meetings in different cities around the world. These gatherings provide members with a unique opportunity to identify and address the challenges the community faces, share innovative strategies and work on new research tools.\nPG Meetings have been held in the following cities:\n\nThe IPAC (International Policy interoperability and data\nAccess Clearinghouse) resource both promotes the interoperability of international norms and facilitates the sharing of clinical and research data. Building on the expertise of PG, the IPAC offers a “one-stop” service for national and international collaborative research projects, along with normative tools and frameworks that respect the laws and regulations of each country while facilitating access. http://www.p3g.org/ipac\n\nIPAC Researchers\n\nFounded on the experience of the PG Research Platforms, the IPAC team includes the international expertise of ELSI (ethical/legal/social) research professionals.\n\nDomains: Regenerative Medicine; Gene Therapy; Cancer; Paediatrics; Incompetent Adults; Deceased Persons; Rare Diseases; Bioethics/Law.\n\nIssues: Biobanking; Consent; Access (Data/Samples); Commercialization; Confidentiality/Privacy; Research Ethics; Governance.\n\nProficiency in: English; French; Spanish; Arabic; Mandarin.\n\nIPAC Services:\n\nI. Data/Sample Collection - ELSI Interoperability\n\nIPAC offers the following interoperability services:\n\nII. DACO - Review of Data and Samples Access Request/Authorization and Compliance\n\nIII. PG DataTrust\n\nDue to concerns surrounding the return of clinically significant results to research participants in the context of translational projects, PG has developed the DataTrust (DT) service. This service supports the process of re-contacting participants and returning individual-level results in translational research projects, when appropriate. The DataTrust (DT) proposes to provide PG as an honest and independent, third party broker to act as the key holder of personal information and associated unique ID (codes). PG-DT safeguards the independence of the research project team, while enhancing ethical compliance with privacy and confidentiality standards in the research setting.\n\nPG was incorporated in 2004 under the leadership of Bartha Knoppers and Dr. Thomas Hudson, and was established by and for researchers working in the area of population genetics. PG is funded by Genome Quebec and Genome Canada, as well as other funding agencies.\n\n", "id": "23875217", "title": "Public Population Project in Genomics"}
{"url": "https://en.wikipedia.org/wiki?curid=11899236", "text": "Diallel cross\n\nA diallel cross is a mating scheme used by plant and animal breeders, as well as geneticists, to investigate the genetic underpinnings of quantitative traits.\n\nIn a full diallel, all parents are crossed to make hybrids in all possible combinations. Variations include half diallels with and without parents, omitting reciprocal crosses. Full diallels require twice as many crosses and entries in experiments, but allow for testing for maternal and paternal effects. If such \"reciprocal\" effects are assumed to be negligible, then a half diallel without reciprocals can be effective. \n\nCommon analysis methods utilize general linear models to identify heterotic groups, estimate general or specific combining ability, interactions with testing environments and years, or estimates of additive, dominant, and epistatic genetic effects and genetic correlations. \n\nThere are four main types of diallel mating design:\n", "id": "11899236", "title": "Diallel cross"}
{"url": "https://en.wikipedia.org/wiki?curid=4995479", "text": "Genomic library\n\nA genomic library is a collection of the total genomic DNA from a single organism. The DNA is stored in a population of identical vectors, each containing a different insert of DNA. In order to construct a genomic library, the organism's DNA is extracted from cells and then digested with a restriction enzyme to cut the DNA into fragments of a specific size. The fragments are then inserted into the vector using DNA ligase. Next, the vector DNA can be taken up by a host organism - commonly a population of Escherichia coli or yeast - with each cell containing only one vector molecule. Using a host cell to carry the vector allows for easy amplification and retrieval of specific clones from the library for analysis.\n\nThere are several kinds of vectors available with various insert capacities. Generally, libraries made from organisms with larger genomes require vectors featuring larger inserts, thereby fewer vector molecules are needed to make the library. Researchers can choose a vector also considering the ideal insert size to find a desired number of clones necessary for full genome coverage.\n\nGenomic libraries are commonly used for sequencing applications. They have played an important role in the whole genome sequencing of several organisms, including the human genome and several model organisms.\n\nThe first DNA-based genome ever fully sequenced was achieved by two-time Nobel Prize winner, Frederick Sanger, in 1977. Sanger and his team of scientists created a library of the bacteriophage, phi X 174, for use in DNA sequencing. The importance of this success contributed to the ever-increasing demand for sequencing genomes to research gene therapy. Teams are now able to catalog polymorphisms in genomes and investigate those candidate genes contributing to maladies such as Parkinson's disease, Alzheimer's disease, multiple sclerosis, rheumatoid arthritis, and Type 1 diabetes. These are due to the advance of genome-wide association studies from the ability to create and sequence genomic libraries. Prior, linkage and candidate-gene studies were some of the only approaches.\n\nConstruction of a genomic library involves creating many recombinant DNA molecules. An organism's genomic DNA is extracted and then digested with a restriction enzyme. For organisms with very small genomes \"(~10 kb)\", the digested fragments can be separated by gel electrophoresis. The separated fragments can then be excised and cloned into the vector separately. However, when a large genome is digested with a restriction enzyme, there are far too many fragments to excise individually. The entire set of fragments must be cloned together with the vector, and separation of clones can occur after. In either case, the fragments are ligated into a vector that has been digested with the same restriction enzyme. The vector containing the inserted fragments of genomic DNA can then be introduced into a host organism.\n\nBelow are the steps for creating a genomic library from a large genome.\n\n\nBelow is a diagram of the above outlined steps.\n\nAfter a genomic library is constructed with a viral vector, such as lambda phage, the titer of the library can be determined. Calculating the titer allows researchers to approximate how many infectious viral particles were successfully created in the library. To do this, dilutions of the library are used to transfect cultures of E. coli of known concentrations. The cultures are then plated on agar plates and incubated overnight. The number of viral plaques are counted and can be used to calculate the total number of infectious viral particles in the library. Most viral vectors also carry a marker that allows clones containing an insert to be distinguished from those that do not have an insert. This allows researchers to also determine the percentage of infectious viral particles actually carrying a fragment of the library.\n\nA similar method can be used to titer genomic libraries made with non-viral vectors, such as plasmids and BACs. A test ligation of the library can be used to transform E. coli. The transformation is then spread on agar plates and incubated overnight. The titer of the transformation is determined by counting the number of colonies present on the plates. These vectors generally have a selectable marker allowing the differentiation of clones containing an insert from those that do not. By doing this test, researchers can also determine the efficiency of the ligation and make adjustments as needed to ensure they get the desired number of clones for the library.\n\nIn order to isolate clones that contain regions of interest from a library, the library must first be screened. One method of screening is hybridization. Each transformed host cell of a library will contain only one vector with one insert of DNA. The whole library can be plated onto a filter over media. The filter and colonies are prepared for hybridization and then labeled with a probe. The target DNA- insert of interest- can be identified by detection such as autoradiography because of the hybridization with the probe as seen below.\n\nAnother method of screening is with polymerase chain reaction (PCR). Some libraries are stored as pools of clones and screening by PCR is an efficient way to identify pools containing specific clones.\n\nGenome size varies among different organisms and the cloning vector must be selected accordingly. For a large genome, a vector with a large capacity should be chosen so that a relatively small number of clones are sufficient for coverage of the entire genome. However, it is often more difficult to characterize an insert contained in a higher capacity vector.\n\nBelow is a table of several kinds of vectors commonly used for genomic libraries and the insert size that each generally holds.\n\nA plasmid is a double stranded circular DNA molecule commonly used for molecular cloning. Plasmids are generally 2 to 4 kilobase-pairs (kb) in length and are capable of carrying inserts up to 15kb. Plasmids contain an origin of replication allowing them to replicate inside a bacterium independently of the host chromosome. Plasmids commonly carry a gene for antibiotic resistance that allows for the selection of bacterial cells containing the plasmid. Many plasmids also carry a reporter gene that allows researchers to distinguish clones containing an insert from those that do not.\n\nPhage λ is a double-stranded DNA virus that infects \"E. coli\". The λ chromosome is 48.5kb long and can carry inserts up to 25kb. These inserts replace non-essential viral sequences in the λ chromosome, while the genes required for formation of viral particles and infection remain intact. The insert DNA is replicated with the viral DNA; thus, together they are packaged into viral particles. These particles are very efficient at infection and multiplication leading to a higher production of the recombinant λ chromosomes. However, due to the smaller insert size, libraries made with λ phage may require many clones for full genome coverage.\n\nCosmid vectors are plasmids that contain a small region of bacteriophage λ DNA called the cos sequence. This sequence allows the cosmid to be packaged into bacteriophage λ particles. These particles- containing a linearized cosmid- are introduced into the host cell by transduction. Once inside the host, the cosmids circularize with the aid of the host's DNA ligase and then function as plasmids. Cosmids are capable of carrying inserts up to 45kb in size.\n\nBacteriophage P1 vectors can hold inserts 70 – 100kb in size. They begin as linear DNA molecules packaged into bacteriophage P1 particles. These particles are injected into an E. coli strain expressing Cre recombinase. The linear P1 vector becomes circularized by recombination between two loxP sites in the vector. P1 vectors generally contain a gene for antibiotic resistance and a positive selection marker to distinguish clones containing an insert from those that do not. P1 vectors also contain a P1 plasmid replicon, which ensures only one copy of the vector is present in a cell. However, there is a second P1 replicon- called the P1 lytic replicon- that is controlled by an inducible promoter. This promoter allows the amplification of more than one copy of the vector per cell prior to DNA extraction.\n\nP1 artificial chromosomes (PACs) have features of both P1 vectors and Bacterial Artificial Chromosomes (BACs). Similar to P1 vectors, they contain a plasmid and a lytic replicon as described above. Unlike P1 vectors, they do not need to be packaged into bacteriophage particles for transduction. Instead they are introduced into E. coli as circular DNA molecules through electroporation just as BACs are. Also similar to BACs, these are relatively harder to prepare due to a single origin of replication.\n\nBacterial artificial chromosomes (BACs) are circular DNA molecules, usually about 7kb in length, that are capable of holding inserts up to 300kb in size. BAC vectors contain a replicon derived from E. coli F factor, which ensures they are maintained at one copy per cell. Once an insert is ligated into a BAC, the BAC is introduced into recombination deficient strains of E. coli by electroporation. Most BAC vectors contain a gene for antibiotic resistance and also a positive selection marker. The figure to the right depicts a BAC vector being cut with a restriction enzyme, followed by the insertion of foreign DNA that is re-annealed by a ligase. Overall, this is a very stable vector, but they may be hard to prepare due to a single origin of replication just like PACs.\n\nYeast artificial chromosomes (YACs) are linear DNA molecules containing the necessary features of an authentic yeast chromosome, including telomeres, a centromere, and an origin of replication. Large inserts of DNA can be ligated into the middle of the YAC so that there is an “arm” of the YAC on either side of the insert. The recombinant YAC is introduced into yeast by transformation; selectable markers present in the YAC allow for the identification of successful transformants. YACs can hold inserts up to 2000kb, but most YAC libraries contain inserts 250-400kb in size. Theoretically there is no upper limit on the size of insert a YAC can hold. It is the quality in the preparation of DNA used for inserts that determines the size limit. The most challenging aspect of using YAC is the fact they are prone to rearrangement.\n\nVector selection requires one to ensure the library made is representative of the entire genome. Any insert of the genome derived from a restriction enzyme should have an equal chance of being in the library compared to any other insert. Furthermore, recombinant molecules should contain large enough inserts ensuring the library size is able to be handled conveniently. This is particularly determined by the number of clones needed to have in a library. The number of clones to get a sampling of all the genes is determined by the size of the organism's genome as well as the average insert size. This is represented by the formula (also known as the Carbon and Clarke formula):\n\nformula_1\n\nwhere,\n\nformula_2 is the necessary number of recombinants\n\nformula_3 is the desired probability that any fragment in the genome will occur at least once in the library created\n\nformula_4 is the fractional proportion of the genome in a single recombinant\n\nformula_4 can be further shown to be:\n\nformula_6\n\nwhere,\n\nformula_7 is the insert size\n\nformula_8 is the genome size\n\nThus, increasing the insert size (by choice of vector) would allow for fewer clones needed to represent a genome. The proportion of the insert size versus the genome size represents the proportion of the respective genome in a single clone. Here is the equation with all parts considered:\n\nformula_9\n\nThe above formula can be used to determine the 99% confidence level that all sequences in a genome are represented by using a vector with an insert size of twenty thousand basepairs (such as the phage lambda vector). The genome size of the organism is three billion basepairs in this example.\n\nformula_10\n\nformula_11\n\nformula_12 clones\n\nThus, approximately 688,060 clones are required to ensure a 99% probability that a given DNA sequence from this three billion basepair genome will be present in a library using a vector with an insert size of twenty thousand basepairs.\n\nAfter a library is created, the genome of an organism can be sequenced to elucidate how genes affect an organism or to compare similar organisms at the genome-level. The aforementioned genome-wide association studies can identify candidate genes stemming from many functional traits. Genes can be isolated through genomic libraries and used on human cell lines or animal models to further research. Furthermore, creating high-fidelity clones with accurate genome representation- and no stability issues- would contribute well as intermediates for shotgun sequencing or the study of complete genes in functional analysis.\n\nOne major use of genomic libraries is hierarchichal shotgun sequencing, which is also called top-down, map-based or clone-by-clone sequencing. This strategy was developed in the 1980s for sequencing whole genomes before high throughput techniques for sequencing were available. Individual clones from genomic libraries can be sheared into smaller fragments, usually 500bp to 1000bp, which are more manageable for sequencing. Once a clone from a genomic library is sequenced, the sequence can be used to screen the library for other clones containing inserts which overlap with the sequenced clone. Any new overlapping clones can then be sequenced forming a contig. This technique, called chromosome walking, can be exploited to sequence entire chromosomes.\n\nWhole genome shotgun sequencing is another method of genome sequencing that does not require a library of high-capacity vectors. Rather, it uses computer algorithms to assemble short sequence reads to cover the entire genome. Genomic libraries are often used in combination with whole genome shotgun sequencing for this reason. A high resolution map can be created by sequencing both ends of inserts from several clones in a genomic library. This map provides sequences of known distances apart, which can be used to help with the assembly of sequence reads acquired through shotgun sequencing. The human genome sequence, which was declared complete in 2003, was assembled using both a BAC library and shotgun sequencing.\n\nGenome-wide association studies are general applications to find specific gene targets and polymorphisms within the human race. In fact, the International HapMap project was created through a partnership of scientists and agencies from several countries to catalog and utilize this data. The goal of this project is to compare genetic sequences of different individuals to elucidate similarities and differences within chromosomal regions. Scientists from all of the participating nations are cataloging these attributes with data from populations of African, Asian, and European ancestry. Such genome-wide assessments may lead to further diagnostic and drug therapies while also helping future teams focus on orchestrating therapeutics with genetic features in mind. These concepts are already being exploited in genetic engineering. For example, a research team has actually constructed a PAC shuttle vector that creates a library representing two-fold coverage of the human genome. This could serve as an incredible resource to identify genes, or sets of genes, causing disease. Moreover, these studies can serve as a powerful way to investigate transcriptional regulation as it has been seen in the study of baculoviruses. Overall, advances in genome library construction and DNA sequencing has allowed for efficient discovery of different molecular targets. Assimilation of these features through such efficient methods can hasten the employment of novel drug candidates.\n\n", "id": "4995479", "title": "Genomic library"}
{"url": "https://en.wikipedia.org/wiki?curid=23303326", "text": "Uniparental inheritance\n\nUniparental inheritance is a non-mendelian form of inheritance that consists of the transmission of genotypes from one parental type to all progeny. That is, all the genes in offspring will originate from only the mother or only the father. This phenomenon is most commonly observed in eukaryotic organelles such as mitochondria and chloroplasts. This is because such organelles contain their own DNA and are capable of independent mitotic replication that does not endure crossing over with the DNA from another parental type. Although uniparental inheritance is the most common form of inheritance in organelles, there is increased evidence of diversity. Some studies found doubly uniparental inheritance (DUI) and biparental transmission to exist in cells. Evidence suggests that even when there is biparental inheritance, crossing-over doesn't always occur. Furthermore, there is evidence that the form of organelle inheritance varied frequently over time. Uniparental inheritance can be divided into multiple subtypes based on the pathway of inheritance.\n\nAlthough most of the eukaryotic sub-cellular parts do not have their own DNA nor are capable of replication independent of the nucleus, there are some exceptions such as mitochondria and chloroplasts. Not only are these organelles capable of independent DNA replication, translation, and transcription, they are commonly known to inherit genes from only one parental type. In the case of mitochondria, maternal inheritance is almost the exclusive form of inheritance. Although, during egg cell fertilization, mitochondria are brought into the fertilized cell both by the egg cell and the sperm, the paternal mitochondria are usually marked with ubiquitin and are later destroyed. Even if they are not destroyed, the DNA's of different mitochondria rarely genetically recombine with one another. Thus, mitochondria in most animals are inherited from the maternal type only.\n\nLike all other genetic concepts, the discovery of uniparental inheritance stems from the days of an Augustinian priest known as Gregor Johann Mendel. The soon-to-be \"father of modern genetics\" spent his days conducting hybridization experiments on pea plants(\"Pisum sativum\") in his monastery's garden. During a period of seven years (1856 to 1863), Mendel cultivated and tested some 29,000 pea plants which led to him deducing the two famous generalizations known as Mendel's Laws of Heredity. The first, the law of segregation, states that \"when any individual produces gametes, the copies of a gene separate, so that each gamete receives only one copy\". The second, the law of independent assortment, states that \"alleles of different genes assort independently of one another during gamete formation\". Although his work was published, it lay dormant until it was rediscovered in 1900 by Hugo de Vries and Carl Correns but it was not until 1909 that non-mendelian inheritance was even suggested. Carl Erich Correns and Erwin Baur, in separately conducted researches on \"Pelargonium\" and \"Mirabilis\" plants, observed a green-white variation (later found as the result of mutations in the chloroplast genome) that did not follow the Mendelian laws of inheritance. Nearly twenty years later, non-mendelian inheritance of a mitochondrial mutation was also observed and, in the sixties, it was proven that chloroplasts and mitochondria have their own DNA and that they are capable translation, transcription, and replication independent of the nucleus. Soon after, the discoveries of uniparental and doubly uniparental inheritance came.\n\n", "id": "23303326", "title": "Uniparental inheritance"}
{"url": "https://en.wikipedia.org/wiki?curid=23762519", "text": "Drifty gene hypothesis\n\nThe \"drifty gene hypothesis\" was proposed by the British biologist John Speakman as an alternative to the thrifty gene hypothesis originally proposed by James V Neel in 1962.\n\nSpeakman's critique of the thrifty gene hypothesis is based on an analysis of the pattern and level of mortality during famines. Despite much anecdotal evidence used to suggest that famines cause substantial mortality, Speakman suggests that where real data are available famines actually involve rather low levels of mortality and there is no evidence that fat people survive famines better than lean people. In fact mortality actually falls mostly on groups such as the very young and very old where differential mortality in relation to body composition is highly unlikely.\n\nMoreover, there is some confusion among proponents of the thrifty gene hypothesis about how long famines have played a role in evolution. On the one hand some proponents suggest famines have been an \"ever present\" threat dating back to the dawn of Australopithecines about 4–6 million years ago, while others indicate that famines may actually only have been important since the invention of agriculture, because crop failures would exert devastating effects on our ancestors. Speakman argues that either scenario poses difficulties. If we have been under intense selection for the past 6 million years then simple calculations of the spread of favorable alelles under positive selection, known since the 1920s, indicate that we should all be obese and diabetic, which we clearly are not. However, if selection has only been acting for the past 15,000 years then there has been insufficient time for thrifty genes to spread at all.\n\nIt is argued instead that the modern distribution of the obese phenotype likely comes about because of genetic drift in the genes encoding the regulation system controlling an upper limit on our body fatness. Hence the name \"drifty\" genes, to contrast the positively selected \"thrifty genes\". Such drift may have started because around 2 million years ago when ancestral humans effectively removed the risk of predation, which was probably a key factor maintaining the upper boundary of the regulation system.\n\nThe drifty gene hypothesis was presented as part of a presidential debate at the 2007 Obesity Society meeting in New Orleans, with the counter-arguments favoring the thrifty gene hypothesis presented by British nutritionist Andrew Prentice. The main thrust of Prentice's argument against the drifty gene idea is that Speakman's critique of the thrifty gene hypothesis ignores the huge impact that famines have on fertility. It is argued by Prentice that while famine may actually have only been a force driving evolution of thrifty genes for the past 15,000 years or so, because famines exert effects on both survival and fertility the selection pressure may have been sufficient even over such a short timescale to generate the current phenotype distribution of BMI. This critique was published back to back with the original 'drifty gene' paper in the International Journal of Obesity in November 2008.\n\nPrentice et al predicted that the emerging molecular genetics field would ultimately provide a way to test between the adaptive 'thrifty gene' idea and the non-adaptive 'drifty gene' idea because it would be possible to find signatures of positive selection in the human genome, at genes that are linked to both obesity and type 2 diabetes, if the 'thrifty gene' hypothesis is correct. Two comprehensive studies have been performed seeking such signatures of selection. Ayub et al (2015) searched for signatures of positive selection at 65 genes linked to type 2 diabetes, and Wang and Speakman (2016) searched for signatures of selection at 115 genes linked to obesity. In both cases there was no evidence for such selection signatures at a higher rate than in random genes selected for matched GC content and recombination rate. These two papers provide strong evidence against the thrifty gene idea, and indeed against any adapative explanation which relies on selection during our recent evolutionary history, but instead provide strong support the 'drifty gene' interpretation.\n\n", "id": "23762519", "title": "Drifty gene hypothesis"}
{"url": "https://en.wikipedia.org/wiki?curid=20575238", "text": "Synthetic rescue\n\nSynthetic rescue (or synthetic recovery or synthetic viability when a lethal phenotype is rescued) refers to a genetic interaction in which a cell that is nonviable or sensitive to a specific drug due to the presence of a genetic mutation becomes viable when the original mutation is combined with a second mutation in a different gene. The second mutation can either be a loss-of-function mutation (equivalent to a knockout) or a gain-of-function mutation.\n\nSynthetic rescue could potentially be exploited for gene therapy, but it also provides information on the function of the genes involved in the interaction\n\nDosage-mediated suppression occurs when the suppression of the mutant phenotype is mediated by the over expression of a second suppressor gene. This can occur when the initial mutations destabilise a protein-protein interaction and over expression of the interacting protein bypass the negative effect of the initial mutation.\n\nInteraction-mediated suppression occurs when a deleterious mutation in a component of a protein complex destabilise the complex. A compensatory mutation in another component of the protein complex can then suppress the deleterious phenotype by re-establishing the interaction between the two proteins. It usually means that the deleterious mutation and the suppressive mutation occurs in two residues that are closely located in the tridimensional structure of the multi-protein complex. As thus this kind of suppression provides indirect information on the molecular structure of the proteins involved.\n\nGenetic suppression can be mediated by tRNA genes when a mutation alters their anticodon sequence. For example, a tRNA designated for the recognition of the codon TCA and the corresponding insertion of serine in the growing polypeptide chain can mutate so that it recognise a TAA stop codon and promote the insertion of serine instead of the termination of the polypeptide chain. This could be particularly useful when a nonsense mutation (TCA >TAA) prevents the expression of a gene by either leading to a partially completed polypeptide or degradation of the mRNA by nonsense-mediated decay. The redundancy of tRNA genes makes sure that such mutation would not prevent the normal insertion of serines when the TCA codon specifies them.\n\n", "id": "20575238", "title": "Synthetic rescue"}
{"url": "https://en.wikipedia.org/wiki?curid=3248511", "text": "Copy-number variation\n\nCopy number variation (CNV) is a phenomenon in which sections of the genome are repeated and the number of repeats in the genome varies between individuals in the human population. Copy number variation is a type of structural variation: specifically, it is a type of duplication or deletion event that affects a considerable number of base pairs. However, note that although modern genomics research is mostly focused on human genomes, copy number variations also occur in a variety of other organisms including \"E. coli\". Recent research indicates that approximately two thirds of the entire human genome is composed of repeats and 4.8-9.5% of the human genome can be classified as copy number variations. In mammals, copy number variations play an important role in generating necessary variation in the population as well as disease phenotype.\n\nCopy number variations can be generally categorized into two main groups: short repeats and long repeats. However, there are no clear boundaries between the two groups and the classification depends on the nature of the loci of interest. Short repeats include mainly bi-nucleotide repeats (two repeating nucleotides e.g. A-C-A-C-A-C...) and tri-nucleotide repeats. Long repeats include repeats of entire genes. This classification based on size of the repeat is the most obvious type of classification as size is an important factor in examining the types of mechanisms that most likely gave rise to the repeats, hence the likely effects of these repeats on phenotype.\n\nOne of the most well known examples of a short copy number variation is the tri-nucleotide repeat of the CAG base pairs in the Huntingtin gene, the gene that is responsible for the neurological disorder Huntington's disease. For this particular case, once the CAG tri-nucleotide repeats more than 36 times, Huntington’s disease will likely develop in the individual and it will likely be inherited by his or her offspring. Interestingly, the number of repeats of the CAG tri-nucleotide is correlated with the age of onset of Huntington’s disease. These types of short repeats are often thought to be due to errors in polymerase activity during replication including polymerase slippage, template switching, and fork switching which will be discussed in detail later. The short repeat size of these copy number variations lends itself to errors in the polymerase as these repeated regions are prone to misrecognition by the polymerase and replicated regions may be replicated again, leading to extra copies of the repeat. In addition, if these tri-nucleotide repeats are in the same reading frame in the coding portion of a gene, it may lead to a long chain of the same amino acid, possibly creating protein aggregates in the cell, and if these short repeats fall into the non-coding portion of the gene, it may affect gene expression and regulation. On the other hand, a variable number of repeats of entire genes is less commonly identified in the genome. One example of a whole gene repeat is the alpha-amylase 1 gene (AMY1) that encodes alpha-amylase which has a significant copy number variation between different populations with different diets. Although the specific mechanism that allows the AMY1 gene to increase or decrease its copy number is still a topic of debate, some hypotheses suggest that the non-homologous end joining or the microhomology-mediated end joining is likely responsible for these whole gene repeats. Repeats of entire genes has immediate effects on expression of that particular gene, and the fact that the copy number variation of the AMY1 gene has been related to diet is a remarkable example of recent human evolutionary adaptation. Although these are the general groups that copy number variations are grouped into, the exact number of base pairs copy number variations affect depends on the specific loci of interest. Currently, using data from all reported copy number variations, the mean size of copy number variant is around 118kb, and the median is around 18kb.\n\nIn terms of the structural architecture of copy number variations, research has suggested and defined hotspot regions in the genome where copy number variations are four times more enriched. These hotspot regions were defined to be regions containing long repeats that are 90-100% similar known as segmental duplications either tandem or interspersed and most importantly, these hotspot regions have an increased rate of chromosomal rearrangement. It was thought that these large-scale chromosomal rearrangements give rise to normal variation and genetic diseases, including copy number variations. Moreover, these copy number variation hotspots are consistent throughout many populations from different continents, implying that these hotspots were either independently acquired by all the populations and passed on through generations, or they were acquired in early human evolution before the populations split, the latter seems more likely. Lastly, spatial biases of the location at which copy number variations are most densely distributed does not seem to occur in the genome. Although it was originally detected by fluorescent in situ hybridization and microsatellite analysis that copy number repeats are localized to regions that are highly repetitive such as telomeres, centromeres, and heterochromatin, recent genome-wide studies have concluded otherwise. Namely, the subtelomeric regions and pericentromeric regions are where most chromosomal rearrangement hotspots are found, and there is no considerable increase in copy number variations in that region. Furthermore, these regions of chromosomal rearrangement hotspots do not have decreased gene numbers, again, implying that there is minimal spatial bias of the genomic location of copy number variations.\n\nCopy number variation was initially thought to occupy an extremely small and negligible portion of the genome through cytogenetic observations. Copy number variations were generally associated only with small tandem repeats or specific genetic disorders, therefore, copy number variations were initially only examined in terms of a specific loci. However, breakthroughs in the past decade or so led to an increasing number of highly accurate ways of identifying and studying copy number variations, one of which is the genome-wide association study that allow copy number variations in general to be located and identified in the genome. Copy number variations were originally studied by cytogenetic techniques, which are techniques that allow one to observe the physical structure of the chromosome. One of these techniques is fluorescent in situ hybridization (FISH) which involves inserting fluorescent probes that require a high degree of complementarity in the genome for binding. Comparative genomic hybridization was also commonly used to detect copy number variations by fluorophore visualization and then comparing the length of the chromosomes. One major drawback of these early techniques is that the genomic resolution is relatively low and only large repeats such as whole gene repeats can be detected.\n\nRecent advances in biotechnology gave rise to many important techniques that are of extremely high genomic resolution and as a result, an increasing number of copy number variations in the genome have been reported. One of these advances involve using bacterial artificial chromosome (BAC) array with around 1 megabase of intervals throughout the entire gene, BACs can also detect copy number variations in rearrangement hotspots allowing for the detection of 119 novel copy number variations. Throughout the past decade or so, high throughput genomic sequencing has revolutionized the field of human genomics and in silico studies have been performed to detect copy number variations in the genome. Reference sequences have been compared to other sequences of interest using fosmids by strictly controlling the fosmid clones to be 40kb. Sequencing end reads would provide adequate information to align the reference sequence to the sequence of interest, and any misalignments are easily noticeable thus concluded to be copy number variations within that region of the clone. This type of detection technique offers a high genomic resolution and precise location of the repeat in the genome, and it can also detect other types of structural variation such as inversions.\nIn addition, another way of detecting copy number variation that can ensure high genomic resolution is using single nucleotide polymorphisms (SNP). Since the International HapMap project had begun, common SNPs that occur between four different populations from different continents have been sequenced and located. Due to the abundance of the human SNP data, the direction of detecting copy number variation has changed to utilize these SNPs. Relying on the fact that human recombination is relatively rare and that many recombination events occur in specific regions of the genome known as recombination hotspots, linkage disequilibrium can be used to identify copy number variations. Efforts have been made in associating copy number variations with specific haplotype SNPs by analyzing the linkage disequilibrium, using these associations, one is able to recognize copy number variations in the genome using SNPs as markers. One drawback of this method is that because SNPs in the International HapMap are not optimized for detecting copy number variations, therefore the data is biased towards large copy number variations. Next-generation sequencing has also been used recently to detect copy number variations with high genomic resolutions. Using whole-genome shot-gun sequencing data, assays have been developed to accurately detect and identify regions of duplications. On the other hand, it is very challenging to detect CNAs in targeted sequencing because it is extremely unlikely that breakpoints will occur inside the scant number of regions captured by a gene panel. Thus, soft-clipped reads and discordant reads are unlikely to be found in targeted sequencing. On average, there is about 1 SNP per 800 bit/s, so over a long enough region, B-allele frequencies (BAF) can be used to detect copy number changes. However, in targeted sequencing, there are not enough heterozygous variants within a short region to detect deviations from the expected 50% BAF. Lastly, high resolution microarrays that have copy number probes as well as SNP probes are the gold standard for detecting copy number changes down to 50 kbs with whole genome coverage.\nAccurately detecting, identifying, and categorizing copy number variations is extremely important because of the complications they bring to DNA sequencing. Traditionally, DNA sequencing relied heavily on sequencing short reads from a large genome and using any overlapping regions of the reads to combine the short reads to form longer reads. This will eventually be mapped together to give the sequence of the entire genome. However, the issues related to copy number variations arise in linking the overlapping regions together. By definition, copy number variation is a region of the genome duplicated a variable number of times in the population and due to the large variation between the number of times portions of the genome are duplicated, when mapping overlapping sequences, it becomes unclear whether or not a region is an overlap or a duplicated region. With all the challenges faced by sequencing to detect copy number variations, high resolution microarrays are the technology of choice.\n\nThe more fascinating question to ask is perhaps not how copy number variations are detected in the genome, but rather how did they arise in the first place. An important idea to bear in mind while considering these mechanisms is that although many theories have been put forward, most of these theories are speculations and conjecture. More importantly, there is no conclusive evidence that correlates a specific copy number variation to a specific mechanism. There are two main types of molecular mechanism for the formation of copy number variations: homologous based and non-homologous based.\n\nOne of the best-recognized theories that leads to copy number variations as well as deletions and inversions is non-allelic homologous recombinations. During meiotic recombination, homologous chromosomes pair up and form two ended double-stranded breaks leading to Holliday junctions. However, in the aberrant mechanism, during the formation of Holliday junctions, the double-stranded breaks are misaligned and the crossover lands in non-allelic positions on the same chromosome. When the Holliday junction is resolved, the unequal crossing over event allows transfer of genetic material between the two homologous chromosomes, and as a result, a portion of the DNA on both the homologues is repeated. Since the repeated regions are no longer segregating independently, the duplicated region of the chromosome is inherited. Another type of homologous recombination based mechanism that can lead to copy number variation is known as break induced replication. When a double stranded break occurs in the genome unexpectedly the cell activates pathways that mediate the repair of the break. Errors in repairing the break, similar to non-allelic homologous recombination, can lead to an increase in copy number of a particular region of the genome. During the repair of a double stranded break, the broken end can invade its homologous chromosome instead of rejoin the original strand. As in the non-allelic homologous recombination mechanism, an extra copy of a particular region is transferred to another chromosome, leading to a duplication event. Furthermore, cohesin proteins are found to aid in the repair system of double stranded breaks through clamping the two ends in close proximity which prevents interchromosomal invasion of the ends. If for any reason, such as activation of ribosomal RNA, cohesin activity is affected then there may be local increase in double stranded break repair errors.\n\nThe other class of possible mechanisms that are hypothesized to lead to copy number variations is non-homologous based. To distinguish between this and homologous based mechanisms, one must understand the concept of homology. Homologous pairing of chromosomes involved using DNA strands that are highly similar to each other (~97%) and these strands must be longer than a certain length to avoid short but highly similar pairings. Non-homologous pairings, on the other hand, rely on only few base pairs of similarity between two strands, therefore it is possible for genetic materials to be exchanged or duplicated in the process of non-homologous based double stranded repairs.\n\nOne type of non-homologous based mechanism is the non-homologous end joining or micro-homology end joining mechanism. These mechanisms are also involved in repairing double stranded breaks but require no homology or limited micro-homology. When these strands are repaired, oftentimes there are small deletions or insertions added into the repaired strand. It is possible that retrotransposons are inserted into the genome through this repair system. If retrotransposons are inserted into a non-allelic position on the chromosome, meiotic recombination can drive the insertion to be recombined into the same strand as an already existing copy of the same region. Another mechanism is the break-fusion-bridge cycle which involves sister chromatids that have both lost its telomeric region due to double stranded breaks. It is proposed that these sister chromatids will fuse together to form one dicentric chromosome, and then segregate into two different nuclei. Because pulling the dicentric chromosome apart causes a double stranded break, the end regions can fuse to other double stranded breaks and repeat the cycle. The fusion of two sister chromatids can cause inverted duplication and when these events are repeated throughout the cycle, the inverted region will be repeated leading to an increase in copy number. The last mechanism that can lead to copy number variations is polymerase slippage, which is also known as template switching. During normal DNA replication, the polymerase on the lagging strand is required to unclamp and re-clamp the replication region continuously. When small scale repeats in the DNA sequence exist already, the polymerase can be ‘confused’ when it re-clamps to continue replication and instead of clamping to the correct base pairs, it may shift a few base pairs and replicate a portion of the repeated region again. Note that although this has been experimentally observed and is a widely accepted mechanism, the molecular interactions that led to this error remains unknown. In addition, because this type of mechanism requires the polymerase to jump around the DNA strand and it is unlikely that the polymerase can re-clamp at another locus some kilobases apart, therefore this is more applicable to short repeats such as bi-nucleotide or tri-nucleotide repeats.\n\nAmylase is an enzyme in saliva that is responsible for the breakdown of starch into monosaccharides, and one type of amylase is encoded by the alpha-amylase gene (AMY1). The AMY1 locus, as well as the amylase enzyme, is one of the most extensively studied and sequenced gene in the human genome. Its homologs are also found in other primates and therefore it is likely that the primate AMY1 gene is ancestral to the human AMY1 gene and was adapted early in primate evolution. AMY1 is a particularly interesting gene to examine as it is one of the most well studied genes which has wide range of variable numbers of copies throughout different human populations. The AMY1 gene is also one of the few genes that had been studied that displayed convincing evidence which correlates its protein function to its copy number. Copy number is known to alter transcription as well as translation levels of a particular gene, however research has shown that the relationship between protein levels and copy number is variable. In the AMY1 genes of European Americans it is found that the concentration of salivary amylase is closely correlated to the copy number of the AMY1 gene. As a result, it was hypothesized that the copy number of the AMY1 gene is closely correlated with its protein function, which is to digest starch.\n\nThe AMY1 gene copy number has been found to be correlated to different levels of starch in diets of different populations. 8 Populations from different continents were categorized into high starch diets and low starch diets and their AMY1 gene copy number was visualized using high resolution FISH and qPCR. It was found that the high starch diet populations which consists of the Japanese, Hadza, and European American populations had a significantly higher (2 times higher) average AMY1 copy number than the low starch diet populations including Biaka, Mbuti, Datog, Yakut populations. It was hypothesized that the levels of starch in one’s regular diet, the substrate for AMY1, can directly affect the copy number of the AMY1 gene. Since it was concluded that the copy number of AMY1 is directly correlated with salivary amylase, the more starch present in the population’s daily diet, the more evolutionarily favorable it is to have multiple copies of the AMY1 gene. The AMY1 gene was the first gene to provide strong evidence for evolution on a molecular genetics level. Moreover, using comparative genomic hybridization, copy number variations of the entire genomes of the Japanese population was compared to that of the Yakut population. It was found that the copy number variation of the AMY1 gene was significantly different from the copy number variation in other genes or regions of the genome, suggesting that the AMY1 gene was under a strong selective pressure that had little or no influence on the other copy number variations. Finally, the variability of length of 783 microsatellites between the two populations were compared to copy number variability of the AMY1 gene. It was found that the AMY1 gene copy number range was larger than that of over 97% of the microsatellites examined. This implies that natural selection played a considerable role in shaping the average number of AMY1 genes in these two populations. However, as only 6 populations were studied, it is important to consider the possibility that there may be other factors in their diet or culture that influenced the AMY1 copy number other than starch.\n\nAlthough it is unclear when the AMY1 gene copy number began to increase, it is known and confirmed that the AMY1 gene existed in early primates. Chimpanzees, the closest evolutionary relatives to humans, were found to have 2 diploid copies of the AMY1 gene that is identical in length to the human AMY1 gene, which is significantly less than that of humans. On the other hand, bonobos, also a close relative of modern humans, was found to have more than 2 diploid copies of the AMY1 gene. Nonetheless, the bonobo AMY1 genes were sequenced and analyzed, and it was found that the coding sequences of the AMY1 genes were disrupting, which may lead to the production of dysfunctional salivary amylase. It can be inferred from the results that the increase in bonobo AMY1 copy number is likely not correlated to the amount of starch in their diet. It was further hypothesized that the increase in copy number began recently during early hominin evolution as none of the great apes had more than two copies of the AMY1 gene that produced functional protein. In addition, it was speculated that the increase in the AMY1 copy number began around 20,000 years ago when humans shifted from a hunter-gatherer lifestyle to agricultural societies, which was also when humans relied heavily on root vegetables high in starch. This hypothesis, although logical, lacks experimental evidence due to the difficulties in gathering information on the shift of human diets, especially on root vegetables that are high in starch as they cannot be directly observed or tested. Recent breakthroughs in DNA sequencing has allowed researchers to sequence older DNA such as that of Neanderthals to a certain degree of accuracy. Perhaps sequencing Neanderthal DNA can provide a time marker as to when the AMY1 gene copy number increased and offer insight into human diet and gene evolution.\n\nCurrently it is unknown which mechanism gave rise to the initial duplication of the amylase gene, and it can imply that the insertion of the retroviral sequences was due to non-homologous end joining, which caused the duplication of the AMY1 gene. However, there is currently no evidence to support this theory and therefore this hypothesis remains conjecture. The recent origin of the multi-copy AMY1 gene implies that depending on the environment, the AMY1 gene copy number can increase and decrease very rapidly relative to genes that do not interact as directly with the environment. The AMY1 gene is an excellent example of how gene dosage affects the survival of an organism in a given environment. The multiple copies of the AMY1 gene gives those who rely more heavily on high starch diets an evolutionary advantage, therefore the high gene copy number persists in the population.\n\nRecently, there had been discussion connecting copy number variations to gene families. Gene families are defined as a set of related genes that serve similar functions but have minor temporal or spatial differences and these genes likely derived from one ancestral gene. The main reason copy number variations are connected to gene families is that there is a possibility that genes in a family may have derived from one ancestral gene which got duplicated into different copies. Mutations accumulate through time in the genes and with natural selection acting on the genes, some mutations lead to environmental advantages allowing those genes to be inherited and eventually clear gene families are separated out. An example of a gene family that may have been created due to copy number variations is the globin gene family. The globin gene family is an elaborate network of genes consisting of alpha and beta globin genes including genes that are expressed in both embryos and adults as well as pseudogenes. These globin genes in the globin family are all well conserved and only differ by a small portion of the gene, indicating that they were derived from a common ancestral gene, perhaps due to duplication of the initial globin gene.\n\nResearch has shown that copy number variations are significantly more common in genes that encode proteins that directly interact with the environment than proteins that are involved in basic cellular activities. It was suggested that the gene dosage effect accompanying copy number variation may lead to detrimental effects if essential cellular functions are disrupted, therefore proteins involved in cellular pathways are subjected to strong purifying selection. In addition, proteins function together and interact with proteins of other pathways, therefore it is important to view the effects of natural selection on bio-molecular pathways rather than on individual proteins. With that being said, it was found that proteins in the periphery of the pathway are enriched in copy number variations whereas proteins in the center of the pathways are depleted in copy number variations. It was explained that proteins in the periphery of the pathway interact with fewer proteins and so a change in protein dosage affected by a change in copy number may have a smaller effect on the overall outcome of the cellular pathway.\n\nIn the past few years, researchers seem to have shifted their focus from detecting, locating, and sequencing copy number variations to in depth analyses of the role of these copy number variations in the human genome and in nature in general. Evidence is needed to further validate the relationship between copy number variations and gene families as well as the role that natural selection plays in shaping these relationships and changes. Furthermore, researchers are also aiming to elucidating the molecular mechanisms involved in copy number variations as it may reveal essential information regarding structural variations in general. Taking a step back, the area of structural variation in the human genome seems to be a rapidly growing research topic. Not only can these research data provide additional evidence for evolution and natural selection, it can also be used to develop treatments for a wide range of genetic diseases.\n\n\n\n\n", "id": "3248511", "title": "Copy-number variation"}
{"url": "https://en.wikipedia.org/wiki?curid=22710990", "text": "Scripps Genomic Health Initiative\n\nThe Scripps Genomic Health Initiative (SGHI) is a ground-breaking study aimed at understanding how personal genetic testing influences and improves health. \n\nLed by Dr. Eric Topol, director of the San Diego-based Scripps Translational Science Institute, the 20-year initiative will determine whether patients make an effort to improve their lifestyle and get regular checkups after learning their genetic predisposition for many common diseases. Researchers will also assess the psychological impact of genomic testing, and whether those who do it are able to prevent or delay disease by taking action after getting their results. \n\nThe study was launched in October 2008 and will follow more than 10,000 adults. A consortium of health care, technology and research leaders have joined forces in the first-of-its-kind research study, including genetic test provider Navigenics Inc., Affymetrix and Microsoft Corp.\n\nStudy participants receive a scan of their genome and a detailed analysis of their genetic risk for more than 20 health conditions that may be changed by lifestyle, including type 2 diabetes, Alzheimer’s disease, heart attack, obesity, and several types of cancer. \n\nSaid Peter Neupert, corporate VP for the Health Solutions Group at Microsoft:\n\"Personalized medicine stands to change the way people approach their health and wellness, as well as open up new genetic research opportunities.\"\n\n\n", "id": "22710990", "title": "Scripps Genomic Health Initiative"}
{"url": "https://en.wikipedia.org/wiki?curid=4040227", "text": "Genotyping\n\nGenotyping is the process of determining differences in the genetic make-up (genotype) of an individual by examining the individual's DNA sequence using biological assays and comparing it to another individual's sequence or a reference sequence. It reveals the alleles an individual has inherited from their parents. Traditionally genotyping is the use of DNA sequences to define biological populations by use of molecular tools. It does not usually involve defining the genes of an individual.\n\nCurrent methods of genotyping include restriction fragment length polymorphism identification (RFLPI) of genomic DNA, random amplified polymorphic detection (RAPD) of genomic DNA, amplified fragment length polymorphism detection (AFLPD), polymerase chain reaction (PCR), DNA sequencing, allele specific oligonucleotide (ASO) probes, and hybridization to DNA microarrays or beads. Genotyping is important in research of genes and gene variants associated with disease. Due to current technological limitations, almost all genotyping is partial. That is, only a small fraction of an individual’s genotype is determined, such as with (epi)GBS (Genotyping by sequencing) or RADseq. New mass-sequencing technologies promise to provide whole-genome genotyping (or whole genome sequencing) in the future.\n\nGenotyping applies to a broad range of individuals, including microorganisms. For example, viruses and bacteria can be genotyped. Genotyping in this context may help in controlling the spreading of pathogens, by tracing the origin of outbreaks. This area is often referred to as molecular epidemiology or forensic microbiology.\n\nHumans can also be genotyped. For example, when testing fatherhood or motherhood, scientists typically only need to examine 10 or 20 genomic regions (like single-nucleotide polymorphism (SNPs)), which represent a tiny fraction of the human genome.\n\nWhen genotyping transgenic organisms, a single genomic region may be all that needs to be examined to determine the genotype. A single PCR assay is typically enough to genotype a transgenic mouse; the mouse is the mammalian model of choice for much of medical research today.\n\n\n'Genotyping Services' from Source Bioscience\n\n", "id": "4040227", "title": "Genotyping"}
{"url": "https://en.wikipedia.org/wiki?curid=23956168", "text": "Copy number analysis\n\nCopy number analysis usually refers to the process of analyzing data produced by a test for DNA copy number variation in patient's sample. Such analysis helps detect chromosomal copy number variation that may cause or may increase risks of various critical disorders.\n\nBAC (Bacterial Artificial Chromosome) arrays were historically the first microarray platform to be used for DNA copy number analysis. This platform is used to identify gross deletions or amplifications in DNA. Such anomalies for example are common in cancer and can be used for diagnosis of many developmental disorders. Data produced by such platforms are usually low to medium resolution in terms of genome coverage. Usually, log-ratio measurements are produced by this technology to represent deviation of patient's copy number state from normal. Such measurements then are studied and those that significantly differ from zero value are announced to represent a part of a chromosome with an anomaly (an abnormal copy number state). Positive log-ratios indicate a region of DNA copy number gain and negative log-ratio values mark a region of DNA copy number loss. Even a single data point can be declared an indication of a copy number gain or a copy number loss in BAC arrays.\n\n", "id": "23956168", "title": "Copy number analysis"}
{"url": "https://en.wikipedia.org/wiki?curid=1236146", "text": "Reassortment\n\nReassortment is the mixing of the genetic material of a species into new combinations in different individuals. Several different processes contribute to reassortment, including assortment of chromosomes, and chromosomal crossover. It is particularly used when two similar viruses that are infecting the same cell exchange genetic material. In particular, reassortment occurs among influenza viruses, whose genomes consist of eight distinct segments of RNA. These segments act like mini-chromosomes, and each time a flu virus is assembled, it requires one copy of each segment.\n\nIf a single host (a human, a chicken, or other animal) is infected by two different strains of the influenza virus, then it is possible that new assembled viral particles will be created from segments whose origin is mixed, some coming from one strain and some coming from another. The new reassortant strain will share properties of both of its parental lineages.\n\nReassortment is responsible for some of the major genetic shifts in the history of the influenza virus. The 1957 and 1968 pandemic flu strains were caused by reassortment between an avian virus and a human virus, whereas the H1N1 virus responsible for the 2009 swine flu outbreak has an unusual mix of swine, avian and human influenza genetic sequences.\n\n\n\n", "id": "1236146", "title": "Reassortment"}
{"url": "https://en.wikipedia.org/wiki?curid=23257752", "text": "Genosome\n\nA genosome (also known as a lipoplex) is a lipid and DNA complex that is used to deliver genes. It can be a form of non-viral gene therapy as the complex does not require any components of a virus in order to transport genetic material. In presence of CT-DNA, genosomes can form through surface electrostatic interaction.\n", "id": "23257752", "title": "Genosome"}
{"url": "https://en.wikipedia.org/wiki?curid=24426190", "text": "WGAViewer\n\nWGAViewer is a bioinformatics software tool which is designed to visualize, annotate, and help interpret the results generated from a genome wide association study (GWAS). Alongside the P values of association, WGAViewer allows a researcher to visualize and consider other supporting evidence, such as the genomic context of the SNP, linkage disequilibrium (LD) with ungenotyped SNPs, gene expression database, and the evidence from other GWAS projects, when determining the potential importance of an individual SNP.\n\nWGAViewer currently offers several classes of annotation of the GWAS results:\n\n(1) Overview of WGA results allowing\n\n(2) Genic annotation of WGA results with explicit reference to:\n\n(3) Annotation for SNPs :\n\n(4) Gene/SNP finding :\nlocating and annotating specific genes, SNPs, or LD proxies for SNPs, and aligning the results with the latest genome build.\n\n(5) Evidence from multiple scans.\n\n(6) Supporting/QC databases:\ndisplaying supporting information, for example, HWE P values, effect size, effect direction, QC scores, or other user-customized data.\n\nWGAViewer is developed on the Java platform.\n\nWGAViewer is developed and maintained by Dr. Dongliang Ge and Dr. David B. Goldstein at Duke University, Institute for Genome Sciences & Policy, Center for Human Genome Variation.\n\nA number of GWAS projects used the WGAViewer software tool.\n\nOne of these projects leads to the identification of the genetic variant predicting the hepatitis C treatment-induced viral clearance. The finding from that project, originally reported in Nature, showed that genotype 1 hepatitis C patients carrying certain genetic variant alleles near the IL28B gene are more possibly to achieve sustained virological response after the treatment of Pegylated interferon-alpha-2a or Pegylated interferon-alpha-2b (brand names Pegasys or PEG-Intron) combined with ribavirin. A later report from Nature demonstrated that the same genetic variants are also associated with the natural clearance of the genotype 1 hepatitis C virus.\n\n", "id": "24426190", "title": "WGAViewer"}
{"url": "https://en.wikipedia.org/wiki?curid=8460828", "text": "Glossary of genetics\n\nThis is a glossary of terms commonly used in the study of genetics and related disciplines in biology. It is intended as introductory material for novices; for more specific and technical detail, please see the article corresponding to each term. Introductory articles in the field include:\n\n\n\n\n\n\n\n\n\n\nSplicing,split-gene,structure gene,scaffolding,solenoid fiber,Swivel point.\n\n\n\n\nTaking glossary of genetic terms\n", "id": "8460828", "title": "Glossary of genetics"}
{"url": "https://en.wikipedia.org/wiki?curid=24763119", "text": "Chromosome engineering\n\nChromosome engineering is \"the controlled generation of chromosomal deletions, inversions, or translocations with defined endpoints.\" By combining chromosomal translocation, chromosomal inversion, and chromosomal deletion, chromosome engineering has been shown to identify the underlying genes that cause certain diseases in mice. In coming years, it is very likely that chromosomal engineering will be able to do the same identification for diseases in humans, as well as all other organisms.\n\nIn an experiment pertaining to chromosome engineering that was conducted in 2006, it was found that chromosome engineering can be effectively used as a method of identifying the causes of genetic disorders such as the continuous gene and aneuploidy syndromes. The experiment was conducted by infecting mice with the human disease, ES, to see the effectiveness of chromosomal engineering in the gene identification of those diseases. After much experimenting, it was found that manipulating chromosomes, or chromosome engineering, is an excellent and efficient method of determining underlying genes in genetic orders and diseases.\n\nIn the future, chromosome engineering will experiment in removing more common disorders such as asthma, diabetes, and cancer. If it can be recognized by the medical community as effective and safe, it should be able to be used regularly in the near future.\n\n", "id": "24763119", "title": "Chromosome engineering"}
{"url": "https://en.wikipedia.org/wiki?curid=1235542", "text": "X-linked recessive inheritance\n\nX-linked recessive inheritance is a mode of inheritance in which a mutation in a gene on the X chromosome causes the phenotype to be expressed in males (who are necessarily hemizygous for the gene mutation because they have one X and one Y chromosome) and in females who are homozygous for the gene mutation, see zygosity.\n\nX-linked inheritance means that the gene causing the trait or the disorder is located on the X chromosome. Females have two X chromosomes, while males have one X and one Y chromosome. Carrier females who have only one copy of the mutation do not usually express the phenotype, although differences in X chromosome inactivation can lead to varying degrees of clinical expression in carrier females since some cells will express one X allele and some will express the other. The current estimate of sequenced X-linked genes is 499 and the total including vaguely defined traits is 983.\n\nSome scholars have suggested discontinuing the terms dominant and recessive when referring to X-linked inheritance due to the multiple mechanisms that can result in the expression of X-linked traits in females, which include cell autonomous expression, skewed X-inactivation, clonal expansion, and somatic mosaicism.\n\nIn humans, generally men are affected and women are carriers for two reasons. The first is the simple statistical fact that if the X-chromosomes in a population that carry a particular X-linked mutation at a frequency of 'f' (for example, 1%) then that will be the frequency that men are likely to express the mutation (since they have only one X), while women will express it at a frequency of f (for example 1% * 1% = 0.01%) since they have two X's and hence two chances to get the normal allele. Thus, X-linked mutations tend to be rare in women. The second reason for female rarity is that women who \"express\" the mutation must have two X chromosomes that carry the trait and they necessarily got one from their father, who would have also expressed the trait because he only had one X chromosome in the first place. If the trait lowers the probability of fathering a child or induces the father to only have children with women who aren't carriers (so as not to create daughters who are carriers rather than expressers and then only if no genetic screening is used) then women become even \"less\" likely to express the trait.\n\nThe most common X-linked recessive disorders are:\n\n\nTheoretically, a mutation in any of the may cause disease, but below are some notable ones, with short description of symptoms:\n\n\n\n\n[Female X-linked disorders]\n", "id": "1235542", "title": "X-linked recessive inheritance"}
{"url": "https://en.wikipedia.org/wiki?curid=24978923", "text": "Gene theft\n\nIn bioethics and law, gene theft or DNA theft is the act of acquiring the genetic material of another human being without his or her permission, often from a public place. The DNA may be harvested from a wide variety of common objects such as discarded cigarettes, used coffee cups, and hairbrushes. This genetic material can then be used for purposes such as establishing paternity, proving genealogical connections or even unmasking private medical conditions.\n\nGreat Britain criminalized the acquisition of DNA without consent in 2006 at the urging of the Human Genetics Commission. Australia's legislature debated a two-year jail sentence for such theft in 2008.\n\nIn the United States, eight states currently have criminal or civil prohibitions on such nonconsensual appropriation of genetic materials. In Alaska, Florida, New Jersey, New York and Oregon, individuals caught swiping DNA face fines or short jail sentences. Lawsuits against \"gene snatchers\" are permitted in Minnesota, New Hampshire and New Mexico. In jurisdictions where such nonconsentual taking of DNA is illegal, exceptions are generally made for law enforcement.\n\nMany bioethicists believe that such conduct is an unethical invasion of human privacy. Professor Jacob Appel has warned that criminals may acquire the capability to copy DNA of innocent people and deposit it at crimes scenes, endangering the blameless and undermining a key tool of forensic investigation.\" However, others defend the appropriation of genetic material on the grounds that doing so may further human knowledge in productive ways One particularly controversial case which received widespread attention in the media was that of Derrell Teat, a wastewater coordinator, who sought to acquire without consent the DNA of a man who was allegedly the last male descendant of her great-great-great grandfather’s brother. Another prominent case was a United States paternity suit involving film producer Steve Bing and billionaire investor Kirk Kerkorian.\n\n", "id": "24978923", "title": "Gene theft"}
{"url": "https://en.wikipedia.org/wiki?curid=24615332", "text": "RNA-directed DNA methylation\n\nRNA-directed DNA methylation (RdDM) is an epigenetic process first discovered in plants (\"Wassenegger et al\", 1994, Cell, Vol 76, 567-576). During RdDM, double-stranded RNAs (dsRNAs) are processed to 21-24 nucleotide small interfering RNAs (siRNAs) and guide methylation of homologous DNA loci. In plants dsRNAs may be generated from four sources:\nBesides RNA molecules, a plethora of proteins are involved in the establishment of RdDM, like Argonautes, DNA methyltransferases, chromatin remodelling complexes. and the plant-specific Polymerase IV and Polymerase V. All these act in concert to add a methyl-group at the 5' position of cytosines. In contrast to animals, cytosines at all sequence context (CG, CHG, CHH) may get de novo methylated in plants.\n\n", "id": "24615332", "title": "RNA-directed DNA methylation"}
{"url": "https://en.wikipedia.org/wiki?curid=7327278", "text": "Transcription bubble\n\nA transcription bubble is a molecular structure that occurs during the transcription of DNA when a limited portion of the DNA double strand is unwound and is about 11bp of DNA. RNA polymerase may then bind to the exposed DNA and begin synthesizing a new strand of RNA. As RNA polymerase progresses down the DNA template strand in the 3' to 5' direction (the DNA coding strand in the 5' to 3' direction), more of the double stranded DNA is unwound downstream of the polymerase (towards the 5' end of the template strand or 3' end of the coding strand) while DNA upstream of the polymerase (towards the 3' end of the template strand or 5' end of the coding strand) re-anneals. Moving of the transcription bubble in the process may be seen with specialized staining techniques, spectroscopy or microscopy.\n\n\n\n", "id": "7327278", "title": "Transcription bubble"}
{"url": "https://en.wikipedia.org/wiki?curid=2162538", "text": "Genetic predisposition\n\nA genetic predisposition is a genetic characteristic which influences the possible phenotypic development of an individual organism within a species or population under the influence of environmental conditions. In medicine, genetic susceptibility to a disease refers to a genetic predisposition to a health problem, which may eventually be triggered by particular environmental or lifestyle factors, such as tobacco smoking or diet. Genetic testing is able to identify individuals who are genetically predisposed to certain diseases.\n\nPredisposition is the capacity we are born with to learn things such as language and concept of self. Negative environmental influences may block the predisposition (ability) we have to do some things. Behaviors displayed by animals can be influenced by genetic predispositions. Genetic predisposition towards certain human behaviors is scientifically investigated by attempts to identify patterns of human behavior that seem to be invariant over long periods of time and in very different cultures.\n\nFor example, philosopher Daniel Dennett has proposed that humans are genetically predisposed to have a theory of mind because there has been evolutionary selection for the human ability to adopt the intentional stance. The \"intentional stance\" is a useful behavioral strategy by which humans assume that others have minds like their own. This assumption allows you to predict the behavior of others based on personal knowledge of what you would do.\n\nE. O. Wilson's and his book Consilience discuss the idea of genetic predisposition to behaviors\n\nThe field of evolutionary psychology explores the idea that certain behaviors have been selected for during the course of evolution.\n\nThe Genetic Information Nondiscrimination Act, which was signed into law by President Bush on May 21, 2008, prohibits discrimination in employment and health insurance based on genetic information.\n\n\n", "id": "2162538", "title": "Genetic predisposition"}
{"url": "https://en.wikipedia.org/wiki?curid=2631477", "text": "Homologous recombination\n\nHomologous recombination is a type of genetic recombination in which nucleotide sequences are exchanged between two similar or identical molecules of DNA. It is most widely used by cells to accurately repair harmful breaks that occur on both strands of DNA, known as double-strand breaks (DSB). Homologous recombination also produces new combinations of DNA sequences during meiosis, the process by which eukaryotes make gamete cells, like sperm and egg cells in animals. These new combinations of DNA represent genetic variation in offspring, which in turn enables populations to adapt during the course of evolution. Homologous recombination is also used in horizontal gene transfer to exchange genetic material between different strains and species of bacteria and viruses.\n\nAlthough homologous recombination varies widely among different organisms and cell types, most forms involve the same basic steps. After a double-strand break occurs, sections of DNA around the 5' ends of the break are cut away in a process called \"resection\". In the \"strand invasion\" step that follows, an overhanging 3' end of the broken DNA molecule then \"invades\" a similar or identical DNA molecule that is not broken. After strand invasion, the further sequence of events may follow either of two main pathways discussed below (see Models); the DSBR (double-strand break repair) pathway or the SDSA (synthesis-dependent strand annealing) pathway. Homologous recombination that occurs during DNA repair tends to result in non-crossover products, in effect restoring the damaged DNA molecule as it existed before the double-strand break.\n\nHomologous recombination is conserved across all three domains of life as well as viruses, suggesting that it is a nearly universal biological mechanism. The discovery of genes for homologous recombination in protists—a diverse group of eukaryotic microorganisms—has been interpreted as evidence that meiosis emerged early in the evolution of eukaryotes. Since their dysfunction has been strongly associated with increased susceptibility to several types of cancer, the proteins that facilitate homologous recombination are topics of active research. Homologous recombination is also used in gene targeting, a technique for introducing genetic changes into target organisms. For their development of this technique, Mario Capecchi, Martin Evans and Oliver Smithies were awarded the 2007 Nobel Prize for Physiology or Medicine; Capecchi and Smithies independently discovered applications to mouse embryonic stem cells, however the highly conserved mechanisms underlying the DSB repair model, including uniform homologous integration of transformed DNA (gene therapy), were first shown in plasmid experiments by Orr-Weaver, Szostack and Rothstein. Researching the plasmid-induced DSB, using γ-irradiation in the 1970s-1980s, led to later experiments using endonucleases (e.g. I-SceI) to cut chromosomes for genetic engineering of mammalian cells, where nonhomologous recombination is more frequent than in yeast.\n\nIn the early 1900s, William Bateson and Reginald Punnett found an exception to one of the principles of inheritance originally described by Gregor Mendel in the 1860s. In contrast to Mendel's notion that traits are independently assorted when passed from parent to child—for example that a cat's hair color and its tail length are inherited independent of each other—Bateson and Punnett showed that certain genes associated with physical traits can be inherited together, or genetically linked. In 1911, after observing that linked traits could on occasion be inherited separately, Thomas Hunt Morgan suggested that \"crossovers\" can occur between linked genes, where one of the linked genes physically crosses over to a different chromosome. Two decades later, Barbara McClintock and Harriet Creighton demonstrated that chromosomal crossover occurs during meiosis, the process of cell division by which sperm and egg cells are made. Within the same year as McClintock's discovery, Curt Stern showed that crossing over—later called \"recombination\"—could also occur in somatic cells like white blood cells and skin cells that divide through mitosis.\n\nIn 1947, the microbiologist Joshua Lederberg showed that bacteria—which had been assumed to reproduce only asexually through binary fission—are capable of genetic recombination, which is more similar to sexual reproduction. This work established \"E. coli\" as a model organism in genetics, and helped Lederberg win the 1958 Nobel Prize in Physiology or Medicine. Building on studies in fungi, in 1964 Robin Holliday proposed a model for recombination in meiosis which introduced key details of how the process can work, including the exchange of material between chromosomes through Holliday junctions. In 1983, Jack Szostak and colleagues presented a model now known as the DSBR pathway, which accounted for observations not explained by the Holliday model. During the next decade, experiments in \"Drosophila\", budding yeast and mammalian cells led to the emergence of other models of homologous recombination, called SDSA pathways, which do not always rely on Holliday junctions.\n\nHomologous recombination (HR) is essential to cell division in eukaryotes like plants, animals, fungi and protists. In cells that divide through mitosis, homologous recombination repairs double-strand breaks in DNA caused by ionizing radiation or DNA-damaging chemicals. Left unrepaired, these double-strand breaks can cause large-scale rearrangement of chromosomes in somatic cells, which can in turn lead to cancer.\n\nIn addition to repairing DNA, homologous recombination also helps produce genetic diversity when cells divide in meiosis to become specialized gamete cells—sperm or egg cells in animals, pollen or ovules in plants, and spores in fungi. It does so by facilitating chromosomal crossover, in which regions of similar but not identical DNA are exchanged between homologous chromosomes. This creates new, possibly beneficial combinations of genes, which can give offspring an evolutionary advantage. Chromosomal crossover often begins when a protein called Spo11 makes a targeted double-strand break in DNA. These sites are non-randomly located on the chromosomes; usually in intergenic promoter regions and preferentially in GC-rich domains These double-strand break sites often occur at recombination hotspots, regions in chromosomes that are about 1,000–2,000 base pairs in length and have high rates of recombination. The absence of a recombination hotspot between two genes on the same chromosome often means that those genes will be inherited by future generations in equal proportion. This represents linkage between the two genes greater than would be expected from genes that independently assort during meiosis.\n\nDouble-strand breaks can be repaired through homologous recombination or through non-homologous end joining (NHEJ). NHEJ is a DNA repair mechanism which, unlike homologous recombination, does not require a long homologous sequence to guide repair. Whether homologous recombination or NHEJ is used to repair double-strand breaks is largely determined by the phase of cell cycle. Homologous recombination repairs DNA before the cell enters mitosis (M phase). It occurs during and shortly after DNA replication, in the S and G phases of the cell cycle, when sister chromatids are more easily available. Compared to homologous chromosomes, which are similar to another chromosome but often have different alleles, sister chromatids are an ideal template for homologous recombination because they are an identical copy of a given chromosome. In contrast to homologous recombination, NHEJ is predominant in the G phase of the cell cycle, when the cell is growing but not yet ready to divide. It occurs less frequently after the G phase, but maintains at least some activity throughout the cell cycle. The mechanisms that regulate homologous recombination and NHEJ throughout the cell cycle vary widely between species.\n\nCyclin-dependent kinases (CDKs), which modify the activity of other proteins by adding phosphate groups to (that is, phosphorylating) them, are important regulators of homologous recombination in eukaryotes. When DNA replication begins in budding yeast, the cyclin-dependent kinase Cdc28 begins homologous recombination by phosphorylating the Sae2 protein. After being so activated by the addition of a phosphate, Sae2 uses its endonuclease activity to make a clean cut near a double-strand break in DNA. This allows a three-part protein known as the MRX complex to bind to DNA, and begins a series of protein-driven reactions that exchange material between two DNA molecules.\n\nThe packaging of eukaryotic DNA into chromatin presents a barrier to all DNA-based processes that require recruitment of enzymes to their sites of action. To allow HR DNA repair, the chromatin must be remodeled. In eukaryotes, ATP dependent chromatin remodeling complexes and histone-modifying enzymes are two predominant factors employed to accomplish this remodeling process.\n\nChromatin relaxation occurs rapidly at the site of a DNA damage. In one of the earliest steps, the stress-activated protein kinase, c-Jun N-terminal kinase (JNK), phosphorylates SIRT6 on serine 10 in response to double-strand breaks or other DNA damage. This post-translational modification facilitates the mobilization of SIRT6 to DNA damage sites, and is required for efficient recruitment of poly (ADP-ribose) polymerase 1 (PARP1) to DNA break sites and for efficient repair of DSBs. PARP1 protein starts to appear at DNA damage sites in less than a second, with half maximum accumulation within 1.6 seconds after the damage occurs. Next the chromatin remodeler Alc1 quickly attaches to the product of PARP1 action, a poly-ADP ribose chain, and Alc1 completes arrival at the DNA damage within 10 seconds of the occurrence of the damage. About half of the maximum chromatin relaxation, presumably due to action of Alc1, occurs by 10 seconds. This then allows recruitment of the DNA repair enzyme MRE11, to initiate DNA repair, within 13 seconds.\n\nγH2AX, the phosphorylated form of H2AX is also involved in the early steps leading to chromatin decondensation after DNA double-strand breaks. The histone variant H2AX constitutes about 10% of the H2A histones in human chromatin. γH2AX (H2AX phosphorylated on serine 139) can be detected as soon as 20 seconds after irradiation of cells (with DNA double-strand break formation), and half maximum accumulation of γH2AX occurs in one minute. The extent of chromatin with phosphorylated γH2AX is about two million base pairs at the site of a DNA double-strand break. γH2AX does not, itself, cause chromatin decondensation, but within 30 seconds of irradiation, RNF8 protein can be detected in association with γH2AX. RNF8 mediates extensive chromatin decondensation, through its subsequent interaction with CHD4, a component of the nucleosome remodeling and deacetylase complex NuRD.\n\nAfter undergoing relaxation subsequent to DNA damage, followed by DNA repair, chromatin recovers to a compaction state close to its pre-damage level after about 20 min.\n\nTwo primary models for how homologous recombination repairs double-strand breaks in DNA are the double-strand break repair (DSBR) pathway (sometimes called the \"double Holliday junction model\") and the synthesis-dependent strand annealing (SDSA) pathway. The two pathways are similar in their first several steps. After a double-strand break occurs, the MRX complex (MRN complex in humans) binds to DNA on either side of the break. Next a resection, in which DNA around the 5' ends of the break is cut back, is carried out in two distinct steps. In the first step of resection, the MRX complex recruits the Sae2 protein. The two proteins then trim back the 5' ends on either side of the break to create short 3' overhangs of single-strand DNA. In the second step, 5'→3' resection is continued by the Sgs1 helicase and the Exo1 and Dna2 nucleases. As a helicase, Sgs1 \"unzips\" the double-strand DNA, while Exo1 and Dna2's nuclease activity allows them to cut the single-stranded DNA produced by Sgs1.\nThe RPA protein, which has high affinity for single-stranded DNA, then binds the 3' overhangs. With the help of several other proteins that mediate the process, the Rad51 protein (and Dmc1, in meiosis) then forms a filament of nucleic acid and protein on the single strand of DNA coated with RPA. This nucleoprotein filament then begins searching for DNA sequences similar to that of the 3' overhang. After finding such a sequence, the single-stranded nucleoprotein filament moves into (invades) the similar or identical recipient DNA duplex in a process called strand invasion. In cells that divide through mitosis, the recipient DNA duplex is generally a sister chromatid, which is identical to the damaged DNA molecule and provides a template for repair. In meiosis, however, the recipient DNA tends to be from a similar but not necessarily identical homologous chromosome. A displacement loop (D-loop) is formed during strand invasion between the invading 3' overhang strand and the homologous chromosome. After strand invasion, a DNA polymerase extends the end of the invading 3' strand by synthesizing new DNA. This changes the D-loop to a cross-shaped structure known as a Holliday junction. Following this, more DNA synthesis occurs on the invading strand (i.e., one of the original 3' overhangs), effectively restoring the strand on the homologous chromosome that was displaced during strand invasion.\n\nAfter the stages of resection, strand invasion and DNA synthesis, the DSBR and SDSA pathways become distinct. The DSBR pathway is unique in that the second 3' overhang (which was not involved in strand invasion) also forms a Holliday junction with the homologous chromosome. The double Holliday junctions are then converted into recombination products by nicking endonucleases, a type of restriction endonuclease which cuts only one DNA strand. The DSBR pathway commonly results in crossover, though it can sometimes result in non-crossover products; the ability of a broken DNA molecule to collect sequences from separated donor loci was shown in mitotic budding yeast using plasmids or endonuclease induction of chromosomal events. Because of this tendency for chromosomal crossover, the DSBR pathway is a likely model of how crossover homologous recombination occurs during meiosis.\n\nWhether recombination in the DSBR pathway results in chromosomal crossover is determined by how the double Holliday junction is cut, or \"resolved\". Chromosomal crossover will occur if one Holliday junction is cut on the crossing strand and the other Holliday junction is cut on the non-crossing strand (in Figure 4, along the horizontal purple arrowheads at one Holliday junction and along the vertical orange arrowheads at the other). Alternatively, if the two Holliday junctions are cut on the crossing strands (along the horizontal purple arrowheads at both Holliday junctions in Figure 4), then chromosomes without crossover will be produced.\n\nHomologous recombination via the SDSA pathway occurs in cells that divide through mitosis and meiosis and results in non-crossover products. In this model, the invading 3' strand is extended along the recipient DNA duplex by a DNA polymerase, and is released as the Holliday junction between the donor and recipient DNA molecules slides in a process called \"branch migration\". The newly synthesized 3' end of the invading strand is then able to anneal to the other 3' overhang in the damaged chromosome through complementary base pairing. After the strands anneal, a small flap of DNA can sometimes remain. Any such flaps are removed, and the SDSA pathway finishes with the resealing, also known as \"ligation\", of any remaining single-stranded gaps.\n\nDuring mitosis, the major homologous recombination pathway for repairing DNA double-strand breaks appears to be the SDSA pathway (rather than the DSBR pathway). The SDSA pathway produces non-crossover recombinants (Figure 4). During meiosis non-crossover recombinants also occur frequently and these appear to arise mainly by the SDSA pathway as well. Non-crossover recombination events occurring during meiosis likely reflect instances of repair of DNA double-strand damages or other types of DNA damages.\n\nThe single-strand annealing (SSA) pathway of homologous recombination repairs double-strand breaks between two repeat sequences. The SSA pathway is unique in that it does not require a separate similar or identical molecule of DNA, like the DSBR or SDSA pathways of homologous recombination. Instead, the SSA pathway only requires a single DNA duplex, and uses the repeat sequences as the identical sequences that homologous recombination needs for repair. The pathway is relatively simple in concept: after two strands of the same DNA duplex are cut back around the site of the double-strand break, the two resulting 3' overhangs then align and anneal to each other, restoring the DNA as a continuous duplex.\n\nAs DNA around the double-strand break is cut back, the single-stranded 3' overhangs being produced are coated with the RPA protein, which prevents the 3' overhangs from sticking to themselves. A protein called Rad52 then binds each of the repeat sequences on either side of the break, and aligns them to enable the two complementary repeat sequences to anneal. After annealing is complete, leftover non-homologous flaps of the 3' overhangs are cut away by a set of nucleases, known as Rad1/Rad10, which are brought to the flaps by the Saw1 and Slx4 proteins. New DNA synthesis fills in any gaps, and ligation restores the DNA duplex as two continuous strands. The DNA sequence between the repeats is always lost, as is one of the two repeats. The SSA pathway is considered mutagenic since it results in such deletions of genetic material.\n\nDuring DNA replication, double-strand breaks can sometimes be encountered at replication forks as DNA helicase unzips the template strand. These defects are repaired in the \"break-induced replication\" (BIR) pathway of homologous recombination. The precise molecular mechanisms of the BIR pathway remain unclear. Three proposed mechanisms have strand invasion as an initial step, but they differ in how they model the migration of the D-loop and later phases of recombination.\n\nThe BIR pathway can also help to maintain the length of telomeres (regions of DNA at the end of eukaryotic chromosomes) in the absence of (or in cooperation with) telomerase. Without working copies of the telomerase enzyme, telomeres typically shorten with each cycle of mitosis, which eventually blocks cell division and leads to senescence. In budding yeast cells where telomerase has been inactivated through mutations, two types of \"survivor\" cells have been observed to avoid senescence longer than expected by elongating their telomeres through BIR pathways.\n\nMaintaining telomere length is critical for cell immortalization, a key feature of cancer. Most cancers maintain telomeres by upregulating telomerase. However, in several types of human cancer, a BIR-like pathway helps to sustain some tumors by acting as an alternative mechanism of telomere maintenance. This fact has led scientists to investigate whether such recombination-based mechanisms of telomere maintenance could thwart anti-cancer drugs like telomerase inhibitors.\n\nHomologous recombination is a major DNA repair process in bacteria. It is also important for producing genetic diversity in bacterial populations, although the process differs substantially from meiotic recombination, which repairs DNA damages and brings about diversity in eukaryotic genomes. Homologous recombination has been most studied and is best understood for \"Escherichia coli\". Double-strand DNA breaks in bacteria are repaired by the RecBCD pathway of homologous recombination. Breaks that occur on only one of the two DNA strands, known as single-strand gaps, are thought to be repaired by the RecF pathway. Both the RecBCD and RecF pathways include a series of reactions known as \"branch migration\", in which single DNA strands are exchanged between two intercrossed molecules of duplex DNA, and \"resolution\", in which those two intercrossed molecules of DNA are cut apart and restored to their normal double-stranded state.\n\nThe RecBCD pathway is the main recombination pathway used in many bacteria to repair double-strand breaks in DNA, and the proteins are found in a broad array of bacteria. These double-strand breaks can be caused by UV light and other radiation, as well as chemical mutagens. Double-strand breaks may also arise by DNA replication through a single-strand nick or gap. Such a situation causes what is known as a collapsed replication fork and is fixed by several pathways of homologous recombination including the RecBCD pathway.\n\nIn this pathway, a three-subunit enzyme complex called RecBCD initiates recombination by binding to a blunt or nearly blunt end of a break in double-strand DNA. After RecBCD binds the DNA end, the RecB and RecD subunits begin unzipping the DNA duplex through helicase activity. The RecB subunit also has a nuclease domain, which cuts the single strand of DNA that emerges from the unzipping process. This unzipping continues until RecBCD encounters a specific nucleotide sequence (5'-GCTGGTGG-3') known as a Chi site.\n\nUpon encountering a Chi site, the activity of the RecBCD enzyme changes drastically. DNA unwinding pauses for a few seconds and then resumes at roughly half the initial speed. This is likely because the slower RecB helicase unwinds the DNA after Chi, rather than the faster RecD helicase, which unwinds the DNA before Chi. Recognition of the Chi site also changes the RecBCD enzyme so that it cuts the DNA strand with Chi and begins loading multiple RecA proteins onto the single-stranded DNA with the newly generated 3' end. The resulting RecA-coated nucleoprotein filament then searches out similar sequences of DNA on a homologous chromosome. The search process induces stretching of the DNA duplex, which enhances homology recognition (a mechanism termed conformational proofreading ). Upon finding such a sequence, the single-stranded nucleoprotein filament moves into the homologous recipient DNA duplex in a process called \"strand invasion\". The invading 3' overhang causes one of the strands of the recipient DNA duplex to be displaced, to form a D-loop. If the D-loop is cut, another swapping of strands forms a cross-shaped structure called a Holliday junction. Resolution of the Holliday junction by some combination of RuvABC or RecG can produce two recombinant DNA molecules with reciprocal genetic types, if the two interacting DNA molecules differ genetically. Alternatively, the invading 3’ end near Chi can prime DNA synthesis and form a replication fork. This type of resolution produces only one type of recombinant (non-reciprocal).\n\nBacteria appear to use the RecF pathway of homologous recombination to repair single-strand gaps in DNA. When the RecBCD pathway is inactivated by mutations and additional mutations inactivate the SbcCD and ExoI nucleases, the RecF pathway can also repair DNA double-strand breaks. In the RecF pathway the RecQ helicase unwinds the DNA and the RecJ nuclease degrades the strand with a 5' end, leaving the strand with the 3' end intact. RecA protein binds to this strand and is either aided by the RecF, RecO, and RecR proteins or stabilized by them. The RecA nucleoprotein filament then searches for a homologous DNA and exchanges places with the identical or nearly identical strand in the homologous DNA.\n\nAlthough the proteins and specific mechanisms involved in their initial phases differ, the two pathways are similar in that they both require single-stranded DNA with a 3' end and the RecA protein for strand invasion. The pathways are also similar in their phases of \"branch migration\", in which the Holliday junction slides in one direction, and \"resolution\", in which the Holliday junctions are cleaved apart by enzymes. The alternative, non-reciprocal type of resolution may also occur by either pathway.\n\nImmediately after strand invasion, the Holliday junction moves along the linked DNA during the branch migration process. It is in this movement of the Holliday junction that base pairs between the two homologous DNA duplexes are exchanged. To catalyze branch migration, the RuvA protein first recognizes and binds to the Holliday junction and recruits the RuvB protein to form the RuvAB complex. Two sets of the RuvB protein, which each form a ring-shaped ATPase, are loaded onto opposite sides of the Holliday junction, where they act as twin pumps that provide the force for branch migration. Between those two rings of RuvB, two sets of the RuvA protein assemble in the center of the Holliday junction such that the DNA at the junction is sandwiched between each set of RuvA. The strands of both DNA duplexes—the \"donor\" and the \"recipient\" duplexes—are unwound on the surface of RuvA as they are guided by the protein from one duplex to the other.\n\nIn the resolution phase of recombination, any Holliday junctions formed by the strand invasion process are cut, thereby restoring two separate DNA molecules. This cleavage is done by RuvAB complex interacting with RuvC, which together form the RuvABC complex. RuvC is an endonuclease that cuts the degenerate sequence 5'-(A/T)TT(G/C)-3'. The sequence is found frequently in DNA, about once every 64 nucleotides. Before cutting, RuvC likely gains access to the Holliday junction by displacing one of the two RuvA tetramers covering the DNA there. Recombination results in either \"splice\" or \"patch\" products, depending on how RuvC cleaves the Holliday junction. Splice products are crossover products, in which there is a rearrangement of genetic material around the site of recombination. Patch products, on the other hand, are non-crossover products in which there is no such rearrangement and there is only a \"patch\" of hybrid DNA in the recombination product.\n\nHomologous recombination is an important method of integrating donor DNA into a recipient organism's genome in horizontal gene transfer, the process by which an organism incorporates foreign DNA from another organism without being the offspring of that organism. Homologous recombination requires incoming DNA to be highly similar to the recipient genome, and so horizontal gene transfer is usually limited to similar bacteria. Studies in several species of bacteria have established that there is a log-linear decrease in recombination frequency with increasing difference in sequence between host and recipient DNA.\n\nIn bacterial conjugation, where DNA is transferred between bacteria through direct cell-to-cell contact, homologous recombination helps integrate foreign DNA into the host genome via the RecBCD pathway. The RecBCD enzyme promotes recombination after DNA is converted from single-strand DNA–in which form it originally enters the bacterium–to double-strand DNA during replication. The RecBCD pathway is also essential for the final phase of transduction, a type of horizontal gene transfer in which DNA is transferred from one bacterium to another by a virus. Foreign, bacterial DNA is sometimes misincorporated in the capsid head of bacteriophage virus particles as DNA is packaged into new bacteriophages during viral replication. When these new bacteriophages infect other bacteria, DNA from the previous host bacterium is injected into the new bacterial host as double-strand DNA. The RecBCD enzyme then incorporates this double-strand DNA into the genome of the new bacterial host.\n\nNatural bacterial transformation involves the transfer of DNA from a donor bacterium to a recipient bacterium, where both donor and recipient are ordinarily of the same species. Transformation, unlike bacterial conjugation and transduction, depends on numerous bacterial gene products that specifically interact to perform this process. Thus transformation is clearly a bacterial adaptation for DNA transfer. In order for a bacterium to bind, take up and integrate donor DNA into its resident chromosome by homologous recombination, it must first enter a special physiological state termed competence. The \"RecA\"/\"Rad51\"/\"DMC1\" gene family plays a central role in homologous recombination during bacterial transformation as it does during eukaryotic meiosis and mitosis. For instance, the RecA protein is essential for transformation in \"Bacillus subtilis\" and \"Streptococcus pneumoniae\", and expression of the RecA gene is induced during the development of competence for transformation in these organisms.\n\nAs part of the transformation process, the RecA protein interacts with entering single-stranded DNA (ssDNA) to form RecA/ssDNA nucleofilaments that scan the resident chromosome for regions of homology and bring the entering ssDNA to the corresponding region, where strand exchange and homologous recombination occur. Thus the process of homologous recombination during bacterial transformation has fundamental similarities to homologous recombination during meiosis.\n\nHomologous recombination occurs in several groups of viruses. In DNA viruses such as herpesvirus, recombination occurs through a break-and-rejoin mechanism like in bacteria and eukaryotes. There is also evidence for recombination in some RNA viruses, specifically positive-sense ssRNA viruses like retroviruses, picornaviruses, and coronaviruses. There is controversy over whether homologous recombination occurs in negative-sense ssRNA viruses like influenza.\n\nIn RNA viruses, homologous recombination can be either precise or imprecise. In the precise type of RNA-RNA recombination, there is no difference between the two parental RNA sequences and the resulting crossover RNA region. Because of this, it is often difficult to determine the location of crossover events between two recombining RNA sequences. In imprecise RNA homologous recombination, the crossover region has some difference with the parental RNA sequences – caused by either addition, deletion, or other modification of nucleotides. The level of precision in crossover is controlled by the sequence context of the two recombining strands of RNA: sequences rich in adenine and uracil decrease crossover precision.\n\nHomologous recombination is important in facilitating viral evolution. For example, if the genomes of two viruses with different disadvantageous mutations undergo recombination, then they may be able to regenerate a fully functional genome. Alternatively, if two similar viruses have infected the same host cell, homologous recombination can allow those two viruses to swap genes and thereby evolve more potent variations of themselves.\n\nHomologous recombination is the proposed mechanism whereby the DNA virus \"human herpesvirus-6\" integrates into human telomeres.\n\nWhen two or more viruses, each containing lethal genomic damage, infect the same host cell, the virus genomes can often pair with each other and undergo homologous recombinational repair to produce viable progeny. This process, known as multiplicity reactivation, has been studied in several bacteriophages, including phage T4. Enzymes employed in recombinational repair in phage T4 are functionally homologous to enzymes employed in bacterial and eukaryotic recombinational repair. In particular, with regard to a gene necessary for the strand exchange reaction, a key step in homologous recombinational repair, there is functional homology from viruses to humans (i. e. \"uvsX\" in phage T4; \"recA\" in E. coli and other bacteria, and \"rad51\" and \"dmc1\" in yeast and other eukaryotes, including humans). Multiplicity reactivation has also been demonstrated in numerous pathogenic viruses.\n\nWithout proper homologous recombination, chromosomes often incorrectly align for the first phase of cell division in meiosis. This causes chromosomes to fail to properly segregate in a process called nondisjunction. In turn, nondisjunction can cause sperm and ova to have too few or too many chromosomes. Down's syndrome, which is caused by an extra copy of chromosome 21, is one of many abnormalities that result from such a failure of homologous recombination in meiosis.\n\nDeficiencies in homologous recombination have been strongly linked to cancer formation in humans. For example, each of the cancer-related diseases Bloom's syndrome, Werner's syndrome and Rothmund-Thomson syndrome are caused by malfunctioning copies of RecQ helicase genes involved in the regulation of homologous recombination: \"BLM\", \"WRN\" and \"RECQL4\", respectively. In the cells of Bloom's syndrome patients, who lack a working copy of the BLM protein, there is an elevated rate of homologous recombination. Experiments in mice deficient in BLM have suggested that the mutation gives rise to cancer through a loss of heterozygosity caused by increased homologous recombination. A loss in heterozygosity refers to the loss of one of two versions—or alleles—of a gene. If one of the lost alleles helps to suppress tumors, like the gene for the retinoblastoma protein for example, then the loss of heterozygosity can lead to cancer.\n\nDecreased rates of homologous recombination cause inefficient DNA repair, which can also lead to cancer. This is the case with BRCA1 and BRCA2, two similar tumor suppressor genes whose malfunctioning has been linked with considerably increased risk for breast and ovarian cancer. Cells missing BRCA1 and BRCA2 have a decreased rate of homologous recombination and increased sensitivity to ionizing radiation, suggesting that decreased homologous recombination leads to increased susceptibility to cancer. Because the only known function of BRCA2 is to help initiate homologous recombination, researchers have speculated that more detailed knowledge of BRCA2's role in homologous recombination may be the key to understanding the causes of breast and ovarian cancer.\n\nTumours with a homologous recombination deficiency (including BRCA defects) are described as HRD-positive.\n\n While the pathways can mechanistically vary, the ability of organisms to perform homologous recombination is universally conserved across all domains of life. Based on the similarity of their amino acid sequences, homologs of a number of proteins can be found in multiple domains of life indicating that they evolved a long time ago, and have since diverged from common ancestral proteins.\n\nRecA recombinase family members are found in almost all organisms with RecA in bacteria, Rad51 and DMC1 in eukaryotes, RadA in archaea, and UvsX in T4 phage.\n\nRelated single stranded binding proteins that are important for homologous recombination, and many other processes, are also found in all domains of life.\n\nRad54, Mre11, Rad50, and a number of other proteins are also found in both archaea and eukaryotes.\n\nThe proteins of the RecA recombinase family of proteins are thought to be descended from a common ancestral recombinase. The RecA recombinase family contains RecA protein from bacteria, the Rad51 and Dmc1 proteins from eukaryotes, and RadA from archaea, and the recombinase paralog proteins. Studies modeling the evolutionary relationships between the Rad51, Dmc1 and RadA proteins indicate that they are monophyletic, or that they share a common molecular ancestor. Within this protein family, Rad51 and Dmc1 are grouped together in a separate clade from RadA. One of the reasons for grouping these three proteins together is that they all possess a modified helix-turn-helix motif, which helps the proteins bind to DNA, toward their N-terminal ends. An ancient gene duplication event of a eukaryotic RecA gene and subsequent mutation has been proposed as a likely origin of the modern RAD51 and DMC1 genes.\n\nThe proteins generally share a long conserved region known as the RecA/Rad51 domain. Within this protein domain are two sequence motifs, Walker A motif and Walker B motif. The Walker A and B motifs allow members of the RecA/Rad51 protein family to engage in ATP binding and ATP hydrolysis.\n\nThe discovery of Dmc1 in several species of \"Giardia\", one of the earliest protists to diverge as a eukaryote, suggests that meiotic homologous recombination—and thus meiosis itself—emerged very early in eukaryotic evolution. In addition to research on Dmc1, studies on the Spo11 protein have provided information on the origins of meiotic recombination. Spo11, a type II topoisomerase, can initiate homologous recombination in meiosis by making targeted double-strand breaks in DNA. Phylogenetic trees based on the sequence of genes similar to SPO11 in animals, fungi, plants, protists and archaea have led scientists to believe that the version Spo11 currently in eukaryotes emerged in the last common ancestor of eukaryotes and archaea.\n\nMany methods for introducing DNA sequences into organisms to create recombinant DNA and genetically modified organisms use the process of homologous recombination. Also called gene targeting, the method is especially common in yeast and mouse genetics. The gene targeting method in knockout mice uses mouse embryonic stem cells to deliver artificial genetic material (mostly of therapeutic interest), which represses the target gene of the mouse by the principle of homologous recombination. The mouse thereby acts as a working model to understand the effects of a specific mammalian gene. In recognition of their discovery of how homologous recombination can be used to introduce genetic modifications in mice through embryonic stem cells, Mario Capecchi, Martin Evans and Oliver Smithies were awarded the 2007 Nobel Prize for Physiology or Medicine.\n\nAdvances in gene targeting technologies which hijack the homologous recombination mechanics of cells are now leading to the development of a new wave of more accurate, isogenic human disease models. These engineered human cell models are thought to more accurately reflect the genetics of human diseases than their mouse model predecessors. This is largely because mutations of interest are introduced into endogenous genes, just as they occur in the real patients, and because they are based on human genomes rather than rat genomes. Furthermore, certain technologies enable the knock-in of a particular mutation rather than just knock-outs associated with older gene targeting technologies.\n\nProtein engineering with homologous recombination develops chimeric proteins by swapping fragments between two parental proteins. These techniques exploit the fact that recombination can introduce a high degree of sequence diversity while preserving a protein's ability to fold into its tertiary structure, or three-dimensional shape. This stands in contrast to other protein engineering techniques, like random point mutagenesis, in which the probability of maintaining protein function declines exponentially with increasing amino acid substitutions. The chimeras produced by recombination techniques are able to maintain their ability to fold because their swapped parental fragments are structurally and evolutionarily conserved. These recombinable \"building blocks\" preserve structurally important interactions like points of physical contact between different amino acids in the protein's structure. Computational methods like SCHEMA and statistical coupling analysis can be used to identify structural subunits suitable for recombination.\n\nTechniques that rely on homologous recombination have been used to engineer new proteins. In a study published in 2007, researchers were able to create chimeras of two enzymes involved in the biosynthesis of isoprenoids, a diverse class of compounds including hormones, visual pigments and certain pheromones. The chimeric proteins acquired an ability to catalyze an essential reaction in isoprenoid biosynthesis—one of the most diverse pathways of biosynthesis found in nature—that was absent in the parent proteins. Protein engineering through recombination has also produced chimeric enzymes with new function in members of a group of proteins known as the cytochrome P450 family, which in humans is involved in detoxifying foreign compounds like drugs, food additives and preservatives.\n\nCancer cells with BRCA mutations have deficiencies in homologous recombination, and drugs to exploit those deficiencies have been developed and used successfully in clinical trials. Olaparib, a PARP1 inhibitor, shrunk or stopped the growth of tumors from breast, ovarian and prostate cancers caused by mutations in the BRCA1 or BRCA2 genes, which are necessary for HR. When BRCA1 or BRCA2 is absent, other types of DNA repair mechanisms must compensate for the deficiency of HR, such as base-excision repair (BER) for stalled replication forks or non-homologous end joining (NHEJ) for double strand breaks. By inhibiting BER in an HR-deficient cell, olaparib applies the concept of synthetic lethality to specifically target cancer cells. While PARP1 inhibitors represent a novel approach to cancer therapy, researchers have cautioned that they may prove insufficient for treating late-stage metastatic cancers. Cancer cells can become resistant to a PARP1 inhibitor if they undergo deletions of mutations in BRCA2, undermining the drug's synthetic lethality by restoring cancer cells' ability to repair DNA by HR.\n\n\n", "id": "2631477", "title": "Homologous recombination"}
{"url": "https://en.wikipedia.org/wiki?curid=1502660", "text": "X-inactivation\n\nX-inactivation (also called lyonization) is a process by which one of the copies of the X chromosome present in female mammals is inactivated. The inactive X chromosome is silenced by its being packaged in such a way that it has a transcriptionally inactive structure called heterochromatin. As nearly all female mammals have two X chromosomes, X-inactivation prevents them from having twice as many X chromosome gene products as males, who only possess a single copy of the X chromosome (see dosage compensation). The choice of which X chromosome will be inactivated is random in placental mammals such as humans, but once an X chromosome is inactivated it will remain inactive throughout the lifetime of the cell and its descendants in the organism. Unlike the random X-inactivation in placental mammals, inactivation in marsupials applies exclusively to the paternally derived X chromosome.\n\nIn 1959 Susumu Ohno showed that the two X-chromosomes of mammals were different: one appeared similar to the autosomes; the other was condensed and heterochromatic. This finding suggested, independently to two groups of investigators, that one of the X-chromosomes underwent inactivation. In 1961, Mary Lyon proposed the random inactivation of one female X chromosome to explain the mottled phenotype of female mice heterozygous for coat color genes. The Lyon hypothesis also accounted for the findings that one copy of the X chromosome in female cells was highly condensed, and that mice with only one copy of the X chromosome developed as infertile females. This suggested to Ernest Beutler, studying heterozygous females for Glucose-6-phosphate dehydrogenase (G6PD) deficiency, that there were two red cell populations of erythrocytes in such heterozygotes: deficient cells and normal cells, depending on whether the inactivated X chromosome contains the normal or defective G6PD allele.\n\nAll mouse cells undergo an early, imprinted inactivation of the paternally-derived X chromosome in two-cell or four-cell stage embryos. The extraembryonic tissues (which give rise to the placenta and other tissues supporting the embryo) retain this early imprinted inactivation, and thus only the maternal X chromosome is active in these tissues.\n\nIn the early blastocyst, this initial, imprinted X-inactivation is reversed in the cells of the inner cell mass (which give rise to the embryo), and in these cells both X chromosomes become active again. Each of these cells then independently and randomly inactivates one copy of the X chromosome. This inactivation event is irreversible during the lifetime of the cell, so all the descendants of a cell which inactivated a particular X chromosome will also inactivate that same chromosome. This phenomenon, which can be observed in the coloration of tortoiseshell cats when females are heterozygous for the X-linked gene, should not be confused with mosaicism, which is a term that specifically refers to differences in the genotype of various cell populations in the same individual; X-inactivation, which is an epigenetic change that results in a different phenotype, is \"not\" a change at the genotypic level. For an individual cell or lineage the inactivation is therefore skewed or 'non-random', and this can give rise to mild symptoms in female 'carriers' of X-linked genetic disorders.\n\nX-inactivation is reversed in the female germline, so that all oocytes contain an active X chromosome.\n\nNormal females possess two X chromosomes, and in any given cell one chromosome will be active (designated as Xa) and one will be inactive (Xi). However, studies of individuals with extra copies of the X chromosome show that in cells with more than two X chromosomes there is still only one Xa, and all the remaining X chromosomes are inactivated. This indicates that the default state of the X chromosome in females is inactivation, but one X chromosome is always selected to remain active.\n\nIt is understood that X-chromosome inactivation is a random process, occurring at about the time of gastrulation in the epiblast (cells that will give rise to the embryo). The maternal and paternal X chromosomes have an equal probability of inactivation. This would suggest that women would be expected to suffer from X-linked disorders approximately 50% as often as men (because women have two X chromosomes, while men have only one); however, in actuality, the occurrence of these disorders in females is much lower than that. One explanation for this disparity is that over 25% of genes on the inactivated X chromosome remain expressed, thus providing women with added protection against defective genes coded by the X-chromosome. Some suggest that this disparity must be proof of preferential (non-random) inactivation. Preferential inactivation of the paternal X-chromosome occurs in both marsupials and in cell lineages that form the membranes surrounding the embryo, whereas in placental mammals either the maternally or the paternally derived X-chromosome may be inactivated in different cell lines.\n\nThe time period for X-chromosome inactivation explains this disparity. Inactivation occurs in the epiblast during gastrulation, which gives rise to the embryo. Inactivation occurs on a cellular level, resulting in a mosaic expression, in which patches of cells have an inactive maternal X-chromosome, while other patches have an inactive paternal X-chromosome. For example, a female heterozygous for haemophilia (an X-linked disease) would have about half of her liver cells functioning properly, which is typically enough to ensure normal blood clotting. Chance could result in significantly more dysfunctional cells; however, such statistical extremes are unlikely. Genetic differences on the chromosome may also render one X-chromosome more likely to undergo inactivation. Also, if one X-chromosome has a mutation hindering its growth or rendering it non viable, cells which randomly inactivated that X will have a selective advantage over cells which randomly inactivated the normal allele. Thus, although inactivation is initially random, cells that inactivate a normal allele (leaving the mutated allele active) will eventually be overgrown and replaced by functionally normal cells in which nearly all have the same X-chromosome activated.\n\nIt is hypothesized that there is an autosomally-encoded 'blocking factor' which binds to the X chromosome and prevents its inactivation. The model postulates that there is a limiting blocking factor, so once the available blocking factor molecule binds to one X chromosome the remaining X chromosome(s) are not protected from inactivation. This model is supported by the existence of a single Xa in cells with many X chromosomes and by the existence of two active X chromosomes in cell lines with twice the normal number of autosomes.\n\nSequences at the X inactivation center (XIC), present on the X chromosome, control the silencing of the X chromosome. The hypothetical blocking factor is predicted to bind to sequences within the XIC.\n\nIt’s easy to see the effect of female X heterozygosity in localized traits, such as the unique coat pattern of a calico cat. It can be more difficult, however, to fully understand the expression of un-localized traits in these females, such as the expression of disease.\n\nSince males only have one copy of the X chromosome, all expressed X-chromosomal genes (or alleles, in the case of multiple variant forms for a given gene in the population) are located on that copy of the chromosome. Females, however, will primarily express the genes or alleles located on the X-chromosomal copy that remains active. Considering the situation for one gene or multiple genes causing individual differences in a particular phenotype (i.e., causing variation observed in the population for that phenotype), in homozygous females it doesn’t particularly matter which copy of the chromosome is inactivated, as the alleles on both copies are the same. However, in females that are heterozygous at the causal genes, the inactivation of one copy of the chromosome over the other can have a direct impact on their phenotypic value. Because of this phenomenon, there is an observed increase in phenotypic variation in females that are heterozygous at the involved gene or genes than in females that are homozygous at that gene or those genes. There are many different ways in which the phenotypic variation can play out. In many cases, heterozygous females may be asymptomatic or only present minor symptoms of a given disorder, such as with X-linked adrenoleukodystrophy.\n\nThe differentiation of phenotype in heterozygous females is furthered by the presence of X-inactivation skewing. Typically, each X-chromosome is silenced in half of the cells, but this process is skewed when preferential inactivation of a chromosome occurs. It is thought that skewing happens either by chance or by a physical characteristic of a chromosome that may cause it to be silenced more or less often, such as an unfavorable mutation.\n\nOn average, each X chromosome is inactivated in half of the cells, however 5-20% of \"apparently normal\" women display X-inactivation skewing. In cases where skewing is present, a broad range of symptom expression can occur, resulting in expression varying from minor to severe depending on the skewing proportion. An extreme case of this was seen where monozygotic female twins had extreme variance in expression of Menkes disease (an X-linked disorder) resulting in the death of one twin while the other remained asymptomatic.\n\nIt’s thought that X-inactivation skewing could be caused by issues in the mechanism that causes inactivation, or by issues in the chromosome itself. However, the link between phenotype and skewing is still being questioned, and should be examined on a case-by-case basis. A study looking at both symptomatic and asymptomatic females who were heterozygous for Duchenne and Becker muscular dystrophies (DMD) found no apparent link between transcript expression and skewed X-Inactivation. The study suggests that both mechanisms are independently regulated, and there are other unknown factors at play.\n\nThe X-inactivation center (or simply XIC) on the X chromosome is necessary and sufficient to cause X-inactivation. Chromosomal translocations which place the XIC on an autosome lead to inactivation of the autosome, and X chromosomes lacking the XIC are not inactivated.\n\nThe XIC contains four non-translated RNA genes, Xist, Tsix, Jpx and Ftx, which are involved in X-inactivation. The XIC also contains binding sites for both known and unknown regulatory proteins.\n\nThe X-inactive specific transcript (Xist) gene encodes a large non-coding RNA that is responsible for mediating the specific silencing of the X chromosome from which it is transcribed. The inactive X chromosome is coated by Xist RNA, whereas the Xa is not (See Figure to the right). X chromosomes which lack the Xist gene cannot be inactivated. Artificially placing and expressing the Xist gene on another chromosome leads to silencing of that chromosome.\n\nPrior to inactivation, both X chromosomes weakly express Xist RNA from the Xist gene. During the inactivation process, the future Xa ceases to express Xist, whereas the future Xi dramatically increases Xist RNA production. On the future Xi, the Xist RNA progressively coats the chromosome, spreading out from the XIC; the Xist RNA does not localize to the Xa. The silencing of genes along the Xi occurs soon after coating by Xist RNA.\n\nLike Xist, the Tsix gene encodes a large RNA which is not believed to encode a protein. The Tsix RNA is transcribed antisense to Xist, meaning that the Tsix gene overlaps the Xist gene and is transcribed on the opposite strand of DNA from the Xist gene. Tsix is a negative regulator of Xist; X chromosomes lacking Tsix expression (and thus having high levels of Xist transcription) are inactivated much more frequently than normal chromosomes.\n\nLike Xist, prior to inactivation, both X chromosomes weakly express Tsix RNA from the Tsix gene. Upon the onset of X-inactivation, the future Xi ceases to express Tsix RNA (and increases Xist expression), whereas Xa continues to express Tsix for several days.\n\nRep A is a long non coding RNA that works with another long non coding RNA, Xist, for X inactivation. Rep A inhibits the function of Tsix, the antisense of Xist, in conjunction with eliminating expression of Xite. It promotes methylation of the Tsix region by attracting PRC2 and thus inactivating one of the X chromosomes.\n\nThe inactive X chromosome does not express the majority of its genes, unlike the active X chromosome. This is due to the silencing of the Xi by repressive heterochromatin, which compacts the Xi DNA and prevents the expression of most genes.\n\nCompared to the Xa, the Xi has high levels of DNA methylation, low levels of histone acetylation, low levels of histone H3 lysine-4 methylation, and high levels of histone H3 lysine-9 methylation and H3 lysine-27 methylation mark which is placed by the PRC2 complex recruited by Xist, all of which are associated with gene silencing. Additionally, a histone variant called macroH2A (H2AFY) is exclusively found on nucleosomes along the Xi.\n\nDNA packaged in heterochromatin, such as the Xi, is more condensed than DNA packaged in euchromatin, such as the Xa. The inactive X forms a discrete body within the nucleus called a Barr body. The Barr body is generally located on the periphery of the nucleus, is late replicating within the cell cycle, and, as it contains the Xi, contains heterochromatin modifications and the Xist RNA.\n\nA fraction of the genes along the X chromosome escape inactivation on the Xi. The Xist gene is expressed at high levels on the Xi and is not expressed on the Xa. Many other genes escape inactivation; some are expressed equally from the Xa and Xi, and others, while expressed from both chromosomes, are still predominantly expressed from the Xa. Up to one quarter of genes on the human Xi are capable of escape. Studies in the mouse suggest that in any given cell type, 3% to 15% of genes escape inactivation, and that escaping gene identity varies between tissues.\n\nMany of the genes which escape inactivation are present along regions of the X chromosome which, unlike the majority of the X chromosome, contain genes also present on the Y chromosome. These regions are termed pseudoautosomal regions, as individuals of either sex will receive two copies of every gene in these regions (like an autosome), unlike the majority of genes along the sex chromosomes. Since individuals of either sex will receive two copies of every gene in a pseudoautosomal region, no dosage compensation is needed for females, so it is postulated that these regions of DNA have evolved mechanisms to escape X-inactivation. The genes of pseudoautosomal regions of the Xi do not have the typical modifications of the Xi and have little Xist RNA bound.\n\nThe existence of genes along the inactive X which are not silenced explains the defects in humans with abnormal numbers of the X chromosome, such as Turner syndrome (X0) or Klinefelter syndrome (XXY). Theoretically, X-inactivation should eliminate the differences in gene dosage between affected individuals and individuals with a normal chromosome complement. In affected individuals, however, X-inactivation is incomplete and the dosage of these non-silenced genes will differ as they escape X-inactivation, similar to an autosomal aneuploidy.\n\nThe precise mechanisms that control escape from X-inactivation are not known, but silenced and escape regions have been shown to have distinct chromatin marks. It has been suggested that escape from X-inactivation might be mediated by expression of long non-coding RNA (lncRNA) within the escaping chromosomal domains.\n\nStanley Michael Gartler used X chromosome inactivation to demonstrate the clonal origin of cancers. Examining normal tissues and tumors from females heterozygous for isoenzymes of the sex-linked G6PD gene demonstrated that tumor cells from such individuals express only one form of G6PD, whereas normal tissues are composed of a nearly equal mixture of cells expressing the two different phenotypes. This pattern suggests that a single cell, and not a population, grows into a cancer. However, this pattern has been proven wrong for many cancer types, suggesting that some cancers may be polyclonal in origin.\n\nBesides, measuring the methylation (inactivation) status of the polymorphic human androgen receptor (HUMARA) located on X-chromosome is considered the most accurate method to assess clonality in female cancer biopsies. A great variety of tumors was tested by this method, some, such as renal cell carcinoma, found monoclonal while others (e.g. mesothelioma) were reported polyclonal.\n\nResearchers have also investigated using X-chromosome inactivation to silence the activity of autosomal chromosomes. For example, Jiang \"et al.\" inserted a copy of the Xist gene into one copy of chromosome 21 in stem cells derived from an individual with trisomy 21 (Down's syndrome). The inserted Xist gene induces Barr body formation, triggers stable heterochromatin modifications, and silences most of the genes on the extra copy of chromosome 21. In these modified stem cells, the Xist-mediated gene silencing seems to reverse some of the defects associated with Down's syndrome.\n\n\n\nReview Article\n", "id": "1502660", "title": "X-inactivation"}
{"url": "https://en.wikipedia.org/wiki?curid=25279865", "text": "Nested association mapping\n\nNested association mapping (NAM) is a technique designed by the labs of Edward Buckler, James Holland, and Michael McMullen for identifying and dissecting the genetic architecture of complex traits in corn (Zea mays). It is important to note that nested association mapping (unlike Association mapping) is a specific technique that cannot be performed outside of a specifically designed population such as the Maize NAM population, \nthe details of which are described below.\n\nNAM was created as a means of combining the advantages and eliminating the disadvantages of two traditional methods for identifying quantitative trait loci: linkage analysis and association mapping. Linkage analysis depends upon recent genetic recombination between two different plant lines (as the result of a genetic cross) to identify general regions of interest, with the advantage of requiring few genetic markers to ensure genome wide coverage and high statistical power per allele. Linkage analysis, however, has the disadvantages of low mapping resolution and low allele richness. Association mapping, by contrast, takes advantage of historic recombination, and is performed by scanning a genome for SNPs in linkage disequilibrium with a trait of interest. Association mapping has advantages over linkage analysis in that it can map with high resolution and has high allelic richness, however, it also requires extensive knowledge of SNPs within the genome and is thus only now becoming possible in diverse species such as maize.\n\nNAM takes advantage of both historic and recent recombination events in order to have the advantages low marker density requirements, high allele richness, high mapping resolution, and high statistical power, with none of the disadvantages of either linkage analysis or association mapping. In these regards, the NAM approach is similar in principle to the MAGIC lines and AMPRILs in Arabidopsis and the Collaborative Cross in mouse.\n\nTwenty-five diverse corn lines were chosen as the parental lines for the NAM population in order to encompass the remarkable diversity of maize and preserve historic linkage disequilibrium. Each parental line was crossed to the B73 maize inbred (chosen as a reference line due to its use in the public maize sequencing project and wide deployment as one of the most successful commercial inbred lines) to create the F1 population. The F1 plants were then self-fertilized for six generations in order to create a total of 200 homozygous recombinant inbred lines (RILs) per family, for a total of 5000 RILs within the NAM population. The lines are publicly available through the USDA-ARS Maize Stock Center.\n\nEach RIL was then genotyped with the same 1106 molecular markers (for this to be possible, the researchers selected markers for which B73 had a rare allele), in order to identify recombination blocks. After genotyping with the 1106 markers, each of the parental lines was either sequenced or high-density genotyped, and the results of that sequencing/genotyping overlaid on the recombination blocks identified for each RIL. The result was 5000 RILs that were either fully sequenced or high density genotyped that, due to genotyping with the common 1106 markers, could all be compared to each other and analyzed together (Figure 1).\n\nThe second aspect of the NAM population characterization is the sequencing of the parental lines. This captures information on the natural variation that went into the population and a record of the extensive recombination captured in the history of maize variation. The first phase of this sequencing was by reduced representation sequencing using next generation sequencing technology, as report in Gore, Chia et al. in 2009. This initial sequencing discovered 1.6 million variable regions in maize, which is now facilitating analysis of a wide range of traits.\n\nAs with traditional QTL mapping strategies, the general goal in Nested Association Mapping is to correlate a phenotype of interest with specific genotypes. One of the creators’ stated goals for the NAM population was to be able to perform genome-wide association studies in maize by looking for associations between SNPs within the NAM population and quantitative traits of interest (e.g. flowering time, plant height, carotene content). As of 2009, however, the sequencing of the original parental lines was not yet completed to the degree necessary to perform these analyses. The NAM population has, however, been successfully used for linkage analysis. In the linkage study that has been released, the unique structure of the NAM population, described in the previous section, allowed for joint stepwise regression and joint inclusive composite interval mapping of the combined NAM families to identify QTLs for flowering time.\n\nThe first publication in which NAM was used to identify QTLs was authored by the Buckler lab on the genetic architecture of maize flowering time, and published in the summer of 2009. In this groundbreaking study, the authors scored days to silking, days to anthesis, and the silking-anthesis interval for nearly one million plants, then performed single and joint stepwise regression and inclusive composite interval mapping (ICIM) to identify 39 QTLs explaining 89% of the variance in days to silking and days to anthesis and 29 QTLs explaining 64% of the variance in the silking-anthesis interval.\n\nNinety-eight percent of the flowering time QTLs identified in this paper were found to affect flowering time by less than one day (as compared to the B73 reference). These relatively small QTL effects, however, were also shown to sum for each family to equal large differences and changes in days to silking. Furthermore, it was observed that while most QTLs were shared between families, each family appears to have functionally distinct alleles for most QTLs. These observations led the authors to propose a model of “Common genes with uncommon variants” to explain flowering time diversity in maize. They tested their model by documenting an allelic series in the previously studied maize flowering time QTL Vgt1 (vegetation-to-transition1) by controlling for genetic background and estimating the effects of vgt1 in each family. They then went on to identify specific sequence variants that corresponded to the allelic series, including one allele containing a miniature transposon strongly associated with early flowering, and other alleles containing SNPs associated with later flowering.\n\nNested association mapping has tremendous potential for the investigation of agronomic traits in maize and other species. As the initial flowering time study demonstrates, NAM has the power to identify QTLs for agriculturally relevant traits and to relate those QTLs to homologs and candidate genes in non-maize species. Furthermore, the NAM lines become a powerful public resource for the maize community, and an opportunity for the sharing of maize germplasm as well as the results of maize studies via common databases (see external links), further facilitating future research into maize agricultural traits. Given that maize is one of the most important agricultural crops worldwide, such research has powerful implications for the genetic improvement of crops, and subsequently, worldwide food security.\n\nSimilar designs are also being created for wheat, barley, sorghum, and \"Arabidopsis thaliana\".\n\nMaize Databases:\n\n", "id": "25279865", "title": "Nested association mapping"}
{"url": "https://en.wikipedia.org/wiki?curid=25273326", "text": "Association mapping\n\nAssociation mapping (genetics), also known as \"linkage disequilibrium mapping\", is a method of mapping quantitative trait loci (QTLs) that takes advantage of historic linkage disequilibrium to link phenotypes (observable characteristics) to genotypes (the genetic constitution of organisms), uncovering genetic associations.\n\nAssociation mapping is based on the idea that traits that have entered a population only recently will still be linked to the surrounding genetic sequence of the original evolutionary ancestor, or in other words, will more often be found within a given haplotype, than outside of it. It is most often performed by scanning the entire genome for significant associations between a panel of SNPs (which, in many cases are spotted onto glass slides to create “SNP chips”) and a particular phenotype. These associations must then be independently verified in order to show that they either (a) contribute to the trait of interest directly, or (b) are linked to/ in linkage disequilibrium with a quantitative trait locus (QTL) that contributes to the trait of interest.\n\nThe advantage of association mapping is that it can map quantitative traits with high resolution in a way that is statistically very powerful. Association mapping, however, also requires extensive knowledge of SNPs within the genome of the organism of interest, and is therefore difficult to perform in species that have not been well studied or do not have well-annotated genomes. Association mapping has been most widely applied to the study of human disease, specifically in the form of a genome-wide association study (GWAS). A genome-wide association study is performed by scanning an entire genome for SNPs associated with a particular trait of interest, or in the case of human disease, with a particular disease of interest. To date, thousands of genome wide associations studies have been performed on the human genome in attempt to identify SNPs associated with a wide variety of complex human diseases (e.g. cancer, Alzheimer's disease, and obesity). The results of all such published GWAS are maintained in an NIH database (figure 1). Whether or not these studies have been clinically and/or therapeutically useful, however, remains controversial.\n\n(A) Association mapping in population where members are assumed to be independent.\nSeveral standard methods to test for association. \nCase control studies – \nCase control studies was among the first approaches utilized to determine whether particular genetic variant is associated with increased risk of disease in humans. Early on, Woofle in 1955, proposed a relative risk statistic that could be used to assess genotype dependent risk. However persistent concern regarding these studies is the adequacy of matching cases and controls. In particular, population stratification can produce false positive associations. In response to this concern, Falk and Rubenstein in 1987, suggested a method for assessing relative risk that uses family based controls, obviating this source of potential error. Basically, the method uses a control sample of the parental alleles or haplotypes not transmitted to affected offspring.\n\n(B) Association mapping population where members are assumed to be related\n\nIn the real world it is very hard to find independent (unrelated) individuals. Population based association mapping has been modified to control population stratification or relatedness in nested association mapping. Still there is one other limitation in population based QTL mapping; when the frequency of the favorable allele should be relatively high to be detected. Usually favorable alleles are rare mutant alleles ( for example usually a resistant parent might be 1 out of 10000 genotypes). Another variant of association mapping in related populations is family based association mapping. In family based association mapping instead of multiple unrelated individuals multiple unrelated families or pedigrees are used. The family based association mapping (external link) can be used in situations where the mutant alleles have been introgressed in populations. One of popular family based association mapping Transmission disequilibrium test. For details please read Family based QTL mapping.\n\n", "id": "25273326", "title": "Association mapping"}
{"url": "https://en.wikipedia.org/wiki?curid=2173109", "text": "Nick translation\n\nNick translation (or head translation), developed in 1977 by Rigby and Paul Berg, is a tagging technique in molecular biology in which DNA Polymerase I is used to replace some of the nucleotides of a DNA sequence with their labeled analogues, creating a tagged DNA sequence which can be used as a probe in fluorescent in situ hybridization (FISH) or blotting techniques. It can also be used for radiolabeling.\n\nThis process is called nick translation because the DNA to be processed is treated with DNAase to produce single-stranded \"nicks\". This is followed by replacement in nicked sites by DNA polymerase I, which elongates the 3' hydroxyl terminus, removing nucleotides by 5'-3' exonuclease activity, replacing them with dNTPs. To radioactively label a DNA fragment for use as a probe in blotting procedures, one of the incorporated nucleotides provided in the reaction is radiolabeled in the \"alpha\" phosphate position. Similarly, a fluorophore can be attached instead for fluorescent labelling, or an antigen for immunodetection. When DNA polymerase I eventually detaches from the DNA, it leaves another nick in the phosphate backbone. The nick has \"translated\" some distance depending on the processivity of the polymerase. This nick could be sealed by DNA ligase, or its 3' hydroxyl group could serve as the template for further DNA polymerase I activity. Proprietary enzyme mixes are available commercially to perform all steps in the procedure in a single incubation.\n\nNick translation could cause double-stranded DNA breaks, if DNA polymerase I encounters another nick on the opposite strand, resulting in two shorter fragments. This does not influence the performance of the labelled probe in in-situ hybridization.\n", "id": "2173109", "title": "Nick translation"}
{"url": "https://en.wikipedia.org/wiki?curid=24105059", "text": "Savior sibling\n\nA savior baby or savior sibling is a child who is born to provide an organ or cell transplant to a sibling that is affected with a fatal disease, such as cancer or Fanconi anemia, that can best be treated by hematopoietic stem cell transplantation. \n\nThe savior sibling is conceived through in vitro fertilization. Fertilized zygotes are tested for genetic compatibility (human leucocyte antigen (HLA) typing), using preimplantation genetic diagnosis (PGD), and only zygotes that are compatible with the existing child are implanted. Zygotes are also tested to make sure they are free of the original genetic disease. The procedure is controversial.\n\nA \"savior sibling\" may be the solution for any disease treated by hematopoietic stem cell transplantation. It is effective against genetically detectable (mostly monogenic) diseases, e.g. Fanconi anemia, Diamond-Blackfan anemia and β-thalassemia, in the ailing sibling, since the savior sibling can be selected to not have inherited the disease. The procedure may also be used in children with leukemia, and in such cases HLA match is the only requirement, and not exclusion of any other obvious genetic disorder.\n\nMultiple embryos are created and preimplantation genetic diagnosis is used to detect and select ones that are free of a genetic disorder and that are also a HLA match for an existing sibling who requires a transplant. Upon birth, umbilical cord blood is taken and used for hematopoietic stem cell transplantation.\n\n\nArguments for or against the use of PGD/HLA tissue typing are based on several key issues including the commodification and welfare of the donor child.\n\nThe main ethical argument against is the possible exploitation of the child, e.g. potential adverse psychological effects on a child born not for itself but to save another. \n\nA survey of 4,000 Americans showed that 61% approved of PGD use for savior siblings.\n\nYury Verlinsky and collaborators described the first case in 2001.\n\nThe novel \"My Sister's Keeper\", later adapted into a film, is about a child who was born as a savior sibling to her sister Kate who is affected by acute promyelocytic leukemia.\n\nIn the British soap opera \"Emmerdale\", Debbie Dingle gave birth to her son Jack, who would serve as a savior sibling to his older sister Sarah, who was suffering from Fanconi anemia.\n\nIn the American show \"Heroes\", one of the protagonists, Mohinder Suresh is revealed to have been conceived to cure his sister Shanti of a deadly disease known as the Shanti Virus. However, Mohinder was born too late to save his sister. Through the first two seasons of the show, Mohinder's blood contains the only cure to the Shanti Virus as a result of the method of his conception.\n\n", "id": "24105059", "title": "Savior sibling"}
{"url": "https://en.wikipedia.org/wiki?curid=553700", "text": "Housekeeping gene\n\nIn molecular biology, housekeeping genes are typically constitutive genes that are required for the maintenance of basic cellular function, and are expressed in all cells of an organism under normal and patho-physiological conditions. Although some housekeeping genes are expressed at relatively constant rates in most non-pathological situations, the expression of other housekeeping genes may vary depending on experimental conditions.\n\nThe origin of the term \"housekeeping gene\" remains obscure. Literature from 1976 used the term to describe specifically tRNA and rRNA. For experimental purposes, the expression of one or multiple housekeeping genes is used as a reference point for the analysis of expression levels of other genes. The key criterion for the use of a housekeeping gene in this manner is that the chosen housekeeping gene is uniformly expressed with low variance under both control and experimental conditions. Validation of housekeeping genes should be performed before their use in gene expression experiments such as RT-PCR.\n\nThe following is a partial list of \"housekeeping genes.\" For a more complete list, see this list compiled by Eli Eisenberg and Erez Lavanon. Entries that appear without a reference are from this updated list from 2013.\n\n\n\n\n\n\n\nRPS19BP1 \n\n\n\n\n\n\nThere is significant overlap in function with regards to some of these proteins. In particular, the Rho-related genes are important in nuclear trafficking (i.e.: mitosis) as well as with mobility along the cytoskeleton in general. These genes of particular interest in cancer research.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"A specialized form of cell signaling\"\n\n\n\n\n\n\n\n\n\n\n\ntetratricopeptide\"\n\n\n\"Although this page is devoted to genes that should be ubiquitously expressed, this section is for genes whose current name reflects their relative upregulation in testes\"\n\n\n", "id": "553700", "title": "Housekeeping gene"}
{"url": "https://en.wikipedia.org/wiki?curid=6682341", "text": "Gene orders\n\nGene orders are the permutation of genome arrangement. A fair amount of research has been done trying to determine whether gene orders evolve according to a molecular clock (molecular clock hypothesis) or in jumps (punctuated equilibrium).\n\nSome research on gene orders in animals' mitochondrial gemones reveal that the mutation rate of gene orders is not a constant in some degrees.\n", "id": "6682341", "title": "Gene orders"}
{"url": "https://en.wikipedia.org/wiki?curid=1505166", "text": "Genetic architecture\n\nGenetic architecture refers to the underlying genetic basis of a phenotypic trait and its variational properties. Phenotypic variation for quantitative traits is, at the most basic level, the result of the segregation of alleles at quantitative trait loci (QTL). Environmental factors and other external influences can also play a role in phenotypic variation. Genetic architecture is a broad term that can be described for any given individual based on information regarding gene and allele number, the distribution of allelic and mutational effects, and patterns of pleiotropy, dominance, and epistasis.\n\nThere are several different experimental views of genetic architecture. Some researchers recognize that the interplay of various genetic mechanisms is incredibly complex, but believe that these mechanisms can be averaged and treated, more or less, like statistical noise. Other researchers claim that each and every gene interaction is significant and that it is necessary to measure and model these individual systemic influences on evolutionary genetics.\n\nGenetic architecture can be studied and applied at many different levels. At the most basic, individual level, genetic architecture describes the genetic basis for differences between individuals, species, and populations. This can include, among other details, how many genes are involved in a specific phenotype and how gene interactions, such as epistasis, influence that phenotype. Line-cross analyses and QTL analyses can be used to study these differences. This is perhaps the most common way that genetic architecture is studied, and though it is useful for supplying pieces of information, it does not generally provide a complete picture of the genetic architecture as a whole.\n\nGenetic architecture can also be used to discuss the evolution of populations. Classical quantitative genetics models, such as that developed by R.A. Fisher, are based on analyses of phenotype in terms of the contributions from different genes and their interactions. Genetic architecture is sometimes studied using a genotype-phenotype map, which graphically depicts the relationship between the genotype and the phenotype.\n\nGenetic architecture is incredibly important for understanding evolutionary theory because it describes phenotypic variation in its underlying genetic terms, and thus it gives us clues about the evolutionary potential of these variations. Therefore, genetic architecture can help us to answer biological questions about speciation, the evolution of sex and recombination, the survival of small populations, inbreeding, understanding diseases, animal and plant breeding, and more.\n\nEvolvability is literally defined as the ability to evolve. In terms of genetics, evolvability is the ability of a genetic system to produce and maintain potentially adaptive genetic variants. There are several aspects of genetic architecture that contribute strongly to the evolvability of a system, including autonomy, mutability, coordination, epistasis, pleiotropy, polygeny, and robustness.\n\nA study published in 2006 used phylogeny to compare the genetic architecture of differing human skin color. In this study, researchers were able to suggest a speculative framework for the evolutionary history underlying current-day phenotypic variation in human skin pigmentation based on the similarities and differences they found in the genotype. Evolutionary history is an important consideration in understanding the genetic basis of any trait, and this study was among the first to utilize these concepts in a paired fashion to determine information about the underlying genetics of a phenotypic trait.\n\nIn 2013, a group of researchers used genome-wide association studies (GWAS) and genome-wide interaction studies (GWIS) to determine the risk of congenital heart defects in patients suffering from Down Syndrome. Down Syndrome is a genetic disorder caused by trisomy of human chromosome 21. The current hypothesis regarding congenital heart defect phenotypes in Down Syndrome individuals is that three copies of functional genomic elements on chromosome 21 and genetic variation of chromosome 21 and non-chromosome 21 loci predispose patients to abnormal heart development. This study identified several congenital heart defect risk loci in Down Syndrome individuals, as well as three copy number variation (CNV) regions that may contribute to congenital heart defects in Down Syndrome individuals.\n\nAnother study, which was published in 2014, sought to identify the genetic architecture of psychiatric disorders. The researchers in this study suggested that there are a large number of contributing loci that are related to various psychiatric disorders. Additionally, they, like many others, suggested that the genetic risk of psychiatric disorders involves the combined effects of many common variants with small effects - in other words, the small effects of a wide number of variants at specific loci add together to produce a large, combined effect on the overall phenotype of the individual. They also acknowledged the presence of large but rare mutations that have a large effect on phenotype. This study showcases the intricacy of genetic architecture by providing an example of many different SNPs and mutations working together, each with a varying effect, to generate a given phenotype.\n\nOther studies regarding genetic architecture are many and varied, but most use similar types of analyses to provide specific information regarding loci involved in producing a phenotype. A study of the human immune system in 2015 uses the same general concepts to identify several loci involved in the development of the immune system, but, like the other studies outlined here, failed to consider other aspects of genetic architecture, such as environmental influences. Unfortunately, many other aspects of genetic architecture remain difficult to quantify.\n\nAlthough there are a few studies that seek to explore the other aspects of genetic architecture, there is little ability with current technologies to link all of the pieces together to build a truly comprehensive model of genetic architecture. For example, in 2003, a study of genetic architecture and the environment was able to show an association of social environment with variation in body size in \"Drosophila melanogaster\". However, this study was not able to tie a direct link to specific genes involved in this variation.\n\n", "id": "1505166", "title": "Genetic architecture"}
{"url": "https://en.wikipedia.org/wiki?curid=431262", "text": "Genetic program\n\nIn biology, a genetic program of a cell is a physiological change brought about by a temporal pattern of activation of a particular subset of genes.\n\nThe metaphor was introduced simultaneously by Ernst Mayr and François Jacob, Jacques Monod in 1961 in two separate articles. \n", "id": "431262", "title": "Genetic program"}
{"url": "https://en.wikipedia.org/wiki?curid=379888", "text": "Inbred strain\n\nInbred strains (also called inbred lines, or rarely for animals linear animals) are individuals of a particular species which are nearly identical to each other in genotype due to long inbreeding. A strain is inbred when it has undergone at least 20 generations of brother x sister or offspring x parent mating, at which point at least 98.6% of the loci in an individual of the strain will be homozygous, and each individual can be treated effectively as clones. Some inbred strains have been bred for over 150 generations, leaving individuals in the population to be isogenic in nature. Inbred strains of animals are frequently used in laboratories for experiments where for the reproducibility of conclusions all the test animals should be as similar as possible. However, for some experiments, genetic diversity in the test population may be desired. Thus outbred strains of most laboratory animals are also available, where an outbred strain is a strain of an organism that is effectively wildtype in nature, where there is as little inbreeding as possible.\n\nCertain plants including the genetic model organism \"Arabidopsis thaliana\" naturally self pollinate, which makes it quite easy to create inbred strains in the laboratory (other plants, including important genetic models such as Maize require transfer of pollen from one flower to another).\n\nInbred strains have been extensively used in research. Several Nobel Prizes that have awarded for work that probably could not have been done without inbred strains. This work includes Medawar's research on immune tolerance, Kohler and Milstein's development of monoclonal antibodies, and Doherty and Zinkernagel's studies of the major histocompatibility complex (MHC).\n\nIsogenic organisms have identical, or near identical genotypes. which is true of inbred strains, since they normally have at least 98.6% similarity by generation 20. this very uniformity means that fewer individuals are required to produce results with the same level of statistical significance when an inbred line is used in comparison to an outbred line in the same experiment\n\nBreeding of inbred strains is often towards specific phenotypes of interest such as behavioural traits like alcohol preference or physical traits like aging. or they can be selected for traits that make them easier to use in experiments like being easy to use in transgenic experiments. One of the key strengths of using inbred strains as a model is that strains are readily available for whatever study one is performing and that there are resources such as the Jackson Laboratory, and Flybase, where one can look up strains with specific phenotypes or genotypes, from among inbred lines, recombinant lines, and Coisogenic strains. Jackson Laboratory has additional features to maintaining mice, one can order mice that have been altered with genetic tools such as Gal4/UAS or CRISPR, meaning that even if the strain does not currently exist, you can still obtain a line of mouse that is useful to your research. Additionally, the embryo's of lines that are of little interest currently can be frozen and preserved until there is an interest in their unique genotypical or phenotypical traits.\n\nfor the analysis of the linkage of quantitative traits, recombinant lines are useful because of their isogenic nature, because the genetic similarity of individuals allows for the replication of a quantitative trait locus analysis. The replication increases the precision of the results from the mapping experiment, and is required for traits such as aging where minor changes in the environment can influence the longevity of an organism, leading to variation in results.\n\nOne type of inbred strain that either has been altered, or naturally mutated so that it is different at a single locus. Such strains are useful in the analysis of variance within an inbred strain or between inbred strains because any differences would be due to the single genetic change, or to a difference in environmental conditions between two individuals of the same strain,\n\none of the more specific uses of Drosophila inbred strains are the use of Gal4/UAS lines in research. Gal4/UAS is a driver system, where Gal4 can be expressed in specific tissues under specific conditions based on its location in the Drosophila genome. Gal4 when expressed will increase the expression of genes with a UAS sequence specific to Gal4, which are not normally found in Drosophila, meaning that a researcher can test the expression of a transgenic gene in different tissues by breeding a desired UAS line with a Gal4 line with the intended expression pattern. Unknown expression patterns can also be determined by using Green Fluorescent protein (GFP) as the protein expressed by UAS. Drosophila in particular has thousands of Gal4 lines with unique and specific expression patterns, making it possible to test most expression patterns within the organism.\n\nInbreeding animals will sometimes lead to genetic drift. The continuous overlaying of like genetics exposes recessive gene patterns that often lead to changes in reproduction performance, fitness, and ability to survive. A decrease in these areas is known as inbreeding depression. A hybrid between two inbred strains can be used to cancel out deleterious recessive genes resulting in an increase in the mentioned areas. This is known as heterosis.\n\nInbred strains, because they are small populations of homozygous individuals, are susceptible to the fixation of new mutations through genetic drift, Jackson laboratory in an information session on genetic drift in mice, calculated a quick estimate of the rate of mutation based on observed traits to be 1 phenotypic mutation every 1.8 generations, though they caution that this is likely an under representation because the data they used was for visible phenotypic changes and not phenotype changes inside of mice strains. they further add that statistically every 6-9 generations, a mutation in the coding sequence is fixed, leading to the creation of a new substrain. Care must be taken when comparing results that two substrains are not compared, because substrains may differ drastically\n\n\"The period before World War I led to the initiation of inbreeding in rats by Dr Helen King in about 1909 and in mice by Dr C. C. Little in 1909. The latter project led to the development of the DBA strain of mice, now widely distributed as the two major sub-strains DBA/1 and DBA/2, which were separated in 1929-1930. DBA mice were nearly lost in 1918, when the main stocks were wiped out by murine paratyphoid, and only three un-pedigreed mice remained alive. Soon after World War I, inbreeding in mice was started on a much larger scale by Dr L. C. Strong, leading in particular to the development of strains C3H and CBA, and by Dr C. C. Little, leading to the C57 family of strains (C57BL, C57BR and C57L). Many of the most popular strains of mice were developed during the next decade, and some are closely related. Evidence from the uniformity of mitochondrian DNA suggests that most of the common inbred mouse strains were probably derived from a single breeding female about 150-200 years ago.\"\n\n\"Many of the most widely used inbred strains of rats were also developed during this period, several of them by Curtis and Dunning at the Columbia University Institute for Cancer Research. Strains dating back to this time include F344, M520 and Z61 and later ACI, ACH, A7322 and COP. Tryon's classic work on selection for maze-bright and dull rats led to the development of the TMB and TMD inbred strains, and later to the common use of inbred rats by experimental psychologists.\"\n\n\nA genealogical chart of mouse inbred strains can be found here on the Jackson Laboratory website and is currently being maintained by the laboratory.\n\nG.M. Rommel first started conducting inbreeding experiments on guinea-pigs in 1906. Strain 2 and 13 guinea-pigs, were derived from these experiments and are still in use today. Sewall Wright took over the experiment in 1915. He was faced with the task of analyzing all of the accumulated data produced by Rommel. Wright became seriously interested in constructing a general mathematical theory of inbreeding. By 1920 Wright had developed his method of path coefficients, which he then used to develop his mathematical theory of inbreeding. Wright introduced the inbreeding coefficient \"F\" as the correlation between uniting gametes in 1922, and most of the subsequent theory of inbreeding has been developed from his work. The definition of the inbreeding coefficient now most widely used is mathematically equivalent to that of Wright.\n\nThe Japanese Medaka fish has a high tolerance for inbreeding, one line having been bred brother-sister for as many as 100 generations without evidence of inbreeding depression, providing a ready tool for laboratory research and genetic manipulations. Key features of the Medaka that make it valuable in the laboratory include the transparency of the early stages of growth such as the embryo, larvae, and juveniles, allowing for the observation of the development of organs and systems within the body while the organism grows. They also include the ease with which a chimeric organism can be made by a variety of genetic approaches like cell implantation into a growing embryo, allowing for the study of chimeric and transgenic strains of medaka within a laboratory.\n\nThough there are many traits about zebrafish that are worthwhile to study including their regeneration, there are relatively few Inbred strains of zebrafish possibly because they experience greater effects from inbreeding depression than mice or Medaka fish, but it is unclear if the effects of inbreeding can be over come so an isogenic strain can be created for laboratory use\n\n", "id": "379888", "title": "Inbred strain"}
{"url": "https://en.wikipedia.org/wiki?curid=5119542", "text": "UBB+1\n\nUBB+1 is shorthand for Ubiquitin-B+1, a frameshifted mutant arising from the Ubiquitin B gene. UBB+1 is thought to arise from molecular misreading, a poorly understood process. Molecular misreading introduces dinucleotide deletions (e.g. ΔGA, ΔGU) into mRNA transcripts. These deletions are not present in genomic DNA. UBB+1 has been observed in the hallmarks of Alzheimer's Disease, as well as other tauopathies and in polyglutamine diseases (e.g.Huntington's disease) but not in synucleinopathies (e.g.Parkinson's disease). Since its discovery it has been shown \"in vitro\" and \"in vivo\" that UBB+1 inhibits the proteasome and gives rise to downstream effects (e.g. a behavioral phenotype; impaired contextual memory). In non-neuronal cells UBB+1 also accumulates suggesting a functional role in non-neuronal diseases. UBB+1 can be truncated by yeast ubiquitin hydrolase 1 (YUH1) and ubiquitin C-terminal hydrolase L3 UCHL3 despite of the fact that the glycine at position 76 as been substituted for a tyrosine.\n", "id": "5119542", "title": "UBB+1"}
{"url": "https://en.wikipedia.org/wiki?curid=3373537", "text": "Three-point cross\n\nIn genetics, a three-point cross is used to determine the loci of three genes in an organism's genome.\n\nAn individual heterozygous for three mutations is crossed with a homozygous recessive individual, and the phenotypes of the progeny are scored. The two most common phenotypes that result are the parental gametes; the two least common phenotypes that result come from a double crossover in gamete formation. By comparing the parental and double-crossover phenotypes, the geneticist can determine which gene is located between the others on the chromosome.\n\nThe recombinant frequency is the ratio of non-parental phenotypes to total individuals. It is expressed as a percentage, which is equivalent to the number of map units (or centiMorgans) between two genes. For example, if 100 out of 1000 individuals display the phenotype resulting from a crossover between genes \"a\" and \"b\", then the recombination frequency is 10 percent and genes \"a\" and \"b\" are 10 map-units apart on the chromosome.\n\nIf the recombination frequency is greater than 50 percent, it means that the genes are unlinked - they are either located on different chromosomes or are sufficiently distant from each other on the same chromosome. Any recombination frequency greater than 50 percent is expressed as exactly 50 percent because, being unlinked, they are equally as likely as not to be separated during gamete formation.\n", "id": "3373537", "title": "Three-point cross"}
{"url": "https://en.wikipedia.org/wiki?curid=3700225", "text": "Spacer DNA\n\nSpacer DNA or intergenic spacer (IGS) is a region of non-coding DNA between genes. The term is used particularly for the spacer DNA between the many tandemly repeated copies of the ribosomal RNA genes.\n\nIn bacteria, spacer DNA sequences are only a few nucleotides long. In eukaryotes, they can be extensive and include repetitive DNA, comprising the majority of the DNA of the genome. In ribosomal DNA, there are spacers within and between gene clusters, called internal transcribed spacer (ITS) and external transcribed spacers (ETS), respectively. In animals, the mitochondrial DNA genes generally have very short spacers. In fungi, mitochondrial DNA spacers are common and variable in length, and they may also be mobile.\n\nSince spacer DNA sequence changes much more rapidly in evolution than the gene sequence, it is thought that spacer DNA does not have a function that depends on its sequence, although it may have sequence-independent function.\n", "id": "3700225", "title": "Spacer DNA"}
{"url": "https://en.wikipedia.org/wiki?curid=4152953", "text": "NlaIII\n\nNlaIII is an endonuclease that cleaves double stranded DNA molecules into fragments:\n\n5'- C A T G| -3'\n\n3'-|G T A C -5'\n\nleaving 3' overhangs of four nucleotides at the ends of each fragment.\n", "id": "4152953", "title": "NlaIII"}
{"url": "https://en.wikipedia.org/wiki?curid=6754094", "text": "Recognition sequence\n\nThe recognition sequence, sometimes also referred to as recognition site, of any DNA-binding protein motif that exhibits binding specificity, refers to the DNA sequence (or subset thereof), to which the domain is specific. Recognition sequences are palindromes .\n\nThe transcription factor Sp1 for example, binds the sequences 5'-(G/T)GGGCGG(G/A)(G/A)(C/T)-3', where (G/T) indicates that the domain will bind a guanine or thymine at this position.\n\nThe restriction endonuclease PstI recognizes, binds, and cleaves the sequence 5'-CTGCAG-3'.\n\nHowever, a recognition sequence refers to a different aspect from that of recognition site. A given recognition sequence can occur one or more times, or not at all on a specific DNA fragment. A recognition site is specified by the position of the site. For example, there are two PstI recognition site in the following DNA sequence fragment, start at base 9 and 31 respectively. A recognition sequence is a specific sequence, usually very short (less than 10 bases). Depending on the degree of specificity of the protein, a DNA-binding protein can bind to more than one specific sequence. For PstI, which has a single sequence specificity, it is 5'-CTGCAG-3'. It is always the same whether at the first recognition site or the second in the following example sequence. For Sp1, which has multiple (16) sequence specificity as shown above, the two recognition sites in the following example sequence fragment are at 18 and 32, and their respective recognition sequences are 5'-GGGGCGGAGC-3' and 5'-TGGGCGGAAC-3'.\n\n5'-AACGTTAGCTGCAGTC\"GGGGCGGAGC\"TAGGCTGCAGGAAT\"TGGGCGGAAC\"CT-3'\n\n", "id": "6754094", "title": "Recognition sequence"}
{"url": "https://en.wikipedia.org/wiki?curid=630611", "text": "Suicide gene\n\nA suicide gene, in genetics, will cause a cell to kill itself through apoptosis. Activation of these genes can be due to many processes, but the main cellular \"switch\" to induce apoptosis is the p53 protein. Stimulation or introduction (through gene therapy) of suicide genes is a potential way of treating cancer or other proliferative diseases. Suicide genes form the basis of a strategy for making cancer cells more vulnerable, more sensitive to chemotherapy. The approach has been to attach parts of genes expressed in cancer cells to other genes for enzymes not found in mammals that can convert a harmless substance into one that is toxic to the tumor. Most suicide genes mediate this sensitivity by coding for viral or bacterial enzymes that convert an inactive drug into toxic antimetabolites that inhibit the synthesis of nucleic acid. Suicide genes must be introduced into the cells in ways that ensure their uptake and expression by as many cancer cells as possible, while limiting their expression by normal cells. Suicide gene therapy for cancer requires the vector to have the capacity to discriminate between target and non target cells, between the cancer cells and normal cells.\n\nThe ultimate goal of cancer therapy is the complete elimination of all cancer cells, while leaving all healthy cells unharmed. One of the most promising therapeutic strategies in this regard is cancer suicide gene therapy (CSGT), which is rapidly progressing into new frontiers.The therapeutic success, in CSGT, is primarily contingent upon precision in delivery of the therapeutic transgenes to the cancer cells only. This is addressed by discovering and targeting unique or / and over-expressed biomarkers displayed on the cancer cells and cancer stem cells. Specificity of cancer therapeutic effects is further enhanced by designing the DNA constructs, which put the therapeutic genes under the control of the cancer cell specific promoters. The delivery of the suicidal genes to the cancer cells involves viral, as well as synthetic vectors, which are guided by cancer specific antibodies and ligands. The delivery options also include engineered stem cells with tropisms towards cancers. Main mechanisms inducing cancer cells' deaths include: transgenic expression of thymidine kinases, cytosine deaminases, intracellular antibodies, telomeraseses, caspases, DNases. Precautions are undertaken to eliminate the risks associated with transgenesis. Progress in genomics and proteomics should help us in identifying the cancer specific biomarkers and metabolic pathways for developing new strategies towards clinical trials of targeted and personalized gene therapy of cancer.By introducing the gene into a malignant tumor, the tumor would reduce in size and possibly disappear completely, provided all the individual cells have received a copy of the gene.\n\nWhen the DNA sample in the virus is taken from the patient's own healthy cells, the virus does not need to be able to differentiate between cancer cells and healthy ones. In addition, the advantage is that it is also able to prevent metastasis upon the death of a tumor.\n\nSuicide genes are often utilized in biotechnology to assist in molecular cloning. Vectors incorporate suicide genes for an organism (such as E. coli). The cloning project focuses on replacing the suicide gene by the desired fragment. Selection of vectors carrying the desired fragment is improved since vectors retaining the suicide gene result in cell death.\n\nThere are two ways that a cell can die: necrosis and apoptosis. Necrosis occurs when a cell is damaged by an external force, such as poison, a bodily injury, an infection or getting cut off from blood supply. When cells die from necrosis, it's a rather messy affair. The death causes inflammation that can cause further distress of injury within the body. Apoptosis, on the other hand, is relatively civil. Many cells undergo programmed cell death, or apoptosis, during fetal development. A form of cell death in which a programmed sequence of events leads to the elimination of cells without releasing harmful substances into the surrounding. Apoptosis plays a crucial role in developing and maintaining the health of the body by eliminating old cells, unnecessary cells, and unhealthy cells. The human body replaces perhaps one million cells per second. When a cell is compelled to commit suicide, proteins called caspases go into action. They break down the cellular components needed for survival, and they spur production of enzymes known as DNase, which destroy the DNA in the nucleus of the cell. The cell shrinks and sends out distress signals, which are answered by macrophages. The macrophages clean away the shrunken cells, leaving no trace, so these cells have no chance to cause the damage that necrotic cells do. Apoptosis also differs from necrosis in that it's essential to human development. For example, in the womb, our fingers and toes are connected to one another by a sort of webbing. Apoptosis is what causes that webbing to disappear, leaving us with 10 separate digits. As our brains develop, the body, the body creates millions more cells than it needs; the ones that don't form synaptic connections undergo apoptosis so that the remaining cells function well. Programmed cell death is also necessary to start the process of menstruation. That's not to say that apoptosis is a perfect process. Rather than dying due to injury, cells that go through apoptosis die in response to signals within the body. When cells recognize viruses and gene mutations, they may induce death to prevent the damage from spreading. Scientist are trying to learn how they can modulate apoptosis, so that they can control which cells live and which undergo programmed cell death. Anti-cancer drugs and radiation, for example, work by triggering apoptosis in diseased cells. Many diseases and disorders are linked with the life and death of cells—increased apoptosis is a characteristic of AIDS, Alzheimer's, and Parkinson's disease, while decreased apoptosis can signal lupus or cancer. Understanding how to regulate apoptosis could be the first step to treating these conditions.\n\nToo little or too much apoptosis can play a role in many diseases. When apoptosis does not work correctly, cells that should be eliminated may persist and become immortal, for example, in cancer and leukemia. when apoptosis works overly well, it kills too many cells and inflicts grave tissue damage. This is the case in strokes and neurodegenerative disorders such as Alzheimer's, Huntington's, and Parkinson's disease. Also known as programmed cell death and cell suicide.\n\nOne of the challenges of cancer treatment is how to destroy malignant tumors without damaging healthy cells. A new method that shows great promise for accomplishing this employs the use of a suicide gene. A suicide gene is a gene which will cause a cell to kill itself through apoptosis. There are currently two methods of suicide gene therapy being used. Gene-directed enzyme-producing therapy (GDEPT) uses a gene taken from the cancer cell and then modified with other genes to form enzymes that are harmless to healthy cells. This foreign enzyme is inserted into the tumor cells where it releases a prodrug, which is a small molecule harmless to healthy cells, but destructive to cancerous cells. The modified suicide gene converts the non-toxic prodrug into a cytotoxic substance. The second method of suicide gene therapy is called virus-directed enzyme-prodrug therapy. This uses a virus, such as herpes simplex or cold virus, as the carrier, or vector, to deliver the modified genes to the cancer cells. Suicide gene therapy is not necessarily expected to completely eliminate the need for chemotherapy and radiation treatment for all cancerous tumors. The damage inflicted upon the tumor cells, however, makes them more susceptible to the chemo or radiation. This approach has already proven effective against prostate and bladder cancers. The application of suicide gene therapy is being expanded to several other forms of cancer as well. Cancer patients often experience depressed immune systems, so they can suffer some side effects of the use of a virus as a delivery agent.\n", "id": "630611", "title": "Suicide gene"}
{"url": "https://en.wikipedia.org/wiki?curid=2008283", "text": "Expression cassette\n\nAn expression cassette is a distinct component of vector DNA consisting of a gene and regulatory sequence to be expressed by a transfected cell. In each successful transformation, the expression cassette directs the cell's machinery to make RNA and protein(s). Some expression cassettes are designed for modular cloning of protein-encoding sequences so that the same cassette can easily be altered to make different proteins.\n\nAn expression cassette is composed of one or more genes and the sequences controlling their expression. An expression cassette comprises three components: a promoter sequence, an open reading frame, and a 3' untranslated region that, in eukaryotes, usually contains a polyadenylation site.\n\nDifferent expression cassettes can be transfected into different organisms including bacteria, yeast, plants, and mammalian cells as long as the correct regulatory sequences are used.\n\n", "id": "2008283", "title": "Expression cassette"}
{"url": "https://en.wikipedia.org/wiki?curid=1793441", "text": "Tandem repeat locus\n\nVariable number of tandem repeat locus (VNTR locus) is any DNA sequence that exist in multiple copies strung together in a variety of tandem lengths. The number of repeat copies present at a locus can be visualized by means of a Multi-locus or Multiple Loci VNTR Analysis (MLVA). In short, oligonucleotide primers are developed for each specific tandem repeat locus, followed by PCR and agarose gel electrophoresis. When the length of the repeat and the size of the flanking regions is known, the number of repeats can be calculated. Analysis of multiple loci will result in a genotype.\n", "id": "1793441", "title": "Tandem repeat locus"}
{"url": "https://en.wikipedia.org/wiki?curid=23994173", "text": "Ambidirectional dominance\n\nAmbidirectional dominance occurs in a situation where multiple genes influence a phenotype and dominance is in different directions depending on the gene. The opposite situation, where all genes show dominance in the same direction, is called directional dominance. According to Broadhurst, ambidirectional dominance is the result of stabilising selection in the evolutionary past. Ambidirectional dominance has been found for exploratory behaviours in mice and Paradise fish.\n", "id": "23994173", "title": "Ambidirectional dominance"}
{"url": "https://en.wikipedia.org/wiki?curid=25966810", "text": "Upstream activating sequence\n\nAn upstream activating sequence or upstream activation sequence (UAS) is a cis-acting regulatory sequence. It is distinct from the promoter and increases the expression of a neighbouring gene. Due to its essential role in activating transcription, the upstream activating sequence is often considered to be analogous to the function of the enhancer in multicellular eukaryotes. Upstream activation sequences are a crucial part of induction, enhancing the expression of the protein of interest through increased transcriptional activity. The upstream activation sequence is found adjacently upstream to a minimal promoter (TATA box) and serves as a binding site for transactivators. If the transcriptional transactivator does not bind to the UAS in the proper orientation then transcription cannot begin. To further understand the function of an upstream activation sequence, it is beneficial to see its role in the cascade of events that lead to transcription activation. The pathway begins when activators bind to their target at the UAS recruiting a mediator. A TATA-binding protein subunit of a transcription factor then binds to the TATA box, recruiting additional transcription factors. The mediator then recruits RNA polymerase II to the pre-initiation complex. Once initiated, RNA polymerase II is released from the complex and transcription begins.\n\nThe property of the \"GAL1-GAL10\" to bind the \"GAL4\" protein is utilised in the GAL4/UAS technique for controlled gene mis-expression in Drosophila. This is the most popular form of binary expression in \"Drosophila melanogaster\", a system which has been adapted for many uses to make \"Drosophila melanogaster\" one of the most genetically tractable multicellular organisms. In this technique, four related binding sites between the \"GAL10\" and \"GAL1\" loci in \"Saccharomyces cerevisiae\" serve as an Upstream Activating Sequences (UAS) element through \"GAL4\" binding. Several studies have been conducted with \"Saccharomyces cerevisiae\" to explore the exact function of upstream activation sequences, often focusing on the aforementioned \"GAL1-GAL10\" intergenic region .\n\nOne study explored the galactose-responsive upstream activation sequence (UAS), looking at the influence of proximity to this UAS for nucleosome positioning. Proximity to the UAS was chosen because deletions of DNA flanking the UAS left the nucleosome array unaltered, indicating that nucleosome positioning was not related to sequence-specific histone-DNA interactions. The role of specific regions of UAS was analyzed by inserting oligonucleotides with different binding properties, leading to the successful identification of a region responsible for the creation of an ordered array. The sequence identified overlapped a binding site for \"GAL4\" protein, which is a positive regulator for transcription which coincides with the function of upstream activating sequences.\n\nAnother study looked at the effect of inserting the UAS into the promoter region of the glyceraldehyde-3-phosphate dehydrogenase gene (GPD) . This hybrid promoter was then utilized to express human immune interferon, a toxic substance to yeast that results in a reduced copy number and low plasmid stability. Relative to the native promoter, expression of the hybrid promoter was induced roughly 150- to 200-fold in the cultures by growth in galactose, induction that wasn't apparent with glucose as the carbon source. When compared to the native GPD promoter, the presence of UAS caused the transcriptional activity to remain equivalently enhanced under induced conditions.\n\nThe inositol-sensitive upstream activation sequence (UAS) has a consensus sequence 5'-CATGTGAAAT-3' and is present in the promoter regions of genes that encode enzymes of phospholipid biosynthesis. These enzymes are regulated by inositol and choline, both of which are phospholipid precursors. Within this consensus sequence, the first six bases are homologous with canonical binding motif for proteins within the bHLH or the basic helix-loop-helix family. Studies have shown that Ino2p and Ino4p, two bHLH regulatory proteins from \"Saccharomyces cerevisiae\", bind to promoter fragments containing this element of the consensus sequence. Additional studies have been designed to explore the function of UAS in more detail largely in part because a large number of phospholipid biosynthetic enzyme activities in the model organism \"Saccharomyces cerevisiae\" show this common pattern of expression.\n\nOne study explored the interaction between Ino4p and Ino2p in more depth, examining the dimerization that takes place between the two prior to binding to the promoter of the \"INO\"1 gene and activating transcription. By isolating 31 recessive suppressors of the \"ino\"4-8 mutant of yeast and determining that 29 were of the same locus, the researchers identified the locus as \"REG1\" . One allele of \"REG1\", the suppressor mutant \"sia1-1\", was capable of suppressing the inositol auxotrophy, revealing a possible pathway for the repression of inositol-sensitive upstream activating sequence-containing genes of yeast.\n", "id": "25966810", "title": "Upstream activating sequence"}
{"url": "https://en.wikipedia.org/wiki?curid=1617618", "text": "Blending inheritance\n\nBlending inheritance is a 19th century concept implied by some contemporary theories of evolution, implying that the inheritance of any characteristic would operate by averaging out the parents' values of that characteristic. This would mean that crossing a red flower variety with a white variety of the same species would yield pink-flowered offspring.\n\nCharles Darwin's theory of inheritance by pangenesis, with contributions to egg or sperm from every part of the body, implied blending inheritance. His reliance on this mechanism led Fleeming Jenkin to attack natural selection on the grounds that blending inheritance would average out any novel beneficial characteristic before selection had time to act.\n\nBlending inheritance was discarded with the general acceptance of particulate Mendelian inheritance during the development of modern genetics after 1900.\n\nCharles Darwin developed his theory of evolution by natural selection on the basis of an understanding of uniform processes in geology, acting over very long periods of time on inheritable variation within populations. One of those processes was competition for resources, as Thomas Malthus had indicated, leading to a struggle to survive and to reproduce. Since some individuals would by chance have traits that allowed them to leave more offspring, those traits would tend to increase in the population. Darwin assembled many lines of evidence to show that variation occurred and that artificial selection by animal and plant breeding had caused change. All of this demanded a reliable mechanism of inheritance.\n\nPangenesis was Darwin's attempt to provide such a mechanism of inheritance. The idea was that each part of the parent's body emitted tiny particles called gemmules, which migrated through the body to contribute to that parent's gametes, their eggs or sperms. The theory had an intuitive appeal, as characteristics of all parts of the body, such as shape of nose, width of shoulders and length of legs are inherited from both the father and the mother. However, it had some serious weaknesses. Firstly, many characteristics can change during an individual's lifetime, and are affected by the environment: blacksmiths can develop strong arm muscles during their work, so the gemmules from these muscles ought to carry this acquired characteristic. That implies the Lamarckian inheritance of acquired characteristics. Secondly, the fact that the gemmules were supposed to mix together on fertilisation implies blending inheritance, namely that the offspring wwould all be intermediate between the father and the mother in every characteristic. That directly contradicts the observed facts of inheritance, not least that children are usually either male or female rather than all intersex, and that traits such as flower colour often re-emerge after a generation, even when they seem to disappear when two varieties are crossed. Darwin was aware of both these objections, and accordingly had strong doubts about blending inheritance, as evidenced in his private correspondence. In a letter to T.H. Huxley, dated November 12, 1857, Darwin wrote:\n\nIn a letter to Alfred Wallace, dated February 6, 1866, Darwin mentioned conducting hybridization experiments with pea plants, not unlike those done by Gregor Mendel, and like him obtaining segregating (unblended) varieties, effectively disproving his theory of pangenesis with blending:\n\nBlending inheritance was also clearly incompatible with Darwin's theory of evolution by natural selection. The engineer Fleeming Jenkin used this to attack natural selection in his 1867 review of Darwin's \"On the Origin of Species\". Jenkin noted, correctly, that if inheritance were by blending, any beneficial trait that might arise in a lineage would have \"blended away\" long before natural selection had time to act. The evolutionary biologist Richard Dawkins commented that \"blending inheritance is incompatible...with obvious fact. If it were really true that variation disappeared, every generation should be more uniform than the previous one. By now, all individuals should be as indistinguishable as clones\", and that Darwin should have said as much to Jenkin. The problem was not with natural selection, but with blending, and in Dawkins's view, Darwin should have settled for saying that the mechanism of inheritance was unknown, but certainly non-blending.\n\nBlending inheritance was dismissed by the eventual widespread acceptance, after his death, of Gregor Mendel's theory of particulate inheritance, which he had presented in \"Experiments on Plant Hybridization\" (1865). In 1892, August Weismann set out the idea of a hereditary material, which he called the germ plasm, confined to the gonads and independent of the rest of the body (the soma). In Weismann's view, the germ plasm formed the body, but the body did not influence the germ plasm, except indirectly by natural selection. This contradicted both Darwin's pangenesis and Lamarckian inheritance. Mendel's work was rediscovered in 1900 by the geneticist Hugo de Vries and others, soon confirmed that same year by experiments by William Bateson. Mendelian inheritance with segregating, particulate alleles came to be understood as the explanation for both discrete and continuously varying characteristics.\n\n\n", "id": "1617618", "title": "Blending inheritance"}
{"url": "https://en.wikipedia.org/wiki?curid=26385783", "text": "SequenceVariantAnalyzer\n\nSequenceVariantAnalyzer (SVA) is a computer program for annotating and analyzing genetic variants called (identified) from a whole genome or exome sequencing study (Shotgun sequencing).\n\nDNA sequence information underpins genetic research, enabling discoversies of important biological or medical benefit. Compared with previous discovery strategies, a whole-genome sequencing study is no longer constrained by differing patterns of linkage disequilibrium, thus, in theory, is more possible to directly identify the genetic variants contributing to biological traits or medical outcomes.\n\nThe rapidly evolving high-throughput DNA sequencing technologies have now allowed the fast generation of large amount of sequence data for the purpose of performing such whole-genome sequencing studies, at a reasonable cost. SequenceVariantAnalyzer, or SVA, is a software tool to analyze the genetic variants identified from such studies.\n\nSVA is designed for two specific aims:\n\n(1) To annotate the biological functions of the identified genetic variants and group them, conveniently;\n\n(2) To find the genetic variants that are associated with or responsible for the biological traits or medical outcomes of interest.\n\nSVA is developed on the Java platform.\n\nSVA is developed and maintained by Dr. Dongliang Ge and Dr. David B. Goldstein at Duke University, Center for Human Genome Variation.\n\n", "id": "26385783", "title": "SequenceVariantAnalyzer"}
{"url": "https://en.wikipedia.org/wiki?curid=26186138", "text": "National Society of Genetic Counselors\n\nThe National Society of Genetic Counselors (NSGC), founded and incorporated in 1979, is the largest association of genetic counselors in the world. Its membership includes genetic counselors and other health care professionals working in the field of medical genetics from the United States, Canada, and around the world. \n\nThe NSGC's stated Vision is to be the leading voice, authority and advocate for the genetic counseling profession. Its stated Mission is to advance the various roles of genetic counselors in health care by fostering education, research, and public policy to ensure the availability of quality genetic services. \n\nA similar association, the Canadian Association of Genetic Counselors, exists for those genetic counselors who are currently practicing in Canada. Counselors may belong to one or both of the CAGC and NSGC.\n\nThe 2010 NSGC Board of Directors consists of the President, President-Elect, Secretary/Treasurer, Secretary/Treasurer-Elect, Immediate Past President, Executive Director, and 7 Directors At Large. \n\nOther leadership roles within the NSGC include the Chair and Vice-Chair positions for the following committees:\n\nOther groups within the NSGC include the Ethics Advisory Group, the Jane Engelberg Memorial Fellowship (JEMF) Advisory Group, and the Audrey Heimler Special Projects Award Committee.\n\nDue to the rapid evolution of the field of medical genetics, with respect to both knowledge and available technology, a commitment to lifelong learning has always been essential to, and valued by, genetic counselors. A primary goal of the NSGC has therefore been to support continuing education for genetic counselors through the provision of Continuing Education Units (CEUs). According to the NSGC CEU Guidelines, one CEU is earned for ten contact hours of participation in organized continuing education/training experience under responsible, qualified direction and instruction. Genetic counselors can earn CEUs in a number of ways, including attending and participating in the NSGC Annual Education Conference or completing reading assignments from the Journal of Genetic Counseling.\n\nThe NSGC produces two publications. The Journal of Genetic Counseling, a peer-reviewed journal published bimonthly, is the official journal of the NSGC. The \"Journal of Genetic Counseling\" welcomes original research, essays, review articles, and letters from practitioners, researchers, instructors, and students. \"Perspectives in Genetic Counseling\", a quarterly newsletter, welcomes submissions from genetic counselors and students. Perspectives aims to highlight current NSGC news, including actions of the Board of Directors, new legislation relevant to medical genetics, activities of counselors and NSGC committees, and current resources for counselors.\n\nThe NSGC Annual Education Conference is held each fall in a U.S. city. In 2009, the meeting was held in Atlanta, GA. The 2010 meeting was held in Dallas, Texas. \n\nThe NSGC also holds Regional Meetings. In 2009, these were held in Washington, D.C., and Chicago, Illinois. \n\nNSGC meetings are designed to provide an opportunity for genetic counselors and related medical professionals to learn about advances in genetics, network with other genetics professionals, and obtain CEU credits.\n\n\n", "id": "26186138", "title": "National Society of Genetic Counselors"}
{"url": "https://en.wikipedia.org/wiki?curid=24861869", "text": "Lineage (genetic)\n\nA genetic lineage is a series of mutations which connect an ancestral genetic type (allele, haplotype, or haplogroup) to derivative type. In cases where the genetic tree is very bushy the order of mutations in the lineage is mostly known, examples are the order of mutations between E1b1b and E1b1b1a1a for the human Y-chromosomesal L0 or L1 nodes.\n\nA genetic lineage can be contrasted with an evolutionary lineage in that a genetic lineage applies to a locus. An example of the difference is that ancient African ape evolved into proto-gorilla and a chimpanzee-human ancestor, which further evolved into chimps and humans. While most human lineages coalesce with chimpanzee lineages, which then converge with gorilla lineages, a few human lineages coalesce with gorilla lineages and then converge with chimpanzee lineages (or chimpanzee lineages that coalesce with gorilla lineages and then converge with human lineages). This occurs because the speciation that marks evolutionary lineages occur as non-discrete events that are composed of 10s to 10000s of individuals in each developing taxa. This allows many variant lineages to be passed millions of years (See 2N-rule; 2 * 20 year/generations * 10,000 inds * ploidy) allows multiple deeply rooted lineages to be passed millions of years, over 2 or more speciation events. Such lineages may randomly undergo fixation at any time.\n\nIn genetics a basal lineage is a genetic lineage that connects a variant allele (type) possessed by a more common ancestor that evolves into two descendant variants possessed by a branch ancestor. An example of a basal lineage is the lineage between mitochondrial 'Eve' and L0 or L1. Basal lineages may have types that are no longer represented in the extant population, only being defined by derivative types such as CRS for L1.\n\nPeripheral lineage (Also: surface lineage) are lineages in which interconnect an extant type to a branch ancestor.\n", "id": "24861869", "title": "Lineage (genetic)"}
{"url": "https://en.wikipedia.org/wiki?curid=12229900", "text": "Genetic exceptionalism\n\nGenetic exceptionalism is the belief that genetic information is special and so must be treated differently from other types of medical information. \n\nFor example, patients are able to obtain information about their blood pressure without involving any medical professionals, but to obtain information about their genetic profile might require an order from a physician and expensive counseling sessions. Disclosure of an individual's genetic information or its meaning, such as telling a woman with red hair that she has a higher risk of skin cancer, has been legally restricted in some places, as providing medical advice.\n\nThat policy approach has been taken by state legislatures to safeguard individuals' genetic information in the United States from individuals, their families, their employers, and the government. The approach builds upon the existing protection required of general health information provided by such laws as the Health Insurance Portability and Accountability Act.\n\n\n", "id": "12229900", "title": "Genetic exceptionalism"}
{"url": "https://en.wikipedia.org/wiki?curid=4964171", "text": "Satellite chromosome\n\nBesides the centromere, one or more secondary constrictions can also be observed in some chromosomes at metaphase. The secondary constrictions are always constant in their positions and hence can be used as markers that identify particular chromosomes. A satellite chromosome or SAT chromosome has a chromosome segment that is separated from the main body of the chromosome by such a secondary constriction. The term is due to Sergei Navashin, in 1912.\n\nThe satellite at metaphase appears to be attached to the rest of the body of chromosomes by a thread of chromatin.\n\nThere are at least 2 SAT chromosomes in each diploid nucleus, and the constriction corresponds to a nucleolar organizer (NOR). The appearance of secondary constrictions at NORs is thought to be due to rRNA transcription and/or structural features of the nucleolus impeding chromosome condensation.\n\nIn humans, the chromosomes number 13, 14, 15, 21 and 22 are examples of SAT chromosomes.\n", "id": "4964171", "title": "Satellite chromosome"}
{"url": "https://en.wikipedia.org/wiki?curid=22095859", "text": "Exome\n\nThe exome is the part of the genome formed by exons, the sequences which when transcribed remain within the mature RNA after introns are removed by RNA splicing. It consists of all DNA that is transcribed into mature RNA in cells of any type as distinct from the transcriptome, which is the RNA that has been transcribed only in a specific cell population. The exome of the human genome consists of roughly 180,000 exons constituting about 1% of the total genome, or about 30 megabases of DNA. Though comprising a very small fraction of the genome, mutations in the exome are thought to harbor 85% of mutations that have a large effect on disease. Exome sequencing has proved to be an efficient strategy to determine the genetic basis of more than two dozen Mendelian or single gene disorders.\n\n", "id": "22095859", "title": "Exome"}
{"url": "https://en.wikipedia.org/wiki?curid=26810695", "text": "Coefficient of coincidence\n\nIn genetics, the coefficient of coincidence (c.o.c.) is a measure of interference in the formation of chromosomal crossovers during meiosis. It is generally the case that, if there is a crossover at one spot on a chromosome, this decreases the likelihood of a crossover in a nearby spot. This is called interference.\n\nThe coefficient of coincidence is typically calculated from recombination rates between three genes. If there are three genes in the order A B C, then we can determine how closely linked they are by frequency of recombination. Knowing the recombination rate between A and B and the recombination rate between B and C, we would naively expect the double recombination rate to be the product of these two rates.\n\nThe coefficient of coincidence is calculated by dividing the \"actual\" frequency of double recombinants by this expected frequency:\n\nInterference is then defined as follows:\n\nThis figure tells us how strongly a crossover in one of the DNA regions (AB or BC) interferes with the formation of a crossover in the other region.\n\nDrosophila females of genotype \"aa bb cc\" were crossed with males of genotype \"aa bb cc\". This led to 1000 progeny of the following phenotypes:\n\nFrom these numbers it is clear that the b/b locus lies between the a/a locus and the c/c locus.\n\nThere are 23 + 152 + 148 + 27 = 350 progeny showing recombination between genes A and B. And there are 81 + 23 + 27 + 89 = 220 progeny showing recombination between genes B and C. Thus the expected rate of double recombination is (350 / 1000) * (220 / 1000) = 0.077, or 77 per 1000.\n\nHowever, there are actually only 23 + 27 = 50 double recombinants. The coefficient of coincidence is therefore 50 / 77 = 0.65.\n\nInterference is 1 − 0.65 = 0.35.\n", "id": "26810695", "title": "Coefficient of coincidence"}
{"url": "https://en.wikipedia.org/wiki?curid=11406702", "text": "Marker-assisted selection\n\nMarker assisted selection or marker aided selection (MAS) is an indirect selection process where a trait of interest is selected based on a marker (morphological, biochemical or DNA/RNA variation) linked to a trait of interest (e.g. productivity, disease resistance, abiotic stress tolerance, and quality), rather than on the trait itself. This process has been extensively researched and proposed for plant and animal breeding, nevertheless, as of 2012, \"breeding programs based on DNA markers for improving quantitative traits in plants are rare\".\n\nFor example, using MAS to select individuals with disease resistance involves identifying a marker allele that is linked with disease resistance rather than the level of disease resistance. The assumption is that the marker associates at high frequency with the gene or quantitative trait locus (QTL) of interest, due to genetic linkage (close proximity, on the chromosome, of the marker locus and the disease resistance-determining locus). MAS can be useful to select for traits that are difficult or expensive to measure, exhibit low heritability and/or are expressed late in development. At certain points in the breeding process the specimens are examined to ensure that they express the desired trait.\n\nThe majority of MAS work in the present era uses DNA-based markers. However, the first markers that allowed indirect selection of a trait of interest were morphological markers. In 1923, Sax first reported association of a simply inherited genetic marker with a quantitative trait in plants when he observed segregation of seed size associated with segregation for a seed coat color marker in beans (\"Phaseolus vulgaris\" L. ). In 1935, Rasmusson demonstrated linkage of flowering time (a quantitative trait) in peas with a simply inherited gene for flower color.\n\nMarkers may be:\n\n\nThe following terms are generally less relevant to discussions of MAS in plant and animal breeding, but are highly relevant in molecular biology research:\n\nA distinction can be made between selectable markers (which eliminate certain genotypes from the population) and screenable markers (which cause certain genotypes to be readily identifiable, at which point the experimenter must \"score\" or evaluate the population and act to retain the preferred genotypes). Most MAS uses screenable markers rather than selectable markers.\n\nThe gene of interest directly causes production of protein(s) or RNA that produce a desired trait or phenotype, whereas markers (a DNA sequence or the morphological or biochemical markers produced due to that DNA) are genetically linked to the gene of interest. The gene of interest and the marker tend to move together during segregation of gametes due to their proximity on the same chromosome and concomitant reduction in recombination (chromosome crossover events) between the marker and gene of interest. For some traits, the gene of interest has been discovered and the presence of desirable alleles can be directly assayed with a high level of confidence. However, if the gene of interest is not known, markers linked to the gene of interest can still be used to select for individuals with desirable alleles of the gene of interest. When markers are used there may be some inaccurate results due to inaccurate tests for the marker. There also can be false positive results when markers are used, due to recombination between the marker of interest and gene (or QTL). A perfect marker would elicit no false positive results. The term 'perfect marker' is sometimes used when tests are performed to detect a SNP or other DNA polymorphism in the gene of interest, if that SNP or other polymorphism is the direct cause of the trait of interest. The term 'marker' is still appropriate to use when directly assaying the gene of interest, because the test of genotype is an indirect test of the trait or phenotype of interest.\n\nAn ideal marker:\n\n\nMorphological markers are associated with several general deficits that reduce their usefulness including:\n\n\nTo avoid problems specific to morphological markers, DNA-based markers have been developed. They are highly polymorphic, exhibit simple inheritance (often codominant), are abundant throughout the genome, are easy and fast to detect, exhibit minimum pleiotropic effects, and detection is not dependent on the developmental stage of the organism. Numerous markers have been mapped to different chromosomes in several crops including rice, wheat, maize, soybean and several others, and in livestock such as cattle, pigs and chickens. Those markers have been used in diversity analysis, parentage detection, DNA fingerprinting, and prediction of hybrid performance. Molecular markers are useful in indirect selection processes, enabling manual selection of individuals for further propagation. \n\n'Major genes' that are responsible for economically important characteristics are frequent in the plant kingdom. Such characteristics include disease resistance, male sterility, self-incompatibility, and others related to shape, color, and architecture of whole plants and are often of mono- or oligogenic in nature. The marker loci that are tightly linked to major genes can be used for selection and are sometimes more efficient than direct selection for the target gene. Such advantages in efficiency may be due for example, to higher expression of the marker mRNA in such cases that the marker is itself a gene. Alternatively, in such cases that the target gene of interest differs between two alleles by a difficult-to-detect single nucleotide polymorphism, an external marker (be it another gene or a polymorphism that is easier to detect, such as a short tandem repeat) may present as the most realistic option.\n\nThere are several indications for the use of molecular markers in the selection of a genetic trait.\n\nIn such situations that:\n\n\nThe cost of genotyping (an example of a molecular marker assay) is reducing while the cost of phenotyping is increasing particularly in developed countries thus increasing the attractiveness of MAS as the development of the technology continues.\n\nGenerally the first step is to map the gene or quantitative trait locus (QTL) of interest first by using different techniques and then using this information for marker assisted selection. Generally, the markers to be used should be close to gene of interest (<5 recombination unit or cM) in order to ensure that only minor fraction of the selected individuals will be recombinants. Generally, not only a single marker but rather two markers are used in order to reduce the chances of an error due to homologous recombination. For example, if two flanking markers are used at same time with an interval between them of approximately 20cM, there is higher probability (99%) for recovery of the target gene.\n\nIn plants QTL mapping is generally achieved using bi-parental cross populations; a cross between two parents which have a contrasting phenotype for the trait of interest are developed. Commonly used populations are recombinant inbred lines (RILs), doubled haploids (DH), back cross and F. Linkage between the phenotype and markers which have already been mapped is tested in these populations in order to determine the position of the QTL. Such techniques are based on linkage and are therefore referred to as \"linkage mapping\".A\n\nIn contrast to two-step QTL mapping and MAS, a single-step method for breeding typical plant populations has been developed.\n\nIn such an approach, in the first few breeding cycles, markers linked to the trait of interest are identified by QTL mapping and later the same information is used in the same population. In this approach, pedigree structure is created from families that are created by crossing number of parents (in three-way or four way crosses). Both phenotyping and genotyping is done using molecular markers mapped the possible location of QTL of interest. This will identify markers and their favorable alleles. Once these favorable marker alleles are identified, the frequency of such alleles will be increased and response to marker assisted selection is estimated. Marker allele(s) with desirable effect will be further used in next selection cycle or other experiments.\n\nRecently high-throughput genotyping techniques are developed which allows marker aided screening of many genotypes. This will help breeders in shifting traditional breeding to marker aided selection. One example of such automation is using DNA isolation robots, capillary electrophoresis and pipetting robots.\n\nOne recent example of capllilary system is Applied Biosystems 3130 Genetic Analyzer. This is the latest generation of 4-capillary electrophoresis instruments for the low to medium throughput laboratories.\n\nA minimum of five or six-backcross generations are required to transfer a gene of interest from a donor (may not be adapted) to a recipient (recurrent – adapted cultivar). The recovery of the recurrent genotype can be accelerated with the use of molecular markers. If the F1 is heterozygous for the marker locus, individuals with the recurrent parent allele(s) at the marker locus in first or subsequent backcross generations will also carry a chromosome tagged by the marker.\n\nGene pyramiding has been proposed and applied to enhance resistance to disease and insects by selecting for two or more than two genes at a time. For example, in rice such pyramids have been developed against bacterial blight and blast. The advantage of use of markers in this case allows to select for QTL-allele-linked markers that have same phenotypic effect.\n\nMAS has also been proved useful for livestock improvement.\n\nA coordinated effort to implement wheat (\"Triticum turgidum\" and \"Triticum aestivum)\" marker assisted selection in the U.S. as well as a resource for marker assisted selection exists at the Wheat CAP (Coordinated Agricultural Project) website.\nFarhad Kahani\n\n\n\n", "id": "11406702", "title": "Marker-assisted selection"}
{"url": "https://en.wikipedia.org/wiki?curid=26385653", "text": "Ectoderm specification\n\nIn \"Xenopus laevis\", the specification of the three germ layers (endoderm, mesoderm and ectoderm) occurs at the blastula stage. Great efforts have been made to determine the factors that specify the endoderm and mesoderm. On the other hand, only a few examples of genes that are required for ectoderm specification have been described in the last decade. The first molecule identified to be required for the specification of ectoderm was the ubiquitin ligase Ectodermin (Ecto, TIF1-γ, TRIM33); later, it was found that the deubiquitinating enzyme, FAM/USP9x, is able to overcome the effects of ubiquitination made by Ectodermin in Smad4 (Dupont et al., 2009). Two transcription factors have been proposed to control gene expression of ectodermal specific genes: POU91/Oct3/4 and FoxIe1/Xema. A new factor specific for the ectoderm, XFDL156, has shown to be essential for suppression of mesoderm differentiation from pluripotent cells.\n\nThe protein Ectodermin, firstly identified in \"Xenopus\" embryos, promotes ectodermal fate and suppresses the mesoderm formation mediated by the signaling of Transforming Growth Factor β (TGFβ) and Bone Morphogenic Proteins (BMP), members of the TGFβ-superfamily. When the TGFβ ligands bind to TGFβ receptors, they cause the activation of the signal transducers R-Smads (Smad2, Smad3). Smad4 forms a complex with activated R-Smads and activates transcription of specific genes in response to TGFβ signal. The BMP pathway transmits its signals in a similar way but through other types of R-Smads (Smad1, Smad5 and Smad8). The transcription factor Smad4 is the only common mediator shared between both TGFβ and the BMP pathways. During ectoderm specification, the function of Smad4 is regulated by ubiquitination and deubiquitination made by Ectodermin and FAM (acronym of Fat acets in mammals) respectively. The ubiquitination state of Smad4 will determine if it is able to respond to signals derived from TGFβ and BMP. The equilibrium of the activity, localization and timing of TGFβ and BMP transducers, Smad4, FAM and of Ectodermin should be achieved in order to be able to modulate the gene expression of genes required for germ layer formation.\n\nA cDNA library from the blastula stage of a frog embryo was cloned into RNA expression plasmids to generate synthetic mRNA. The mRNA was then injected into several \"Xenopus\" embryos at a four-cell stage and looked in early blastula embryos for an expansion of the region of the ectodermal marker Sox2 and diminution of the expression of the mesodermal marker Xbra. Ectodermin was one out of 50 clones to present this phenotype when injected into embryos. The identification of FAM was done through a siRNA screen to find deubiquitinases that regulate the response to TGFβ.\n\nEctodermin mRNA is maternally deposited in the animal pole of the egg. In the early blastula stage of the embryo, Ectodermin mRNA and protein forms a gradient that goes from the animal pole (highest concentration) down to the marginal zone (lowest concentration) to prevent TGFβ and nodal signals that induce mesoderm originating from the vegetal pole. Ectodermin mRNA is enriched in the dorsal side of the embryo, and at the end of this stage the expression gradually disappears. Smad4 is ubiquitinated by Ectodermin in the nucleus and exported to the cytoplasm where it can be deubiquitinated by FAM; this way Smad4 can be recycled and be functional again. Although there is no expression profile of FAM in early embryos in \"Xenopus\", in the zebra fish, FAM homolog is expressed ubiquitously at a two-cell stage but as development proceeds then its only expressed in the cephalic central nervous system.\n\nEctodermin is a ubiquitin E3 ligase that inhibits the TGFβ and the BMP signaling pathways by inhibition of Smad4 via ubiquitination of Lysine 519 and also though direct binding to phospho-Smad2. Injection of Ecto mRNA in the marginal zone leads to an expansion of the early ectodermal marker, Sox2, and a reduction of mesodermal markers (Xbra, Eomes, Vent-1, Mix-1 and Mixer). The opposite happens in knockdown experiments of Ectodermin by using a morpholino strategy; embryos become more sensitive to Activin response, they show an increase and expansion of the expression of mesodermal specific genes and down-regulate the expression of neural plate and epidermis marker (Sox2 and cytokeratin respectively). In line with a RING-finger dependent ubiquitin-ligase activity of Ectodermin, an Ecto RING-finger mutant (C97A/C100A) is inactive in gain-of-function. Gain-of-function of FAM increases the responses from BMP and TGFβ and its loss-of-function by mutation in a critical residue for its activity caused inhibition of TGFβ response.\n\nThe molecular function of human ectodermin to act as a negative regulator of Smad4 suggests that this specific function is conserved among the vertebrate lineage. The sequence identity between FAM homologs is higher than 90% when comparing the homologs of \"Xenopus\", zebrafish, mouse, and human, suggesting that this might also be conserved among other organisms. Indeed, knockout gene inactivation in mouse embryos showed that the function of ectodermin as inhibitor of TGF-beta signaling is conserved. Embryos lacking of ectodermin show defective development of the anterior visceral endoderm (AVE), which is the first tissue that is induced by TGF-beta signals in mouse embryos; in accordance with loss of an inhibitor, ectodermin-/- embryos showed enlarged AVE induction. As the AVE is a natural source of secreted TGF-beta antagonists, this primary AVE expansion caused secondarily, at later stages, an inhibition of extracellular TGF-beta ligands, resulting in embryos lacking of mesoderm development. This model was confirmed by the finding that ectodermin-/- embryos were rescued to wild type (normal AVE, normal mesoderm development) by lowering the genetic dosage of the main TGF-beta ligand of the emrbyo, Nodal. Further supporting a role as TGF-beta inhibitor, tissue-selective deletion of ectodermin from the epiblast (from which the mesoderm, but not the AVE, derive) left the AVE untouched but caused this time an expansion of anterior mesodermal fates, indicative of increased responsiveness to TGF-beta signals. Collectively, these data confirmed with genetic tools a cell-autonomous role for ectodermin as inhibitor of Smad4 responses previously identified in Xenopus embryos and human cell lines.\n\nDuring early in development in \"Xenopus\", the transcription factor FoxI1e/Xema activates epidermal differentiation and represses endoderm and mesoderm specific genes in animal caps (Suri et al., 2005). It is suggested that FoxI1e is active before the ectoderm differentiates into epidermis and the central nervous system.\n\nMir et al., 2005 identified FoxI1e (Xema) by selecting genes that were down-regulated under mesoderm-inducing signals in the ectoderm compared to vegetal region of an early blastula embryo. Also, high expression of this gene was observed in animal caps in embryos that lack VegT compared to wild type.\n\nFoxI1e mRNA is expressed zygotically (stage 8.5) and reaches higher level of expression early in gastrulation and maintains that level in neurula, tailbud until early tadpole stages. FoxI1e has a peculiar mosaic expression pattern, it is expressed first in the dorsal ectoderm and while gastrula progresses, the expression goes through the ventral side and its expression is down-regulated in the dorsal side when the neural plate is forming. FoxI1e is dependent on BMP signals in the neurula stage, limiting the localization of FoxI1e to the ventral side of the ectoderm.\n\nFoxI1e/Xema belongs to the FoxI1 class of fork head transcription factor family, known to participate in mesoderm formation, eye development and ventral head specification. It has been proposed that Notch and/or NODAL, expressed in the vegetal/mesoderm region of the early blastula embryo, could potentially be the inhibitors of FoxI1e.\n\nInhibition of FoxI1e mRNA maturation by a splice-blocking morpholino shows malformations in the development of epidermis and pervious system and down-regulates of ectoderm specific genes, whereas FoxI1e over-expression inhibits the formation of mesoderm and endoderm. Vegetal structures form late blastula masses that normally would give rise to endoderm and mesoderm, when injected with FoxI1e mRNA, they are able to express ectodermal specific markers (pan-ectodermal E-cadherin, epithelial cytokeratin, neural crest marker Slug and neural marker Sox-2) while endodermal markers (endodermin, Xsox17a) decreased in expression.\n\nThe p53 protein binds to the promoters of early mesodermal genes. p53 is maternally deposited transcript that forms a transcriptional factor complex with Smad2 and drives the expression of genes involved in mesoderm induction and activation of TGFβ target genes. The zinc (Zn)-finger nuclear protein XFDL159, expressed in the animal cap, acts as an ectoderm factor their specifies the ectoderm by inhibiting p53 from activating genes for mesoderm differentiation.\n\nConstruction of a cDNA library from animal caps at a stage of 11.5, cloned into expression vector and generated mRNA. The synthetic RNA was then injected into embryos and the animal caps of these collected embryos were obtained and submitted to activin treatment. Xbra was recovered by selecting the clone that represses the mesodermal marker Xbra.\n\nSince XFDL156 is a factor that interacts with p53, the localization of this protein is in the nucleus (Sasai et al., 2008). The mRNA of XFDL156 is maternally deposited and then expressed zygotically. The gene expression timeline shows a higher level of expression at early gastrula and a half decrease in expression at mid-gastrula and by the stage 20 the expression fades.\n\nXFDR zinc finger binds to the regulatory region of p53 located at the C-terminal domain and its expression is not affected by the presence of activin, FoxI1e or XLPOU91 transcription factors.\n\nLoss-of-function by morpholino, causes incorrect mesodermal differentiation in the ectoedermal regions; this is caused by the desuppression of mesodermal markers (Xbra, VegT and Mix.2). Gain-of-functions causes decrease in expression of mesodermal markers.\n\nThe human and mouse homologs of XFDR156 are able to complement XFDR function of interacting with p53 and inhibiting it to act as a transcription factor.\n", "id": "26385653", "title": "Ectoderm specification"}
{"url": "https://en.wikipedia.org/wiki?curid=24277379", "text": "Transcriptional bursting\n\nTranscriptional bursting, also known as transcriptional pulsing, is a fundamental property of genes in which transcription from DNA to RNA can occur in \"bursts\" or \"pulses\", which has been observed in diverse organisms, from bacteria to mammals. This phenomenon came to light with the advent of technologies, such as MS2 tagging and single molecule RNA fluorescence in situ hybridisation, to detect RNA production in single cells, through precise measurements of RNA number or RNA appearance at the gene. Other, more widespread techniques, such as Northern blotting, microarrays, RT-PCR and RNA-Seq, measure bulk RNA levels from homogenous population extracts. These techniques lose dynamic information from individual cells and give the impression that transcription is a continuous smooth process. Observed at an individual cell level, transcription is irregular, with strong periods of activity interspersed by long periods of inactivity.\n\nBursting may result from the stochastic nature of biochemical events superimposed upon a two step fluctuation. In its most simple form, the gene is proposed to exist in two states, one where activity is negligible and one where there is a certain probability of activation. Only in the second state does transcription readily occur. It seems likely that some rudimentary eukaryotes have genes which do not show bursting. The genes are always in the permissive state, with a simple probability describing the numbers of RNAs generated.\n\nMore recent data indicate the two state model can be an oversimplification. Transcription of the c-Fos gene in response to serum stimulation can, for the most part, be summarised by two states, although at certain times after stimulation, a third state better explains the variance in the data. Another model suggests a two state model can apply, but with each cell having a different transcription rate in the active state. Other analyses indicate a spectrum or continuum of activity states. The nuclear and signaling landscapes of complex eukaryotic nuclei may favour more than two simple states- for example, there are over several dozen post-translational modifications of nucleosomes and perhaps a hundred different proteins involved in the average eukaryotic transcription reaction.\n\nWhat do the repressive and permissive states represent? An attractive idea is that the repressed state is a closed chromatin conformation whilst the permissive states are more open. Another hypothesis is that the fluctuations between states reflect reversible transitions in the binding and dissociation of pre-initiation complexes. Bursts may also result from bursty signalling, cell cycle effects or movement of chromatin to and from transcription factories. Bursting dynamics have been demonstrated to be influenced by cell size and the frequency of extracellular signalling. Recent data suggest different degrees of supercoiling distinguish the permissive and inactive states.\n\nThe bursting phenomenon, as opposed to simple probabilistic models of transcription, can account for the high variability (see transcriptional noise) in gene expression occurring between cells in isogenic populations. This variability in turn can have tremendous consequences on cell behaviour, and must be mitigated or integrated. Suggested mechanisms by which noise can be dampened include strong extracellular signalling, diffusion of RNA and protein in cell syncitia, promoter proximal pausing, and nuclear retention of transcripts. In certain contexts, such as the survival of microbes in rapidly changing stressful environments, the expression variability may be essential. Variability also impacts upon the effectiveness of clinical treatment, with resistance of bacteria to antibiotics demonstrably caused by non-genetic differences. Similar phenomena may contribute to the resistance of sub-populations of cancer cells to chemotherapy. Spontaneous variability in gene expression is also proposed to act as a source of cell fate diversity in self-organizing differentiation processes, and may act as a barrier to effective cellular reprogramming strategies.\n", "id": "24277379", "title": "Transcriptional bursting"}
{"url": "https://en.wikipedia.org/wiki?curid=26796464", "text": "Överkalix study\n\nThe Överkalix study () was a study conducted on the physiological effects of various environmental factors on transgenerational epigenetic inheritance. The study was conducted utilizing historical records, including harvests and food prices, in Överkalix, a small isolated municipality in northeast Sweden. The study was of 303 probands, 164 men and 139 women, born in 1890, 1905, or 1920, and their 1,818 children and grandchildren. 44 were still alive in 1995 when mortality follow-up stopped. Mortality risk ratios (RR) on children and grandchildren were determined based on available food supply, as indicated by historical data.\n\nAmong the sex-specific effects noted; a greater body mass index (BMI) at 9 years in sons, but not daughters, of fathers who began smoking early. The paternal grandfather's food supply was only linked to the mortality RR of grandsons and not granddaughters. The paternal grandmother's food supply was only associated with the granddaughters' mortality risk ratio. When the grandmother had a good food supply was associated with a twofold higher mortality (RR). \nThis transgenerational inheritance was observed with exposure during the slow growth period (SGP). The SGP is the time before the start of puberty, when environmental factors have a larger impact on the body. The ancestors' SGP in this study, was set between the ages of 9-12 for boys and 8–10 years for girls. This occurred in the SGP of both grandparents, or during the gestation period/infant life of the grandmothers, but not during either grandparent's puberty. The father's poor food supply and the mother's good food supply were associated with a lower risk of cardiovascular death.\n\nOnly the female probands experienced a twofold higher mortality RR when the paternal grandmother had good food availability during her SGP, compared to the mortality risk of those whose paternal grandmothers had poor food supply during the SGP.\n\nUsing the same data, another investigation highlighted that a sharp change in food availability in paternal grandmothers' resulted in an increased risk of cardiovascular mortality in granddaughters adults' life. Such an effect was not observed in other grandparents. The grandparents were considered exposed if they experienced drastic change in their early life ranging from embryo to 13 years old.\n\nSex-specific effects can be due to parental imprinting a process that results in allele-specific differences in transcription, DNA methylation, and DNA replication timing. Imprinting is an important process in human development, and its deregulation can cause certain defined disease states of other imprinted human disease loci. The establishment of parental imprints occurs during gametogenesis as homologous DNA passes through sperm or egg; subsequently during embryogenesis and into adulthood, alleles of imprinted genes are maintained in two \"conformational\"/epigenetic states: paternal or maternal. Thus, genomic imprints template their own replication, are heritable, can be identified by molecular analysis, and serve as markers of the parental origin of genomic regions.\n\nThe estimation of percentage of human genes subject to parental imprinting is approximately one to two percent, currently parental imprinting has been identified in fewer than 100 distinct named genes.\n\n", "id": "26796464", "title": "Överkalix study"}
{"url": "https://en.wikipedia.org/wiki?curid=12205580", "text": "Gruber Prize in Genetics\n\nThe Gruber Prize in Genetics, established in 2001, is one of three international awards worth US$500,000 made by the Gruber Foundation, a non-profit organization based at Yale University in New Haven, Connecticut.\n\nThe Genetics Prize honors leading scientists for distinguished contributions in any realm of genetics research. The Foundation’s other international prizes are in Cosmology, Neuroscience, Justice, and Women’s Rights.\n\n\n", "id": "12205580", "title": "Gruber Prize in Genetics"}
{"url": "https://en.wikipedia.org/wiki?curid=26082169", "text": "Molecular Interaction Maps\n\nMolecular Interaction Maps, also known as MIMs, is a graphic notation to depict cellular and molecular interactions. It was created by Kurt W. Kohn in 1999. The MIM convention is capable of unambiguous representation of networks containing multi-protein complexes, protein modifications, and enzymes that are substrates of other enzymes. This graphical representation makes it possible to view all of the many interactions in which a given molecule may be involved, and it can portray competing interactions, which are common in bioregulatory networks. In order to facilitate linkage to databases, each molecular species is represented only once in a diagram. The MIM notation forms the basis of, and further development of the MIM notation is coordinated with, the Systems Biology Graphical Notation (SBGN) consortium, an international effort to standardize diagrams depicting biochemical and cellular processes studied in systems biology. An update to the notation was published in 2006.\n\n\nMIM diagrams are typically accompanied by a set of citations and comments related to scientific publications describing the interactions within the diagram. Additionally, the diagrams are accompanied by a listing associating elements on the diagram to commonly used identifiers such as genes name as described by HUGO Gene Nomenclature Committee.\n\n", "id": "26082169", "title": "Molecular Interaction Maps"}
{"url": "https://en.wikipedia.org/wiki?curid=52994", "text": "Barr body\n\nA Barr body (named after discoverer Murray Barr) is the inactive X chromosome in a female somatic cell, rendered inactive in a process called lyonization, in those species in which sex is determined by the presence of the Y (including humans) or W chromosome rather than the diploidy of the X. The Lyon hypothesis states that in cells with multiple X chromosomes, all but one are inactivated during mammalian embryogenesis. This happens early in embryonic development at random in mammals, except in marsupials and in some extra-embryonic tissues of some placental mammals, in which the father's X chromosome is always deactivated.\n\nIn humans with more than one X chromosome, the number of Barr bodies visible at interphase is always one fewer than the total number of X chromosomes. For example, men with Klinefelter syndrome (47,XXY karyotype) have a single Barr body, whereas women with a 47,XXX karyotype have two Barr bodies. Barr bodies can be seen on the nucleus of neutrophils. In rare instances, girls can be born with a genetic disorder called Turner syndrome in which they have only one X chromosome, and thus, no Barr bodies in their cells.\n\nA typical human female has only one Barr body per somatic cell, while a typical human male has none.\n\nMammalian X-chromosome inactivation is initiated from the X inactivation centre or \"Xic\", usually found near the centromere. The center contains twelve genes, seven of which code for proteins, five for untranslated RNAs, of which only two are known to play an active role in the X inactivation process, \"Xist\" and \"Tsix\". The centre also appears to be important in chromosome counting: ensuring that random inactivation only takes place when two or more X-chromosomes are present. The provision of an extra artificial \"Xic\" in early embryogenesis can induce inactivation of the single X found in male cells.\n\nThe roles of \"Xist\" and \"Tsix\" appear to be antagonistic. The loss of \"Tsix\" expression on the future inactive X chromosome results in an increase in levels of \"Xist\" around the \"Xic\". Meanwhile, on the future active X \"Tsix\" levels are maintained; thus the levels of \"Xist\" remain low. This shift allows \"Xist\" to begin coating the future inactive chromosome, spreading out from the \"Xic\". In non-random inactivation this choice appears to be fixed and current evidence suggests that the maternally inherited gene may be imprinted.\n\nIt is thought that this constitutes the mechanism of choice, and allows downstream processes to establish the compact state of the Barr body. These changes include histone modifications, such as histone H3 methylation(i.e. H3K27me3 by PRC2 which is recruited by Xist) and histone H2A ubiquitination, as well as direct modification of the DNA itself, via the methylation of CpG sites. These changes help inactivate gene expression on the inactive X-chromosome and to bring about its compaction to form the Barr body. \n\nReactivation of a Barr body is also possible, and has been seen in breast cancer patients. One study showed that the frequency of Barr bodies in breast carcinoma were significantly lower than in healthy controls, indicating reactivation of these once inactivated X chromosomes. \n\n\nLinks to full text articles are provided where access is free, in other cases only the abstract has been linked.\n", "id": "52994", "title": "Barr body"}
{"url": "https://en.wikipedia.org/wiki?curid=26544101", "text": "Genetic matchmaking\n\nGenetic matchmaking is the idea of matching couples for romantic relationships based on their biological compatibility. The initial idea was conceptualized by Claus Wedekind through his famous \"sweaty t-shirt\" experiment. Males were asked to wear T-shirts for two consecutive nights, and then females were asked to smell the T-shirts and rate the body odors for attractiveness. Human body odor has been associated with the human leukocyte antigens (HLA) genomic region. They discovered that females were attracted to men who had dissimilar HLA alleles from them. Furthermore, these females reported that the body odors of HLA-dissimilar males reminded them of their current partners or ex-partners providing further evidence of biological compatibility.\n\nFollowing the seminal research done by Dr. Wedekind, several studies found corroborating evidence for biological compatibility. Garver-Apgar \"et al.\" presented evidence for HLA-dissimilar alleles playing a factor in the healthiness of romantic relationships. They discovered that as the proportion of HLA-similar alleles increased between couples, females reported being less sexually responsive to their partners, less satisfaction from being aroused by their partners, and having additional sexual partners (while with their current partner). Additionally, Ober \"et al.\" conducted an independent study on a population of American Hutterites by comparing the HLA alleles of married couples. They discovered that married couples were less likely to share HLA alleles than expected from random chance; thus their results were consistent with tendencies for same-HLA alleled partners to avoid mating. Further evidence of the importance of genetic compatibility can be found in the finding that couples sharing a higher proportion of HLA alleles tend to have recurring spontaneous abortions, reduced body mass in babies, and longer intervals between successive births.\n\nThere are several biological reasons why females would be attracted to and mate with men with dissimilar HLA alleles:\n\n\nThere has been skepticism throughout the science community claiming the idea to be ridiculous. \"They are just trying to make a buck,\" Dr. Rocio Moran, medical director of the General Genetics Clinic at the Cleveland Clinic. \"If it's genetic, it must be real science.\" Many skeptics state that the science of love cannot be simplified to just a few genes.\n\n", "id": "26544101", "title": "Genetic matchmaking"}
{"url": "https://en.wikipedia.org/wiki?curid=26405771", "text": "Polony sequencing\n\nPolony sequencing is an inexpensive but highly accurate multiplex sequencing technique that can be used to “read” millions of immobilized DNA sequences in parallel. This technique was first developed by Dr. George Church's group at Harvard Medical School. Unlike other sequencing techniques, Polony sequencing technology is an open platform with freely downloadable, open source software and protocols. Also, the hardware of this technique can be easily set up with a commonly available epifluorescence microscopy and a computer-controlled flowcell/fluidics system. Polony sequencing is generally performed on paired-end tags library that each molecule of DNA template is of 135 bp in length with two 17–18 bp paired genomic tags separated and flanked by common sequences. The current read length of this technique is 26 bases per amplicon and 13 bases per tag, leaving a gap of 4–5 bases in each tag.\n\nThe protocol of Polony sequencing can be broken into three main parts, which are the paired end-tag library construction, template amplification and DNA sequencing.\n\nThis protocol begins by randomly shearing the tested genomic DNA into a tight size distribution. The sheared DNA molecules are then subjected for the end repair and A-tailed treatment. The end repair treatment converts any damaged or incompatible protruding ends of DNA to 5’-phosphorylated and blunt-ended DNA, enabling immediate blunt-end ligation, while the A-tailing treatment adds an A to the 3’ end of the sheared DNA. DNA molecules with a length of 1 kb are selected by loading on the 6% TBE PAGE gel. In the next step, the DNA molecules are circularized with T-tailed 30 bp long synthetic oligonucleotides (T30), which contains two outward-facing MmeI recognition sites, and the resulting circularized DNA undergoes rolling circle replication. The amplified circularized DNA molecules are then digested with MmeI (type IIs restriction endonuclease) which will cuts at a distance from its recognition site, releasing the T30 fragment flanked by 17–18 bp tags (≈70 bp in length). The paired-tag molecules need to be end-repaired prior to the ligation of ePCR (emulsion PCR) primer oligonucleotides (FDV2 and RDV2) to their both ends. The resulting 135 bp library molecules are size-selected and nick translated. Lastly, amplify the 135 bp paired end-tag library molecules with PCR to increase the amount of library material and eliminate extraneous ligation products in a single step. The resulted DNA template consists of a 44 bp FDV sequence, a 17–18 bp proximal tag, the T30 sequence, a 17-18 bp distal tag, and a 25 bp RDV sequence.\n\nThe mono-sized, paramagnetic streptavidin–coated beads are pre-loaded with dual biotin forward primer. Streptavidin has a very strong affinity for biotin, thus the forward primer will bind firmly on the surface of the beads. Next, an aqueous phase is prepared with the pre-loaded beads, PCR mixture, forward and reverse primers, and the paired end-tag library. This is mixed and vortexed with an oil phase to create the emulsion. Ideally, each droplet of water in the oil emulsion has one bead and one molecule of template DNA, permitting millions of non-interacting amplification within a milliliter-scale volume by performing PCR.\n\nAfter amplification, the emulsion from preceding step is broken using isopropanol and detergent buffer (10 mM Tris pH 7.5, 1 mM EDTA pH 8.0, 100 mM NaCl, 1% (v/v) Triton X‐100, 1% (w/v) SDS), following with a series of vortexing, centrifuging, and magnetic separation. The resulted solution is a suspension of empty, clonal and non-clonal beads, which arise from emulsion droplets that initially have zero, one or multiple DNA template molecules, respectively. The amplified bead could be enriched in the following step.\n\nThe enrichment of amplified beads is achieved through hybridization to a larger, low density, non-magnetic polystyrene beads that pre-loaded with a biotinylated capture oligonucleotides (DNA sequence that complementary to ePCR amplicon sequence). The mixture is then centrifuged to separate the amplified and capture beads complex from the unamplified beads. The amplified, capture bead complex has a lower density and thus will remain in the supernatant while the unamplified beads form a pellet. The supernatant is recovered and treated with NaOH which will break the complex. The paramagnetic amplified beads are separated from the non-magnetic capture beads by magnetic separation. This enrichment protocol is capable in enriching five times of amplified beads. \n\nThe purpose of bead capping is to attach a “capping” oligonucleotide to the 3’ end of both unextended forward ePCR primers and the RDV segment of template DNA. The cap that being use is an amino group that prevents fluorescent probes from ligating to these ends and at the same time, helping the subsequent coupling of template DNA to the aminosilanated flow cell coverslip.\n\nFirst, the coverslips are washed and aminosilane-treated, enabling the subsequent covalent coupling of template DNA on it and eliminating any fluorescent contamination. The amplified, enriched beads are mixed with acrylamide and poured into a shallow mold formed by a Teflon-masked microscope slide. Immediately, place the aminosilane-treated coverslip on top of the acrylamide gel and allow to polymerize for 45 minutes. Next, invert the slide/coverslip stack and remove the microscope slide from gel. The silane-treated coverslips will bind covalently to the gel while the Teflon on the surface of microscope slide will enable the better removal of slide from the acrylamide gel. The coverslips then bonded to the flow cell body and any unattached beads will be removed.\n\nThe biochemistry of Polony sequencing mainly relies on the discriminatory capacities of ligases and polymerases. First, a series of anchor primers are flowed through the cells and hybridize to the synthetic oligonucleotide sequences at the immediate 3’ or 5’ end of the 17-18 bp proximal or distal genomic DNA tags. Next, an enzymatic ligation reaction of the anchor primer to a population of degenenerate nonamers that are labeled with fluorescent dyes is performed.\nDifferentially labeled nonamers:\n\nThe fluorophore-tagged nonamers anneal with differential success to the tag sequences according to a strategy similar to that of degenerate primers, but instead of submission to polymerases, nonamers are selectively ligated onto adjoining DNA- the anchor primer. The fixation of the fluorophore molecule provides a fluorescent signal that indicates whether there is an A, C, G, or T at the query position on the genomic DNA tag. After four-colour imaging, the anchor primer/nonamer complexes are stripped off and a new cycle is begun by replacing the anchor primer. A new mixture of the fluorescently tagged nonamers is introduced, for which the query position is shifted one base further into the genomic DNA tag. \n\nSeven bases from the 5’ to 3’ direction and six bases from the 3’ end could be queried in this fashion. The ultimate result is a read length of 26 bases per run (13 bases from each of the paired tags) with a 4-base to 5-base gap in the middle of each tag.\n\nThe polony sequencing generates millions of 26 reads per run and this information needed to be normalized and converted to sequence. These can be done by the software that has been developed by Church Lab. All of the software is free and could be downloaded from the website.\n\nThe sequencing instrument used in this technique could be set up by the commonly available fluorescence microscope and a computer controlled flowcell. The required instruments cost around US$130,000 in 2005. A dedicated polony sequencing machine, Polonator, was developed in 2009 and sold at US$170,000 by Dover. It and featured open source software, reagents, and protocols and was intended for use in the Personal Genome Project.\n\nPolony sequencing allows for a high throughput and high consensus accuracies of DNA sequencing based on a commonly available, inexpensive instrument. Also, it is a very flexible technique that enables variable application including BAC (bacterial artificial chromosome) and bacterial genome resequencing, as well as SAGE (serial analysis of gene expression) tag and barcode sequencing. Furthermore, the polony sequencing technique is emphasized as an open system that shares everything including the software that have been developed, protocol and reagents.\n\nHowever, although the raw data acquisition could be achieved as high as 786 gigabits but only 1 bit of information out of 10,000 bits collected is useful. Another challenge of this technique is the uniformity of the relative amplification of individual targets. The non-uniform amplification could lower the efficiency of sequencing and posted as the biggest obstacle in this technique.\n\nPolony sequencing is a development of the polony technology from the late 1990s and 2000s. Methods were developed in 2003 to sequence \"in situ\" polonies using single-base extension which could achieved 5-6 bases reads. By 2005, these early attempts had been overhauled to develop the existing polony sequencing technology. The highly parallel sequencing-by-ligation concepts of polony sequencing has contributed the basis for later sequencing technologies such as ABI Solid Sequencing.\n\n", "id": "26405771", "title": "Polony sequencing"}
{"url": "https://en.wikipedia.org/wiki?curid=27122321", "text": "Synaptic tagging\n\nSynaptic tagging, or the synaptic tagging hypothesis, was first proposed in 1997 by Frey and Morris; it seeks to explain how neural signaling at a particular synapse creates a target for subsequent plasticity-related product (PRP) trafficking essential for sustained LTP and LTD. Although the molecular identity of the tags remains unknown, it has been established that they form as a result of high or low frequency stimulation, interact with incoming PRPs, and have a limited lifespan.\n\nFurther investigations have suggested that plasticity-related products include mRNA and proteins from both the soma and dendritic shaft that must be captured by molecules within the dendritic spine to achieve persistent LTP and LTD. This idea was articulated in the synaptic tag-and-capture hypothesis. Overall, synaptic tagging elaborates on the molecular underpinnings of how L-LTP is generated and leads to memory formation.\n\nFrey and Morris laid the groundwork for the synaptic tagging hypothesis, stating:\n\"We propose that LTP initiates the creation of a short-lasting protein-synthesis-independent 'synaptic tag' at the potentiated synapse which sequesters the relevant protein(s) to establish late LTP. In support of this idea, we now show that weak tetanic stimulation, which ordinarily leads only to early LTP, or repeated tetanization in the presence of protein-synthesis inhibitors, each results in protein-synthesis-dependent late LTP, provided repeated tetanization has already been applied at another input to the same population of neurons. The synaptic tag decays in less than three hours. These findings indicate that the persistence of LTP depends not only on local events during its induction, but also on the prior activity of the neuron.\"\nL-LTP inducing stimulus induces two independent processes including a dendritic biological tag that identifies the synapse as having been stimulated, and a genomic cascade that produces new mRNAs and proteins (plasticity products). While weak stimulation also tags synapses, it does not produce the cascade. Proteins produced in the cascade are characteristically promiscuous, in that they will attach to any recently tagged synapse. However, as Frey and Morris discovered, the tag is temporary and will disappear if no protein presents itself for capture. Therefore, the tag and protein production must overlap if L-LTP is to be induced by the high-frequency stimulation.\nThe experiment performed by Frey and Morris involved the stimulation of two different sets of Schaffer collateral fibers that synapsed on same population of CA1 cells. They then recorded field EPSP associated with each stimulus on either S1 or S2 pathways to produce E-LTP and L-LTP on different synapses within the same neuron, based on the intensity of the stimulus. Results showed 1) that E-LTP produced by weak stimulation could be turned into L-LTP if a strong S2 stimulus was delivered before or after and 2)that the ability to convert E-LTP to L-LTP decreased as the interval between the two stimulations increased, creating temporal dependence. When they blocked protein synthesis prior to the delivery of strong S2 stimulation, the conversion to L-LTP was prevented, showing importance of translating the mRNAs produced by the genomic cascade.\n\nSubsequent research has identified an additional property of synaptic tagging that involves associations between late LTP and LTD. This phenomenon was first identified by Sajikumar and Frey in 2004 and is now referred to as \"cross-tagging\". It involves late-associative interactions between LTP and LTD induced in sets of independent synaptic inputs: late-LTP induced in one set of synaptic inputs can transform early-LTD into late-LTD in another set of inputs. The opposite effect also occurs: early LTP induced in the first synapse can be transformed into late LTP if followed by a late LTD-inducing stimulus in an independent synapse. This phenomenon is seen because the synthesis of nonspecific plasticity related proteins (PRPs) by late-LTP or -LTD in the first synapse is sufficient to transform early-LTD/LTP to late-LTD/LTP in the second synapse after synaptic tags have been set. \n\nBlitzer and his research team proposed a modification to the theory in 2005, stating that the proteins captured by the synaptic tag are actually local proteins that are translated from mRNAs located in the dendrites. This means that mRNAs are not a product of genomic cascade initiated by strong stimulus, but rather, is delivered as a result of continual basal transcription. They proposed that even weakly stimulated synapses that were tagged, yet lack the genomic cascade, can accept proteins that were produced nearby from a strong stimulation.\n\nSynaptic tagging/ tag-and-capture theory potentially addresses the significant problem of explaining how mRNA, proteins, and other molecules may be specifically trafficked to certain dendritic spines during late phase LTP. It has long been known that the late phase of LTP depends on protein synthesis within the particular dendritic spine, as proven by injecting anisomycin into a dendritic spine and observing the resulting absence of late LTP. To achieve translation within the dendritic spine, neurons must synthesize the mRNA in the nucleus, package it within a ribonucleoprotein complex, initiate transport, prevent translation during transport, and ultimately deliver the RNP complex to the appropriate dendritic spine. These processes span a number of disciplines and synaptic tagging/tag-and-capture cannot explain them all; nevertheless, synaptic tagging likely plays an important role in directing mRNA trafficking to the appropriate dendritic spine and signaling the mRNA-RNP complex to dissociate and enter the dendritic spine.\n\nA cell’s identity and the identities of subcellular structures are largely determined by RNA transcripts. Considering this premise, it follows that cellular transcription, trafficking, and translation of mRNA undergo modification at a number of different junctures. Beginning with transcription, mRNA molecules are potentially modified via alternate splicing of exons and introns. The alternate splicing mechanisms allow cells to produce a diverse set of proteins from a single gene within the genome. Recent developments in next-generation sequencing have allowed for greater understanding of the diversity eukaryotic cells achieve through splice variants.\n\nTranscribed mRNA must reach the intended dendritic spine for the spine to express L-LTP. Neurons may transport mRNA to specific dendritic spines in a package along with a transport ribonucleoprotein (RNP) complex; the transport RNP complex is a subtype of an RNA granule. Granules containing two proteins of known importance to synaptic plasticity, CaMKII (Calmodulin-dependent Kinase II) and the immediate early gene Arc, have been identified to associate with a type of the motor protein kinesin, KIF5. Furthermore, there is evidence that polyadenylated mRNA associates with microtubules in mammalian neurons, at least in vitro. Since mRNA transcripts undergo polyadenlyation prior to export from the nucleus, this suggests that the mRNA essential for late-phase LTP may travel along the microtubules within the dendritic shaft prior to reaching the dendritic spine. \n\nOnce the RNA/RNP complex arrives via motor protein to an area within the vicinity of the specific dendritic spine, it must somehow get “captured” by a process within the dendritic spine. This process likely involves the synaptic tag created by synaptic stimulation of sufficient strength. Synaptic tagging may result in capture of the RNA/RNP complex via any number of possible mechanisms such as:\n\n\nSince the 1980s, it has become more and more clear that the dendrites contain the ribosomes, proteins, and RNA components to achieve local and autonomous protein translation. Many mRNAs shown to be localized in the dendrites encode proteins known to be involved in LTP, including AMPA receptor and CaMKII subunits, and cytoskeleton-related proteins MAP2 and Arc.\n\nResearchers provided evidence of local synthesis, by examining the distribution of Arc mRNA after selective stimulation of certain synapses of a hippocampal cell. They found that Arc mRNA was localized at the activated synapses, and Arc protein appeared there simultaneously. This suggests that the mRNA was translated locally.\nThese mRNA transcripts are translated in a cap-dependent manner, meaning they use a \"cap\" anchoring point to facilitate ribosome attachment to the 5' untranslated region. Eukaryotic initiation factor 4 group (eIF4) members recruit ribosomal subunits to the mRNA terminus, and assembly of the eIF4F initiation complex is a target of translational control: phosphorylation of eIF4F exposes the cap for rapid reloading, quickening the rate-limiting step of translation. It is suggested that eIF4F complex formation is regulated during LTP to increase local translation. In addition, excessive eIF4F complex destabilizes LTP.\n\nResearchers have identified sequences within the mRNA that determine its final destination - called localization elements (LEs), zipcodes, and targeting elements (TEs). These are recognized by RNA binding proteins, of which some potential candidates are MARTA and ZBP1. They recognize the TEs, and this interaction results in formation of ribonucleotide protein (RNP) complexes, which travel along cytoskeleton filaments to the spine with the help of motor proteins. Dendritic TEs have been identified in the untranslated region of several mRNAs, like MAP2 and alphaCaMKII.\n\nSynaptic tagging is likely to involve the acquisition of molecular maintenance mechanisms by a synapse that would then allow for the conservation of synaptic changes. There are several proposed processes through which synaptic tagging functions. One model suggests that the tag allows for local protein synthesis at the specified synapse that then leads to modifications in synaptic strength. One example of this suggested mechanism involves the anchoring of PKMzeta mRNA to the tagged synapse. This anchor would then restrict the activity of translated PKMzeta, an important plasticity related protein, to this location. A different model proposes that short-term synaptic changes induced by the stimulus are themselves the tag; subsequently delivered or translated protein products act to strengthen this change. For example, the removal of AMPA receptors due to low-frequency stimulation leading to LTD is stabilized by a new protein product that would be inactive at synapses where internalization had not occurred. The tag could also be a latent memory trace, as another model suggests. The activity of proteins would then be required for the memory trace to lead to sustained changes in synaptic strength. According to this model, changes induced by the latent memory trace, such as the growth of new filipodia, are themselves the tag. These tags require protein products for stabilization, synapse formation, and synapse stabilization. Finally, another model proposes that the required molecular products get directed into the appropriate dendritic branches and then find the specific synapses under efficacy modification, by following Ca++ microconcentration gradients through voltage-gated Ca++ channels.\n\nWhile the information gained on the synaptic tagging hypothesis mainly resulted from experiments that apply stimulation to synapse, a similar model can be applied when considering the process of learning in a broader behavioral sense. Fabricio Ballarini and colleagues created this behavioral tagging model by testing spatial object recognition, contextual conditioning, and conditioned taste aversion in rats with weak training, which is a training that normal only creates a short term memory. However, they paired this weak training with a separate, arbitrary behavioral event that induces protein synthesis and found that so long as the two behavioral events were coupled within a certain time frame, the weak training was sufficient to produce long term memory of that learning task. The researchers believed that the weak learning established a \"learning tag\" to be used later when proteins arrived as a result of the other task, resulting in the formation of long-term memory for even the weak training. This behavioral learning model mirrors the synaptic tagging model, in which a weak stimulation establishes E-LTP that may be serve as the tag used in converting the weak potentiation to the stronger, more persistent L-LTP, once the high-frequency stimulation presents itself.\n", "id": "27122321", "title": "Synaptic tagging"}
{"url": "https://en.wikipedia.org/wiki?curid=6161842", "text": "Translational frameshift\n\nTranslational frameshifting or ribosomal frameshifting refers to an alternative process of protein translation. A protein is translated from one end of the mRNA to the other, from the 5' to the 3' end. Normally a protein is translated from a template mRNA with consecutive blocks of 3 nucleotides being read as single amino acids. However, certain organisms may exhibit a change or shift in the ribosomes frame by one or two nucleotides when translating the genetic code. This is deemed translational or ribosomal frameshifting. The process can be programmed by the nucleotide sequence of the mRNA and is sometimes affected by the secondary or tertiary mRNA structure. It has been described mainly in viruses (especially retroviruses), retrotransposons and bacterial insertion elements, and also in some cellular genes.\n\nProteins are translated unidirectionally by reading tri-nucleotides on the mRNA strand also known as codons. Therefore, a shift of any number of nucleotides that is not divisible by 3 in the reading frame will result in subsequent codons to be read differently. This effectively changes the ribosomal reading frame. For example, the following sentence when read from the beginning makes sense to a reader:\n\nHowever, changing the reading frame by say shifting the first reading up one letter between the T and H on the first word:\n\nNow the sentence makes absolutely no sense. In the case of a translating ribosome, a frameshift can result in nonsense being created after the frameshift or a completely different protein being created after the frameshift. When referring to translational frameshifting, the latter is always implied, the former being usually a result of a point mutation such as a deletion.\n\nThe main distinction between frameshifts resulting from mutation and those resulting from ribosomal frameshifting is that the latter are controlled by various mechanisms found in codons. These mechanisms emerge from the fact that ribosomes do not translate proteins at a steady rate, regardless of the sequence. Certain codons take longer to translate, because there are not equal amounts of tRNA of that particular codon in the cytosol. Due to this lag, there exist in small sections of codons sequences that control the rate of ribosomal frameshifting. Sections of less accessible codons that slow ribosomal transaction are known as \"choke points,\" and sections of easily accessible codons which result in faster ribosomal transaction are \"slippery sequences.\" Slippery sequences can potentially make the reading ribosome \"slip\" and skip a number of nucleotides (usually only 1) and read a completely different frame hereafter. Choke points reduce the probability of this happening.\n\nThis type of frameshifting may be programmed to occur at particular recoding sites and is important in some viruses (e.g. SARS, HIV) and some cellular genes (e.g. \"prfB\" a release factor). Its use is primarily for compacting more genetic information into a shorter amount of genetic material.\n\n\n", "id": "6161842", "title": "Translational frameshift"}
{"url": "https://en.wikipedia.org/wiki?curid=11808249", "text": "Genome-wide association study\n\nIn genetics, a genome-wide association study (GWA study, or GWAS), also known as whole genome association study (WGA study, or WGAS), is an observational study of a genome-wide set of genetic variants in different individuals to see if any variant is associated with a trait. GWASs typically focus on associations between single-nucleotide polymorphisms (SNPs) and traits like major human diseases, but can equally be applied to any other organism.\n\nWhen applied to human data, GWA studies compare the DNA of participants having varying phenotypes for a particular trait or disease. These participants may be people with a disease (cases) and similar people without the disease (controls), or they may be people with different phenotypes for a particular trait, for example blood pressure. This approach is known as phenotype-first, in which the participants are classified first by their clinical manifestation(s), as opposed to genotype-first. Each person gives a sample of DNA, from which millions of genetic variants are read using SNP arrays. If one type of the variant (one allele) is more frequent in people with the disease, the variant is said to be \"associated\" with the disease. The associated SNPs are then considered to mark a region of the human genome that may influence the risk of disease.\n\nGWA studies investigate the entire genome, in contrast to methods that specifically test a small number of pre-specified genetic regions. Hence, GWAS is a \"non-candidate-driven\" approach, in contrast to \"gene-specific candidate-driven studies\". GWA studies identify SNPs and other variants in DNA associated with a disease, but they cannot on their own specify which genes are causal.\n\nThe first successful GWAS was published in 2005. It investigated patients with age-related macular degeneration and found two SNPs with significantly altered allele frequency compared to healthy controls. , hundreds or thousands of individuals are tested in a typical GWA study, over 3,000 human GWA studies have examined over 1,800 diseases and traits, and thousands of SNP associations have been found.\n\nAny two human genomes differ in millions of different ways. There are small variations in the individual nucleotides of the genomes (SNPs) as well as many larger variations, such as deletions, insertions and copy number variations. Any of these may cause alterations in an individual's traits, or phenotype, which can be anything from disease risk to physical properties such as height. Around the year 2000, prior to the introduction of GWA studies, the primary method of investigation was through inheritance studies of genetic linkage in families. This approach had proven highly useful towards single gene disorders. However, for common and complex diseases the results of genetic linkage studies proved hard to reproduce. A suggested alternative to linkage studies was the genetic association study. This study type asks if the allele of a genetic variant is found more often than expected in individuals with the phenotype of interest (e.g. with the disease being studied). Early calculations on statistical power indicated that this approach could be better than linkage studies at detecting weak genetic effects.\n\nIn addition to the conceptual framework several additional factors enabled the GWA studies. One was the advent of biobanks, which are repositories of human genetic material that greatly reduced the cost and difficulty of collecting sufficient numbers of biological specimens for study. Another was the International HapMap Project, which, from 2003 identified a majority of the common SNPs interrogated in a GWA study. The haploblock structure identified by HapMap project also allowed the focus on the subset of SNPs that would describe most of the variation. Also the development of the methods to genotype all these SNPs using genotyping arrays was an important prerequisite.\n\nThe most common approach of GWA studies is the case-control setup, which compares two large groups of individuals, one healthy control group and one case group affected by a disease. All individuals in each group are genotyped for the majority of common known SNPs. The exact number of SNPs depends on the genotyping technology, but are typically one million or more. For each of these SNPs it is then investigated if the allele frequency is significantly altered between the case and the control group. In such setups, the fundamental unit for reporting effect sizes is the odds ratio. The odds ratio is the ratio of two odds, which in the context of GWA studies are the odds of disease for individuals having a specific allele and the odds of disease for individuals who do not have that same allele. When the allele frequency in the case group is much higher than in the control group, the odds ratio is higher than 1, and vice versa for lower allele frequency. Additionally, a P-value for the significance of the odds ratio is typically calculated using a simple chi-squared test. Finding odds ratios that are significantly different from 1 is the objective of the GWA study because this shows that a SNP is associated with disease.\n\nThere are several variations to this case-control approach. A common alternative to case-control GWA studies is the analysis of quantitative phenotypic data, e.g. height or biomarker concentrations or even gene expression. Likewise, alternative statistics designed for dominance or recessive penetrance patterns can be used. Calculations are typically done using bioinformatics software such as SNPTEST and PLINK, which also include support for many of these alternative statistics. Earlier GWAS focused on the effect of individual SNPs. However, the empirical evidence shows that complex interactions among two or more SNPs, epistasis, might contribute to complex diseases. Moreover, the researchers try to integrate GWA data with other biological data such as protein protein interaction network to extract more informative results.\n\nA key step in the majority of GWA studies is the imputation of genotypes at SNPs not on the genotype chip used in the study. This process greatly increases the number of SNPs that can be tested for association, increases the power of the study, and facilitates meta-analysis of GWAS across distinct cohorts. Genotype imputation is carried out by statistical methods that combine the GWAS data together with a reference panel of haplotypes. These methods take advantage of sharing of haplotypes between individuals over short stretches of sequence to impute alleles. Existing software packages for genotype imputation include IMPUTE2 and MaCH.\n\nIn addition to the calculation of association, it is common to take into account any variables that could potentially confound the results. Sex and age are common examples of confounding variables. Moreover, it is also known that many genetic variations are associated with the geographical and historical populations in which the mutations first arose. Because of this association, studies must take account of the geographical and ethnical background of participants by controlling for what is called population stratification.\n\nAfter odds ratios and P-values have been calculated for all SNPs, a common approach is to create a Manhattan plot. In the context of GWA studies, this plot shows the negative logarithm of the P-value as a function of genomic location. Thus the SNPs with the most significant association stand out on the plot, usually as stacks of points because of haploblock structure. Importantly, the P-value threshold for significance is corrected for multiple testing issues. The exact threshold varies by study, but the conventional threshold is to be significant in the face of hundreds of thousands to millions of tested SNPs. GWA studies typically perform the first analysis in a discovery cohort, followed by validation of the most significant SNPs in an independent validation cohort.\n\nAttempts have been made at creating comprehensive catalogues of SNPs that have been identified from GWA studies. As of 2009, SNPs associated with diseases are numbered in the thousands.\n\nThe first GWA study, conducted in 2005, compared 96 patients with age-related macular degeneration (ARMD) with 50 healthy controls. It identified two SNPs with significantly altered allele frequency between the two groups. These SNPs were located in the gene encoding complement factor H, which was an unexpected finding in the research of ARMD. The findings from these first GWA studies have subsequently prompted further functional research towards therapeutical manipulation of the complement system in ARMD. Another landmark publication in the history of GWA studies was the Wellcome Trust Case Control Consortium (WTCCC) study, the largest GWA study ever conducted at the time of its publication in 2007. The WTCCC included 14,000 cases of seven common diseases (~2,000 individuals for each of coronary heart disease, type 1 diabetes, type 2 diabetes, rheumatoid arthritis, Crohn's disease, bipolar disorder, and hypertension) and 3,000 shared controls. This study was successful in uncovering many new disease genes underlying these diseases.\n\nSince these first landmark GWA studies, there have been two general trends. One has been towards larger and larger sample sizes. At the end of 2011, the largest sample sizes were in the range of 200,000 individuals. The reason is the drive towards reliably detecting risk-SNPs that have smaller odds ratios and lower allele frequency. Another trend has been towards the use of more narrowly defined phenotypes, such as blood lipids, proinsulin or similar biomarkers. These are called \"intermediate phenotypes\", and their analyses may be of value to functional research into biomarkers. A variation of GWAS uses participants that are first-degree \"relatives\" of people with a disease. This type of study has been named genome-wide association study by proxy (\"GWAX\").\n\nA central point of debate on GWA studies has been that most of the SNP variations found by GWA studies are associated with only a small increased risk of the disease, and have only a small predictive value. The median odds ratio is 1.33 per risk-SNP, with only a few showing odds ratios above 3.0. These magnitudes are considered small because they do not explain much of the heritable variation. This heritable variation is known from heritability studies based on monozygotic twins. For example, it is known that 80-90% of variance in height can be explained by hereditary differences, but GWA studies only account for a minority of this variance.\n\nA challenge for future successful GWA study is to apply the findings in a way that accelerates drug and diagnostics development, including better integration of genetic studies into the drug-development process and a focus on the role of genetic variation in maintaining health as a blueprint for designing new drugs and diagnostics. Several studies have looked into the use of risk-SNP markers as a means of directly improving the accuracy of prognosis. Some have found that the accuracy of prognosis improves, while others report only minor benefits from this use. Generally, a problem with this direct approach is the small magnitudes of the effects observed. A small effect ultimately translates into a poor separation of cases and controls and thus only a small improvement of prognosis accuracy. An alternative application is therefore the potential for GWA studies to elucidate pathophysiology.\n\nOne such success is related to identifying the genetic variant associated with response to anti-hepatitis C virus treatment. For genotype 1 hepatitis C treated with Pegylated interferon-alpha-2a or Pegylated interferon-alpha-2b combined with ribavirin, a GWA study has shown that SNPs near the human IL28B gene, encoding interferon lambda 3, are associated with significant differences in response to the treatment. A later report demonstrated that the same genetic variants are also associated with the natural clearance of the genotype 1 hepatitis C virus. These major findings facilitated the development of personalized medicine and allowed physicians to customize medical decisions based on the patient's genotype.\n\nThe goal of elucidating pathophysiology has also led to increased interest in the association between risk-SNPs and the gene expression of nearby genes, the so-called expression quantitative trait loci (eQTL) studies. The reason is that GWAS studies identify risk-SNPs, but not risk-genes, and specification of genes is one step closer towards actionable drug targets. As a result, major GWA studies by 2011 typically included extensive eQTL analysis. One of the strongest eQTL effects observed for a GWA-identified risk SNP is the SORT1 locus. Functional follow up studies of this locus using small interfering RNA and gene knock-out mice have shed light on the metabolism of low-density lipoproteins, which have important clinical implications for cardiovascular disease.\n\nGWA studies have several issues and limitations that can be taken care of through proper quality control and study setup. Lack of well defined case and control groups, insufficient sample size, control for multiple testing and control for population stratification are common problems. Particularly the statistical issue of multiple testing wherein it has been noted that \"the GWA approach can be problematic because the massive number of statistical tests performed presents an unprecedented potential for false-positive results\". Ignoring these correctible issues has been cited as contributing to a general sense of problems with the GWA methodology. In addition to easily correctible problems such as these, some more subtle but important issues have surfaced. A high-profile GWA study that investigated individuals with very long life spans to identify SNPs associated with longevity is an example of this. The publication came under scrutiny because of a discrepancy between the type of genotyping array in the case and control group, which caused several SNPs to be falsely highlighted as associated with longevity. The study was subsequently retracted.\n\nIn addition to these preventable issues, GWA studies have attracted more fundamental criticism, mainly because of their assumption that common genetic variation plays a large role in explaining the heritable variation of common disease. This aspect of GWA studies has attracted the criticism that, although it could not have been known prospectively, GWA studies were ultimately not worth the expenditure. Alternative strategies suggested involve linkage analysis. More recently, the rapidly decreasing price of complete genome sequencing have also provided a realistic alternative to genotyping array-based GWA studies. It can be discussed if the use of this new technique is still referred to as a GWA study, but high-throughput sequencing does have potential to side-step some of the shortcomings of non-sequencing GWA.\n\nGenotyping arrays designed for GWAS rely on linkage disequilibrium to provide coverage of the entire genome by genotyping a subset of variants. Because of this, the reported associated variants are unlikely to be the actual causal variants. Associated regions can contain hundreds of variants spanning large regions and encompassing many different genes, making the biological interpretation of GWAS loci more difficult. Fine-mapping is a process to refine these lists of associated variants to a credible set most likely to include the causal variant.\n\nFine-mapping requires all variants in the associated region to have been genotyped or imputed (dense coverage), very stringent quality control resulting in high-quality genotypes, and large sample sizes sufficient in separating out highly correlated signals. There are several different methods to perform fine-mapping, and all methods produce a posterior probability that a variant in that locus is causal. Because the requirements are often difficult to satisfy, there are still limited examples of these methods being more generally applied.\n\n\n", "id": "11808249", "title": "Genome-wide association study"}
{"url": "https://en.wikipedia.org/wiki?curid=3700085", "text": "Bead theory\n\nThe bead theory is a disproved hypothesis that genes are arranged on the chromosome like beads on a necklace. According to this theory, the existence of a gene as a unit of inheritance is recognized through its mutant alleles. A mutant allele affects a single phenotypic character, maps to one chromosome locus, gives a mutant phenotype when paired and shows a Mendelian ratio when intercrossed. Several tenets of the bead theory are worth emphasizing :-\n1. The gene is viewed as a fundamental unit of structure, indivisible by crossing over. Crossing over take place between genes ( the beads in this model ) but never within them.\n2. The gene is viewed as the fundamental unit of change or mutation. It changes in toto from one allelic form into another; there are no smaller components within it that can change.\n3. The gene is viewed as the fundamental unit of function ( although the precise function of gene is not specified in this model ). Parts of a gene, if they exist cannot function.\nSeymour Benzer showed in the 1950s that the bead theory was not correct. He demonstrated that a gene can be defined as a unit of function. A gene can be subdivided into a linear array of sites that are mutable and that can be recombined. The smallest units of mutation and recombination are now known to be correlated with single nucleotide pairs.\n\nNew york W.H. Freeman;2000 \n", "id": "3700085", "title": "Bead theory"}
{"url": "https://en.wikipedia.org/wiki?curid=10902825", "text": "Ecogenetics\n\nEcogenetics is a branch of genetics that studies genetic traits related to the response to environmental substances. Or, a contraction of ecological genetics, the study of the relationship between a natural population and its genetic structure.\nEcogenetics principally deals with effects of preexisting genetically determined variability on the response to environmental agents. The word environmental is defined broadly to include the physical, chemical, biological, atmospheric, and climate agents. Ecogenetics, therefore, is an all-embracing term, and concepts such as pharmacogenetics are seen as subcomponents of ecogenetics. This work grew logically from the book entitled Pollutants and High Risk Groups (1978), which presented an overview of the various host factors i.e. age, heredity, diet, preexisting diseases, and lifestyles affect environmentally induced disease. As in all rapidly expanding areas, this will certainly not be the last word or topic but will serve as a conduit for present and future biomedical researchers and policymakers as they approach this field. It should be emphasized the primary intention of Ecogenetics is to provide an objective and critical evaluation of the scientific literature pertaining to genetic factors and differential susceptibility to environmental agents, with particular emphasis on those agents typically considered pollutants. It is important to realize though that ones genetic makeup, while important, is but one of an array of host factors contributing to overall adaptive capacity of the individual. In many instances, it is possible for such factors to interact in ways that may enhance or offset the effect of each other.\n\nRed Blood Cell Conditions\nThere is a broad group of genetic diseases that result in either producing or predisposing afflicted individuals to the development of hemolytic anemias. For example, these diseases include, abnormal hemoglobin, inability to manufacture one or the other of the peptide globin chains of the hemoglobin, and deficiencies of the Embden-Meyerhoff monophosphate.\n\nLiver Metabolism\nIndividuals lacking the ability to detoxify and excrete PCB’s represent such a high risk of total liver failure that because of ecological factors they are the high-risk group that may not be a pertinent choice for ecological stabilization.\n\nCardiovascular Diseases\nThe pathologic lesion of Atherosclerosis is a plaque like substance that thickens the innermost and middle of the three layers of the artery wall. The thickening of the intimal and medial layers results from the accumulation of the proliferating smooth muscle cells that are encompassed by interstitial substances such as collagen, elastin, glycosamninoglycans, and fibrin.\n\nRespiratory Diseases\nThere are three genetically based respiratory diseases that can directly correspond with ecological functions that not only induce but can prevent future involvement of vaccines. These include lung cancer and the upper and lower respiratory tract associated with a serum Ig A deficiency.\n\nvan Zyl, Jay. Built to Thrive: using innovation to make your mark in a connected world. Chapter 5 Ecogenetics. San Francisco. 2011\nCalabrese, J Edward. \"Ecogenetics : genetic variation in susceptibility to environmental agents\". Environmental Science and Technology. New York. 1984.\n", "id": "10902825", "title": "Ecogenetics"}
{"url": "https://en.wikipedia.org/wiki?curid=28414953", "text": "Vasa gene\n\nVasa is an RNA binding protein with an ATP-dependent RNA helicase that is a member of the DEAD box family of proteins. The vasa gene, is essential for germ cell development and was first identified in \"Drosophila melanogaster\", but has since been found to be conserved in a variety of vertebrates and invertebrates including humans. The Vasa protein is found primarily in germ cells in embryos and adults, where it is involved in germ cell determination and function, as well as in multipotent stem cells, where its exact function is unknown.\nThe Vasa gene is a member of the DEAD box family of RNA helicases in \"Drosophila melanogaster.\" Its human ortholog, Ddx4, is located on human chromosome 5q. It is syntenic to mouse chromosome 13, where the mouse vasa gene, is located. The gene is conserved in many invertebrates and vertebrate species such as \"C. elegans\", \"Xenopus\", Zebrafish, flatworms, echinderms, molluscs, nematodes, mice and rats as an important part of germ line maintenance and function.\n\nAll vertebrate species, including \"Drosophila\", have only one vasa ortholog but C. elegans has four Vasa genes, but only one (GLH-1) is essential.\n\nAll DEAD box genes, including Vasa, have 9 conserved sequence motifs. The Vasa gene family evolved from a duplication event followed by acquiring certain domains. Early in the evolution of multicellular animals, the duplication of PL10 related DEAD-box gene occurred. This resulted in animals having both Vasa and PL10 genes, but plants and fungi only have PL10 genes and no Vasa. After the duplication event, the N-terminal region acquired Zn-knuckle domains which are now conserved in invertebrates. Vertebrates and insects both have lost the Zn-knuckle domains. The number of these domains vary between different species Vasa genes. An important property of Zn-knuckles, which can be categorized as classical zinc fingers, is that they are able to bind to single and double stranded DNA or RNA. The presence of Zn-knuckles in invertebrates and absence in vertebrates may be an indication of differences in target binding sites. Their presence may be important to functions outside germ line development. An exception to this theory is the presence of Zn-knuckles in all four C. elegans Vasa genes, which are restricted to functions in the germ line. \n\nThe protein product in humans has 724 amino acids, a molecular mass of 79 kDa and 8 conserved domains in all DEAD-box proteins that is involved in RNA helicase activity. Domain V contains the DEAD motif. As with other Vasa related proteins, human Vasa has a N terminus rich in glycine and RGG motif repeats that function in RNA binding.\n\nVasa is regulated at the transcript and protein level. Developing embryos and adults regulate Vasa expression to cell and tissue specific locations. In Drosophila, zygotic transcription of Vasa occurs at pole cells, and stays germ-line specific throughout the life of the organism.\n\nThe Vasa promoter is regulated through methylation. In cells were Vasa is transcribed successfully, the promoter is hypomethylated and in all other cells it is methylated. When Vasa is hypermethylated in testes, spermatogenesis defects may occur.\n\nPost-transcriptionally Vasa has several splice forms in different animals. In \"P. hawaienis\", Vasa transcript is uniformly distributed in the embryo and is localized depending on the stabilization of the 3’UTR (Untranslated Region) to the germ line cells. Translation can be inhibited by cis regulatory elements in the transcript's 5' and 3' UTRs. They may inhibit translation by forming secondary RNA structures or binding trans-acting factors. Vasa expression localization is directed by repressing these translation inhibitory pathways.\n\nPost-translationally, in \"Drosophila\", Vasa protein is localized to the pole plasm during embryonic development. Many other proteins in \"Drosophila\" are also localized to the poles. For example, Oskar protein was found to localize to pole plasm and may be involved in anchoring Vasa to polar granules in the posterior pole of the oocyte. Another enzyme, fat facets, may further stabilize Vasa in the pole plasm. Other post-translational modification includes phosphorylation of the Vasa ortholog in \"C. elegans\", and arginine methylation in a conserved region of mice, \"Xenopus\" and \"Drosophila\" Vasa genes. \n\nOne of main function of Vasa protein is in germ cell determination and function. It uses ATP dependent RNA helicase catalytic activity to regulate the translation of multiple mRNAs. Vasa unwinds the duplex RNA by binding and bending short stretches of the duplex in a non-processive manner. The conserved domain may act as chaperones by unwinding RNA secondary structures and refolding properly. pre-mRNA splicing, ribosome biogenesis, nuclear export, translational regulation and degradation.\n\nVasa was found to bind RNA in a sequence specific manner. In the Drosophila embryos, Vasa binds the Uracil rich motif of the mei-P26 UTR. A mutation in Vasa reduced the interaction of between Mei-P26 and initiation factor elF58 which in turn significantly reduced translation of the gene.\n\nRecent evidence in invertebrates have found that Vasa has a role in multipotent stem cells, but the exact the function is unknown.\n\nA null mutation causes female sterility due to severe defects in oogenesis but males are fertile.\n\nHomozygous mutations for partial loss of function allows eggs to be fertilized but embryos lack germ cells.\n\nMutations in Vasa homolog, \"Mvh\", cause defects in spermatogenesis but females are fertile. Male sterility may be due to deficiencies in germ cell proliferation and differentiation (the mouse homolog of Droso.). Female fertility may be due to functional redundancy by other DEAD-box family members. Null mutation still allows primordial germ cells to form but have severe defects.\n\nAlthough there are no studies done on Vasa mutations in humans, it is likely that it would cause sterility.\n\nThese sex-specific phenotypes in mice and \"Drosophila\" mutants suggest that Vasa either regulated differently or has different target functions in the two germ line types.\n\nVasa expression is restricted to tissue specific cells. Until recently it was thought that Vasa protein can only be found in gametes and is undetectable in somatic cells. Within germ cells, Vasa is expressed in the cytoplasm. During embryogenesis, Vasa is expressed in migratory primordial germ cells (PGCs) at the gonadal ridge in both males and females.This specificity allows Vasa to be used as a highly specific marker for germ cells. In a patient with Sertoli cell syndrome, no Vasa signal was detected from testicular biopsy. However, recent studies show that Vasa functions in other cells as well.\n\nA study on \"Macrostomum lignano\" found Vasa expression in multipotent neoblast stem cells in addition to germ cells. However, RNAi knockdown revealed that either Vasa is non-essential in this organism or is made functionally redundant by other Vasa-like genes. Similar results were found on studies of colonial ascidians, oysters, teleosts, Xenopus, the parasitic wasp, and the crustacean \"Parhyale hawaiensis\".\n\nVasa expression has been observed in epithelial ovarian cancer cells. It was found to deter the DNA damage-induced G2 checkpoint by downregulating the expression of another gene. Vasa is also present in chicken embryonic stem cells where it induces expression of germ line genes. This function still supports the most important role of Vasa in germ line development. In Cnidarians, Vasa has a role in nerve cells and gland cells. Other examples include Vasa in multipotent stem cell cluster of \"Polyascus polygenea\" buds and stolon, Vasa in auxiliary cells of oyster ovaries, Vasa in non-germ-line lineages in snails, Vasa in progenitor mesodermal posterior growth zone of polychaete annelid, and Vasa present in non-genetical segments during Oligochaete development. But no reports of vasa expressed outside of germ line cells in vertebrates or insects.\n\nIn \"Drosophila\", \"vasa\" expression is seen in germ cells, specifically the germline stem cells (GSC's) of female ovaries and in the early stages of spermatogensis in the male testis.\n\nDue to the localization of \"vasa\", immunohistochemistry staining can be done with vasa antibodies. For example, \"vasa\" antibody staining is specific for germline stem cells in the \"D. melanogaster\" germarium.\n\nThis protein is localized to the cytoplasm of fetal germ cells and to the cytoplasm of developing oocytes in the mammals.\n", "id": "28414953", "title": "Vasa gene"}
{"url": "https://en.wikipedia.org/wiki?curid=1795200", "text": "Candidate gene\n\nThe candidate gene approach to conducting genetic association studies focuses on associations between genetic variation within pre-specified genes of interest and phenotypes or disease states. This is in contrast to genome-wide association studies (GWAS), which scan the entire genome for common genetic variation. Candidate genes are most often selected for study based on \"a priori\" knowledge of the gene's biological functional impact on the trait or disease in question. The rationale behind focusing on allelic variation in specific, biologically relevant regions of the genome is that certain mutations will directly impact the function of the gene in question, and lead to the phenotype or disease state being investigated. This approach usually uses the case-control study design to try to answer the question, \"Is one allele of a candidate gene more frequently seen in subjects with the disease than in subjects without the disease?\"\n\nSuitable candidate genes are generally selected based on known biological, physiological, or functional relevance to the disease in question. This approach is limited by its reliance on existing knowledge about known or theoretical biology of disease. However, more recently developed molecular tools are allowing insight into disease mechanisms and pinpointing potential regions of interest in the genome. Genome-wide association studies and quantitative trait locus (QTL) mapping examine common variation across the entire genome, and as such can detect a new region of interest that is in or near a potential candidate gene. Microarray data allow researchers to examine differential gene expression between cases and controls, and can help pinpoint new potential genes of interest.\n\nThe great variability between organisms can sometimes make it difficult to distinguish normal variation in SNP from a candidate gene from disease-associated variation. In analyzing large amounts of data, there are several other factors that can help lead to the most probable variant. These factors include priorities in SNPs, relative risk of functional change in genes, and linkage disequilibrium among SNPs.\n\nIn addition, the availability of genetic information through online databases enables researchers to mine existing data and web-based resources for new candidate gene targets. Many online databases are available to research genes across species.\n\nBefore the candidate-gene approach was fully developed, various other methods were used to identify genes linked to disease-states. These methods studied genetic linkage and positional cloning through the use of a genetic screen, and were effective at identifying relative risk genes in Mendelian diseases. However, these methods are not as beneficial when studying complex diseases for several reasons:\n\n\nDespite the drawbacks of linkage analysis studies, they are nevertheless useful in preliminary studies to isolate genes linked to disease.\n\nA study of candidate genes seeks to balance the use of data while attempting to minimize the chance of creating false positive or negative results. Because this balance can often be difficult, there are several criticisms of the candidate gene approach that are important to understand before beginning such a study.\n\nOne critique is that findings of association within candidate-gene studies have not been easily replicated in follow up studies. Population stratification has often been blamed for this inconsistency; therefore caution must be taken in regards to what criteria define a certain phenotype, as well as other variations in design study.\n\nAdditionally, because these studies incorporate \"a priori\" knowledge, some critics argue that our knowledge is not sufficient to make predictions from. Therefore, results gained from these 'hypothesis-driven' approaches are dependent on the ability to select plausible candidates from the genome, rather than use an anonymous approach. The limited knowledge of complex disease can result in 'information bottleneck', which can be overcome by comparative genomics across different species. This bias can also be overcome by carefully choosing genes based on what factors are most likely to be involved in phenotype.\n\nThese critiques are important to remember as one examines their experimental approach. With any other scientific method, the candidate gene approach itself is subject to critique, but still proven to be a powerfully effective tool for studying the genetic makeup of complex traits.\n\nThe candidate gene approach is a powerful tool to studying complex diseases, particularly if its limitations are overcome by a wide complementary approach. One of the earliest successes in this field was finding a single base mutation in the non-coding region of the APOC3 (apolipoprotein C3 gene) that associated with  higher risks of hypertriglyceridemia and atherosclerosis (see David J. Galton Lancet 1983 pp444-446). In a study by Kim et al., genes linked to the obesity trait in both pigs and humans were discovered using comparative genomics and chromosomal heritability. By using these two methods, the researchers were able to overcome the criticism that candidate gene studies are solely focused on prior knowledge. Comparative genomics was completed by examining both human and pig QTL through a method known as GCTA (genome-wide complex trait analysis), which allowed the researchers to then map genetic variance to specific chromosomes. This allowed the parameter of heritability to provide understanding of where phenotypic variation was on specific chromosomal regions, thus extending to candidate markers and genes within these regions. Other studies may also use computational methods to find candidate genes in a widespread, complementary way, such as one study by Tiffin et al. studying genes linked to type 2 diabetes.\n\nMany studies have similarly used candidate genes as part of a multi-disciplinary approach to examining a trait or phenotype. One example of manipulating candidate genes can be seen in a study completed by Martin E. Feder on heat-shock proteins and their function in \"Drosophila melanogaster\". Feder designed a holistic approach to study Hsp70, a candidate gene that was hypothesized to play a role in how an organism adapted to stress. \"Drosophila\" is a highly useful model organism for studying this trait due to the way it can support a diverse number of genetic approaches for studying a candidate gene. The different approaches this study took included both genetically modifying the candidate gene (using site-specific homologous recombination and the expression of various proteins), as well as examining the natural variation of Hsp70. He concluded that the results of these studies gave a multi-faceted view of Hsp70. By engineering and modifying these candidate genes, they were able to confirm the ways in which this gene was linked to a change phenotype. Understanding the natural and historical context in which these phenotypes operate by examining the natural genome structure complemented this.\n\n", "id": "1795200", "title": "Candidate gene"}
{"url": "https://en.wikipedia.org/wiki?curid=4363681", "text": "Gene dosage\n\nGene dosage is the number of copies of a particular gene present in a genome. Gene dosage is known to be related to the amount of gene product the cell is able to express, however, amount of gene product produced in a cell is more commonly dependent on regulation of gene expression. Nonetheless, changes in gene dosage (copy number variations) due to gene insertions or deletions can have significant phenotypic consequences.\n\nIn eukaryotes, most genes found in the cell are expressed as autosomal genes (see autosome) and are found in two copies, alterations to this two-copy gene dosage is significantly associated with quantitative or qualitative phenotype traits and is linked to many genetic health problems such as those associated with spinal muscular atrophy and Down syndrome. In Down syndrome, the gene expression on chromosome 21 has increased 50%, and this results in significant health and mental disabilities (1 in 800 human live births have Down syndrome).\n\nProkaryotes reproduce through asexual reproduction, usually by binary fission. The bacterial chromosome is present only in one copy per cell, but there can still be variation in gene dosage due to DNA replication which starts at the origin of replication and ends at the termination site. The genes that are closer to the origin site would be replicated first and would consequently be present in two copies in the cell for a longer time than the genes that are closer to the termination site. These slight gene dosage differences are responsible for variation in gene expression depending on the position on the chromosome. \n\n", "id": "4363681", "title": "Gene dosage"}
{"url": "https://en.wikipedia.org/wiki?curid=2345336", "text": "Gene mapping\n\nGene mapping describes the methods used to identify the locus of a gene and the distances between genes.\n\nThe essence of all genome mapping is to place a collection of molecular markers onto their respective positions on the genome. Molecular markers come in all forms. Genes can be viewed as one special type of genetic markers in the construction of genome maps, and mapped the same way as any other markers.\n\nThere are two distinctive types of \"Maps\" used in the field of genome mapping: genetic maps and physical maps. While both maps are a collection of genetic markers and gene loci, genetic maps' distances are based on the genetic linkage information, while physical maps use actual physical distances usually measured in number of base pairs. While the physical map could be a more \"accurate\" representation of the genome, genetic maps often offer insights into the nature of different regions of the chromosome, e.g. the genetic distance to physical distance ratio varies greatly at different genomic regions which reflects different recombination rates, and such rate is often indicative of euchromatic (usually gene-rich) vs heterochromatic (usually gene poor) regions of the genome.\n\nResearchers begin a genetic map by collecting samples of blood or tissue from family members that carry a prominent disease or trait and family members that don't. Scientists then isolate DNA from the samples and closely examine it, looking for unique patterns in the DNA of the family members who do carry the disease that the DNA of those who don't carry the disease don't have. These unique molecular patterns in the DNA are referred to as polymorphisms, or markers.\n\nThe first steps of building a genetic map are the development of genetic markers and a mapping population. The closer two markers are on the chromosome, the more likely they are to be passed on to the next generation together. Therefore, the \"co-segregation\" patterns of all markers can be used to reconstruct their order. With this in mind, the genotypes of each genetic marker are recorded for both parents and each individual in the following generations. The quality of the genetic maps is largely dependent upon these factors: the number of genetic markers on the map and the size of the mapping population. The two factors are interlinked, as a larger mapping population could increase the \"resolution\" of the map and prevent the map being \"saturated\".\n\nIn gene mapping, any sequence feature that can be faithfully distinguished from the two parents can be used as a genetic marker. Genes, in this regard, are represented by \"traits\" that can be faithfully distinguished between two parents. Their linkage with other genetic markers are calculated same way as if they are common markers and the actual gene loci are then bracketed in a region between the two nearest neighbouring markers. The entire process is then repeated by looking at more markers which target that region to map the gene neighbourhood to a higher resolution until a specific causative locus can be identified. This process is often referred to as \"positional cloning\", and it is used extensively in the study of plant species.\n\nSince actual base-pair distances are generally hard or impossible to directly measure, physical maps are actually constructed by first shattering the genome into hierarchically smaller pieces. By characterizing each single piece and assembling back together, the overlapping path or \"tiling path\" of these small fragments would allow researchers to infer physical distances between genomic features. The fragmentation of the genome can be achieved by restriction enzyme cutting or by physically shattering the genome by processes like sonication. Once cut, the DNA fragments are separated by electrophoresis. The resulting pattern of DNA migration (i.e. its genetic fingerprint) is used to identify what stretch of DNA is in the clone. By analyzing the fingerprints, contigs are assembled by automated (FPC) or manual means (pathfinders) into overlapping DNA stretches. Now a good choice of clones can be made to efficiently sequence the clones to determine the DNA sequence of the organism under study.\n\nIn physical mapping, there are no direct ways of marking up a specific gene since the mapping does not include any information that concerns traits and functions. Genetic markers can be linked to a physical map by processes like in situ hybridization. By this approach, physical map contigs can be \"anchored\" onto a genetic map. The clones used in the physical map contigs can then be sequenced on a local scale to help new genetic marker design and identification of the causative loci.\n\nMacrorestriction is a type of physical mapping wherein the high molecular weight DNA is digested with a restriction enzyme having a low number of restriction sites.\n\nThere are alternative ways to determine how DNA in a group of clones overlaps without completely sequencing the clones. Once the map is determined, the clones can be used as a resource to efficiently contain large stretches of the genome. This type of mapping is more accurate than genetic maps.\n\nGenome sequencing is sometimes mistakenly referred to as \"genome mapping\" by non-biologists. The process of \"shotgun sequencing\" resembles the process of physical mapping: it shatters the genome into small fragments, characterizes each fragment, then puts them back together (more recent sequencing technologies are drastically different). While the scope, purpose and process are totally different, a genome assembly can be viewed as the \"ultimate\" form of physical map, in that it provides in a much better way all the information that a traditional physical map can offer.\n\nIdentification of genes is usually the first step in understanding a genome of a species; mapping of the gene is usually the first step of identification of the gene. Gene mapping is usually the starting point of many important downstream studies.\n\nThe process to identify a genetic element that is responsible for a disease is also referred to as \"mapping\". If the locus in which the search is performed is already considerably constrained, the search is called the \"fine mapping\" of a gene. This information is derived from the investigation of disease manifestations in large families (genetic linkage) or from populations-based genetic association studies.\n\n\n", "id": "2345336", "title": "Gene mapping"}
{"url": "https://en.wikipedia.org/wiki?curid=16464479", "text": "Genetic carrier\n\nA hereditary carrier (or just carrier), is a person or other organism that has inherited a recessive allele for a genetic trait or mutation but does not display that trait or show symptoms of the disease. Carriers are, however, able to pass the allele onto their offspring, who may then express the genetic if they inherit the recessive allele from both parents. The chance of two carriers having a child with the disease is 25%. This phenomenon is a direct result of the recessive nature of many genes.\n\nQueen Victoria, and her daughters Princesses Alice and Beatrix, were carriers of the X-linked hemophilia gene (an abnormal allele of a gene, necessary to produce one of the blood clotting factors). Both had children who continued to pass on the gene to succeeding generations of the royal houses of Spain and Russia, into which they married. Since males only have one X chromosome, males who carried the altered gene had hemophilia. Females have two X chromosomes, so one copy of an X-linked recessive gene would cause them to be an asymptomatic carrier. These females simply passed it to half of their children.\n\nUp to 1 in 25 individuals of Northern European ancestry may be considered carriers of mutations (CFTR genes) that can lead to cystic fibrosis. The disease appears only when two of these carriers have children, as each pregnancy between them will have a 25% chance of producing a child with the disease. However, it is also thought that carriers of CF may be more resistant to diarrhea during typhoid fever or cholera, and are therefore not truly asymptomatic. This resistance leads to increased fitness of the carriers, known as a heterozygote advantage, and thereby increases the frequency of the altered genes in the population. Although only about 1 of every 3,000 Caucasian newborns has CF, there are more than 900 known mutations of the gene that causes CF. Current tests look for the most common mutations.\n\nGenetic testing can be used to tell if a person carries one or more mutations of the CF gene and how many copies of each mutation. The test looks at a person’s DNA, which is taken from cells in a blood sample or from cells that are gently scraped from inside the mouth.\n\nThe mutations screened by the test vary according to a person's ethnic group or by the occurrence of CF already in the family. More than 10 million Americans, including 1 in 25 Caucasian Americans, are carriers of one mutation of the CF gene. CF is present in other races, though not as frequently as in Caucasian individuals. 1 in 46 Hispanic Americans, 1 in 65 African Americans, and 1 in 90 Asian Americans carry a mutation of the CF gene.\n\nSickle cell anemia is the most common genetic disorder among African Americans in the United States. While approximately 8% are carriers, 1 in 375 African Americans are born with the disease. Carriers are typically asymptomatic, but they may show symptoms at high altitudes or under oxygen-poor environments as in instances of extreme exercise. Carriers are also known to be resistant to malaria, suggesting there is a heterozygote advantage in certain regions of Africa. This is a probable explanation for why the disease is most prevalent among African Americans \n", "id": "16464479", "title": "Genetic carrier"}
{"url": "https://en.wikipedia.org/wiki?curid=21660626", "text": "Cisgenesis\n\nCisgenesis is a product designation for a category of genetically engineered plants. A variety of classification schemes have been proposed that order genetically modified organisms based on the nature of introduced genotypical changes, rather than the process of genetic engineering.\n\nCisgenesis (from \"same\" and \"beginning\") is one term for organisms that have been engineered using a process in which genes are artificially transferred between organisms that could otherwise be conventionally bred. Unlike in transgenesis, genes are only transferred between closely related organisms. However, while future technologies may allow genomes to be directly edited within an individual organism, currently nucleic acid sequences must be isolated and introduced using the same technologies that are used to produce transgenic organisms. The term was first introduced in 2000 by Henk J. Schouten and Henk Jochemsen, and in 2004 a PhD thesis by Jan Schaart of Wageningen University in 2004, discussing making strawberries less susceptible to \"Botrytis cinerea\".\n\nIn Europe, currently, this process is governed by the same laws as transgenesis but researchers at Wageningen University in the Netherlands feel that this should be changed and regulated in the same way as conventionally bred plants. However, other scientists, writing in Nature Biotechnology, have disagreed. In 2012 the European Food Safety Authority (EFSA) issued a report with their risk assessment of cisgenic and intragenic plants. They compared the hazards associated with plants produced by cisgenesis and intragenesis with those obtained either by conventional plant breeding techniques or transgenesis. The EFSA concluded that \"similar hazards can be associated with cisgenic and conventionally bred plants, while novel hazards can be associated with intragenic and transgenic plants.\"\n\nCisgenesis has been applied to transfer of natural resistance genes to the devastating disease \"Phytophthora infestans\" in potato and scab \"(Venturia inaequalis)\" in apple.\n\nCisgenesis and transgenesis use artificial gene transfer, which results in less extensive change to an organism's genome than mutagenesis, which was widely used before genetic engineering was developed.\n\nSome people believe that cisgenesis should not face as much regulatory oversight as genetic modification created through transgenesis as it is possible, if not practical, to transfer alleles among closely related species even by traditional crossing. The primary biological advantage of cisgenesis is that it does not disrupt favorable heterozygous states, particularly in asexually propagated crops such as potato, which do not breed true to seed. One application of cisgenesis is to create blight resistant potato plants by transferring known resistance loci wild genotypes into modern, high yielding varieties.\n\nThe Dutch government has proposed to exclude cisgenic plants from the European GMO Regulation, in view of the safety of cisgenic plants compared to classically bred plants, and their contribution to durable food production.\n\nA related classification scheme proposed by Kaare Nielsen is:\n", "id": "21660626", "title": "Cisgenesis"}
{"url": "https://en.wikipedia.org/wiki?curid=3557327", "text": "Canalisation (genetics)\n\nCanalisation is a measure of the ability of a population to produce the same phenotype regardless of variability of its environment or genotype. It is a form of evolutionary robustness. The term was coined in 1942 by C. H. Waddington to capture the fact that \"developmental reactions, as they occur in organisms submitted to natural selection...are adjusted so as to bring about one definite end-result regardless of minor variations in conditions during the course of the reaction\". He used this word rather than robustness to take into account that biological systems are not robust in quite the same way as, for example, engineered systems.\n\nBiological robustness or canalisation comes about when developmental pathways are shaped by evolution. Waddington introduced the concept of the epigenetic landscape, in which the state of an organism rolls \"downhill\" during development. In this metaphor, a canalised trait is illustrated as a valley (which he called a creode) enclosed by high ridges, safely guiding the phenotype to its \"fate\". Waddington claimed that canals form in the epigenetic landscape during evolution, and that this heuristic is useful for understanding the unique qualities of biological robustness.\n\nWaddington used the concept of canalisation to explain his experiments on genetic assimilation. In these experiments, he exposed \"Drosophila\" pupae to heat shock. This environmental disturbance caused some flies to develop a crossveinless phenotype. He then selected for crossveinless. Eventually, the crossveinless phenotype appeared even without heat shock. Through this process of genetic assimilation, an environmentally induced phenotype had become inherited. Waddington explained this as the formation of a new canal in the epigenetic landscape.\n\nIt is, however, possible to explain genetic assimilation using only quantitative genetics and a threshold model, with no reference to the concept of canalisation. However, theoretical models that incorporate a complex genotype–phenotype map have found evidence for the evolution of phenotypic robustness contributing to genetic assimilation, even when selection is only for developmental stability and not for a particular phenotype, and so the quantitative genetics models do not apply. These studies suggest that the canalisation heuristic may still be useful, beyond the more simple concept of robustness.\n\nNeither canalisation nor robustness are simple quantities to quantify: it is always necessary to specify which trait is canalised (robust) to which perturbations. For example, perturbations can come either from the environment or from mutations. It has been suggested that different perturbations have congruent effects on development taking place on an epigenetic landscape. This could, however, depend on the molecular mechanism responsible for robustness, and be different in different cases.\n\nThe canalisation metaphor suggests that phenotypes are very robust to small perturbations, for which development does not exit the canal, and rapidly returns down, with little effect on the final outcome of development. But perturbations whose magnitude exceeds a certain threshold will break out of the canal, moving the developmental process into uncharted territory. Strong robustness up to a limit, with little robustness beyond, is a pattern that could increase evolvability in a fluctuating environment. Genetic canalisation could allow for evolutionary capacitance, where genetic diversity outside the canal accumulates in a population over time, sheltered from natural selection because it does not normally affect phenotypes. This hidden diversity could then be unleashed by extreme changes in the environment or by molecular switches, releasing previously cryptic genetic variation that can then contribute to a rapid burst of evolution.\n\n", "id": "3557327", "title": "Canalisation (genetics)"}
{"url": "https://en.wikipedia.org/wiki?curid=709360", "text": "Modifications (genetics)\n\nModifications are changes or differences between organisms in the same species that are due to differences in their environment. This is in contrast to mutations, which are changes in the genomes of organisms. Environmental differences that can affect an organism's characteristics (phenotype) include substrate, light, warmth, stress, exercise, and so on. Modifications are typically not heritable, however in some cases epigenetic modifications can be inherited. In both cases, there is no change to the primary DNA sequence (genotype), rather an influence on gene expression which is the cause of the altered phenotype.\n\nIn heredity the genes of the parents are passed on to their offspring unchanged. That is why the organisms which carry the same genotype should be identical in every feature. However, this is not the case. Due to environmental conditions they can vary from each other up to a certain point. There are two types of modifications: the continuous modification and the switching modification.\n\nTo illustrate the modificability you can take a look at our cultivated plants. The harvest of those plants do not only depend on the quality of the seeds but also greatly on environmental factors like the condition of the soil, the nutrient content of the soil, the fertilization, the humidity and temperature as well as the interference of other plants.\n\nIf you take a lowland dandelion and plant half of it in the lowland and the other half in the mountains the result will be a dandelion with big leaves in the lowland and one with small leaves in the mountains. The reason is that in the lowland the environmental conditions are different from those in the mountains.\n", "id": "709360", "title": "Modifications (genetics)"}
{"url": "https://en.wikipedia.org/wiki?curid=771660", "text": "Immediate early gene\n\nImmediate early genes (IEGs) are genes which are activated transiently and rapidly in response to a wide variety of cellular stimuli. They represent a standing response mechanism that is activated at the transcription level in the first round of response to stimuli, before any new proteins are synthesized. Thus IEGs are distinct from \"late response\" genes, which can only be activated later, following the synthesis of early response gene products. Thus IEGs have been called the \"gateway to the genomic response\". The term can describe viral regulatory proteins that are synthesized following viral infection of a host cell, or cellular proteins that are made immediately following stimulation of a resting cell by extracellular signals.\n\nAbout 40 cellular IEGs have been identified so far. The earliest known and best characterized include \"c-fos\", \"c-myc\" and \"c-jun\", genes that were found to be homologous to retroviral oncogenes. Thus IEGs are well known as early regulators of cell growth and differentiation signals. However, other findings suggest roles for IEGs in many other cellular processes.\n\nIn their role as \"gateways to genomic response\", many IEG products are naturally transcription factors or other DNA-binding proteins. However, other important classes of IEG products include secreted proteins, cytoskeletal proteins, and receptor subunits.\n\nSome IEGs such as zif268 and Arc have been implicated in learning and memory and long-term potentiation.\n\n", "id": "771660", "title": "Immediate early gene"}
{"url": "https://en.wikipedia.org/wiki?curid=29027916", "text": "Gene transfer agent\n\nA gene transfer agent (GTA) is a phage-like element produced by several bacteria and archaea that mediates horizontal gene transfer. GTAs package random segments of DNA present in the host bacterium, which can be transduced to a recipient cell. GTAs originated from different viruses have been found in several bacterial and archaeal lineages, such as Alphaproteobacteria, Spirochaetes and methanogenic archaea. \n\nGenetic material brought to the host chromosome by bacteriophages and other mobile elements can be subject to exaptation. The size distribution of prophages seems to follow a bimodal distribution, where one peak represents intact prophages and the second is formed by prophages that underwent rapid decay followed by purifying selection. GTAs are a particular case of exapted, or domesticated, prophages. Using the bacteriophage genes for capsid formation and DNA packaging, they are expressed under the control of the host and mediate horizontal gene transfer between individual cells of the same bacterial population.\n\nThe GTA produced by the alphaproteobacterium \"Rhodobacter capsulatus\", named \"R. capsulatus\" GTA (RcGTA), is currently the best studied GTA. When cultured in laboratory conditions, a subset of the bacterial population induces production of RcGTA upon entry into stationary phase, which are subsequently released from the cells through cell lysis . Most of the RcGTA structural genes are encoded in a ~ 15 kb genetic cluster on the bacterial chromosome. However, other genes required for RcGTA function, such as the genes required for RcGTA release through cell lysis are located separately.\nProduction of RcGTA appears to be controlled by the host cell, because several host systems, including a quorum sensing system and a histidine kinase and response regulator are required for RcGTA transduction. Furthermore, the ability of cells to receive genetic material transduced by RcGTA requires a capsular polysaccharide receptor, which is regulated by the quorum sensing system.\n\nRcGTA-like clusters are found in several alphaproteobacteria. Recently, several members of the order Rhodobacterales have been demonstrated to produce functional RcGTA-like particles. Moreover, groups of genes with homology to the RcGTA are present in the chromosomes of various types of alphaproteobacteria. \n\nAnother known case of a GTA present in alphaproteobacteria is the \"Bartonella\" GTA (BaGTA). The Bartonellaceae do not contain RcGTA homologs, but many of the species within this family contain the BaGTA, a GTA that originated from a different bacteriophage. The BaGTA acts jointly with another group of genes of bacteriophage origin, which cause run-off replication, a phenomenon by which replication starts at a prophage replication origin but does not stop at the usual prophage boundaries. The run-off replication causes the amplification of a chromosome region containing genes such as secretion systems or adhesins. As a consequence, the higher copy number of this region increases the probability that genes encoding host-association factors will be horizontally transferred. This property distinguishes the mechanisms of the BaGTA and the RcGTA, which have been named to be specialist or generalist GTA systems, respectively. The evolutionary patterns caused by the facilitated horizontal transfer caused by the \"Bartonella\" specialist GTA have been suggested to have caused the \"Bartonella\" radiation, in which these bacteria adapted to commensal or pathogenic lifestyles in various groups of mammalian hosts. \n\n", "id": "29027916", "title": "Gene transfer agent"}
{"url": "https://en.wikipedia.org/wiki?curid=1795026", "text": "Genetic marker\n\nA genetic marker is a gene or DNA sequence with a known location on a chromosome that can be used to identify individuals or species. It can be described as a variation (which may arise due to mutation or alteration in the genomic loci) that can be observed. A genetic marker may be a short DNA sequence, such as a sequence surrounding a single base-pair change (single nucleotide polymorphism, SNP), or a long one, like minisatellites.\n\nFor many years, gene mapping was limited to identifying organisms by traditional phenotype markers. This included genes that encoded easily observable characteristics such as blood types or seed shapes. The insufficient number of these types of characteristics in several organisms limited the mapping efforts that could be done. This prompted the development of gene markers which could identify genetic characteristics that are not readily observable in organisms (such as protein variation).\n\nSome commonly used types of genetic markers are:\n\nMolecular genetic markers can be divided into two classes a) biochemical markers which detect variation at the gene product level such as changes in proteins and amino acids and b) molecular markers which detect variation at the DNA level such as nucleotide changes: deletion, duplication, inversion and/or insertion. Markers can exhibit two modes of inheritance, i.e. dominant/recessive or co-dominant. If the genetic pattern of homozygotes can be distinguished from that of heterozygotes, then a marker is said to be co-dominant. Generally co-dominant markers are more informative than the dominant markers.\n\nGenetic markers can be used to study the relationship between an inherited disease and its genetic cause (for example, a particular mutation of a gene that results in a defective protein). It is known that pieces of DNA that lie near each other on a chromosome tend to be inherited together. This property enables the use of a marker, which can then be used to determine the precise inheritance pattern of the gene that has not yet been exactly localized.\nGenetic markers are employed in genealogical DNA testing for genetic genealogy to determine genetic distance between individuals or populations. Uniparental markers (on mitochondrial or Y chromosomal DNA) are studied for assessing maternal or paternal lineages. Autosomal markers are used for all ancestry.\n\nGenetic markers have to be easily identifiable, associated with a specific locus, and highly polymorphic, because homozygotes do not provide any information. Detection of the marker can be direct by RNA sequencing, or indirect using allozymes.\n\nSome of the methods used to study the genome or phylogenetics are RFLP, Amplified fragment length polymorphism (AFLP), RAPD, SSR. They can be used to create genetic maps of whatever organism is being studied.\n\nThere was a debate over what the transmissible agent of CTVT (canine transmissible venereal tumor) was. Many researchers hypothesized that virus like particles were responsible for transforming the cell, while others thought that the cell itself was able to infect other canines as an allograft. With the aid of genetic markers, researchers were able to provide conclusive evidence that the cancerous tumor cell evolved into a transmissible parasite. Furthermore, molecular genetic markers were used to resolve the issue of natural transmission, the breed of origin (phylogenetics), and the age of the canine tumor.\n\nGenetic markers have also been used to measure the genomic response to selection in livestock. Natural and artificial selection leads to a change in the genetic makeup of the cell. The presence of different alleles due to a distorted segregation at the genetic markers is indicative of the difference between selected and non-selected livestock.\n\n\n", "id": "1795026", "title": "Genetic marker"}
{"url": "https://en.wikipedia.org/wiki?curid=21757930", "text": "Particulate inheritance\n\nParticulate inheritance is a pattern of inheritance discovered by Mendelian genetics theorists, such as William Bateson, Ronald Fisher or Gregor Mendel himself, showing that phenotypic traits can be passed from generation to generation through \"discrete particles\" known as genes, which can keep their ability to be expressed while not always appearing in a descending generation.\n\nEarly in the 19th century, scientists had already recognized that Earth has been inhabited by living creatures for a very long time. On the other hand, they did not understand what mechanisms actually drove biological diversity. They also did not understand how physical traits are inherited from one generation to the next. Blending inheritance was the common ideal at the time, but was later discredited by the experiments of Gregor Mendel. Mendel proposed the theory of particulate inheritance by using pea plants (\"Pisum sativum\") to explain how variation can be inherited and maintained over time.\n\n\nSince Mendel used experimental methods to devise his particulate inheritance theory, he developed three basic laws of inheritance: the Law of Segregation, the Law of Independent Assortment, and the Law of Dominance:\n\nMendel's experiment with tall and short pea plants demonstrates how each individual plant has two particles called alleles. When a pea plant produces gametes (reproductive cells), it segregates one allele to each one.\n\nThe law states that when the parents differ from each other in two or more pairs of contrasting characters, the inheritance of one pair of characters is independent to that of the other pair of characters.\n\nIn the pea plants, Mendel observed that the \"T\" allele (dominant) masked the effects of the \"t\" allele (recessive). The terms \"dominant\" and \"recessive\" are used for the masking and the covered allele, respectively. All offspring from this cross are heterozygotes in terms of their genotypes. They also are tall (because the allele for tall masks the allele for short) in terms of their \"phenotype\".\n\nIn a 1918 publication titled \"The Supposition of Mendelian Inheritance Among Close Relatives,\" R.A. Fisher showed that particulate inheritance was capable of generating the vast amount of variation we see among closely related individuals. This helped to reconcile the Biometric and Mendelian schools of thought at the time, and was an important step in the modern synthesis.\n\n", "id": "21757930", "title": "Particulate inheritance"}
{"url": "https://en.wikipedia.org/wiki?curid=14445573", "text": "Extrachromosomal array\n\nAn extrachromosomal array is a method for mosaic analysis in genetics. It is a cosmid, and contains two functioning (wild-type) closely linked genes: a \"gene of interest\" and a \"mosaic marker\". Such an array is injected into germ line cells, which already contain mutant (specifically, loss of function) alleles of all three genes in their chromosomal DNA. The cosmid, which is not packed correctly during mitosis, is occasionally present in only one daughter cell following cell division. The daughter cell containing the array expresses the gene of interest; the cell lacking the array does not.\n\nThe \"mosaic marker\" is a gene which exhibits a visible phenotype change between the functioning and non-functioning alleles. For example, \"ncl-1\", located in chromosomal DNA, exhibits a larger nucleolus than the wild-type allele, which is in the array. Thus, cells which exhibit larger nucleoli have usually not retained the extrachromosomal array.\n\nThe \"gene of interest\" is the target of the mosaic analysis. Cells lacking the extrachromosomal array also lack the functional gene of interest. Cells which develop normally without the array do not require the gene of interest for normal function. Cells which do not develop normally are said to require the gene. In this way, those cell lineages which require a specific gene can be identified.\n\nExtrachromosomal arrays replace an earlier technique involving a duplicated piece of chromosome called a \"free duplication\". The latter technique required that the gene of interest and the mosaic marker be closely linked on the duplication; the former allows free choice of mosaic marker and target gene.\n\nMiller LM, Waring DA, Kim SK (1996). Mosaic analysis using a \"ncl-1\" (+) extrachromosomal array reveals that \"lin-31\" acts in the Pn.p cells during \"Caenorhabditis elegans\" vulval development. \"Genetics 143\" (3): 1181-1191.\n", "id": "14445573", "title": "Extrachromosomal array"}
{"url": "https://en.wikipedia.org/wiki?curid=16017459", "text": "Genetic epidemiology\n\nGenetic epidemiology is the study of the role of genetic factors in determining health and disease in families and in populations, and the interplay of such genetic factors with environmental factors. Genetic epidemiology seeks to derive a statistical and quantitative analysis of how genetics work in large groups.\n\nThe use of the term \"Genetic epidemiology\" emerged in the mid 1980s as a new scientific field.\n\nIn formal language, genetic epidemiology was defined by Newton Morton, one of the pioneers of the field, as \"a science which deals with the etiology, distribution, and control of disease in groups of relatives and with inherited causes of disease in populations\". It is closely allied to both molecular epidemiology and statistical genetics, but these overlapping fields each have distinct emphases, societies and journals.\n\nOne definition of the field closely follows that of behavior genetics, defining genetic epidemiology as \"the scientific discipline that deals with the analysis of the familial distribution of traits, with a view to understanding any possible genetic basis\", and that \"seeks to understand both the genetic and environmental factors and how they interact to produce various diseases and traits in humans\". The adopts a similar definition, \"Genetic epidemiology is the study of the aetiology, distribution, and control of disease in groups of relatives and of inherited causes of disease in populations.\"\n\nAs early as the 4th century BC, Hippocrates suggested in his essay “On Airs, Waters, and Places” that factors such as behavior and environment may play a role in disease. Epidemiology entered a more systematic phase with the work of John Graunt, who in 1662 tried to quantify mortality in London using a statistical approach, tabulating various factors he thought played a role in mortality rates. John Snow is considered to be the father of epidemiology, and was the first to use statistics to discover and target the cause of disease, specifically of cholera outbreaks in 1854 in London. He investigated the cases of cholera and plotted them onto a map identifying the most likely cause of cholera, which was shown to be contaminated water wells.\n\nModern genetics began on the foundation of Gregor Mendels work. Once this became widely known, it spurred a revolution in studies of hereditary throughout the animal kingdom; with studies showing genetic transmission and control over characteristics and traits. As gene variation was shown to affect disease, work began on quantifying factors affecting disease, accelerating in the 20th century. The period since the second world war saw the greatest advancement of the field, with scientists such as Newton Morton helping form the field of genetic epidemiology as it is known today, with the application of modern genetics to the statistical study of disease, as well as the establishment of large-scale epidemiological studies such as the Framingham Heart Study.\n\nIn the 1960s and 1970s, epidemiology played a part in strategies for the worldwide eradication of naturally occurring smallpox.\n\nTraditionally, the study of the role of genetics in disease progresses through the following study designs, each answering a slightly different question:\n\n\nThis traditional approach has proved highly successful in identifying monogenic disorders and locating the genes responsible.\n\nMore recently, the scope of genetic epidemiology has expanded to include common diseases for which many genes each make a smaller contribution (polygenic, multifactorial or multigenic disorders). This has developed rapidly in the first decade of the 21st century following completion of the Human Genome Project, as advances in genotyping technology and associated reductions in cost has made it feasible to conduct large-scale genome-wide association studies that genotype many thousands of single nucleotide polymorphisms in thousands of individuals. These have led to the discovery of many genetic polymorphisms that influence the risk of developing many common diseases.\n\nGenetic epidemiological research follows 3 discreet steps, as outlined by M.Tevfik Dorak:\nThese research methodologies can be assessed through either family or population studies.\n\n\n\n", "id": "16017459", "title": "Genetic epidemiology"}
{"url": "https://en.wikipedia.org/wiki?curid=29344406", "text": "Pseudolinkage\n\nIn genetics, pseudolinkage is a characteristic of a heterozygote for a reciprocal translocation, in which genes located near the translocation breakpoint behave as if they are linked even though they originated on nonhomologous chromosomes.\n\nLinkage is the proximity of two or more markers on a chromosome; the closer together the markers are, the lower the probability that they will be separated by recombination. Genes are said to be linked when the frequency of parental type progeny exceeds that of recombinant progeny.\n\nDuring meiosis in a translocation homozygote, chromosomes segregate normally according to Mendelian principles. Even though the genes have been rearranged during crossover, both haploid sets of chromosomes in the individual have the same rearrangement. As a result, all chromosomes will find a single partner with which to pair at meiosis, and there will be no deleterious consequences for the progeny.\n\nIn translocation heterozygote, however, certain patterns of chromosome segregation during meiosis produce genetically unbalanced gametes that at fertilization become deleterious to the zygote. In a translocation heterozygote, the two haploid sets of chromosomes do not carry the same arrangement of genetic information. As a result, during prophase of the first meiotic division, the translocated chromosomes and their normal homologs assume a crosslike configuration in which four chromosomes, rather than the normal two, pair to achieve a maximum of synapsis between similar regions. We denote the chromosomes carrying translocated material with a T and the chromosomes with a normal order of genes with an N. Chromosomes N1 and T1 have homologous centromeres found in wild type on chromosome 1; N2 and T2 have centromeres found in wild type on chromosome 2.\nDuring anaphase of meiosis I, the mechanisms that attach the spindle to the chromosomes in this crosslike configuration still usually ensure the disjunction of homologous centromeres, bringing homologous chromosomes to opposite spindle poles. Depending on the arrangement of the four chromosomes on the metaphase plate, this normal disjunction of homologos produces one of two equally likely patterns of segregation.\n\nIn the alternate segregation pattern, the two translocation chromosomes (T1 and T2) go to one pole, while the two normal chromosomes (N1 and N2) move to the opposite pole. Both kinds of gametes resulting from this segregation (T1, T2, and N1, N2) carry the correct haploid number of genes; and the zygotes formed by union of these gametes with normal gamete will be viable.\n\nIn the adjacent-1 segregation pattern, homologous centromeres disjoin so that T1 and N2 go to one pole, while the N1 and T2 go to the opposite pole. Consequently, each gamete contains a large duplication (of the region found in both the normal and the translocated chromosome in that gamete) and a correspondingly large deletion (of the region found in neither of the chromosomes in that gametes), which make them genetically unbalanced. Zygotes formed by union of these gametes with a normal gametes are usually not viable.\n\nBecause of the unusual cruciform pairing configuration in translocation heterozygotes, nondisjunction of homologous centromeres occurs at a measurable but low rate. This nondisjunction produces an adjacent-2 segregation pattern in which the homologous centromeres N1 and T1 go to the same spindle pole while the homologous centromeres N2 and T2 go to the other spindle pole. The resulting genetic imbalances are lethal after fertilization to the zygotes containing them.\n\nThus, in a translocation heterozygote, only the alternate segregation pattern yields viable progeny in outcrosses, the equally likely adjacent-1 pattern and the rare adjacent-2 pattern do not.\nBecause of this, genes near the translocation breakpoints on the nonhomologous chromosomes participating in a reciprocal translocation exhibit pseudolinkage: They behave as if they are linked.\n\n", "id": "29344406", "title": "Pseudolinkage"}
{"url": "https://en.wikipedia.org/wiki?curid=21505", "text": "Nucleotide\n\nNucleotides are organic molecules that serve as the monomer units for forming the nucleic acid polymers deoxyribonucleic acid (DNA) and ribonucleic acid (RNA), both of which are essential biomolecules in all life-forms on Earth. Nucleotides are the building blocks of nucleic acids; they are composed of three subunit molecules: a nitrogenous base, a five-carbon sugar (ribose or deoxyribose), and at least one phosphate group. They are also known as \"phosphate\" nucleotides.\n\nA nucleoside is a nitrogenous base and a 5-carbon sugar. Thus a nucleoside plus a phosphate group yields a nucleotide.\n\nNucleotides also play a central role in life-form metabolism at the fundamental, cellular level. They carry packets of chemical energy—in the form of the nucleoside triphosphates ATP, GTP, CTP and UTP—throughout the cell to the many cellular functions that demand energy, which include synthesizing amino acids, proteins and cell membranes and parts; moving the cell and moving cell parts, both internally and intercellularly; dividing the cell, etc. In addition, nucleotides participate in cell signaling (cGMP and cAMP), and are incorporated into important cofactors of enzymatic reactions (e.g. coenzyme A, FAD, FMN, NAD, and NADP).\n\nIn experimental biochemistry, nucleotides can be radiolabeled with radionuclides to yield radionucleotides.\n\nA nucleotide is composed of three distinctive chemical sub-units: a five-carbon sugar molecule, a nitrogenous base—which two together are called a nucleoside—and one phosphate group. With all three joined, a nucleotide is also termed a \"nucleoside \"mono\"phosphate\". The chemistry sources ACS Style Guide and IUPAC Gold Book prescribe that a nucleotide should contain only one phosphate group, but common usage in molecular biology textbooks often extends the definition to include molecules with two, or with three, phosphates. Thus, the terms \"nucleoside \"di\"phosphate\" or \"nucleoside \"tri\"phosphate\" may also indicate nucleotides.\n\nNucleotides contain either a purine or a pyrimidine base—i.e., the nitrogenous base molecule, also known as a nucleobase—and are termed \"ribo\"nucleotides if the sugar is ribose, or \"deoxyribo\"nucleotides if the sugar is deoxyribose. Individual phosphate molecules repetitively connect the sugar-ring molecules in two adjacent nucleotide monomers, thereby connecting the nucleotide monomers of a nucleic acid end-to-end into a long chain. These chain-joins of sugar and phosphate molecules create a 'backbone' strand for a single- or double helix. In any one strand, the chemical orientation (directionality) of the chain-joins runs from the 5'-end to the 3'-end (\"read\": 5 prime-end to 3 prime-end)—referring to the five carbon sites on sugar molecules in adjacent nucleotides. In a double helix, the two strands are oriented in opposite directions, which permits base pairing and complementarity between the base-pairs, all which is essential for replicating or transcribing the encoded information found in DNA.\n\nUnlike in nucleic acid nucleotides, singular cyclic nucleotides are formed when the phosphate group is bound twice to the same sugar molecule, i.e., at the corners of the sugar hydroxyl groups. These individual nucleotides function in cell metabolism rather than the nucleic acid structures of long-chain molecules.\n\nNucleic acids then are polymeric macromolecules assembled from nucleotides, the monomer-units of nucleic acids. The purine bases adenine and guanine and pyrimidine base cytosine occur in both DNA and RNA, while the pyrimidine bases thymine (in DNA) and uracil (in RNA) in just one. Adenine forms a base pair with thymine with two hydrogen bonds, while guanine pairs with cytosine with three hydrogen bonds.\n\nNucleotides can be synthesized by a variety of means both in vitro and in vivo.\n\nIn vivo, nucleotides can be synthesized de novo or recycled through salvage pathways. The components used in de novo nucleotide synthesis are derived from biosynthetic precursors of carbohydrate and amino acid metabolism, and from ammonia and carbon dioxide. The liver is the major organ of de novo synthesis of all four nucleotides. De novo synthesis of pyrimidines and purines follows two different pathways. Pyrimidines are synthesized first from aspartate and carbamoyl-phosphate in the cytoplasm to the common precursor ring structure orotic acid, onto which a phosphorylated ribosyl unit is covalently linked. Purines, however, are first synthesized from the sugar template onto which the ring synthesis occurs. For reference, the syntheses of the purine and pyrimidine nucleotides are carried out by several enzymes in the cytoplasm of the cell, not within a specific organelle. Nucleotides undergo breakdown such that useful parts can be reused in synthesis reactions to create new nucleotides.\n\nIn vitro, protecting groups may be used during laboratory production of nucleotides. A purified nucleoside is protected to create a phosphoramidite, which can then be used to obtain analogues not found in nature and/or to synthesize an oligonucleotide.\n\nThe synthesis of the pyrimidines CTP and UTP occurs in the cytoplasm and starts with the formation of carbamoyl phosphate from glutamine and CO. Next, aspartate carbamoyltransferase catalyzes a condensation reaction between aspartate and carbamoyl phosphate to form carbamoyl aspartic acid, which is cyclized into 4,5-dihydroorotic acid by dihydroorotase. The latter is converted to orotate by dihydroorotate oxidase. The net reaction is:\n\nOrotate is covalently linked with a phosphorylated ribosyl unit. The covalent linkage between the ribose and pyrimidine occurs at position C of the ribose unit, which contains a pyrophosphate, and N of the pyrimidine ring. Orotate phosphoribosyltransferase (PRPP transferase) catalyzes the net reaction yielding orotidine monophosphate (OMP):\n\nOrotidine 5'-monophosphate is decarboxylated by orotidine-5'-phosphate decarboxylase to form uridine monophosphate (UMP). PRPP transferase catalyzes both the ribosylation and decarboxylation reactions, forming UMP from orotic acid in the presence of PRPP. It is from UMP that other pyrimidine nucleotides are derived. UMP is phosphorylated by two kinases to uridine triphosphate (UTP) via two sequential reactions with ATP. First the diphosphate form UDP is produced, which in turn is phosphorylated to UTP. Both steps are fueled by ATP hydrolysis:\n\nCTP is subsequently formed by amination of UTP by the catalytic activity of CTP synthetase. Glutamine is the NH donor and the reaction is fueled by ATP hydrolysis, too:\n\nCytidine monophosphate (CMP) is derived from cytidine triphosphate (CTP) with subsequent loss of two phosphates.\n\nThe atoms that are used to build the purine nucleotides come from a variety of sources: \n\nThe de novo synthesis of purine nucleotides by which these precursors are incorporated into the purine ring proceeds by a 10-step pathway to the branch-point intermediate IMP, the nucleotide of the base hypoxanthine. AMP and GMP are subsequently synthesized from this intermediate via separate, two-step pathways. Thus, purine moieties are initially formed as part of the ribonucleotides rather than as free bases.\n\nSix enzymes take part in IMP synthesis. Three of them are multifunctional:\n\nThe pathway starts with the formation of PRPP. PRPS1 is the enzyme that activates R5P, which is formed primarily by the pentose phosphate pathway, to PRPP by reacting it with ATP. The reaction is unusual in that a pyrophosphoryl group is directly transferred from ATP to C of R5P and that the product has the α configuration about C1. This reaction is also shared with the pathways for the synthesis of Trp, His, and the pyrimidine nucleotides. Being on a major metabolic crossroad and requiring much energy, this reaction is highly regulated.\n\nIn the first reaction unique to purine nucleotide biosynthesis, PPAT catalyzes the displacement of PRPP's pyrophosphate group (PP) by an amide nitrogen donated from either glutamine (N), glycine (N&C), aspartate (N), folic acid (C), or CO. This is the committed step in purine synthesis. The reaction occurs with the inversion of configuration about ribose C, thereby forming β-5-phosphorybosylamine (5-PRA) and establishing the anomeric form of the future nucleotide.\n\nNext, a glycine is incorporated fueled by ATP hydrolysis and the carboxyl group forms an amine bond to the NH previously introduced. A one-carbon unit from folic acid coenzyme N-formyl-THF is then added to the amino group of the substituted glycine followed by the closure of the imidazole ring. Next, a second NH group is transferred from a glutamine to the first carbon of the glycine unit. A carboxylation of the second carbon of the glycin unit is concomittantly added. This new carbon is modified by the additional of a third NH unit, this time transferred from an aspartate residue. Finally, a second one-carbon unit from formyl-THF is added to the nitrogen group and the ring covalently closed to form the common purine precursor inosine monophosphate (IMP).\n\nInosine monophosphate is converted to adenosine monophosphate in two steps. First, GTP hydrolysis fuels the addition of aspartate to IMP by adenylosuccinate synthase, substituting the carbonyl oxygen for a nitrogen and forming the intermediate adenylosuccinate. Fumarate is then cleaved off forming adenosine monophosphate. This step is catalyzed by adenylosuccinate lyase.\n\nInosine monophosphate is converted to guanosine monophosphate by the oxidation of IMP forming xanthylate, followed by the insertion of an amino group at C. NAD is the electron acceptor in the oxidation reaction. The amide group transfer from glutamine is fueled by ATP hydrolysis.\n\nIn humans, pyrimidine rings (C, T, U) can be degraded completely to CO and NH (urea excretion). That having been said, purine rings (G, A) cannot. Instead they are degraded to the metabolically inert uric acid which is then excreted from the body. Uric acid is formed when GMP is split into the base guanine and ribose. Guanine is deaminated to xanthine which in turn is oxidized to uric acid. This last reaction is irreversible. Similarly, uric acid can be formed when AMP is deaminated to IMP from which the ribose unit is removed to form hypoxanthine. Hypoxanthine is oxidized to xanthine and finally to uric acid. Instead of uric acid secretion, guanine and IMP can be used for recycling purposes and nucleic acid synthesis in the presence of PRPP and aspartate (NH donor).\n\nAn unnatural base pair (UBP) is a designed subunit (or nucleobase) of DNA which is created in a laboratory and does not occur in nature. In 2012, a group of American scientists led by Floyd Romesberg, a chemical biologist at the Scripps Research Institute in San Diego, California, published that his team designed an unnatural base pair (UBP). The two new artificial nucleotides or \"Unnatural Base Pair\" (UBP) were named d5SICS and dNaM. More technically, these artificial nucleotides bearing hydrophobic nucleobases, feature two fused aromatic rings that form a (d5SICS–dNaM) complex or base pair in DNA. In 2014 the same team from the Scripps Research Institute reported that they synthesized a stretch of circular DNA known as a plasmid containing natural T-A and C-G base pairs along with the best-performing UBP Romesberg's laboratory had designed, and inserted it into cells of the common bacterium \"E. coli\" that successfully replicated the unnatural base pairs through multiple generations. This is the first known example of a living organism passing along an expanded genetic code to subsequent generations. This was in part achieved by the addition of a supportive algal gene that expresses a nucleotide triphosphate transporter which efficiently imports the triphosphates of both d5SICSTP and dNaMTP into \"E. coli\" bacteria. Then, the natural bacterial replication pathways use them to accurately replicate the plasmid containing d5SICS–dNaM.\n\nThe successful incorporation of a third base pair is a significant breakthrough toward the goal of greatly expanding the number of amino acids which can be encoded by DNA, from the existing 21 amino acids to a theoretically possible 172, thereby expanding the potential for living organisms to produce novel proteins. The artificial strings of DNA do not encode for anything yet, but scientists speculate they could be designed to manufacture new proteins which could have industrial or pharmaceutical uses.\n\nNucleotide (abbreviated \"nt\") is a common unit of length for single-stranded nucleic acids, similar to how base pair is a unit of length for double-stranded nucleic acids.\n\nA study done by the Department of Sports Science at the University of Hull in Hull, UK has shown that nucleotides have significant impact on cortisol levels in saliva. Post exercise, the experimental nucleotide group had lower cortisol levels in their blood than the control or the placebo. Additionally, post supplement values of Immunoglobulin A were significantly higher than either the placebo or the control. The study concluded, \"nucleotide supplementation blunts the response of the hormones associated with physiological stress.\"\n\nAnother study conducted in 2013 looked at the impact nucleotide supplementation had on the immune system in athletes. In the study, all athletes were male and were highly skilled in taekwondo. Out of the twenty athletes tested, half received a placebo and half received 480 mg per day of nucleotide supplement. After thirty days, the study concluded that nucleotide supplementation may counteract the impairment of the body's immune function after heavy exercise.\n\nThe IUPAC has designated the symbols for nucleotides. Apart from the five (A, G, C, T/U) bases, often degenerate bases are used especially for designing PCR primers. These nucleotide codes are listed here. Some primer sequences may also include the character \"I\", which codes for the non-standard nucleotide inosine. Inosine occurs in tRNAs, and will pair with adenine, cytosine, or thymine. This character does not appear in the following table however, because it does not represent a degeneracy. While inosine can serve a similar function as the degeneracy \"D\", it is an actual nucleotide, rather than a representation of a mix of nucleotides that covers each possible pairing needed.\n\n", "id": "21505", "title": "Nucleotide"}
{"url": "https://en.wikipedia.org/wiki?curid=29481838", "text": "Genetic imbalance\n\nGenetic imbalance is to describe situation when the genome of a cell or organism has more copies of some genes than other genes due to chromosomal rearrangements or aneuploidy.\nChanges in gene dosage, the number of times a given gene is present in the cell nucleus, can create a genetic imbalance.\n\nThis imbalance in gene dosage alters the amount of a particular protein relative to all other proteins, and this alternation in the relative amounts of protein can have a variety of phenotypic effects. These effects are depending on how the proteins function and how critical the maintenance of a precise ratio of proteins is to the survival of the organism.\n\nDiminishing the dosage of most genes produces no obvious change in phenotype. For some genes the phenotypic consequences of a decrease in gene dosage are noticeable but not catastrophic. For example, Drosophila containing only one copy of the wild type Nocth gene have visible wing abnormalities but otherwise seem to function normally. For some rare genes, the normal diploid level of gene expression is essential to individual survival; fewer than two copies of such a gene results in lethality. In Drosophila, a single dose of the locus known as Triplolethal is in an otherwise diploid individual.\n\nAlthough a single dose of any gene may not cause substantial harm to the individual, the genetic imbalance resulting from a single dose of many genes at the same time can be lethal. Humans, for example, cannot survive, even as heterozygotes, with deletions that remove more than about 3% of any part of their haploid genome.\n\n", "id": "29481838", "title": "Genetic imbalance"}
{"url": "https://en.wikipedia.org/wiki?curid=3973644", "text": "Homogeneously staining region\n\nHomogeneously staining regions (HSRs) are chromosomal segments with various lengths and uniform staining intensity after G banding. This type of aberration is also known as Copy Number Gains or Amplification.\n\nAn HSR is one type of change in a chromosome's structure which is frequently observed in the nucleus of human cancer cells. In the region of a chromosome where an HSR occurs, a segment of the chromosome, which presumably contains a gene or genes that give selective advantage to the progression of the cancer, is amplified or duplicated many times. As a result of the duplication this chromosomal segment is greatly lengthened and expanded such that when it is stained with a fluorescent probe specific to the region (Fluorescent in situ hybridization), rather than causing a focal fluorescent signal as in a normal chromosome, the probe \"paints\" a broad fluorescent signal over the whole of the amplified region. It is because of the appearance of this broadly staining region that this chromosomal abnormality was named a homogeneously staining region.\n\n", "id": "3973644", "title": "Homogeneously staining region"}
{"url": "https://en.wikipedia.org/wiki?curid=14271172", "text": "KMT2A\n\nHistone-lysine N-methyltransferase 2A also known as acute lymphoblastic leukemia 1 (ALL-1), myeloid/lymphoid or mixed-lineage leukemia 1 (MLL1), or zinc finger protein HRX (HRX) is an enzyme that in humans is encoded by the KMT2A gene.\n\nMLL1 is a histone methyltransferase deemed a positive global regulator of gene transcription. This protein belongs to the group of histone-modifying enzymes comprising transactivation domain 9aaTAD and is involved in the epigenetic maintenance of transcriptional memory. Its role as an epigenetic regulator of neuronal function is an ongoing area of research.\n\nKMT2A gene encodes a transcriptional coactivator that plays an essential role in regulating gene expression during early development and hematopoiesis. The encoded protein contains multiple conserved functional domains. One of these domains, the SET domain, is responsible for its histone H3 lysine 4 (H3K4) methyltransferase activity which mediates chromatin modifications associated with epigenetic transcriptional activation. Enriched in the nucleus, the MLL1 enzyme trimethylates H3K4 (H3K4me3). It also upregulates mono- and dimethylation of H3K4. This protein is processed by the enzyme Taspase 1 into two fragments, MLL-C (~180 kDa) and MLL-N (~320 kDa). These fragments then assemble into different multi-protein complexes that regulate the transcription of specific target genes, including many of the HOX genes.\n\nTranscriptome profiling after deletion of MLL1 in cortical neurons revealed decreased promoter-bound H3K4me3 peaks at 318 genes, with 31 of these having significantly decreased expression and promoter binding. Among them were \"Meis2\", a homeobox transcription factor critical for development of forebrain neurons and \"Satb2\", a protein involved in neuronal differentiation.\n\nMultiple chromosomal translocations involving this gene are the cause of certain acute lymphoid leukemias and acute myeloid leukemias. Alternate splicing results in multiple transcript variants.\n\nMLL1 has been shown to be an important epigenetic regulator of complex behaviors. Rodent models of MLL1 dysfunction in forebrain neurons showed that conditional deletion results in elevated anxiety and defective cognition. Interestingly, prefrontal cortex-specific knockout of MLL1 results in the same phenotypes, as well as working memory deficits.\n\nMLL1 has been found to be an important regulator of epiblast-derived stem cells, post-implantation epiblast derived stem cells which display pluripotency yet many recognizable differences from the traditional embryonic stem cells derived from inner cell mass prior to implantation. Suppression of MLL1 expression was shown to be adequate for inducing ESC-like morphology and behavior within 72 hours of treatment. It has been proposed that that the small molecule inhibitor MM-401, which was used to inhibit MLL1, changes the distribution of H3K4me1, the single methylation of the histone H3 lysine 4, to be significantly downregulated at MLL1 targets thus leading to decreased expression of MLL1 targets, rather than a direct regulation of pluripotency core markers.\n\nKMT2A gene has 37 exons and resides on chromosome 11 at q23.\n\nKMT2A has over a dozen of binding partners and is cleaved into two pieces, a larger N-terminal fragment, involved in gene repression, and a smaller C-terminal fragment, which is a transcriptional activator. The cleavage, followed by the association of the two fragments, is necessary for KMT2A to be fully active. Like many other methyltransferases, the KMT2 family members exist in multisubunit nuclear complexes (human COMPASS), where other subunits also mediate the enzymatic activity. The 9aaTAD transactivation domains of E proteins and MLL are very similar and both bind to the KIX domain of general transcriptional mediator CBP.\n\nInterestingly, abnormal H3K4 trimethylation has been implicated in several neurological disorders such as autism. Humans with cognitive and neurodevelopmental disease often have dysregulation of H3K4 methylation in prefrontal cortex (PFC) neurons. It also may participate in the process of GAD67 downregulation in schizophrenia.\n\nRearrangements of the MLL1 gene are associated with aggressive acute leukemias, both lymphoblastic and myeloid. Despite being an aggressive leukemia, the MLL1 rearranged sub-type had the lowest mutation rates reported for any cancer.\n\nMutations in MLL1 cause and Acute lymphoblastic leukemia. The leukemia cells of up to 80 percent of infants with ALL-1 have a chromosomal rearrangement that fuses the MLL1 gene to a gene on a different chromosome.\n\nMLL (gene) has been shown to interact with:\n\n\n", "id": "14271172", "title": "KMT2A"}
{"url": "https://en.wikipedia.org/wiki?curid=29583215", "text": "Cotransformation\n\nCotransformation is the simultaneous transformation of two or more genes.\n\nOnly genes in the same chromosomal vicinity can be transformed; the closer together the genes lie, the more frequently they will be cotransformed. By contrast, genes sufficiently far apart that they cannot appear together on a fragment of foreign DNA will almost never be cotransformed, because transformation is so inefficient that recipient cells usually take up only a single DNA.\n\nIn one study of natural transformation, investigators isolated \"B. subtilis\" bacteria with two mutations—\"trpC2\" and \"hisB2\"—that made them Trp- , His- auxotrophs. These double auxotrophs served as the recipient in the study, wild-type cells (Trp+ , His+ ) were the donors. In this study, the numbers of Trp+ and His+ transformants were equal. Further tests showed that 40 of every 100 Trp+ transferred colonies were also His+. Similarly, tests of the His+ transformants showed that roughly 40% are also Trp+ . Thus, in 40% of the analyzed colonies, the \"trpC+\" and \"hisB+\" genes had been cotransformed.\n\nSince during transformation, donor DNA replaces only a small percentage of the recipient’s chromosome, why are the two \"B.subtilis\" genes cotransformed with such high frequency? Because the \"trpC\" and \"hisB\" genes lie very close together on the chromosome and are thus genetically linked. Although the donor chromosome is fragmented into small pieces of about 20 kb during its extraction for the transformation process, the wild-type \"trpC+\" and \"hisB+\" alleles are so close that they often appear in the same donor DNA molecule. Sequence analysis shows that \"trpC\" and \"hisB\" genes are only about 7 kb apart.\n\n", "id": "29583215", "title": "Cotransformation"}
{"url": "https://en.wikipedia.org/wiki?curid=29591222", "text": "DNA annotation\n\nDNA annotation or genome annotation is the process of identifying the locations of genes and all of the coding regions in a genome and determining what those genes do. An annotation (irrespective of the context) is a note added by way of explanation or commentary. Once a genome is sequenced, it needs to be annotated to make sense of it.\n\nFor DNA annotation, a previously unknown sequence representation of genetic material is enriched with information relating genomic position to intron-exon boundaries, regulatory sequences, repeats, gene names and protein products. This annotation is stored in genomic databases such as Mouse Genome Informatics, FlyBase, and WormBase. Educational materials on some aspects of biological annotation from the 2006 Gene Ontology annotation camp and similar events are available at the Gene Ontology website.\n\nThe National Center for Biomedical Ontology (www.bioontology.org) develops tools for automated annotation of database records based on the textual descriptions of those records.\n\nAs a general method, dcGO has an automated procedure for statistically inferring associations between ontology terms and protein domains or combinations of domains from the existing gene/protein-level annotations.\n\nGenome annotation consists of three main steps:.\n\nAutomatic annotation tools try to perform all this by computer analysis, as opposed to manual annotation (a.k.a. curation) which involves human expertise. Ideally, these approaches co-exist and complement each other in the same annotation pipeline.\n\nThe simpliest way to perform gene annotation relies on homology based search tools, like BLAST, to search for homologous genes in specific databases, the resulting information is then used to annotate genes and genomes. However, nowadays more and more additional information is added to the annotation platform. The additional information allows manual annotators to deconvolute discrepancies between genes that are given the same annotation. Some databases use genome context information, similarity scores, experimental data, and integrations of other resources to provide genome annotations through their Subsystems approach. Other databases (e.g. Ensembl) rely on both curated data sources as well as a range of different software tools in their automated genome annotation pipeline.\n\n\"Structural annotation\" consists of the identification of genomic elements.\n\n\"Functional annotation\" consists of attaching biological information to genomic elements.\n\nThese steps may involve both biological experiments and \"in silico\" analysis. Proteogenomics based approaches utilize information from expressed proteins, often derived from mass spectrometry, to improve genomics annotations.\n\nA variety of software tools have been developed to permit scientists to view and share genome annotations.\n\nGenome annotation remains a major challenge for scientists investigating the human genome, now that the genome sequences of more than a thousand human individuals and several model organisms are largely complete. Identifying the locations of genes and other genetic control elements is often described as defining the biological \"parts list\" for the assembly and normal operation of an organism. Scientists are still at an early stage in the process of delineating this parts list and in understanding how all the parts \"fit together\".\n\nGenome annotation is an active area of investigation and involves a number of different organizations in the life science community which publish the results of their efforts in publicly available biological databases accessible via the web and other electronic means. Here is an alphabetical listing of on-going projects relevant to genome annotation:\n\nAt Wikipedia, genome annotation has started to become automated under the auspices of the which operates a bot that harvests gene data from research databases and creates gene stubs on that basis.\n", "id": "29591222", "title": "DNA annotation"}
{"url": "https://en.wikipedia.org/wiki?curid=8421442", "text": "Trans-acting\n\nIn the field of molecular biology, trans-acting (trans-regulatory, trans-regulation), in general, means \"acting from a different molecule\" (\"i.e.\", intermolecular). It may be considered the opposite of cis-acting (cis-regulatory, cis-regulation), which, in general, means \"acting from the same molecule\" (\"i.e.\", intramolecular).\n\nIn the context of transcription regulation, a trans-acting element is usually a DNA sequence that contains a gene. This gene codes for a protein (or microRNA or other diffusible molecule) that will be used in the regulation of another target gene.\nThe trans-acting gene may be on the same chromosome as the target gene, but the activity is via the intermediary protein or RNA that it encodes. Cis-acting elements, on the other hand, do not code for protein or RNA. Both the trans-acting gene and the protein/RNA that it encodes are said to \"act in trans\" on the target gene.\n\n", "id": "8421442", "title": "Trans-acting"}
{"url": "https://en.wikipedia.org/wiki?curid=24840027", "text": "Diversity Arrays Technology\n\nDiversity Arrays Technology (DArT) is the name of a technology used in molecular genetics to develop sequence markers for genotyping and other genetic analysis.\n\nDArT is based on microarray hybridizations that detect the presence versus absence of individual fragments in genomic representations. The technology has significant advantages over other array based Single-nucleotide polymorphism detection technologies in the analysis of polyploid plants.\n\n", "id": "24840027", "title": "Diversity Arrays Technology"}
{"url": "https://en.wikipedia.org/wiki?curid=29726014", "text": "Genetic gain\n\n\"Genetic gain\" is the amount of increase in performance that is achieved through artificial genetic improvement programs. This is usually used to refer to the increase after one generation has passed.\n", "id": "29726014", "title": "Genetic gain"}
{"url": "https://en.wikipedia.org/wiki?curid=18866828", "text": "Obligate carrier\n\nAn obligate carrier is an individual who may be clinically unaffected but who must carry a gene mutation based on analysis of the family history; usually applies to disorders inherited in an autosomal recessive and X-linked recessive manner.\n\nIn X-linked recessive disorders, only females can be the carriers of the recessive mutation, making them obligate carriers of this type of disease. Females acquire one X-chromosome from their father and one from their mother, and this means they can either be heterozygous for the mutated allele or homozygous. If heterozygous, she is a carrier of the mutated allele because the disease is recessive. If homozygous, she has the disease. An affected father with an X-linked recessive trait will always pass the trait on to the daughter. Therefore, all daughters of an affected male are obligate carriers. On the other hand, a carrier mother has a 50% chance of passing her mutated X-chromosome to the daughter. This makes all daughters of carrier mothers possible carriers but not necessarily obligate carriers. Males cannot be obligate or possible carriers of X-linked recessive traits because they only have one X-chromosome, and so are always phenotypically affected when receiving the mutated X-chromosome from their mother.\n\nFemales that are heterozygous for X-linked recessive disorders are obligate carriers, but can never be phenotypically affected, and this is because of X-inactivation. Heterozygous females have an X-chromosome from each parent; one with a mutated gene and one with a functional copy of the same gene. When the mutated chromosome is randomly inactivated in order to maintain the copy number, presence of the functional copy results in a normal phenotype. Males only have one copy of any gene on the X-chromosome, and because they do not undergo X-inactivation, they only have the mutated gene. As a result, these types of diseases most commonly phenotypically affect males and rarely females.\n\nHemophilia, or haemophilia, is an X-linked recessive disorder that impairs the body's control over blood clotting. Haemophilia A and Haemophilia B arise from mutations in the genes for factor VIII and factor IX, respectively. Females with this disease are almost exclusively unaffected, obligate carriers. The mutations can be passed on to offspring by mothers and fathers, but the phenotype is only expressed in males that inherit the mutation. All daughters of a hemophiliac father are obligate carriers of the disease.\n\nIn an autosomal recessive disease, if an individual is heterozygous for the mutant allele, they are a carrier because the disease is recessive. If homozygous, they have the disease. All offspring of an affected individual are either heterozygous or homozygous for the mutated allele. Consequently, all unaffected (heterozygous) offspring of an affected individual are obligate carriers of the disease because they will necessarily carry the mutated allele.\n\nDue to the predictable patterns of heritable disorders, techniques can be used to detect past, present, and future disease prevalence in individuals among a family. Specifically, pedigrees and laboratory methods are used to search for and predict obligate carriers for a specific disease such as hemophilia. After analysis of family history, one way to be completely sure that an individual is an obligate carrier is through genetic tests, such as mutational analysis. This allows professionals to see if the specific mutation exists in the chromosome of the individual. In potential hemophiliacs, factor assays are used to measure the amount of blood clotting in an individual. However, some carriers might have completely normal clotting levels and so this method is not always useful. Genetic counselling informs patients that may have a family history of a certain disease about their risk of disease and potential risk in their children.\n", "id": "18866828", "title": "Obligate carrier"}
{"url": "https://en.wikipedia.org/wiki?curid=2491663", "text": "Endoreduplication\n\nEndoreduplication (also referred to as endoreplication or endocycling) is replication of the nuclear genome in the absence of mitosis, which leads to elevated nuclear gene content and polyploidy. Endoreplication can be understood simply as a variant form of the mitotic cell cycle (G1-S-G2-M) in which mitosis is circumvented entirely, due to modulation of cyclin-dependent kinase (CDK) activity. Examples of endoreplication characterized in arthropod, mammalian, and plant species suggest that it is a universal developmental mechanism responsible for the differentiation and morphogenesis of cell types that fulfill an array of biological functions. While endoreplication is often limited to specific cell types in animals, it is considerably more widespread in plants, such that polyploidy can be detected in the majority of plant tissues.\n\nEndoreplicating cell types that have been studied extensively in model organisms\nEndoreplication, endomitosis and polytenization are three somewhat different processes resulting in polyploidization of a cell in a regulated manner. In endoreplication cells skip M phase completely, resulting in a mononucleated polyploid cell. Endomitosis is a type of cell cycle variation where mitosis is initiated, but some of the processes are not completed. Depending on how far the cell progresses through mitosis, this will give rise to a mononucleated or binucleated polyploid cell. Polytenization arises with under- or overamplification of some genomic regions, creating polytene chromosomes.\n\nBased on the wide array of cell types in which endoreplication occurs, a variety of hypotheses have been generated to explain the functional importance of this phenomenon. Unfortunately, experimental evidence to support these conclusions is somewhat limited:\n\nCell/Organism Size:\nCell ploidy often correlates with cell size, and in some instances, disruption of endoreplication results in diminished cell and tissue size suggesting that endoreplication may serve as a mechanism for tissue growth. Relative to mitosis, endoreplication does not require cytoskeletal rearrangement or the production of new cell membrane and it often occurs in cells that have already differentiated. As such it may represent an energetically efficient alternative to cell proliferation among differentiated cell types that can no longer afford to undergo mitosis. While evidence establishing a connection between ploidy and tissue size is prevalent in the literature, contrary examples also exist.\n\nCell Differentiation:\nIn developing plant tissues the transition from mitosis to endoreplication often coincides with cell differentiation and morphogenesis. However it remains to be determined whether endoreplication and polypoidy contribute to cell differentiation or vice versa. Interestingly, targeted inhibition of endoreplication in trichome progenitors results in the production of multicellular trichomes that exhibit relatively normal morphology, but ultimately dedifferentiate and undergo absorption into the leaf epidermis. This result suggests that endoreplication and polyploidy may be required for the maintenance of cell identity.\n\nOogenesis and Embryonic Development:\nEndoreplication is commonly observed in cells responsible for the nourishment and protection of oocytes and embryos. It has been suggested that increased gene copy number might allow for the mass production of proteins required to meet the metabolic demands of embryogenesis and early development. Consistent with this notion, mutation of the Myc oncogene in \"Drosophila\" follicle cells results in reduced endoreplication and abortive oogenesis. However, reduction of endoreplication in maize endosperm has limited effect on the accumulation of starch and storage proteins, suggesting that the nutritional requirements of the developing embryo may involve the nucleotides that comprise the polyploid genome rather than the proteins it encodes.\n\nBuffering the Genome:\nAnother intriguing hypothesis is that endoreplication buffers against DNA damage and mutation because it provides extra copies of important genes. However, this notion is purely speculative and there is limited evidence to the contrary. For example, analysis of polyploid yeast strains suggests that they are more sensitive to radiation than diploid strains.\n\nStress Response:\nResearch in plants suggests that endoreplication may also play a role in modulating stress responses. By manipulating expression of E2fe (a repressor of endocycling in plants), researchers were able to demonstrate that increased cell ploidy lessens the negative impact of drought stress on leaf size. Given that the sessile lifestyle of plants necessitates a capacity to adapt to environmental conditions, it is appealing to speculate that widespread polyploidization contributes to their developmental plasticity\n\nThe best-studied example of a mitosis-to-endocycle transition occurs in \"Drosophila\" follicle cells and is activated by Notch signaling. Entry into endocycles involves modulation of mitotic and S-phase cyclin-dependent kinase (CDK) activity. Inhibition of M-phase CDK activity is accomplished via transcriptional activation of Cdh/fzr and repression of the G2-M regulator string/cdc25. Cdh/fzr is responsible for activation of the anaphase-promoting complex (APC) and subsequent proteolysis of the mitotic cyclins. String/cdc25 is a phosphatase that stimulates mitotic cyclin-CDK complex activity. Upregulation of S-phase CDK activity is accomplished via transcriptional repression of the inhibitory kinase dacapo. Together, these changes allow for the circumvention of mitotic entry, progression through G1, and entry into S-phase. The induction of endomitosis in mammalian megakaryocytes involves activation of the c-mpl receptor by the thrombopoietin (TPO) cytokine and is mediated by ERK1/2 signaling. As with Drosophila follicle cells, endoreplication in megakaryocytes results from activation of S-phase cyclin-CDK complexes and inhibition of mitotic cyclin-CDK activity.\n\nEntry into S-phase during endoreplication (and mitosis) is regulated through the formation of a prereplicative complex (pre-RC) at replication origins, followed by recruitment and activation of the DNA replication machinery. In the context of endoreplication these events are facilitated by an oscillation in cyclin E-Cdk2 activity. Cyclin E-Cdk2 activity drives the recruitment and activation of the replication machinery, but it also inhibits pre-RC formation, presumably to ensure that only one round of replication occurs per cycle. Failure to maintain control over pre-RC formation at replication origins results in a phenomenon known as “rereplication” which is common in cancer cells. The mechanism by which cyclin E-Cdk2 inhibits pre-RC formation involves downregulation of APC-Cdh1-mediated proteolysis and accumulation of the protein Geminin, which is responsible for sequestration of the pre-RC component Cdt1.\n\nOscillations in Cyclin E-Cdk2 activity are modulated via transcriptional and post-transcriptional mechanisms. Expression of cyclin E is activated by E2F transcription factors that were shown to be required for endoreplication. Recent work suggests that observed oscillations in E2F and cyclin E protein levels result from a negative-feedback loop involving Cul4-dependent ubiquitination and degradation of E2F. Post-transcriptional regulation of cyclin E-Cdk2 activity involves Ago/Fbw7-mediated proteolytic degradation of cyclin E and direct inhibition by factors such as Dacapo and p57. True endomitosis in the anther tapetum of the liliaceous plant Eremurus is described. The nuclear membrane does not disappear, but during metaphase the chromosomes are condensed, often considerably more than in normal mitosis. When the pollen mother cells (PMCs) go through the last premeiotic mitosis, the tapetal cells have one diploid nucleus which divides while the cell remains undivided. The two diploid nuclei may undergo an endomitosis and the resulting tetraploid nuclei a second endomitosis. An alternative pathway is an ordinary mitosis-again without cell division instead of one of the endomitotic cycles. The cytological picture in the tapetum is further complicated by restitution in anaphase and fusion of metaphase and anaphase groups during mitosis, processes which could give rise to cells with one, two, or three nuclei, instead of the expected two or four. No sign of the so-called \"inhibited\" mitosis is seen in these tapetal cells. When the PMCs are in leptotene-zygotene, very few tapetal nuclei are in endomitosis. When the PMCs have reached diplotene, almost 100% of cells which are not in interphase show an endomitotic stage.\n\nPolyploidy and aneuploidy are common phenomena in cancer cells. Given that oncogenesis and endoreplication likely involve subversion of common cell cycle regulatory mechanisms, a thorough understanding of endoreplication may provide important insights for cancer biology.\n\nThe unisexual salamanders (genus \"Ambystoma\") are the oldest known unisexual vertebrate lineage, having arisen about 5 million years ago. In these polyploid unisexual females, an extra premeiotic endomitotic replication of the genome, doubles the number of chromosomes. As a result, the mature eggs that are produced subsequent to the two meiotic divisions have the same ploidy as the somatic cells of the adult female salamander. Synapsis and recombination during meiotic prophase I in these unisexual females is thought to ordinarily occur between identical sister chromosomes and occasionally between homologous chromosomes. Thus little, if any, genetic variation is produced. Recombination between homeologous chromosomes occurs rarely, if at all. Since production of genetic variation is weak, at best, it is unlikely to provide a benefit sufficient to account for the maintenance of meiosis for millions of years. Perhaps the efficient recombinational repair of DNA damages at each generation provided by meiosis has been a sufficient advantage to maintain meiosis.\n", "id": "2491663", "title": "Endoreduplication"}
{"url": "https://en.wikipedia.org/wiki?curid=9522674", "text": "Mendelian error\n\nA Mendelian error in the genetic analysis of a species, describes an allele in an individual which could not have been received from either of its biological parents by Mendelian inheritance. Inheritance is defined by a set of related individuals who have the same or similar phenotypes for a locus of a particular gene. A Mendelian error means that the very structure of the inheritance as defined by analysis of the parental genes is incorrect: one parent of one individual\nis not actually the parent indicated; therefore the assumption is that the parental information is incorrect.\n\nPossible explanations for Mendelian errors are genotyping errors, erroneous assignment of the individuals as relatives, or de novo mutations. Mendelian error is established by demonstrating the existence of a trait which is inconsistent with every possible combination of genotype compatible with the individual. This method of determination requires pedigree checking, however, and establishing a contradiction between phenotype and pedigree is an NP-complete problem. Genetic inconsistencies which do not correspond to this definition are Non-Mendelian Errors. \n\nStatistical genetics analysis is used to detect these errors and to detect the possibility of the individual being linked to a specific disease linked to a single gene. Examples of such diseases in humans caused by single genes are Huntington's disease or Marfan syndrome.\n\n\n", "id": "9522674", "title": "Mendelian error"}
{"url": "https://en.wikipedia.org/wiki?curid=22974812", "text": "Morbid map\n\nIn genetics, a morbid map is a chart or diagram of diseases and the chromosomal location of genes the diseases are associated with. A morbid map exists as an appendix of the Online Mendelian Inheritance in Man (OMIM) knowledgebase, listing chromosomes and the genes mapped to specific sites on those chromosomes, and this format most clearly reveals the relationship between gene and phenotype.\n\n", "id": "22974812", "title": "Morbid map"}
{"url": "https://en.wikipedia.org/wiki?curid=1420406", "text": "Sex linkage\n\nSex linkage is the phenotypic expression of an allele related to the allosome (sex chromosome) of the individual. In autosomal chromosomes both sexes have the same probability of existing (see Fisher's principle), but since humans have many more genes on the female X chromosome than on the male Y chromosome, these are much more common than Y-linked traits.\n\nIn mammals, the female is homogametic, with two X chromosomes (XX), while the male is the heterogametic sex, with one X and one Y chromosome (XY). Genes on the X or Y chromosome are called sex-linked. In ZW sex-determination system used by birds the opposite is true: the male is the homogametic sex (ZZ), and the female is heterogametic (ZW).\n\nX-linked recessive traits are expressed in all heterogametics, but are only expressed in those homogametics that are homozygous for the recessive allele. For example, an X-linked recessive allele in humans causes haemophilia, which is much more common in males than females because they are hemizygous (see zygosity) and therefore express the trait when they inherit one mutant allele. In contrast, a female must inherit two mutant alleles, a less frequent event since the mutant allele is rare in the population.\n\nThe incidence of recessive X-linked phenotypes in females is the square of that in males (squaring a proportion less than one gives an outcome closer to 0 than the original). If 1 in 20 males in a human population are red-green color blind, then 1 in 400 females in the population are expected to be color-blind (/)*(/). (The term 'color-blind' is not completely accurate. There are degrees of weakness in color vision and it is now called 'color vision deficiency'.)\n\nX-linked traits are maternally inherited from carrier mothers or from an affected father. Each son born to a carrier mother has a 50% probability of inheriting the X-chromosome carrying the mutant allele. There are a few Y-linked traits; these are inherited from father.\n\nIn classical genetics, a reciprocal cross is performed to test if a trait is sex-linked.\n\nEach child of a mother affected with an X-linked dominant trait has a 50% chance of inheriting the mutation and thus being affected with the disorder. If only the father is affected, 100% of the daughters will be affected, since they inherit their father's X-chromosome, and 0% of the sons will be affected, since they inherit their father's Y-chromosome.\n\n\nFemales possessing one X-linked recessive mutation are considered carriers and will generally not manifest clinical symptoms of the disorder. All males possessing an X-linked recessive mutation will be affected, since males have only a single X-chromosome and therefore have only one copy of X-linked genes. All offspring of a carrier female have a 25% chance of inheriting the mutation if the father does not carry the recessive allele. All female children of an affected father will be carriers (assuming the mother is not affected or a carrier), as daughters possess their father's X-chromosome. If the mother is not a carrier, no male children of an affected father will be affected, as males only inherit their father's Y-chromosome.\n\n\nVarious failures in the SRY genes\n\n\nIt is important to distinguish between sex-linked characters, which are controlled by genes on sex chromosomes, and two other categories.\n\nSex-influenced or sex-conditioned traits are phenotypes affected by whether they appear in a male or female body. Even in a homozygous dominant or recessive female the condition may not be expressed fully. Example: baldness in humans.\n\nThese are characters only expressed in one sex. They may be caused by genes on either autosomal or sex chromosomes. Examples: female sterility in \"Drosophila\"; and many polymorphic characters in insects, especially in relation to mimicry. Closely linked genes on autosomes called \"supergenes\" are often responsible for the latter.\n\n", "id": "1420406", "title": "Sex linkage"}
{"url": "https://en.wikipedia.org/wiki?curid=6948407", "text": "BioModels Database\n\nBioModels Database is a free and open-source repository for storing, exchanging and retrieving quantitative models of biological interest created in 2006. All the models in the curated section of BioModels Database have been described in peer-reviewed scientific literature.\n\nThe models stored in the curated branch of BioModels Database are compliant with MIRIAM, the standard of model curation and annotation. The models have been simulated by curators to check that when run in simulations, they provide the same results as described in the publication. Model components are annotated, so the users can conveniently identify each model element and retrieve further information from other resources.\n\nModellers can submit the models in SBML and CellML. Models can subsequently be downloaded in SBML, VCML, XPP, SciLab, Octave, BioPAX and RDF/XML. The reaction networks of models are presented in some graphic formats, such as PNG, SVG and graphic Java applet, in which some networks were presented by following Systems Biology Graphical Notation. And a human readable summary of each model is available in PDF.\n\nBioModels Database is composed of several branches. The curated branch hosts models that are well curated and annotated. The non-curated-branch provides models that are still not curated, are non-curatable (spatial models, steady-state models etc.), or too huge to be curated. Non-curated models can be later moved into the curated branch. The repository also hosts models which were automatically generated from pathways databases.\n\nAll the models are freely available under the Creative Commons CC0 Public Domain Dedication, and can be easily accessed via the website or Web Services. One can also download archives of all the models from the EBI FTP server.\n\nBioModels Database announced its 30th release on May 10, 2016. It now publicly provides 144,553 models. This corresponds to 1,483 models published in the literature and 143,070 models automatically generated from pathway resources.\n\nDeposition of models in BioModels Database is advocated by many scientific journals, included Molecular Systems Biology, all the journals of the Public Library of Science, all the journals of BioMed Central and all the journals published by the Royal Society of Chemistry.\n\nBioModels Database is developed by the BioModels.net Team at the EMBL-EBI, UK, the Le Novère lab at the Babraham Institute, UK, and the SBML Team in Caltech, USA.\n\nBioModels Development has benefited from the funds of the European Molecular Biology Laboratory, the Biotechnology and Biological Sciences Research Council, the Innovative Medicines Initiative, the Seventh Framework Programme (FP7), the National Institute of General Medical Sciences, the DARPA, and the National Center for Research Resources.\n\n", "id": "6948407", "title": "BioModels Database"}
{"url": "https://en.wikipedia.org/wiki?curid=25058426", "text": "Expression quantitative trait loci\n\nExpression quantitative trait loci (eQTLs) are genomic loci that contribute to variation in expression levels of mRNAs.\n\nExpression traits differ from most other classical complex traits in one important respect—the measured mRNA or protein trait is almost always the product of a single gene with a specific chromosomal location. eQTLs that map to the approximate location of their gene-of-origin are referred to as local eQTLs. In contrast, those that map far from the location of their gene of origin, often on different chromosomes, are referred to as distant eQTLs. Often, these two types of eQTLs are referred to as cis and trans, respectively, but these terms are best reserved for instances when the regulatory mechanism (cis vs. trans) of the underlying sequence has been established. The first genome-wide study of gene expression was carried out in yeast and published in 2002. Many expression QTL studies followed in plants and animals, including humans. The initial wave of eQTL studies employed microarrays to measure genome-wide gene expression; more recent studies have employed massively parallel RNA sequencing.\n\nSome cis eQTLs are detected in many tissue types but the majority of trans eQTLs are tissue-dependent (dynamic). eQTLs may act in cis (locally) or trans (at a distance) to a gene. The abundance of a gene transcript is directly modified by polymorphism in regulatory elements. Consequently, transcript abundance might be considered as a quantitative trait that can be mapped with considerable power. These have been named expression QTLs (eQTLs). The combination of whole-genome genetic association studies and the measurement of global gene expression allows the systematic identification of eQTLs. By assaying gene\nexpression and genetic variation simultaneously on a genome-wide basis in a large number of individuals, statistical genetic methods can be used to map the genetic factors that underpin individual differences in quantitative levels of expression of many thousands of\ntranscripts. Studies have shown that single nucleotide polymorphisms (SNPs) reproducibly associated with complex disorders as well as certain pharmacologic phenotypes are found to be significantly enriched for eQTLs, relative to frequency-matched control SNPs.\n\nMapping eQTLs is done using standard QTL mapping methods that test the linkage between variation in expression and genetic polymorphisms. The only considerable difference is that eQTL studies can involve a million or more expression microtraits. Standard gene mapping software packages can be used, although it is often faster to use custom code such as QTL Reaper or the web-based eQTL mapping system GeneNetwork. GeneNetwork hosts many large eQTL mapping data sets and provide access to fast algorithms to map single loci and epistatic interactions. As is true in all QTL mapping studies, the final steps in defining DNA variants that cause variation in traits are usually difficult and require a second round of experimentation. This is especially the case for trans eQTLs that do not benefit from the strong prior probability that relevant variants are in the immediate vicinity of the parent gene. Statistical, graphical, and bioinformatic methods are used to evaluate positional candidate genes and entire systems of interactions.\n\n", "id": "25058426", "title": "Expression quantitative trait loci"}
{"url": "https://en.wikipedia.org/wiki?curid=6246830", "text": "Noninvasive genotyping\n\nNoninvasive genotyping is a modern technique for obtaining DNA for genotyping that is characterized by the indirect sampling of specimen, not requiring harm to, handling of, or even the presence of the organism of interest. Beginning in the early 1990s, with the advent of PCR, researchers have been able to obtain high-quality DNA samples from small quantities of hair, feathers, scales, or excrement. These noninvasive samples are an improvement over older allozyme and DNA sampling techniques that often required larger samples of tissue or the destruction of the studied organism. Noninvasive genotyping is widely utilized in conservation efforts, where capture and sampling may be difficult or disruptive to behavior. Additionally, in medicine, this technique is being applied in humans for the diagnosis of genetic disease and early detection of tumors. In this context, invasivity takes on a separate definition where noninvasive sampling also includes simple blood samples.\n\nIn conservation, noninvasive genotyping has been used to supplement traditional techniques with broadly ranging levels of success. Modern DNA amplification methods allow researchers to use fecal or hair samples collected from the field to assess basic information about the specimen, including sex or species. Despite the potential that noninvasive genotyping has in conservation genetics efforts, the efficiency of this method is in question, as field samples often suffer from degradation and contamination or are difficult to procure. For instance, a team of researchers successfully used coyote fecal samples to estimate the abundance of a population in Georgia, thereby avoiding the substantial difficulty and consequences involved in trapping and procuring samples from the animals.\n\nThe most common use of noninvasive genotyping in medicine is non-invasive prenatal diagnosis (NIPD), which provides an alternative to riskier techniques such as amniocentesis. With the discovery of cell-free fetal DNA in maternal plasma, NIPD became a popular method for determining sex, paternity, aneuploidy, and the occurrence of monogenic diseases as it requires only a simple blood sample. One NIPD provider maintains that a 10 mL blood sample will provide 99% accurate detection of basic genomic abnormalities as early as 10 weeks into pregnancy. The karyotype below is that of an individual with trisomy 21, or Down Syndrome, which is what is most routinely checked for by NIPD screens.\n\nThis same technique is also utilized to identify the incidence of tumor DNA in the blood, which can both provide early detection of tumor growth and indicate relapse in cancer. Circulating tumor DNA can be found in the blood before metastasis occurs and, therefore, detection of certain mutant alleles may enhance survival rates in cancer patients. In a recent study, ctDNA was shown to be \"a broadly applicable, sensitive, and specific biomarker that can be used for a variety of clinical and research purposes in patients with multiple different types of cancer\". This technique is often referred to as a liquid biopsy, and has not been widely implemented in clinical settings although its impact could be quite large. Although blood-borne ctDNA remains the most clinically significant noninvasive cancer detection, other studies have emerged that investigate other potential methods, including detection of colorectal cancer via fecal samples.\n\nThe method by which samples are collected in noninvasive genotyping is what separates the technique from traditional genotyping, and there are a number of ways that this is accomplished. In the field, procured samples of tissue are captured, the tissue is dissolved, and the DNA is purified, although the exact procedure differs between different samples. Following the collection of DNA samples, PCR technology is utilized to amplify particular genetic sequences, with PCR primer specificity avoiding contamination from other DNA sources. Then, the DNA can be analyzed using a number of genomic techniques, similarly to traditionally obtained samples.\n", "id": "6246830", "title": "Noninvasive genotyping"}
{"url": "https://en.wikipedia.org/wiki?curid=89242", "text": "Gene knockout\n\nA gene knockout (abbreviation: KO) is a genetic technique in which one of an organism's genes is made inoperative (\"knocked out\" of the organism). However, KO can also refer to the gene that is knocked out or the organism that carries the gene knockout. Knockout organisms or simply knockouts are used to study gene function, usually by investigating the effect of gene loss. Researchers draw inferences from the difference between the knockout organism and normal individuals.\n\nThe KO technique is essentially the opposite of a gene knockin. Knocking out two genes simultaneously in an organism is known as a double knockout (DKO). Similarly the terms triple knockout (TKO) and quadruple knockouts (QKO) are used to describe three or four knocked out genes, respectively. However, one needs to distinguish between heterozygous and homozygous KOs. In the former, only one of two gene copies (alleles) is knocked out, in the latter both are knocked out.\n\nKnockouts are accomplished through a variety of techniques. Originally, naturally occurring mutations were identified and then gene loss or inactivation had to be established by DNA sequencing or other methods.\n\nThe directed creation of a KO begins in the test tube with a plasmid, a bacterial artificial chromosome or other DNA construct, and proceeding to cell culture. Individual cells are genetically transfected with the DNA construct. Often the goal is to create a transgenic animal that has the altered gene. If so, embryonic stem cells are genetically transformed and inserted into early embryos. Resulting animals with the genetic change in their germline cells can then often pass the gene knockout to future generations.\n\nTo create knockout moss, transfection of protoplasts is the preferred method. Such transformed \"Physcomitrella\"-protoplasts directly regenerate into fertile moss plants. Eight weeks after transfection, the plants can be screened for gene targeting via PCR.\n\nThe construct is engineered to recombine with the target gene, which is accomplished by incorporating sequences from the gene itself into the construct. Recombination then occurs in the region of that sequence within the gene, resulting in the insertion of a foreign sequence to disrupt the gene. With its sequence interrupted, the altered gene in most cases will be translated into a nonfunctional protein, if it is translated at all.\n\nA conditional knockout allows gene deletion in a tissue or time specific manner. This is done by introducing short sequences called loxP sites around the gene. These sequences will be introduced into the germ-line via the same mechanism as a knock-out. This germ-line can then be crossed to another germline containing Cre-recombinase which is a viral enzyme that can recognize these sequences, recombines them and deletes the gene flanked by these sites.\n\nBecause the desired type of DNA recombination is a rare event in the case of most cells and most constructs, the foreign sequence chosen for insertion usually includes a reporter. This enables easy selection of cells or individuals in which knockout was successful. Sometimes the DNA construct inserts into a chromosome without the desired homologous recombination with the target gene. To eliminate such cells, the DNA construct often contains a second region of DNA that allows such cells to be identified and discarded.\n\nIn diploid organisms, which contain two alleles for most genes, and may as well contain several related genes that collaborate in the same role, additional rounds of transformation and selection are performed until every targeted gene is knocked out. Selective breeding may be required to produce homozygous knockout animals.\n\nGene knockin is similar to gene knockout, but it replaces a gene with another instead of deleting it.\n\nKnockouts are primarily used to understand the role of a specific gene or DNA region by comparing the knockout organism to a wildtype with a similar genetic background.\n\nKnockout organisms are also used as screening tools in the development of drugs, to target specific biological processes or deficiencies by using a specific knockout, or to understand the mechanism of action of a drug by using a library of knockout organisms spanning the entire genome, such as in \"Saccharomyces cerevisiae\".\n\n\n", "id": "89242", "title": "Gene knockout"}
{"url": "https://en.wikipedia.org/wiki?curid=23520833", "text": "Gene signature\n\nA gene signature or gene expression signature is a single or combined group of genes in a cell with a uniquely characteristic pattern of gene expression that occurs as a result of an altered or unaltered biological process or pathogenic medical condition. This is not to be confused with the concept of gene expression profiling. Activating pathways in a regular physiological process or a physiological response to a stimulus results in a cascade of signal transduction and interactions that elicit altered levels of gene expression, which is classified as the gene signature of that physiological process or response. The clinical applications of gene signatures breakdown into prognostic, diagnostic and predictive signatures. The phenotypes that may theoretically be defined by a gene expression signature range from those that predict the survival or prognosis of an individual with a disease, those that are used to differentiate between different subtypes of a disease, to those that predict activation of a particular pathway. Ideally, gene signatures can be used to select a group of patients for whom a particular treatment will be effective.\n\nIn 1995, 2 studies conducted identified unique approaches to analyzing global gene expression of a genome which collectively promoted the value of identifying and analyzing gene signatures for physiological relevance. The first study reports a technique that improves expressed sequence tag (EST) analysis, known as Serial Analysis of Gene Expression (SAGE) that hinged on sequencing and quantifying mRNA samples which acquired levels of gene expression that eventually revealed characteristic gene expression patterns.\n\nThe second study identified a technique that is now widely known as the microarray which quantifies complementary DNA (cDNA) hybridization on a glass slide to analyze the expression of many genes in parallel. These studies drew greater attention to the wealth of information that analysis of gene signatures bear that may or may not be physiologically relevant.\n\nPressing forward, the latter technique has revolutionized research in genetics and DNA chip technology as it is a widely adopted technique to profile gene expression signatures such that these physiological responses can be cataloged in repositories such as NCBI Gene Expression Omnibus. This catalogue of prognostic, diagnostic and predictive gene expression signatures allow for predictions of onset of pathogenic diseases in patients, tumour and cancer classification, and enhanced therapeutic strategies that predict the optimal target candidates subjects and genes.\n\nToday, microarrays and other quantitative methods such as RNA-seq that encompass gene expression profiling, are moving towards promotion of re-analysis and integration of the large, publicly available database of gene expression signatures and profiles to uncover the full threshold of information these expression signatures hold.\n\nPrognostic refers to predicting the likely outcome or course of a disease. Classifying a biological phenotype or medical condition based on a specific gene signature or multiple gene signatures, can serve as a prognostic biomarker for the associated phenotype or condition. This concept termed prognostic gene signature, serves to offer insight into the overall outcome of the condition regardless of therapeutic intervention. Several studies have been conducted with focus on identifying prognostic gene signatures with the hopes of improving the diagnostic methods and therapeutic courses adopted in a clinical settings. It is important to note that prognostic gene signatures are not a target of therapy; they offer additional information to consider when discussing details such as duration or dosage or drug sensitivity etc. in therapeutic intervention. The criteria a gene signature must meet to be deemed a prognostic marker include demonstration of its association with the outcomes of the condition, reproducibility and validation of its association in an independent group of patients and lastly, the prognostic value must demonstrate independence from other standard factors in a multivariate analysis. The applications of these prognostic signatures include prognostic assays for breast cancer, hepatocellular carcinoma, leukaemia and are continually being developed for other types of cancers and disorders as well.\n\nA diagnostic gene signature serves as a biomarker that distinguishes phenotypically similar medical conditions that have a threshold of severity consisting of mild, moderate or severe phenotypes. Establishing verified methods of diagnosing clinically indolent and significant cases allows practitioners to provide more accurate care and therapeutic options that range from no therapy, preventative care to symptomatic relief. These diagnostic signatures also allow for a more accurate representation of test samples used in research. Similar to the procedure of validation of prognostic gene signature, a criteria exists for classifying a gene signature as a biomarker for a disorder or diseases outlined by Chau et al.\n\nA predictive gene signature is similar to a predictive biomarker, where it predicts the effect of treatment in patients or study participants that exhibit a particular disease phenotype. A predictive gene signature unlike a prognostic gene signature can be a target for therapy. The information predictive signatures provide are more rigorous than that of prognostic signatures as they are based off of treatment groups with therapeutic intervention on the likely benefit from treatment, completely independent of prognosis. Predictive gene signatures addresses the paramount need for ways to personalize and tailor therapeutic intervention in diseases. These signatures have implications in facilitating personalized medicine through identification of more novel therapeutic targets and identifying the most qualified subjects for optimal benefit of specific treatments.\n\n", "id": "23520833", "title": "Gene signature"}
{"url": "https://en.wikipedia.org/wiki?curid=11978476", "text": "Genetic stock center\n\nGenetic stock centers are collections of pure genetic stock available for use in research. They are often housed at research universities, and include everything from single cell life to plants, fish, and small mammals such as mice and rats. Genetic Stock Centers often charge for research stock on a two tier scale, with non profit researchers getting stock at a lower cost than commercial researchers. Dr Myron Gordon, for example, established the Xiphophorus genetic stock center in 1939 to raise pure strains when he realized that certain Xiphophorus hybrids would be useful in cancer research. He understood that his research could not be duplicated by other scientists without pure genetic stock to use as a base. The strains that Dr Gordon started remain pure and are used to this day.\n\n", "id": "11978476", "title": "Genetic stock center"}
{"url": "https://en.wikipedia.org/wiki?curid=1404523", "text": "Hayflick limit\n\nThe Hayflick limit or Hayflick phenomenon is the number of times a normal human cell population will divide until cell division stops. Empirical evidence shows that the telomeres associated with each cell's DNA will get slightly shorter with each new cell division until they shorten to a critical length.\n\nThe concept of the Hayflick limit was advanced by American anatomist Leonard Hayflick in 1961, at the Wistar Institute in Philadelphia, Pennsylvania, USA. Hayflick demonstrated that a population of normal human fetal cells in a cell culture will divide between 40 and 60 times. The population will then enter a senescence phase, which refutes the contention by Nobel laureate Alexis Carrel that normal cells are immortal. Each mitosis slightly shortens each of the telomeres on the DNA of the cells. Telomere shortening in humans eventually makes cell division impossible, and this aging of the cell population appears to correlate with the overall physical aging of the human body.\n\nAustralian Nobel laureate Sir Macfarlane Burnet coined the name \"Hayflick limit\" in his book \"Intrinsic Mutagenesis: A Genetic Approach to Ageing\", published in 1974.\n\nPrior to Leonard Hayflick's discovery, it was believed that vertebrate cells had an unlimited potential to replicate. Alexis Carrel, a Nobel prize-winning surgeon, had stated \"that all cells explanted in culture are immortal, and that the lack of continuous cell replication was due to ignorance on how best to cultivate the cells\". He supported this hypothesis by claiming to have cultivated fibroblasts from chicken hearts and to have kept the culture growing for 34 years. This indicated that cells of vertebrates could continue to divide indefinitely in a culture. However, other scientists have been unable to repeat Carrel's result.\n\nCarrel's result is suspected to be due to an error in experimental procedure. To provide required nutrients, embryonic stem cells of chickens may have been re-added to the culture daily. This would have easily allowed the cultivation of new, fresh cells in the culture, so there was not an infinite reproduction of the original cells. If this is true, it has been speculated that Carrel knew about the error, but he never admitted it.\n\nAlso, it has been theorized that the cells Carrel used were young enough to contain pluripotent stem cells, which, if supplied with a supporting telomerase-activation nutrient, would have been capable of staving off replicative senescence, or even possibly reversing it. Cultures not containing telomerase-active pluripotent stem cells would have been populated with telomerase-inactive cells, which would have been subject to the 50–60 mitosis cycles until apoptosis occurs as described in Hayflick's findings.\n\nHayflick first became suspicious of Carrel's theory while working in a lab at the Wistar Institute. Hayflick was preparing normal human cells to be exposed to extracts of cancer cells when he noticed the normal cells had stopped proliferating. At first he thought that he had made a technical error in preparing the experiment, but later he began to think that the cell division processes had a counting mechanism. Working with Paul Moorhead, a cytogeneticist, he designed an experiment to test Carrel's theory of cell division.\n\nThe experiment proceeded as follows. Hayflick and Moorhead mixed equal numbers of normal human male fibroblasts that had divided many times (cells at the 40th population doubling) with female fibroblasts that had divided only a few times (cells at the 10th population doubling). Unmixed cell populations were kept as controls. When the male control culture stopped dividing, the mixed culture was examined and only female cells were found. This showed that the old male cells \"remembered\" they were old, even when surrounded by young cells, and that technical errors or contaminating viruses were unlikely explanations as to why only the male cell component had died.<ref name=\"doi:10.1038/35036093\"></ref>\n\nThe cells had stopped dividing and had become senescent based purely upon how many times the cell had divided.\n\nThese results disproved the immortality theory of Carrel and established the Hayflick limit as a credible biological theory that, unlike Carrel's experiment, has been repeated by other scientists.\n\nHayflick describes three phases in the life of a cell. At the start of his experiment he named the primary culture \"phase one\". Phase two is defined as the period when cells are proliferating – Hayflick called it the time of \"luxuriant growth\". After months of doubling, the cells eventually reach phase three, a phenomenon of senescence – cell growth diminishes and then cell division stops altogether.\n\nThe Hayflick limit has been found to correlate with the length of the telomere region at the end of a strand of DNA. During the process of DNA replication, small segments of DNA at each end of the DNA strand (telomeres) are unable to be copied and are lost after each time DNA is duplicated. This occurs due to the uneven nature of DNA replication, where leading and lagging strands are not replicated symmetrically. The telomere region of DNA does not code for any protein; it is simply a repeated code on the end region of linear eukaryotic chromosomes that is lost. After many divisions, the telomeres become depleted and the cell begins apoptosis. This is a mechanism that prevents replication error that would cause mutations in DNA. Once the telomeres are depleted, due to the cell dividing many times, it will no longer divide. This is when the cell has reached its Hayflick limit.\n\nThis process does not take place in most cancer cells due to an enzyme called telomerase. This enzyme maintains telomere length, which results in the telomeres of cancer cells never shortening. This gives these cells infinite replicative potential. A proposed treatment for cancer is the usage of telomerase inhibitors that would prevent the restoration of the telomere, allowing the cell to die like other body cells. On the other hand, telomerase activators might repair or extend the telomeres of healthy cells, thus extending their Hayflick limit. Telomerase activation might also lengthen the telomeres of immune system cells enough to prevent cancerous cells from developing from cells with very short telomeres.\n\nIt has been speculated that the limited replicative capability of human fibroblasts in culture may have significance for human aging, even though the number of replications observed in culture is far greater than the number that would be expected for non-stem cells in vivo during a normal postnatal lifespan. Although it had been thought that the replicative capacity of human cell lines was inversely correlated with the age of the human donor from whom the cell lines were derived, it is now clear that no such correlation exists.\n\nComparisons of different species indicate that cellular replicative capacity correlates primarily with species body mass, not species lifespan. Thus it appears that the limited capacity of cells to replicate in culture may not be directly relevant to organismal aging.\n\n\n\n", "id": "1404523", "title": "Hayflick limit"}
{"url": "https://en.wikipedia.org/wiki?curid=3937644", "text": "Ribotyping\n\nRibotyping is a molecular technique for bacterial identification and characterization that uses information from rRNA-based phylogenetic analyses. It is rapid and specific method widely used in clinical diagnostics and analysis of microbial communities in food, water, and beverages.\n\nAll bacteria have ribosomal genes, but the exact sequence is unique to each species, serving as a genetic fingerprint. Therefore, sequencing the particular 16S gene and comparing it to a database would yield identification of the particular species.\n\nIt involves the digestion of bacterial genomic DNA with specific restriction enzymes. Each restriction enzyme cuts DNA at a specific nucleotide sequence, resulting in fragments of different lengths.\n\nThose fragments are then run on a Gel electrophoresis, where they are separated according to size: the application of electrical field to the gel in which they are suspended causes the movement of DNA fragments (all negatively charged due to the presence of phosphate groups) through a matrix towards the positively charged end of the field. Small fragments move more easily and rapidly through the matrix, reaching a bigger distance from the starting position than larger fragments.\n\nFollowing the separation in the gel matrix, the DNA fragments are moved onto nylon membranes and hybridized with a labelled 16S or 23S rRNA probe. This way only the fragments coding for such rRNA are visualised and can be analyzed. The pattern is then digitized and used to identify the origin of the DNA by a comparison with reference organisms in a computer database.\n\nConceptually, ribotyping is similar to probing restriction fragments of chromosomal DNA with cloned probes (randomly cloned probes or probes derived from a specific coding sequence such as that of a virulence factor).\n\n", "id": "3937644", "title": "Ribotyping"}
{"url": "https://en.wikipedia.org/wiki?curid=26858920", "text": "Inclusive composite interval mapping\n\nIn statistical genetics, inclusive composite interval mapping (ICIM) has been proposed as an approach to QTL (quantitative trait locus) mapping for populations derived from bi-parental crosses. QTL mapping is based on genetic linkage map and phenotypic data and attempts to locate individual genetic factors on chromosomes and to estimate their genetic effects.\n\nTwo genetic assumptions used in ICIM are (1) the genotypic value of an individual is the summation of effects from all genes affecting the trait of interest; and (2) linked QTL are separated by at least one blank marker interval. Under the two assumptions, they proved that additive effect of the QTL located in a marker interval can be completely absorbed by the regression coefficients of the two flanking markers, while the QTL dominance effect causes marker dominance effects, as well as additive by additive and dominance by dominance interactions between the two flanking markers. By including two multiplication variables between flanking markers, the additive and dominance effects of one QTL can be completely absorbed. As a consequence, an inclusive linear model of phenotype regressing on all genetic markers (and marker multiplications) can be used to fit the positions, and additive (and dominance) effects of all QTL in the genome. A two-step strategy was adopted in ICIM for additive and dominance QTL mapping. In the first step, stepwise regression was applied to identify the most significant marker variables in the linear model. In the second step, one-dimensional scanning or interval mapping was conducted for detecting QTL and estimating its additive and dominance effects, based on the phenotypic values adjusted by the regression model in the first step.\n\nThrough computer simulations they studied the asymptotic properties of ICIM in additive QTL mapping as well. The test statistic LOD score linearly increases as the increase in population size. The larger of the QTL effect, the greater the corresponding LOD score increases. When population size is greater than 200, the position estimation of ICIM for QTL explaining more than 5% of the phenotypic variance is unbiased. For smaller population size, there is a tendency that the QTL was identified towards the center of the chromosome. When population size is greater than 200, the effect estimation of ICIM for QTL explaining more than 5% of phenotypic variance is unbiased. For smaller sample size, the QTL effect was always overestimated.\n\nUnder the same assumptions in additive and dominance QTL mapping of ICIM, an additive by additive epistatic effect between two interacting QTL can be completely absorbed by the four marker interaction variables between the two pairs of flanking markers [5]. That is to say, the coefficients of four marker interactions of two pairs of flanking markers contain the genetic information of the additive by additive epistasis between the two marker intervals. As a consequence, a linear model of phenotype regressing on both markers and marker multiplications can fit the positions and effects of all QTL and their digenic interactions. Similar to the additive QTL mapping of ICIM, two-step strategy was also adopted in additive by additive epistasis mapping. In the first step, stepwise regression was applied to identify the most significant marker and marker interactions. In the second step, two-dimensional scanning was conducted for detecting additive by additive QTL and estimating the genetic effects, based on the phenotypic values adjusted by the regression model in the first step.\n\nTake a barley doubled haploid population as an example, nine additive QTL affecting kernel weight were identified to be distributed on five out of the seven chromosomes, explaining 81% of the phenotypic variance. In this population additive effects have explained most of the phenotypic variance, approximating the estimated heritability in the broad sense, which indicates that most of the genetic variance was caused by additive QTL.\n\nBesides that, ICIM has been successfully used in wild and cultivated soybeans in mapping conserved salt tolerance QTL, in rice mapping tiller angle QTL, and grain length QTL, in wheat mapping flour and noodle color components and yellow pigment content, and adult-plant resistance to stripe rust QTL, etc. Some of these detected QTL has been fine mapped.\n\nBi-parental populations are mostly used in QTL linkage mapping. QTL not segregating between the two parents cannot be detected. To find most, if not all, genes controlling a trait of interest, multiple parents have to be used. Complex cross populations have been proposed in recent years for this purpose. These crosses allow a more powerful understanding of the genetic basis of quantitative traits in more relevant genetic backgrounds. They extended ICIM to map Maize Nested Association Mapping (NAM). design recently proposed by the Buckler laboratory at Cornell University. QTL detection efficiency of ICIM in this design was investigated through extensive simulations. In the actual maize NAM population, ICIM detected a total of 52 additive QTL affecting the silk flowering time in maize. These QTL have explained 79% of the phenotypic variance in this population.\n\nThere is software that implements ICIM additive and epistasis mapping. Its function is: (1) implementation of mapping methods including single marker analysis, interval mapping, ICIM for additive and dominance, ICIM for digenic epistasis, selective phenotyping, etc.; (2) QTL linkage analysis more than twenty mapping populations derived from bi-parental cross, including backcross, double haploid, recombinant inbred lines, etc.; (3) Power analysis for simulated populations under the genetic models user defined; and (4) QTL mapping for non-idealized chromosome segment substitution lines .\n", "id": "26858920", "title": "Inclusive composite interval mapping"}
{"url": "https://en.wikipedia.org/wiki?curid=6476284", "text": "Insertional mutagenesis\n\nInsertional mutagenesis is mutagenesis of DNA by the insertion of one or more bases.\n\nInsertional mutations can occur naturally, mediated by virus or transposon, or can be artificially created for research purposes in the lab.\n\nThis is a technique used to study the function of genes. A transposon, such as the \"Drosophila melanogaster\" P element, is allowed to integrate at random locations in the genome of the organism being studied. Mutants generated by this method are then screened for any unusual phenotypes. If such a phenotype is found then it can be assumed that the insertion has caused the gene relating to the usual phenotype to be inactivated. Because the sequence of the transposon is known, the gene can be identified, either by sequencing the whole genome and searching for the sequence, or using the polymerase chain reaction to amplify specifically that gene.\n\nAs mentioned in the introduction, insertional mutagenesis refers to mutation of an organism caused by the insertion of additional DNA bases into the organism's preexisting DNA. Because many viruses (not all of them) integrate their own genome into the genome of their host cells in order to replicate, mutagenesis caused by viral infections is a fairly common occurrence. Not all integrating viruses cause insertional mutagenesis, however.\n\nIt is important to note that not all DNA insertions will lead to a noticeable mutation. In recent gene therapy trials, the lentiviral vectors used showed no tendency to disrupt gene function or promote oncogenic development. Because of these advances, gene therapy with integrating vectors is now considered safe and is the preferred method of gene transfer due to the permanent nature of the integration compared to the transient persistence of non-integrating viruses. For those viruses such as gamma-retroviruses that do tend to integrate their DNA in genetically unfavorable locations, the severity of any ensuing mutation depends entirely on the location within the host's genome wherein the viral DNA is inserted. If the DNA is inserted into the middle of an essential gene the effects on the cell will be drastic. Additionally, insertion into the promoter region of a gene can cause equally drastic effects. For instance, if the viral DNA is inserted into a repressor, the gene corresponding to that promoter may be over expressed – leading to an overabundance of its product and altered cellular activity. If the DNA is inserted into an enhancer region, the gene may be under-expressed – leading to relative absence of its product, which can significantly interrupt the activity of the cell.\n\nAlteration of different genes will have varying effects on the cell. Not all mutations will significantly affect the proliferation of the cell. However, if the insertion occurs in an essential gene or a gene that is involved in cellular replication or programmed cell death, the insertion may compromise the viability of the cell or even cause the cell to replicate interminably – leading to the formation of a tumor, which may become cancerous.\n\nBelow is an example of a significant change in cell activity due to insertion of a viral gene into a portion of the hosts genome that controls replication.\n\nVirus insertional mutagenesis is possible with both replication competent virus and the self-inactivating vectors that are commonly used in gene therapy. The virus inserts a gene (known as a viral onocogene) normally near the cellular myc (c-myc)gene. The c-myc gene is normally turned off in the cell; however when it is turned on it is able to push the cell into the G1 phase of the cell cycle and cause the cell to begin replication, causing unchecked cell proliferation while allowing the viral gene to be replicated. After many replications where the viral gene stays latent tumours begin to grow. These tumours are normally derived from one mutated/transformed cell (clonal in origin). Avian leukosis virus is an example of a virus that causes a disease by insertional mutagenesis. Newly hatched chicks infected with Avian leukosis virus will begin to form tumours that will begin to appear in their bursa of fabricus (like the human thymus). This viral gene insertion is also known as a promoter insertion as it drives the expression of the c-myc gene. There is an example of an insertional mutagenesis event caused by a retrotransposon in the human genome where it causes Fukuyama-type muscular dystrophy.\n\nInsertional inactivation is a technique used in recombinant DNA engineering where a plasmid (such as pBR322) is used to disable expression of a gene.\n\nThe inactivation of a gene by inserting a fragment of DNA into the middle of its coding sequence. Any future products from the inactivated gene will not work because of the extra codes added to it. An example is the use of pBR322, which has genes that respectively encode polypeptides that confer resistance to ampicillin and tetracyclin antibiotics. Hence, when a genetic region is interrupted by integration of pBR322, the gene function is lost but new gene function (resistance to specific antibiotics) is gained.\n\nAn alternative strategy for insertional mutagenesis has been used in vertebrate animals to find genes that cause cancer. In this case a transposon, e.g. Sleeping Beauty, is designed to interrupt a gene in such a way that it causes maximal genetic havoc. Specifically, the transposon contains signals to truncate expression of an interrupted gene at the site of the insertion and then restart expression of a second truncated gene. This method has been used to identify oncogenes.\n\n", "id": "6476284", "title": "Insertional mutagenesis"}
{"url": "https://en.wikipedia.org/wiki?curid=30351511", "text": "Allele frequency net database\n\nThe allele frequency net database is a database containing the allele frequencies of immune genes and their corresponding alleles in different populations.\n\n", "id": "30351511", "title": "Allele frequency net database"}
{"url": "https://en.wikipedia.org/wiki?curid=30356480", "text": "CaSNP\n\nCaSNP is database for storing data about copy number alterations from SNP arrays for different types of cancer.\n\n", "id": "30356480", "title": "CaSNP"}
{"url": "https://en.wikipedia.org/wiki?curid=30367004", "text": "ACLAME\n\nACLAME (The CLAssification of Mobile genetic Elements) is a database of sequenced mobile genetic elements.\n\n\n", "id": "30367004", "title": "ACLAME"}
{"url": "https://en.wikipedia.org/wiki?curid=30367108", "text": "Comparative regulatory genomics database\n\nComparative regulatory genomics database (CORG) is a catalogue of conserved non-coding sequence blocks.\n\n\n", "id": "30367108", "title": "Comparative regulatory genomics database"}
{"url": "https://en.wikipedia.org/wiki?curid=13106778", "text": "Phylostratum\n\nPhylostratum is a set of genes from an organism that coalesce to founder genes having common phylogenetic origin.\n", "id": "13106778", "title": "Phylostratum"}
{"url": "https://en.wikipedia.org/wiki?curid=30371632", "text": "Gypsy (database)\n\nGypsy (GyDB) is a wiki-style database of mobile genetic elements.\n\n\n", "id": "30371632", "title": "Gypsy (database)"}
{"url": "https://en.wikipedia.org/wiki?curid=30385383", "text": "Red-suffusion rosy-faced lovebird mutation\n\nThe red-suffusion rose-faced lovebird (\"Agapornis roseicollis\"), also known as the red-pied lovebird, is not a true colour mutation of lovebird species. Many breeders believe it is due to a health issue, most likely dealing with the bird's liver. Some think the red-pied has some genetic relations with the Lutino rosy-faced lovebird mutation, as many cases of red spots appear in Lutino lovebirds. Although many breeders of parrots have claimed that this is a genetic mutation, no one has been able to successfully reproduce it through a series of generations.\n\n", "id": "30385383", "title": "Red-suffusion rosy-faced lovebird mutation"}
{"url": "https://en.wikipedia.org/wiki?curid=30386163", "text": "Snpstr\n\nA SNPSTR is a compound genetic marker composed of one or more SNPs and one microsatellite (STR). Autosomal SNPSTRs, which contain a SNP and a microsatellite within 500 base pairs of one another, were discovered in 2002. More recently a database that contains all SNPSTRs in five model genomes, including human, has been created.\n\nThere has been widespread and growing interest in genetic markers suitable for drawing population genetic inferences about past demographic events and to detect the effects of selection. Single nucleotide polymorphisms (SNPs) and microsatellites (or short tandem repeats, STRs) have received great attention in the analysis of human population history, even though they have both disadvantages. It was thus suggested that the combination of these two markers could give rise to better conclusions\n\n", "id": "30386163", "title": "Snpstr"}
{"url": "https://en.wikipedia.org/wiki?curid=30407569", "text": "MICdb\n\nMICdb (Microsatellites database) is a database of non-redundant microsatellites from prokaryotic genomes.\n\n\n", "id": "30407569", "title": "MICdb"}
{"url": "https://en.wikipedia.org/wiki?curid=30423565", "text": "Nucleosome positioning region database\n\nNucleosome Positioning Region Database (NPRD),is a database of nucleosome formation sites (NFSs).\n\n\n", "id": "30423565", "title": "Nucleosome positioning region database"}
{"url": "https://en.wikipedia.org/wiki?curid=11692451", "text": "Centimorgan\n\nIn genetics, a centimorgan (abbreviated cM) or map unit (m.u.) is a unit for measuring genetic linkage. It is defined as the distance between chromosome positions (also termed loci or markers) for which the expected average number of intervening chromosomal crossovers in a single generation is 0.01. It is often used to infer distance along a chromosome. However, It is not a true physical distance.\n\nThe number of base-pairs to which it corresponds varies widely across the genome (different regions of a chromosome have different propensities towards crossover) and it also depends on if the meiosis where the crossing-over takes place is a part of oogenesis (formation of female gametes) or spermatogenesis (formation of male gametes).\n\nOne centimorgan corresponds to about 1 million base pairs in humans on average. The relationship is only rough as the physical chromosomal distance corresponding to one centimorgan varies from place to place in the genome, and also varies between men and women since recombination during gamete formation in females is significantly more frequent than in males. Morton et al. calculated that the female genome is 4782 centimorgans long, while the male genome is only 2809 centimorgans long. \"Plasmodium falciparum\" has an average recombination distance of ~15 kb per centimorgan: markers separated by 15 kb of DNA (15,000 nucleotides) have an expected rate of chromosomal crossovers of 0.01 per generation. Note that non-syntenic genes (genes residing on different chromosomes) are inherently unlinked, and cM distances have no meaning between them.\n\nBecause genetic recombination between two markers is detected only if there are an odd number of chromosomal crossovers between the two markers, the distance in centimorgans does not correspond exactly to the probability of genetic recombination. Assuming Haldane's map function, where the number of chromosomal crossovers is according to a Poisson distribution, a genetic distance of \"d\" centimorgans will lead to an odd number of chromosomal crossovers, and hence a detectable genetic recombination, with probability\nwhere sinh is the hyperbolic sine function. The probability of recombination is approximately \"d\"/100 for small values of \"d\" and approaches 50% as \"d\" goes to infinity.\n\nThe formula can be inverted, giving the distance in centimorgans as a function of the recombination probability:\n\nThe centimorgan was named in honor of geneticist Thomas Hunt Morgan by his student Alfred Henry Sturtevant. Note that the parent unit of the centimorgan, the Morgan, is rarely used today.\n\n", "id": "11692451", "title": "Centimorgan"}
{"url": "https://en.wikipedia.org/wiki?curid=30443797", "text": "Patome\n\nPatome is a database of biological sequence data of issued patents and/or published applications\n\n\n", "id": "30443797", "title": "Patome"}
{"url": "https://en.wikipedia.org/wiki?curid=3699327", "text": "X hyperactivation\n\nX hyperactivation refers to the process in Drosophila by which the structural genes of the male X chromosome are transcribed at the same rate as the two X chromosomes of the female combined.\n", "id": "3699327", "title": "X hyperactivation"}
{"url": "https://en.wikipedia.org/wiki?curid=16699474", "text": "Adductomics\n\nAdductomics is the study of DNA adducts in the context of an entire genome. DNA adducts are compounds that bind to DNA, causing damage and mutations. These mutations can result in cancer and birth defects in multicellular organisms. The science of adductomics seeks to identify all DNA adducts and the target sequence of each adduct.\n\nThe term \"adductome\" first appeared in a journal article in 2005. Although originally the term related to adducts of DNA, the adductomic approach has now been adopted by protein chemists in their attempts to identify protein adducts.\n", "id": "16699474", "title": "Adductomics"}
{"url": "https://en.wikipedia.org/wiki?curid=30483675", "text": "Recode (database)\n\nRECODE is a database of \"programmed\" frameshifts, bypassing and codon redefinition used for gene expression.\n\n\n", "id": "30483675", "title": "Recode (database)"}
{"url": "https://en.wikipedia.org/wiki?curid=30483900", "text": "RegTransBase\n\nRegTransBase is database of regulatory interactions and transcription factor binding sites in prokaryotes\n\n\n", "id": "30483900", "title": "RegTransBase"}
{"url": "https://en.wikipedia.org/wiki?curid=10925031", "text": "Glossary of gene expression terms\n\n\n\n\n\n\n\n\n\n\nKaryoptyping\n\n\n\n\n\n\n\n\n\n\nZ form gene.\n", "id": "10925031", "title": "Glossary of gene expression terms"}
{"url": "https://en.wikipedia.org/wiki?curid=30497848", "text": "UgMicroSatdb\n\nUgMicroSatdb (UniGene Microsatellites database) is a database of microsatellites present in uniGene.\n\n\n", "id": "30497848", "title": "UgMicroSatdb"}
{"url": "https://en.wikipedia.org/wiki?curid=4099290", "text": "Transgene\n\nA transgene is a gene or genetic material that has been transferred naturally, or by any of a number of genetic engineering techniques from one organism to another. The introduction of a transgene (called \"transgenesis\") has the potential to change the phenotype of an organism.\n\nIn its most precise usage, the term \"transgene\" describes a segment of DNA containing a gene sequence that has been isolated from one organism and is introduced into a different organism. This non-native segment of DNA may either retain the ability to produce RNA or protein in the transgenic organism or alter the normal function of the transgenic organism's genetic code. In general, the DNA is incorporated into the organism's germ line. For example, in higher vertebrates this can be accomplished by injecting the foreign DNA into the nucleus of a fertilized ovum. This technique is routinely used to introduce human disease genes or other genes of interest into strains of laboratory mice to study the function or pathology involved with that particular gene.\n\nThe construction of a transgene requires the assembly of a few main parts. The transgene must contain a promoter, which is a regulatory sequence that will determine where and when the transgene is active, an exon, a protein coding sequence (usually derived from the cDNA for the protein of interest), and a stop sequence. These are typically combined in a bacterial plasmid and the coding sequences are typically chosen from transgenes with previously known functions.\n\nTransgenic or genetically modified organisms, be they bacteria, viruses or fungi, serve all kinds of research purposes. Transgenic plants, insects, fish and mammals have been bred. Transgenic plants such as corn and soybean have replaced wild strains in agriculture in some countries (e.g. the United States). Transgene escape has been documented for GMO crops since 2001 with persistence and invasiveness. Transgenetic organisms pose ethical questions and may cause biosafety problems.\n\nThe idea of shaping an organism to fit a specific need isn't a new science; selective breeding of animals and plants started before recorded history. However, until the late 1900s farmers and scientist could breed new strains of a plant or organism only from closely related species, because the DNA had to be compatible for offspring to be able to reproduce another generation.\n\nIn the 1970 and 1980s, scientists passed this hurdle by inventing procedures for combining the DNA of two vastly different species with genetic engineering. The organisms produced by these procedures were termed transgenic. Transgenesis is the same as gene therapy in the sense that they both transform cells for a specific purpose. However, they are completely different in their purposes, as gene therapy aims to cure a defect in cells, and transgenesis seeks to produce a genetically modified organism by incorporating the specific transgene into every cell and changing the genome. Transgenesis will therefore change the germ cells, not only the somatic cells, in order to ensure that the transgenes are passed down to the offspring when the organisms reproduce. Transgenes alter the genome by blocking the function of a host gene; they can either replace the host gene with one that codes for a different protein, or introduce an additional gene.\n\nThe first transgenic organism was created in 1974 when Annie Chang and Stanley Cohen expressed \"Staphylococcus aureus\" genes in \"Escherichia coli\". In 1978, yeast cells were the first eukaryotic organisms to undergo gene transfer. Mouse cells were first transformed in 1979, followed by mouse embryos in 1980. Most of the very first transmutations were performed by microinjection of DNA directly into cells. Scientist were able to develop other methods to perform the transformations, such as incorporating transgenes into retroviruses and then infecting cells, using electroinfusion which takes advantage of an electric current to pass foreign DNA through the cell wall, biolistics which is the procedure of shooting DNA bullets into cells, and also delivering DNA into the egg that has just been fertilized.\n\nThe first transgenic animals were only intended for genetic research to study the specific function of a gene, and by 2003, thousands of genes had been studied.\n\nA variety of transgenic plants have been designed for agriculture to produce genetically modified crops, such as corn, soybean, rapeseed oil, cotton, rice and more. , these GMO crops were planted on 170 million hectares globally.\n\nOne example of a transgenic plant species is golden rice. In 1997, five million children developed xerophthalmia, a medical condition caused by vitamin A deficiency, in Southeast Asia alone. Of those children, a quarter million went blind. To combat this, scientists used biolistics to insert the daffodil phytoene synthase gene into Asia indigenous rice cultivars. The daffodil insertion increased the production of ß-carotene. The product was a transgenic rice species rich in vitamin A, called golden rice. Little is known about the impact of golden rice on xerophthalmia because anti-GMO campaigns have prevented the full commercial release of golden rice into agricultural systems in need.\n\nThe escape of genetically-engineered plant genes via hybridization with wild relatives was first discussed and examined in Mexico and Europe in the mid-1990s. There is agreement that escape of transgenes is inevitable, even \"some proof that it is happening\". Up until 2008 there were few documented cases.\n\nCorn sampled in 2000 from the Sierra Juarez, Oaxaca, Mexico contained a transgenic 35S promoter, while a large sample taken by a different method from the same region in 2003 and 2004 did not. A sample from another region from 2002 also did not, but directed samples taken in 2004 did, suggesting transgene persistence or re-introduction. A 2009 study found recombinant proteins in 3.1% and 1.8% of samples, most commonly in southeast Mexico. Seed and grain import from the United States could explain the frequency and distribution of transgenes in west-central Mexico, but not in the southeast. Also, 5.0% of corn seed lots in Mexican corn stocks expressed recombinant proteins despite the moratorium on GM crops.\n\nIn 2011, transgenic cotton was found in Mexico among wild cotton, after 15 years of GMO cotton cultivation.\n\nTransgenic rapeseed \"Brassicus napus\", hybridized with a native Japanese species \"Brassica rapa\", was found in Japan in 2011 after they had been identified 2006 in Québec, Canada. They were persistent over a 6-year study period, without herbicide selection pressure and despite hybridization with the wild form. This was the first report of the introgression—the stable incorporation of genes from one gene pool into another—of an herbicide resistance transgene from \"Brassica napus\" into the wild form gene pool.\n\nTransgenic creeping bentgrass, engineered to be glyphosate-tolerant as \"one of the first wind-pollinated, perennial, and highly outcrossing transgenic crops\", was planted in 2003 as part of a large (about 160 ha) field trial in central Oregon near Madras, Oregon. In 2004, its pollen was found to have reached wild growing bentgrass populations up to 14 kilometres away. Cross-pollinating \"Agrostis gigantea\" was even found at a distance of 21 kilometres. The grower, Scotts Company could not remove all genetically engineered plants, and in 2007, the U.S. Department of Agriculture fined Scotts $500,000 for noncompliance with regulations.\n\nThe long-term monitoring and controlling of a particular transgene has been shown not to be feasible. The European Food Safety Authority published a guidance for risk assessment in 2010.\n\nGenetically modified mice are the most common animal model for transgenic research. Transgenic mice are currently being used to study a variety of diseases including cancer, obesity, heart disease, arthritis, anxiety, and Parkinson’s disease. The two most common types of genetically modified mice are knockout mice and oncomice. Knockout mice are a type of mouse model that uses transgenic insertion to disrupt an existing gene’s expression. In order to create knockout mice, a transgene with the desired sequence is inserted into an isolated mouse blastocyst using electroporation. Then, homologous recombination occurs naturally within some cells, replacing the gene of interest with the designed transgene. Through this process, researchers were able to demonstrate that a transgene can be integrated into the genome of an animal, serve a specific function within the cell, and be passed down to future generations.\n\nOncomice are another genetically modified mouse species created by inserting transgenes that increase the animal’s vulnerability to cancer. Cancer researchers utilize oncomice to study the profiles of different cancers in order to apply this knowledge to human studies.\n\nMultiple studies have been conducted concerning transgenesis in \"Drosophila melanogaster\", the fruit fly. This organism has been a helpful genetic model for over 100 years, due to its well-understood developmental pattern. The transfer of transgenes into the \"Drosophila\" genome has been performed using various techniques, including P element, Cre-loxP, and ΦC31 insertion. The most practiced method used thus far to insert transgenes into the \"Drosophila\" genome utilizes P elements. The transposable P elements, also known as transposons, are segments of bacterial DNA that are translocated into the genome, without the presence of a complementary sequence in the host’s genome. P elements are administered in pairs of two, which flank the DNA insertion region of interest. Additionally, P elements often consist of two plasmid components, one known as the P element transposase and the other, the P transposon backbone. The transposase plasmid portion drives the transposition of the P transposon backbone, containing the transgene of interest and often a marker, between the two terminal sites of the transposon. Success of this insertion results in the nonreversible addition of the transgene of interest into the genome. While this method has been proven effective, the insertion sites of the P elements are often uncontrollable, resulting in an unfavorable, random insertion of the transgene into the \"Drosophila\" genome.\n\nTo improve the location and precision of the transgenic process, an enzyme known as Cre has been introduced. Cre has proven to be a key element in a process known as recombination-mediated cassette exchange (RMCE). While it has shown to have a lower efficiency of transgenic transformation than the P element transposases, Cre greatly lessens the labor-intensive abundance of balancing random P insertions. Cre aids in the targeted transgenesis of the DNA gene segment of interest, as it supports the mapping of the transgene insertion sites, known as loxP sites. These sites, unlike P elements, can be specifically inserted to flank a chromosomal segment of interest, aiding in targeted transgenesis. The Cre transposase is important in the catalytic cleavage of the base pairs present at the carefully positioned loxP sites, permitting more specific insertions of the transgenic donor plasmid of interest.\n\nTo overcome the limitations and low yields that transposon-mediated and Cre-loxP transformation methods produce, the bacteriophage ΦC31 has recently been utilized. Recent breakthrough studies involve the microinjection of the bacteriophage ΦC31 integrase, which shows improved transgene insertion of large DNA fragments that are unable to be transposed by P elements alone. This method involves the recombination between an attachment (attP) site in the phage and an attachment site in the bacterial host genome (attB). Compared to usual P element transgene insertion methods, ΦC31 integrates the entire transgene vector, including bacterial sequences and antibiotic resistance genes. Unfortunately, the presence of these additional insertions has been found to affect the level and reproducibility of transgene expression.\n\nThe study of application of transgenes is a rapidly growing area of molecular biology. In fact, it is predicted that in the next two decades, 300 000 lines of transgenic mice will be generated. Researchers have identified many applications for transgenes, particularly in the medical field. Scientists are focusing on the use of transgenes to study the function of the human genome in order to better understand disease, adapting animal organs for transplantation into humans, and the production of pharmaceutical products such as insulin, growth hormone, and blood anti-clotting factors from the milk of transgenic cows.\n\nThere are currently five thousand known genetic diseases, and the potential to treat these diseases using transgenic animals is, perhaps, one of the most promising applications of transgenes. There is a potential to use human gene therapy to replace a mutated gene with an unmutated copy of a transgene in order to treat the genetic disorder. This can be done through the use of Cre-Lox or knockout. Moreover, genetic disorders are being studied through the use of transgenic mice, pigs, rabbits, and rats. More recently, scientists have also begun using transgenic goats to study genetic disorders related to fertility.\n\nTransgenes may soon be used for xenotransplantation from pig organs. Through the study of xeno-organ rejection, it was found that an acute rejection of the transplanted organ occurs upon the organ's contact with blood from the recipient due to the recognition of foreign antibodies on endothelial cells of the transplanted organ. Scientists have identified the antigen in pigs that causes this reaction, and therefore are able to transplant the organ without immediate rejection by removal of the antigen. However, the antigen begins to be expressed later on, and rejection occurs. Therefore, further research is being conducted.\n\nTransgenes are being used by manufactures to produce goods such as milk with high levels of proteins, silk from the milk of goats, and microorganisms that are capable of producing proteins that contain enzymes that increase the rate of industrial reactions. Agricultural applications aim to selectively breed animals for particular traits and animals that are resistant to diseases.\n\nTransgene use in humans is currently fraught with issues. Transformation of genes into human cells has not been perfected yet. The most famous example of this involved certain patients developing T-cell leukemia after being treated for X-linked severe combined immunodeficiency (X-SCID). This was attributed to the close proximity of the inserted gene to the LMO2 promoter, which controls the transcription of the LMO2 proto-oncogene.\nIn common with most forms of genetic engineering, the use of transgenes for purposes other than to correct life-threatening genetic abnormalities is a major bioethical issue.\n\n\nGlowing monkeys 'to aid research' \n", "id": "4099290", "title": "Transgene"}
{"url": "https://en.wikipedia.org/wiki?curid=2739148", "text": "Tandemly arrayed genes\n\nTandemly arrayed genes (TAGs) are a gene cluster created by tandem duplications, a process in which one gene is duplicated and the copy is found adjacent to the original. They serve to encode large numbers of genes at a time. \n\nTAGs represent a large proportion of genes in a genome, including between 14% to 17% of the human, mouse, and rat genomes. TAG clusters may have as few as two genes, with small clusters predominating, but may consist of hundreds of genes. An example are tandem clusters of rRNA encoding genes. These genes are transcribed faster than they would be if only a single copy of the gene was available. Additionally, a single RNA gene may not be able to provide enough RNA, but tandem repeats of the gene allow sufficient RNA to be produced. For example, cells in a human embryo contain between five and ten million ribosomes, and cell number doubles within 24 hours. In order to provide the necessary ribosomes, multiple RNA polymerases must consecutively transcribe multiple rRNA genes. \n\nIn some species, such as Arabidopsis thaliana and Oryza sativa, most TAGs are the result of unequal chromosomal crossover during genetic recombination.\n\n\n", "id": "2739148", "title": "Tandemly arrayed genes"}
{"url": "https://en.wikipedia.org/wiki?curid=27058650", "text": "Plasticity product\n\nPlasticity Product is a term coined by Jerry Rudy to refer to mRNA genetic artifacts and protein products triggered by transcription factors leading to long-lasting long term potentiation.\n\nThe term \"plasticity product\" was coined by Jerry Rudy to refer to mRNA genetic artifacts and protein products triggered by transcription factors, leading long-lasting long term potentiation (L-LTP) and sustained alterations in synaptic strength.\n\nRudy differentiates between two types of long term potentiation: S-LTP (short-lasting) and L-LTP (long-lasting). In S-LTP the stimulus is strong enough to induce long-term potentiation but too weak to trigger intracellular events necessary to sustain synaptic changes. L-LTP is much less transient than S-LTP and involves the generation of new proteins through translation and transcription.\n\nInduction of L-LTP depends on the transcription of new mRNA and the translation of these new mRNA into proteins. These steps are encompassed by the genomic signaling hypothesis as follows:\n\nSupport for the genomic signaling hypothesis comes from studies conducted by Nguyen et al. demonstrating the inability to induce L-LTP following transcription inhibition immediately following the inducing stimulus but not if transcription is blocked later. The temporal effects of this inhibition suggests that L-LTP is dependent on newly synthesized \"plasticity products.\"\n\ncAMP-responsive element-binding (CREB) protein, a transcription factor, is also implicated in changes in synaptic plasticity. Inhibition of CREB translation likewise inhibited synaptic changes. CREB is activated in its phosphorylated form, acting as a molecular switch for production of plasticity products.\n\nThere are two waves of protein synthesis following LTP induction. The first involves local transcription and translation of mRNA and the second involves the genomic signaling cascade.\n\nA plasticity product must fit these criteria:\n", "id": "27058650", "title": "Plasticity product"}
{"url": "https://en.wikipedia.org/wiki?curid=30604495", "text": "Mutant protein\n\nA mutant protein is the protein product encoded by a gene with mutation. Mutated protein can have single amino acid change (minor, but still in many cases significant change leading to disease) or wide-range amino acid changes by e.g. truncation of C-terminus after introducing premature stop codon.\n", "id": "30604495", "title": "Mutant protein"}
{"url": "https://en.wikipedia.org/wiki?curid=31474", "text": "Transcription factor\n\nIn molecular biology, a transcription factor (TF) (or sequence-specific DNA-binding factor) is a protein that controls the rate of transcription of genetic information from DNA to messenger RNA, by binding to a specific DNA sequence. The function of TFs is to regulate - turn on and off - genes in order to make sure that they are expressed in the right cell at the right time and in the right amount throughout the life of the cell and the organism. Groups of TFs function in a coordinated fashion to direct cell division, cell growth, and cell death throughout life; cell migration and organization (body plan) during embryonic development; and intermittently in response to signals from outside the cell, such as a hormone. There are up to 2600 TFs in the human genome.\n\nTFs work alone or with other proteins in a complex, by promoting (as an activator), or blocking (as a repressor) the recruitment of RNA polymerase (the enzyme that performs the transcription of genetic information from DNA to RNA) to specific genes.\n\nA defining feature of TFs is that they contain at least one DNA-binding domain (DBD), which attaches to a specific sequence of DNA adjacent to the genes that they regulate. TFs are grouped into classes based on their DBDs. Other proteins such as coactivators, chromatin remodelers, histone acetyltransferases, histone deacetylases, kinases, and methylases are also essential to gene regulation, but lack DNA-binding domains, and therefore are not TFs.\n\nTFs are of interest in medicine because TF mutations can cause specific diseases, and medications can be potentially targeted toward them. \n\nTranscription factors are essential for the regulation of gene expression and are, as a consequence, found in all living organisms. The number of transcription factors found within an organism increases with genome size, and larger genomes tend to have more transcription factors per gene.\n\nThere are approximately 2600 proteins in the human genome that contain DNA-binding domains, and most of these are presumed to function as transcription factors, though other studies indicate it to be a smaller number. Therefore, approximately 10% of genes in the genome code for transcription factors, which makes this family the single largest family of human proteins. Furthermore, genes are often flanked by several binding sites for distinct transcription factors, and efficient expression of each of these genes requires the cooperative action of several different transcription factors (see, for example, hepatocyte nuclear factors). Hence, the combinatorial use of a subset of the approximately 2000 human transcription factors easily accounts for the unique regulation of each gene in the human genome during development.\n\nTranscription factors bind to either enhancer or promoter regions of DNA adjacent to the genes that they regulate. Depending on the transcription factor, the transcription of the adjacent gene is either up- or down-regulated. Transcription factors use a variety of mechanisms for the regulation of gene expression. These mechanisms include:\n\nTranscription factors are one of the groups of proteins that read and interpret the genetic \"blueprint\" in the DNA. They bind to the DNA and help initiate a program of increased or decreased gene transcription. As such, they are vital for many important cellular processes. Below are some of the important functions and biological roles transcription factors are involved in:\n\nIn eukaryotes, an important class of transcription factors called general transcription factors (GTFs) are necessary for transcription to occur. Many of these GTFs do not actually bind DNA, but rather are part of the large transcription preinitiation complex that interacts with RNA polymerase directly. The most common GTFs are TFIIA, TFIIB, TFIID (see also TATA binding protein), TFIIE, TFIIF, and TFIIH. The preinitiation complex binds to promoter regions of DNA upstream to the gene that they regulate.\n\nOther transcription factors differentially regulate the expression of various genes by binding to enhancer regions of DNA adjacent to regulated genes. These transcription factors are critical to making sure that genes are expressed in the right cell at the right time and in the right amount, depending on the changing requirements of the organism.\n\nMany transcription factors in multicellular organisms are involved in development. Responding to stimuli, these transcription factors turn on/off the transcription of the appropriate genes, which, in turn, allows for changes in cell morphology or activities needed for cell fate determination and cellular differentiation. The Hox transcription factor family, for example, is important for proper body pattern formation in organisms as diverse as fruit flies to humans. Another example is the transcription factor encoded by the Sex-determining Region Y (SRY) gene, which plays a major role in determining sex in humans.\n\nCells can communicate with each other by releasing molecules that produce signaling cascades within another receptive cell. If the signal requires upregulation or downregulation of genes in the recipient cell, often transcription factors will be downstream in the signaling cascade. Estrogen signaling is an example of a fairly short signaling cascade that involves the estrogen receptor transcription factor: Estrogen is secreted by tissues such as the ovaries and placenta, crosses the cell membrane of the recipient cell, and is bound by the estrogen receptor in the cell's cytoplasm. The estrogen receptor then goes to the cell's nucleus and binds to its DNA-binding sites, changing the transcriptional regulation of the associated genes.\n\nNot only do transcription factors act downstream of signaling cascades related to biological stimuli but they can also be downstream of signaling cascades involved in environmental stimuli. Examples include heat shock factor (HSF), which upregulates genes necessary for survival at higher temperatures, hypoxia inducible factor (HIF), which upregulates genes necessary for cell survival in low-oxygen environments, and sterol regulatory element binding protein (SREBP), which helps maintain proper lipid levels in the cell.\n\nMany transcription factors, especially some that are proto-oncogenes or tumor suppressors, help regulate the cell cycle and as such determine how large a cell will get and when it can divide into two daughter cells. One example is the Myc oncogene, which has important roles in cell growth and apoptosis.\n\nTranscription factors can also be used to alter gene expression in a host cell to promote pathogenesis. A well studied example of this are the transcription-activator like effectors (TAL effectors) secreted by Xanthomonas bacteria. When injected into plants, these proteins can enter the nucleus of the plant cell, bind plant promoter sequences, and activate transcription of plant genes that aid in bacterial infection. TAL effectors contain a central repeat region in which there is a simple relationship between the identity of two critical residues in sequential repeats and sequential DNA bases in the TAL effector’s target site. This property likely makes it easier for these proteins to evolve in order to better compete with the defense mechanisms of the host cell.\n\nIt is common in biology for important processes to have multiple layers of regulation and control. This is also true with transcription factors: Not only do transcription factors control the rates of transcription to regulate the amounts of gene products (RNA and protein) available to the cell but transcription factors themselves are regulated (often by other transcription factors). Below is a brief synopsis of some of the ways that the activity of transcription factors can be regulated:\n\nTranscription factors (like all proteins) are transcribed from a gene on a chromosome into RNA, and then the RNA is translated into protein. Any of these steps can be regulated to affect the production (and thus activity) of a transcription factor. An implication of this is that transcription factors can regulate themselves. For example, in a negative feedback loop, the transcription factor acts as its own repressor: If the transcription factor protein binds the DNA of its own gene, it down-regulates the production of more of itself. This is one mechanism to maintain low levels of a transcription factor in a cell.\n\nIn eukaryotes, transcription factors (like most proteins) are transcribed in the nucleus but are then translated in the cell's cytoplasm. Many proteins that are active in the nucleus contain nuclear localization signals that direct them to the nucleus. But, for many transcription factors, this is a key point in their regulation. Important classes of transcription factors such as some nuclear receptors must first bind a ligand while in the cytoplasm before they can relocate to the nucleus.\n\nTranscription factors may be activated (or deactivated) through their signal-sensing domain by a number of mechanisms including:\n\nIn eukaryotes, DNA is organized with the help of histones into compact particles called nucleosomes, where sequences of about 147 DNA base pairs make ~1.65 turns around histone protein octamers. DNA within nucleosomes is inaccessible to many transcription factors. Some transcription factors, so-called pioneering factors are still able to bind their DNA binding sites on the nucleosomal DNA. For most other transcription factors, the nucleosome should be actively unwound by molecular motors such as chromatin remodelers. Alternatively, the nucleosome can be partially unwrapped by thermal fluctuations, allowing temporary access to the transcription factor binding site. In many cases, a transcription factor needs to compete for binding to its DNA binding site with other transcription factors and histones or non-histone chromatin proteins. Pairs of transcription factors and other proteins can play antagonistic roles (activator versus repressor) in the regulation of the same gene.\n\nMost transcription factors do not work alone. Many large TF families form complex homotypic or heterotypic interactions through dimerization. For gene transcription to occur, a number of transcription factors must bind to DNA regulatory sequences. This collection of transcription factors, in turn, recruit intermediary proteins such as cofactors that allow efficient recruitment of the preinitiation complex and RNA polymerase. Thus, for a single transcription factor to initiate transcription, all of these other proteins must also be present, and the transcription factor must be in a state where it can bind to them if necessary.\nCofactors are proteins that modulate the effects of transcription factors. Cofactors are interchangeable between specific gene promoters; the protein complex that occupies the promoter DNA and the amino acid sequence of the cofactor determine its spatial conformation. For example, certain steroid receptors can exchange cofactors with NF-κB, which is a switch between inflammation and cellular differentiation; thereby steroids can affect the inflammatory response and function of certain tissues.\n\nTranscription factors are modular in structure and contain the following domains:\n\nTAD is domain of the transcription factor that binds other proteins such as transcription coregulators. Proteins containing TADs are Gal4, Gcn4, Oaf1, Leu3, Rtg3, Pho4, Gln3 in yeast and p53, NFAT, NF-κB and VP16 in mammals. Many TADs are as short as 9 amino acids (present in e.g., p53, VP16, MLL, E2A, HSF1, NF-IL6, NFAT1 and NF-κB Gal4, Pdr1, Oaf1, Gcn4, VP16, Pho4, Msn2, Ino2 and P201).\n\nThe portion (domain) of the transcription factor that binds DNA is called its DNA-binding domain. Below is a partial list of some of the major families of DNA-binding domains/transcription factors:\n\nThe DNA sequence that a transcription factor binds to is called a transcription factor-binding site or response element.\n\nTranscription factors interact with their binding sites using a combination of electrostatic (of which hydrogen bonds are a special case) and Van der Waals forces. Due to the nature of these chemical interactions, most transcription factors bind DNA in a sequence specific manner. However, not all bases in the transcription factor-binding site may actually interact with the transcription factor. In addition, some of these interactions may be weaker than others. Thus, transcription factors do not bind just one sequence but are capable of binding a subset of closely related sequences, each with a different strength of interaction.\n\nFor example, although the consensus binding site for the TATA-binding protein (TBP) is TATAAAA, the TBP transcription factor can also bind similar sequences such as TATATAT or TATATAA.\n\nBecause transcription factors can bind a set of related sequences and these sequences tend to be short, potential transcription factor binding sites can occur by chance if the DNA sequence is long enough. It is unlikely, however, that a transcription factor will bind all compatible sequences in the genome of the cell. Other constraints, such as DNA accessibility in the cell or availability of cofactors may also help dictate where a transcription factor will actually bind. Thus, given the genome sequence it is still difficult to predict where a transcription factor will actually bind in a living cell.\n\nAdditional recognition specificity, however, may be obtained through the use of more than one DNA-binding domain (for example tandem DBDs in the same transcription factor or through dimerization of two transcription factors) that bind to two or more adjacent sequences of DNA.\n\nTranscription factors are of clinical significance for at least two reasons: (1) mutations can be associated with specific diseases, and (2) they can be targets of medications.\n\nDue to their important roles in development, intercellular signaling, and cell cycle, some human diseases have been associated with mutations in transcription factors.\n\nMany transcription factors are either tumor suppressors or oncogenes, and, thus, mutations or aberrant regulation of them is associated with cancer. Three groups of transcription factors are known to be important in human cancer: (1) the NF-kappaB and AP-1 families, (2) the STAT family and (3) the steroid receptors.\n\nBelow are a few of the more well-studied examples:\n\nApproximately 10% of currently prescribed drugs directly target the nuclear receptor class of transcription factors. Examples include tamoxifen and bicalutamide for the treatment of breast and prostate cancer, respectively, and various types of anti-inflammatory and anabolic steroids. In addition, transcription factors are often indirectly modulated by drugs through signaling cascades. It might be possible to directly target other less-explored transcription factors such as NF-κB with drugs. Transcription factors outside the nuclear receptor family are thought to be more difficult to target with small molecule therapeutics since it is not clear that they are \"drugable\" but progress has been made on Pax2 and the notch pathway.\n\nGene duplications have played a crucial role in the evolution of species. This applies particularly to transcription factors. Once they occur as duplicates, accumulated mutations encoding for one copy can take place without negatively affecting the regulation of downstream targets. However, changes of the DNA binding specificities of the single-copy LEAFY transcription factor, which occurs in most land plants, have recently been elucidated. In that respect, a single-copy transcription factor can undergo a change of specificity through a promiscuous intermediate without losing function. Similar mechanisms have been proposed in the context of all alternative phylogenetic hypotheses, and the role of transcription factors in the evolution of all species.\n\nThere are different technologies available to analyze transcription factors. On the genomic level, DNA-sequencing and database research are commonly used The protein version of the transcription factor is detectable by using specific antibodies. The sample is detected on a western blot. By using electrophoretic mobility shift assay (EMSA), the activation profile of transcription factors can be detected. A multiplex approach for activation profiling is a TF chip system where several different transcription factors can be detected in parallel. This technology is based on DNA microarrays, providing the specific DNA-binding sequence for the transcription factor protein on the array surface.\n\nAs described in more detail below, transcription factors may be classified by their (1) mechanism of action, (2) regulatory function, or (3) sequence homology (and hence structural similarity) in their DNA-binding domains.\n\nThere are two mechanistic classes of transcription factors:\n\n\nTranscription factors have been classified according to their regulatory function:\n\nTranscription factors are often classified based on the sequence similarity and hence the tertiary structure of their DNA-binding domains:\n\n\n", "id": "31474", "title": "Transcription factor"}
{"url": "https://en.wikipedia.org/wiki?curid=30894898", "text": "Functional element SNPs database\n\nThe Functional Element SNPs Database (FESD) is a biological database of SNPs in Molecular biology.\nThe database is a tool designed to organize functional elements into categories in human gene regions and to output their sequences needed for genotyping experiments as well as provide a set of SNPs that lie within each region. The database defines functional elements into ten types: promoter regions, CpG islands,5' untranslated regions (5'-UTRs), translation start sites, splice sites, coding exons, introns, translation stop sites, polyadenylation signals, and 3' UTRs. People may reference this database for haplotype information or obtain a flanking sequence for genotyping. This may help in finding mutations that contribute to common and polygenic diseases. Researchers can manually choose a group of SNPs of special interest for certain functional elements along with their corresponding sequences. The database combines information from sources such as HapMap, UCSC GoldenPath, dbSNP, OMIM, and TRANSFAC. Users can obtain information about tagSNPs and simulate LD blocks for each gene. FESD is still a developing database and is not widely known so was unable to find projects that used the database so I found research using similar databases or databases that are combined in FESD’s information pool.\n\nThe database is a reference of all known SNPs in functional units that may affect a given phenotype in which some cases may be a disease. You select SNps based on disease, gene, or factor. The database has a link on the HapMap Project website. The database may very well have been used in one of the following research examples. The Korean HapMap Project website is quoted saying \"We have developed a series of software programs for association studies as well as the comparison and analysis of Korean HapMap data with other populations, such as European, Chinese, Japanese, and African populations. The developed soft- ware includes HapMapSNPAnalyzer, SNPflank, HWE Test, FESD, D2GSNP, SNP@Domain, KMSD, KFOD, KFRG, and SNP@WEB.\" I am unaware how many species are included; however, there is an alphabetical list of all the genes included in case you know the gene name but not the disease it is associated with or factor it is located in. Some of the genes were from Humans, mice, and some were from D. Melanogaster. Information cannot be submitted to the database publicly. The database is updated often and the information is pooled from the sites listed on the page. There is also a user guide tab that shows you what each process should look like.\n\nThe study involved the investigation of ITIH3 (inter alpha trypsin inhibitor heavy chain 3). Researchers used Functional analysis, Linkage disequilibrium mapping,SNP markers, and the HapMap database. FESD Version II may have been used since it has the information from HapMap as well as other databases. The found that the gene was on chromosome 3. In vitro functional analyses showed that this SNP enhanced the transcriptional level of the ITIH3 gene. Furthermore, we found expression of the ITIH3 protein in the vascular smooth muscle cells and macrophages in the human atherosclerotic lesions, suggesting ITIH3 SNP to be a novel genetic risk factor of MI. The gene has been shown to be related to the proinflammatory process of immune response. ITIH3 makes a complex with the locally synthesized hyaluronan (HA) and interacts with inflammatory cells. The ITIH3-HA complex has been reported to be involved in inflammatory diseases, including rheumatoid arthritis and inflammatory bowel diseases. The most statistically-significant SNP did not substitute an amino acid of ITIH3 protein. The researchers hypothesized that this synonymous SNP affected the transcriptional regulation, because several papers reported that some transcriptional factors bound to the exonic coding sequences of some genes and regulated their transcriptional level. There was expression of ITIH3 protein in the atherosclerotic lesion. ITIH3 might play a critical role in the pathogenesis of atherosclerosis and subsequent myocardial infarction.\nThese Researchers used snp selecting web tools and again could have used FESD Version II to select their SNPs. They hypothesized that the polymorphisms in circadian genes may be associated with prognosis of hepatocellular carcinoma. The circadian negative feedback regulation genes- CRY1, CRY2, PER1, PER2 and PER3 were identified based on a comprehensive literature review and the potentially functional SNPs were selected through web tools. Finally, a total of twelve potentially functional SNPs in five genes were selected, including CRY1: rs1056560, rs3809236; CRY2: rs6798, rs2292910; PER1: rs2585405, rs3027178; PER2: rs934945, rs2304669; PER3: rs2640908, rs172933, rs2859390, rs228669. When the dominant genetic model was tested, their data showed that only SNP rs2640908 in PER3 gene was significantly associated with overall survival of hepatocellular carcinoma patients. Patients carrying at least one variant allele of rs2640908 had a significantly decreased risk of death when compared with those carrying homozygous wild-type alleles.\n\nA polymorphism interaction analysis (PIA) algorithm was used to obtain cooperating single nucleotide polymorphisms (SNPs) that contribute to RA disease. The acquired SNP pairs were used to construct a SNP-SNP network. Sub-networks defined by hub SNPs were then extracted and turned into gene modules by mapping SNPs to genes using dbSNP database. The FESD Version II would also be a good choice in database because it also covers the dbSNP database. They ran the Haseman Elston test, then a random forest algorithm. The shared SNPs detected were then ran through a polymorphism interaction analysis (PIA) algorithm. They performed Gene ontology analysis of the 5 modules that were created. They suggested that Ets-1 may be an important transcription factor in the cytokine-mediated inflammatory pathway and destructive cascade characteristic of RA. The ATP-binding cassette (ABC) transporter, ABCA1, was induced during differentiation of human monocytes into macrophages, and there was a dual regulatory function for ABCA1 in macrophage lipid metabolism and inflammation. Extracellular signals are transduced intracellularly via multiple pathways, resulting in alterations in the transcription and translation of specific proteins. Some of these signaling pathways result in the production of proteins, including cytokines and matrix metalloproteinases, which are implicated in the pathogenesis of RA. The zinc-finger protein 238 (ZNF238) is attached to zinc-finger proteins that can regulate the human immunodeficiency virus type 1. Hub gene CD160 is a potential RA association gene. The CD160 receptor represents a unique triggering surface molecule that is expressed by cytotoxic NK cells, it participates in the inflammatory response and determines the type of subsequent specific immunity.\n\n", "id": "30894898", "title": "Functional element SNPs database"}
{"url": "https://en.wikipedia.org/wiki?curid=31001609", "text": "Single molecule fluorescent sequencing\n\nSingle molecule fluorescent sequencing is one method of DNA sequencing. The core principle is the imaging of individual fluorophore molecules, each corresponding to one base. By working on single molecule level, amplification of DNA is not required, avoiding amplification bias. The method lends itself to parallelization by probing many sequences simultaneously, imaging all of them at the same time. \n\nThe principle can be applied stepwise e.g. the Helicos implementation, or in realtime as in the Pacific Biosciences implementation.\n", "id": "31001609", "title": "Single molecule fluorescent sequencing"}
{"url": "https://en.wikipedia.org/wiki?curid=1920371", "text": "Underdominance\n\nIn genetics, underdominance (referred to in some texts as \"negative overdominance\") is the opposite of overdominance. It is the selection against the heterozygote, causing disruptive selection and divergent genotypes. Underdominance exists in situations where the heterozygotic genotype is inferior in fitness to either the dominant or recessive homozygotic genotype. Compared to examples of overdominance in actual populations, underdominance is considered more unstable and may lead to the fixation of either allele.\n\nAn example of stable underdominance may occur in individuals who are heterozygotic for polymorphisms that would make them better suited for one of two niches. Let us consider a situation in which a population is completely homozygotic for an \"A\" allele, allowing exploitation of a particular resource. Eventually, a polymorphic \"a\" allele may be introduced into the population, resulting in an individual who is capable of exploiting a different resource. This would result in an \"aa\" homozygotic invasion of the population due to nonexistent competition of the unexploited resource. The frequency of \"aa\" individuals would increase until the abundance of the \"a\" resource begins to decline. Eventually, the \"AA\" and \"aa\" genotypes would reach equilibrium with each other, with \"Aa\" heterozygotic individuals potentially experiencing a reduced fitness compared to those individuals who are homozygotic for utilization of either resource. This example of underdominance is stable because any shift in equilibrium would result in selection for the rare allele due to increased resource abundance. This compensatory selection would ultimately return the dimorphic system to underdominant equilibrium.\n\nAn example of stable underdominance can be found in the African butterfly species \"Pseudacraea eurytus\", which utilizes Batesian mimicry to escape predation. This species possesses two alleles which each confer an appearance similar to that of another local butterfly species that is toxic to its predator. Individuals who are heterozygous for this trait appear to be intermediate in appearance and thus experience increased predation and lowered overall fitness.\n\nModels of stable underdominance have shown potential in driving the introduction of refractory genes into pest populations that are responsible for the spread of infective diseases such as malaria and dengue fever. A refractory gene alone would not have higher fitness than the native genes, but engineered underdominance may prove effective as a mechanism to spread such a gene. In this model, two genetics constructs are introduced into two non-homologous chromosomes. Each construct is lethal when expressed individually but can be suppressed by the other construct. In this way, individuals with only one of the two constructed genes (heterozygotes) are selected against, but homozygotes with both or neither construct are genetically healthy. Analysis of this model using simple population genetics shows that successful spread of refractory genes using this engineered underdominance is possible with relatively small release of the constructed genotype into the population.\n\nA similar system of manipulation of pest populations was achieved in a population of \"Drosophila melanogaster\" by using a knock-down/rescue system. The genetic construct in this system employs a dsRNAi knockdown of a C-reactive protein, RpL14, as well as a rescue element (a complete copy of the wild type RpL14 gene). Individuals that are heterozygous for this construct experience lowered fitness due to limited restoration of the RpL14 gene, which results in reduced female fertility and delayed development, along with various other mutations that ultimately lower fitness by 70-80%. This system of underdominance allowed manipulation of the population and ultimate fixation of the constructed genotype and has potential applications in a number of settings, including agriculture and the reduction of various pest-carried diseases.\n\n\n", "id": "1920371", "title": "Underdominance"}
{"url": "https://en.wikipedia.org/wiki?curid=31084513", "text": "Centre for Molecular Medicine and Therapeutics\n\nThe Centre for Molecular Medicine and Therapeutics (CMMT) is part of the University of British Columbia's Faculty of Medicine. The Centre is located at the British Columbia Children's Hospital Research Institute (BCCHR) in Vancouver, British Columbia, Canada. Research at CMMT is focused on discovering genetic susceptibility to illnesses such as Huntington Disease, Type 2 diabetes and bipolar disorder.\n\nIn 2008, the founder of CMMT, Dr. Michael Hayden, was named \"Researcher of the Year\" by the Canada Institutes of Health Research. In 2011, he was appointed to the Order of Canada in 2011 for his contributions to Huntington's Disease research.\n\nPrincipal investigators within the Centre are involved in other initiatives. Dr. Daniel Goldowitz is the Scientific Director of the Kids Brain Health network (formerly NeuroDevNet), which is a Canada Networks of Centres of Excellence. The life sciences research laboratory consists of over 200 staff, eight of whom are UBC faculty members. Four of the principal investigators at the Centre are Canada Research Chairs.\n\n\nCMMT’s development started in the early nineties with an informal discussion between CMMT’s current Director, Dr. Michael Hayden, and Merck Frosst. In 1992, a $15 million commitment over five years from Merck Frosst Canada provided the initial funding for CMMT. The following year, in 1993, UBC Board of Governors and Senate approved CMMT as the first Centre in the Faculty of Medicine. That same year, the province of British Columbia pledged $9 million to build a dedicated building for CMMT.\n\nIn 1998, the current CMMT building, located at the Child & Family Research Institute on the BC Children’s and Women’s Hospital site was completed and the Transgenic Core Facility was established. During 1999 and 2000, the Scientific Stores Core Facility and the DNA Sequencing Core Facility were established.\n\nSince 2000, CMMT has further expanded its facilities and services, including the addition of a Bioanalyzer Core Facility in 2003, expansion of the Transgenic Core Facility in 2005 and the establishment of the Genotype and Gene Expression Core Facility in 2006.\n\n\n\n", "id": "31084513", "title": "Centre for Molecular Medicine and Therapeutics"}
{"url": "https://en.wikipedia.org/wiki?curid=31126328", "text": "TcoF-DB\n\nThe Dragon Database for Human Transcription Co-Factors and Transcription Factor Interacting Proteins (TcoF-DB) is a database that facilitates the exploration of proteins involved in the regulation of transcription in humans by binding to regulatory DNA regions (transcription factors) and proteins involved in the regulation of transcription in humans by interacting with transcription factors and not binding to regulatory DNA regions (transcription co-factors). The database describes a total of 529 (potential) human transcription co-factors interacting with a total of 1365 human transcription factors.\n\n", "id": "31126328", "title": "TcoF-DB"}
{"url": "https://en.wikipedia.org/wiki?curid=23252051", "text": "Additive genetic effects\n\nAdditive genetic effects are the contributions to the final phenotype from more than one gene, or from alleles of a single gene (in heterozygotes), that combine in such a way that the sum of their effects in unison is equal to the sum of their effects individually. Genetic effects that are not additive involve dominance (of alleles at a single locus) or epistasis (of alleles at more different loci).\n\n", "id": "23252051", "title": "Additive genetic effects"}
{"url": "https://en.wikipedia.org/wiki?curid=30130914", "text": "Constitutive ablation\n\nConstitutive ablation refers to gene expression which results in cell death. Constitutive cell ablation can be induced by diphtheria toxin (DT) in zebrafish.\n\n", "id": "30130914", "title": "Constitutive ablation"}
{"url": "https://en.wikipedia.org/wiki?curid=24719493", "text": "TA cloning\n\nTA cloning (also known as rapid cloning or T cloning) is a subcloning technique that avoids the use of restriction enzymes and is easier and quicker than traditional subcloning. So it is beneficial type of cloning. The technique relies on the ability of adenine (A) and thymine (T) (complementary basepairs) on different DNA fragments to hybridize and, in the presence of ligase, become ligated together. PCR products are usually amplified using Taq DNA polymerase which preferentially adds an adenine to the 3' end of the product. Such PCR amplified inserts are cloned into linearized vectors that have complementary 3' thymine overhangs.\n\nThe insert is created by PCR using Taq DNA polymerase. This polymerase lacks 3' to 5' proofreading activity and, with a high probability, adds a single, 3'-adenine overhang to each end of the PCR product. It is best if the PCR primers have guanines at the 5' end as this maximizes probability of Taq DNA polymerase adding the terminal adenosine overhang. Thermostable polymerases containing extensive 3´ to 5´ exonuclease activity should not be used as they do not leave the 3´ adenine-overhangs.\n\nThe target vector is linearized and cut with a blunt-end restriction enzyme. This vector is then tailed with dideoxythymidine triphosphate (ddTTP) using terminal transferase. It is important to use ddTTP to ensure the addition of only one T residue. This tailing leaves the vector with a single 3'-overhanging thymine residue on each blunt end. Manufacturers commonly sell TA Cloning \"kits\" with a wide range of prepared vectors that have already been linearized and tagged with an overhanging thymine .\n\nGiven that there is no need for restriction enzymes other than for generating the linearized vector, the procedure is much simpler and faster than traditional subcloning. There is also no need to add restriction sites when designing primers and thus shorter primers can be used saving time and money. In addition, in instances where there are no viable restriction sites that can be used for traditional cloning, TA cloning is often used as an alternative. The major downside of TA cloning is that directional cloning is not possible, so the gene has a 50% chance of getting cloned in the reverse direction.\n\nTOPO cloning\n", "id": "24719493", "title": "TA cloning"}
{"url": "https://en.wikipedia.org/wiki?curid=27884325", "text": "RecF pathway\n\nThe RecF pathway, also called the RecFOR pathway, is a pathway of homologous recombination that repairs DNA in bacteria. It repairs breaks that occur on only one of DNA's two strands, known as single-strand gaps. The RecF pathway can also repair double-strand breaks in DNA when the RecBCD pathway, another pathway of homologous recombination in bacteria, is inactivated by mutations. Like the RecBCD pathway, the RecF pathway requires RecA for strand invasion. The two pathways are also similar in their phases of branch migration, in which the Holliday junction slides in one direction, and resolution, in which the Holliday junctions are cleaved apart by enzymes.\n\nThe RecF pathway begins when RecJ, an exonuclease that cleaves single-stranded DNA in the 5 → 3′ direction, binds to the 5' end of a single-strand gap in DNA and starts moving upstream while cleaving the 5' strand. Although RecJ can function without them, single-strand binding protein (SSBP) and the RecQ helicase greatly increase how much the 5' is cut back. When present, SSBP binds to the 3' overhang remaining after RecJ finishes cutting back the 5' strand. By binding to the single-stranded DNA, SSBP ensures that the 3' DNA overhang does not stick to itself through self-complementation.\n\nThe RecA protein can be loaded onto the SSBP-coated 3' overhang in one of two distinct pathways, one that requires the RecFOR enzyme or one that requires the RecOR enzyme. In the RecFOR pathway, the RecFR complex binds where the single-strand DNA of the 3' meets the double-strand DNA. RecO then displaces SSBP from the ssDNA, although SSBP remains attached to RecO. RecFOR then loads RecA onto a recessed 5' end of this ssDNA-dsDNA junction. The RecR subunit in RecFR then interacts with RecO to form the RecFOR complex. In doing so, the RecR subunit helps to both detach the SSBP molecules from RecO and load molecules of the RecA protein onto the 3' overhang.\n\nThe RecOR pathway of RecA loading differs from the RecFOR pathway in several respects, most notably its molecular interaction requirements and its ideal DNA substrate. Unlike the RecFOR pathway, the RecOR pathway requires an interaction between RecO and the C-terminus of SSBP. The RecOR pathway also does not need a ssDNA-dsDNA junction to begin loading RecA onto the 3' overhang, whereas the RecFOR pathway typically does to work efficiently. Thus, the RecOR pathway in most conditions is more efficient than the RecFOR pathway in loading RecA.\n", "id": "27884325", "title": "RecF pathway"}
{"url": "https://en.wikipedia.org/wiki?curid=31133097", "text": "Ontario Genomics Institute\n\nOntario Genomics is a not-for-profit organization that manages cutting-edge genomics research projects and platforms.\n\nOntario Genomics acts as a catalyst for developing and applying genomic technologies across seven key sectors – agriculture, bioproducts, energy, forestry, health, mining, and water – to grow the province’s knowledge-based economy and create jobs and social benefits for all Ontarians.\n\nOntario Genomics (formerly the Ontario Genomics Institute) is a not-for-profit intermediary organization funded by the Ontario government and the federal research funding agency Genome Canada.\n\nEstablished in 2000, following a landmark decision by the Canadian Government to support the science of genomics, Ontario Genomics is the only entity focused solely on stimulating, enabling and nurturing genomics innovation in the province of Ontario.\n\nSince its inception, OG has:\n\nOntario Genomics' mission is to spark, support and sustain Ontario’s genomics technology pipeline as a key driver of the province’s knowledge-based economy. To accomplish this, OG works with four key stakeholders: Researchers, Companies, Startups and Policy Makers.\n\nResearchers:\n\nOG works with researchers at Ontario institutions to help them access funding and find the right industry partners. Support services include:\n\n· Working closely with scientists to develop competitive proposals for funding opportunities/\n\n· Connecting researchers with the right partners to increase likelihood of success.\n\n· Running small-scale internal competitions for proof-of-concept commercially oriented research projects.\n\nCompanies:\n\nOG works with established companies across seven sectors to understand their business challenges and help identify potential genomics solutions to boost productivity. Support services include:\n\n· Connections with academic experts.\n\n· Design and planning of collaborative proof-of-concept projects with academic partners.\n\n· Access to Genome Canada funding and small-scale financing to de-risk technology development and adoption.\n\nStartups:\n\nOG works with start-up companies in the genomics space to secure financing and grow. Services include:\n\n· Business advice and mentoring.\n\n· Connections with our network of industry partners and funders.\n\n· An avenue to Genome Canada funding for joint projects with academic researchers.\n\n· Small-scale proof-of-concept funding to facilitate follow-on investments.\n\nPolicymakers:\n\nOG works strategically with governments, educators, companies, researchers and other stakeholders to help them envision and embrace genomics innovation in their practices and help them act on relevant opportunities for Ontario. The goal of such work is to engender a culture with:\n\n· An innovation-friendly regulatory system\n\n· Proactive and anticipatory policies\n\n· Scientific collaborations and synergies\n\n· Engaged and informed young researchers\n\n· Tooled and connected entrepreneurs\n\n\n\n", "id": "31133097", "title": "Ontario Genomics Institute"}
{"url": "https://en.wikipedia.org/wiki?curid=510237", "text": "Reading frame\n\nIn molecular biology, a reading frame is a way of dividing the sequence of nucleotides in a nucleic acid (DNA or RNA) molecule into a set of consecutive, non-overlapping triplets. Where these triplets equate to amino acids or stop signals during translation, they are called codons.\n\nA single strand of a nucleic acid molecule has a phosphoryl end, called the 5′-end, and a hydroxyl or 3′-end. These define the 5'→3' direction. There are three reading frames that can be read in this 5'→3' direction, each beginning from a different nucleotide in a triplet. In a double stranded nucleic acid, an additional three reading frames may be read from the other, complementary strand in the 5'→3' direction along this strand. As the two strands of a double-stranded nucleic acid molecule are antiparallel, the 5'→3' direction on the second strand corresponds to the 3'→5' direction along the first strand.\n\nIn general, at the most, one reading frame in a given section of a nucleic acid, is biologically relevant (open reading frame). Some viral transcripts can be translated using multiple, overlapping reading frames. There is one known example of overlapping reading frames in mammalian mitochondrial DNA: coding portions of genes for 2 subunits of ATPase overlap.\n\nDNA encodes protein sequence by a series of three-nucleotide codons. Any given sequence of DNA can therefore be read in six different ways: Three reading frames in one direction (starting at different nucleotides) and three in the opposite direction. During transcription, the RNA polymerase read the template DNA strand in the 3'→5' direction, but the mRNA is formed in the 5' to 3' direction. The mRNA is single-stranded and therefore only contains three possible reading frames, of which only one is translated. The codons of the mRNA reading frame are translated in the 5'→3' direction into amino acids by a ribosome to produce a polypeptide chain.\n\nAn open reading frame (ORF) is a reading frame that has the potential to be transcribed into RNA and translated into protein. It requires a continuous sequence of DNA from a start codon, through a subsequent region which usually has a length that is a multiple of 3 nucleotides, to a stop codon in the same reading frame.\n\nWhen a putative amino acid sequence resulting from the translation of an ORF remained unknown in mitochondrial and chloroplast genomes, the corresponding open reading frame was called an unidentified reading frame (URF). For example, the MT-ATP8 gene was first described as URF A6L when the complete human mitochondrial genome was sequenced.\n\nThe usage of multiple reading frames leads to the possibility of overlapping genes; there may be many of these in virus, prokaryote, and mitochondrial genomes. Some viruses, e.g. Hepatitis B virus and BYDV, use several overlapping genes in different reading frames.\n\nIn rare cases, a ribosome may shift from one frame to another during translation of an mRNA (translational frameshift). This causes the first part of the mRNA to be translated in one reading frame, and the latter part to be translated in a different reading frame. This is distinct from a frameshift mutation, as the nucleotide sequence (DNA or RNA) is not altered—only the frame in which it is read.\n\n", "id": "510237", "title": "Reading frame"}
{"url": "https://en.wikipedia.org/wiki?curid=1435541", "text": "Non-Mendelian inheritance\n\nNon-Mendelian inheritance is a general term that refers to any pattern of inheritance in which traits do not segregate in accordance with Mendel's laws. These laws describe the inheritance of traits linked to single genes on chromosomes in the nucleus. In Mendelian inheritance, each parent contributes one of two possible alleles for a trait. If the genotypes of both parents in a genetic cross are known, Mendel’s laws can be used to determine the distribution of phenotypes expected for the population of offspring. There are several situations in which the proportions of phenotypes observed in the progeny do not match the predicted values.\n\nNon-Mendelian inheritance plays a role in several disease processes.\n\nExtranuclear inheritance (also known as cytoplasmic inheritance) is a form of non-Mendelian inheritance first discovered by Carl Correns in 1908. While working with \"Mirabilis jalapa\" Correns observed that leaf color was dependent only on the genotype of the maternal parent. Based on these data, he determined that the trait was transmitted through a character present in the cytoplasm of the ovule. Later research by Ruth Sager and others identified DNA present in chloroplasts as being responsible for the unusual inheritance pattern observed. Work on the poky strain of the mold \"Neurospora crassa\" begun by Mary and Hershel Mitchell ultimately led to the discovery of genetic material in mitochondria as well.\n\nAccording to the endosymbiont theory, mitochondria and chloroplasts were once free living organisms that were each taken up by a eukaryotic cell. Over time, mitochondria and chloroplasts formed a symbiotic relationship with their eukaryotic hosts. Although the transfer of a number of genes from these organelles to the nucleus prevents them from living independently, each still possesses genetic material in the form of double stranded DNA.\n\nIt is the transmission of this organellar DNA that is responsible for the phenomenon of extranuclear inheritance. Both chloroplasts and mitochondria are present in the cytoplasm of maternal gametes only. Paternal gametes (sperm for example) do not have cytoplasmic mitochondria. Thus, the phenotype of traits linked to genes found in either chloroplasts or mitochondria are determined exclusively by the maternal parent.\n\nIn humans, mitochondrial diseases are a class of diseases, many of which affect the muscles and the eye.\n\nGene conversion can be one of the major forms of non-Mendelian inheritance. Gene conversion arises during DNA repair via DNA recombination, by which a piece of DNA sequence information is transferred from one DNA helix (which remains unchanged) to another DNA helix, whose sequence is altered. This may occur as a mismatch repair between the strands of DNA which are derived from different parents. Thus the mismatch repair can convert one allele into the other. This phenomenon can be detected through the offspring non-Mendelian ratios, and is frequently observed, e.g., in fungal crosses.\n\nAnother form of non-Mendelian inheritance is known as infectious heredity. Infectious particles such as viruses may infect host cells and continue to reside in the cytoplasm of these cells. If the presence of these particles results in an altered phenotype, then this phenotype may be subsequently transmitted to progeny. Because this phenotype is dependent only on the presence of the invader in the host cell’s cytoplasm, inheritance will be determined only by the infected status of the maternal parent. This will result in a uniparental transmission of the trait, just as in extranuclear inheritance.\n\nOne of the most well studied examples of infectious heredity is the killer phenomenon exhibited in yeast. Two double-stranded RNA viruses, designated L and M, are responsible for this phenotype. The L virus codes for the capsid proteins of both viruses, as well as an RNA polymerase. Thus the M virus can only infect cells already harboring L virus particles. The M viral RNA encodes a toxin which is secreted from the host cell. It kills susceptible cells growing in close proximity to the host. The M viral RNA also renders the host cell immune to the lethal effects of the toxin. For a cell to be susceptible it must therefore be either uninfected, or harbor only the L virus.\n\nThe L and M viruses are not capable of exiting their host cell through conventional means. They can only transfer from cell to cell when their host undergoes mating. All progeny of a mating involving a doubly infected yeast cell will also be infected with the L and M viruses. Therefore, the killer phenotype will be passed down to all progeny.\n\nHeritable traits that result from infection with foreign particles have also been identified in \"Drosophila\". Wild type flies normally full recover after being anesthetized with carbon dioxide. Certain lines of flies have been identified that die off after exposure to the compound. This carbon dioxide sensitivity is passed down from mothers to their progeny. This sensitivity is due to infection with σ (Sigma) virus, a rhabdovirus only capable of infecting \"Drosophila\".\n\nAlthough this process is usually associated with viruses, recent research has shown that the \"Wolbachia\" bacterium is also capable of inserting its genome into that of its host.\n\nGenomic imprinting represents yet another example of non-Mendelian inheritance. Just as in conventional inheritance, genes for a given trait are passed down to progeny from both parents. However, these genes are epigenetically marked before transmission, altering their levels of expression. These imprints are created before gamete formation and are erased during the creation of germ line cells. Therefore, a new pattern of imprinting can be made with each generation.\n\nGenes are imprinted differently depending on the parental origin of the chromosome that contains them. In mice, the insulin-like growth factor 2 gene undergoes imprinting. The protein encoded by this gene helps to regulate body size. Mice that possess two functional copies of this gene are larger than those with two mutant copies. The size of mice that are heterozygous at this locus depends on the parent from which the wild type allele came. If the functional allele originated from the mother, the offspring will exhibit dwarfism, whereas a paternal allele will generate a normal sized mouse. This is because the maternal Igf2 gene is imprinted. Imprinting results in the inactivation of the Igf2 gene on the chromosome passed down by the mother.\n\nImprints are formed due to the differential methylation of paternal and maternal alleles. This results in differing expression between alleles from the two parents. Sites with significant methylation are associated with low levels of gene expression. Higher gene expression is found at unmethylated sites. In this mode of inheritance, phenotype is determined not only by the specific allele transmitted to the offspring, but also by the sex of the parent that transmitted it.\n\nIndividuals who possess cells with genetic differences from the other cells in their body are termed mosaics. These differences can result from mutations that occur in different tissues and at different periods of development. If a mutation happens in the non-gamete forming tissues, it is characterized as somatic. Germline mutations occur in the egg or sperm cells and can be passed on to offspring. Mutations that occur early on in development will affect a greater number of cells and can result in an individual that can be identified as a mosaic strictly based on phenotype.\n\nMosaicism also results from a phenomenon known as X-inactivation. All female mammals have two X chromosomes. To prevent lethal gene dosage problems, one of these chromosomes is inactivated following fertilization. This process occurs randomly for all of the cells in the organism’s body. Because a given female’s two X chromosomes will almost certainly differ in their specific pattern of alleles, this will result in differing cell phenotypes depending on which chromosome is silenced. Calico cats, which are almost all female, demonstrate one of the most commonly observed manifestations of this process.\n\nTrinucleotide repeat disorders also follow a non-Mendelian pattern of inheritance. These diseases are all caused by the expansion of microsatellite tandem repeats consisting of a stretch of three nucleotides. Typically in individuals, the number of repeated units is relatively low. With each successive generation, there is a chance that the number of repeats will expand. As this occurs, progeny can progress to premutation and ultimately affected status. Individuals with a number of repeats that falls in the premutation range have a good chance of having affected children. Those who progress to affected status will exhibit symptoms of their particular disease. Prominent trinucleotide repeat disorders include Fragile X syndrome and Huntington's disease. In the case of Fragile X syndrome it is thought that the symptoms result from the increased methylation and accompanying reduced expression of the fragile X mental retardation gene in individuals with a sufficient number of repeats.\n\n\n", "id": "1435541", "title": "Non-Mendelian inheritance"}
{"url": "https://en.wikipedia.org/wiki?curid=7153075", "text": "Congenic\n\nIn genetics, two organisms that differ in only one locus and a linked segment of chromosome are defined as congenic. Similarly, organisms that are coisogenic differ in one locus only and not in the surrounding chromosome. Unlike congenic organisms, coisogenic organisms cannot be bred and only occur through spontaneous or targeted mutation at the locus.\n\nCongenic strains are generated in the laboratory by mating two inbred strains (usually rats or mice), and back-crossing the descendants 5–10 generations with one of the original strains, known as the \"recipient\" strain. Typically selection for either phenotype or genotype is performed prior to each back-cross generation. In this manner either an interesting phenotype, or a defined chromosomal region assayed by genotype, is passed from the \"donor\" strain onto an otherwise uniform \"recipient\" background. Congenic mice or rats can then be compared to the pure recipient strain to determine whether they are phenotypically different if selection was for a genotypic region, or to identify the critical genetic locus, if selection was for a phenotype.\n\nSpeed congenics can be produced in as little as five back-cross generations, through the selection at each generation of offspring that not only retain the desired chromosomal fragment, but also 'lose' the maximum amount of background genetic information from the donor strain. This is also known as \"marker-assisted\" congenics, due to the use of genetic markers, typically microsatellite markers, but now, more commonly, single nucleotide polymorphism markers (SNPs). The process can be further aided by the superovulation of females, to produce many more eggs.\n\n\nCongenic strains are discussed in detail in Lee Silver's online book \"Mouse Genetics: Concepts and Applications\":\n", "id": "7153075", "title": "Congenic"}
{"url": "https://en.wikipedia.org/wiki?curid=9997271", "text": "Immunogenetics\n\nImmunogenetics or immungenetics is the branch of medical genetics that explores the relationship between the immune system and genetics.\n\nAutoimmune diseases, such as type 1 diabetes, are complex genetic traits which result from defects in the immune system. Identification of genes defining the immune defects may identify new target genes for therapeutic approaches. Alternatively, genetic variations can also help to define the immunological pathway leading to disease.\n\nIts origin is usually attributed to Edward Jenner, who discovered in 1796 that cowpox, or vaccinia, induced protection against human smallpox.\nIt took two centuries before the World Health Organization announced in 1979 that smallpox had been eradicated.\n\nThe term immunogenetics is based on the two words immunology and genetics, and is defined as \"a sub-category of genetics focussing on the genetic basis of the immune reaction\" according to MeSH.\nGenetics (based on Greek γενεά \"geneá\" \"descent\" and γένεσις \"génesis\" \"origin\") is the science researching the transfer of characteristics from one generation to the next. The genes of an organism (strands of DNA) and the transfer of genes from the parent to the child generation of an organism in the scope of possible variations are the basis of genetics.\n\nImmunology deals with the biological and biochemical basis for the body's defense against germs (such as bacteria, viruses, and fungi), as well as against foreign agents such as biological toxins and environmental pollutants, and failures and malfunctions of these defence mechanisms. Apart from these external effects on the organism, there are also defence reactions regarding the body's own cells, e.g. in the scope of the bodily reactions on cancer and the lacking reaction of a body on healthy cells in the scope of an immune-mediated disease. Hence, immunology is a sub-category of biology.\n\nThe term immunogenetics comprises all processes of an organism, which are, on the one hand, controlled and influenced by the genes of the organism, and are, on the other hand, significant with regard to the immunological defence reactions of the organism.\n\nThe medical history of immunology and the immune system dates back to the 19th century. However, the prevention and early defence of diseases was an essential task for all shamans, medicine men and early \"doctors\" during the entire human evolution. The first Nobel Prize in the field of immunogenetics was awarded to Baruj Benacerraf, Jean Dausset and George Davis Snell in 1980 for discovering genetically determined cellular surface structures, which control immunological reactions.\n\nIn the last 20 years, research activities focussed on a large number of different questions in immunogenetics. Both the acceleration of and the decreasing costs for the sequencing of the genes have resulted in a more intensive research of both academic and commercial working groups. Current research topics particularly deal with forecasts on the course of diseases and therapy recommendations due to genetic dispositions and how these dispositions can be affected by agents (gene therapy).\n\nA special focus is often laid on the forecast regarding and therapy of genetically based autoimmune diseases, which include all diseases caused by an extreme reaction of the immune system against the body's own tissue. By mistake, the immune system recognises the body's own tissue as a foreign object which is to be fought. This can result in serious inflammatory reactions which may permanently damage the respective organs. Autoimmune diseases, the outbreak and/or course of which can be visible in the individual genome of the organism, include multiple sclerosis, diabetes type I, rheumatoid arthritis and Crohn's disease (an example for an immune-mediated disease without genetic disposition is HIV, which is caused by virus). As for multiple sclerosis, an article in the journal \"Nature\" dated May 2010 (Baranzini et al.: Genome, epigenome and RNA sequences of monozygotic twins discordant for multiple sclerosis. Nature 2010, 464; S. 1351-1356 – for link see below) showed that this autoimmune disease is not caused by a genetic variation but the course and the treatability are considerably influenced by genetic dispositions. This research was based on analysing three monovular pairs of twins, of which one twin has contracted multiple sclerosis whereas the other one has not.\n\n", "id": "9997271", "title": "Immunogenetics"}
{"url": "https://en.wikipedia.org/wiki?curid=13839719", "text": "Loop-mediated isothermal amplification\n\nLoop mediated isothermal amplification (LAMP) is a single tube technique for the amplification of DNA. This may be of use in future as a low cost alternative to detect certain diseases. It may be combined with a reverse transcription step to allow the detection of RNA.\n\nLAMP is an isothermal nucleic acid amplification technique. In contrast to the polymerase chain reaction (PCR) technology in which the reaction is carried out with a series of alternating temperature steps or cycles, isothermal amplification is carried out at a constant temperature, and does not require a thermal cycler.\n\nIn LAMP, the target sequence is amplified at a constant temperature of 60–65 °C using either two or three sets of primers and a polymerase with high strand displacement activity in addition to a replication activity. Typically, 4 different primers are used to identify 6 distinct regions on the target gene, which adds highly to the specificity. An additional pair of \"loop primers\" can further accelerate the reaction. \nDue to the specific nature of the action of these primers, the amount of DNA produced in LAMP is considerably higher than PCR based amplification.\n\nDetection of amplification product can be determined via photometry for turbidity caused by an increasing quantity of magnesium pyrophosphate precipitate in solution as a byproduct of amplification. This allows easy visualization by the naked eye, especially for larger reaction volumes, or via simple detection approaches for smaller volumes. \nThe reaction can be followed in real-time either by measuring the turbidity \nor by fluorescence using intercalating dyes such as SYTO 9. \nDyes such as SYBR green, can be used to create a visible color change that can be seen with the naked eye without the need for expensive equipment, or a response that can more accurately be measured by instrumentation. Dye molecules intercalate or directly label the DNA, and in turn can be correlated to the number of copies initially present. Hence, LAMP can also be quantitative. In-tube detection of DNA amplification is possible using manganese loaded calcein which starts fluorescing upon complexation of manganese by pyrophosphate during in vitro DNA synthesis.Moreover, visual detection of the LAMP amplicons by the unaided eye was based on their ability to hybridize with the complementary gold-bound ss-DNA and thus prevent the normal red to purple-blue color change that would otherwise occur by salt-induced aggregation of the gold particles. So, a LAMP method combined with amplicon detection by AuNP has advantages over previously other methods in terms of reduced assay time, amplicon confirmation by hybridization and use of simpler equipment (i.e., no need for a thermocycler, electrophoresis equipment or a UV trans-illuminator.\n\nLAMP is a relatively new DNA amplification technique, which due to its simplicity, ruggedness, and low cost could provide major advantages. \nLAMP has the potential to be used as a simple screening assay in the field or at the point of care by clinicians. \nBecause LAMP is isothermal, which eradicates the need for expensive thermocyclers used in conventional PCR, it may be a particularly useful method for infectious disease diagnosis in low and middle income countries. While LAMP is widely being studied for detecting infectious diseases such as tuberculosis, malaria, and sleeping sickness. In developing regions, it has yet to be extensively validated for other common pathogens.\n\nLAMP has been observed to be less sensitive (more resistant) than PCR to inhibitors in complex samples such as blood, likely due to use of a different DNA polymerase (typically \"Bst\" DNA polymerase rather than \"Taq\" polymerase as in PCR). Several reports describe successful detection of pathogens from minimally processed samples such as heat-treated blood, or in presence of clinical sample matrices. This feature of LAMP may be useful in low-resource or field settings where a conventional DNA or RNA extraction prior to diagnostic testing may be impractical.\n\nLAMP is less versatile than PCR, the most familiar nucleic acid amplification technique. LAMP is useful primarily as a diagnostic or detection technique, but is not useful for cloning or myriad other molecular biology applications enabled by PCR. Because LAMP uses 4 (or 6) primers targeting 6 (or 8) regions within a fairly small segment of the genome, and because primer design is subject to numerous constraints, it is difficult to design primer sets for LAMP \"by eye\". Free, open-source or commercial software packages are generally used to assist with LAMP primer design, although the primer design constraints mean there is less freedom to choose the target site than with PCR. In a diagnostic application, this must be balanced against the need to choose an appropriate target (e.g., a conserved site in a highly variable viral genome, or a target that is specific for a particular strain of pathogen).\n\nMultiplexing approaches for LAMP are less developed than for PCR. The larger number of primers per target in LAMP increases the likelihood of primer-primer interactions for multiplexed target sets. The product of LAMP is a series of concatemers of the target region, giving rise to a characteristic \"ladder\" or banding pattern on a gel, rather than a single band as with PCR. Although this is not a problem when detecting single targets with LAMP, \"traditional\" (endpoint) multiplex PCR applications wherein identity of a target is confirmed by size of a band on a gel are not feasible with LAMP. Multiplexing in LAMP has been achieved by choosing a target region with a restriction site, and digesting prior to running on a gel, such that each product gives rise to a distinct size of fragment, although this approach adds complexity to the experimental design and protocol. The use of a strand-displacing DNA polymerase in LAMP also precludes the use of hydrolysis probes, e.g. TaqMan probes, which rely upon the 5'-3' exonuclease activity of \"Taq\" polymerase. An alternative real-time multiplexing approach based on fluorescence quenchers has been reported.\n", "id": "13839719", "title": "Loop-mediated isothermal amplification"}
{"url": "https://en.wikipedia.org/wiki?curid=10977613", "text": "RNA triphosphatase\n\nIn molecular biology, RNA 5'-triphosphatases (RTPases) are phosphatases that cleave the 5'-terminal γ-β phosphoanhydride bond of nascent messenger RNA molecules, enabling the addition of a five-prime cap as part of post-transcriptional modifications. RTPases generate 5'-diphosphate-ended mRNA and a phosphate ion from 5'-triphosphate-ended precursor mRNA. mRNA guanylyltransferase then adds a backwards guanosine monophosphate (GMP) group from GTP, generating pyrophosphate, and mRNA (guanine-N7-)-methyltransferase methylates the guanine to form the final 5'-cap structure.\n\nThere are two families of RTPases known so far:\n\n", "id": "10977613", "title": "RNA triphosphatase"}
{"url": "https://en.wikipedia.org/wiki?curid=32379480", "text": "Neurocriminology\n\nNeurocriminology is an emerging sub-discipline of biocriminology and criminology that applies brain imaging techniques and principles from neuroscience to understand, predict, and prevent crime.\n\nThe main idea behind neurocriminology is that crime is only partially a social and environmental problem, and biological factors along with medical conditions play a significant role. Deviant brain theories have always been part of biocriminology, which explains crime with biological reasons. Neurocriminology has become mainstream during the past two decades, since contemporary biocriminologists focus almost exclusively on brain due to significant advances in neuroscience. Even though neurocriminology is still at odds with traditional sociological theories of crime, it is becoming more popular in the scientific community.\n\nThe origins of neurocriminology go back to one of the founders of modern criminology, 19th-century Italian psychiatrist and prison doctor Cesare Lombroso, whose beliefs that the crime originated from brain abnormalities were partly based on phrenological theories about the shape and size of the human head. Lombroso conducted a postmortem on a serial killer and rapist, who had an unusual indentation at the base of the skull. Lombroso discovered a hollow part in the killer’s brain where the cerebellum would be. Lombroso’s theory was that crime originated in part from abnormal brain physiology and that violent criminals where throwbacks to less evolved human types identifiable by ape-like physical characteristics. Criminals, he believed, could be identified by physical traits, such as a large jaw and sloping forehead. The contemporary neuroscientists further developed his idea that physiology and traits of the brain underlie all crime. The term “neurocriminology” was first introduced by James Hilborn (Cognitive Centre of Canada) and adopted by the leading researcher in the field, Dr. Adrian Raine, the chair of the Criminology Department at University of Pennsylvania. He was the first to conduct brain imaging study on violent criminals.\n\nMany recent studies have revealed that sometimes structural and functional abnormalities are so striking that anyone can see them. Some violent offenders, however, have subtle structural or functional abnormalities and even highly experienced neuroradiologists cannot detect these irregularities right away. Yet, the abnormalities can be detected using brain imaging and state-of-the-art analytic tools.\n\nStudies on structural deficiencies suggest that people consistently behaving antisocially have structurally impaired brains. The abnormalities can be either of general character or affect specific regions of the brain that control emotions, aggression or are responsible for ethical decisions:\n\n\"Low Number of Neurons in the Prefrontal Cortex\".\nA study in 2000 determined that people with a history of persistent antisocial behavior had an 11 percent reduction in the volume of gray matter in the prefrontal cortex, while white matter volume was normal.\nSimilarly, A 2009 meta-analysis study, which pooled together the findings of 12 anatomical brain-imaging studies conducted on offender populations, found that the prefrontal cortex of the brain is indeed structurally impaired in offenders.\n\n\"Underdeveloped Amygdalae\". Two studies found that both the left and especially the right amygdalae are impaired in psychopaths. The psychopaths had on average 18 percent reduction in the volume of the right amygdala.\n\n\"Cavum Septum Pellucidum Maldevelopment\". A study in 2010 suggested that people with cavum septum pellucidum were prone to psychopathy, antisocial personality disorder, and had more charges and convictions for criminal offenses. This brain maldevelopment was especially linked to lifelong antisocial behavior, i.e. a reckless disregarded for self and others, lack of remorse, and aggression.\n\n\"Bigger Right Hippocampus\". A 2004 study suggested that the psychopaths’ right hippocampus that partially controls emotions and regulates aggression was significantly bigger than the left. This asymmetry was also true in normal people, but it was much more noticeable in psychopaths.\n\n\"Increase in the Volume of the Striatum\". A study in 2010 found that psychopathic individuals showed a 10 percent increase in the volume of the striatum.\n\n\"Damage by Foreign Objects\". A large number of studies on structural damage by foreign objects convincingly shows that adults suffering head injuries damaging the prefrontal cortex show impulsive and antisocial behavior that does not conform to the norms of society.\nThere is a number of famous life stories showing the same causal connection. For example, P. Gage was a well-respected, well-liked, and responsible gentleman. In 1848 because of a construction accident he suffered a serious damage to his brain when a metal rod propelled by an explosive entered his lower left cheek and exited from the top-middle part of his head. Gage healed quickly. After that accident, however, he became erratic, disrespectful, and vulgar. Gage had been transformed from a well-controlled, well-respected person to an individual with psychopathic traits.\n\n\"Damage by Tumors\". There is also a number of famous U.S. criminal cases showing that damage of the brain by tumors can result in the same transformation as the damage by foreign objects. \nCharles Whitman, for instance, was a young man who studied architectural engineering at the University of Texas. Whitman had no history of violence or crime. As a child, he scored 138 on the Stanford-Binet IQ test, placing in the 99th percentile. He was an Eagle Scout, volunteered as a scoutmaster, and served in Marine Corps. In 1966 Whitman unexpectedly killed his mother as well as wife, ascended the belltower of the University of Texas, Austin, and fired a rifle at students below. He killed 15 people and wounding 31 more before police officers shot him. Whitman in his final note complained of inability to control his thoughts and requested an autopsy, which revealed a brain tumor in the hypothalamus region of his brain, a growth that, some hypothesized, put pressure on his amygdala.\nMichael Oft’s was a teacher in Virginia who had no prior psychiatric nor deviant behavior history. At the age of forty, his behavior suddenly changed. He began to frequent massage parlors, collect child pornography, abuse his step-daughter, and was soon found guilty of child molestation. Mr. Oft opted for a treatment program for pedophiles, but still couldn’t resist soliciting sexual favors from staff and other clients at the rehabilitation center. A neurologist advised a brain scan, which showed a tumor growing at the base of his orbitofrontal cortex, compressing the right prefrontal region of his brain. After the tumor was removed, Mr. Oft’s emotion, behavior and sexual activity returned to normal. But after several months of normal behavior Mr. Oft again began to collect child pornography. Neurologists rescanned his brain and found that the tumor had grown back. After the second surgery removing the tumor, his behavior have been totally appropriate.\n\nSimilarly to neurophysiological studies neurofunctional showed that brains of criminals and psychopaths not only are structures differently but also operate in a different way. As you can see below, both structural and functional abnormalities tend to affect the same areas of the brain. These are the major abnormalities found:\n\n\"Lack of Activation in the Prefrontal Cortex\". A number of studies replicated the observance that violent criminals’ brains showed a significant reduction in prefrontal glucose metabolism.\n\n\"Reduced Activity In The Amygdala\". A study found that individuals with high psychopathy scores showed reduced activity in the amygdala during emotional, personal moral decision-making. \n\"Dysfunctional Posterior Cingulate\". Two studies found that posterior cingulate functions poorly in adult criminal psychopaths and aggressive patients.\n\n\"Reduced Cerebral Blood Flow in Angular Gyrus\". A couple of studies found reduced cerebral blood flow in angular gyrus of murderers and impulsive, violent criminals.\n\n\"Higher Activation of Subcortical Limbic Regions\". A 1998 study showed higher activation of subcortical limbic regions of two groups of reactive and proactive murderers, especially in the more “emotional” right hemisphere of the brain.\n\n\"Functional Disturbances of the Hippocampus and Its Parahippocampal Gyrus\". A number of studies suggest that this region of the brain is not working properly in murders and violent offenders in general.\n\nUnlike the founding farther of criminology, Cesare Lombroso, who thought that crime was fundamentally biological in its origin and criminals lacked free will altogether, contemporary neurocriminologists seem to take the middle ground approach. They do not argue that biological factors alone cause behavioral problems, but recognize that behavior results from interaction between biology and environment. Some authors, however, are more determinist in their views. As one author writes: \"\"Free will may exist (it may simply be beyond our current science), but one thing seems clear: if free will does exist, it has little room in which to operate. It can at best be a small factor riding on top of vast neural networks shaped by genes and environment. In fact, free will may end up being so small that we eventually think about bad decision-making in the same way we think about any physical process, such as diabetes or lung disease.\"\"\n\nUS legal defense teams increasingly use brain scans as mitigating evidence in trials of violent criminals and sex offenders. See Neurolaw for more. Here are some of the most famous cases:\n\nIn 1991, a sixty-five-year-old advertising executive with no prior history of crime or violence after an argument strangled his wife, opened the window and threw her out of their 12th-floor apartment. His defense team had a structural brain scan done using MRI and PET scan. The images showed a big piece missing from the prefrontal cortex of the brain, i.e., a subarachnoid cyst was growing in his left frontal lobe. The defense team used these images to argue that Weinstein had an impaired ability to regulate his emotions and make rational decisions. The team went with an insanity defense, and the prosecution and defense agreed to a plea of manslaughter. As a result, Weinstein was given a seven-year sentence in contrast to the twenty-five-year sentence he would have served if he had been convicted of second-degree murder.\n\nBustamante was a well-behaved teenager who suddenly at the age of 22 became a career criminal. His crimes included theft, breaking and entering, drug offenses, and robbery. In 1990 Bustamante was charged with a homicide. The defense team discovered that the client had suffered a head injury from a crowbar at the age of twenty. Bustamante’s behavior changed fundamentally after that, transforming him from a normal individual into an impulsive and emotionally labile criminal. The defense team had his client’s brain scanned, which revealed malfunctioning of the prefrontal cortex. At the end the jury believed that Bustamante’s brain was not normal and spared him from the death penalty.\n\nIn 1999, Page robbed, raped and killed a female student in Denver. He later was found guilty of first-degree murder and was a candidate for the death penalty. Professor A. Raine from the University of Pennsylvania was an expert witness for defense and brought Page into a laboratory to assess his brain function. Brain imaging scans revealed a distinct lack of activation in the ventral prefrontal cortex. Professor Raine argued for a deep-rooted biological explanation for Mr. Page’s violence, who escaped death penalty partly on the basis of his brain pathology.\n\nEven though currently there are no preventive programs in place utilizing the recent discoveries in neurocriminology, there are a number of offender rehabilitation programs (Cognitive Centre of Canada).\n\nSome scientists propose using brain imaging to help decide which soon-to-be-released offenders are at greater risk for reoffending. The brain imaging data would be used along with common factors like age, prior arrests, and marital status. To support this idea, in a 2013 study, Professor Kent Kiehl from the University of New Mexico studying the population of 96 male offenders in the state’s prisons found that offenders with low activity in the anterior cingulate cortex where twice as likely to commit an offense in the four years after their release as those who had high activity in this region. Similarly, Dustin Pardini conducted that which shows that men with a smaller amygdala are three times more likely to commit violence three years after their release.\n\nTrials demonstrated the efficacy of a number of medications, i.e. stimulants antipsychotics, antidepressants and mood stabilizers, in diminishing aggression in adolescents and children.\nEven a simple omega-3 supplements in the diets of young offenders reduces offending and aggression.\n\nMeditation can also affect brains, and even change them permanently. In 2003 Professor Richie Davidson from the University of Wisconsin performed a revolutionary study. People were randomly selected into either a mindfulness training group or a control group that was put on a waiting list for training. Davidson showed that even eight weekly sessions of meditation enhanced left frontal EEG functioning. Similar study was later replicated by Professor Holzel.\n\n", "id": "32379480", "title": "Neurocriminology"}
{"url": "https://en.wikipedia.org/wiki?curid=11510606", "text": "Common misunderstandings of genetics\n\nDuring the latter half of the 20th century, the fields of genetics and molecular biology matured greatly, significantly increasing understanding of biological heredity. As with other complex and evolving fields of knowledge, the public awareness of these advances has primarily been through the mass media, and a number of common misunderstandings of genetics have arisen. Common misunderstandings include the following ideas:\n\nWhile there are many examples of animals that display certain well-defined behaviour that is genetically programmed, these examples have been extrapolated to a popular misconception that all patterns of behaviour, and more generally the phenotype, are rigidly genetically determined. There is good evidence that some basic aspects of human behaviour, such as circadian rhythms are genetically-based, but it is clear that many other aspects are not.\n\nIn the first place, much phenotypic variability does not stem from genes themselves. For example:\n\nIn the early years of genetics it was suggested that there might be \"a gene for\" a wide range of particular characteristics. This was partly because the examples studied from Mendel onwards inevitably focused on genes whose effects could be readily identified; partly that it was easier to teach science that way; and partly because the mathematics of evolutionary dynamics is simpler if there is a simple mapping between genes and phenotypic characteristics.\n\nThese have led to the general perception that there is \"a gene for\" arbitrary traits,\nleading to controversy in particular cases such as the purported \"gay gene\". However, in light of the known complexities of gene expression networks (and phenomena such as epigenetics), it is clear that instances where a single gene \"codes for\" a single, discernible phenotypic effect are rare, and that media presentations of \"a gene for X\" grossly oversimplify the vast majority of situations.\n\nIt is widely believed that genes provide a \"blueprint\" for the body in much the same way that architectural or mechanical engineering blueprints describe buildings or machines. At a superficial level, genes and conventional blueprints share the common property of being low dimensional (genes are organised as a one-dimensional string of nucleotides; blueprints are typically two-dimensional drawings on paper) but containing information about fully three-dimensional structures. However, this view ignores the fundamental differences between genes and blueprints in the nature of the mapping from low order information to the high order object.\n\nIn the case of biological systems, a long and complicated chain of interactions separates genetic information from macroscopic structures and functions. The following simplified diagram of causality illustrates this:\n\nEven at the small scale, the relationship between genes and proteins (once thought of as \"one gene, one polypeptide\") is known to be complicated, with approximately 5 proteins in the human body for each gene. More significantly, the causal chains from genes to functionality are not separate or isolated but are entangled together, most obviously in metabolic pathways (such as the Calvin and citric acid cycles) which link a succession of enzymes (and, thus, gene products) to form a coherent biochemical system. Furthermore, information flow in the chain is not exclusively one-way. While the central dogma of molecular biology describes how information cannot be passed back to inheritable genetic information, the other causal arrows in this chain can be bidirectional, with complex feedbacks ultimately regulating gene expression.\n\nInstead of being a simple, linear mapping, this complex relationship between genotype and phenotype is not straightforward to decode. Rather than describing genetic information as a blueprint, some have suggested that a more appropriate analogy is that of a recipe for cooking, where a collection of ingredients is combined via a set of instructions to form an emergent structure, such as a cake, that is not described explicitly in the recipe itself.\n\nIt is popularly supposed that a gene is \"a linear sequence of nucleotides along a segment of DNA that provides the coded instructions for synthesis of RNA\" and even some current medical dictionaries define a gene as \"a hereditary unit that occupies a specific location on a chromosome, determines a particular characteristic in an organism by directing the formation of a specific protein, and is capable of replicating itself at each cell division.\"\n\nIn fact, as the diagram illustrates schematically, genes are much more complicated and elusive concepts. A reasonable modern definition of a gene is \"a locatable region of genomic sequence, corresponding to a unit of inheritance, which is associated with regulatory regions, transcribed regions and/or other functional sequence regions.\" One of the major complicating factors is that the exons which code for the proteins are often separated by many introns, which used to be called \"junk DNA\" but appear to have various as-yet-ill-understood purposes. The exons can be combined in different orders (splice variants) to produce different proteins. For example the gene called \"Dscam\" in Drosophila has 110 introns and therefore tens of thousands of possible splice variants.\n\nThis kind of misperception is perpetuated when mainstream media report that an organism's genome has been \"deciphered\" when they mean that it has simply been sequenced.\n", "id": "11510606", "title": "Common misunderstandings of genetics"}
{"url": "https://en.wikipedia.org/wiki?curid=32499267", "text": "XDNA\n\nxDNA (also known as \"expanded DNA\" or \"benzo-homologated DNA\") is a size-expanded nucleotide system synthesized from the fusion of a benzene ring and one of the four natural bases: adenine, guanine, cytosine, and thymine. This size expansion produces an 8 letter alphabet which has a larger information density by a factor of 2 compared to natural DNA's (often referred to as B-DNA in literature) 4 letter alphabet. As with normal base-pairing, A pairs with xT, C pairs with xG, G pairs with xC, and T pairs with xA. The double helix is thus 2.4Å wider than a natural double helix. While similar in structure to B-DNA, xDNA has unique absorption, fluorescence, and stacking properties.\n\nInitially synthesized as an enzyme probe by Nelson J. Leonard's group, benzo-homologated adenine was the first base synthesized. Later, Eric T. Kool's group finished synthesizing the remaining three expanded bases, eventually followed by yDNA (\"wide\" DNA), another benzo-homologated nucleotide system, and naphtho-homologated xxDNA and yyDNA. xDNA is more stable when compared to regular DNA when subjected to higher temperature, and while entire strands of xDNA, yDNA, xxDNA and yyDNA exist, they are currently difficult to synthesize and maintain.Experiments with xDNA provide new insight into the behavior of natural B-DNA. The extended bases xA, xC, xG, and xT are naturally fluorescent, and single strands composed of only extended bases can recognize and bind to single strands of natural DNA, making them useful tools for studying biological systems. xDNA is most commonly formed with base pairs between a natural and expanded nucleobase, however x-nucleobases can also be paired together. Current research supports xDNA as a viable genetic encoding system in the near future.\n\nThe first nucleotide to be expanded was the purine adenine. Nelson J. Leonard and colleagues synthesized this original x-nucleotide, which was referred to as \"expanded adenine\". xA was used as a probe in the investigation of active sites of ATP-dependent enzymes, more specifically what modifications the substrate could take while still being functional. Almost two decades later, the other three bases were successfully expanded and later integrated into a double helix by Eric T. Kool and colleagues. Their goal was to create a synthetic genetic system which mimics and surpasses the functions of the natural genetic system, and to broaden the applications of DNA both in living cells and in experimental biochemistry. Once the expanded base set was created, the goal shifted to identifying or developing faithful replication enzymes and further optimizing the expanded DNA alphabet.\n\nIn benzo-homologated purines (xA and xG), the benzene ring is bound to the nitrogenous base through nitrogen-carbon (N-C) bonds. Benzo-homologated pyrimidines are formed through carbon-carbon (C-C) bonds between the base and the benzene. Thus far, x-nucleobases have been added to strands of DNA using phosphoramidite derivatives, as traditional polymerases have been unsuccessful in synthesizing strands of xDNA. X-nucleotides are poor candidates as substrates for B-DNA polymerases as their size interferes with binding at the catalytic domain. Attempts at using template-independent enzymes have been successful as they have a reduced geometric constraint for substrates. Terminal deoxynucleotidyl transferase (TdT) has been used previously to synthesize strands of bases which have been bound to fluorophores. Using TdT, up to 30 monomers can be combined to form a double-helix of xDNA, however this oligomeric xDNA appears to inhibit its own extension beyond this length due to the overwhelming hydrogen bonding. In order to minimize inhibition, xDNA can be hybridized into a regular helix.\n\nFor xDNA to be used as a substitute structure for information storage, it requires a reliable replication mechanism. Research into xDNA replication using a Klenow fragment from DNA Polymerase I shows that a natural base partner is selectively added in instances of single-nucleotide insertion. However, DNA Polymerase IV (Dpo4) has been able to successfully use xDNA for these types of insertions with high fidelity, making it a promising candidate for future research in extending replicates of xDNA. xDNA's mismatch sensitivity is similar to that of B-DNA.\n\nSimilar to natural bases, x-nucleotides selectively assemble into a duplex-structure resembling B-DNA. xDNA was originally synthesized by incorporating a benzene ring into the nitrogenous base. However, other expanded bases have been able to incorporate thiophene and benzo[b]thiophene as well. xDNA and yDNA use benzene rings to widen the bases and are thus termed \"benzo-homologated\". Another form of expanded nucleobases known as yyDNA incorporate naphthalene into the base and are \"naptho-homologated\". xDNA has a rise of 3.2Å and a twist of 32°, significantly smaller than B-DNA, which has a rise of 3.3Å and a twist of 34.2° xDNA nucleotides can occur on both strands—either alone (known as \"doubly expanded DNA\") or mixed with natural bases—or exclusively on one strand or the other. Similar to B-DNA, xDNA can recognize and bind complimentary single-stranded DNA or RNA sequences.\n\nDuplexes formed from xDNA are similar to natural duplexes aside from the distance between the two sugar-phosphate backbones. xDNA helices have a greater number of base pairs per turn of the helix as a result of a reduced distance between neighbour nucleotides. NMR spectra report that xDNA helices are anti-parallel, right-handed and take an \"anti\" conformation around the glycosidic bond, with a C2'-endo sugar pucker. Helices created from xDNA are more likely to take a B-helix over an A-helix conformation, and have an increased major groove width by 6.5Å (where the backbones are farthest apart) and decreased minor groove width by 5.5Å (where the backbones are closest together) compared to B-DNA. Altering groove width affects the xDNA's ability to associate with DNA-binding proteins, but as long as the expanded nucleotides are exclusive to one strand, recognition sites are sufficiently similar to B-DNA to allow bonding of transcription factors and small polyamide molecules. Mixed helices present the possibility of recognizing the four expanded bases using other DNA-binding molecules.\n\nExpanded nucleotides and their oligomeric helices share many properties with their natural B-DNA counterparts, including their pairing preference: A with T, C with G. The various differences in chemical properties between xDNA and B-DNA support the hypothesis that the benzene ring which expands x-nucleobases is not, in fact, chemically inert. xDNA is more hydrophobic than B-DNA, and also has a smaller HOMO-LUMO gap (distance between the highest occupied molecular orbital and lowest unoccupied molecular orbital) as a result of modified saturation. xDNA has higher melting temperatures than B-DNA (a mixed decamer of xA and T has a melting temperature of 55.6 °C, 34.3 °C higher than the same decamer of A and T), and exhibits an \"all-or-nothing\" melting behaviour.\n\nUnder lab conditions, xDNA orients itself in the \"syn\" conformation. This unfortunately does not expose the binding face of the xDNA nucleotides to face the neighbouring strand for binding, meaning that extra measures must be applied to alter the conformation of xDNA before attempting to form helices. However, the \"anti\" and \"syn\" orientations are practically identical energetically in expanded bases. This conformational preference is seen primarily in pyrimidines, and purines display minimal preference for orientation.\n\nStacking of the nucleotides in a double helix is a major determinant of the helix's stability. With the added surface area and hydrogen available for bonding, stacking potential for the nucleobases increases with the addition of a benzene spacer. By increasing the separation between the nitrogenous bases and either sugar-phosphate backbone, the helix's stacking energy is less variable and therefore more stable. The energies for natural nucleobase pairs vary from 18 to 52 kJ/mol. This variance is only 14-40 kJ/mol for xDNA.\n\nDue to an increased overlap between and expanded strand of DNA and its neighbouring strand, there are greater interstrand interactions in expanded and mixed helices, resulting in a significant increase in the helix's stability. xDNA has enhanced stacking abilities resultant from changes in inter- and intrastrand hydrogen bonding that arise from the addition of a benzene spacer, but expanding the bases does not alter hydrogen's contribution to the stability of the duplex. These stacking abilities are exploited by helices consisting of both xDNA and B-DNA in order to optimize the strength of the helix. Increased stacking is seen most prominently in strands consisting only of A and xA and T and xT, as T-xA has stronger stacking interactions than T-A.\n\nThe energy resultant from pyrimidines ranges from 30-49 kJ/mol. The range for purines is between 40-58kJ/mol. By replacing one nucleotide in a double-helix with an expanded nucleotide, the strength of the stacking interactions increases by 50%. Expanding both nucleotides results in a 90% increase in stacking strength. While xG has an overall negative effect on the binding strength of the helix, the other three expanded bases outweigh this with their positive effects. The change in energy caused by expanding the bases is mostly dependent on the rotation of the bond about the nucleobases' centers of mass, and center of mass stacking interactions improve the stacking potential of the helix. Because the size-expanded bases widen the helix, it is more thermally stable with a higher melting temperature.\n\nThe addition of a benzene spacer in x-nucleobases affects the bases' optical absorption spectra. Time-dependent density functional theory (TDDFT) applied to xDNA revealed that the benzene component of the highest occupied molecular orbitals (HOMO) in the x-bases pins the absorption onset at an earlier point than natural bases. Another unusual feature of xDNA absorption spectra is the red-shifted excimers of xA in the low range. In terms of stacking fingerprints, there is a more pronounced hypochromicity seen in consecutive xA-T base pairs.\n\nImplications of xDNA's altered absorption include applications in nanoelectronic technology and nanobiotechnology. The reduced spacing between x-nucleotides makes the helix stiffer, thus it is not as easily affected by substrate, electrode, and functional nanoparticle forces. Other alterations to natural nucleotides resulting in different absorption spectra will broaden these applications in the future.\n\nOne unique property of xDNA is its inherent fluorescence. Natural bases can be bound directly to fluorophores for use in microarrays, \"in situ\" hibridization, and polymorphism analysis. However, these fluorescent natural bases often fail as a result of self-quenching, which diminishes their fluorescent intensity and reduces their applicability as visual DNA tags. The pi interactions between the rings in x-nucleobases result in an inherent fluorescence in the violet-blue range, with a Stokes shift between 50-80 nm. They also have a quantum yield in the range of 0.3-0.6. xC has the greatest fluorescent emission.\n\nAfter the creation of and successful research surrounding xDNA, more forms of expanded nucleotides were investigated. yDNA is a second, similar system of nucleotides which uses a benzene ring to expand the four natural bases. xxDNA and yyDNA use naphthalene, a polycyclic molecule consisting of two hydrocarbon rings. The two rings expand the base even wider, further altering its chemical properties.\n\nThe success and implications of xDNA prompted research to examine other factors which could alter B-DNA's chemical properties and create a new system for information storage with broader applications. yDNA also uses a benzene ring, similar to xDNA, with the only difference being the site of addition of the aromatic ring. The location of the benzene ring changes the preferred structure of the expanded helix. The altered conformation makes yDNA more similar to B-DNA in its orientation by changing the interstrand hydrogen bonds. Stability is highly dependent on the bases' rotation about the link between the base and the sugar of the backbone. yDNA's altered preference for this orientation makes it more stable overall than xDNA. The location of the benzene spacer also affects the bases' groove geometry, altering neighbour interactions. The base pairs between y-nucleotides and natural nucleotides is planar, rather than slightly twisted as with xDNA. This decreases the rise of the helix even further than achieved by xDNA. \n\nWhile xDNA and yDNA are quite similar in most properties, including their increased stacking interactions, yDNA shows superior mismatch recognition. y-pyrimidines display slightly stronger stacking interactions than x-pyrimidines as a result of the distance between the two anomeric carbons, which is slightly larger in yDNA. xDNA still has stronger stacking interactions in model helices, but adding either x- or y-pyrimidines to a natural double helix strengthens the intra- and interstrand interactions, increasing overall helix stability. In the end, which of the two has the strongest overall stacking interactions is dependent on the sequence; xT and yT bind A with similar strength, but the stacking energy of yC bound to G is stronger than xC by 4kJ/mol. yDNA and other expanded bases are part of a very young field which is highly understudied. Research suggest that the ideal conformation still remains to be discovered, but knowing that the benzene location affects the orientation and structure of expanded nucleobases adds information to their future design.\n\nDoubly-expanded (or \"naphtho-homologated\") nucleobases incorporate a naphthalene spacer instead of a benzene ring, widening the base twice as much with its two-ringed structure. These structures (known as xxDNA and yyDNA) are 4.8Å wider than natural bases and were once again created as a result of Leonard's research on expanded adenine in ATP-dependent enzymes in 1984. No literature was published on these doubly-expanded bases for nearly three decades until 2013 when the first xxG was produced by Sharma, Lait, and Wetmore and incorporated along with xxA into a natural helix. Although very little research has been performed on xxDNA, xx-purine neighbours have already been shown to increase intrastrand stacking energy by up to 119% (as opposed to 62% in x-purines). xx-purine and pyrimidine interactions show an overall decrease in stacking energies, but the overall stability of duplexes including pyrimidines and xx-purines increases by 22%, more than twofold that of pyrimidines and x-purines.\n\nxDNA has many applications in chemical and biological research, including expanding upon applications of natural DNA, such as scaffolding. In order to create self-assembling nanostructures, a scaffold is needed as a sort of trellis to support the growth. DNA has been used as a means to this end in the past, but expanded scaffolds make larger scaffolds for more complex self-assembly an option. xDNA's electrical conduction properties also make it a prime candidate as a molecular wire, as its π-π interactions help it efficiently conduct electricity. Its 8-letter alphabet (A, T, C, G, xA, xT, xC, xG) gives it the potential to store 2 times increase in storage density, where \"n\" represents the number of letters in a sequence. For example, combining 6 nucleotides of with B-DNA yields 4096 possible sequences, whereas a combination of the same number of nucleotides created with xDNA yields 262,144 possible sequences. Additionally, xDNA can be used as a fluorescent probe at enzyme active sites, as was its original application by Leonard et al.\n\nxDNA has also been applied to the study of protein-DNA interactions. Due to xDNA's natural fluorescing properties, it can easily be visualized in both lab and living conditions. xDNA is becoming more easy to create and oligomerize, and its high-affinity binding to complementary DNA and RNA sequences means that it can not only help locate these sequences floating around in the cell, but also when they are already interacting with other structures within the cell. xDNA also has potential applications in assays that employ TdT as it may improve reporters, and can be used as an affinity tag for interstrand bonding.\n\n", "id": "32499267", "title": "XDNA"}
{"url": "https://en.wikipedia.org/wiki?curid=32575511", "text": "Gene polymorphism\n\nA gene is said to be polymorphic if more than one allele occupies that gene’s locus within a population. In addition to having more than one allele at a specific locus, each allele must also occur in the population at a rate of at least 1% to generally be considered polymorphic.\n\nGene polymorphisms can occur in the coding or non-coding region of the genome. The majority of polymorphisms are silent, meaning that they occur in the non-coding region and therefore do not alter the function or expression of a gene. If a gene polymorphism is located in the coding region of the genome, it can alter the expression of the gene and create variation within a given population. This variation is maintained by balancing selection. For example, in dogs the E locus, can have any of five different alleles, known as E, E, E, E, and e. Varying combinations of these alleles contribute to the pigmentation and patterns seen in dog coats.\n\nA polymorphic variant of a gene can lead to the abnormal expression or to the production of an abnormal form of the protein; this abnormality may cause or be associated with disease. For example, a polymorphic variant of the enzyme CYP4A11 in which thymidine replaces cytosine at the gene's nucleotide 8590 position encodes a CYP4A11 protein that substitutes phenylalanine with serine at the protein's amino acid position 434. This variant protein has reduced enzyme activity in metabolizing arachidonic acid to the blood pressure-regulating eicosanoid, 20-Hydroxyeicosatetraenoic acid. A study has shown that humans bearing this variant in one or both of their CYP4A11 genes have an increased incidence of hypertension, ischemic stroke, and coronary artery disease.\n\nMost notably, the genes coding for the Major Histocompatibility Complex (MHC) are in fact the most polymorphic genes known. MHC molecules are involved in the immune system and interact with T-cells. There are more than 800 different alleles of human MHC class I and II genes, and it has been estimated that there are 200 variants at the HLA-B HLA-DRB1 loci alone.\n\n\nMutation results due to DNA sequence changes specifically that happen once an allele is transferred from one generation to another and initiate alterations in the allele status from normal to abnormal. In contrast, gene polymorphism is defined as a variation that occurs in allele in a DNA sequence. Gene polymorphisms generally occur in populations at a frequency of 1% or more while mutations usually occur at a frequency of less than 1%. Additionally, gene polymorphisms are present in all cells of an individual's body while mutations have the ability to be present in a specific subset of somatic cells. An example of this phenomenon is the mutation of somatic cells leading to cancer in a specific area of an organism's body.\n\nGene polymorphisms are caused by duplications, deletions, and a mutation of triplication of high quantity of DNA base pairs sequences. In addition, Polymorphisms may occur due to changes inside introns or changes in regions for one or multiple DNA bases that are between genes. If the changes occur in a gene’ coding sequence, then different phenotypes may appear as a result of protein variation that is caused by sequence changes. Polymorphisms can be identified in the laboratory using a variety of methods. The first step is amplifying the coding sequence of the gene using PCR. Once amplified, a number of methods can be used to identify a mutation or polymorphism in the coding sequence including DNA sequencing and single strand conformation polymorphism analysis (SSCP).\n\nThere are four types of gene polymorphisms: single nucleotide polymorphisms, small-scale insertions and deletions, polymorphic repetitive elements and microsatellite variation.\n\nSNPs are a single nucleotide changes that happen in the genome in a particular location. The single nucleotide polymorphism is known to be the most common form of genetic variation. A major and the cause of this SNPs is the replacement of the nucleotide cytosine (C) with thymine (T) in a part of the DNA. SNPs may cause a disease through the affection in a specific gene or regulatory region near this gene resulting in disturbance in the gene's function.\n\nSmall insertions and deletion are called INDELs and this type of gene polymorphism is dependent on insertion or deletion of DNA bases in an organism. Nowadays, two million INDELs have been discovered in approximately seventy-nine various humans genomes. Furthermore, small insertions/ deletions are existed on genes coding exons and this may consider a fundamental factor that leads to diseases inheritance in humans.\n\nAlu which is a repetitive element from Alu family, can trigger a polymorphism in human genome. Alu element is defined as a small area of DNA sequence with 300 base pairs. Alu element also has a role in RNA polymerase III for its compression of a RNA promoter. It has been found that Alu is repeated in more than 10% of human genome. Insertion and repetitive of Alu element in human genome can cause mutations and disorders that are related to carcinogenesis.\n\nMicrosatellites are characterized for the repetition for 1-6 base pairs of DNA sequence. In genetics, microsatellites are commonly used as a molecular markers especially for identifying the relationship between alleles. Diseases that are correlated with microsatellites are Fragile X syndrome, Myotonic dystrophy, Friedreich's ataxia, Kennedy disease, Huntington's disease, Haw river syndrome, and Spinocerebellar ataxia.\n\nPolymorphisms have been discovered in multiple XPD exons. XPD refers to ‘’’xeroderma pigmentosum group D’’’ and is involved in a DNA repair mechanism used during DNA replication. XPD works by cutting and removing segments of DNA that have been damaged due to things such as cigarette smoking and inhalation of other environmental carcinogens. Asp312Asn and Lys751Gln are the two common polymorphisms of XPD that result in a change in a single amino acid. This variation in Asn and Gln alleles is has been related to individuals having a reduced DNA repair efficiency. Several studies have been conducted to see if this diminished capacity to repair DNA is related to an increased risk of lung cancer. These studies examined the XPD gene in lung cancer patients of varying age, gender, race, and pack-years. The studies provided mixed results, from concluding individuals who are homozygous for the Asn allele or homozygous for the Gln allele had an increased risk of developing lung cancer, to finding no statistical significance between smokers who have either allele polymorphism and their susceptibility to lung cancer. Research continues to be conducted to determine the relationship between XPD polymorphisms and lung cancer risk.\n\nAsthma is an inflammatory disease of the lungs and more than 100 loci have been identified as contributing to the development and severity of the condition. By using the traditional linkage analysis, these asthma correlated genes were able to be identified in small quantities using Genome-wide association studies (GWAS). There have been a number of studies looking into various polymorphisms of asthma-associated genes and how those polymorphisms interact with the carrier's environment. One example is the gene CD14, which is known to have a polymorphism that is associated with increased amounts of CD14 protein as well as reduced levels of IgE serum. A study was conducted on 624 children looking at their IgE serum levels as it related to the polymorphism in CD14. The study found that IgE serum levels differed in children with the C allele in the CD14/-260 gene based on the type of allergens they regularly exposed to. Children who were in regular contact with house pets showed higher serum levels of IgE while children who were regularly exposed to stable animals showed lower serum levels of IgE. Continued research into gene-environment interactions may lead to more specialized treatment plans based on an individual's surroundings.\n", "id": "32575511", "title": "Gene polymorphism"}
{"url": "https://en.wikipedia.org/wiki?curid=32706791", "text": "Genome instability\n\nGenome instability (also genetic instability or genomic instability) refers to a high frequency of mutations within the genome of a cellular lineage. These mutations can include changes in nucleic acid sequences, chromosomal rearrangements or aneuploidy. Genome instability does occur in bacteria. In multicellular organisms genome instability is central to carcinogenesis, and in humans it is also a factor in some neurodegenerative diseases such as amyotrophic lateral sclerosis or the neuromuscular disease myotonic dystrophy.\n\nThe sources of genome instability have only recently begun to be elucidated. A high frequency of externally caused DNA damage can be one source of genome instability since DNA damages can cause inaccurate translesion synthesis past the damages or errors in repair, leading to mutation. Another source of genome instability may be epigenetic or mutational reductions in expression of DNA repair genes. Because endogenous (metabolically-caused) DNA damage is very frequent, occurring on average more than 60,000 times a day in the genomes of human cells, any reduced DNA repair is likely an important source of genome instability.\n\nUsually, all cells in an individual in a given species (plant or animal) show a constant number of chromosomes, which constitute what is known as the karyotype defining this species (see also List of number of chromosomes of various organisms), although some species present a very high karyotypic variability. In humans, mutations that would change an amino acid within the protein coding region of the genome occur at an average of only 0.35 per generation (less than one mutated protein per generation).\n\nSometimes, in a species with a stable karyotype, random variations that modify the normal number of chromosomes may be observed. In other cases, there are structural alterations (chromosomal translocations, deletions ...) that modify the standard chromosomal complement. In these cases, it is indicated that the affected organism presents genome instability (also \"genetic instability\", or even \"chromosomic instability\"). The process of genome instability often leads to a situation of aneuploidy, in which the cells present a chromosomic number that is either higher or lower than the normal complement for the species.\n\nIn the cell cycle, DNA is usually most vulnerable during replication. The replisome must be able to navigate obstacles such as tightly wound chromatin with bound proteins, single and double stranded breaks which can lead to the stalling of the replication fork. Each protein or enzyme in the replisome must perform its function well to result in a perfect copy of DNA. Mutations of proteins such as DNA polymerase, ligase, can lead to impairment of replication and lead to spontaneous chromosomal exchanges. Proteins such as Tel1, Mec1 (ATR, ATM in humans) can detect single and double-stranded breaks and recruit factors such as Rmr3 helicase to stabilize the replication fork in order to prevent its collapse. Mutations in Tel1, Mec1, and Rmr3 helicase result in a significant increase of chromosomal recombination. ATR responds specifically to stalled replication forks and single-stranded breaks resulting from UV damage while ATM responds directly to double-stranded breaks. These proteins also prevent progression into mitosis by inhibiting the firing of late replication origins until the DNA breaks are fixed by phosphorylating CHK1, CHK2 which results in a signaling cascade arresting the cell in S-phase. For single stranded breaks, replication occurs until the location of the break, then the other strand is nicked to form a double stranded break, which can then be repaired by Break Induced Replication or homologous recombination using the sister chromatid as an error-free template. In addition to S-phase checkpoints, G1 and G2 checkpoints exist to check for transient DNA damage which could be caused by mutagens such as UV damage. An example is the Saccharomyces pombe gene rad9 which arrests the cells in late S/G2 phase in the presence of DNA damage caused by radiation. The yeast cells with defective rad9 failed to arrest following radiation, continued cell division and died rapidly while the cells with wild-type rad9 successfully arrested in late S/G2 phase and remained viable. The cells that arrested were able to survive due to the increased time in S/G2 phase allowing for DNA repair enzymes to function fully.\n\nThere are hotspots in the genome where DNA sequences are prone to gaps and breaks after inhibition of DNA synthesis such as in the aforementioned checkpoint arrest. These sites are called fragile sites, and can occur commonly as naturally present in most mammalian genomes or occur rarely as a result of mutations, such as DNA-repeat expansion. Rare fragile sites can lead to genetic disease such as fragile X mental retardation syndrome, myotonic dystrophy, Friedrich’s ataxia, and Huntington’s disease, most of which are caused by expansion of repeats at the DNA, RNA, or protein level. Although, seemingly harmful, these common fragile sites are conserved all the way to yeast and bacteria. These ubiquitous sites are characterized by trinucleotide repeats, most commonly CGG, CAG, GAA, and GCN. These trinucleotide repeats can form into hairpins, leading to difficulty of replication. Under replication stress, such as defective machinery or further DNA damage, DNA breaks and gaps can form at these fragile sites. Using a sister chromatid as repair is not a fool-proof backup as the surrounding DNA information of the n and n+1 repeat is virtually the same, leading to copy number variation. For example, the 16th copy of CGG might be mapped to the 13th copy of CGG in the sister chromatid since the surrounding DNA is both CGGCGGCGG…, leading to 3 extra copies of CGG in the final DNA sequence.\n\nIn both E. coli and Saccromyces Pombe, transcription sites tend to have higher recombination and mutation rates. Interestingly, the coding or non-transcribed strand accumulates more mutations than the template strand. This is due to the fact that the coding strand is single-stranded during transcription, which is chemically more unstable than double-stranded DNA. During elongation of transcription, supercoiling can occur behind an elongating RNA polymerase, leading to single-stranded breaks. When the coding strand is single-stranded, it can also hybridize with itself, creating DNA secondary structures that can compromise replication. In E. coli, when attempting to transcribe GAA triplets such as those found in Friedrich’s ataxia, the resulting RNA and template strand can form mismatched loops between different repeats, leading the complementary segment in the coding-strand available to form its own loops which impede replication. Furthermore, replication of DNA and transcription of DNA are not temporally independent; they can occur at the same time and lead to collisions between the replication fork and RNA polymerase complex. In S. cerevisiae, Rrm3 helicase is found at highly transcribed genes in the yeast genome, which is recruited to stabilize a stalling replication fork as described above. This suggests that transcription is an obstacle to replication, which can lead to increased stress in the chromatin spanning the short distance between the unwound replication fork and transcription start site, potentially causing single-stranded DNA breaks. In yeast, proteins act as barriers at the 3’ of the transcription unit to prevent further travel of the DNA replication fork.\n\nIn some portions of the genome, variability is essential to survival. One such locale is the Ig genes. In a pre-B cell, the region consists of all V, D, and J segments. During development of the B cell, a specific V, D, and J segment is chosen to be spliced together to form the final gene, which is catalyzed by RAG1 and RAG2 recombinases. Activation-Induced Cytidine Deaminase (AID) then converts cytidine into uracil. Uracil normally does not exist in DNA, and thus the base is excised and the nick is converted into a double-stranded break which is repaired by non-homologous end joining (NHEJ). This procedure is very error-prone and leads to somatic hypermutation. This genomic instability is crucial in ensuring mammalian survival against infection. V, D, J recombination can ensure millions of unique B-cell receptors; however, random repair by NHEJ introduces variation which can create a receptor that can bind with higher affinity to antigens.\n\nOf about 200 neurological and neuromuscular disorders, 15 have a clear link to an inherited or acquired defect in one of the DNA repair pathways or excessive genotoxic oxidative stress. Five of them (xeroderma pigmentosum, Cockayne's syndrome, trichothiodystrophy, Down's syndrome, and triple-A syndrome) have a defect in the DNA nucleotide excision repair pathway. Six (spinocerebellar ataxia with axonal neuropathy-1, Huntington's disease, Alzheimer's disease, Parkinson's disease, Down's syndrome and amyotrophic lateral sclerosis) seem to result from increased oxidative stress, and the inability of the base excision repair pathway to handle the damage to DNA that this causes. Four of them (Huntington's disease, various spinocerebellar ataxias, Friedreich’s ataxia and myotonic dystrophy types 1 and 2) often have an unusual expansion of repeat sequences in DNA, likely attributable to genome instability. Four (ataxia-telangiectasia, ataxia-telangiectasia-like disorder, Nijmegen breakage syndrome and Alzheimer's disease) are defective in genes involved in repairing DNA double-strand breaks. Overall, it seems that oxidative stress is a major cause of genomic instability in the brain. A particular neurological disease arises when a pathway that normally prevents oxidative stress is deficient, or a DNA repair pathway that normally repairs damage caused by oxidative stress is deficient.\n\nIn cancer, genome instability can occur prior to or as a consequence of transformation. Genome instability can refer to the accumulation of extra copies of DNA or chromosomes, chromosomal translocations, chromosomal inversions, chromosome deletions, single-strand breaks in DNA, double-strand breaks in DNA, the intercalation of foreign substances into the DNA double helix, or any abnormal changes in DNA tertiary structure that can cause either the loss of DNA, or the misexpression of genes. Situations of genome instability (as well as aneuploidy) are common in cancer cells, and they are considered a \"hallmark\" for these cells. The unpredictable nature of these events are also a main contributor to the heterogeneity observed among tumour cells.\n\nIt is currently accepted that sporadic tumors (non-familial ones) are originated due to the accumulation of several genetic errors. An average cancer of the breast or colon can have about 60 to 70 protein altering mutations, of which about 3 or 4 may be \"driver\" mutations, and the remaining ones may be \"passenger\" mutations Any genetic or epigenetic lesion increasing the mutation rate will have as a consequence an increase in the acquisition of new mutations, increasing then the probability to develop a tumor. During the process of tumorogenesis, it is known that diploid cells acquire mutations in genes responsible for maintaining genome integrity (\"caretaker genes\"), as well as in genes that are directly controlling cellular proliferation (\"gatekeeper genes\"). Genetic instability can originate due to deficiencies in DNA repair, or due to loss or gain of chromosomes, or due to large scale chromosomal reorganizations. Losing genetic stability will favour tumor development, because it favours the generation of mutants that can be selected by the environment.\n\nThe tumor microenvironment has an inhibitory effect on DNA repair pathways contributing to genomic instability, which promotes tumor survival, proliferation, and malignant transformation.\n\nThe protein coding regions of the human genome, collectively called the exome, constitutes only 1.5% of the total genome. As pointed out above, ordinarily there are only an average of 0.35 mutations in the exome per generation (parent to child) in humans. In the entire genome (including non-protein coding regions) there are only about 70 new mutations per generation in humans.\n\nThe likely major underlying cause of mutations in cancer is DNA damage. For example, in the case of lung cancer, DNA damage is caused by agents in exogenous genotoxic tobacco smoke (e.g. acrolein, formaldehyde, acrylonitrile, 1,3-butadiene, acetaldehyde, ethylene oxide and isoprene). Endogenous (metabolically-caused) DNA damage is also very frequent, occurring on average more than 60,000 times a day in the genomes of human cells (see DNA damage (naturally occurring)). Externally and endogenously caused damages may be converted into mutations by inaccurate translesion synthesis or inaccurate DNA repair (e.g. by non-homologous end joining). In addition, DNA damages can also give rise to epigenetic alterations during DNA repair. Both mutations and epigenetic alterations (epimutations) can contribute to progression to cancer.\n\nAs noted above, about 3 or 4 driver mutations and 60 passenger mutations occur in the exome (protein coding region) of a cancer. However, a much larger number of mutations occur in the non-protein-coding regions of DNA. The average number of DNA sequence mutations in the entire genome of a breast cancer tissue sample is about 20,000. In an average melanoma tissue sample (where melanomas have a higher exome mutation frequency) the total number of DNA sequence mutations is about 80,000.\n\nThe high frequency of mutations in the total genome within cancers suggests that, often, an early carcinogenic alteration may be a deficiency in DNA repair. Mutation rates substantially increase (sometimes by 100-fold) in cells defective in DNA mismatch repair or in homologous recombinational DNA repair. Also, chromosomal rearrangements and aneuploidy increase in humans defective in DNA repair gene BLM.\n\nA deficiency in DNA repair, itself, can allow DNA damages to accumulate, and error-prone translesion synthesis past some of those damages may give rise to mutations. In addition, faulty repair of these accumulated DNA damages may give rise to epigenetic alterations or epimutations. While a mutation or epimutation in a DNA repair gene, itself, would not confer a selective advantage, such a repair defect may be carried along as a passenger in a cell when the cell acquires an additional mutation/epimutation that does provide a proliferative advantage. Such cells, with both proliferative advantages and one or more DNA repair defects (causing a very high mutation rate), likely give rise to the 20,000 to 80,000 total genome mutations frequently seen in cancers.\n\nIn somatic cells, deficiencies in DNA repair sometimes arise by mutations in DNA repair genes, but much more often are due to epigenetic reductions in expression of DNA repair genes. Thus, in a sequence of 113 colorectal cancers, only four had somatic missense mutations in the DNA repair gene MGMT, while the majority of these cancers had reduced MGMT expression due to methylation of the MGMT promoter region. Five reports, listed in the article Epigenetics (see section \"DNA repair epigenetics in cancer\") presented evidence that between 40% and 90% of colorectal cancers have reduced MGMT expression due to methylation of the MGMT promoter region.\n\nSimilarly, for 119 cases of colorectal cancers classified as mismatch repair deficient and lacking DNA repair gene PMS2 expression, Pms2 was deficient in 6 due to mutations in the PMS2 gene, while in 103 cases PMS2 expression was deficient because its pairing partner MLH1 was repressed due to promoter methylation (PMS2 protein is unstable in the absence of MLH1). The other 10 cases of loss of PMS2 expression were likely due to epigenetic overexpression of the microRNA, miR-155, which down-regulates MLH1.\n\nIn cancer epigenetics (see section Frequencies of epimutations in DNA repair genes), there is a partial listing of epigenetic deficiencies found in DNA repair genes in sporadic cancers. These include frequencies of between 13–100% of epigenetic defects in genes BRCA1, WRN, FANCB, FANCF, MGMT, MLH1, MSH2, MSH4, ERCC1, XPF, NEIL1 and ATM located in cancers including breast, ovarian, colorectal and head and neck. Two or three epigenetic deficiencies in expression of ERCC1, XPF and/or PMS2 were found to occur simultaneously in the majority of the 49 colon cancers evaluated. Some of these DNA repair deficiencies can be caused by epimutations in microRNAs as summarized in the MicroRNA article section titled miRNA, DNA repair and cancer.\n\nCancers usually result from disruption of a tumor repressor or dysregulation of an oncogene. Knowing that B-cells experience DNA breaks through development can give insight to the genome of lymphomas. Many types of lymphoma are caused by chromosomal translocation, which can arise from breaks in DNA leading to incorrect joining. In Burkitt’s lymphoma, c-myc, an oncogene encoding a transcription factor, is translocated after the promoter of the immunoglobulin gene, leading dysregulation of c-myc transcription. Since immunoglobulins are essential to a lymphocyte and highly expressed to increase detection of antigens, c-myc is then also highly expressed leading to transcription of its targets which are involved in cell proliferation. Mantle cell lymphoma is characterized by fusion of cyclin D1 to the immunoglobulin locus. Cyclin D1 inhibits Rb, a tumor suppressor, leading to tumorigenesis. Follicular lymphoma results from the translocation of immunoglobulin promoter to the Bcl-2 gene, giving rise to large amounts of Bcl-2 protein which inhibits apoptosis. DNA-damaged B-cells no longer undergo apoptosis leading to further mutations which could affect driver genes leading to tumorigenesis. The location of translocation in the oncogene shares structural properties of the target regions of AID, suggesting that the oncogene was a potential target of AID, leading to a double-stranded break that was translocated to the immunoglobulin gene locus through NHEJ repair.\n", "id": "32706791", "title": "Genome instability"}
{"url": "https://en.wikipedia.org/wiki?curid=32778957", "text": "Ultra-conserved element\n\nAn ultra-conserved element (UCE) is a region of DNA that is identical in at least two different species.\nOne of the first studies of UCEs showed that certain human DNA sequences of length 200 nucleotides or greater were entirely conserved (identical nucleic acid sequence) in human, rats, and mice. Despite often being noncoding DNA, some ultra-conserved elements have been found to be transcriptionally active, giving non-coding RNA molecules.\n\nPerfect conservation of these long stretches of DNA is thought to imply evolutionary importance as these regions appear to have experienced strong negative (purifying) selection for 300-400 million years. The probability of finding ultra-conserved elements by chance (under neutral evolution) has been estimated at less than 10 in 2.9 billion bases.\n\n481 ultra-conserved elements have been identified in the human genome. A database collecting genomic information about ultra-conserved elements (UCbase) that share 100% identity among human, mouse and rat is available at http://ucbase.unimore.it. A small number of those which are transcribed have been connected with human carcinomas and leukemias. For example, TUC338 is strongly upregulated in human hepatocellular carcinoma cells. Indeed, UCEs are often affected by copy number variation in cancer cells, much more than in healthy contexts, suggesting that altering the copy number of ultraconserved elements may be deleterious and associated with cancer. A study comparing ultra-conserved elements between humans and the Japanese puffer fish \"Takifugu rubripes\" proposed an importance in vertebrate development. Several ultra-conserved elements are located near transcriptional regulators or developmental genes. Other functions include enhancing and splicing regulation. Double-knockouts of UCEs near the ARX gene in mice caused a shrunken hippocampus in the brain. The knockout effects are not lethal in laboratory mice, but could be in the wild.\n\n\nRyu et al. BMC Evolutionary Biology 2012\nhttp://www.biomedcentral.com/1471-2148/12/236 \n", "id": "32778957", "title": "Ultra-conserved element"}
{"url": "https://en.wikipedia.org/wiki?curid=33373383", "text": "GWASdb\n\nGWASdb is an online bioinformatics database combines collections of GVs from GWAS and their comprehensive functional annotations, as well as disease classifications.\n\nThousands of genetic variants (GVs) associated with human traits and diseases have been identified by Genome-Wide Association Studies (GWAS). The advent of high throughput technologies, such as next-generation sequencing and very high-density microarrays, enables us to capture genome-wide variation on a much larger scale.<ref name='10.1073/pnas.0903103106'> </ref> With increasing sample sizes, GWAS studies based on these technologies will produce more information at higher resolutions. We will be able to detect many traits/diseases associated GVs, such as Single Nucleotide Polymorphisms (SNPs), Copy Number Variations (CNVs), and insertions and deletions (Indels).<ref name='10.1038/nature09534'> </ref>\n\nExisting resources also have limitations in satisfying the increasing demands of current GWAS research: (i) Many true disease susceptibility loci have relatively moderate P values which are ignored in existing databases. GVs with moderate effect sizes, usually filtered by strict cutoffs, can be directly related to diseases through gene-gene interaction in the context of regulatory networks and pathways. (ii) Most of the existing databases focus only on one or several aspects of the functional annotations, and not on GV-disease relationships. An integrative, comprehensive, up-to-date GWAS-based knowledgebase that focuses on disease classification is needed.\n\nGWASdb, a database that combines collections of GVs from GWAS together with their functional annotations and disease classifications. The database provides the following information: (i) In addition to all the GVs annotated in the NHGRI GWAS Catalog, we manually curated the GVs that are marginally significant (P value < 1.0×10-3) collected from supplementary materials of each original publication. (ii) We provide extensive functional annotations for these GVs. (iii) The GVs have been manually classified according to disease using Disease-Ontology Lite (DOLite) and Human Phenotype Ontology (HPO). The database can be used to conduct gene-based pathway enrichment and PPI network association analysis for diseases with sufficient variants.\n", "id": "33373383", "title": "GWASdb"}
{"url": "https://en.wikipedia.org/wiki?curid=582473", "text": "Oogenesis\n\nOogenesis, ovogenesis, or oögenesis is the differentiation of the ovum (egg cell) into a cell competent to further development when fertilized. It is developed from the primary oocyte by maturation. \n\nIn mammals, the first part of oogenesis starts in the germinal epithelium, which gives rise to the development of ovarian follicles, the functional unit of the ovary.\n\nOogenesis consists of several sub-processes: oocytogenesis, ootidogenesis, and finally maturation to form an ovum (oogenesis proper). Folliculogenesis is a separate sub-process that accompanies and supports all three oogenetic sub-processes.\n\nOogonium —(Oocytogenesis)—> Primary Oocyte —(Meiosis I)—> First Polar Body (Discarded afterward) + Secondary oocyte —(Meiosis II)—> Second Polar Body (Discarded afterward) + Ovum\n\nOocyte meiosis, important to all animal life cycles yet unlike all other instances of animal cell division, occurs completely without the aid of spindle-coordinating centrosomes.\n\nThe creation of oogonia traditionally doesn't belong to oogenesis proper, but, instead, to the common process of gametogenesis, which, in the female human, begins with the processes of folliculogenesis, oocytogenesis, and ootidogenesis.\n\nOogenesis starts with the process of developing oogonia, which occurs via the transformation of primordial follicles into primary oocytes, a process called oocytogenesis. Oocytogenesis is complete either before or shortly after birth.\n\nIt is commonly believed that, when oocytogenesis is complete, no additional primary oocytes are created, in contrast to the male process of spermatogenesis, where gametocytes are continuously created. In other words, primary oocytes reach their maximum development at ~20 weeks of gestational age, when approximately seven million primary oocytes have been created; however, at birth, this number has already been reduced to approximately 1-2 million.\n\nRecently, however, two publications have challenged the belief that a finite number of oocytes are set around the time of birth. The renewal of ovarian follicles from germline stem cells (originating from bone marrow and peripheral blood) has been reported in the postnatal mouse ovary. In contrast, DNA clock measurements do not indicate ongoing oogenesis during human females' lifetimes.\nThus, further experiments are required to determine the true dynamics of small follicle formation.\n\nThe succeeding phase of ootidogenesis occurs when the primary oocyte develops into an ootid. This is achieved by the process of meiosis. In fact, a primary oocyte is, by its biological definition, a cell whose primary function is to divide by the process of meiosis.\n\nHowever, although this process begins at prenatal age, it stops at prophase I. In late fetal life, all oocytes, still primary oocytes, have halted at this stage of development, called the dictyate. After menarche, these cells then continue to develop, although only a few do so every menstrual cycle.\n\nMeiosis I of ootidogenesis begins during embryonic development, but halts in the diplotene stage of prophase I until puberty. The mouse oocyte in the dictyate (prolonged diplotene) stage actively repairs DNA damage, whereas DNA repair is not detectable in the pre-dictyate (leptotene, zygotene and pachytene) stages of meiosis. For those primary oocytes that continue to develop in each menstrual cycle, however, synapsis occurs and tetrads form, enabling chromosomal crossover to occur. As a result of meiosis I, the primary oocyte has now developed into the secondary oocyte and the first polar body.\n\nImmediately after meiosis I, the haploid secondary oocyte initiates meiosis II. However, this process is also halted at the metaphase II stage until fertilization, if such should ever occur. When meiosis II has completed, an ootid and another polar body have now been created.\n\nSynchronously with ootidogenesis, the ovarian follicle surrounding the ootid has developed from a primordial follicle to a preovulatory one.\n\nBoth polar bodies disintegrate at the end of Meiosis II, leaving only the ootid, which then eventually undergoes maturation into a mature ovum.\n\nThe function of forming polar bodies is to discard the extra haploid sets of chromosomes that have resulted as a consequence of meiosis.\n\n\"In vitro maturation\" (\"IVM\") is the technique of letting ovarian follicles mature in vitro. It can potentially be performed before an IVF. In such cases, ovarian hyperstimulation isn't essential. Rather, oocytes can mature outside the body prior to IVF. Hence, no (or at least a lower dose of) gonadotropins have to be injected in the body. However, there still isn't enough evidence to prove the effectiveness and security of the technique.\n\nSome algae and the oomycetes produce eggs in oogonia. In the brown alga \"Fucus\", all four egg cells survive oogenesis, which is an exception to the rule that generally only one product of female meiosis survives to maturity.\n\nIn plants, oogenesis occurs inside the female gametophyte via mitosis. In many plants such as bryophytes, ferns, and gymnosperms, egg cells are formed in archegonia. In flowering plants, the female gametophyte has been reduced to an eight-celled embryo sac within the ovule inside the ovary of the flower. Oogenesis occurs within the embryo sac and leads to the formation of a single egg cell per ovule.\n\nIn \"ascaris\", the oocyte does not even begin meiosis until the sperm touches it, in contrast to mammals, where meiosis is completed in the estrus cycle.\n\n\n\n", "id": "582473", "title": "Oogenesis"}
{"url": "https://en.wikipedia.org/wiki?curid=1031641", "text": "Pathogenicity island\n\nPathogenicity islands (PAIs), as termed in 1990, are a distinct class of genomic islands acquired by microorganisms through horizontal gene transfer. Pathogenicity islands are found in both animal and plant pathogens. Additionally, PAIs are found in both gram-positive and gram-negative bacteria. They are transferred through horizontal gene transfer events such as transfer by a plasmid, phage, or conjugative transposon. Therefore, PAIs contribute to microorganisms' ability to evolve.\n\nOne species of bacteria may have more than one PAI. For example, \"Salmonella\" has at least five.\n\nAn analogous genomic structure in rhizobia is termed a \"symbiosis island\".\n\nPathogenicity islands (PAIs) are incorporated in the genome, chromosomally or extrachromosomally, of pathogenic organisms, but are usually absent from those nonpathogenic organisms of the same or closely related species. They may be located on a bacterial chromosome or may be transferred within a plasmid or can be found in bacteriophage genomes. The GC-content and codon usage of pathogenicity islands often differs from that of the rest of the genome, potentially aiding in their detection within a given DNA sequence, unless the donor and recipient of the PAI have similar GC-content\n\nPAIs are discrete genetic units flanked by direct repeats, insertion sequences or tRNA genes, which act as sites for recombination into the DNA. Cryptic mobility genes may also be present, indicating the provenance as transduction. PAIs are flanked by direct repeats; the sequence of bases at two ends of the inserted sequence are the same. They carry functional genes, such as integrases, transposases, or part of insertion sequences, to enable insertion into host DNA. PAIs are often associated with tRNA genes, which target sites for this integration event. They can be transferred as a single unit to new bacterial cells, thus conferring virulence to formerly benign strains.\n\nPAIs, a type of mobile genetic elements, may range from 10-200 kb and encode genes which contribute to the virulence of the respective pathogen. Pathogenicity islands carry genes encoding one or more virulence factors, including, but not limited to, adhesins, secretion systems (like type III secretion system), toxins, invasins, modulins, effectors, superantigens, iron uptake systems, o-antigen synthesis, serum resistance, immunoglobulin A proteases, apoptosis, capsule synthesis, and plant tumorigenesis via \"A. tumefaciens\".\n\nThere are various combinations of regulation involving pathogenicity islands. The first combination is that the pathogenicity island contains the genes to regulate the virulence genes encoded on the PAI. The second combination is that the pathogenicity island contains the genes to regulate genes located outside of the pathogenecity island. Additionally, regulatory genes outside of the PAI may regulate virulence genes in the pathogenicity island. Regulation genes typically encoded on PAIs include AraC-like proteins and two-component response regulators.\n\nPAIs can be considered unstable DNA regions as they are susceptible to deletions or mobilization. This may be due to the structure of PAIs, with direct repeats, insertion sequences and association with tRNA that enables deletion and mobilization at higher frequencies. Additionally, deletions of pathogenicity islands inserted in the genome can result in disrupting tRNA and subsequently affect the metabolism of the cell.\n\n\n", "id": "1031641", "title": "Pathogenicity island"}
{"url": "https://en.wikipedia.org/wiki?curid=17204799", "text": "Antagonistic pleiotropy hypothesis\n\nThe antagonistic pleiotropy hypothesis was first proposed by George C. Williams in 1957 as an evolutionary explanation for senescence. Pleiotropy is the phenomenon where one gene controls for more than one phenotypic trait in an organism. Antagonistic pleiotropy is when one gene controls for more than one trait where at least one of these traits is beneficial to the organism's fitness and at least one is detrimental to the organism's fitness. The theme of G.C. William's idea about antagonistic pleiotropy was that if a gene caused both increased reproduction in early life and aging in later life, then senescence would be adaptive in evolution. For example, one study suggests that since follicular depletion in human females causes both more regular cycles in early life and loss of fertility later in life through menopause, it can be selected for by having its early benefits outweigh its late costs.\n\nAntagonistic pleiotropy is one of the several reasons evolutionary biologists give for organisms never being able to reach perfection through natural selection. Antagonistically pleiotropic genes are the explanation for fitness trade-offs. This means that genes that are pleiotropic control for some beneficial traits and some detrimental traits; thus, if they happen to persist through natural selection, this will prevent organisms from reaching perfection because if they possess the benefits of the gene, they must also possess the imperfections or faults. An example of this would be female rodents that live in a nest with other females and may end up feeding young that are not theirs due to their intense parental drive. This strong parental drive will be selected for, but the organisms will still make the mistake of feeding young that are not theirs and mis-allocating their resources.\n\nAntagonistic pleiotropy has several negative consequences. It results in delayed adaptation, an altered path of evolution, and reduced adaptation of other traits. In addition, the overall benefit of alleles is cut down significantly (by about half) by pleiotropy. Still, antagonistic pleiotropy has some evolutionary benefits. In fact, the conservation of genes is directly related to the pleiotropic character of an organism. This implies that genes that control for multiple traits, even if the traits have different implications for the organism's fitness, have more staying power in an evolutionary context.\n\nIt is generally accepted that the evolution of secondary sexual characteristics persists until the relative costs of survival outweigh the benefits of reproductive success. At the level of genes, this means a trade-off between variation and expression of selected traits. Strong, persistent sexual selection should result in decreased genetic variation for these traits. However, higher levels of variation have been reported in sexually-selected traits compared to non-sexually selected traits. This phenomenon is especially clear in lek species, where males confer no immediate advantage to the female. Female choice presumably depends on correlating male displays (secondary sexual characteristics) with overall genetic quality. If such directional sexual selection depletes variation in males, why would female choice continue to exist? Rowe and Houle answer this question (the lek paradox) using the notion of genetic capture, which couples the sexually-selected traits with the overall condition of the organism. They posit that the genes for secondary sexual characteristics must be pleiotropically linked to condition, a measure of the organism's fitness. In other words, the genetic variation in secondary sexual characteristics is a maintained due to variation in the organism's condition.\n\nThe survival of many serious genetic disorders in our long evolutionary history has led researchers to reassess the role of antagonistic pleiotropy in disease. If genetic disorders are defined by the existence of deleterious alleles, then natural selection acting over evolutionary time would result in a lower frequency of mutations than are currently observed. In a recent article, Carter and Nguyen identify several genetic disorders, arguing that far from being a rare phenomenon, antagonistic pleiotropy might be a fundamental mechanism for the survival of these non-optimal alleles.\n\nIn one of these studies, 99 individuals with Laron syndrome (a rare form of dwarfism) were monitored alongside their non-dwarf kin for a period of ten years. Patients with Laron syndrome possess one of three genotypes for the growth hormone receptor gene (GHR). Most patients have an A->G splice site mutation in position 180 in exon 6. Some others possess a nonsense mutation (R43X), while the rest are heterozygous for the two mutations. Interestingly, Laron syndrome patients experienced a lower incidence of cancer mortality and diabetes compared to their non-dwarf kin. This suggests a role for antagonistic pleiotropy, whereby a deleterious mutation is preserved in a population because it still confers some survival benefit.\n\nAnother instance of antagonistic pleiotropy is manifested in Huntington's disease, a rare neurodegenerative disorder characterized by a high number of CAG repeats within the Huntingtin gene. The onset of Huntington's is usually observed post-reproductive age and generally involves involuntary muscle spasms, cognitive difficulties and psychiatric problems. Incidentally, the high number of CAG repeats is associated with increased activity of p53, a tumor suppressing protein that participates in apoptosis. It has been hypothesized that this explains the lower rates of cancer among Huntington's patients. Huntington's disease is also correlated with high fecundity.\n\nAdditionally, it was found that individuals with a higher pro-inflammatory ratio TNFα/IL-10 had a significantly higher incidence of death due to cardiovascular disease in old age. Yet, it was hypothesized that this genotype was prevalent in as higher ratios of TNFα/IL-10 allows individuals to more effectively combat infection during reproductive years.\n\nSickle cell anemia, Beta-thalassemia, and cystic fibrosis are some other examples of the role antagonistic pleiotropy may play in genetic disorders.\n\nAlthough there are so many negative effects related to genes that are antagonistically pleiotropic, it is still present among most forms of life. Indeed, pleiotropy is one of the most common traits possessed by genes overall. In addition to that, pleiotropy is under strong stabilizing selection. In one experiment with mice and the morphology of the mandible, 1/5 of the loci had effects of pleiotropy for the entire mandible. One other example was in the Russian biologist Dmitry K. Belyaev's study on the domestication of the fox. In Dmitry K. Belyaev's farm-fox experiment, wild foxes were bred for docile behavior alone. After 40 generations, other physiological changes had surfaced including shortened tails, floppy ears, a white star in the forehead, rolled tails, shorter legs. Since the only thing being selected for was behavior, this leads scientists to believe that these secondary characteristics were controlled by the same gene or genes as docile behavior.\n\nAn antagonistically pleiotropic gene can be selected for if it has beneficial effects in early life while having its negative effects in later life because genes tend to have larger impacts on fitness in an organism's prime than in their old age. An example of this is testosterone levels in male humans. Higher levels of this hormone lead to increased fitness in early life, while causing decreased fitness in later life due to a higher risk for prostate cancer. This is an example of antagonistic pleiotropy being an explanation for senescence. Senescence is the act of ageing in individuals; it's the failure over time of the individual's life processes by natural causes. Williams's theory has been the motivation for many of the experimental studies on the reasons for aging in the last 25 years. However there is more than one theory out there for aging. The competing model to explain senescence is Medawar's \"mutation accumulation\" hypothesis, saying that \"over evolutionary time, late-acting mutations will accumulate at a much faster rate than early-acting mutation. These late-acting mutations will thus lead to declining viability and/or fertility as an organism ages.\" Medawar's theory is based around the older concept of selection shadow that had been discussed throughout the early 1900s and led to Medawar's theory after discussions with J. B. S. Haldane in the 1940s.\n\nA prominent explanation for aging at the molecular level is the DNA damage theory of aging. It has been proposed that genetic elements that regulate DNA repair in somatic cells may constitute an important example of age-dependent pleiotropic \"genes\". As pointed out by Vijg, genome repair and maintenance is beneficial early in life by swiftly eliminating DNA damage or damaged cells. However, studies of DNA repair in the brain and in muscle indicate that the transition from mitotic cell division to the post-mitotic condition that occurs early in life is accompanied by a reduction in DNA repair. The reduced expression of DNA repair is presumably part of an evolutionary adaptation for diverting the resources of the cell that were previously used for DNA repair, as well as for replication and cell division, to more essential neuronal and muscular functions.\n\nThe harmful effect of this genetically controlled reduction in expression is to allow increased accumulation of DNA damage. Reduced DNA repair causes increased impairment of transcription and progressive loss of cell and tissue function. However, these harmful effects of DNA damage are cumulative and most severe in chronologically older individuals whose numbers diminish with time (by causes of death that can be independent of senescence). As a consequence, the beneficial effects of the genetic elements that control the reduction of DNA repair early in life would predominate. Thus regulatory genetic elements that reduce expression of DNA repair genes in post-mitotic cells appear to be important examples of the postulated pleiotropic \"genes\" that are beneficial in youth but deleterious at an older age.\n\n", "id": "17204799", "title": "Antagonistic pleiotropy hypothesis"}
{"url": "https://en.wikipedia.org/wiki?curid=31671293", "text": "Daisy chaining DNA\n\nDaisy chaining DNA is the process of when DNA undergoing PCR amplification becomes tangled and forms a 'daisy chain.' During PCR, primers or dNTP's will eventually be used up and limit further reactions. The depletion of primers causes daisy chaining. Since the denaturing and annealing processes still continue without primers, the single-stranded DNA molecules reanneal to themselves. However, this reannealing does not always occur with another complementary strand. It is this imperfect match up that causes 'tangles'. These tangles look like a daisy chain.\n", "id": "31671293", "title": "Daisy chaining DNA"}
{"url": "https://en.wikipedia.org/wiki?curid=341038", "text": "Reporter gene\n\nIn molecular biology, a reporter gene (often simply reporter) is a gene that researchers attach to a regulatory sequence of another gene of interest in bacteria, cell culture, animals or plants. Certain genes are chosen as reporters because the characteristics they confer on organisms expressing them are easily identified and measured, or because they are selectable markers. Reporter genes are often used as an indication of whether a certain gene has been taken up by or expressed in the cell or organism population.\n\nTo introduce a reporter gene into an organism, scientists place the reporter gene and the gene of interest in the same DNA construct to be inserted into the cell or organism. For bacteria or prokaryotic cells in culture, this is usually in the form of a circular DNA molecule called a plasmid. It is important to use a reporter gene that is not natively expressed in the cell or organism under study, since the expression of the reporter is being used as a marker for successful uptake of the gene of interest.\n\nCommonly used reporter genes that induce visually identifiable characteristics usually involve fluorescent and luminescent proteins. Examples include the gene that encodes jellyfish green fluorescent protein (GFP), which causes cells that express it to glow green under blue light, the enzyme luciferase, which catalyzes a reaction with luciferin to produce light, and the red fluorescent protein from the gene . The GUS gene has been commonly used in plants but luciferase and GFP are becoming more common.\n\nA common reporter in bacteria is the \"E. coli\" \"lacZ\" gene, which encodes the protein beta-galactosidase. This enzyme causes bacteria expressing the gene to appear blue when grown on a medium that contains the substrate analog X-gal. An example of a selectable-marker which is also a reporter in bacteria is the chloramphenicol acetyltransferase (CAT) gene, which confers resistance to the antibiotic chloramphenicol.\n\nMany methods of transfection and transformation – two ways of expressing a foreign or modified gene in an organism – are effective in only a small percentage of a population subjected to the techniques. Thus, a method for identifying those few successful gene uptake events is necessary. Reporter genes used in this way are normally expressed under their own promoter independent from that of the introduced gene of interest; the reporter gene can be expressed constitutively (that is, it is \"always on\") or inducibly with an external intervention such as the introduction of Isopropyl β-D-1-thiogalactopyranoside (IPTG) in the β-galactosidase system. As a result, the reporter gene's expression is independent of the gene of interest's expression, which is an advantage when the gene of interest is only expressed under certain specific conditions or in tissues that are difficult to access.\n\nIn the case of selectable-marker reporters such as CAT, the transfected population of bacteria can be grown on a substrate that contains chloramphenicol. Only those cells that have successfully taken up the construct containing the CAT gene will survive and multiply under these conditions.\n\nReporter genes can also be used to assay for the expression of the gene of interest, which may produce a protein that has little obvious or immediate effect on the cell culture or organism. In these cases, the reporter is directly attached to the gene of interest to create a gene fusion. The two genes are under the same promoter elements and are transcribed into a single messenger RNA molecule. The mRNA is then translated into protein. In these cases it is important that both proteins be able to properly fold into their active conformations and interact with their substrates despite being fused. In building the DNA construct, a segment of DNA coding for a flexible polypeptide linker region is usually included so that the reporter and the gene product will only minimally interfere with one another. Reporter gene assay have been increasingly used in high throughput screening (HTS) to identify small molecule inhibitors and activators of protein targets and pathways for drug discovery and chemical biology. Because the reporter enzymes themselves (e.g. firefly luciferase) can be direct targets of small molecules and confound the interpretation of HTS data, novel coincidence reporter designs incorporating artifact suppression have been developed \nReporter genes can be used to assay for the activity of a particular promoter in a cell or organism. In this case there is no separate \"gene of interest\"; the reporter gene is simply placed under the control of the target promoter and the reporter gene product's activity is quantitatively measured. The results are normally reported relative to the activity under a \"consensus\" promoter known to induce strong gene expression.\n\nA more complex use of reporter genes on a large scale is in two-hybrid screening, which aims to identify proteins that natively interact with one another \"in vivo\".\n\n\n", "id": "341038", "title": "Reporter gene"}
{"url": "https://en.wikipedia.org/wiki?curid=10807783", "text": "Allotype (immunology)\n\nIn immunology, an immunoglobulin allotype is the allele of the antibody chains found in the individual. The word allotype comes from two Greek roots, \"allo \"meaning 'other or differing from the norm' and \"typos \"meaning 'mark'. Thus allotype refers to the idea that each immunoglobulin has unique sequences particular to the individual's genome that manifest in its constant region (normally).\n\nThe most important types are Gm (heavy chain) and km (light chain).\n\nIt can be used in resolving paternity disputes.\n\n\n", "id": "10807783", "title": "Allotype (immunology)"}
{"url": "https://en.wikipedia.org/wiki?curid=33905720", "text": "SCCmec\n\nSCC\"mec, or staphylococcal cassette chromosome \"mec, is a mobile genetic element of \"Staphylococcus\" bacterial species. This genetic sequence includes the \"mecA\" gene coding for resistance to the antibiotic methicillin and is the only known way for \"Staphylococcus\" strains to spread the gene in the wild by horizontal gene transfer.\n\nNot all SCC\"mec\" elements are identical (in fact, SCC elements without the \"mecA\" gene do exist.) SCC\"mec\" elements have been classified into six types (I through VI) on the basis of two specific regions of their nucleotide sequences. One region is the \"mec\" complex including the \"mecA\" gene. The other is the \"ccr\" gene complex including genes coding for recombinases.\n\nThe \"mec\" complex is divided further into five types (I through V) based on the arrangement of regulatory genetic features such as \"mecR1\", an inducer.\n\nThe SCC\"mec\" found in methicillin-resistant \"Staphylococcus aureus\" likely originated in coagulase-negative staphylococcal species and was acquired by \"S. aureus\".\n\nStaphylococcal strains isolated from pig farms were found to carry several different types of SCC\"mec\", suggesting that they may serve as a reservoir of these elements.\n\n", "id": "33905720", "title": "SCCmec"}
{"url": "https://en.wikipedia.org/wiki?curid=1857574", "text": "Polly and Molly\n\nPolly and Molly (born 1997), two ewes, were the first mammals to have been successfully cloned from an adult somatic cell and to be transgenic animals at the same time. This is not to be confused with Dolly the Sheep, the first animal to be successfully cloned from an adult somatic cell where there was no genetic modification carried out on the adult donor nucleus. Polly and Molly, like Dolly the Sheep, were cloned at the Roslin Institute in Edinburgh, Scotland.\n\nThe creation of Polly and Molly built on the somatic nuclear transfer experiments that led to the cloning of Dolly the Sheep. The crucial difference was that in creating Polly and Molly, scientists used cells into which a new gene had been inserted. The gene chosen was a therapeutic protein to demonstrate the potential of such recombinant DNA technology combined with animal cloning. This could hopefully be used to produce pharmacological and therapeutic proteins to treat human diseases. The protein in question was the human blood clotting factor IX. Another difference from Dolly the Sheep was the source cell type of the nucleus that was transferred. In the case of Dolly the Sheep, the nucleus that was transferred came from mammary gland cells from a 6-year-old ewe; in the case of Polly and Molly, fibroblast cells were used.\n\nPrior to the production of Polly and Molly, the only demonstrated way to make a transgenic animal was by microinjection of DNA into the pronuclei of fertilized oocytes (eggs). However, only a small proportion of the animals will integrate the injected DNA into their genome. In the rare cases that they do integrate this new genetic information, the pattern of expression of the injected transgene's protein due to the random integration is very variable. As the aim of such research is to produce an animal that expresses a particular protein in high levels in, for example, its milk, microinjection is a very costly procedure that does not usually produce the desired animal.\n\nIn mice, there is an additional option for genetic transfer that is not available in other animals. Embryonic stem cells provide a means to transfer new DNA into the germline. They also allow precise genetic modifications by gene targeting. Modified embryonic stem cells can be selected in vitro before the experiment moves on further for the production of an animal. Embryonic stem cells capable of contributing to the germline of livestock species such as sheep have not been isolated.\n\nThe production of Dolly the Sheep and also Megan and Morag, the two sheep that led to the production of Dolly, demonstrated that viable sheep can be produced by nuclear transfer from a variety of somatic cell types which have been cultured in vitro. Polly and Molly represented the further step in which somatic cells were cultured in vitro, just as in the case with the previous sheep. However, in this case they were transfected with foreign DNA, and the transfected cells which stably integrated this new piece of genetic information were selected. The nuclei of these somatic cells was then transferred into an empty oocyte, as in the procedure of nuclear transfer, and this was used to produce several transgenic animals. A cell type PDFF was used. PDFF5 would produce male animals and were not transduced. Cell type PDFF2 produced female animals and were transduced. Of the gestations that occurred, three PDFF2 animals were born, two of which survived birth, 7LL8 and 7LL12. These animals were transfected but contained a marker gene \"not\" the cloned gene of interest. These were named \"Holly\" and \"Olly\". Two more subsets of female-producing PDFF2 cells, PDFF2-12 and PDFF2-13, also produced animals which had the cell of interest together with the marker. Of these lambs, 7LL12, 7LL15, and 7LL13 were born alive and healthy. Two of these were named Polly and Molly.\n\nThe transgene that was inserted in the donor somatic cells was designed to express the human clotting factor IX protein in the milk of sheep. This protein plays an essential role in blood coagulation, and deficiency leads to the disease haemophilia B of which treatment requires intravenous infusion of factor IX. The production of this protein in livestock milk, a process known as pharming, would provide a source of this therapeutic protein that would reduce the cost and also would be free of potential infectious risk associated with the current source of this protein (human blood).\n", "id": "1857574", "title": "Polly and Molly"}
{"url": "https://en.wikipedia.org/wiki?curid=23048", "text": "Prion\n\nPrions are infectious agents composed entirely of a protein material that can fold in multiple, structurally abstract ways, at least one of which is transmissible to other prion proteins, leading to disease in a manner that is epidemiologically comparable to the spread of viral infection. Prions composed of the prion protein (PrP) are believed to be the cause of transmissible spongiform encephalopathies (TSEs) among other diseases.\n\nPrions were initially identified as the causative agent in animal TSEs derived from scrapie in sheep and later bovine spongiform encephalopathy (BSE)—known popularly as \"mad cow disease\". Human prion diseases include Creutzfeldt–Jakob disease (CJD) and its variant (vCJD), Gerstmann–Sträussler–Scheinker syndrome, fatal familial insomnia, and kuru. All known prion diseases in mammals affect the structure of the brain or other neural tissue. No effective medical treatment is known. The illness is progressive and always fatal.\n\nA 2015 study concluded that multiple system atrophy (MSA), a rare human neurodegenerative disease, is caused by a misfolded version of a protein called alpha-synuclein, and is therefore also classifiable as a prion disease. Several yeast proteins have also been identified as having prionogenic properties.\n\nA protein as a stand-alone infectious agent stands in contrast to all other known infectious agents such as viruses, bacteria, fungi, and parasites, all of which contain nucleic acids (DNA, RNA, or both). For this reason, a minority of researchers still consider the prion/TSE hypothesis unproven.\n\nPrions may propagate by transmitting their misfolded protein state. When a prion enters a healthy organism, it induces existing, properly folded proteins to convert into the misfolded prion form. In this way, the prion acts as a template to guide the misfolding of more proteins into prion form. In yeast, this refolding is assisted by chaperone proteins such as Hsp104. These refolded prions can then go on to convert more proteins themselves, leading to a chain reaction resulting in large amounts of the prion form. All known prions induce the formation of an amyloid fold, in which the protein polymerises into an aggregate consisting of tightly packed beta sheets. Amyloid aggregates are fibrils, growing at their ends, and replicate when breakage causes two growing ends to become four growing ends. The incubation period of prion diseases is determined by the exponential growth rate associated with prion replication, which is a balance between the linear growth and the breakage of aggregates. The propagation of the prion depends on the presence of normally folded protein in which the prion can induce misfolding; animals that do not express the normal form of the prion protein can neither develop nor transmit the disease.\n\nPrion aggregates are extremely stable and accumulate in infected tissue, causing tissue damage and cell death. This structural stability means that prions are resistant to denaturation by chemical and physical agents, making disposal and containment of these particles difficult. Prion structure varies slightly between species, but nonetheless prion replication is subject to epimutation and natural selection just like other forms of replication.\n\nDuring the 1960s, two London-based researchers, radiation biologist Tikvah Alper and biophysicist John Stanley Griffith, developed the hypothesis that some transmissible spongiform encephalopathies are caused by an infectious agent consisting solely of proteins. Earlier investigations by E. J. Field into scrapie and kuru had identified the transfer of pathologically inert polysaccharides that only become infectious in the host. Alper and Griffith wanted to account for the discovery that the mysterious infectious agent causing the diseases scrapie and Creutzfeldt–Jakob disease resisted ionizing radiation. (A single ionizing \"hit\" normally destroys an entire infectious particle, and the dose needed to hit half the particles depends on the size of the particles. Empirical results of ionizing doses applied to the unknown infectious substance evidenced an infectious particle size too small to be a viral mechanism.)\n\nFrancis Crick recognized the potential significance of the Griffith protein-only hypothesis for scrapie propagation in the second edition of his \"Central dogma of molecular biology\" (1970): While asserting that the flow of sequence information from protein to protein, or from protein to RNA and DNA was \"precluded\", he noted that Griffith's hypothesis was a potential contradiction (although it was not so promoted by Griffith). The revised hypothesis was later formulated, in part, to accommodate reverse transcription (which both Howard Temin and David Baltimore discovered in 1970).\n\nIn 1982, Stanley B. Prusiner of the University of California, San Francisco announced that his team had purified the hypothetical infectious prion, and that the infectious agent consisted mainly of a specific protein – though they did not manage to isolate the protein until two years after Prusiner's announcement. While the infectious agent was named a prion, the specific protein that the prion was composed of is also known as the Prion Protein (PrP), though this protein may occur both in infectious and non-infectious forms. Prusiner won the Nobel Prize in Physiology or Medicine in 1997 for his research into prions.\n\nThe protein that prions are made of (PrP) is found throughout the body, even in healthy people and animals. However, PrP found in infectious material has a different structure and is resistant to proteases, the enzymes in the body that can normally break down proteins. The normal form of the protein is called PrP, while the infectious form is called PrP — the \"C\" refers to 'cellular' PrP, while the \"Sc\" refers to 'scrapie', the prototypic prion disease, occurring in sheep. While PrP is structurally well-defined, PrP is certainly polydisperse and defined at a relatively poor level. PrP can be induced to fold into other more-or-less well-defined isoforms in vitro, and their relationship to the form(s) that are pathogenic in vivo is not yet clear.\n\nPrP is a normal protein found on the membranes of cells. It has 209 amino acids (in humans), one disulfide bond, a molecular mass of 35–36 kDa and a mainly alpha-helical structure. Several topological forms exist; one cell surface form anchored via glycolipid and two transmembrane forms. The normal protein is not sedimentable; meaning that it cannot be separated by centrifuging techniques. Its function is a complex issue that continues to be investigated. PrP binds copper (II) ions with high affinity. The significance of this finding is not clear, but it is presumed to relate to PrP structure or function. PrP is readily digested by proteinase K and can be liberated from the cell surface in vitro by the enzyme phosphoinositide phospholipase C (PI-PLC), which cleaves the glycophosphatidylinositol (GPI) glycolipid anchor. PrP has been reported to play important roles in cell-cell adhesion and intracellular signaling \"in vivo\", and may therefore be involved in cell-cell communication in the brain.\n\nProtease-resistant PrP-like protein (PrP) is an isoform of PrP from which is structurally altered and converted into a misfolded proteinase K-resistant form \"in vitro\". To model conversion of PrP to PrP in vitro, Saborio \"et al\". rapidly converted PrP into a PrP by a procedure involving cyclic amplification of protein misfolding. The term \"PrP\" has been made to distinguish between PrP, which is isolated from infectious tissue and associated with the transmissible spongiform encephalopathy agent. For example, unlike PrP, PrP may not necessarily be infectious.\n\nThe infectious isoform of PrP, known as PrP, is able to convert normal PrP proteins into the infectious isoform by changing their conformation, or shape; this, in turn, alters the way the proteins interconnect. PrP always causes prion disease. Although the exact 3D structure of PrP is not known, it has a higher proportion of β-sheet structure in place of the normal α-helix structure. Aggregations of these abnormal isoforms form highly structured amyloid fibers, which accumulate to form plaques. It is unclear as to whether these aggregates are the cause of cell damage or are simply a side-effect of the underlying disease process. The end of each fiber acts as a template onto which free protein molecules may attach, allowing the fiber to grow. Under most circumstances, only PrP molecules with an identical amino acid sequence to the infectious PrP are incorporated into the growing fiber. However, rare cross-species transmission is also possible.\n\nThe physiological function of the prion protein remains a controversial matter. While data from in vitro experiments suggest many dissimilar roles, studies on PrP knockout mice have provided only limited information because these animals exhibit only minor abnormalities. In research done in mice, it was found that the cleavage of PrP proteins in peripheral nerves causes the activation of myelin repair in Schwann cells and that the lack of PrP proteins caused demyelination in those cells.\n\nA review of evidence in 2005 suggested that PrP may have a normal function in maintenance of long-term memory. As well, a 2004 study found that mice lacking genes for normal cellular PrP protein show altered hippocampal long-term potentiation.\n\nA 2006 article from the Whitehead Institute for Biomedical Research indicates that PrP expression on stem cells is necessary for an organism's self-renewal of bone marrow. The study showed that all long-term hematopoietic stem cells express PrP on their cell membrane and that hematopoietic tissues with PrP-null stem cells exhibit increased sensitivity to cell depletion.\n\nThe first hypothesis that tried to explain how prions replicate in a protein-only manner was the heterodimer model. This model assumed that a single PrP molecule binds to a single PrP molecule and catalyzes its conversion into PrP. The two PrP molecules then come apart and can go on to convert more PrP. However, a model of prion replication must explain both how prions propagate, and why their spontaneous appearance is so rare. Manfred Eigen showed that the heterodimer model requires PrP to be an extraordinarily effective catalyst, increasing the rate of the conversion reaction by a factor of around 10. This problem does not arise if PrP exists only in aggregated forms such as amyloid, where cooperativity may act as a barrier to spontaneous conversion. What is more, despite considerable effort, infectious monomeric PrP has never been isolated.\n\nAn alternative model assumes that PrP exists only as fibrils, and that fibril ends bind PrP and convert it into PrP. If this were all, then the quantity of prions would increase linearly, forming ever longer fibrils. But exponential growth of both PrP and of the quantity of infectious particles is observed during prion disease. This can be explained by taking into account fibril breakage. A mathematical solution for the exponential growth rate resulting from the combination of fibril growth and fibril breakage has been found. The exponential growth rate depends largely on the square root of the PrP concentration. The incubation period is determined by the exponential growth rate, and in vivo data on prion diseases in transgenic mice match this prediction. The same square root dependence is also seen in vitro in experiments with a variety of different amyloid proteins.\n\nThe mechanism of prion replication has implications for designing drugs. Since the incubation period of prion diseases is so long, an effective drug does not need to eliminate all prions, but simply needs to slow down the rate of exponential growth. Models predict that the most effective way to achieve this, using a drug with the lowest possible dose, is to find a drug that binds to fibril ends and blocks them from growing any further.\n\nUntil 2015 all known mammalian prion diseases were considered to be caused by the prion protein, PrP; in 2015 Multiple System Atrophy was found to be likely caused by a new prion, the misfolded form of a protein called alpha-synuclein. The endogenous, properly folded form of the prion protein is denoted PrP (for Common\" or Cellular\"), whereas the disease-linked, misfolded form is denoted PrP (for \"Scrapie\"), after one of the diseases first linked to prions and neurodegeneration.) The precise structure of the prion is not known, though they can be formed by combining PrP, polyadenylic acid, and lipids in a protein misfolding cyclic amplification (PMCA) reaction. Proteins showing prion-type behavior are also found in some fungi, which has been useful in helping to understand mammalian prions. Fungal prions do not appear to cause disease in their hosts.\n\nPrions cause neurodegenerative disease by aggregating extracellularly within the central nervous system to form plaques known as amyloid, which disrupt the normal tissue structure. This disruption is characterized by \"holes\" in the tissue with resultant spongy architecture due to the vacuole formation in the neurons. Other histological changes include astrogliosis and the absence of an inflammatory reaction. While the incubation period for prion diseases is relatively long (5 to 20 years), once symptoms appear the disease progresses rapidly, leading to brain damage and death. Neurodegenerative symptoms can include convulsions, dementia, ataxia (balance and coordination dysfunction), and behavioural or personality changes.\n\nAll known prion diseases, collectively called transmissible spongiform encephalopathies (TSEs), are untreatable and fatal. However, a vaccine developed in mice may provide insight into providing a vaccine to resist prion infections in humans. Additionally, in 2006 scientists announced that they had genetically engineered cattle lacking a necessary gene for prion production – thus theoretically making them immune to BSE, building on research indicating that mice lacking normally occurring prion protein are resistant to infection by scrapie prion protein. In 2013, a study revealed that 1 in 2,000 people in the United Kingdom might harbour the infectious prion protein that causes vCJD.\n\nMany different mammalian species can be affected by prion diseases, as the prion protein (PrP) is very similar in all mammals. Due to small differences in PrP between different species it is unusual for a prion disease to transmit from one species to another. The human prion disease variant Creutzfeldt–Jakob disease, however, is believed to be caused by a prion that typically infects cattle, causing Bovine spongiform encephalopathy and is transmitted through infected meat.\n\nIt has been recognized that prion diseases can arise in three different ways: acquired, familial, or sporadic. It is often assumed that the diseased form directly interacts with the normal form to make it rearrange its structure. One idea, the \"Protein X\" hypothesis, is that an as-yet unidentified cellular protein (Protein X) enables the conversion of PrP to PrP by bringing a molecule of each of the two together into a complex.\n\nCurrent research suggests that the primary method of infection in animals is through ingestion. It is thought that prions may be deposited in the environment through the remains of dead animals and via urine, saliva, and other body fluids. They may then linger in the soil by binding to clay and other minerals.\n\nA University of California research team, led by Nobel Prize winner Stanley Prusiner, has provided evidence for the theory that infection can occur from prions in manure. And, since manure is present in many areas surrounding water reservoirs, as well as used on many crop fields, it raises the possibility of widespread transmission. It was reported in January 2011 that researchers had discovered prions spreading through airborne transmission on aerosol particles, in an animal testing experiment focusing on scrapie infection in laboratory mice. Preliminary evidence supporting the notion that prions can be transmitted through use of urine-derived human menopausal gonadotropin, administered for the treatment of infertility, was published in 2011.\n\nIn 2015, researchers at The University of Texas Health Science Center at Houston found that plants can be a vector for prions. When researchers fed hamsters grass that grew on ground where a deer that died with chronic wasting disease (CWD) was buried, the hamsters became ill with CWD, suggesting that prions can bind to plants, which then take them up into the leaf and stem structure, where they can be eaten by herbivores, thus completing the cycle. It is thus possible that there is a progressively accumulating number of prions in the environment.\n\nInfectious particles possessing nucleic acid are dependent upon it to direct their continued replication. Prions, however, are infectious by their effect on normal versions of the protein. Sterilizing prions, therefore, requires the denaturation of the protein to a state in which the molecule is no longer able to induce the abnormal folding of normal proteins. In general, prions are quite resistant to proteases, heat, ionizing radiation, and formaldehyde treatments, although their infectivity can be reduced by such treatments. Effective prion decontamination relies upon protein hydrolysis or reduction or destruction of protein tertiary structure. Examples include sodium hypochlorite, sodium hydroxide, and strongly acidic detergents such as LpH. 134 °C (274 °F) for 18 minutes in a pressurized steam autoclave has been found to be somewhat effective in deactivating the agent of disease. Ozone sterilization is currently being studied as a potential method for prion denaturation and deactivation. Renaturation of a completely denatured prion to infectious status has not yet been achieved; however, partially denatured prions can be renatured to an infective status under certain artificial conditions.\n\nThe World Health Organization recommends any of the following three procedures for the sterilization of all heat-resistant surgical instruments to ensure that they are not contaminated with prions:\n\n\nWhile PrP is considered the only mammalian prion, prion-like domains have been found in a variety of other mammalian proteins. Some of these proteins have been implicated in the ontogeny of age-related neurodegenerative disorders such as amyotrophic lateral sclerosis (ALS, known as Motor Neurone Disease outside the US), frontotemporal lobar degeneration with ubiquitin-positive inclusions (FTLD-U), Alzheimer's disease, and Huntington's disease, as well as some forms of Systemic Amyloidosis including AA (Secondary) Amyloidosis that develops in humans and animals with inflammatory and infectious diseases such as Tuberculosis, Crohn's disease, Rheumatoid arthritis, and HIV AIDS. AA amyloidosis, like prion disease, may be transmissible. This has given rise to the 'prion paradigm', where otherwise harmless proteins can be converted to a pathogenic form by a small number of misfolded, nucleating proteins.\n\nThe definition of a prion-like domain arises from the study of fungal prions. In yeast, prionogenic proteins have a portable prion domain that is both necessary and sufficient for self-templating and protein aggregation. This has been shown by attaching the prion domain to a reporter protein, which then aggregates like a known prion. Similarly, removing the prion domain from a fungal prion protein inhibits prionogenesis. This modular view of prion behaviour has led to the hypothesis that similar prion domains are present in animal proteins, in addition to PrP. These fungal prion domains have several characteristic sequence features. They are typically enriched in asparagine, glutamine, tyrosine and glycine residues, with an asparagine bias being particularly conducive to the aggregative property of prions. Historically, prionogenesis has been seen as independent of sequence and only dependent on relative residue content. However, this has been shown to be false, with the spacing of prolines and charged residues having been shown to be critical in amyloid formation.\n\nBioinformatic screens have predicted that over 250 human proteins contain prion-like domains (PrLD). These domains are hypothesized to have the same transmissible, amyloidogenic properties of PrP and known fungal proteins. As in yeast, proteins involved in gene expression and RNA binding seem to be particularly enriched in PrLD's, compared to other classes of protein. In particular, 29 of the known 210 proteins with an RNA recognition motif also have a putative prion domain. Meanwhile, several of these RNA-binding proteins have been independently identified as pathogenic in cases of ALS, FTLD-U, Alzheimer's disease, and Huntington's disease.\n\nThe pathogenicity of prions and proteins with prion-like domains arises from their self-templating ability and the resulting exponential growth of amyloid fibrils. The presence of amyloid fibrils in patients with degenerative diseases has been well documented. These amyloid fibrils are seen as the result of pathogenic proteins that self-propagate and form highly stable, non-functional aggregates. While this does not necessarily imply a causal relationship between amyloid and degenerative diseases, the toxicity of certain amyloid forms and the overproduction of amyloid in familial cases of degenerative disorders supports the idea that amyloid formation is generally toxic.\n\nSpecifically, aggregation of TDP-43, an RNA-binding protein, has been found in ALS/MND patients, and mutations in the genes coding for these proteins have been identified in familial cases of ALS/MND. These mutations promote the misfolding of the proteins into a prion-like conformation. The misfolded form of TDP-43 forms cytoplasmic inclusions in afflicted neurons, and is found depleted in the nucleus. In addition to ALS/MND and FTLD-U, TDP-43 pathology is a feature of many cases of Alzheimer's disease, Parkinson's disease and Huntington's disease. The misfolding of TDP-43 is largely directed by its prion-like domain. This domain is inherently prone to misfolding, while pathological mutations in TDP-43 have been found to increase this propensity to misfold, explaining the presence of these mutations in familial cases of ALS/MND. As in yeast, the prion-like domain of TDP-43 has been shown to be both necessary and sufficient for protein misfolding and aggregation.\n\nSimilarly, pathogenic mutations have been identified in the prion-like domains of heterogeneous nuclear riboproteins hnRNPA2B1 and hnRNPA1 in familial cases of muscle, brain, bone and motor neuron degeneration. The wild-type form of all of these proteins show a tendency to self-assemble into amyloid fibrils, while the pathogenic mutations exacerbate this behaviour and lead to excess accumulation.\n\nFungal proteins exhibiting templated conformational change were discovered in the yeast \"Saccharomyces cerevisiae\" by Reed Wickner in the early 1990s. For their mechanistic similarity to mammalian prions, they were termed yeast prions. Subsequent to this, a prion has also been found in the fungus \"Podospora anserina\". These prions behave similarly to PrP, but, in general, are nontoxic to their hosts. Susan Lindquist's group at the Whitehead Institute has argued some of the fungal prions are not associated with any disease state, but may have a useful role; however, researchers at the NIH have also provided arguments suggesting that fungal prions could be considered a diseased state. There is mounting evidence that fungal proteins have evolved specific functions that are beneficial to the microorganism that enhance their ability to adapt to their diverse environments.\n\n, there are eight known prion proteins in fungi, seven in \"Saccharomyces cerevisiae\" (Sup35, Rnq1, Ure2, Swi1, Mot3, Cyc8, and Mod5) and one in \"Podospora anserina\" (HET-s). The article that reported the discovery of a prion form, the Mca1 protein, was retracted due to the fact that the data could not be reproduced. Notably, most of the fungal prions are based on glutamine/asparagine-rich sequences, with the exception of HET-s and Mod5.\n\nResearch into fungal prions has given strong support to the protein-only concept, since purified protein extracted from cells with a prion state has been demonstrated to convert the normal form of the protein into a misfolded form \"in vitro\", and in the process, preserve the information corresponding to different strains of the prion state. It has also shed some light on prion domains, which are regions in a protein that promote the conversion into a prion. Fungal prions have helped to suggest mechanisms of conversion that may apply to all prions, though fungal prions appear distinct from infectious mammalian prions in the lack of cofactor required for propagation. The characteristic prion domains may vary between species—e.g., characteristic fungal prion domains are not found in mammalian prions.\n\nAdvancements in computer modeling have allowed scientists to identify compounds that can treat prion-caused diseases, such as one compound found to bind a cavity in the PrP and stabilize the conformation, reducing the amount of harmful PrP.\n\nAntiprion antibodies capable of crossing the blood-brain-barrier and targeting cytosolic prion protein (an otherwise major obstacle in prion therapeutics) have been described.\n\nIn the last decade, some progress dealing with ultra-high-pressure inactivation of prion infectivity in processed meat has been reported.\n\nIn 2011, it was discovered that prions could be degraded by lichens.\n\nThere continues to be a very practical problem with diagnosis of prion diseases, including BSE and CJD. They have an incubation period of months to decades, during which there are no symptoms, even though the pathway of converting the normal brain PrP protein into the toxic, disease-related PrP form has started. At present, there is virtually no way to detect PrP reliably except by examining the brain using neuropathological and immunohistochemical methods after death. Accumulation of the abnormally folded PrP form of the PrP protein is a characteristic of the disease, but it is present at very low levels in easily accessible body fluids like blood or urine. Researchers have tried to develop methods to measure PrP, but there are still no fully accepted methods for use in materials such as blood.\n\nIn 2010, a team from New York described detection of PrP even when initially present at only one part in a hundred billion (10) in brain tissue. The method combines amplification with a novel technology called Surround Optical Fiber Immunoassay (SOFIA) and some specific antibodies against PrP. After amplifying and then concentrating any PrP, the samples are labelled with a fluorescent dye using an antibody for specificity and then finally loaded into a micro-capillary tube. This tube is placed in a specially constructed apparatus so that it is totally surrounded by optical fibres to capture all light emitted once the dye is excited using a laser.\n\nThe RT-QuIC assay, a microplate reader-based prion detection method which uses as reagents normally folded prions, fluorescently labelled so that they \"light up\" when they are misfolded; samples suspected of containing misfolded prions are added and misfolded reagents can be detected by standard fluorescence detection methods.\n\nAstemizole has been found to have anti-prion activity.\n\nAnother type of chemical that may be effective against prion infection is the luminescent conjugated polythiophenes, fluorescent compounds that are often used to stain tissue samples. In a 2015 study, led by Adriano Aguzzi, professor of neurobiology at the University of Zurich, found that when they injected mice with a prion disease and then with polythiophenes, the mice survived eighty percent longer than the control mice that were only injected with the prion disease.\n\nWhether prions cause disease or are merely a symptom caused by a different agent is still debated by a minority of researchers. The following sections describe several hypotheses: Some pertain to the composition of the infectious agent (protein-only, protein with other components, virus, or other), while others pertain to its mechanism of reproduction.\n\nPrior to the discovery of prions, it was thought that all pathogens used nucleic acids to direct their replication. The \"protein-only hypothesis\" states that a protein structure can replicate without the use of nucleic acids. This was initially controversial as it contradicts the central dogma of molecular biology, which describes nucleic acid as the central form of replicative information.\n\nEvidence in favor of a protein-only hypothesis includes:\n\nA gene for the normal protein has been identified: the \"PRNP\" gene. In all inherited cases of prion disease, there is a mutation in the \"PRNP\" gene. Many different \"PRNP\" mutations have been identified and these proteins are more likely to fold into abnormal prion. Although this discovery puts a hole in the general prion hypothesis, that prions can aggregate only proteins of identical amino acid make-up. These mutations can occur throughout the gene. Some mutations involve expansion of the octapeptide repeat region at the N-terminal of PrP. Other mutations that have been identified as a cause of inherited prion disease occur at positions 102, 117 & 198 (GSS), 178, 200, 210 & 232 (CJD) and 178 (Fatal Familial Insomnia, FFI). The cause of prion disease can be sporadic, genetic, or infectious, or a combination of these factors. For example, to have scrapie, both an infectious agent and a susceptible genotype must be present.\n\nDespite much effort, significant titers of prion infectivity have never been produced by refolding pure PrP molecules, raising doubt about the validity of the \"protein only\" hypothesis. In addition, the \"protein only\" hypothesis fails to provide a molecular explanation for the ability of prion strains to target specific areas of the brain in distinct patterns. These shortcomings, along with additional experimental data, have given rise to the \"multi-component\" or \"cofactor variation\" hypothesis.\n\nIn 2007, biochemist Surachai Supattapone and his colleagues at Dartmouth College produced purified infectious prions \"de novo\" from defined components (PrP, co-purified lipids, and a synthetic polyanionic molecule). These researchers also showed that the polyanionic molecule required for prion formation was selectively incorporated into high-affinity complexes with PrP molecules, leading them to hypothesize that infectious prions may be composed of multiple host components, including PrP, lipid, and polyanionic molecules, rather than PrP alone.\n\nIn 2010, Jiyan Ma and colleagues at The Ohio State University produced infectious prions from a recipe of bacterially expressed recombinant PrP, POPG phospholipid, and RNA, further supporting the multi-component hypothesis. This finding is in contrast to studies that found minimally infectious prions produced from recombinant PrP alone.\n\nIn 2012, Supattapone and colleagues purified the membrane lipid phosphatidylethanolamine as a solitary endogenous cofactor capable of facilitating the formation of high-titer recombinant prions derived from multiple prion strains. They also reported that the cofactor is essential for maintaining the infectious conformation of PrP, and that cofactor molecules dictate the strain properties of infectious prions.\n\nReports suggest that imbalance of brain metal homeostasis may be a cause of PrP-associated neurotoxicity, though the underlying mechanisms are difficult to explain based on existing information. Proposed hypotheses include a functional role for PrP in metal metabolism, and loss of this function due to aggregation to the disease-associated PrP form as the cause of brain metal imbalance. Other views suggest gain of toxic function by PrP due to sequestration of PrP-associated metals within the aggregates, resulting in the generation of redox-active PrP complexes. The physiological implications of some PrP-metal interactions are known, while others are still unclear. The pathological implications of PrP-metal interaction include metal-induced oxidative damage, and in some instances conversion of PrP to a PrP-like form.\n\nThe protein-only hypothesis has been criticised by those maintaining that the simplest explanation of the evidence to date is viral. For more than a decade, Yale University neuropathologist Laura Manuelidis has been proposing that prion diseases are caused instead by an unidentified slow virus. In January 2007, she and her colleagues published an article reporting to have found a virus in 10%, or less, of their scrapie-infected cells in culture.\n\nEvidence in favor of a viral hypothesis includes:\n\nStudies propagating TSE infectivity in cell-free reactions and in purified component chemical reactions is thought to strongly suggest against TSE viral nature. However, some viruses, such as Poliovirus, have the ability to replicate in cell-free reactions.\n\nThe 'virino hypothesis' postulates that the TSE agent is a foreign, self replicating nucleic acid or nucleic acid fragment bound to PrP.\n\n\"Spiroplasma\" is a cell wall–deficient bacterium related to \"Mycoplasma\", which some think may be the cause of the TSEs. The lack of a cell wall means it is not susceptible to conventional antibiotics such as penicillin, which target cell wall synthesis. Frank O. Bastian of Louisiana State University first discovered \"Spiroplasma\"-like inclusions in the brain of a CJD patient during an autopsy in 1979 and has hypothesized that this bacterium could possibly be the cause of the TSEs.\n\nHowever, , with the exception of \"Spiroplasma mirum\" strain SMCA causing spongiform microcystic encephalitis in suckling rats, other researchers have been unable to duplicate these findings, casting doubt on the \"Spiroplasma\" hypothesis. In defense of the \"Spiroplasma\" hypothesis, Bastian pointed out that \"Spiroplasma\" is hard to culture and that strain variation makes it hard to detect certain strains using PCR and other techniques, thus giving a false negative.\n\n\"Acinetobacter\" is a bacterium which some think is the cause of the TSEs.\n\nThe word \"prion\", coined in 1982 by Stanley B. Prusiner, is a portmanteau derived from protein and infection, hence prion, and is short for \"proteinaceous infectious particle\", in reference to its ability to self-propagate and transmit its conformation to other proteins. Its main pronunciation is , although , as the homographic name of the bird is pronounced, is also heard. In his 1982 paper introducing the term, Prusiner specified that it be \"pronounced \"pree\"-on.\"\n\n\n\n\n\n\n", "id": "23048", "title": "Prion"}
{"url": "https://en.wikipedia.org/wiki?curid=162736", "text": "Heterogamy\n\nHeterogamy (from Ancient Greek ἕτερος \"heteros\", \"other, another\" and γάμος \"gámos\", \"marriage\") has a number of biological definitions.\n\nIn reproductive biology, heterogamy is the alternation of differently organized generations, applied to the alternation between parthenogenetic and a sexual generation. This type of heterogamy occurs for example in some aphids.\n\nAlternately, \"heterogamy\" or \"heterogamous\" is often used as a synonym of heterogametic, meaning the presence of two unlike chromosomes in a sex. For example, XY males and ZW females are called the heterogamous sex.\n\nIn cell biology, heterogamy is a synonym of anisogamy, the condition of having differently sized male and female gametes produced by different sexes or mating types in a species.\n\nIn botany, a plant is heterogamous when it carries at least two different types of flowers in regard to their reproductive structures, for example male and female flowers or bisexual and female flowers. Stamens and carpels are not regularly present in each flower or floret.\n\nIn sociology, heterogamy refers to a marriage between two individuals that differ in a certain criterion, and is contrasted with homogamy for a marriage or union between partners that match according to that criterion. For example, ethnic heterogamy refers to marriages involving individuals of different ethnic groups. Age heterogamy refers to marriages involving partners of significantly different ages. Heterogamy and homogamy are also used to describe marriage or union between people of unlike and like sex (or gender) respectively.\n\n", "id": "162736", "title": "Heterogamy"}
{"url": "https://en.wikipedia.org/wiki?curid=34112061", "text": "Bioecological model\n\nThe bioecological model is a theoretical model of gene–environment interactions in human development. This model, first proposed by Urie Bronfenbrenner and Stephen J. Ceci, in 1994, is an extension of Bronfenbrenner's original theoretical model of human development, called ecological systems theory. Bronfenbrenner developed the bioecological model after recognizing that the individual was overlooked in other theories of human development, which were largely focused on the context of development (e.g., the environment).\n\nThe bioecological model of human development can be applied to both children and maturing adults, and is thus a lifespan approach to development. The framework emphasizes the importance of understanding bidirectional influences between individuals’ development and their surrounding environmental contexts.In the bioecological model, in contrast to his earlier models, Bronfenbrenner also includes time (known as the chronosystem in his model) as an important component in the way that people and environments change. The bioecological model proposed a new method of conducting research which was heavily influential in developmental psychology and is still considered relevant today.\n\nBronfenbrenner named his original model \"Ecological Systems Theory\". Through this framework, Bronfenbrenner identified the need to understand individuals’ development within their environments. In order to conceptualize environmental contexts, Bronfenbrenner described four ecological systems:\n\n\nBronfenbrenner suggested that individuals constantly interact with these systems. He also stated that both individuals and their environments constantly affect one another. However, in this original model, Bronfenbrenner recognized there was not enough focus on individuals’ own role in their development, and thus began further developing this model.\n\nBronfenbrenner informally discussed new ideas concerning Ecological Systems Theory throughout the late 1970s and early 1980s during lectures and presentations to the psychological community. As he examined his original theory, he began to identify the role of other key factors in development. In 1986, Bronfenbrenner published his new theory and named it Bioecological Systems Theory.\n\nOne of Bronfenbrenner’s main influences was Lev Vygotsky, a Russian teacher and psychologist. Vygotsky created the social learning theory of development in the 1920s and 1930s to understand how people learn in social contexts and how social environments influence the learning process (1962). Vygotsky recognized that learning always occurs and cannot be separated from a social context and that this process is integral to a child's development.\n\nKurt Lewin, a German forerunner of ecological systems models and the founder of modern social psychology, pioneered the use of theory and experimentation to test hypotheses. He focused on the life space, which he defined as a person's psychological activities that occur within a kind of psychological field. The life space that contains all the events in the past, present, and future that shape and affect an individual. This focus on individuality led him to diagram the life space, containing arrows leading to and from possible life goals, both positive and negative. In sum, Lewin’s ecological systems model emphasized situational and proximal causes. Additionally, Lewin's theory demonstrates that behavior is a function of the current person by their environment, which are all affected by past experience.\n\nBronfenbrenner was also influenced by his colleague, Stephen J. Ceci, with whom he co-authored the article “Nature-nurture reconceptualized in developmental perspective: A bioecological theory” in 1994. Ceci is a developmental psychologist who redefined modern developmental psychology’s approach to intellectual development. He focused on predicting a pattern of associations among ecological, genetic, and cognitive variables as a function of proximal processes. Together, Bronfenbrenner and Ceci published the beginnings of the bioecological model and made it an accessible framework to use in understanding developmental processes.\n\nThe history of bioecological systems theory is divided into two periods. The first period resulted in the publication of Bronfenbrenner's theory of ecological systems theory, titled \"The Ecology of Human Development\", in 1979. Bronfenbrenner described the second period as a time of criticism and evaluation of his original work.\n\nThe development of ecological systems theory arose because Bronfenbrenner noted a lack of focus on the role of context in terms of development. He argued the environment in which children operate is important because development may be shaped by their interactions with the specific environment. He urged his colleagues to study development in terms of ecological contexts, that is the normal environments of children (schools, homes, daycares). Researchers heeded his advice and a great deal of research flourished in the early 1980s that focused on context.\n\nHowever, where prior research was ignoring context, Bronfenbrenner felt current research focused too much on context and ignored development. In his justification for a new theory, Bronfenbrenner wrote he was not pleased with the direction of research in the mid 1980s and that he felt there were other realms of development that were overlooked.\n\nIn comparison to the original theory, bioecological systems theory adds more emphasis to the person in the context of development. Additionally, Bronfenbrenner chose to leave out key features of the ecological systems theory (e.g., ecological validity and ecological experiments) during his development of bioecological systems theory. As a whole, Bronfenbrenner’s new theory continued to go through a series of transformations as he continuously analyzed different factors in human development. Critical components of bioecological systems theory did not emerge all at once. Instead, his ideas evolved and adapted to the research and ideas of the times. For example, the role of proximal processes, which is now recognized as a key feature of bioecological systems theory, did not emerge until the 1990s. This theory went through a series of transformations and elaborations until 2005 when Bronfenbrenner died.\n\nBronfenbrenner further developed the model by adding the chronosystem, which refers to how the person and environments change over time. He also placed a greater emphasis on processes and the role of the biological person. The Process–Person–Context–Time Model (PPCT) has since become the bedrock of the bioecological model. PPCT includes four concepts. The interactions between the concepts form the basis for the theory.\n\nThus, the bioecological model highlights the importance of understanding a person’s development within environmental systems. It further explains that both the person and the environment affect one another bidirectionally. Although even Bronfenbrenner himself critiqued the falsifiability of the model, the bioecological model has real world applications for developmental research, practice, and policies (as demonstrated below).\n\nIn addition to adding to the theoretical understanding of human development, the bioecological model lends itself to changes in the conceptualization of the research endeavor. In some of his earliest comments on the state of developmental research, Bronfenbrenner lamented that developmental research concerned itself with studying “strange behavior of children in strange situations for the briefest possible period of time”. He proposed, rather, that developmental science should take as its goal a study of children in context in order to best determine which processes are naturally “developmentally generative” (promote development) and which are naturally “developmentally disruptive” (prevent development).\n\nBronfenbrenner set up a contrast to the traditional “confirmatory” approach to hypothesis testing (in which research is done to “confirm” that a hypothesis is correct or incorrect) when specifying the types of research needed to support the bioecological model of development. In Bronfenbrenner’s view, the dynamic nature of the model calls for “primarily generative” research designs that explore interactions between proximal processes (see Proposition 1) and the developing person, environment, time, and developmental outcome (Proposition 2). Bronfenbrenner called this type of research the “discovery mode” of developmental science.\n\nTo best capture such dynamic processes, developmental research designs would ideally be longitudinal (over time), rather than cross-sectional (a single point in time), and conducted in children’s natural environments, rather than a laboratory. Such designs would thus occur in schools, homes, day-care centers, and other environments in which proximal processes are most likely to occur. The bioecological model also proposes that the most scientifically rich studies would include more than one distinct but theoretically related proximal process in the same design. Indeed, studies that claim to be based upon bioecological theory should include elements of process, person, context, and time, and should include explicit explanation and acknowledgement if one of the elements is lacking. Based on the interactions of proposed elements of the PPCT model, appropriate statistical analyses of PPCT data would likely include explorations of mediation and moderation effects, as well as multilevel modeling of data to account for the nesting of different components of the model. Moreover, research that includes both genetic and environmental components would capture even more of the bioecological model’s elements.\n\nIn today’s interconnected world, communication and electronic devices have a huge effect on the micro-cellular structure and complex bio-chemistry of our brains which eventually affects our personality, behavior and characteristics. A research conducted by Harvard medical school, highlighted the power of human imagination and the structural changes it causes to the human brain at the micro-cellular level thus bringing about changes in character personality or behaviour. The scientist argues that if imagination could bring about such permanent changes in the brain, the effects of digital technology on a human brain can produce behavioural changes such as shorter attention span, reduction in communication skills and abstract thinking. However, there is enough research to prove the positive effects of ‘brain-training’ computer games aimed at keeping the brain cells active. (Greenfield, 2014, #13)\n\nAccording to technology writer Nicholas Carr, technology has always determined the development of the brain and the way we think throughout History. As he illustrates with examples of reading and the rise in the use of internet. He observes that development of reading habits motivated our brains to be concentrate on the text and imagine, whereas the over exposure of internet reinforce our capability to scan and filter information productively and easily. (Taylor, 2012, #14)\nAttention is the key requisite which paves the way to all aspects of learning viz. perception, memory, language, creativity, reasoning, problem solving and decision making. Attention has been considered to be a highly malleable quality which can be influenced by the environment. Research shows that the invention of digital entertainment changed the attention span by presenting children with visual stimuli and very little need of imagination. In such an environment distraction becomes the norm and memory, focusing gets affected. This is in sharp contradiction to the children of the past who spent a great amount of their time reading, which requires deep and consistent attention, imagination, memory and which gave way to minimal distractions. (Taylor, 2012, #14)\n\nHowever studies have shown that a visual media improves visual-spatial capabilities (Green, Bavelier 2010, #15) and capacity to identify relevant information in text filled with hyperlinks and ads. Thus technology maybe making children viewing information differently compared to the children of yesteryear. In short children are more skilled at remembering where to find information rather than remembering information itself. Thus the brain may be allowed to be involved in higher order activities such as contemplation, critical thinking and problem solving. (Taylor, 2012, #14)\n\nFeel / ability to empathize \nAccording to a study on two groups of sixth-graders, kids who had no access to electronic devices for five days picked up on emotions and non-verbal cues better than those who used their devices during that time. (De Loatch, 2015, #16)\n\nEcological systems theory (Bronfenbrenner, 1979) emerged before the advent of Internet revolution and the developmental influence of then available technology (e.g., television) was conceptually situated in the child’s microsystem. Johnson and Puplampu (2008) proposed the ecological techno-subsystem, a dimension of the microsystem. Ecological techno-subsystem, a refinement to Bronfenbrenner’s theoretical organization of environmental influences on child development.\n\nThe word ecological techno-subsystem comprises both child interaction with living (e.g., peers, parents, teachers) and non-living (e.g., hardware, gadgets) elements of communication, information, and recreation technologies in immediate or direct environments.\n\nAn experimental study of 128 children from first to sixth grade was conducted to assess their cognitive development, based on their use of internet at home and socioeconomic characteristics of the family.\n\nMeasures and Results \nThree constructs, corresponding to three ecological systems/subsystems, were measured: child cognitive development (bioecology), indices of child use of the Internet at home (techno-subsystem), and family socioeconomic characteristics (microsystem).\n\nFor the chosen sample of children, both father’s education and mother’s employment were associated with measures of child cognitive development. Educated fathers tended to have children who scored high on three of the four cognitive measures (expressive language, visual perception, and auditory memory) as they may provide enhanced language models and stimulating learning environments to their children. Employed mothers tended to have children who had high scores on the measure of metacognitive planning as they could train their children to develop internet usage skills and effective management of information .\n\nThe indices of use of internet at home accounted for more variance in their development than the indices of socioeconomic status. The ecological techno-subsystem improves our understanding of the influences of environment on child development by stressing the effect of digital technologies on cognitive growth during childhood.\n\nWhile Internet use during childhood has been associated with negative developmental outcomes, research increasingly suggests that the Internet provides children with more developmental advantages than disadvantages \"(Greenfield & Yan, 2006).\"\n\nThe Internet, although rich in graphic display, is primarily a text-based medium; “the more a child uses the Internet, the more he/she reads” \"(Jackson et al., 2007, p. 188)\". Exposure to computer during the preschool years increased school readiness later \"(Li and Atkins (2004).\"\n\nJackson and colleagues (2006) provided low income children with home-based Internet access and continuously recorded time online. Their findings indicated that children who used the Internet more had higher scores on standardized tests of reading achievement and higher grade point compared to those children who used the Internet less (p. 429).\n\nFrom a developmental perspective, Internet use stimulates cognitive processes involved in interpreting text and images \"(Johnson, 2006).\" Metacognitive processes such as planning, search strategies, and evaluation of information are exercised when navigating websites \"(Tarpley, 2001).\" \"DeBell and Chapman (2006)\" concluded that Internet use promotes cognitive development in children, “specifically in the area of visual intelligence, where certain computer activities -- particularly games -- may enhance the ability to monitor several visual stimuli at once, to read diagrams, recognize icons, and visualize spatial relationships” (p. 3).\n\nComprehensive theoretical description of the developmental effect of Internet use is required. The recently proposed ecological techno-subsystem \"(Johnson & Puplampu, 2008)\" provides a conceptual framework for understanding the effect of Internet use on child development.\n\n\n13. \"Greenfield, Susan (2014). Modern technology changing way brains work. Retrieved from http://www.dailymail.co.uk/sciencetech/article-565207/Modern-technology-changing-way-brains-work-says-neuroscientist.html\"\n\n14. \"Taylor, Jim (2012). How technology is changing the way\nchildren think and focus. Retrieved from https://www.psychologytoday.com/blog/the-power-prime/201212/how-technology-is-changing-the-way-children-think-and-focus.\"\n\n15. \n\n16. \"De Loatch (2015). Four Negative sides of Technology. Retrieved\n<nowiki>from http://www.edudemic.com/the-4-negative-side-effects-of-technology/</nowiki>\"\n", "id": "34112061", "title": "Bioecological model"}
{"url": "https://en.wikipedia.org/wiki?curid=18143331", "text": "Neurogenetics\n\nNeurogenetics studies the role of genetics in the development and function of the nervous system. It considers neural characteristics as phenotypes (i.e. manifestations, measurable or not, of the genetic make-up of an individual), and is mainly based on the observation that the nervous systems of individuals, even of those belonging to the same species, may not be identical. As the name implies, it draws aspects from both the studies of neuroscience and genetics, focusing in particular how the genetic code an organism carries affects its expressed traits. Mutations in this genetic sequence can have a wide range of effects on the quality of life of the individual. Neurological diseases, behavior and personality are all studied in the context of neurogenetics. The field of neurogenetics emerged in the mid to late 1900s with advances closely following advancements made in available technology. Currently, neurogenetics is the center of much research utilizing the cutting edge of research techniques.\n\nThe field of neurogenetics emerged from advances made in molecular biology, genetics and a desire to understand the link between genes, behavior, the brain, and neurological disorders and diseases. The field started to expand in the 1960s through the research of Seymour Benzer, considered by some to be the father of neurogenetics. His pioneering work with \"Drosophila\" helped to elucidate the link between circadian rhythms and genes, which led to further investigations into other behavior traits. He also started conducting research in neurodegeneration in fruit flies in an attempt to discover ways to suppress neurological diseases in humans. Many of the techniques he used and conclusions he drew would drive the field forward. \nEarly analysis relied on statistical interpretation through processes such as LOD (logarithm of odds) scores of pedigrees and other observational methods such as affected sib-pairs, which looks at phenotype and IBD (identity by descent) configuration. Many of the disorders studied early on including Alzheimer’s, Huntington's and amyotrophic lateral sclerosis (ALS) are still at the center of much research to this day. By the late 1980s new advances in genetics such as recombinant DNA technology and reverse genetics allowed for the broader use of DNA polymorphisms to test for linkage between DNA and gene defects. This process is referred to sometimes as linkage analysis. By the 1990s ever advancing technology had made genetic analysis more feasible and available. This decade saw a marked increase in identifying the specific role genes played in relation to neurological disorders. Advancements were made in but not limited to: Fragile X syndrome, Alzheimer’s, Parkinson’s, epilepsy and ALS.\n\nWhile the genetic basis of simple diseases and disorders has been accurately pinpointed, the genetics behind more complex, neurological disorders is still a source of ongoing research. New developments such as the genome wide association studies (GWAS) have brought vast new resources within grasp. With this new information genetic variability within the human population and possibly linked diseases can be more readily discerned. Neurodegenerative diseases are a more common subset of neurological disorders, with examples being Alzheimer's disease and Parkinson's disease. Currently no viable treatments exist that actually reverse the progression of neurodegenerative diseases; however, neurogenetics is emerging as one field that might yield a causative connection. The discovery of linkages could then lead to therapeutic drugs, which could reverse brain degeneration.\n\nOne of the most noticeable results of further research into neurogenetics is a greater knowledge of gene loci that show linkage to neurological diseases. The table below represents a sampling of specific gene locations identified to play a role in selected neurological diseases based on prevalence in the United States. \n\nLogarithm of odds (LOD) is a statistical technique used to estimate the probability of gene linkage between traits. LOD is often used in conjunction with pedigrees, maps of a family’s genetic make-up, in order to yield more accurate estimations. A key benefit of this technique is its ability to give reliable results in both large and small sample sizes, which is a marked advantage in laboratory research.\n\nQuantitative trait loci (QTL) mapping is another statistical method used to determine the chromosomal positions of a set of genes responsible for a given trait. By identifying specific genetic markers for the genes of interest in a recombinant inbred strain, the amount of interaction between these genes and their relation to the observed phenotype can be determined through complex statistical analysis. \nIn a neurogenetics laboratory, the phenotype of a model organisms is observed by assessing the morphology of their brain through thin slices. QTL mapping can also be carried out in humans, though brain morphologies are examined using nuclear magnetic resonance imaging (MRI) rather than brain slices. Human beings pose a greater challenge for QTL analysis because the genetic population cannot be as carefully controlled as that of an inbred recombinant population, which can result in sources of statistical error.\n\nRecombinant DNA is an important method of research in many fields, including neurogenetics. It is used to make alterations to an organism’s genome, usually causing it to over- or under-express a certain gene of interest, or express a mutated form of it. The results of these experiments can provide information on that gene’s role in the organism’s body, and it importance in survival and fitness. The hosts are then screened with the aid of a toxic drug that the selectable marker is resistant to. The use of recombinant DNA is an example of a reverse genetics, where researchers create a mutant genotype and analyze the resulting phenotype. In forward genetics, an organism with a particular phenotype is identified first, and its genotype is then analyzed.\n\nModel organisms are an important tool in many areas of research, including the field of neurogenetics. By studying creatures with simpler nervous systems and with smaller genomes, scientists can better understand their biological processes and apply them to more complex organisms, such as humans. Due to their low-maintenance and highly mapped genomes, mice, \"Drosophila\", and \"C. elegans\" are very common. Zebrafish and prairie voles have also become more common, especially in the social and behavioral scopes of neurogenetics.\n\nIn addition to examining how genetic mutations affect the actual structure of the brain, researchers in neurogenetics also examine how these mutations affect cognition and behavior. One method of examining this involves purposely engineering model organisms with mutations of certain genes of interest. These animals are then classically conditioned to perform certain types of tasks, such as pulling a lever in order to gain a reward. The speed of their learning, the retention of the learned behavior, and other factors are then compared to the results of healthy organisms to determine what kind of an effect – if any – the mutation has had on these higher processes. The results of this research can help identify genes that may be associated with conditions involving cognitive and learning deficiencies.\n\nMany research facilities seek out volunteers with certain conditions or illnesses to participate in studies. Model organisms, while important, cannot completely model the complexity of the human body, making volunteers a key part to the progression of research. Along with gathering some basic information about medical history and the extent of their symptoms, samples are taken from the participants, including blood, cerebrospinal fluid, and/or muscle tissue. These tissue samples are then genetically sequenced, and the genomes are added to current database collections. The growth of these data bases will eventually allow researchers to better understand the genetic nuances of these conditions and bring therapy treatments closer to reality. Current areas of interest in this field have a wide range, spanning anywhere from the maintenance of circadian rhythms, the progression of neurodegenerative disorders, the persistence of periodic disorders, and the effects of mitochondrial decay on metabolism.\n\nAdvances in molecular biology techniques and the species-wide genome project have made it possible to map out an individual's entire genome. Whether genetic or environmental factors are primarily responsible for an individual's personality has long been a topic of debate. Thanks to the advances being made in the field of neurogenetics, researchers have begun to tackle this question by beginning to map out genes and correlate them to different personality traits. There is little to no evidence to suggest that the presence of a \"single\" gene indicates that an individual will express one style of behavior over another; rather, having a specific gene could make one more predisposed to displaying this type of behavior. It is starting to become clear that most genetically influenced behaviors are due to the effects of many variants within \"many\" genes, in addition to other neurological regulating factors like neurotransmitter levels. Due to fact that many behavioral characteristics have been conserved across species for generations, researchers are able to use animal subjects such as mice and rats, but also fruit flies, worms, and zebrafish, to try to determine specific genes that correlate to behavior and attempt to match these with human genes.\n\nWhile it is true that variation between species can appear to be pronounced, at their most basic they share many similar behavior traits which are necessary for survival. Such traits include mating, aggression, foraging, social behavior and sleep patterns. This conservation of behavior across species has led biologists to hypothesize that these traits could possibly have similar, if not the same, genetic causes and pathways. Studies conducted on the genomes of a plethora of organisms have revealed that many organisms have homologous genes, meaning that some genetic material has been conserved between species. If these organisms shared a common evolutionary ancestor, then this might imply that aspects of behavior can be inherited from previous generations, lending support to the genetic causes – as opposed to the environmental causes – of behavior. Variations in personalities and behavioral traits seen amongst individuals of the same species could be explained by differing levels of expression of these genes and their corresponding proteins.\n\nThere is also research being conducted on how an individual's genes can cause varying levels of aggression and aggression control . Throughout the animal kingdom, varying styles, types and levels of aggression can be observed leading scientists to believe that there might be a genetic contribution that has conserved this particular behavioral trait. For some species varying levels of aggression have indeed exhibited direct correlation to a higher level of Darwinian fitness.\n\n A great deal of research has been done on the effects of genes and the formation of the brain and the central nervous system. The following wiki links may prove helpful:\n\n\nThere are many genes and proteins that contribute to the formation and development of the CNS, many of which can be found in the aforementioned links. Of particular importance are those that code for BMPs, BMP inhibitors and \"SHH\". When expressed during early development, BMP's are responsible for the differentiation of epidermal cells from the ventral ectoderm. Inhibitors of BMPs, such as \"NOG\" and \"CHRD\", promote differentiation of ectoderm cells into prospective neural tissue on the dorsal side. If any of these genes are improperly regulated, then proper formation and differentiation will not occur. \nBMP also plays a very important role in the patterning that occurs after the formation of the neural tube. Due to the graded response the cells of the neural tube have to BMP and Shh signaling, these pathways are in competition to determine the fate of preneural cells. BMP promotes dorsal differentiation of pre-neural cells into sensory neurons and Shh promotes ventral differentiation into motor neurons. There are many other genes that help to determine neural fate and proper development include, \"RELN\", \"SOX9\", \"WNT\", Notch and Delta coding genes, \"HOX\", and various cadherin coding genes like \"CDH1\" and \"CDH2\".\n\nSome recent research has shown that the level of gene expression changes drastically in the brain at different periods throughout the life cycle. For example, during prenatal development the amount of mRNA in the brain (an indicator of gene expression) is exceptionally high, and drops to a significantly lower level not long after birth. The only other point of the life cycle during which expression is this high is during the mid- to late-life period, during 50–70 years of age. While the increased expression during the prenatal period can be explained by the rapid growth and formation of the brain tissue, the reason behind the surge of late-life expression remains a topic of ongoing research.\n\nNeurogenetics is a field that is rapidly expanding and growing. The current areas of research are very diverse in their focuses. One area deals with molecular processes and the function of certain proteins, often in conjunction with cell signaling and neurotransmitter release, cell development and repair, or neuronal plasticity. Behavioral and cognitive areas of research continue to expand in an effort to pinpoint contributing genetic factors. As a result of the expanding neurogenetics field a better understanding of specific neurological disorders and phenotypes has arisen with direct correlation to genetic mutations. With severe disorders such as epilepsy, brain malformations, or mental retardation a single gene or causative condition has been identified 60% of the time; however, the milder the intellectual handicap the lower chance a specific genetic cause has been pinpointed. Autism for example is only linked to a specific, mutated gene about 15–20% of the time while the mildest forms of mental handicaps are only being accounted for genetically less than 5% of the time. Research in neurogenetics has yielded some promising results, though, in that mutations at specific gene loci have been linked to harmful phenotypes and their resulting disorders. For instance a frameshift mutation or a missense mutation at the \"DCX\" gene location causes a neuronal migration defect also known as lissencephaly. Another example is the \"ROBO3\" gene where a mutation alters axon length negatively impacting neuronal connections. Horizontal gaze palsy with progressive scoliosis (HGPPS) accompanies a mutation here. These are just a few examples of what current research in the field of neurogenetics has achieved.\n\n", "id": "18143331", "title": "Neurogenetics"}
{"url": "https://en.wikipedia.org/wiki?curid=33712524", "text": "Ecophenotypic variation\n\nEcophenotypic variation (\"ecophenotype\") refers to phenotypical variation as a function of life station. In wide-ranging species, the contributions of heredity and environment are not always certain, but their interplay can sometimes be determined by experiment.\n\nPlants display the most obvious examples of ecophenotypic variation. One example are trees growing in the woods developing long straight trunks, with branching crowns high in the canopy, while the same species growing alone in the open develops a spreading form, branching much lower to the ground. Genotypes often have much flexibility in the modification and expression of phenotypes; in many organisms these phenotypes are very different under varying environmental conditions. The plant \"Hieracium umbellatum\" is found growing in two different habitats in Sweden. One habitat is rocky sea-side cliffs, where the plants are bushy with broad leaves and expanded inflorescences; the other is among sand dunes where the plants grow prostrate with narrow leaves and compact inflorescences. These habitats alternate along the coast of Sweden and the habitat that the seeds of \"H. umbellatum\" land in determines the phenotype that grows. Invasive plants such as the honeysuckle can thrive by altering their morphology in response to changes in the environment, which gives them a competitive advantage. Another example of a plants phenotypic reaction and adaptation with its environment is how Thlaspi caerulescens can absorb the metals in the soil to use to its advantage in defending against harmful microbes and bacteria in its leaves. The more immediate responses shown by vascular plants to their environment, for instance a vine's ability to conform to the wall or tree upon which it grows, are not usually considered ecophenotypic, even though the mechanisms may be related.\n\nSince animals are far less plastic than plants, ecophenotypic variation is noteworthy. When encountered, it can cause confusion in identification if it is not anticipated. The most obvious examples are again common observations, as the dwarfing of aquarium fish living in a restricted environment. In asexual reproduction, the parent passes on the entire genome to the next generation. Mutations to the genes are the only source of genetic variation. In sexual reproduction, each parent contributes half of his or her genome to the offspring; thus the offspring contain a mixture of genetic material. Adaptations are traits that increase fitness, the driving force for natural selection. The level of fitness associated with an allele can only be ascertained by comparison with alternate alleles. Traits that increase the survival rate of a species contribute to an animal's fitness, but selection will only favor such traits insofar as survival improves the reproductive success of the organism. More interesting are examples where causation is less clear. Among mollusks, examples include the muricid snail species \"Nucella lamellosa\", which in rough, shallow waters is generally less spiny than in deeper, quiet waters. In unionid freshwater bivalves, there are lake, small river, and large river forms of several species. In vertebrates, experiments on mice show reduced length of ears and tails in response to being reared in a lower temperature, a phenomenon known as Allen's rule.\n\nIn humans, environmental differences due to lifestyle choices are a consideration, for instance the differences between someone who spends much time on the sofa before the television, beer in hand, and an individual who spends his time in the gym or the soccer field can be pronounced. Franz Boas found that cephalic index was to some degree dependent on where a child was born, independent of the child's genetic or cultural heritage.\nAnother way in which environmental differences can cause physical and/or behavioral alterations is in being put under great levels of stress, causing a wide range of effects. Chronic Stress has been proven to cause health issues in many individuals. \"Early childhood attempts to cope with fear or rejection ... set up psychological patterns of behavior for the person's later life. Those behaviors in turn affect the biochemical imbalances in the brain's neuronal systems. Those altered imbalances in turn reinforce the behaviors, and the cycle feeds upon itself.\" \"The body reacts biochemically to excessive stress as it attempts to regain its healthy dynamic balance\" ; \"In psychologically stressful situations, hormones may be brought into play to remedy the imbalance the body finds itself in.\"\n\nIn the General Adaption Syndrome, which is the biological response to stress, there are three stages.\n1.) The \"Alarm Action\" - heart rate increases, blood sugar levels rise, pupils dilate, and digestion slows.\n2.) The \"Resistance\" or \"Adaptive\" stage - The body attempts to repair the damage which caused the emergency arousal\n3.) The \"Exhaustion Stage\" - The body grows ill; Mentally, possibly by neurosis or even psychotic disturbances, or physically, having the possibility to trigger several kinds of cardiovascular and kidney diseases, and Quite commonly, certain forms of asthma.\n", "id": "33712524", "title": "Ecophenotypic variation"}
{"url": "https://en.wikipedia.org/wiki?curid=481020", "text": "Nondisjunction\n\nNondisjunction is the failure of homologous chromosomes or sister chromatids to separate properly during cell division. There are three forms of nondisjunction: failure of a pair of homologous chromosomes to separate in meiosis I, failure of sister chromatids to separate during meiosis II, and failure of sister chromatids to separate during mitosis. Nondisjunction results in daughter cells with abnormal chromosome numbers (aneuploidy).\n\nCalvin Bridges and Thomas Hunt Morgan are credited with discovering nondisjunction in \"Drosophila melanogaster\" sex chromosomes in the spring of 1910, while working in the Zoological Laboratory of Columbia University.\n\nIn general, nondisjunction can occur in any form of cell division that involves ordered distribution of chromosomal material. Higher animals have three distinct forms of such cell divisions: Meiosis I and meiosis II are specialized forms of cell division occurring during generation of gametes (eggs and sperm) for sexual reproduction, mitosis is the form of cell division used by all other cells of the body.\n\nOvulated eggs become arrested in metaphase II until fertilization triggers the second meiotic division. Similar to the segregation events of mitosis, the pairs of sister chromatids resulting from the separation of bivalents in meiosis I are further separated in anaphase of meiosis II. In oocytes, one sister chromatid is segregated into the second polar body, while the other stays inside the egg. During spermatogenesis, each meiotic division is symmetric such that each primary spermatocyte gives rise to 2 secondary spermatocytes after meiosis I, and eventually 4 spermatids after meiosis II. Meiosis II-nondisjunction may also result in aneuploidy syndromes, but only to a much smaller extent than do segregation failures in meiosis I.\n\nDivision of somatic cells through mitosis is preceded by replication of the genetic material in S phase. As a result, each chromosome consists of two sister chromatids held together at the centromere. In the anaphase of mitosis, sister chromatids separate and migrate to opposite cell poles before the cell divides. Nondisjunction during mitosis leads to one daughter receiving both sister chromatids of the affected chromosome while the other gets none. This is known as a chromatin bridge or an anaphase bridge. \nMitotic nondisjunction results in somatic mosaicism, since only daughter cells originating from the cell where the nondisjunction event has occurred will have an abnormal number of chromosomes. Nondisjunction during mitosis can contribute to the development of some forms of cancer, e.g. retinoblastoma (see below).\nChromosome nondisjunction in mitosis can be atrributed to the inactivation of topoisomerase II, condensin, or separase.\nMeiotic nondisjunction has been well studied in \"Saccharomyces cerevisiae\". This yeast undergoes mitosis similarly to other eukaryotes. Chromosome bridges occur when sister chromatids are held together post replication by DNA-DNA topological entanglement and the cohesion complex. During anaphase, cohesion is cleaved by separase. \nTopoisomerase II and condensin are responsible for removing catenations.\n\nThe spindle assembly checkpoint (SAC) is a molecular safe-guarding mechanism that governs proper chromosome segregation in eukaryotic cells.\nSAC inhibits progression into anaphase until all homologous chromosomes (bivalents, or tetrads) are properly aligned to the spindle apparatus. Only then, SAC releases its inhibition of the anaphase promoting complex (APC), which in turn irreversibly triggers progression through anaphase.\n\nSurveys of cases of human aneuploidy syndromes have shown that most of them are maternally derived. This raises the question: Why is female meiosis more error prone? The most obvious difference between female oogenesis and male spermatogenesis is the prolonged arrest of oocytes in late stages of prophase I for many years up to several decades. Male gametes on the other hand quickly go through all stages of meiosis I and II. \nAnother important difference between male and female meiosis concerns the frequency of recombination between homologous chromosomes: In the male, almost all chromosome pairs are joined by at least one crossover, while more than 10% of human oocytes contain at least one bivalent without any crossover event. Failures of recombination or inappropriately located crossovers have been well documented as contributors to the occurrence of nondisjunction in humans.\n\nDue to the prolonged arrest of human oocytes, weakening of cohesive ties holding together chromosomes and reduced activity of the SAC may contribute to maternal age-related errors in segregation control.\nThe cohesin complex is responsible for keeping together sister chromatids and provides binding sites for spindle attachment. Cohesin is loaded onto newly replicated chromosomes in oogonia during fetal development. Mature oocytes have only limited capacity for reloading cohesin after completion of S phase. The prolonged arrest of human oocytes prior to completion of meiosis I may therefore result in considerable loss of cohesin over time. Loss of cohesin is assumed to contribute to incorrect microtubule-kinetochore attachment and chromosome segregation errors during meiotic divisions.\n\nThe result of this error is a cell with an imbalance of chromosomes. Such a cell is said to be aneuploid. Loss of a single chromosome (2n-1), in which the daughter cell(s) with the defect will have one chromosome missing from one of its pairs, is referred to as a monosomy. Gaining a single chromosome, in which the daughter cell(s) with the defect will have one chromosome in addition to its pairs is referred to as a trisomy. In the event that an aneuploidic gamete is fertilized, a number of syndromes might result.\n\nThe only known survivable monosomy in humans is Turner syndrome, where the affected individual is monosomic for the X chromosome (see below). Other monosomies are usually lethal during early fetal development, and survival is only possible if not all the cells of the body are affected in case of a mosaicism (see below), or if the normal number of chromosomes is restored via duplication of the single monosomic chromosome (\"chromosome rescue\").\n\nComplete loss of an entire X chromosome accounts for about half the cases of Turner syndrome. The importance of both X chromosomes during embryonic development is underscored by the observation that the overwhelming majority (>99%) of fetuses with only one X chromosome (karyotype 45, X0) are spontaneously aborted.\n\nThe term autosomal trisomy means that a chromosome other than the sex chromosomes X and Y is present in 3 copies instead of the normal number of 2 in diploid cells.\n\nDown syndrome, a trisomy of chromosome 21, is the most common anomaly of chromosome number in humans. The majority of cases results from nondisjunction during maternal meiosis I. Trisomy occurs in at least 0.3% of newborns and in nearly 25% of spontaneous abortions. It is the leading cause of pregnancy wastage and is the most common known cause of mental retardation. It is well documented that advanced maternal age is associated with greater risk of meiotic nondisjunction leading to Down syndrome. This may be associated with the prolonged meiotic arrest of human oocytes potentially lasting for more than four decades.\n\nHuman trisomies compatible with live birth, other than Down syndrome (trisomy 21), are Edwards syndrome (trisomy 18) and Patau syndrome (trisomy 13). Complete trisomies of other chromosomes are usually not viable and represent a relatively frequent cause of miscarriage. Only in rare cases of a mosaicism, the presence of a normal cell line, in addition to the trisomic cell line, may support the development of a viable trisomy of the other chromosomes.\n\nThe term \"sex chromosome aneuploidy\" summarizes conditions with an abnormal number of sex chromosomes, i.e. other than XX (female) or XY (male). Formally, X chromosome monosomy (Turner syndrome, see above) can also be classified as a form of sex chromosome aneuploidy.\n\nKlinefelter syndrome is the most common sex chromosome aneuploidy in humans. It represents the most frequent cause of hypogonadism and infertility in men. Most cases are caused by nondisjunction errors in paternal meiosis I. \nAbout eighty percent of individuals with this syndrome have one extra X chromosome resulting in the karyotype XXY. The remaining cases have either multiple additional sex chromosomes (48,XXXY; 48,XXYY; 49,XXXXY), mosaicism (46,XY/47,XXY), or structural chromosome abnormalities.\n\nThe incidence of XYY syndrome is approximately 1 in 800-1000 male births. Many cases remain undiagnosed because of their normal appearance and fertility, and the absence of severe symptoms. The extra Y chromosome is usually a result of nondisjunction during paternal meiosis II.\n\nTrisomy X is a form of sex chromosome aneuploidy where females have three instead of two X chromosomes. Most patients are only mildly affected by neuropsychological and physical symptoms. Studies examining the origin of the extra X chromosome observed that about 58-63% of cases were caused by nondisjunction in maternal meiosis I, 16-18% by nondisjunction in maternal meiosis II, and the remaining cases by post-zygotic, i.e. mitotic, nondisjunction.\n\nUniparental disomy denotes the situation where both chromosomes of a chromosome pair are inherited from the same parent and are therefore identical. This phenomenon most likely is the result of a pregnancy that started as a trisomy due to nondisjunction. Since most trisomies are lethal, the fetus only survives because it loses one of the three chromosomes and becomes disomic. Uniparental disomy of chromosome 15 is, for example, seen in some cases of Prader-Willi syndrome and Angelman syndrome.\n\nMosaicism syndromes can be caused by mitotic nondisjunction in early fetal development. As a consequence, the organism evolves as a mixture of cell lines with differing ploidy (number of chromosomes). Mosaicism may be present in some tissues, but not in others. Affected individuals may have a patchy or assymmetric appearance. Examples of mosaicism syndromes include Pallister-Killian syndrome and Hypomelanosis of Ito.\n\nDevelopment of cancer often involves multiple alterations of the cellular genome (Knudson hypothesis). Human retinoblastoma is a well studied example of a cancer type where mitotic nondisjunction can contribute to malignant transformation: Mutations of the RB1 gene, which is located on chromosome 13 and encodes the tumor suppressor retinoblastoma protein, can be detected by cytogenetic analysis in many cases of retinoblastoma. Mutations of the RB1 locus in one copy of chromosome 13 are sometimes accompanied by loss of the other wild-type chromosome 13 through mitotic nondisjunction. By this combination of lesions, affected cells completely lose expression of functioning tumor suppressor protein.\n\nPre-implantation genetic diagnosis (PGD or PIGD) is a technique used to identify genetically normal embryos and is useful for couples who have a family history of genetic disorders. This is an option for people choosing to procreate through IVF. PGD is considered difficult due to it being both time consuming and having success rates only comparable to routine IVF.\n\nKaryotyping involves performing an amniocentesis in order to study the cells of an unborn fetus during metophase 1. Light microscopy can be used to visually determine if aneuploidy is an issue.\n\nPolar body diagnosis (PBD) can be use to detect maternally derived chromosomal aneuploidies as well as translocations in oocytes. The advantage of PBD over PGD is that it can be accomplished in a short amount of time. This is accomplished through zona drilling or laser drilling.\n\nBlastomere biopsy is a technique in which blastomeres are removed from the zona pellucida. It is commonly used to detect aneuploidy. Genetic analysis is conducted once the procedure is complete. Additional studies are needed to assess the risk associated with the procedure.\n\nExposure of spermatozoa to lifestyle, environmental and/or occupational hazards may increase the risk of aneuploidy. Cigarette smoke is a known aneugen (aneuploidy inducing agent). It is associated with increases in aneuploidy ranging from 1.5 to 3.0-fold. Other studies indicate factors such as alcohol consumption, occupational exposure to benzene, and exposure to the insecticides fenvalerate and carbaryl also increase aneuploidy.\n", "id": "481020", "title": "Nondisjunction"}
{"url": "https://en.wikipedia.org/wiki?curid=34142465", "text": "Linkage based QTL mapping\n\nMendel’s experimental results on law of segregation and independent assortment ( presented in 1965 and published in 1866 in the Proceedings of the Brunn Society of Natural History) has led founding stone in genetical studies in plants and humans. The studies were much accelerated after 1900, when Mendel’s law was rediscovered by de Veries, Correns, and von Tschermak-Seysenegg. Morgan demonstrated phenomena of sex linkage in Drosphila in 1910. Subsequently, the idea of gene mapping was first proposed by a 19-year-old college student working in\n\nLinkage Mapping of QTLs involves using linkage behaviour between QTL or major genes with the markers of interest.\n\nThomas Hunt Morgan's laboratory in 1911. Based on Morgan's observation that the recombination fraction between two loci increases with the distance between them, Alfred Sturtevant neglected his homework one evening and constructed the first genetic map covering six loci in Drosophila (Sturtevant, 1913).\nThe statistical foundations of gene mapping were subsequently laid by Haldane, Hogben, Fisher, Penrose, Smith, and Morton (Morton, 1955). Genetic dissection of simple Mendalian and Mendalian-like traits has been greatly enhanced by numerous pioneering contribution of Newton E. Morton over the decades. The same methods are widely used today for investigating and sometime determining genetic basic of complex traits. The lod score method (Morton, 1955), which constitutes the basis of most of linkage studies, has been recognized as pivotal contribution.\nUnfortunately, linkage analysis in humans, animals and plants languished for more than a half century after Sturtevant's discovery because of the lack of genetic markers and of adequate computing power. By the early 1970s, computing resources were sufficiently widely distributed to stimulate algorithmic breakthroughs in likelihood evaluation on human pedigree data (Elston and Stewart, 1971; Lange and Elston, 1975; Ott, 1976; Cannings et al., 1978). Armed with Ott's computer program LIPED, geneticists were well poised to embark on human linkage studies. \nInitially the mapping work was limited to some of morphological markers and the speed was quite low. Mapping efforts are speeded by development of DNA markers that provided virtually unlimited supply of genetic markers – an idea first conceived by Botstein and colleagues for yeast crosses (Petes and Botstein, 1977) and subsequently for human families (Botstein et al., 1980). The idea of using markers to map in such families is to trace inheritance of existing human pedigrees as if one had set up crosses in the laboratory. After a decade of first discovery of molecular markers, the very first genome map in plants was reported in maize and tomato (Helentjaris et al., 1986) using RFLP marker in a F2 population. The familial linkage methods were very popular for qualitatively inherited molecular markers and traits of interest in humans, animals and plants. Later on the focus was turned to more complex traits – quantitatively inherited traits with high environmental influence. Such complex traits are affected by both environment and genes.\n\nStudy designs for mapping: Experimental Population vs Natural Population\n\nNatural populations include collection of individuals from a natural habitat connected with or without having pedigree. Pedigree data, if available are unmanipulated i.e. not crossed in experimenter’s interest. Pedigree can not be manipulated in human and some of animals and leaves no options of using natural populations including families whatever available in nature (for review: mapping in natural populations by Slate, 2005). QTL analysis in such population is challenging because the number of alleles segregating at the QTL is unknown, the marker phases may be unknown or only partially known, the marker and QTL allele frequencies must be estimated from the data, inbreeding loops may exist in the pedigree, and markers may be noninformative or ungenotyped. Although it is possible to simplify the analysis of complex pedigree data by fragmenting the pedigree into smaller component families, methods that fully account for complex relationships between individuals are expected to provide greater power to detect QTL (Almasy and Blangero 1998). General natural pedigree are analyzed using a mixed effects model in which components of variance (e.g. additive genetic variance, maternal effects, environmental variance) can be estimated from a pedigreed population of individuals of any structure (Slate, 2005. Variance components are usually estimated by restricted maximum-likelihood (REML). Natural population can be further classified into mapping populations consisting of related individuals (family pedigrees) or unrelated populations.\n\nExperimental crosses\n\nThe traditional experimental crosses are based on single experimental cross, such as the backcross, F2 or full-sib family, initiated with two different lines. The principle behind genetic mapping that uses an experimental cross is the occurrence of recombination events between genetic loci (measured by the recombination fraction) when gametes are formed and transmitted from parents to offspring. By estimating the recombination fraction between markers and QTLs, the genomic location of the QTL of traits can be determined. QTL mapping in such experimental crosses in fixed effects modeling.\n\nLinkage vs Association\n\nLinkage and association analysis are primary tool for gene discovery, localization and functional analysis. While conceptual underpinning of these approaches have been long known, advances in recent decades in molecular genetics, development in efficient algorithms, and computing power have enabled the large scale application of these methods. While linkage studies seek to identify loci that cosegregate with the trait within families, association studies seek to identify particular variants that are associated with the phenotype at the population level. These are complementary methods that, together, provide means to probe the genome and describe etiology of complex human traits. In linkage studies, we seek to identify the loci that cosegregate with a specific genomic region, tagged by polymorphic markers, within families. In contrast, in association studies, we seek a correlation between a specific genetic variation and trait variation in sample of individuals, implicating a causal role of the variant. Linkage tests are powerful and specific for gene discovery, the localization of locus can be achieved only to a certain level of precision – on order of megabases – that potentially represents a region that potentially include hundreds of genes.\n\n\nQTL mapping\nFamily based QTL mapping\n\nGenetic linkage\n", "id": "34142465", "title": "Linkage based QTL mapping"}
{"url": "https://en.wikipedia.org/wiki?curid=8269077", "text": "Clastogen\n\nA clastogen is a mutagenic agent giving rise to or inducing disruption or breakages of chromosomes, leading to sections of the chromosome being deleted, added, or rearranged. This process is a form of mutagenesis, and can lead to carcinogenesis, as cells that are not killed by the clastogenic effect may become cancerous. Known clastogens include acridine yellow, benzene, ethylene oxide, arsenic, phosphine and mimosine. Exposure to clastogens increases frequency of abnormal germ cells in paternal males, contributing to developmental effects in the fetus upon fertilization.\n\nThe term \"clastogenic\" refers to volcanic eruptions which cause a particular type of ejecta.\n\n", "id": "8269077", "title": "Clastogen"}
{"url": "https://en.wikipedia.org/wiki?curid=34550223", "text": "Positional Sequencing\n\nPositional Sequencing is a method of sequencing DNA that simultaneously generates information about both identity and location of nucleotide sequences. The method involves detecting the location of sequence specific recognition events (e.g., such as hybridization of probes of known sequence) on single DNA molecules in each read, and generating maps of the location of such events. Multiple reads can be assembled into a consensus map that identifies the multiple locations of a specific sub-sequence. The assembly process is greatly facilitated by knowledge of the location of each sub-sequence, as well as the fact that individual reads produce non-contiguous sequence data over length scales that can be orders of magnitude greater than what can be achieved with Sanger sequencing or nextgen sequencing by synthesis.\n\nA collection of maps may be used to reconstruct single-base resolved sequence in a process analogous to sequence reconstruction in sequencing by hybridization. Ambiguities in the reconstruction of sequences are resolved through the knowledge of the relative position of overlapping sequence specific recognition events. By varying the parameters (e.g., length of read, density of recognition events, resolution of the detector) governing a specific implementation of the method, it is possible to query all size scales of DNA variation, from single nucleotide sequence all the way to large structural variants and chromosomal aneuploidies.\n\n", "id": "34550223", "title": "Positional Sequencing"}
{"url": "https://en.wikipedia.org/wiki?curid=34556489", "text": "Anticancer gene\n\nAnticancer genes are genes that, when ectopically overexpressed, specifically destroy tumour cells without harming normal cells. This cell destruction can be due to a variety of mechanisms, such as apoptosis, mitotic catastrophe followed by apoptosis or necrosis, and autophagy. Anticancer genes emerged from studies on cancer cells in the late 1990s.\n\nThe first anticancer gene to be isolated was Apoptin, a gene encoded by the chicken anaemia virus genome. Brevinin-2R is a short anti-microbial peptide of only 25 amino acids, a so-called non-hemolytic defensin, isolated from the skin of the frog species Rana ridibunda. The adenovirus E4orf4 is a viral protein with tumour-selective cell killing capabilities. HAMLET encodes the milk protein α-lactalbumin and is active against cancer cells only when complexed with oleic acid. Mda-7 (also known as IL-24) encodes a secreted cytokine and belongs to the IL-10 gene family. Noxa, is a BH3-only protein of the Bcl-2 family, has recently been discovered as a specific killer of breast cancer cells. Parvovirus-H1 NS1 is another viral protein carrying tumour-selective apoptosis capabilities. ORCTL3, a cation transporter, was recently discovered as a novel anti-cancer gene. Par-4 encodes a protein that features a leucine zipper and mediates many diverse signals for apoptosis at its endogenous expression level. TRAIL (TNF related apoptosis-inducing ligand) is a member of the TNF family of apoptosis-inducing ligands TP53 is another anti-cancer/anti-tumor gene (elephants have twenty copies of the TP53 gene).\n\nSome of these genes are in clinical development. TRAIL, Mda-7, HAMLET are the clinically most advanced anticancer genes.\n\n", "id": "34556489", "title": "Anticancer gene"}
{"url": "https://en.wikipedia.org/wiki?curid=34656547", "text": "Staggered extension process\n\nThe staggered extension process (also referred to as StEP) is a common technique used in biotechnology and molecular biology to create new, mutated genes with qualities of one or more initial genes.\n\nThe technique itself is a modified polymerase chain reaction with very short (approximately 10 seconds) cycles.\nIn these cycles the elongation of DNA is very quick (only a few hundred base pairs) and synthesized fragments anneal with complementary fragments of other strands. In this way, mutations of the initial genes are shuffled and in the end genes with new combinations of mutations are amplified.\n\nThe StEP protocol has been found to be useful as a method of directed evolution for the discovery of enzymes useful to industry.\n", "id": "34656547", "title": "Staggered extension process"}
{"url": "https://en.wikipedia.org/wiki?curid=34661539", "text": "Cytotaxonomy\n\nCytotaxonomy is the branch of biology dealing with the relationships and classification of organisms using comparative studies of chromosomes.\n\nCytotexonomy is branch of texonomy,which uses the characteristics of cellular structures,such as sometic chromosome to classify organism.imporovent in the cytological techniques permit chromosomal studies.The Study of primate chromosome is another very active field,which provide very much information on relationships(chiarelli,1996).\nThe number, structure, and behaviour of chromosomes is of great value in taxonomy, with chromosome number being the most widely used and quoted character. Chromosome numbers are usually determined at mitosis and quoted as the diploid number (2n), unless dealing with a polyploid series in which case the base number or number of chromosomes in the genome of the original haploid is quoted. Another useful taxonomic character is the position of the centromere. Meiotic behaviour may show the heterozygosity of inversions. This may be constant for a taxon, offering further taxonomic evidence. Cytological data is regarded as having more significance than other taxonomic evidence.\n", "id": "34661539", "title": "Cytotaxonomy"}
{"url": "https://en.wikipedia.org/wiki?curid=34853851", "text": "Missing heritability problem\n\nThe \"missing heritability\" problem can be defined as the fact that single genetic variations cannot account for much of the heritability of diseases, behaviors, and other phenotypes. This is a problem that has significant implications for medicine, since a person's susceptibility to disease may depend more on \"the combined effect of all the genes in the background than on the disease genes in the foreground\", or the role of genes may have been severely overestimated.\n\nThe 'missing heritability' problem was named as such in 2008 (after the \"missing baryon problem\" in physics). The Human Genome Project led to optimistic forecasts that the large genetic contributions to many traits and diseases (which were identified by quantitative genetics and behavioral genetics in particular) would soon be mapped and pinned down to specific genes and their genetic variants by methods such as candidate-gene studies which used small samples with limited genetic sequencing to focus on specific genes believed to be involved, examining the SNP kinds of variants. While many hits were found, they often failed to replicate in other studies.\n\nThe exponential fall in genome sequencing costs led to the use of GWAS studies which could simultaneously examine all candidate-genes in larger samples than the original finding, where the candidate-gene hits were found to almost always be false positives and only 2-6% replicate; in the specific case of intelligence candidate-gene hits, only 1 candidate-gene hit replicated, the top 25 schizophrenia candidate-genes were no more associated with schizophrenia than chance, and of 15 neuroimaging hits, none did. The editorial board of \"Behavior Genetics\" noted, in setting more stringent requirements for candidate-gene publications, that \"the literature on candidate gene associations is full of reports that have not stood up to rigorous replication...it now seems likely that many of the published findings of the last decade are wrong or misleading and have not contributed to real advances in knowledge\". Other researchers have characterized the literature as having \"yielded an infinitude of publications with very few consistent replications\" and called for a phase out of candidate-gene studies in favor of polygenic scores.\n\nThis led to a dilemma. Standard genetics methods have long estimated large heritabilities such as 80% for traits such as height or intelligence, yet none of the genes had been found despite sample sizes that, while small, should have been able to detect variants of reasonable effect size such as 1 inch or 5 IQ points. If genes have such strong cumulative effects - where were they? Several resolutions have been proposed, that the missing heritability is some combination of:\n\n", "id": "34853851", "title": "Missing heritability problem"}
{"url": "https://en.wikipedia.org/wiki?curid=5558617", "text": "BLOSUM\n\nIn bioinformatics, the BLOSUM (BLOcks SUbstitution Matrix) matrix is a substitution matrix used for sequence alignment of proteins. BLOSUM matrices are used to score alignments between evolutionarily divergent protein sequences. They are based on local alignments. BLOSUM matrices were first introduced in a paper by Steven Henikoff and Jorja Henikoff. They scanned the BLOCKS database for very conserved regions of protein families (that do not have gaps in the sequence alignment) and then counted the relative frequencies of amino acids and their substitution probabilities. Then, they calculated a log-odds score for each of the 210 possible substitution pairs of the 20 standard amino acids. All BLOSUM matrices are based on observed alignments; they are not extrapolated from comparisons of closely related proteins like the PAM Matrices.\n\nThe genetic instructions of every replicating cell in a living organism are contained within its DNA. Throughout the cell's lifetime, this information is transcribed and replicated by cellular mechanisms to produce proteins or to provide instructions for daughter cells during cell division, and the possibility exists that the DNA may be altered during these processes. This is known as a mutation. At the molecular level, there are regulatory systems that correct most — but not all — of these changes to the DNA before it is replicated.\n\nThe functionality of a protein is highly dependent on its structure. Changing a single amino acid in a protein may reduce its ability to carry out this function, or the mutation may even change the function that the protein carries out. Changes like these may severely impact a crucial function in a cell, potentially causing the cell — and in extreme cases, the organism — to die. Conversely, the change may allow the cell to continue functioning albeit differently, and the mutation can be passed on to the organism's offspring. If this change does not result in any significant physical disadvantage to the offspring, the possibility exists that this mutation will persist within the population. The possibility also exists that the change in function becomes advantageous.\n\nThe 20 amino acids translated by the genetic code vary greatly by the physical and chemical properties of their side chains. However, these amino acids can be categorised into groups with similar physicochemical properties. Substituting an amino acid with another from the same category is more likely to have a smaller impact on the structure and function of a protein than replacement with an amino acid from a different category.\n\nSequence alignment is a fundamental research method for modern biology. The most common sequence alignment for protein is to look for similarity between different sequences in order to infer function or establish evolutionary relationships. This helps researchers better understand the origin and function of genes through the nature of homology and conservation. Substitution matrices are utilized in algorithms to calculate the similarity of different sequences of proteins; however, the utility of Dayhoff Matrix has decreased over time due to the requirement of sequences with a similarity more than 85%. In order to fill in this gap, Henikoff and Henikoff introduced BLOSUM (BLOcks SUbstitution Matrix) matrix which led to marked improvements in alignments and in searches using queries from each of the groups of related proteins.\n\nBLOSUM: Blocks Substitution Matrix, a substitution matrix used for sequence alignment of proteins.\n\nScoring metrics (statistical versus biological): When evaluating a sequence alignment, one would like to know how meaningful it is. This requires a scoring matrix, or a table of values that describes the probability of a biologically meaningful amino-acid or nucleotide residue-pair occurring in an alignment. Scores for each position are obtained frequencies of substitutions in blocks of local alignments of protein sequences.\n\nSeveral sets of BLOSUM matrices exist using different alignment databases, named with numbers. BLOSUM matrices with high numbers are designed for comparing closely related sequences, while those with low numbers are designed for comparing distant related sequences. For example, BLOSUM80 is used for less divergent alignments, and BLOSUM45 is used for more divergent alignments. The matrices were created by merging (clustering) all sequences that were more similar than a given percentage into one single sequence and then comparing those sequences (that were all more divergent than the given percentage value) only; thus reducing the contribution of closely related sequences. The percentage used was appended to the name, giving BLOSUM80 for example where sequences that were more than 80% identical were clustered.\n\nBLOSUM r: the matrix built from blocks with less than r% of similarity\n– E.g., BLOSUM62 is the matrix built using sequences with less than 62% similarity (sequences with ≥ 62% identity were clustered)\n– Note: BLOSUM 62 is the default matrix for protein BLAST. Experimentation has shown that the BLOSUM-62 matrix is among the best for detecting most weak protein similarities.\n\nBLOSUM matrices are obtained by using blocks of similar amino acid sequences as data, then applying statistical methods to the data to obtain the similarity scores.\nStatistical Methods Steps : \nEliminate the sequences that are less than r% identical. There are two ways to eliminate the sequences. It can be done either by removing sequences from the block or just by finding similar sequences and replace them by new sequences which could represent the cluster. Elimination is done to remove protein sequences that are less similar than the specified threshold.\n\nA database storing the sequence alignments of the most conserved regions of protein families. These alignments are used to derive the BLOSUM matrices. Only the sequences with a percentage of identity higher are used.\nBy using the block, counting the pairs of amino acids in each column of the multiple alignment.\n\nIt gives the ratio of the occurrence each amino acid combination in the observed data to the expected value of occurrence of the pair.\nIt is rounded off and used in the substitution matrix.\n\nformula_1\n\nIn which formula_2 is the possibility of observed and formula_3 is the possibility of expected.\n\nThe odds for relatedness are calculated from log odd ratio, which are then rounded off to get the substitution matrices BLOSUM matrices.\n\nA scoring matrix or a table of values is required for evaluating the significance of a sequence alignment, such as describing the probability of a biologically meaningful amino-acid or nucleotide residue-pair occurring in an alignment. Typically, when two nucleotide sequences are being compared, all that is being scored is whether or not two bases are the same at one position. All matches and mismatches are respectively given the same score (typically +1 or +5 for matches, and -1 or -4 for mismatches). But it is different for proteins. Substitution matrices for amino acids are more complicated and implicitly take into account everything that might affect the frequency with which any amino acid is substituted for another. The objective is to provide a relatively heavy penalty for aligning two residues together if they have a low probability of being homologous (correctly aligned by evolutionary descent). Two major forces drive the amino-acid substitution rates away from uniformity: substitutions occur with the different frequencies, and lessen functionally tolerated than others. Thus, substitutions are selected against.\n\nCommonly used substitution matrices include the blocks substitution (BLOSUM) and point accepted mutation (PAM) matrices. Both are based on taking sets of high-confidence alignments of many homologous proteins and assessing the frequencies of all substitutions, but they are computed using different methods.\n\nScores within a BLOSUM are log-odds scores that measure, in an alignment, the logarithm for the ratio of the likelihood of two amino acids appearing with a biological sense and the likelihood of the same amino acids appearing by chance. The matrices are based on the minimum percentage identity of the aligned protein sequence used in calculating them. Every possible identity or substitution is assigned a score based on its observed frequencies in the alignment of related proteins. A positive score is given to the more likely substitutions while a negative score is given to the less likely substitutions.\n\nTo calculate a BLOSUM matrix, the following equation is used: \n\nHere, formula_5 is the probability of two amino acids formula_6 and formula_7 replacing each other in a homologous sequence, and formula_8 and formula_9 are the background probabilities of finding the amino acids formula_6 and formula_7 in any protein sequence. The factor formula_12 is a scaling factor, set such that the matrix contains easily computable integer values.\n\nBLOSUM62: midrange\n\nBLOSUM80: more related proteins\n\nBLOSUM45: distantly related proteins\n\nAn article in Nature Biotechnology revealed that the BLOSUM62 used for so many years as a standard is not exactly accurate according to the algorithm described by Henikoff and Henikoff. Surprisingly, the miscalculated BLOSUM62 improves search performance.\n\nThe BLOSUM62 matrix with the amino acids in the table grouped according to the chemistry of the side chain, as in (a). Each value in the matrix is calculated by dividing the frequency of occurrence of the amino acid pair in the BLOCKS database, clustered at the 62% level, divided by the probability that the same two amino acids might align by chance. The ratio is then converted to a logarithm and expressed as a log odds score, as for PAM. BLOSUM matrices are usually scaled in half-bit units. A score of zero indicates that the frequency with which a given two amino acids were found aligned in the database was as expected by chance, while a positive score indicates that the alignment was found more often than by chance, and negative score indicates that the alignment was found less often than by chance.\n\nBLOSUM scores was used to predict and understand the surface gene variants among hepatitis B virus carriers and T-cell epitopes.\n\nDNA sequences of HBsAg were obtained from 180 patients, in which 51 were chronic HBV carrier and 129 newly diagnosed patients, and compared with consensus sequences built with 168 HBV sequences imported from GenBank. Literature review and BLOSUM scores were used to define potentially altered antigenicity.\n\nA novel input representation has been developed consisting of a combination of sparse encoding, Blosum encoding, and input derived from hidden Markov models. this method predicts T-cell epitopes for the genome of hepatitis C virus and discuss possible applications of the prediction method to guide the process of rational vaccine design.\n\nBLOSUM matrices are also used as a scoring matrix when comparing DNA sequences or protein sequences to judge the quality of the alignment. This form of scoring system is utilized by a wide range of alignment software including BLAST.\n\nIn addition to BLOSUM matrices, a previously developed scoring matrix can be used. This is known as a PAM. The two result in the same scoring outcome, but use differing methodologies. BLOSUM looks directly at mutations in motifs of related sequences while PAM's extrapolate evolutionary information based on closely related sequences.\n\nSince both PAM and BLOSUM are different methods for showing the same scoring information, the two can be compared but due to the very different method of obtaining this score, a PAM100 does not equal a BLOSUM100.\n\n", "id": "5558617", "title": "BLOSUM"}
{"url": "https://en.wikipedia.org/wiki?curid=15603385", "text": "Y-DNA haplogroups by ethnic group\n\nThe various ethnolinguistic groups found in the Caucasus, Central Asia, Europe, the Middle East, North Africa and/or South Asia demonstrate differing rates of particular Y-DNA haplogroups.\n\nIn the table below, the first two columns identify ethnolinguistic groups. Subsequent columns represent the sample size (\"n\") of the study or studies cited, and the percentage of each haplogroup found in that particular sample.\n\n\n\n", "id": "15603385", "title": "Y-DNA haplogroups by ethnic group"}
{"url": "https://en.wikipedia.org/wiki?curid=4468576", "text": "Cre recombinase\n\nCre recombinase is a tyrosine recombinase enzyme derived from the P1 Bacteriophage. The enzyme uses a topoisomerase I like mechanism to carry out site specific recombination events. The enzyme (38kDa) is a member of the integrase family of site specific recombinase and it is known to catalyse the site specific recombination event between two DNA recognition sites (LoxP sites). This 34 base pair (bp) loxP recognition site consists of two 13 bp palindromic sequences which flank an 8bp spacer region. The products of Cre-mediated recombination at loxP sites are dependent upon the location and relative orientation of the loxP sites. Two separate DNA species both containing loxP sites can undergo fusion as the result of Cre mediated recombination. DNA sequences found between two loxP sites are said to be \"floxed\". In this case the products of Cre mediated recombination depends upon the orientation of the loxP sites. DNA found between two loxP sites oriented in the same direction will be excised as a circular loop of DNA whilst intervening DNA between two loxP sites that are opposingly orientated will be inverted. The enzyme requires no additional cofactors (such as ATP) or accessory proteins for its function.\n\nThe enzyme plays important roles in the life cycle of the P1 Bacteriophage such as cyclization of the linear genome and resolution of dimeric chromosomes that form after DNA replication.\n\nCre recombinase is a widely used tool in the field of molecular biology. The enzyme's unique and specific recombination system is exploited to manipulate genes and chromosomes in a huge range of research, such as gene knock out or knock in studies. The enzyme's ability to operate efficiently in a wide range of cellular environments (including mammals, plants, bacteria, and yeast) enables the Cre-Lox recombination system to be used in a vast number of organisms, making it a particularly useful tool in scientific research.\n\nStudies carried out in 1981 by Sternberg and Hamilton demonstrated that the bacteriophage 'P1' had a unique site specific recombination system. EcoRI fragments of the P1 Bacteriophage genome were generated and cloned into lambda vectors. A 6.5kb EcoRI fragment (Fragment 7) was found to permit efficient recombination events. The mechanism of these recombination events was known to be unique as they occurred in the absence of bacterial RecA and RecBCD proteins. The components of this recombination system were elucidated using deletion mutagenesis studies. These studies showed that a P1gene product and a recombination site were both required for efficient recombination events to occur. The P1 gene product was named Cre (Causes Recombination) and the recombination site was named loxP (locus of crossing (x) over, P1). The Cre protein was purified in 1983 and was found to be a 35,000 Da protein. No high energy cofactors such as ATP or accessory proteins are required for the recombinase activity of the purified protein. Early studies also demonstrated that Cre binds to non specific DNA sequences whilst having a 20 fold higher affinity for loxP sequences and results of early DNA footprinting studies also suggested that Cre molecules bind loxP sites as dimers.\n\nCre recombinase consists of 343 amino acids that form two distinct domains. The amino terminal domain encompasses residues 20–129 and this domain contains 5 alpha helical segments linked by a series of short loops. Helices A & E are involved in the formation of the recombinase tetramer with the C terminus region of helix E known to form contacts with the C terminal domain of adjacent subunits. Helices B & D form direct contacts with the major groove of the loxP DNA. These two helices are thought to make three direct contacts to DNA bases at the loxP site. \nThe carboxy terminal domain of the enzyme consists of amino acids 132–341 and it harbours the active site of the enzyme. The overall structure of this domain shares a great deal of structural resemblance to the catalytic domain of other enzymes of the same family such as λ Integrase and HP1 Integrase. This domain is predominantly helical in structure with 9 distinct helices (F−N). The terminal helix (N) protrudes from the main body of the carboxy domain and this helix is reputed to play a role in mediating interactions with other subunits. Crystal structures demonstrate that this terminal N helix buries its hydrophobic surface into an acceptor pocket of an adjacent Cre subunit.\n\nThe effect of the two-domain structure is to form a C-shaped clamp that grasps the DNA from opposite sides.\n\nThe active site of the Cre enzyme consists of the conserved catalytic triad residues Arg 173, His 289, Arg 292 as well as the conserved nucleophilic residues Tyr 324 and Trp 315. Unlike some recombinase enzymes such as Flp recombinase, Cre does not form a shared active site between separate subunits and all the residues that contribute to the active site are found on a single subunit. Consequently, when two Cre molecules bind at a single loxP site two active sites are present. Cre mediated recombination requires the formation of a synapse in which two Cre-LoxP complexes associate to form what is known as the synapse tetramer in which 4 distinct active sites are present. \nTyr 324 acts as a nucleophile to form a covalent 3’-phosphotyrosine linkage to the DNA substrate. The scissile phosphate (phosphate targeted for nucleophilic attack at the cleavage site) is coordinated by the side chains of the 3 amino acid residues of the catalytic triad (Arg 173, His 289 & Trp 315). The indole nitrogen of tryptophan 315 also forms a hydrogen bond to this scissile phosphate. (n.b A Histidine occupies this site in other tyrosine recombinase family members and performs the same function). This reaction cleaves the DNA and frees a 5’ hydroxyl group. This process occurs in the active site of two out of the four recombinase subunits present at the synapse tetramer. If the 5’ hydroxyl groups attack the 3’-phosphotyrosine linkage one pair of the DNA strands will exchange to form a Holliday Junction intermediate.\n\nCre recombinase plays important roles in the life cycle of the P1 Bacteriophage. Upon infection of a cell the Cre-loxP system is used to cause circularization of the P1 DNA. In addition to this Cre is also used to resolve dimeric lysogenic P1 DNA that forms during the cell division of the phage.\n\nThe simplicity and robustness of the Cre-loxP systems has enabled scientists to exploit the Cre enzyme in order to manipulate DNA both in vivo and in vitro. See Cre-Lox Recombination for more details. The Cre enzyme can be expressed in many different organisms such as plants, bacteria, mammals, yeast. In 1992 Cre was expressed and found to be functional in a mouse host. Promoter regions can be manipulated to allow precise temporal control of Cre enzyme expression (E.g. RU486-sensitive chimeric regulator GLVP). As the enzyme has a specific 34bp DNA substrate the genome of the organism would have to be 10 bp in length for there to be a likely occurrence of a loxP site. As mammalian genomes are on average in the region of 3 bp there is a very low chance of finding an endogenous loxP site. For Cre to be functional in a foreign host, exogenous loxP sites must be engineered. This allows precise control over the activity of the Cre enzyme in test organisms.\n\nIndependently, Joe Z. Tsien has pioneered the use of Cre-loxP system for neuroscience research to achieve cell type- and region-specific gene manipulation in the adult brain where hundreds of distinct neuron types may exist and nearly all neurons in the adult brain are in post-mitotic state. Tsien and his colleagues demonstrated Cre-mediated recombination can occur in the post-mitotic pyramidal neurons in the adult mouse forebrain.; The clear demonstration of its usefulness in precisely defining the complex relationship between specific cells/circuits and behaviors for brain research, has promoted the NIH to initiate the NIH Blueprint for Neuroscience Research Cre-driver mouse projects in early 2000. To date, NIH Blueprint for Neuroscience Research Cre projects have created several hundreds of Cre driver mouse lines which are currently used by the worldwide neuroscience community.\n\nIn recent years, Cre recombinase has been improved with conversion to preferred mammalian codons, the removal of reported cryptic splice sites, an altered stop codon, and reduced CpG content to reduce the risk of epigenetic silencing in mammals. A number of mutants with enhanced accuracy have also been identified.\n\n", "id": "4468576", "title": "Cre recombinase"}
{"url": "https://en.wikipedia.org/wiki?curid=35586688", "text": "GenGIS\n\nGenGIS merges geographic, ecological and phylogenetic biodiversity data in a single interactive visualization and analysis environment. A key feature of GenGIS is the testing of geographic axes that can correspond to routes of migration or gradients that influence community similarity. Data can also be explored using graphical summaries of data on a site-by-site basis, as 3D geophylogenies, or custom visualizations developed using a plugin framework. Standard statistical test such as linear regression and Mantel are provided, and the R statistical language can be accessed directly within GenGIS. Since its release, GenGIS has been used to investigate the phylogeography of viruses and bacteriophages, bacteria, and eukaryotes.\n\n\n", "id": "35586688", "title": "GenGIS"}
{"url": "https://en.wikipedia.org/wiki?curid=30876867", "text": "Molecular cloning\n\nMolecular cloning is a set of experimental methods in molecular biology that are used to assemble recombinant DNA molecules and to direct their replication within host organisms. The use of the word \"cloning\" refers to the fact that the method involves the replication of one molecule to produce a population of cells with identical DNA molecules. Molecular cloning generally uses DNA sequences from two different organisms: the species that is the source of the DNA to be cloned, and the species that will serve as the living host for replication of the recombinant DNA. Molecular cloning methods are central to many contemporary areas of modern biology and medicine.\n\nIn a conventional molecular cloning experiment, the DNA to be cloned is obtained from an organism of interest, then treated with enzymes in the test tube to generate smaller DNA fragments. Subsequently, these fragments are then combined with vector DNA to generate recombinant DNA molecules. The recombinant DNA is then introduced into a host organism (typically an easy-to-grow, benign, laboratory strain of \"E. coli\" bacteria). This will generate a population of organisms in which recombinant DNA molecules are replicated along with the host DNA. Because they contain foreign DNA fragments, these are transgenic or genetically modified microorganisms (GMO). This process takes advantage of the fact that a single bacterial cell can be induced to take up and replicate a single recombinant DNA molecule. This single cell can then be expanded exponentially to generate a large amount of bacteria, each of which contain copies of the original recombinant molecule. Thus, both the resulting bacterial population, and the recombinant DNA molecule, are commonly referred to as \"clones\". Strictly speaking, \"recombinant DNA\" refers to DNA molecules, while \"molecular cloning\" refers to the experimental methods used to assemble them. The idea arose that different DNA sequences could be inserted into a plasmid and that these foreign sequences would be carried into bacteria and digested as part of the plasmid. That is, these plasmids could serve as cloning vectors to carry genes. \n\nVirtually any DNA sequence can be cloned and amplified, but there are some factors that might limit the success of the process. Examples of the DNA sequences that are difficult to clone are inverted repeats, origins of replication, centromeres and telomeres. Another characteristic that limits chances of success is large size of DNA sequence. Inserts larger than 10kbp have very limited success, but bacteriophages such as bacteriophage λ can be modified to successfully insert a sequence up to 40 kbp.\n\nPrior to the 1970s, our understanding of genetics and molecular biology was severely hampered by an inability to isolate and study individual genes from complex organisms. This changed dramatically with the advent of molecular cloning methods. Microbiologists, seeking to understand the molecular mechanisms through which bacteria restricted the growth of bacteriophage, isolated restriction endonucleases, enzymes that could cleave DNA molecules only when specific DNA sequences were encountered. They showed that restriction enzymes cleaved chromosome-length DNA molecules at specific locations, and that specific sections of the larger molecule could be purified by size fractionation. Using a second enzyme, DNA ligase, fragments generated by restriction enzymes could be joined in new combinations, termed recombinant DNA. By recombining DNA segments of interest with vector DNA, such as bacteriophage or plasmids, which naturally replicate inside bacteria, large quantities of purified recombinant DNA molecules could be produced in bacterial cultures. The first recombinant DNA molecules were generated and studied in 1972.\n\nMolecular cloning takes advantage of the fact that the chemical structure of DNA is fundamentally the same in all living organisms. Therefore, if any segment of DNA from any organism is inserted into a DNA segment containing the molecular sequences required for DNA replication, and the resulting recombinant DNA is introduced into the organism from which the replication sequences were obtained, then the foreign DNA will be replicated along with the host cell's DNA in the transgenic organism.\n\nMolecular cloning is similar to polymerase chain reaction (PCR) in that it permits the replication of DNA sequence. The fundamental difference between the two methods is that molecular cloning involves replication of the DNA in a living microorganism, while PCR replicates DNA in an \"in vitro\" solution, free of living cells.\n\nIn standard molecular cloning experiments, the cloning of any DNA fragment essentially involves seven steps: (1) Choice of host organism and cloning vector, (2) Preparation of vector DNA, (3) Preparation of DNA to be cloned, (4) Creation of recombinant DNA, (5) Introduction of recombinant DNA into host organism, (6) Selection of organisms containing recombinant DNA, (7) Screening for clones with desired DNA inserts and biological properties.\n\nAlthough the detailed planning of the cloning can be done in any text editor, together with online utilities for e.g. PCR primer design, dedicated software exist for the purpose. Software for the purpose include for example ApE (open source), DNAStrider (open source), Serial Cloner (gratis) and Collagene (open source). \n\nNotably, the growing capacity and fidelity of DNA synthesis platforms allows for increasingly intricate designs in molecular engineering. These projects may include very long strands of novel DNA sequence and/or test entire libraries simultaneously, as opposed to of individual sequences. These shifts introduce complexity that require design to move away from the flat nucleotide-based representation and towards a higher level of abstraction. Examples of such tools are GenoCAD, Teselagen (free for academia) or GeneticConstructor (free for academics).\n\nAlthough a very large number of host organisms and molecular cloning vectors are in use, the great majority of molecular cloning experiments begin with a laboratory strain of the bacterium \"E. coli\" (\"Escherichia coli\") and a plasmid cloning vector. \"E. coli\" and plasmid vectors are in common use because they are technically sophisticated, versatile, widely available, and offer rapid growth of recombinant organisms with minimal equipment. If the DNA to be cloned is exceptionally large (hundreds of thousands to millions of base pairs), then a bacterial artificial chromosome or yeast artificial chromosome vector is often chosen.\n\nSpecialized applications may call for specialized host-vector systems. For example, if the experimentalists wish to harvest a particular protein from the recombinant organism, then an expression vector is chosen that contains appropriate signals for transcription and translation in the desired host organism. Alternatively, if replication of the DNA in different species is desired (for example, transfer of DNA from bacteria to plants), then a multiple host range vector (also termed shuttle vector) may be selected. In practice, however, specialized molecular cloning experiments usually begin with cloning into a bacterial plasmid, followed by subcloning into a specialized vector.\n\nWhatever combination of host and vector are used, the vector almost always contains four DNA segments that are critically important to its function and experimental utility:\n\nThe cloning vector is treated with a restriction endonuclease to cleave the DNA at the site where foreign DNA will be inserted. The restriction enzyme is chosen to generate a configuration at the cleavage site that is compatible with the ends of the foreign DNA (see DNA end). Typically, this is done by cleaving the vector DNA and foreign DNA with the same restriction enzyme, for example EcoRI. Most modern vectors contain a variety of convenient cleavage sites that are unique within the vector molecule (so that the vector can only be cleaved at a single site) and are located within a gene (frequently beta-galactosidase) whose inactivation can be used to distinguish recombinant from non-recombinant organisms at a later step in the process. To improve the ratio of recombinant to non-recombinant organisms, the cleaved vector may be treated with an enzyme (alkaline phosphatase) that dephosphorylates the vector ends. Vector molecules with dephosphorylated ends are unable to replicate, and replication can only be restored if foreign DNA is integrated into the cleavage site.\n\nFor cloning of genomic DNA, the DNA to be cloned is extracted from the organism of interest. Virtually any tissue source can be used (even tissues from extinct animals ), as long as the DNA is not extensively degraded. The DNA is then purified using simple methods to remove contaminating proteins (extraction with phenol), RNA (ribonuclease) and smaller molecules (precipitation and/or chromatography). Polymerase chain reaction (PCR) methods are often used for amplification of specific DNA or RNA (RT-PCR) sequences prior to molecular cloning.\n\nDNA for cloning experiments may also be obtained from RNA using reverse transcriptase (complementary DNA or cDNA cloning), or in the form of synthetic DNA (artificial gene synthesis). cDNA cloning is usually used to obtain clones representative of the mRNA population of the cells of interest, while synthetic DNA is used to obtain any precise sequence defined by the designer.\n\nThe purified DNA is then treated with a restriction enzyme to generate fragments with ends capable of being linked to those of the vector. If necessary, short double-stranded segments of DNA (\"linkers\") containing desired restriction sites may be added to create end structures that are compatible with the vector.\n\nThe creation of recombinant DNA is in many ways the simplest step of the molecular cloning process. DNA prepared from the vector and foreign source are simply mixed together at appropriate concentrations and exposed to an enzyme (DNA ligase) that covalently links the ends together. This joining reaction is often termed ligation. The resulting DNA mixture containing randomly joined ends is then ready for introduction into the host organism.\n\nDNA ligase only recognizes and acts on the ends of linear DNA molecules, usually resulting in a complex mixture of DNA molecules with randomly joined ends. The desired products (vector DNA covalently linked to foreign DNA) will be present, but other sequences (e.g. foreign DNA linked to itself, vector DNA linked to itself and higher-order combinations of vector and foreign DNA) are also usually present. This complex mixture is sorted out in subsequent steps of the cloning process, after the DNA mixture is introduced into cells.\n\nThe DNA mixture, previously manipulated in vitro, is moved back into a living cell, referred to as the host organism. The methods used to get DNA into cells are varied, and the name applied to this step in the molecular cloning process will often depend upon the experimental method that is chosen (e.g. transformation, transduction, transfection, electroporation).\n\nWhen microorganisms are able to take up and replicate DNA from their local environment, the process is termed transformation, and cells that are in a physiological state such that they can take up DNA are said to be competent. In mammalian cell culture, the analogous process of introducing DNA into cells is commonly termed transfection. Both transformation and transfection usually require preparation of the cells through a special growth regime and chemical treatment process that will vary with the specific species and cell types that are used.\n\nElectroporation uses high voltage electrical pulses to translocate DNA across the cell membrane (and cell wall, if present). In contrast, transduction involves the packaging of DNA into virus-derived particles, and using these virus-like particles to introduce the encapsulated DNA into the cell through a process resembling viral infection. Although electroporation and transduction are highly specialized methods, they may be the most efficient methods to move DNA into cells.\n\nWhichever method is used, the introduction of recombinant DNA into the chosen host organism is usually a low efficiency process; that is, only a small fraction of the cells will actually take up DNA. Experimental scientists deal with this issue through a step of artificial genetic selection, in which cells that have not taken up DNA are selectively killed, and only those cells that can actively replicate DNA containing the selectable marker gene encoded by the vector are able to survive.\n\nWhen bacterial cells are used as host organisms, the selectable marker is usually a gene that confers resistance to an antibiotic that would otherwise kill the cells, typically ampicillin. Cells harboring the plasmid will survive when exposed to the antibiotic, while those that have failed to take up plasmid sequences will die. When mammalian cells (e.g. human or mouse cells) are used, a similar strategy is used, except that the marker gene (in this case typically encoded as part of the kanMX cassette) confers resistance to the antibiotic Geneticin.\n\nModern bacterial cloning vectors (e.g. pUC19 and later derivatives including the pGEM vectors) use the blue-white screening system to distinguish colonies (clones) of transgenic cells from those that contain the parental vector (i.e. vector DNA with no recombinant sequence inserted). In these vectors, foreign DNA is inserted into a sequence that encodes an essential part of beta-galactosidase, an enzyme whose activity results in formation of a blue-colored colony on the culture medium that is used for this work. Insertion of the foreign DNA into the beta-galactosidase coding sequence disables the function of the enzyme, so that colonies containing transformed DNA remain colorless (white). Therefore, experimentalists are easily able to identify and conduct further studies on transgenic bacterial clones, while ignoring those that do not contain recombinant DNA.\n\nThe total population of individual clones obtained in a molecular cloning experiment is often termed a DNA library. Libraries may be highly complex (as when cloning complete genomic DNA from an organism) or relatively simple (as when moving a previously cloned DNA fragment into a different plasmid), but it is almost always necessary to examine a number of different clones to be sure that the desired DNA construct is obtained. This may be accomplished through a very wide range of experimental methods, including the use of nucleic acid hybridizations, antibody probes, polymerase chain reaction, restriction fragment analysis and/or DNA sequencing.\n\nMolecular cloning provides scientists with an essentially unlimited quantity of any individual DNA segments derived from any genome. This material can be used for a wide range of purposes, including those in both basic and applied biological science. A few of the more important applications are summarized here.\n\nMolecular cloning has led directly to the elucidation of the complete DNA sequence of the genomes of a very large number of species and to an exploration of genetic diversity within individual species, work that has been done mostly by determining the DNA sequence of large numbers of randomly cloned fragments of the genome, and assembling the overlapping sequences.\n\nAt the level of individual genes, molecular clones are used to generate probes that are used for examining how genes are expressed, and how that expression is related to other processes in biology, including the metabolic environment, extracellular signals, development, learning, senescence and cell death. Cloned genes can also provide tools to examine the biological function and importance of individual genes, by allowing investigators to inactivate the genes, or make more subtle mutations using regional mutagenesis or site-directed mutagenesis.\n\nObtaining the molecular clone of a gene can lead to the development of organisms that produce the protein product of the cloned genes, termed a recombinant protein. In practice, it is frequently more difficult to develop an organism that produces an active form of the recombinant protein in desirable quantities than it is to clone the gene. This is because the molecular signals for gene expression are complex and variable, and because protein folding, stability and transport can be very challenging.\n\nMany useful proteins are currently available as recombinant products. These include--(1) medically useful proteins whose administration can correct a defective or poorly expressed gene (e.g. recombinant factor VIII, a blood-clotting factor deficient in some forms of hemophilia, and recombinant insulin, used to treat some forms of diabetes), (2) proteins that can be administered to assist in a life-threatening emergency (e.g. tissue plasminogen activator, used to treat strokes), (3) recombinant subunit vaccines, in which a purified protein can be used to immunize patients against infectious diseases, without exposing them to the infectious agent itself (e.g. hepatitis B vaccine), and (4) recombinant proteins as standard material for diagnostic laboratory tests.\n\nOnce characterized and manipulated to provide signals for appropriate expression, cloned genes may be inserted into organisms, generating transgenic organisms, also termed genetically modified organisms (GMOs). Although most GMOs are generated for purposes of basic biological research (see for example, transgenic mouse), a number of GMOs have been developed for commercial use, ranging from animals and plants that produce pharmaceuticals or other compounds (pharming), herbicide-resistant crop plants, and fluorescent tropical fish (GloFish) for home entertainment.\n\nGene therapy involves supplying a functional gene to cells lacking that function, with the aim of correcting a genetic disorder or acquired disease. Gene therapy can be broadly divided into two categories. The first is alteration of germ cells, that is, sperm or eggs, which results in a permanent genetic change for the whole organism and subsequent generations. This “germ line gene therapy” is considered by many to be unethical in human beings. The second type of gene therapy, “somatic cell gene therapy”, is analogous to an organ transplant. In this case, one or more specific tissues are targeted by direct treatment or by removal of the tissue, addition of the therapeutic gene or genes in the laboratory, and return of the treated cells to the patient. Clinical trials of somatic cell gene therapy began in the late 1990s, mostly for the treatment of cancers and blood, liver, and lung disorders.\n\nDespite a great deal of publicity and promises, the history of human gene therapy has been characterized by relatively limited success. The effect of introducing a gene into cells often promotes only partial and/or transient relief from the symptoms of the disease being treated. Some gene therapy trial patients have suffered adverse consequences of the treatment itself, including deaths. In some cases, the adverse effects result from disruption of essential genes within the patient's genome by insertional inactivation. In others, viral vectors used for gene therapy have been contaminated with infectious virus. Nevertheless, gene therapy is still held to be a promising future area of medicine, and is an area where there is a significant level of research and development activity.\n", "id": "30876867", "title": "Molecular cloning"}
{"url": "https://en.wikipedia.org/wiki?curid=35775080", "text": "Complex segregation analysis\n\nComplex segregation analysis (CSA) is a technique within genetic epidemiology to determine whether there is evidence that a major gene underlies the distribution of a given phenotypic trait. CSA also provides evidence to whether the implicated trait is inherited in a Mendelian dominant, recessive, or codominant manner.\n\nCSA is often a preliminary step in genetic epidemiology. The purpose of CSA is to provide initial evidence that a single gene has a major effect on a particular phenotypic trait. Only phenotypic information, not genotypic information, is required for CSA. CSA can provide evidence, but not definitively prove a trait is under the control of a single gene. Evidence from CSA studies can be used to justify which phenotypes might be appropriate for more in-depth studies such as linkage analysis.\n\nCSA requires phenotypic information on family members in a pedigree. A variety of models with different parameters and assumptions about the nature of the inheritance of the trait are fit to the data. CSA studies may include non-genetic models which assume the trait has no genetic component and is only determined by environmental factors, models which include environmental components as well as multi-gene heritability components, and models which include environment, multi-gene heritability, and a single major gene to best fit the data. CSA software uses a maximum likelihood estimator to assign the best fitting coefficients to each component in all models. Nested models are then tested for their goodness of fit starting at the most complex. If two models are found to fit equally well, the more complex model is rejected in favor of the simpler model. If the best fitting model includes a single major gene component, there is evidence that the trait of interest is under Mendelian control.\n\n\n", "id": "35775080", "title": "Complex segregation analysis"}
{"url": "https://en.wikipedia.org/wiki?curid=35598338", "text": "Neofunctionalization\n\nNeofunctionalization, one of the possible outcomes of functional divergence, occurs when one gene copy, or paralog, takes on a totally new function after a gene duplication event. Neofunctionalization is an adaptive mutation process; meaning one of the gene copies must mutate to develop a function that was not present in the ancestral gene. In other words, one of the duplicates retains its original function, while the other accumulates molecular changes such that, in time, it can perform a different task. This process is thought to be free of selective pressure because one gene copy can mutate without adversely affecting the fitness of the organism since ancestral function is retained in the other copy.\n\nThe process of Neofunctionalization begins with a gene duplication event, which is thought to occur as a defense mechanism against the accumulation of deleterious mutations. Following the gene duplication event there are two identical copies of the ancestral gene performing exactly the same function. This redundancy allows one the copies to take on a new function. In the event that the new function is advantageous, natural selection positively selects for it and the new mutation becomes fixed in the population.\nThe occurrence of Neofunctionalization can most often be attributed to changes in the coding region or changes in the regulatory elements of a gene. It is much more rare to see major changes in protein function, such as subunit structure or substrate and ligand affinity, as a result of Neofunctionalization.\n\nNeofunctionalization is also commonly referred to as \"mutation during non-functionality” or “mutation during redundancy”. Regardless of if the mutation arises after non-functionality of a gene or due to redundant gene copies, the important aspect is that in both scenarios one copy of the duplicated gene is freed from selective constraints and by chance acquires a new function which is then improved by natural selection. This process is thought to occur very rarely in evolution for two major reasons. The first reason is that functional changes typically require a large number of amino acid changes; which has a low probability of occurrence. Secondly, because deleterious mutations occur much more frequently than advantageous mutations in evolution. This makes the likelihood that gene function is lost over time (i.e. pseudogenization) far greater than the likelihood of the emergence of a new gene function. \nWalsh discovered that the relative probability of Neofunctionalization is determined by the selective advantage and the relative rate of advantageous mutations. This was proven in his derivation of the relative probability of Neofunctionalization to pseudogenization, which is given by: formula_1 where ρ is the ratio of advantageous mutation rate to null mutation rate and S is the population selection 4NeS (Ne: effective population size S: selection intensity).\n\nIn 1936, Muller originally proposed Neofunctionalization as a possible outcome of a gene duplication event. In 1970, Ohno suggested that Neofunctionalization was the only evolutionary mechanism that gave rise to new gene functions in a population. He also believed that Neofunctionalization was the only alternative to pseudogenization. Ohta (1987) was among the first to suggest that other mechanisms may exist for the preservation of duplicated genes in the population. Today, subfunctionalization is a widely accepted alternative fixation process for gene duplicates in the population and is currently the only other possible outcome of functional divergence.\n\nNeosubfunctionalization occurs when Neofunctionalization is the end result of subfunctionalization. In other words, once a gene duplication event occurs forming parologs that after an evolutionary period subfunctionalize, one gene copy continues on this evolutionary journey and accumulates mutations that give rise to a new function. Some believe that Neofunctionalization is the end stage for all subfunctionalized genes. For instance, according to Rastogi and Liberles “Neofunctionalization is the terminal fate of all duplicate gene copies retained in the genome and subfuctionlization merely exist as a transient state to preserve the duplicate gene copy.” The results of their study become punctuated as population size increases.\n\nThe evolution of the antifreeze protein in the Antarctic zoarcid fish provides a prime example of Neofunctionalization after gene duplication. In the case of the Antarctic zoarcid fish type III antifreeze protein gene diverged from a parologus copy of sialic acid synthase (SAS) gene. The ancestral SAS gene was found to have both sialic acid synthase and rudimentary ice-binding functionalities. After duplication one of the paralogs began to accumulate mutations that lead to the replacement of SAS domains of the gene allowing for further development and optimization of the antifreeze functionality. The new gene is now capable of noncolligative freezing-point depression, and thus is neofunctionalized. This specialization allows Antarctic zoarcid fish to survive in the frigid temperatures of the Antarctic Seas.\n\nLimitations exist in Neofunctionalization as model for functional divergence primarily because: (1) the amount of nucleotide changes giving rise to a new function must be very minimal; making the probability for pseudogenization much higher than neofunctionalization after a gene duplication event.\n(2) After a gene duplication event both copies may be subjected to selective pressure equivalent to that constraining the ancestral gene; meaning that neither copy is available for Neofunctionalization. \n(3) In many cases positive Darwinian selection presents a more parsimonious explanation for the divergence of multigene families.\n\n", "id": "35598338", "title": "Neofunctionalization"}
{"url": "https://en.wikipedia.org/wiki?curid=245186", "text": "Deletion (genetics)\n\nIn genetics, a deletion (also called gene deletion, deficiency, or deletion mutation) (sign: Δ) is a mutation (a genetic aberration) in which a part of a chromosome or a sequence of DNA is lost during DNA replication. Any number of nucleotides can be deleted, from a single base to an entire piece of chromosome. The smallest single base deletion mutations are believed to occur by a single base flipping in the template DNA, followed by template DNA strand slippage, within the DNA polymerase active site. Deletions can be caused by errors in chromosomal crossover during meiosis, which causes several serious genetic diseases. Deletions that do not occur in multiples of three bases can cause a frameshift by changing the 3-nucleotide protein reading frame of the genetic sequence. One of the regulatory mechanisms that helps reduce the number of mutations is the exonuclease enzyme that follows DNA polymerase and removes sections of DNA that do not correspond to the nucleotides on the DNA template. The examples are given below of types and effects of deletions are representative of eukaryotic organisms, particularly humans, but are not relevant to prokaryotic organisms such as bacteria.\n\nCauses include the following:\n\nFor synapsis to occur between a chromosome with a large intercalary deficiency and a normal complete homolog, the unpaired region of the normal homolog must loop out of the linear structure into a deletion or compensation loop.\n\nTypes of deletion include the following:\nMicrodeletion is usually found in children with physical abnormalities. A large amount of deletion would result in immediate abortion (miscarriage).\n\nSmall deletions are less likely to be fatal; large deletions are usually fatal — there are always variations based on which genes are lost. Some medium-sized deletions lead to recognizable human disorders, e.g. Williams syndrome.\n\nDeletion of a number of pairs that is not evenly divisible by three will lead to a frameshift mutation, causing all of the codons occurring after the deletion to be read incorrectly during translation, producing a severely altered and potentially nonfunctional protein. In contrast, a deletion that is evenly divisible by three is called an \"in-frame\" deletion.\n\nDeletions are responsible for an array of genetic disorders, including some cases of male infertility and two thirds of cases of Duchenne muscular dystrophy. Deletion of part of the short arm of chromosome 5 results in Cri du chat syndrome. Deletions in the SMN-encoding gene cause spinal muscular atrophy, the most common genetic cause of infant death.\n\nRecent work suggests that some deletions of highly conserved sequences (CONDELs) may be responsible for the evolutionary differences present among closely related species. Such deletions in humans are referred to as hCONDELs may be responsible for the anatomical and behavioral differences between humans, chimpanzees and other mammals.\n\nThe introduction of molecular techniques in conjunction with classical cytogenetic methods has in recent years greatly improved the diagnostic potential for chromosomal abnormalities. In particular, microarray-comparative genomic hybridization (CGH) based on the use of BAC clones promises a sensitive strategy for the detection of DNA copy-number changes on a genome-wide scale. The resolution of detection could be as high as >30,000 \"bands\" and the size of chromosomal deletion detected could as small as 5–20 kb in length. Other computation methods were selected to discover DNA sequencing deletion errors such as end-sequence profiling.\n\n", "id": "245186", "title": "Deletion (genetics)"}
{"url": "https://en.wikipedia.org/wiki?curid=35927838", "text": "Britten–Davidson model\n\nThe Britten–Davidson model, also known as the gene-battery model, is a hypothesis for the regulation of protein synthesis in eukaryotes. Proposed by Roy John Britten and Eric H. Davidson in 1969, the model postulates four classes of DNA sequence: an integrator gene, a producer gene, a receptor site, and a sensor site. The sensor site regulates the integrator gene, responsible for synthesis of activator RNA. The integrator gene cannot synthesize activator RNA unless the sensor site is activated. Activation and deactivation of the sensor site is done by external stimuli, such as hormones. The activator RNA then binds with a nearby receptor site, which stimulates the synthesis of mRNA at the structural gene.\n\nThis theory would explain how several different integrators could be synthesized at the same time. In addition, this theory would explain the pattern of repetitive DNA sequences followed by a unique DNA sequence that exists in genes.,\n\n", "id": "35927838", "title": "Britten–Davidson model"}
{"url": "https://en.wikipedia.org/wiki?curid=35708672", "text": "Developmental homeostasis\n\nDevelopmental homeostasis is a process in which animals develop more or less normally, despite defective genes and deficient environments. It is an organisms ability to overcome certain circumstances in order to develop normally. This can be either a physical or mental circumstance that interferes with either a physical or mental trait. Many species have a specific norm, where those who fit that norm prosper while those who don't are killed or find it difficult to thrive. It is important that the animal be able to interact with the other group members effectively. Animals must learn their species' norms early to live a normal, successful life for that species.\n\nDevelopmental homeostasis determines how a species adapts to live a normal life. Therefore, it has been the focus of many experiments. These experiments are geared and designed to test the threshold for certain species to overcome and prosper despite certain circumstances.\n\nDue to the fact that survival of the species is based on the ability to interact with their own in a normal manner, these experiments—which usually interfere with that—walk a fine line for animal rights and what is acceptable. The rhesus monkey experiments show how the young monkey needs the physical contact with other monkeys to learn the social behaviors and interact with each other effectively. This is one of the ways mental developmental homeostasis has been researched. One way physical developmental homeostasis was tested was in the facial symmetry experiment where people were asked to rate which of the faces they saw as better looking. This experiment resulted in the pictures with more symmetrical faces being called better looking. This is not just found in humans but other experiments such as the brush-legged wolf spider and the barn swallow birds. Favored traits give the bearer an advantage in attracting high quality mates.\n\nIn species that value developmental homeostasis, both physically and mentally, the ability of one to adapt to social norms seems to increase the likelihood of having a reproductive advantage being able to attract mates and leave offspring.\n\nDevelopmental homeostasis attributes to the way many animals develop. This contains the way they develop normally or abnormally despite faulty genes and an insufficient environment. This property of development reduces the variation around a mean value for a phenotype, and reflects the ability of developmental processes to suppress some outcomes in order to generate an adaptive phenotype more reliably.\n\nThe effects of developmental homeostasis were demonstrated in an experiment conducted four decades ago by Margaret and Harry Harlow. They wanted to test what the consequences of infant rhesus monkeys would be from being separated from their mothers and having little interaction with other monkeys. First, they were separated from their mothers for six to twelve hours after birth and placed with a human made mechanical substitute in which they called the \"surrogate mother.\" These surrogates were made of a wire cylinder structure and a terry cloth structure and were placed with the babies is a cage. The terry cloth structure was also placed with a nursing bottle. The monkeys preferred and clung to the cloth surrogate mother. In a short period of time the monkeys took on side effects of weight loss and not abnormal physical development. Once the monkeys reached an age where they could eat solid foods, they were separated from their cloth mothers for three days. When they were reunited with their mothers, they clung to them and did not wander off. Young rhesus monkeys in their natural habitat normally explore and venture off. Harlow concluded from this that the need for contact comfort was stronger than the need to explore. Being isolated from social interaction also caused negative outcomes later in their life. The surrogate-raised monkeys exhibited depression and aggressive behavior as they would clutch to themselves, rock constantly back and forth, and avoid interaction with others. Not only affecting their social behavior, it also impacted them interacting sexually. The monkeys underwent little if any sexual posturing or reproduction in their lifetime.\n\nTo test how much social experience is necessary or needed in normal development the Harlows performed another experiment in that they isolated young rhesus monkeys by limiting the interaction time with other monkeys. They gave them fifteen minutes a day with three other monkeys to interact. In the beginning, the monkeys would attach to each other and begin to play. Normal behavior of these monkeys in their natural habitat is that after the age of one month they play with other monkeys of their own species and by six months are constantly interacting in groups. Results showed that over time the monkeys developed relatively normal behavior and that limiting their social interaction as babies did not affect them interacting sexually and socially. They also did not exhibit any abnormal aggression or depression when socializing with other monkeys.\n\nIn 1960, Harry Harlow began studying partial and total isolation of infant monkeys by raising them in cages. Partial isolation involved raising monkeys in bare wire cages that allowed them to see, smell, and hear other monkeys, but did not give them the opportunity for physical contact. Total social isolation involved rearing monkeys in isolation chambers that took away any and all contact with other monkeys. The results showed that when the monkeys were placed in partial isolation they displayed various abnormal behaviors such as blank staring, circling in their cages numerous times, and self-mutilation. The cages were placed in several locations. A few of the monkeys remained being tested in solitary confinement for fifteen years.\n\nIn the total isolation experiments the baby monkeys were left alone for three, six, twelve, or twenty-four months. The experiments resulted in the monkeys that were severely psychologically disturbed and abnormal development. When first removed from isolation, they usually went into emotional shock by clutching to themselves and rocking multiple times. One of six monkeys isolated for three months refused to eat after release and died five days later. The longer the time in isolation the more detrimental it was for the monkeys to act socially when placed with other monkeys. Harlow pointed out that no monkey had actually died during isolation. Harlow tried to reintegrate the monkeys who had been isolated for six months by placing them with monkeys who had been reared normally. He stated that it produced “severe deficits in virtually every aspect of social behavior” and that it “achieved only limited recovery of simple social responses.” \n\nThe ethics of pulling a baby primate away from their mother at young ages isn't morally right. Removing baby primates from their mothers can cause high psychological trauma in these young primates that can inhibit proper growth and social skills. In addition to lasting effects, these animals may live in small, cramped, dirty environments, which can cause health issues within baby primates. Health problems can also arise sooner and with more impact on a young primate that has just been removed from the comfort of their mother because their immune system is not fully active as it would be with an older primate. The psychological effects of the physiological suffering of these young primates last a lifetime.\nOverall, questions arise about animal experimentation being morally right or wrong. Labs that abuse the ability to conduct animal experimentation are often shut down or lose federal funding, University of Pennsylvania's lab is a good example of this. Some statistics about animal experimentation: 20 million animals are experimented on and are killed annually, an estimated 8 million are experimented on painfully, and reports have shown that ten percent of those tested on do not receive pain killers to minimize the pain these animals are put through. Animal experimentation has become something that is debated about publicly. Those who do not believe in animal experimentation raise the questions about if it is wrong to cause pain to humans then isn't it wrong to cause pain to animals? \nSome advocates of animal experimentation that say that these experiments allow scientific discovery to advance—but some feel that those who support animal experimentation are selfish to inflict pain on other animals as a method to reduce or eliminate pain for humans. The question is not so much a comparison between the pain of humans versus the pain of animals as much as it is the amount of suffering for the animal subjects.\n\nDevelopmental homeostasis plays a role in the development of symmetrical and asymmetrical bodies. This effect in species shows that individuals in species who have symmetrical features are more likely to obtain a mate over one who has asymmetrical features.\n\nThe symmetry in the facial features of humans could be found as appealing to men and women. This could be due to the ability to find prospective mates in responding positively to either body or facial symmetry. This could be because attributes announce the individual's capacity to overcome challenges to normal development. The human preference for symmetry can be explained by humans looking at mate choice, mainly because symmetry is a reliable indicator in the genetic quality mates look for. The cognitive mechanisms and adaptive significance of facial attractiveness have been an interest of psychologists. Men and women find traits such as averageness, symmetry, and masculinity (in male) or femininity (in female) in faces attractive. It is noted that humans find symmetrical faces more attractive than asymmetrical faces. Disruptions to development caused by mutations or by an inability to secure critical material resources early in life could generate asymmetries in appearance. If body asymmetry reflects sub-optimal development of the brain or other important organs, a preference for symmetrical traits could enable the selective individual to acquire a partner with \"good genes\" to transfer to their offspring. Disruptions to development could be caused by stress, illness, or genetic factors.\n\nIn 1998 a study of facial symmetry and how beauty was experimented on by Gillian Rhodes called \"Facial symmetry and the perception of beauty\". In the experiment, images of human faces were digitally manipulated to show various amounts of symmetry. People were asked to rate the faces, and found that the faces with the most symmetry were the most attractive.\n\nDevelopmental homeostasis is present not only in humans, but in animals as well. The choosing of symmetrical features over asymmetrical features have been observed in birds, lizards, Araneae,and even insects. For example, barn swallow females have been reported to prefer males whose long outer feathers are the same length on each side. Symmetrical males gain an reproductive advantage because of the female mate choice.\n\nAnother example is the behavior exhibited in the damselfly \"Lestes viridis\", where males with more symmetrical hindwings were mated with more than their unmated rivals with asymmetrical hindwings. These examples show that individuals with high quality features are able to reproduce their large and symmetrical traits, while individuals who have lower quality features suffer costs to produce their asymmetrical traits.\n\nOne species that chooses their mates based on body symmetry is the brush-legged wolf spider. Males who have larger tufts on one leg than the other (asymmetrical) tend to be smaller, and have poorer fitness than the males who have symmetrical tufts. During courtship, the males wave their hair foreleg tufts, and symmetrical males be chosen over asymmetrical males by the females.\n", "id": "35708672", "title": "Developmental homeostasis"}
{"url": "https://en.wikipedia.org/wiki?curid=35989789", "text": "Neighbor-net\n\nNeighborNet is an algorithm for constructing phylogenetic networks which is loosely based on the neighbor joining algorithm. Like neighbor joining, the method takes a distance matrix as input, and works by agglomerating clusters. However, the NeighborNet algorithm can lead to collections of clusters which overlap and do not form a hierarchy, and are represented using a type of phylogenetic network called a split network. If the distance matrix satisfies the Kalmanson combinatorial conditions then Neighbor-net will return the corresponding circular ordering. The method is implemented in the SplitsTree package.\n\nExamples of the application of Neighbor-net can be found in virology\n, horticulture, dinosaur genetics, comparative linguistics, and archaeology.\n", "id": "35989789", "title": "Neighbor-net"}
{"url": "https://en.wikipedia.org/wiki?curid=5762008", "text": "Microbial genetics\n\nMicrobial genetics is a subject area within microbiology and genetic engineering. It studies the genetics of very small (micro) organisms; bacteria, archaea, viruses and some protozoa and fungi. This involves the study of the genotype of microbial species and also the expression system in the form of phenotypes.\n\nSince the discovery of microorganisms by Robert Hooke and Antoni van Leeuwenhoek during the period 1665-1885 they have been used to study many processes and have had applications in various areas of study in genetics.\nFor example: Microorganisms' rapid growth rates and short generation times are used by scientists to study evolution.\nMicrobial genetics also has applications in being able to study processes and pathways that are similar to those found in humans such as drug metabolism.\n\nBacteria have been on this planet for approximately 3.5 billion years, and are classified by their shape. Bacterial genetics studies the mechanisms of their heritable information, their chromosomes, plasmids, transposons, and phages.\n\nGene transfer systems that have been extensively studied in bacteria include genetic transformation, conjugation and transduction. Natural transformation is a bacterial adaptation for DNA transfer between two cells through the intervening medium. The uptake of donor DNA and its recombinational incorporation into the recipient chromosome depends on the expression of numerous bacterial genes whose products direct this process. In general, transformation is a complex, energy-requiring developmental process that appears to be an adaptation for repairing DNA damage.\n\nBacterial conjugation is the transfer of genetic material between bacterial cells by direct cell-to-cell contact or by a bridge-like connection between two cells. Bacterial conjugation has been extensively studied in \"Escherichia coli\", but also occurs in other bacteria such as \"Mycobacterium smegmatis\". Conjugation requires stable and extended contact between a donor and a recipient strain, is DNase resistant, and the transferred DNA is incorporated into the recipient chromosome by homologous recombination. \"E. coli\" conjugation is mediated by expression of plasmid genes, whereas mycobacterial conjugation is mediated by genes on the bacterial chromosome.\n\nTransduction is the process by which foreign DNA is introduced into a cell by a virus or viral vector. Transduction is a common tool used by molecular biologists to stably introduce a foreign gene into a host cell's genome.\n\nArchaea is a domain of organisms that are prokaryotic, single-celled, and are thought to have developed 4 billion years ago. They share a common ancestor with bacteria, but are more closely related to eukaryotes in comparison to bacteria. Some Archaea are able to survive extreme environments, which leads to many applications in the field of genetics. One of such applications is the use of archaeal enzymes, which would be better able to survive harsh conditions \"in vitro\".\n\nGene transfer and genetic exchange have been studied in the halophilic archaeon \"Halobacterium volcanii\" and the hyperthermophilic archaeons \"Sulfolobus solfataricus\" and \"Sulfolobus acidocaldarius\". \"H. volcani\" forms cytoplasmic bridges between cells that appear to be used for transfer of DNA from one cell to another in either direction. When \"S. solfataricus\" and \"S. acidocaldarius\" are exposed to DNA damaging agents, species-specific cellular aggregation is induced. Cellular aggregation mediates chromosomal marker exchange and genetic recombination with high frequency. Cellular aggregation is thought to enhance species specific DNA transfer between \"Sulfolobus\" cells in order to provide increased repair of damaged DNA by means of homologous recombination.\n\nFungi can be both multicellular and unicellular organisms, and are distinguished from other microbes by the way they obtain nutrients. Fungi secrete enzymes into their surroundings, to break down organic matter. Fungal genetics uses yeast, and filamentous fungi as model organisms for eukaryotic genetic research, including cell cycle regulation, chromatin structure and gene regulation.\n\nStudies of the fungus \"Neurospora crassa\" have contributed substantially to understanding how genes work. \"N. crassa\" is a type of red bread mold of the phylum \"Ascomycota\". It is used as a model organism because it is easy to grow and has a haploid life cycle that makes genetic analysis simple since recessive traits will show up in the offspring. Analysis of genetic recombination is facilitated by the ordered arrangement of the products of meiosis in ascospores. In its natural environment, \"N. crassa\" lives mainly in tropical and sub-tropical regions. It often can be found growing on dead plant matter after fires.\n\n\"Neurospora\" was used by Edward Tatum and George Beadle in their experiments for which they won the Nobel Prize in Physiology or Medicine in 1958. The results of these experiments led directly to the one gene-one enzyme hypothesis that specific genes code for specific proteins. This concept proved to be the opening gun in what became molecular genetics and all the developments that have followed from that.\n\n\"Saccharomyces cerevisiae\" is a yeast of the phylum \"Ascomycota\". During vegetative growth that ordinarily occurs when nutrients are abundant, \"S. cerevisiae\" reproduces by mitosis as diploid cells. However, when starved, these cells undergo meiosis to form haploid spores. Mating occurs when haploid cells of opposite mating types MATa and MATα come into contact. Ruderfer et al. pointed out that, in nature, such contacts are frequent between closely related yeast cells for two reasons. The first is that cells of opposite mating type are present together in the same acus, the sac that contains the cells directly produced by a single meiosis, and these cells can mate with each other. The second reason is that haploid cells of one mating type, upon cell division, often produce cells of the opposite mating type. An analysis of the ancestry of natural \"S. cerevisiae\" strains concluded that outcrossing occurs very infrequently (only about once every 50,000 cell divisions). The relative rarity in nature of meiotic events that result from outcrossing suggests that the possible long-term benefits of outcrossing (e.g. generation of diversity) are unlikely to be sufficient for generally maintaining sex from one generation to the next. Rather, a short term benefit, such as meiotic recombinational repair of DNA damages caused by stressful conditions (such as starvation) may be the key to the maintenance of sex in \"S. cerevisiae\".\n\n\"Candida albicans\" is a diploid fungus that grows both as a yeast and as a filament. \"C. albicans\" is the most common fungal pathogen in humans. It causes both debilitating mucosal infections and potentially life-threatening systemic infections. \"C. albicans\" has maintained an elaborate, but largely hidden, mating apparatus. Johnson suggested that mating strategies may allow \"C. albicans\" to survive in the hostile environment of a mammalian host.\n\nAmong the 250 known species of aspergilli, about 33% have an identified sexual state. Among those \"Aspergillus\" species that exhibit a sexual cycle the overwhelming majority in nature are homothallic (self-fertilizing). Selfing in the homothallic fungus \"Aspergillus nidulans\" involves activation of the same mating pathways characteristic of sex in outcrossing species, i.e. self-fertilization does not bypass required pathways for outcrossing sex but instead requires activation of these pathways within a single individual. Fusion of haploid nuclei occurs within reproductive structures termed \"cleistothecia\", in which the diploid zygote undergoes meiotic divisions to yield haploid ascospores.\n\nProtozoa are unicellular organisms, which have nuclei, and ultramicroscopic cellular bodies within their cytoplasm. One particular aspect of protozoa that are of interest to human geneticists are their flagella, which are very similar to human sperm flagella.\n\nStudies of \"Paramecium\" have contributed to our understanding of the function of meiosis. Like all ciliates, \"Paramecium\" has a polyploid macronucleus, and one or more diploid micronuclei. The macronucleus controls non-reproductive cell functions, expressing the genes needed for daily functioning. The micronucleus is the generative, or germline nucleus, containing the genetic material that is passed along from one generation to the next.\n\nIn the asexual fission phase of growth, during which cell divisions occur by mitosis rather than meiosis, clonal aging occurs leading to a gradual loss of vitality. In some species, such as the well studied \"Paramecium tetraurelia\", the asexual line of clonally aging paramecia loses vitality and expires after about 200 fissions if the cells fail to undergo meiosis followed by either autogamy (self-fertilizaion) or conjugation (outcrossing) (see aging in \"Paramecium\"). DNA damage increases dramatically during successive clonal cell divisions and is a likely cause of clonal aging in \"P. tetraurelia\".\n\nWhen clonally aged \"P. tetraurelia\" are stimulated to undergo meiosis in association with either autogamy or conjugation, the progeny are rejuvenated, and are able to have many more mitotic binary fission divisions. During either of these processes the micronuclei of the cell(s) undergo meiosis, the old macronucleus disintegrates and a new macronucleus is formed by replication of the micronuclear DNA that had recently undergone meiosis. There is apparently little, if any, DNA damage in the new macronucleus, suggesting that rejuvenation is associated with the repair of these damages in the micronucleus during meiosis.\n\nViruses are capsid-encoding organisms composed of proteins and nucleic acids that can self-assemble after replication in a host cell using the host's replication machinery. There is a disagreement in science about whether viruses are living due to their lack of ribosomes. Comprehending the viral genome is important not only for studies in genetics but also for understanding their pathogenic properties.\n\nMany types of virus are capable of genetic recombination. When two or more individual viruses of the same type infect a cell, their genomes may recombine with each other to produce recombinant virus progeny. Both DNA and RNA viruses can undergo recombination. \nWhen two or more viruses, each containing lethal genomic damage infect the same host cell, the virus genomes often can pair with each other and undergo homologous recombinational repair to produce viable progeny. This process is known as multiplicity reactivation. Enzymes employed in multiplicity reactivation are functionally homologous to enzymes employed in bacterial and eukaryotic recombinational repair. Multiplicity reactivation has been found to occur with pathogenic viruses including influenza virus, HIV-1, adenovirus simian virus 40, vaccinia virus, reovirus, poliovirus and herpes simplex virus as well as numerous bacteriophages.\n\nMicrobes are ideally suited for biochemical and genetics studies and have made huge contributions to these fields of science such as the demonstration that DNA is the genetic material, that the gene has a simple linear structure, that the genetic code is a triplet code, and that gene expression is regulated by specific genetic processes. Jacques Monod and François Jacob used \"Escherichia coli\", a type of bacteria, in order to develop the operon model of gene expression, which lay down the basis of gene expression and regulation. Furthermore, the hereditary processes of single-celled eukaryotic microorganisms are similar to those in multi-cellular organisms allowing researchers to gather information on this process as well. Another bacterium which has greatly contributed to the field of genetics is \"Thermus aquaticus\", which is a bacterium that tolerates high temperatures. From this microbe scientists isolated the enzyme Taq polymerase, which is now used in the powerful experimental technique, Polymerase chain reaction(PCR). Additionally the development of recombinant DNA technology through the use of bacteria has led to the birth of modern genetic engineering and biotechnology.\n\nUsing microbes, protocols were developed to insert genes into bacterial plasmids, taking advantage of their fast reproduction, to make biofactories for the gene of interest. Such genetically engineered bacteria can produce pharmaceuticals such as insulin, human growth hormone, interferons and blood clotting factors. Microbes synthesize a variety of enzymes for industrial applications, such as fermented foods, laboratory test reagents, dairy products (such as renin), and even in clothing (such as \"Trichoderma\" fungus whose enzyme is used to give jeans a stone washed appearance).\n", "id": "5762008", "title": "Microbial genetics"}
{"url": "https://en.wikipedia.org/wiki?curid=155624", "text": "Heritability\n\nHeritability is a statistic used in the fields of breeding and genetics that estimates the degree of variation in a phenotypic trait in a population that is due to genetic variation between individuals in that population. In other words, the concept of heritability can alternately be expressed in the form of the following question: \"What is the proportion of the variation in a given trait within a population that is \"not\" explained by the environment of random chance?\"\n\nOther causes of measured variation in a trait are characterized as environmental factors, including measurement error. In human studies of heritability these are often apportioned into factors from \"shared environment\" and \"non-shared environment\" based on whether they tend to result in persons brought up in the same household more or less similar to persons who were not.\n\nHeritability is estimated by comparing individual phenotypic variation among related individuals in a population. Heritability is an important concept in quantitative genetics, particularly in selective breeding and behavior genetics (for instance, twin studies).\n\nHeritability measures the fraction of phenotype variability that can be attributed to genetic variation. This is not the same as saying that this fraction of an individual phenotype is caused by genetics. In addition, heritability can change without any genetic change occurring, such as when the environment starts contributing to more variation. As a case in point, consider that both genes and environment have the potential to influence intelligence. Heritability could increase if genetic variation increases, causing individuals to show more phenotypic variation, like showing different levels of intelligence. On the other hand, heritability might also increase if the environmental variation decreases, causing individuals to show less phenotypic variation, like showing more similar levels of intelligence. Heritability increases when genetics are contributing more variation or because non-genetic factors are contributing less variation; what matters is the relative contribution. Heritability is specific to a particular population in a particular environment.\n\nThe extent of dependence of phenotype on environment can also be a function of the genes involved. Matters of heritability are complicated because genes may canalize a phenotype, making its expression almost inevitable in all occurring environments. Individuals with the same genotype can also exhibit different phenotypes through a mechanism called phenotypic plasticity, which makes heritability difficult to measure in some cases. Recent insights in molecular biology have identified changes in transcriptional activity of individual genes associated with environmental changes. However, there are a large number of genes whose transcription is not affected by the environment.\n\nEstimates of heritability use statistical analyses to help to identify the causes of differences between individuals. Since heritability is concerned with variance, it is necessarily an account of the differences between individuals in a population. Heritability can be univariate – examining a single trait – or multivariate – examining the genetic and environmental associations between multiple traits at once. This allows a test of the genetic overlap between different phenotypes: for instance hair color and eye color. Environment and genetics may also interact, and heritability analyses can test for and examine these interactions (GxE models).\n\nA prerequisite for heritability analyses is that there is some population variation to account for. This last point highlights the fact that heritability cannot take into account the effect of factors which are invariant in the population. Factors may be invariant if they are absent and do not exist in the population, such as no one having access to a particular antibiotic, or because they are omni-present, like if everyone is drinking coffee. In practice, all human behavioral traits vary and almost all traits show some heritability.\n\nAny particular phenotype can be modeled as the sum of genetic and environmental effects:\nLikewise the phenotypic variance in the trait – Var (P) – is the sum of effects as follows:\nIn a planned experiment Cov(\"G\",\"E\") can be controlled and held at 0. In this case, heritability is defined as:\n\n\"H\" is the broad-sense heritability. This reflects all the genetic contributions to a population's phenotypic variance including additive, dominant, and epistatic (multi-genic interactions), as well as maternal and paternal effects, where individuals are directly affected by their parents' phenotype, such as with milk production in mammals.\n\nA particularly important component of the genetic variance is the additive variance, Var(A), which is the variance due to the average effects (additive effects) of the alleles. Since each parent passes a single allele per locus to each offspring, parent-offspring resemblance depends upon the average effect of single alleles. Additive variance represents, therefore, the genetic component of variance responsible for parent-offspring resemblance. The additive genetic portion of the phenotypic variance is known as Narrow-sense heritability and is defined as\n\nAn upper case \"H\" is used to denote broad sense, and lower case \"h\" for narrow sense.\n\nFor traits which are not continuous but dichotomous such as an additional toe or certain diseases, the contribution of the various alleles can be considered to be a sum, which past a threshold, manifests itself as the trait, giving the liability threshold model in which heritability can be estimated and selection modeled.\n\nAdditive variance is important for selection. If a selective pressure such as improving livestock is exerted, the response of the trait is directly related to narrow-sense heritability. The mean of the trait will increase in the next generation as a function of how much the mean of the selected parents differs from the mean of the population from which the selected parents were chosen. The observed \"response to selection\" leads to an estimate of the narrow-sense heritability (called realized heritability). This is the principle underlying artificial selection or breeding.\n\nThe simplest genetic model involves a single locus with two alleles (b and B) affecting one quantitative phenotype.\n\nThe number of B alleles can vary from 0, 1, or 2. For any genotype, BB, the expected phenotype can then be written as the sum of the overall mean, a linear effect, and a dominance deviation:\n\nThe additive genetic variance at this locus is the weighted average of the squares of the additive effects:\n\nwhere formula_7\n\nThere is a similar relationship for variance of dominance deviations:\n\nwhere formula_9\n\nThe linear regression of phenotype on genotype is shown in Figure 1.\n\nSince only \"P\" can be observed or measured directly, heritability must be estimated from the similarities observed in subjects varying in their level of genetic or environmental similarity. The statistical analyses required to estimate the genetic and environmental components of variance depend on the sample characteristics. Briefly, better estimates are obtained using data from individuals with widely varying levels of genetic relationship - such as twins, siblings, parents and offspring, rather than from more distantly related (and therefore less similar) subjects. The standard error for heritability estimates is improved with large sample sizes.\n\nIn non-human populations it is often possible to collect information in a controlled way. For example, among farm animals it is easy to arrange for a bull to produce offspring from a large number of cows and to control environments. Such experimental control is generally not possible when gathering human data, relying on naturally occurring relationships and environments.\n\nStudies of human heritability often utilize adoption study designs, often with identical twins who have been separated early in life and raised in different environments (see for example Fig. 2). Such individuals have identical genotypes and can be used to separate the effects of genotype and environment. A limit of this design is the common prenatal environment and the relatively low numbers of twins reared apart. A second and more common design is the twin study in which the similarity of identical and fraternal twins is used to estimate heritability. These studies can be limited by the fact that identical twins are not completely genetically identical, potentially resulting in an underestimation of heritability. Studies of twins also examine differences between twins and non-twin siblings, for instance to examine phenomena such as intrauterine competition (for example, twin-to-twin transfusion syndrome).\n\nHeritability estimates are always relative to the genetic and environmental factors in the population, and are not absolute measurements of the contribution of genetic and environmental factors to a phenotype. Heritability estimates reflect the amount of variation in genotypic effects compared to variation in environmental effects.\n\nHeritability can be made larger by diversifying the genetic background, e.g., by using only very out bred individuals (which increases Var) and/or by minimizing environmental effects (decreasing Var). The converse also holds. Due to such effects, different populations of a species might have different heritabilities for the same trait.\n\nIn observational studies, or because of evokative effects (where a genome evokes environments by its effect on them), G and E may covary: gene environment correlation. Depending on the methods used to estimate heritability, correlations between genetic factors and shared or non-shared environments may or may not be confounded with heritability.\n\nA common estimate of heritability is called the Heritability Index (HI), which ranges from 0 - 1. An HI index of 0 means that none of the variability of the trait among individuals in the study sample is the result of genetic factors; an HI of 1 indicates that all of the variability of the trait among individuals in the study sample is the result of genetic factors.\n\nHeritability estimates are often misinterpreted if it is not understood that they refer to the \"proportion of variation among individuals\" on a trait that is linked with genetic factors. It does not indicate the degree of genetic influence on the development of a trait of an individual. For example, it is incorrect to say that since the heritability of personality traits is about .6, that means that 60% of your personality is inherited from your parents and 40% comes from the environment.\n\nEven a highly heritable trait (such as eye color) assumes environmental inputs which are required for development: for instance temperatures and an atmosphere supporting life, etc. A more useful distinction than \"nature vs. nurture\" is \"obligate vs. facultative\"—under typical environmental ranges, what traits are more \"obligate\" (e.g., the nose—everyone has a nose) or more \"facultative\" (sensitive to environmental variations, such as specific language learned during infancy). Another useful distinction is between traits that are likely to be adaptations (such as the umbilical cord) vs. those that are byproducts of adaptations (such as the belly button), or are due to random variation (non-adaptive variation in belly button shape, e.g., convex or concave).\n\nThere are essentially two schools of thought regarding estimation of heritability.\n\nOne school of thought was developed by Sewall Wright at The University of Chicago, and further popularized by C. C. Li (University of Chicago) and J. L. Lush (Iowa State University). It is based on the analysis of correlations and, by extension, regression. Path Analysis was developed by Sewall Wright as a way of estimating heritability.\n\nThe second was originally developed by R. A. Fisher and expanded at The University of Edinburgh, Iowa State University, and North Carolina State University, as well as other schools. It is based on the analysis of variance of breeding studies, using the intraclass correlation of relatives. Various methods of estimating components of variance (and, hence, heritability) from ANOVA are used in these analyses.\n\nThe first school of estimation uses regression and correlation to estimate heritability.\n\nCalculating the strength of selection, \"S\" (the difference in mean trait between the population as a whole and the selected parents of the next generation, also called the \"selection differential\") and response to selection \"R\" (the difference in offspring and whole parental generation mean trait) in an artificial selection experiment will allow calculation of realized heritability as the response to selection relative to the strength of selection, \"h\"=R/S as in Fig. 3.\n\nIn the comparison of relatives, we find that in general,\n\nwhere \"r\" can be thought of as the coefficient of relatedness, \"b\" is the coefficient of regression and \"t\" is the coefficient of correlation.\n\nHeritability may be estimated by comparing parent and offspring traits (as in Fig. 4). The slope of the line (0.57) approximates the heritability of the trait when offspring values are regressed against the average trait in the parents. If only one parent's value is used then heritability is twice the slope. (Note that this is the source of the term \"regression,\" since the offspring values always tend to regress to the mean value for the population, \"i.e.\", the slope is always less than one). This regression effect also underlies the DeFries Fulker method for analyzing twins selected for one member being affected.\n\nA basic approach to heritability can be taken using full-Sib designs: comparing similarity between siblings who share both a biological mother and a father. When there is only additive gene action, this sibling phenotypic correlation is an index of \"familiarity\" – the sum of half the additive genetic variance plus full effect of the common environment. It thus places an upper-limit on additive heritability of twice the full-Sib phenotypic correlation. Half-Sib designs compare phenotypic traits of siblings that share one parent with other sibling groups.\n\nHeritability for traits in humans is most frequently estimated by comparing resemblances between twins (Fig. 2 & 5). \"The advantage of twin studies, is that the total variance can be split up into genetic, shared or common environmental, and unique environmental components, enabling an accurate estimation of heritability\". Fraternal or dizygotic (DZ) twins on average share half their genes (assuming there is no assortative mating for the trait), and so identical or monozygotic (MZ) twins on average are twice as genetically similar as DZ twins. A crude estimate of heritability, then, is approximately twice the difference in correlation between MZ and DZ twins, i.e. Falconer's formula \"H\"=2(r(MZ)-r(DZ)).\n\nThe effect of shared environment, \"c\", contributes to similarity between siblings due to the commonality of the environment they are raised in. Shared environment is approximated by the DZ correlation minus half heritability, which is the degree to which DZ twins share the same genes, \"c\"=DZ-1/2\"h\". Unique environmental variance, \"e\", reflects the degree to which identical twins raised together are dissimilar, \"e\"=1-r(MZ).\n\nThe methodology of the classical twin study has been criticized, but some of these criticisms do not take into account the methodological innovations and refinements described above.\n\nWhile often heritability is analyzed in single generations: comparing MZ twins raised apart, or comparing the similarity of MZ and DZ twins, considerable power can be gained using more complex relationships. By studying a trait in multi-generational families, the multiple recombination of genetic and environmental effects can be decomposed using software such as ASReml and heritability estimated. This design is helpful for untangling confounds such as reverse causality, maternal effects such as the prenatal environment, and confounding of genetic dominance, shared environment, and maternal gene effects but is generally less powerful than the twin design for obtaining heritability estimates.\n\nThe second set of methods of estimation of heritability involves ANOVA and estimation of variance components.\n\nWe use the basic discussion of Kempthorne (1957 [1969]). Considering only the most basic of genetic models, we can look at the quantitative contribution of a single locus with genotype G as\n\nwhere formula_12 is the effect of genotype G and formula_13 is the environmental effect.\n\nConsider an experiment with a group of sires and their progeny from random dams. Since the progeny get half of their genes from the father and half from their (random) mother, the progeny equation is\n\nConsider the experiment above. We have two groups of progeny we can compare. The first is comparing the various progeny for an individual sire (called \"within sire group\"). The variance will include terms for genetic variance (since they did not all get the same genotype) and environmental variance. This is thought of as an \"error\" term.\n\nThe second group of progeny are comparisons of means of half sibs with each other (called \"among sire group\"). In addition to the error term as in the within sire groups, we have an addition term due to the differences among different means of half sibs. The intraclass correlation is\nsince environmental effects are independent of each other.\n\nIn an experiment with formula_16 sires and formula_17 progeny per sire, we can calculate the following ANOVA, using formula_18 as the genetic variance and formula_19 as the environmental variance:\n\nThe formula_20 term is the intraclass correlation among half sibs. We can easily calculate formula_21. The Expected Mean Square is calculated from the relationship of the individuals (progeny within a sire are all half-sibs, for example), and an understanding of intraclass correlations.\n\nFor a model with additive and dominance terms, but not others, the equation for a single locus is\n\nwhere\n\nformula_23 is the additive effect of the i allele, formula_24 is the additive effect of the j allele, formula_5 is the dominance deviation for the ij genotype, and formula_13 is the environment.\n\nExperiments can be run with a similar setup to the one given in Table 1. Using different relationship groups, we can evaluate different intraclass correlations. Using formula_27 as the additive genetic variance and formula_28 as the dominance deviation variance, intraclass correlations become linear functions of these parameters. In general,\n\nwhere formula_17 and formula_31 are found as\n\nformula_32P[ alleles drawn at random from the relationship pair are identical by descent], and\n\nformula_33P[ genotypes drawn at random from the relationship pair are identical by descent].\n\nSome common relationships and their coefficients are given in Table 2.\n\nWhen a large, complex pedigree is available for estimating heritability, the most efficient use of the data is in a restricted maximum likelihood (REML) model. The raw data will usually have three or more data points for each individual: a code for the sire, a code for the dam and one or several trait values. Different trait values may be for different traits or for different time points of measurement.\n\nThe currently popular methodology relies on high degrees of certainty over the identities of the sire and dam; it is not common to treat the sire identity probabilistically. This is not usually a problem, since the methodology is rarely applied to wild populations (although it has been used for several wild ungulate and bird populations), and sires are invariably known with a very high degree of certainty in breeding programmes. There are also algorithms that account for uncertain paternity.\n\nThe pedigrees can be viewed using programs such as Pedigree Viewer , and analyzed with programs such as ASReml, VCE , WOMBAT or the BLUPF90 family of programs .\n\nIn selective breeding of plants and animals, the expected response to selection of a trait with known narrow-sense heritability formula_34 can be estimated using the \"breeder's equation\":\n\nIn this equation, the Response to Selection (R) is defined as the realized average difference between the parent generation and the next generation, and the Selection Differential (S) is defined as the average difference between the parent generation and the selected parents.\n\nFor example, imagine that a plant breeder is involved in a selective breeding project with the aim of increasing the number of kernels per ear of corn. For the sake of argument, let us assume that the average ear of corn in the parent generation has 100 kernels. Let us also assume that the selected parents produce corn with an average of 120 kernels per ear. If h equals 0.5, then the next generation will produce corn with an average of 0.5(120-100) = 10 additional kernels per ear. Therefore, the total number of kernels per ear of corn will equal, on average, 110.\n\nNote that heritability in the above equation is equal to the ratio formula_36 only if the genotype and the environmental noise follow Gaussian distributions \n\nHeritability estimates' prominent critics, such as Steven Rose, Jay Joseph, and Richard Bentall, focus largely on heritability estimates in behavioral sciences and social sciences. Bentall has claimed that such heritability scores are typically calculated counterintuitively to derive numerically high scores, that heritability is misinterpreted as genetic determination, and that this alleged bias distracts from other factors that researches have found more causally important, such as childhood abuse causing later psychosis.\n\nThe controversy over heritability estimates is largely via their basis in twin studies. The scarce success of molecular-genetic studies to corroborate such population-genetic studies' conclusions is the \"missing heritability\" problem. Eric Turkheimer has argued that newer molecular methods have vindicated the conventional interpretation of twin studies, although it remains mostly unclear how to explain the relations between genes and behaviors. According to Turkheimer, both genes and environment are heritable, genetic contribution varies by environment, and a focus on heritability distracts from other important factors. Overall, however, \"heritability\" is a concept widely applicable.\n\n\n\n\n", "id": "155624", "title": "Heritability"}
{"url": "https://en.wikipedia.org/wiki?curid=31907314", "text": "SUI1 protein domain\n\nIn molecular biology, the protein domain SUI1 is a translation initiation factor often found in the fungus,\n\"Saccharomyces cerevisiae\" (Baker's yeast) but it is also found in other eukaryotes and prokaryotes as well as archaea.\n\nSUI1 is a translation initiation factor that directs the ribosome to the translation start site, helped by eIF2 and the initiator Met-tRNA. SUI1 ensures that translation initiation commences from the correct start codon (usually AUG), by stabilizing the pre-initiation complex around the start codon. SUI1 promotes a high initiation fidelity for the AUG codon, discriminating against non-AUG codons. .\n\nThe primary structure of the SUI1 protein is made up of 108 amino acids. The protein domain has a structure made of a seven-bladed beta-propeller and it also contains a C-terminal alpha helix. Homologuess of SUI1 have been found in mammals, insects and plants. SUI1 is also evolutionary related to hypothetical proteins from \"Escherichia coli\" (yciH), \"Haemophilus influenzae\" (HI1225) and \"Methanococcus vannielii\".\n", "id": "31907314", "title": "SUI1 protein domain"}
{"url": "https://en.wikipedia.org/wiki?curid=31907362", "text": "SWAP protein domain\n\nIn molecular biology, the protein domain SWAP is derived from the term Suppressor-of-White-APricot, a splicing regulator from the model organism \"Drosophila melanogaster\". The protein domain is found in regulators that control splicing. It is found in splicing regulatory proteins.\nWhen a gene is expressed the DNA must be transcribed into messenger RNA (mRNA). However, it sometimes contains intervening or interrupting sequences named introns. mRNA splicing helps to remove these sequences, leaving a more favourable sequence. mRNA splicing is an essential event in the post-transcriptional modification process of gene expression. SWAP helps to control this process in all cells except gametes.\n\nThe role of the protein domain SWAP is to control sex-independent pre-mRNA processing in somatic cells, that is, in every cell except the sex cells This includes autoregulation, whereby it regulates the splicing of its own pre-mRNA. The mammalian homologue of SWAP acts as a thyroid hormone regulated gene. This mean it is controlled by the thyroid.\n\nSWAP proteins share a colinearly arrayed series of novel sequence motifs. This means that they have been conserved over time. The SWAP protein in different organisms share some similarity in terms of sequence and may have been related at some point in evolutionary history.\n", "id": "31907362", "title": "SWAP protein domain"}
{"url": "https://en.wikipedia.org/wiki?curid=31905848", "text": "S4 protein domain\n\nIn molecular biology, S4 domain refers to a small RNA-binding protein domain found in a ribosomal protein named S4. The S4 domain is approximately 60-65 amino acid residues long, occurs in a single copy at various positions in different proteins and was originally found in pseudouridine synthases, a bacterial ribosome-associated protein. \n\nThe S4 protein helps to initiate assembly of the 16S rRNA. In this way proteins serve to organise and stabilise the rRNA tertiary structure.\n\nThe function of the S4 domain is to be an RNA-binding protein. S4 is a multifunctional protein, and it must bind to the 16S ribosomal RNA. In addition, the S4 domain binds a complex pseudoknot and represses translation. More specifically, this protein domain delivers nucleotide-modifying enzymes to RNA and to regulates translation through structure specific RNA binding.\n\nThe S4 protein domain is composed of three alpha helices and five beta strands. It is organized as an antiparallel sheet in a Greek key motif.\n", "id": "31905848", "title": "S4 protein domain"}
{"url": "https://en.wikipedia.org/wiki?curid=36771480", "text": "Nutrigenetics\n\nNutrigenetics aims to identify how genetic variation affects response to nutrients. This knowledge can be applied to optimise health, and prevent or treat diseases. The ultimate aim of nutrigenetics is to offer people personalized nutrition based on their genetic makeup.\n\nDue to naturally occurring mutations humans differ in their DNA, which is called variation or polymorphism of DNA. The most common type of DNA polymorphism are SNPs (short for “single nucleotide polymorphism”). SNPs may influence the way individuals absorb, transport, store or metabolize nutrients. This may determine requirements for different nutrients and this assumption forms the basis for nutrigenetic sciences.\nMoreover, different metabolic potential of the human body can imply an advantage in terms of natural selection. For that reason, for example, the ability to digest lactose, the principal sugar of milk, also in adulthood spread in cattle-rising populations.\n\nThe identification of the necessary genotype is carried out by means of a blood analysis or a cheek swab. Subsequently, the DNA is analyzed in different ways.\nA common way to study the genetic data is the so-called “candidate gene approach” when one possible risk gene is identified. After experiments on cell cultures, animals or humans scientists can establish a positive or negative correlation between the expression of this candidate gene and nutritional aspects.\nAnother popular scientific method is a genome-wide association study which also leads to the identification of relevant gene variants.\nIn particular, nutrigenetic analyses are based on the effect of nutritional components on the genome, proteome, metabolome and transcriptome.\n\nA major goal for nutrigenetic researchers is to identify genes that make certain individuals more susceptible to obesity and obesity-related diseases. The thrifty gene hypothesis is an example of a nutrigenetic factor in obesity. The thrifty gene theoretically causes bearers to store high-calorie foods as body fat, a most likely as an evolved protection against starvation during famines. However, the potential \"thrifty genes\" that may be affected by nutritional factors have yet to be identified. Future advancements in nutrigenetics research may potentially prove the existence of thrifty genes as well as find counter-effects in order to prevent obesity and obesity-related diseases.\n\nIn the long run, nutrigenetics should allow nutritionists and physicians to individualize health and diet recommendations. Consequently, preventive medicine, diagnostics and therapies could be optimized. In fact, comparative trials such as a current study from the German Sport University Cologne prove that health counseling based on the results of a nutrigenetic analysis is more successful than conventional diet counseling.\nNutrigenetics offers a lot of potential. In the past only a few nutrigenetic studies have been conducted. Lately, however, more scientists discover this study field for further investigation.\n", "id": "36771480", "title": "Nutrigenetics"}
{"url": "https://en.wikipedia.org/wiki?curid=36436244", "text": "Chemical genetics\n\nChemical genetics is the investigation of the function of proteins and signal transduction pathways in cells by the screening of chemical libraries of small molecules. Chemical genetics is analogous to classical genetic screen where random mutations are introduced in organisms, the phenotype of these mutants is observed, and finally the specific gene mutation (genotype) that produced that phenotype is identified. In chemical genetics, the phenotype is disturbed not by introduction of mutations, but by exposure to small molecule tool compounds. Phenotypic screening of chemical libraries is used to identify drug targets (forward genetics) or to validate those targets in experimental models of disease (reverse genetics). Recent applications of this topic have been implicated in signal transduction, which may play a role in discovering new cancer treatments. Chemical genetics can serve as a unifying study between chemistry and biology.\n\nChemical genetic screens are performed using libraries of small molecules that have known activities or simply diverse chemical structures. These screens can be done in a high-throughput mode, using 96 well-plates, where each well contains cells treated with a unique compound. In addition to cells, \"Xenopus\" or zebrafish embryos can also be screened in 96 well format where compounds are dissolved in the media in which embryos grow. Embryos are let developed until the stage of interest and then the phenotype can be analyzed. Several concentrations can be tested in order to determine the toxic and the optimal concentrations.\n\nAdding compounds to developing embryos allow comprehension of mechanism of action of drugs, their toxicity and developmental processes involving their targets. Chemical screens have been mostly performed on either wild type or transgenic \"Xenopus\" and zebrafish organisms as they produce a large amount of synchronized, fast-to-develop and transparent eggs easy to visually score. The use of chemicals in developmental biology offers two main advantages. Firstly, it is easy to perform high-throughput screen using wide spectrum or specific target compounds and reveal important genes or pathways involved in developmental processes. Secondly, it allows narrowing the time of action of a particular gene. It can also be used as a tool in drug development to test toxicity in whole organism. Procedures such as FETAX (Frog Embryo Teratogenesis Assay – Xenopus) are being developed to implement chemical screenings to test toxicity. Zebrafish and \"Xenopus\" embryos have also been used to identify new drugs targeting a particular gene of interest.\n\n", "id": "36436244", "title": "Chemical genetics"}
{"url": "https://en.wikipedia.org/wiki?curid=36861017", "text": "Terminal crossbreeding\n\nTerminal crossbreeding is a breeding system used in animal production. It involves two (different) breeds of animal that have been crossbred. The female offspring of that cross is then mated with a male (the terminal male) of a third breed, producing the terminal crossbred animal.\n\nThe first crossbreeding may produce a superior animal due to hybrid vigor. Often, this crossbreed is part of a rotational crossbreeding scheme; if it also incorporates terminal crossbreeding, it is then called a rotaterminal system. By mating the crossbreed with a third breed, hybrid vigor may be further enhanced.\n\n", "id": "36861017", "title": "Terminal crossbreeding"}
{"url": "https://en.wikipedia.org/wiki?curid=14032539", "text": "Recombinase-mediated cassette exchange\n\nIn the field of reverse genetics RMCE (recombinase-mediated cassette exchange) is of increasing relevance. Based on the features of site-specific recombination processes (SSRs), the procedure permits the systematic, repeated modification of higher eukaryotic genomes by targeted integration. For RMCE, this is achieved by the clean exchange of a preexisting gene cassette for an analogous cassette carrying the \"gene of interest\" (GOI).\n\nThe genetic modification of mammalian cells is a standard procedure for the production of correctly modified proteins with pharmaceutical relevance. To be successful, the transfer and expression of the transgene has to be highly efficient and should have a largely predictable outcome. Current developments in the field of gene therapy are based on the same principles. Traditional procedures used for transfer of GOIs are not sufficiently reliable, mostly because the relevant epigenetic influences have not been sufficiently explored: transgenes integrate into chromosomes with low efficiency and at \"loci\" that provide only sub-optimal conditions for their expression. As a consequence the newly introduced information may not be realized (expressed), the gene(s) may be lost and/or re-insert and they may render the target cells in unstable state. It is exactly this point where RMCE enters the field. The procedure was introduced in 1994 and it uses the tools yeasts and bacteriophages have evolved for the efficient replication of important genetic information:\n\nMost yeast strains contain circular, plasmid-like DNAs called \"two-micron circles\". The persistence of these entities is granted by a recombinase called \"flippase\" or \"Flp\". Four monomers of this enzyme associate with two identical short (48 bp) target sites, called \"FRT\" (\"flip-recombinase targets\"), resulting in their crossover. The outcome of such a process depends on the relative orientation of the participating FRTs leading to\n\n\nThis spectrum of options could be extended significantly by the generation of spacer mutants for extended 48 bp \"FRT\" sites (cross-hatched half-arrows in Figure 1). Each mutant Fn recombines with an identical mutant Fn with an efficiency equal to the wildtype sites (F x F). A cross-interaction (F x Fn) is strictly prevented by the particular design of these components. This sets the stage for the situation depicted in Figure 1A:\n\n\nFirst applied for the Tyr-recombinase Flp, this novel procedure is not only relevant to the rational construction of biotechnologically significant cell lines, but it also finds increasing use for the systematic generation of stem cells. Stem cells can be used to replace damaged tissue or to generate transgenic animals with largely pre-determined properties.\n\nIt has been previously established that coexpression of both Cre and Flp recombinases catalyzes the exchange of sequences flanked by single loxP and FRT sites integrated into the genome at a random location. However, these studies did not explore whether such an approach could be used to modify conditional mouse alleles carrying single or multiple loxP and FRT sites. dual RMCE (dRMCE; Osterwalder et al., 2010) was recently developed as a re-engineering tool applicable to the vast numbers of mouse conditional alleles that harbor wild-type loxP and FRT sites and therefore are not compatible with conventional RMCE. The general dRMCE strategy takes advantage of the fact that most conditional alleles encode a selection cassette flanked by FRT sites, in addition to loxP sites that flank functionally relevant exons ('floxed' exons). The FRT-flanked selection cassette is in general placed outside the loxP-flanked region, which renders these alleles directly compatible with dRMCE. Simultaneous expression of Cre and Flp recombinases induces cis recombination and formation of the deleted allele, which then serves as a 'docking site' at which to insert the replacement vector by trans recombination. The correctly replaced locus would encode the custom modification and a different drug-selection cassette flanked by single loxP and FRT sites. dRMCE therefore appears as a very efficient tool for targeted re-engineering of thousands of mouse alleles produced by the IKMC consortium.\n\nMultiplexing setups rely on the fact that each F-Fn pair (consisting of a wildtype \"FRT\" site and a mutant called \"n\") or each Fn-Fm pair (consisting of two mutants, \"m\" and \"n\") constitutes a unique \"address\" in the genome. A prerequisite are differences in four out of the eight spacer positions (see Figure 1B). If the difference is below this threshold, some cross-interaction between the mutants may occur leading to a faulty deletion of the sequence between the heterospecific (Fm/Fn or F/Fn) sites.\n\n13 FRT-mutants have meanwhile become available, which permit the establishment of several unique genomic addresses (for instance F-Fn and Fm-Fo). These addresses will be recognized by donor plasmids that have been designed according to the same principles, permitting successive (but also synchronous) modifications at the predetermined \"loci\". These modifications can be driven to completion in case the compatible donor plasmid(s) are provided at an excess (mass-action principles). Figure 2 illustrates one use of the multiplexing principle: the stepwise extension of a coding region in which a basic expression unit is provided with genomic insulators, enhancers, or other \"cis\"-acting elements.\n\nA recent variation of the general concept is based on PhiC31 (an integrase of the Ser-class), which permits introduction of another RMCE target at a secondary site the first RMCE-based modification has occurred. This is due to the fact that each phiC31-catalyzed exchange destroys the attP and attB sites it has addressed converting them to \"att\"R and \"att\"L product sites, respectively. While these changes permit the subsequent mounting of new (and most likely remote) targets, they do not enable addressing several RMCE targets , nor do they permit \"serial RMCE\", i.e. successive, stepwise modifications at a given genomic locus.\n\nIt should be noted that this is different for Flp-RMCE, for which the post-RMCE status of \"FRT\"s corresponds to their initial state. This property enables the intentional, repeated mobilization of a target cassette by the addition of a new donor plasmid with compatible architecture. These \"multiplexing-RMCE\" options open unlimited possibilities for serial- and parallel specific modifications of pre-determined RMCE-targets \n\nGeneration of transgenic knock-out/-in mice and their genetic modification by RMCE.\n\nInsertion of a target cassette in a mammalian host cell line (CHO DG44 in suspension culture) and exchang with an ER stress reporter construct via targeted integration (RMCE).\n\n\n\n", "id": "14032539", "title": "Recombinase-mediated cassette exchange"}
{"url": "https://en.wikipedia.org/wiki?curid=6279588", "text": "Extranuclear inheritance\n\nExtranuclear inheritance or cytoplasmic inheritance is the transmission of genes that occur outside the nucleus. It is found in most eukaryotes and is commonly known to occur in cytoplasmic organelles such as mitochondria and chloroplasts or from cellular parasites like viruses or bacteria.\n\nMitochondria are organelles which function to transform energy as a result of cellular respiration. Chloroplasts are organelles which function to produce sugars via photosynthesis in plants and algae. The genes located in mitochondria and chloroplasts are very important for proper cellular function, yet the genomes replicate independently of the DNA located in the nucleus, which is typically arranged in chromosomes that only replicate one time preceding cellular division. The extranuclear genomes of mitochondria and chloroplasts however replicate independently of cell division. They replicate in response to a cell's increasing energy needs which adjust during that cell's lifespan. Since they replicate independently, genomic recombination of these genomes is rarely found in offspring contrary to nuclear genomes, in which recombination is common.\nMitochondrial disease are received from the mother, fathers don't as sperm do not contribute (but a sperm contains a mitochondrion for its energy production).\n\nExtranuclear transmission of viral genomes and symbiotic bacteria is also possible. An example of viral genome transmission is perinatal transmission. This occurs from mother to fetus during the perinatal period, which begins before birth and ends about 1 month after birth. During this time viral material may be passed from mother to child in the bloodstream or breastmilk. This is of particular concern with mothers carrying HIV or Hepatitis C viruses. Symbiotic cytoplasmic bacteria are also inherited in organisms such as insects and protists.\n\nThree general types of extranuclear inheritance exist.\n\n", "id": "6279588", "title": "Extranuclear inheritance"}
{"url": "https://en.wikipedia.org/wiki?curid=37084359", "text": "List of genetic codes\n\nWhile there is a lot of commonality, different parts of the tree of life use slightly different genetic codes.\nNotably the mitochondrial codes vary.\n\nWhen translating from genome to protein, the use of the correct genetic code is essential.\n\n\nThe alternative translation tables (2 to 31) involve 62 codon reassignments that are recapitulated in the list of all known alternative codons.\n\nThree translation tables do have a peculiar status:\n\nOther mechanisms also play a part in protein biosynthesis, such as post-transcriptional modification.\n\n", "id": "37084359", "title": "List of genetic codes"}
{"url": "https://en.wikipedia.org/wiki?curid=12139474", "text": "Histone-modifying enzymes\n\nThe packaging of the eukaryotic genome into highly condensed chromatin makes it inaccessible to the factors required for gene transcription, DNA replication, recombination and repair. Eukaryotes have developed intricate mechanisms to overcome this repressive barrier imposed by the chromatin. The nucleosome is composed of an octamer of the four core histones (H3, H4, H2A, H2B) around which 146 base pairs of DNA are wrapped. Several distinct classes of enzyme can modify histones at multiple sites. The figure on the right enlists those histone-modifying enzymes whose specificity has been determined. There are at least eight distinct types of modifications found on histones (see the legend box on the top left of the figure). Enzymes have been identified for acetylation,\nmethylation, demethylation, phosphorylation, \nubiquitination, sumoylation,\nADP-ribosylation, \ndeimination, and \nproline isomerization. For a detailed example of histone modifications in transcription regulation see RNA polymerase control by chromatin structure and table \"Examples of histone modifications in transcriptional regulation\".\n\n", "id": "12139474", "title": "Histone-modifying enzymes"}
{"url": "https://en.wikipedia.org/wiki?curid=37518608", "text": "Helicos single molecule fluorescent sequencing\n\nThe Helicos Genetic Analysis System platform was the first commercial NGS (Next Generation Sequencing) implementation to use the principle of single molecule fluorescent sequencing, a method of identifying the exact sequence of a piece of DNA. It was marketed by the now defunct Helicos Biosciences.\n\nThe fragments of DNA molecules are first hybridized in place on disposable glass flow cells. Fluorescent nucleotides are then added one-by-one, with a terminating nucleotide used to pause the process until an image has been captured. From the image, one nucleotide from each DNA sequence can be determined. The fluorescent molecule is then cut away, and the process is repeated until the fragments have been completely sequenced.\n\nThis sequencing method and equipment were used to sequence the genome of the M13 bacteriophage.\n\nThe Helicos Genetic Analysis System is capable of sequencing nucleic acids, from several nucleotides to several thousand nucleotides. However, the yield of sequences per unit mass is dependent on the number of 3’ end hydroxyl groups, and thus having relatively short templates for sequencing is more efficient than having long templates. Helicos recommends a length less than 1000nt (nucleotides), optimally about 100-200nt. Long fragments can be cleaved by shearing the DNA (the recommended approach), or restriction enzymes. Short fragments are removed to improve yield.\n\nDNA samples are hybridized to a primer immobilized on a flow cell for sequencing, so it is usually necessary to generate a nucleic acid with an end compatible for hybridization to those surfaces. The target sequence attached to the flow cell surface could, in theory, be any sequence which can be synthesized, but, in practice, the standard commercially available flow cell is oligo(dT)50. To be compatible with the oligo(dT)50 primer on the flow cell surface, it is necessary to generate a poly(dA) tail of at least 50 nt at the 3’ end of the molecule to be sequenced. Because the fill and lock step will fill in excess A’s but not excess T’s, it is desirable for the A tail to be at least as long as oligo(dT) on the surface. Generation of a 3’ poly(dA) tail can be accomplished with a variety of different ligases or polymerases. If there is sufficient DNA to measure both mass and average length, it is possible to determine the proper amount of dATP to be added to generate poly(dA) tails 90 to 200 nucleotides long. To generate tails of this length, it is first necessary to estimate how many 3’ ends there are in the sample and then use the right ratio of DNA, dATP, and terminal transferase to obtain the optimal size range of tails.\n\nIf the tailed DNA targeted for sequencing is hybridized to the flow cell directly after tailing, it would have a free 3’ hydroxyl that could be extended in the sequencing reaction just like the surface-bound primer and potentially confuse the sequence determination. Thus, prior to sequencing, it is also necessary to block the 3’ ends of the molecules to be sequenced. Any 3’ end treatment that makes the molecule unsuitable for extension can be used. Typically, tailed molecules are blocked using terminal transferase and a dideoxynucleotide, but any treatment that leaves a 3’ phosphate or other modification that prevents extension can be similarly effective.\n\nThe single molecule fluorescent sequencing is carried out on a glass flow cell with 25 channels for the same or different samples. The system can be run with either one or two flow cells at a time. In the standard configuration, each channel is equivalent and holds approximately 8 μl. Samples are generally loaded with higher volume (usually 20 μl or more) to ensure even hybridization along the length of the flow cell. Samples are inserted into the flow cell via the sample loader included with the overall system. Each channel is individually addressable, and sample is applied using a vacuum. Hybridization to the flow cell is typically carried out at 55◦C for 1 hr.\n\nGenerally, samples for sequencing are prepared in such a way that the poly(A) tail is longer than the oligo(dT)50 on the surface of the flow cell. To avoid sequencing the unpaired A residues, a fill and lock treatment is needed. After hybridization, the temperature is lowered to 37◦C, and then dTTP and Virtual Terminator nucleotides corresponding to dATP, dCTP, and dGTP are added along with DNA polymerase. Virtual terminator nucleotides incorporate opposite the complementary base and prevent further incorporation because of the chemical structure appended to the nucleotide. Thus, all of the unpaired dAs present in the poly(A) tail are filled in with TTP. The hybridized molecule is locked in place when the polymerase encounters the first non-A residue and inserts the appropriate virtual terminator nucleotide. Because every DNA molecule should now have a dye attached, an image will include all molecules capable of nucleotide incorporation. Also, because the label could correspond to any base, no sequence information is obtained at this stage. Thus, for most molecules, sequencing commences with the second base of the original molecule.\n\nIn order to sequence the hybridized DNAs, it is first necessary to cleave off the fluorescent dye and terminator moieties present on the virtual terminator nucleotides. The current generation of nucleotides is synthesized with a disulfide linkage that can be rapidly and completely cleaved. Following cleavage, the now-separated fluorescent dyes are washed away and then new polymerase and a single fluorescent nucleotide are added. After excitation of the fluorescent moiety by the system laser, another image is taken, and, on a standard sequencing run, this cyclic process is repeated 120 times. The number of sequencing cycles is user adjustable and can be modified depending on user needs for run time and length of read. During a standard run, two 25-channel flow cells are used, with each flow cell alternating between the chemistry cycle and the imaging cycle. \n\nDuring the imaging process, four lasers illuminate 1100 Fields of View (FOV) per channel with pictures taken by four CCD (Charge-coupled device) cameras via a confocal microscope. Though single molecules are visualized, multiple photon emissions are registered for each molecule, with the time spent at each FOV dependent on the brightness of the dye in the particular nucleotide as well as camera speed and detection efficiency. At the present time, the imaging process is the rate-determining step, and run time could be reduced at the expense of throughput by reducing the number of FOV per channel.\n\nUnder optimal conditions, for a standard 120-cycle, 1100 field-of view run, 12,000,000 to 20,000,000 reads that are 25 nucleotides or longer and align to the reference genome should be expected from each channel, for a total of up to 1,000,000,000 aligned reads and 35 Gb of sequence from each run. A full run takes up to 8 days to complete.\n\n\n\n", "id": "37518608", "title": "Helicos single molecule fluorescent sequencing"}
{"url": "https://en.wikipedia.org/wiki?curid=35008525", "text": "SOS chromotest\n\nThe SOS chromotest is a biological assay to assess the genotoxic potential of chemical compounds. The test is a colorimetric assay which measures the expression of genes induced by genotoxic agents in \"Escherichia coli\", by means of a fusion with the structural gene for β-galactosidase. The test is performed over a few hours in columns of a 96-well microplate with increasing concentrations of test samples. This test was developed as a practical complement or alternative to the traditional Ames test assay for genotoxicity, which involves growing bacteria on agar plates and comparing natural mutation rates to mutation rates of bacteria exposed to potentially mutagenic compounds or samples. The SOS chromotest is comparable in accuracy and sensitivity to established methods such as the Ames test and is a useful tool to screen genotoxic compounds, which could prove carcinogenic in humans, in order to single out chemicals for further in-depth analysis.\n\nAs with other bacterial gentoxicity and mutagenicity assays, compounds requiring metabolic activation for activity can be investigated with the addition of S9 microsomal rat liver extract.\n\nThe SOS response plays a central role in the response of \"E. coli\" to genotoxic compounds because it responds to a wide array of chemical agents. Triggering of this system can and has been used as an early sign of DNA damage. Two genes play a key role in the SOS response: lexA encodes a repressor for all the genes in the system, and recA encodes a protein able to cleave the LexA repressor upon activation by an SOS inducing signal (caused in this case by the presence of a genotoxic compound). Although the exact mechanism of the SOS response is still unknown, it is induced when DNA lesions perturb or stop DNA replication. .\n\nVarious end-points are possible indicators of the triggering of the SOS system; activation of the RecA protein, cleavage of the LexA repressor, expression of any of the SOS genes, etc. One of the simplest assays consists of monitoring the expression of an SOS gene by means of fusion with lacZ, the structural gene for \"E. coli\" β-galactosidase.\n\nThe SOS chromotest consists of incubating the \"E. coli\" with increasing concentrations of the chemical to be tested. After allowing time for protein synthesis, β-galactosidase activity is assayed using a simple colorimetric assay. By including a lactose analog which yields a colored compound upon degradation, an easily observable or quantifiable change in colour is used as a metric. Since the chemical tested may inhibit protein synthesis at higher concentrations, which would lead to an underestimation of B-galactosidase induction, alkaline phosphatase is assayed simultaneously with β-galactosidase in order to scale the data to survivability of the cells.\n\nThe assay can easily be completed in a number of hours. If using a micro-plate reader, the test is quantitative and dose-response curves have an initial linear region. The slope of this linear region allows unequivocal association of each compound with a single parameter, the SOS-inducing potency (SOSIP), which reflects the inducing activity of the compound.\nThis assay provides both a qualitative (visible observation of colour gradient) for screening purposes, or quantitative measurement (spectrophotometry) for calculation of commonly accepted metrics. In the quantitative assay, the dose-response (colour production linked to beta-galactosidase production) of a compound is plotted, with the slope of the initial linear region used as a universal parameter, the SOS-inducing potency (SOSIP), which reflects the ability of a compound to induce the SOS response (measured indirectly through production of beta-galactosidase and the breakdown of a lactose analog). Typically, the lactose analog is X-Gal, which produces a blue colour when cleaved by beta-galactosidase. The dose response is also scaled by the survival of cells, measured through the breakdown of alkaline phosphatase (which produces a yellow colour), allowing for the calculation of the SOSIP.\n\nAlthough the SOSIP is a concentration-based metric, the same method can be used for complex environmental mixtures where the concentration or even compounds of interest are unknown. Through the calculation of the intermediate SOS-induction factor (SOSIF), which can be plotted against dilution in the same manner in order to give an illustration of the dose-response without analytical measurements of samples beforehand.\n\nTHE SOS chromotest is considered to be the most simple and rapid short-term test for genotoxicity. It serves as a useful and cost effective complement to the traditional Ames test for a number of reasons. Firstly, because of its simplicity and rapidity, the SOS chromotest may be used as a screening test for a large number of potentially genotoxic compounds. Secondly, it may allow the detection of genotoxic chemicals which yield false negatives in the Ames test (such as estradiol, a compound of growing concern). Thirdly, it can prove an effective method to discriminate false positive results in the Ames test.\n", "id": "35008525", "title": "SOS chromotest"}
{"url": "https://en.wikipedia.org/wiki?curid=30860403", "text": "Geneticist\n\nA geneticist is a biologist who studies genetics, the science of genes, heredity, and variation of organisms.\n\nA geneticist can be employed as a scientist or lecturer. Geneticists perform general research on genetic processes as well as development of genetic technologies to aid in the medicine and agriculture industries. Some geneticists perform experiments in model organisms such as Drosophila, C. elegans, Zebrafish, rodents or Humans and analyze data to interpret the inheritance of biological traits. A geneticist can be a scientist who has earned a Ph.D in Genetics or a physician (who has earned any of the following medical degrees: MBBS/MBChB (non-U.S.), D.O. (U.S.-only), or M.D.) who has been trained in genetics as a specialization. They evaluate, diagnose, and manage patients with hereditary conditions or congenital malformations, genetic risk calculations, and mutation analysis as well as refer patients to other medical specialties. The geneticist carries out studies, tests and counsels patients with genetic disorders.\n\nGeneticists participate in courses from many areas, such as biology, chemistry, physics, microbiology, cell biology, bioinformatics, and mathematics. They also participate in more specific genetics courses such as molecular genetics, transmission genetics, population genetics, quantitative genetics, ecological genetics, and genomics.\n\nGeneticists can work in many different fields, doing a variety of jobs. There are many careers for geneticists in medicine, agriculture, wildlife, general sciences, or many other fields.\n\nListed below are a few examples of careers a geneticist may pursue. \n", "id": "30860403", "title": "Geneticist"}
