{"url": "https://en.wikipedia.org/wiki?curid=31562046", "text": "Synthetic ion channels\n\nSynthetic ion channels are \"de novo\" chemical compounds that insert into lipid bilayers, form pores, and allow ions to flow from one side to the other. They are man-made analogues of natural ion channels, and are thus also known as artificial ion channels. Compared to biological channels, they usually allow fluxes of similar magnitude but are \n\nSynthetic channels, like natural channels, are usually characterized by a combination of single-molecule (e.g., voltage-clamp of planar bilayers) and ensemble techniques (flux in vesicles). The study of synthetic ion channels can potentially lead to new single-molecule sensing technologies as well as new therapeutics.\n\nWhile semi-synthetic ion channels, often based on modified peptidic channels like gramicidin, had been prepared since the 1970s, the first attempt to prepare a synthetic ion channel was made in 1982 using a substituted β-cyclodextrin.\n\nInspired by gramicidin, this molecule was designed to be a barrel-shaped entity spanning a single leaflet of a bilayer membrane, becoming \"active\" only when two molecules in opposite leaflets come together in an end-to-end fashion. While the compound does induce ion-fluxes in vesicles, the data does not unambiguously show \"channel\" formation (as opposed to other transport mechanisms; see Mechanism).\n\nNa transport by such channels was first reported by two groups of investigators in 1989–1990.\n\nWith the adoption of voltage clamp technique to synthetic channel research in the early 1990s, researchers were able to observe quantized electrical activities from synthetic molecules, often considered the signature evidence for ion channels. This led to a sustained increase in research activity over the next two decades. In 2009, over 25 peer-reviewed papers were published on the topic, and a series of comprehensive reviews are available.\n\nPassive transport of ions across a membrane can take place by three main mechanisms: by ferrying, through defects in a disrupted membrane, or through a defined trajectory; these corresponds to \"ionophore\", \"detergent\", and \"ion channel\" transporters. While synthetic ion channel research attempts to prepare compounds that show conductance \"via\" a defined path, the elucidation of mechanism is difficult and seldom unambiguous. The two main methods of characterization both have their drawbacks, and as a consequence, often function is defined but mechanism presumed.\nOne line of evidence for ion transport comes from macroscopic examination of statistical ensembles. All these techniques use intact vesicles with an entrapped volume, with ion channel activities reported by different spectroscopic methods.\n\nIn a typical case, a dye is entrapped within the population of vesicles. This dye is selected to be respond colorimetrically or fluorometrically to the presence of an ion; this ion is typically absent from the inside of the vesicle but present in the outside. Without an ion transporter, the lipid bilayer as a kinetic barrier to block ion flux, and the dye remains \"dark\" indefinitely.\n\nAs an ion transporter allows ions on the outside to diffuse in, its addition will affect the color/fluorescence property of the dye. By macroscopically monitoring the dye's properties over time, and controlling outside factors, the ability of a compound to act as an ion transporter can be measured.\n\nObserving ion transport, however, does not pin down ion channel as the mechanism. Any class of transporter can lead to the same observation, and additional corroborating evidence is usually required. Sophisticated experiments intended to probe selectivity, gating, and other channel parameters have been developed over the past two decades and recently summarized.\n\nAn alternative to the ensemble-based method described above is the voltage-clamp experiment. In a voltage-clamp experiment, two compartments of electrolyte are divided by an aperture, usually between 5-250 micrometres in diameter. A lipid bilayer is painted across this aperture, thus electrically separating the compartments; the molecular nature can be ascertained by measuring its capacitance.\n\nUpon the addition of an (ideal) ion channel, a defined path between the two compartments is formed. Through this pore, ions flow down the potential and electrochemical gradient rapidly (>10/second), the maximum flux limited by the geometry and dimensions of the pore. At some later instant the pore may close or collapse, whereupon the current returns to zero. This open-state current, originating and amplified from a single-molecule event, is typically on the order of pA to nA, with time-resolution of approx. millisecond. Ideal or close-to-ideal events is termed \"\"square-tops\"\" in the literature, and have been considered as signature for a channel-based mechanism.\n\nIt is notable that the events observed at this scale are truly stochastic - that is, they are the result of random molecular collision and conformation changes. As the membrane area is much larger than that of a pore, multiple copies may open and close independently of one another, giving rise to the staircase like appearance (Panel C in figure); these ideal events are often modelled as Markov processes.\n\nBy using the \"activity grid notation\", synthetic ion channels studied with the voltage-clamp method during the period 1982-2010 have been critically reviewed. While the ideal traces are most frequently analyzed and reported in the literature, many records are decidedly non-ideal, with a subset was shown to be fractal. Developing methods for analyzing these non-ideal traces and clarifying their relationship to transport mechanism is an area of contemporary research.\nA diverse and large pool of synthetic molecules have been reported to act as ion transporters in lipid membranes. A selection is described here to demonstrate the breadth of feasible \"structures\" and attainable \"functions\". Comprehensive reviews for the literature up to 2010 are available in a tripartite series.\n\nMost (but not all; see minimalist channels) synthetic channels have chemical structures substantially larger than typical small molecules (molecular weights ~1-5kDa). This originates from the need to be amphiphilic, that is, have both sufficient hydrophobic portions to allow partitioning into lipid bilayer, as well as polar or charged \"headgroups\" to assert a defined orientation and geometry with respect to the membrane.\n\nIon channels containing calixarenes of ring size 3 and 4 have both been reported. For calix[4]arene, two conformations are accessible, and examples of both \"1,3-alt\" and \"cone\" conformation have been developed.\n\nThe first synthetic ion channel was constructed by partial substitution on the primary rim of β-cyclodextrin. Other substituted β-cyclodextrins have since been reported, including thiol-modified cyclodextrins, an anion-selective oligobutylene channel, and various poly-ethyleneoxide linked starburst oligomers. Structure-activity relationships for a large suite of cyclodextrin \"half-channels\" prepared by \"click\"-chemistry has been recently reported.\n\nAlternating D/L peptide macrocycles are known to self-aggregate into nanotubes, and the resulting nanotubes have been shown to act as ion channels in lipid membranes.\n\nOther architectures use peptide helices as a scaffold to attach other functionalities, such as crown ethers of different sizes. The property of these peptide-crown channels depend strongly on the identity of the capping end-groups.\n\nSemi-synthetic bio-hybrid channels constructed by modifications of natural ion channels had been constructed. Leveraging modern synthetic organic chemistry, these allows pinpoint modifications of existing structures to either elucidate their transport mechanisms or to graft on new functionalities.\n\nGramicidin and alamethicin had been popular starting points for selective modifications. The above diagram illustrates one example, where a crown-ether was fixed across the mouth of the ion-passing portal. This channel shows discrete conductance but different ion selectivity than wild type gramicidin in voltage-clamp experiments.\n\nWhile modification of large protein channels using mutagenesis are generally considered out of the scope of \"synthetic\" channels, the demarcation is not sharp, as supramolecular or covalent bonding of cyclodextrins to alpha-hemolysin demonstrates.\n\nAn ion channel can be characterized by its \"opening characteristics\", \"ion selectivity\", and \"control of flux\" (gating). Many synthetic ion channels show unique properties in one or more of these aspects.\n\nAn \"ion-channel forming\" molecule can often show multiple types of conductance activities in planar bilayer membranes. Each of these modes of action can be characterized by their\n\n\nThese \"events\" are not necessarily uniform throughout their durations, and as a result a variety of shapes of conducting traces are possible.\n\nThe majority of synthetic ion channels follow an Eisenman I sequence (Cs > Rb > K > Na » Li) in their selectivity for alkali metal cations, suggesting that the origin of the selectivity is governed by the difference in energy required to remove water from a fully hydrated cation. A few synthetic channels show other patterns of ion selectivity, and only a single instance in which a synthetic channel following the opposite selectivity sequence (Eisenman XI; Cs < Rb < K < Na « Li) had been reported.\n\nMost synthetic channels are Ohmic in conductance, that is, the current passed (both individually and as an ensemble) is proportional to the potential across the membrane. Some rare channels, however, show current-voltage characteristics that is non-linear. Specifically, two different types of non-Ohmic conductance are known: \n\nThe former requires asymmetry with respect to the mid-plane of the lipid bilayer, and is realized often by introducing an overall molecular dipole. The latter, demonstrated in natural channels such as alamethicin, is rarely encountered in synthetic ion channels. They may be related to lipid ion channels, but to date their mechanism remains elusive.\n\nCertain synthetic ion channels have conductances that can be modulated by additional of external chemicals. Both up-modulation (channels are turned on by ligand) and down-modulation (channels are turned off by ligands) are known: different mechanisms, including formation of supramolecular aggregates, as well as inter- and intramolecular blockage.\n\nRegulatory elements that responds to other signals are known; examples include photomodulated conductances as well as \"thermal switches\" constructed by isomerization of the carbamate group. To date, no mechanosensitive synthetic ion channels have been reported.\n", "id": "31562046", "title": "Synthetic ion channels"}
{"url": "https://en.wikipedia.org/wiki?curid=505575", "text": "Phosphodiester bond\n\nA phosphodiester bond occurs when exactly two of the hydrogen groups in phosphoric acid react with hydroxyl groups on other molecules to form two ester bonds.\n\nPhosphodiester bonds are central to all life on Earth, as they make up the backbone of the strands of nucleic acid. In DNA and RNA, the phosphodiester bond is the linkage between the 3' carbon atom of one sugar molecule and the 5' carbon atom of another, deoxyribose in DNA and ribose in RNA. Strong covalent bonds form between the phosphate group and two 5-carbon ring carbohydrates (pentoses) over two ester bonds.\n\nThe phosphate groups in the phosphodiester bond are negatively charged. Because the phosphate groups have a pK near 0, they are negatively charged at pH 7 . This repulsion forces the phosphates to take opposite sides of the DNA strands and is neutralized by proteins (histones), metal ions such as magnesium, and polyamines.\n\nIn order for the phosphodiester bond to be formed and the nucleotides to be joined, the tri-phosphate or di-phosphate forms of the nucleotide building blocks are broken apart to give off energy required to drive the enzyme-catalyzed reaction. When a single phosphate or two phosphates known as pyrophosphates break away and catalyze the reaction, the phosphodiester bond is formed.\n\nHydrolysis of phosphodiester bonds can be catalyzed by the action of phosphodiesterases which play an important role in repairing DNA sequences.\n\nThe phosphodiester linkage between two ribonucleotides can be broken by alkaline hydrolysis, whereas the linkage between two deoxyribonucleotides is more stable under these conditions. The relative ease of RNA hydrolysis is an effect of the presence of the 2' hydroxyl group.\n\nA phosphodiesterase is an enzyme that catalyzes the hydrolysis of phosphodiester bonds, for instance a bond in a molecule of cyclic AMP or cyclic GMP.\n\nAn enzyme that plays an important role in the repair of oxidative DNA damage is the 3'-phosphodiesterase.\n\nDuring the replication of DNA, there is a hole between the phosphates in the backbone left by DNA polymerase I. DNA ligase is able to form a phosphodiester bond between the nucleotides.\n\n", "id": "505575", "title": "Phosphodiester bond"}
{"url": "https://en.wikipedia.org/wiki?curid=33891046", "text": "Disease gene identification\n\nDisease gene identification is a process by which scientists identify the mutant genotypes responsible for an inherited genetic disorder. Mutations in these genes can include single nucleotide substitutions, single nucleotide additions/deletions, deletion of the entire gene, and other genetic abnormalities.\n\nKnowledge of which genes (when non-functional) cause which disorders will simplify diagnosis of patients and provide insights into the functional characteristics of the mutation. The advent of modern-day high-throughput sequencing technologies combined with insights provided from the growing field of genomics is resulting in more rapid disease gene identification, thus allowing scientists to identify more complex mutations.\n\nDisease gene identification techniques often follow the same overall procedure. DNA is first collected from several patients who are believed to have the same genetic disease. Then, their DNA samples are analyzed and screened to determine probable regions where the mutation could potentially reside. These techniques are mentioned below. These probable regions are then lined-up with one another and the overlapping region should contain the mutant gene. If enough of the genome sequence is known, that region is searched for candidate genes. Coding regions of these genes are then sequenced until a mutation is discovered or another patient is discovered, in which case the analysis can be repeated, potentially narrowing down the region of interest. \nThe differences between most disease gene identification procedures are in the second step (where DNA samples are analyzed and screened to determine regions in which the mutation could reside).\n\nWithout the aid of the whole-genome sequences, pre-genomics investigations looked at select regions of the genome, often with only minimal knowledge of the gene sequences they were looking at. Genetic techniques capable of providing this sort of information include Restriction Fragment Length Polymorphism (RFLP) analysis and microsatellite analysis.\n\nLoss of heterozygosity (LOH) is a technique that can only be used to compare two samples from the same individual. LOH analysis is often used when identifying cancer-causing oncogenes in that one sample consists of (mutant) tumor DNA and the other (control) sample consists of genomic DNA from non-cancerous cells from the same individual. RFLPs and microsatellite markers provide patterns of DNA polymorphisms, which can be interpreted as residing in a heterozygous region or a homozygous region of the genome. Provided that all individuals are affected with the same disease resulting from a manifestation of a deletion of a single copy of the same gene, all individuals will contain one region where their control sample is heterozygous but the mutant sample is homozygous - this region will contain the disease gene.\n\nWith the advent of modern laboratory techniques such as High-throughput sequencing and software capable of genome-wide analysis, sequence acquisition has become increasingly less expensive and time-consuming, thus providing significant benefits to science in the form of more efficient disease gene identification techniques.\n\nIdentity by descent (IBD) mapping generally uses single nucleotide polymorphism (SNP) arrays to survey known polymorphic sites throughout the genome of affected individuals and their parents and/or siblings, both affected and unaffected. While these SNPs probably do not cause the disease, they provide valuable insight into the makeup of the genomes in question. A region of the genome is considered identical by descent if contiguous SNPs share the same genotype. When comparing an affected individual to his/her affected sibling, all identical regions are recorded (ex. Shaded in red in above figure). Given that an affected sibling and an unaffected sibling do not have the same disease phenotype, their DNA must by definition be different (barring the presence of a genetic or environmental modifier). Thus, the IBD mapping results can be further supplemented by removing any regions that are identical in both affected individuals and unaffected siblings. This is then repeated for multiple families, thus generating a small, overlapping fragment, which theoretically contains the disease gene.\n\nHomozygosity/Autozygosity mapping is a powerful technique, but is only valid when searching for a mutation segregating within a small, closed population. Such a small population, possibly created by the founder effect, will have a limited gene pool, and thus any inherited disease will probably be a result of two copies of the same mutation segregating on the same haplotype. Since affected individuals will probably be homozygous in the regions, looking at SNPs in a region is an adequate marker of regions of homozygosity and heterozygosity. Modern day SNP arrays are used to survey the genome and identify large regions of homozygosity. Homozygous blocks in the genomes of affected individuals can then be laid on top of each other, and the overlapping region should contain the disease gene.\n\nThis analysis is often extended by analyzing autozygosity, an extension of homozygosity, in the genomes of affected individuals. This can be accomplished by plotting a cumulative LOD score alongside the overlaid blocks of homozygosity. By taking into consideration the population allele frequencies for all SNPs via autozygosity mapping, the results of homozygosity can be confirmed. Furthermore, if two suspicious regions appear as a result of homozygosity mapping, autozygosity mapping may be able to distinguish between the two (ex. If one block of homozygosity is a result of a very non-diverse region of the genome, the LOD score will be very low).\n\nTools for Homozygosity Mapping\n\nGenome-wide knockdown studies are an example of the reverse genetics made possible by the acquisition of whole genome sequences, and the advent of genomics and gene-silencing technologies, mainly siRNA and deletion mapping. Genome-wide knockdown studies involve systematic knockdown or deletion of genes or segments of the genome. This is generally done in prokaryotes or in a tissue culture environment due to the massive number of knockdowns that must be performed. After the systematic knockout is completed (and possibly confirmed by mRNA expression analysis), the phenotypic results of the knockdown/knockout can be observed. Observation parameters can be selected to target a highly specific phenotype. The resulting dataset is then be queried for samples which exhibit phenotypes matching the disease in question – the gene(s) knocked down/out in said samples can then be considered candidate disease genes for the individual in question.\n\nWhole exome sequencing is a brute-force approach that involves using modern day sequencing technology and DNA sequence assembly tools to piece together all coding portions of the genome. The sequence is then compared to a reference genome and any differences are noted. After filtering out all known benign polymorphisms, synonymous changes, and intronic changes (that do not affect splice sites), only potentially pathogenic variants will be left. This technique can be combined with other techniques to further exclude potentially pathogenic variants should more than one be identified.\n", "id": "33891046", "title": "Disease gene identification"}
{"url": "https://en.wikipedia.org/wiki?curid=33785040", "text": "Ultrasensitivity\n\nIn molecular biology, ultrasensitivity describes an output response that is more sensitive to stimulus change than the hyperbolic Michaelis-Menten response. Ultrasensitivity is one of the biochemical switches in the cell cycle and has been implicated in a number of important cellular events, including exiting G2 cell cycle arrests in \"Xenopus laevis\" oocytes, a stage to which the cell or organism would not want to return.\n\nUltrasensitivity is a cellular system which triggers entry into a different cellular state. Ultrasensitivity gives a small response to first input signal, but an increase in the input signal produces higher and higher levels of output. This acts to filter out noise, as small stimuli and threshold concentrations of the stimulus (input signal) is necessary for the trigger which allows the system to get activated quickly. Ultrasensitive responses are represented by sigmoidal graphs, which resemble cooperativity. The quantification of ultrasensitivity is often performed approximately by the Hill equation (biochemistry):\n\nWhere Hill's coefficient (n) may represent quantitative measure of ultrasensitive response.\n\nZero-order ultrasensitivity was first described by Albert Goldbeter and Daniel Koshland, Jr in 1981 in a paper in the Proceedings of the National Academy of Sciences. They showed using mathematical modeling that modification of enzymes operating outside of first order kinetics required only small changes in the concentration of the effector to produce larger changes in the amount of modified protein. This amplification provided added sensitivity in biological control, and implicated the importance of this in many biological systems.\n\nMany biological processes are binary (ON-OFF), such as cell fate decisions, metabolic states, and signaling pathways. Ultrasensitivity is a switch that helps decision-making in such biological processes. For example, in apoptotic process, a model showed that a positive feedback of inhibition of caspase 3 (Casp3) and Casp9 by inhibitors of apoptosis can bring about ultrasensitivity (bistability). This positive feedback cooperates with Casp3-mediated feedback cleavage of Casp9 to generate irreversibility in caspase activation (switch ON), which leads to cell apoptosis. Another model also showed similar but different positive feedback controls in Bcl-2 family proteins in apoptotic process.\n\nRecently, Jeyeraman et al. have proposed that the phenomenon of ultrasensitivity may be further subdivided into three sub-regimes, separated by sharp stimulus threshold values: OFF, OFF-ON-OFF, and ON. Based on their model, they proposed that this sub-regime of ultrasensitivity, OFF-ON-OFF, is like a switch-like adaption which can be accomplished by coupling N phosphorylation–dephosphorylation cycles unidirectionally, without any explicit feedback loops.\n\nOther recent work has emphasized that not only is the topology of networks important for creating ultrasensitivity responses, but that their composition (enzymes vs. transcription factors) strongly affects whether they will exhibit robust ultrasensitivity. Mathematical modeling suggests for a broad array of network topologies that a combination of enzymes and transcription factors tends to provide more robust ultransensitivity than that seen in networks composed entirely of transcription factors or composed entirely of enzymes.\n\nUltrasensitivity can be achieved through several mechanisms: \n\nMultipstep ultrasensitivity occurs when a single effector acts on several steps in a cascade. Successive cascade signals can result in higher levels of noise being introduced into the signal that can interfere with the final output. This is especially relevant for large cascades, such as the flagellar regulatory system in which the master regulator signal is transmitted through multiple intermediate regulators before activating transcription. Cascade ultrasensitivity can reduce noise and therefore require less input for activation. Additionally, multiple phosphorylation events are an example of ultrasensitivity. Recent modeling has shown that multiple phosphorylation sites on membrane proteins could serve to locally saturate enzyme activity. Proteins at the membrane are greatly reduced in mobility compared to those in the cytoplasm, this means that a membrane tethered enzyme acting upon a membrane protein will take longer to diffuse away. With the addition of multiple phosphorylation sites upon the membrane substrate, the enzyme can - by a combination of increased local concentration of enzyme and increased substrates - quickly reach saturation.\n\nBuffering Mechanisms such as molecular titration can generate ultrasensitivity. \"In vitro\", this can be observed for the simple mechanism:\nWhere the monomeric form of A is active and it can be inactivated by binding B to form the heterodimer AB. When the concentration of formula_2 (formula_2 = [B] + [AB]) is much greater than the formula_4, this system exhibits a threshold determined by the concentration of formula_2. At concentrations of formula_6 (formula_6 = [A] +[AB]), lower than formula_2, B acts as a buffer to free A and nearly all A will be found as AB. However, at the equivalence point, when formula_6 ≈ formula_2, formula_2 can no longer buffer the increase in formula_6, so a small increase in formula_6 causes a large increase in A. The strength of the ultrasensitivity of [A] to changes in formula_6 is determined by formula_2/formula_4. Ultrasensitivity occurs when this ratio is greater than one and is increased as the ratio increases. Above the equivalence point, formula_6 and A are again linearly related. \n\"In vivo\", the synthesis of A and B as well as the degradation of all three components complicates generation of ultrasensitivity. If the synthesis rates of A and B are equal this system still exhibits ultrasensitivity at the equivalence point.\n\nOne example of a buffering mechanism is protein sequestration, which is a common mechanism found in signalling and regulatory networks. In 2009, Buchler and Cross constructed a synthetic genetic network that was regulated by protein sequestration of a transcriptional activator by a dominant-negative inhibitor. They showed that this system results in a flexibile ultrasensitive response in gene expression. It is flexible in that the degree of ultrasensitivity can be altered by changing expression levels of the dominant-negative inhibitor. Figure 1 in their article illustrates how an active transcription factor can be sequestered by an inhibitor into the inactive complex AB that is unable to bind DNA. This type of mechanism results in an \"all-or-none\" response, or ultransensitivy, when the concentration of the regulatory protein increases to the point of depleting the inhibitor. Robust buffering against a response exists below this concentration threshold, and when it is reached any small increase in input is amplified into a large change in output.\n\nSignal transduction is regulated in various ways and one of the ways is translocation. Regulated translocation generates ultrasensitive response in mainly three ways:\n\nTranslocation is one way of regulating signal transduction, and it can generate ultrasensitive switch-like responses or multistep-feedback loop mechanisms. A switch-like response will occur if translocation raises the local concentration of a signaling protein. For example, epidermal growth factor (EGF) receptors can be internalized through clathrin-independent endocytosis (CIE) and/or clathrin-dependent endocytosis (CDE) in ligand concentration-dependent manner. The distribution of receptors into the two pathways was shown to be EGF concentration-dependent. In the presence of low concentrations of EGF, the receptor was exclusively internalized via CDE, whereas at high concentrations, receptors were equally distributed between CDE and CIE.\nZero-order ultrasensitivity takes place under saturating conditions. For example, consider an enzymatic step with a kinase, phosphatase, and substrate. Steady state levels of the phosphorylated substrate have an ultrasensitive response when there is enough substrate to saturate all available kinases and phosphatases. Under these conditions, small changes in the ratio of kinase to phosphatase activity can dramatically change the number of phosphorylated substrate (For a graph illustrating this behavior, see ). This enhancement in sensitivity of steady state phosphorylated substrate to Km, or the ratio of kinase to phosphatase activity, is termed zero-order to distinguish it from the first order behavior described by Michaelis-Menten dynamics, wherein the steady state concentration responds in a more gradual fashion than the switch-like behavior exhibited in ultrasensitivity.\n\nUsing the notation from Goldbeter & Koshland, let W be a certain substrate protein and let W' be a covalently modified version of W. The conversion of W to W' is catalyzed by some enzyme <chem>E1</chem> and the reverse conversion of W' to W is catalyzed by a second enzyme <chem>E2</chem> according to following equations:\n\nThe concentrations of all necessary components (such as ATP) are assumed to be constant and represented in the kinetic constants.\nUsing the chemical equations above, the reaction rate equations for each component are:\n\nThe total concentration of each component is given by:\n\nThe zero order mechanism assumes that the formula_26 or formula_27. In other words, the system is in a Michaelis-Menten steady state, which means, to a good approximation, formula_28 and formula_29 are constant.\nFrom these kinetic expressions one can solve for formula_30 at steady state defining formula_31 and formula_32\n\nwhere formula_34 and\n\nWhen the formula_30 is plotted against the molar ratio formula_40 and formula_41 it can be seen that the W to W' conversion occurs over a much smaller change in the formula_30 ratio than it would under first order (non-saturating) conditions, which is the telltale sign of ultrasensitivity.\n\nPositive feedback loops can cause ultrasensitive responses. An example of this is seen in the transcription of certain eukaryotic genes in which non-cooperative transcription factor binding changes positive feedback loops of histone modification that results in an ultrasensitive activation of transcription. The binding of a transcription factor recruits histone acetyltransferases and methyltransferases. The acetylation and methylation of histones recruits more acetyltransferases and methyltransferases that results in a positive feedback loop. Ultimately, this results in activation of transcription.\n\nAdditionally, positive feedback can induce bistability in Cyclin B1- by the two regulators Wee1 and Cdc25C, leading to the cell's decision to commit to mitosis. The system cannot be stable at intermediate levels of Cyclin B1, and the transition between the two stable states is abrupt when increasing levels of Cyclin B1 switches the system from low to high activity. Exhibiting hysteresis, for different levels of Cyclin B1, the switches from low to high and high to low states vary. However, the emergence of a bistable system is highly influenced by the sensitivity of its feedback loops. It has been shown in \"Xenopus\" egg extracts that Cdc25C hyperphosphorylation is a highly ultrasensitive function of Cdk activity, displaying a high value of the Hill coefficient (approx. 11), and the dephosphorylation step of Ser 287 in Cdc25C (also involved in Cdc25C activation) is even more ultrasensitive, displaying a Hill coefficient of approximately 32.\n\nA proposed mechanism of ultrasensitvity, called allovalency, suggests that activity \"derives from a high local concentration of interaction sites moving independently of each other\" \nAllovalency was first proposed when it was believed to occur in the pathway in which Sic1, is degraded in order for Cdk1-Clb (B-type cyclins) to allow entry into mitosis. Sic1 must be phosphorylated multiple times in order to be recognized and degraded by Cdc4 of the SCF Complex. Since Cdc4 only has one recognition site for these phosphorylated residues it was suggested that as the amount of phosphorylation increases, it exponentially increases the likelihood that Sic1 is recognized and degraded by Cdc4. This type of interaction was thought to be relatively immune to loss of any one site and easily tuned to any given threshold by adjusting the properties of individual sites. Assumptions for the allovalency mechanism were based on a general mathematical model that describes the interaction between a polyvalent disordered ligand and a single receptor site \n\nIt was later found that the ultrasentivity in Cdk1 levels by degregation of Sic1 is in fact due to a positive feedback loop.\n\nModeling by Dushek \"et al.\" proposes a possible mechanism for ultrasensitivity outside of the zero-order regime. For the case of membrane-bound enzymes acting on membrane-bound substrates with multiple enzymatic sites (such as tyrosine-phosphorylated receptors like the T-Cell receptor), ultrasensitive responses could be seen, crucially dependent on three factors: \n1) limited diffusion in the membrane,\n2) multiple binding sites on the substrate, and\n3) brief enzymatic inactivation following catalysis.\n\nUnder these particular conditions, although the enzyme may be in excess of the substrate (first-order regime), the enzyme is effectively locally saturated with substrate due to the multiple binding sites, leading to switch-like responses. This mechanism of ultrasensitivity is independent of enzyme concentration, however the signal is significantly enhanced depending on the number of binding sites on the substrate. Both conditional factors (limited diffusion and inactivation) are physiologically plausible, but have yet to be experimentally confirmed. Dushek’s modeling found increasing Hill cooperativity numbers with more substrate sites (phosphorylation sites), and with greater steric/diffusional hindrance between enzyme and substrate. This mechanism of ultrasensitivity based on local enzyme saturation arises partly from passive properties of slow membrane diffusion, and therefore may be generally applicable.\n\nThe bacterial flagellar motor has been proposed to follow a dissipative allosteric model, where ultrasensitivity comes as a combination of protein binding affinity and energy contributions from the proton motive force (see Flagellar motors and chemotaxis below).\n\nIn a living cell, ultrasensitive modules are embedded in a bigger network with upstream and downstream components. This components may constrain the range of inputs that the module will receive as well as the range of the module’s outputs that network will be able to detect. Altszyler et al. (2014) studied how the effective ultrasensitivity of a modular system is affected by these restrictions. They found for some ultrasensitive motifs that dynamic range limitations imposed by downstream components can produce effective sensitivities much larger than that of the original module when considered in isolation.\n\nUltrasensitive behavior is typically represented by a sigmoidal curve, as small alterations in the stimulus formula_43 can trigger large changes in the response formula_44. One such relation is the Hill equation:\n\nwhere formula_46 is the Hill coefficient which quantifies the steepness of the sigmoidal stimulus-response curve and it is therefore a sensitivity parameter. It is often used to assess the cooperativity of a system. A Hill coefficient greater than one is indicative of positive cooperativity and thus, the system exhibits ultrasensitivity. \nSystems with a Hill coefficient of 1 are noncooperative and follow the classical Michaelis-Menten kinetics. Enzymes exhibiting noncooperative activity are represented by hyperbolic stimulus/response curves, compared to sigmoidal curves for cooperative (ultrasensitive) enzymes.\nIn mitogen-activated protein kinase (MAPK) signaling (see example below), the ultrasensitivity of the signaling is supported by the sigmoidal stimulus/response curve that is comparable to an enzyme with a Hill coefficient of 4.0-5.0. This is even more ultrasensitive to the cooperative binding activity of hemoglobin, which has a Hill coefficient of 2.8.\n\nFrom a operational point of view the Hill coefficient can be calculated as:\n\nwhere <ce>EC90</ce> and <ce>EC10</ce> are the input values needed to produce the 10% and 90% of the maximal response, respectively.\n\nGlobal sensitivity measure such as Hill coefficient do not characterise the local behaviours of the s-shaped curves. Instead, these features are well captured by the response coefficient measure defined as:\n\nAltszyler et al. (2017) have shown that these ultrasensitivity measures can be linked by the following equation:\n\nwhere formula_50 denoted the mean value of the variable x over the range [a,b].\n\nConsider two coupled ultrasensitive modules, disregarding effects of sequestration of molecular components between layers. In this case, the expression for the system’s dose-response curve, formula_51, results from the mathematical composition of the functions, formula_52, which describe the input/output relationship of isolated modules formula_53:\n\nBrown et al. (1997) have shown that the local ultrasensitivity of the different layers combines multiplicatively:\n\nIn connection with this result, Ferrell et al. (1997) showed, for Hill-type modules, that the overall cascade global ultrasensitivity had to be less than or equal to the product of the global ultrasensitivity estimations of each cascade's layer,\n\nwhere formula_57 and formula_58 are the Hill coefficient of modules 1 and 2 respectively.\n\nAltszyler et al. (2017) have shown that the cascade's global ultrasensitivity can be analytically calculated:\n\nwhere formula_60 and formula_61 delimited the Hill input’s working range of the composite system, i.e. the input values for the i-layer so that the last layer (corresponding to formula_62 in this case) reached the 10% and 90% of it maximal output level. It followed this equation that the system's Hill coefficient formula_46 could be written as the product of two factors, formula_64 and formula_65, which characterized local average sensitivities over the relevant input region for each layer: formula_66, with formula_53 in this case.\n\nFor the more general case of a cascade of formula_68 modules, the Hill Coefficient can be expressed as:\n\nSeveral authors have reported the existence of supramultiplicative behavior in signaling cascades (i.e. the ultrasensitivity of the combination of layers is higher than the product of individual ultrasensitivities), but in many cases the ultimate origin of supramultiplicativity remained elusive. Altszyler et al. (2017) framework naturally suggested a general scenario where supramultiplicative behavior could take place. This could occur when, for a given module, the corresponding Hill's input working range was located in an input region with local ultrasensitivities higher than the global ultrasensitivity of the respective dose-response curve.\n\nA ubiquitous signaling motif that exhibits ultrasensitivity is the MAPK (mitogen-activated protein kinase) cascade, which can take a graded input signal and produce a switch-like output, such as gene transcription or cell cycle progression. In this common motif, MAPK is activated by an earlier kinase in the cascade, called MAPK kinase, or MAPKK. Similarly, MAPKK is activated by MAPKK kinase, or MAPKKK. These kinases are sequentially phosphorylated when MAPKKK is activated, usually via a signal received by a membrane-bound receptor protein. MAPKKK activates MAPKK, and MAPKK activates MAPK. Ultrasensitivity arises in this system due to several features:\n\nBesides the MAPK cascade, ultrasensitivity has also been reported in muscle glycolysis, in the phosphorylation of isocitrate dehydrogenase and in the activation of the calmodulin-dependent protein kinase II (CAMKII).\n\nAn ultrasensitive switch has been engineered by combining a simple linear signaling protein (N-WASP) with one to five SH3 interaction modules that have autoinhibitory and cooperative properties. Addition of a single SH3 module created a switch that was activated in a linear fashion by exogenous SH3-binding peptide. Increasing number of domains increased ultrasensitivity. A construct with three SH3 modules was activated with an apparent Hill coefficient of 2.7 and a construct with five SH3 module was activated with an apparent Hill coefficient of 3.9.\n\nDuring G2 phase of the cell cycle, Cdk1 and cyclin B1 makes a complex and forms maturation promoting factor (MPF). The complex accumulates in the nucleus due to phosphorylation of the cyclin B1 at multiple sites, which inhibits nuclear export of the complex. Phosphorylation of Thr19 and Tyr15 residues of Cdk1 by Wee1 and MYT1 keeps the complex inactive and inhibits entry into mitosis whereas dephosphorylation of Cdk1 by CDC25C phosphatase at Thr19 and Tyr15 residues, activates the complex which is necessary in order to enter mitosis. Cdc25C phosphatase is present in the cytoplasm and in late G2 phase it is translocated into the nucleus by signaling such as PIK1, PIK3. The regulated translocation and accumulation of the multiple required signaling cascade components, MPF and its activator Cdc25, in the nucleus generates efficient activation of the MPF and produces switch-like, ultrasensitive entry into mitosis.\n\nThe figure shows different possible mechanisms for how increased regulation of the localization of signaling components by the stimulus (input signal) shifts the output from Michaelian response to ultrasensitive response. When stimulus is regulating only inhibition of Cdk1-cyclinB1 nuclear export, the outcome is Michaelian response, Fig (a). But if the stimulus can regulate localization of multiple components of the signaling cascade, i.e. inhibition of Cdk1-cyclinB1 nuclear export and translocation of the Cdc25C to nucleus, then the outcome is ultrasensitive response, Fig (b). As more components of the signaling cascade are regulated and localized by the stimulus—i.e. inhibition of Cdk1-cyclinB1 nuclear export, translocation of the Cdc25C to the nucleus, and activation of Cdc25C—the output response becomes more and more ultrasensitive, Fig(c).\n\nDuring mitosis, mitotic spindle orientation is essential for determining the site of cleavage furrowing and position of daughter cells for subsequent cell fate determination. This orientation is achieved by polarizing cortical factors and rapid alignment of the spindle with the polarity axis. In fruit flies, three cortical factors have been found to regulate the position of the spindle: heterotrimeric G protein α subunit (Gαi), Partner of Inscuteable (Pins), and Mushroom body defect (Mud). Gαi localizes at apical cortex to recruit Pins. Upon binding to GDP-bound Gαi, Pins is activated and recruits Mud to achieve polarized distribution of cortical factors. N-terminal tetratricopeptide repeats (TPRs) in Pins is the binding region for Mud, but is autoinhibited by intrinsic C-terminal GoLoco domains (GLs) in the absence of Gαi. Activation of Pins by Gαi binding to GLs is highly ultrasensitive and is achieved through the following decoy mechanism: GLs 1 and 2 act as a decoy domains, competing with the regulatory domain, GL3, for Gαi inputs. This intramolecular decoy mechanism allows Pins to establish its threshold and steepness in response to distinct Gαi concentration. At low Gαi inputs, the decoy GLs 1 and 2 are preferentially bound. At intermediate Gαi concentration, the decoys are nearly saturated, and GL3 begins to be populated. At higher Gαi concentration, the decoys are fully saturated and Gαi binds to GL3, leading to Pins activation. Ultrasensitivity of Pins in response to Gαi ensures that Pins is activated only at the apical cortex where Gαi concentration is above the threshold, allowing for maximal Mud recruitment.\n\nGTPases are enzymes capable of binding and hydrolyzing guanosine triphosphate (GTP). Small GTPases, such as Ran and Ras, can exist in either a GTP-bound form (active) or a GDP-bound form (inactive), and the conversion between these two forms grants them a switch-like behavior. As such, small GTPases are involved in multiple cellular events, including nuclear translocation and signaling. The transition between the active and inactive states is facilitated by guanine nucleotide exchange factors (GEFs) and GTPase activating proteins (GAPs).\n\nComputational studies on the switching behavior of GTPases have revealed that the GTPase-GAP-GEF system displays ultrasensitivity. In their study, Lipshtat et al. simulated the effects of the levels of GEF and GAP activation on the Rap activation signaling network in response to signals from activated α2-adrenergic (α2R) receptors, which lead to degradation of the activated Rap GAP. They found that the switching behavior of Rap activation was ultrasensitive to changes in the concentration (i.e. amplitude) and the duration of the α2R signal, yielding Hill coefficients of nH=2.9 and nH=1.7, respectively (a Hill coefficient greater than nH=1 is characteristic of ultrasensitivity ). The authors confirmed this experimentally by treating neuroblasts with HU-210, which activates RAP through degradation of Rap GAP. Ultrasensitivity was observed both in a dose-dependent manner (nH=5±0.2), by treating cells with different HU-210 concentrations for a fixed time, and in a duration-dependent manner (nH=8.6±0.8), by treating cells with a fixed HU-210 concentration during varying times.\n\nBy further studying system, the authors determined that (the degree of responsiveness and ultrasensitivity) was heavily dependent on two parameters: the initial ratio of kGAP/kGEF, where the k’s incorporate both the concentration of active GAP or GEF and their corresponding kinetic rates; and the signal impact, which is the product of the degradation rate of activated GAP and either the signal amplitude or the signal duration. The parameter kGAP/kGEF affects the steepness of the transition from the two states of the GTPase switch, with higher values (~10) leading to ultrasensitivity. The signal impact affects the switching point. Therefore, by depending on the ratio of concentrations rather than on individual concentrations, the switch-like behavior of the system can also be displayed outside of the zero-order regime.\n\nPersistent stimulation at the neuronal synapse can lead to markedly different outcomes for the post-synaptic neuron. Extended weak signaling can result in long-term depression (LTD), in which activation of the post-synaptic neuron requires a stronger signal than before LTD was initiated. In contrast, long-term potentiation (LTP) occurs when the post-synaptic neuron is subjected to a strong stimulus, and this results in strengthening of the neural synapse (i.e., less neurotransmitter signal is required for activation).\n\nIn the CA1 region of the hippocampus, the decision between LTD and LTP is mediated solely by the level of intracellular Ca2+ at the post-synaptic dendritic spine. Low levels of Ca2+ (resulting from low-level stimulation) activates the protein phophatase calcineurin, which induces LTD. Higher levels of Ca2+ results in activation of Ca2+/calmodulin-dependent protein kinase II (CaMKII), which leads to LTP. The difference in Ca2+ concentration required for a cell to undergo LTP is only marginally higher than for LTD, and because neurons show bistability (either LTP or LTD) following persistent stimulation, this suggests that one or more components of the system respond in a switch-like, or ultrasensitive manner. Bradshaw et al. demonstrated that CaMKII (the LTP inducer) responds to intracellular calcium levels in an ultrasensitive manner, with <10% activity at 1.0 uM and ~90% activity at 1.5 uM, resulting in a Hill coefficient of ~8. Further experiments showed that this ultrasenstivity was mediated by cooperative binding of CaMKII by two molecules of calmodulin (CaM), and autophosphorylation of activated CaMKII leading to a positive feedback loop.\n\nIn this way, intracellular calcium can induce a graded, non-ultrasensitive activation of calcineurin at low levels, leading to LTD, whereas the ultrasensitive activation of CaMKII results in a threshold intracellular calcium level that generates a positive feedback loop that amplifies the signal and leads to the opposite cellular outcome: LTP. Thus, binding of a single substrate to multiple enzymes with different sensitivities facilitates a bistable decision for the cell to undergo LTD or LTP.\n\nIt has been suggested that zero-order ultrasensitivity may generate thresholds during development allowing for the conversion of a graded morphogen input to a binary switch-like response. Melen et al. (2005) have found evidence for such a system in the patterning of the Drosophila embryonic ventral ectoderm. In this system, graded mitogen activated protein kinase (MAPK) activity is converted to a binary output, the all-or-none degradation of the Yan transcriptional repressor. They found that MAPK phosphorylation of Yan is both essential and sufficient for Yan's degradation. Consistent with zero-order ultrasensitivity an increase in Yan protein lengthened the time required for degradation but had no effect on the border of Yan degradation in developing embryos. Their results are consistent with a situation where a large pool of Yan becomes either completely degraded or maintained. The particular response of each cell depends on whether or not the rate of reversible Yan phosphorylation by MAPK is greater or less than dephosphorylation. Thus, a small increase in MAPK phosphorylation can cause it to be the dominant process in the cell and lead to complete degradation of Yan.\n\nMultistep-feedback loop mechanism also leads to ultrasensitivity. \nThere is paper introducing that engineering synthetic feedback loops using yeast mating mitogen-activated protein (MAP) kinase pathway as a model system.\n\nIn Yeast mating pathway: alpha-factor activates receptor, Ste2, and Ste4 and activated Ste4 recruits Ste5 complex to membrane, allowing PAK-like kinase Ste20 (membrane-localized) to activate MAPKKK Ste11. Ste11 and downstream kinases, Ste7 (MAPKK) and Fus3 (MAPK), are colocalized on the scaffold and activation of cascade leads to transcriptional program. They used pathway modulators outside of core cascade, Ste50 promotes activation of Ste11 by Ste20; Msg5 (negative, red) is MAPK phosphatase that deactivates Fus3 (Fig.2A).\n\nWhat they built was circuit with enhanced ultrasensitive switch behavior by constitutively expressing a negative modulator, Msg5 which is one of MAPK phoaphatase and inducibly expressing a positive modulator, Ste50 which is pathway modulators outside of core cascade(Fig.2B). \nThe success of this recruitment-based engineering strategy suggests that it may be possible to reprogram cellular responses with high precision.\n\nThe rotational direction of \"E. coli\" is controlled by the flagellar motor switch. A ring of 34 FliM proteins around the rotor bind CheY, whose phosphorylation state determines whether the motor rotates in a clockwise or counterclockwise manner. The rapid switching mechanism is attributed to an ultrasensitive response, which has a Hill coefficient of ~10. This system has been proposed to follow a dissipative allosteric model, in which rotational switching is a result of both CheY binding and energy consumption from the proton motive force, which also powers the flagellar rotation.\n\nRecently it has been shown that a Michaelian signaling pathway can be converted to an ultrasensitive signaling pathway by the introduction of two positive feedback loops. In this synthetic biology approach, Palani and Sarkar began with a linear, graded response pathway, a pathway that showed a proportional increase in signal output relative to the amount of signal input, over a certain range of inputs. This simple pathway was composed of a membrane receptor, a kinase and a transcription factor. Upon activation the membrane receptor phosphorylates the kinase, which moves into the nucleus and phosphorylates the transcription factor, which turns on gene expression. To transform this graded response system into an ultrasensitive, or switch-like signaling pathway, the investigators created two positive feedback loops. In the engineered system, activation of the membrane receptor resulted in increased expression of both the receptor itself and the transcription factor. This was accomplished by placing a promoter specific for this transcription factor upstream of both genes. The authors were able to demonstrate that the synthetic pathway displayed high ultrasensitivity and bistability.\n\nRecent computational analysis of the effects of a signaling protein's concentration on the presence of an ultrasensitive response has come to complementary conclusions about the influence of a signaling protein's concentration on the conversion of a graded response to an ultrasensitive one. Rather than focus on the generation of signaling proteins through positive feedback, however, the study instead focused on how the dynamics of a signaling protein's exit from the system influences the response. Soyer, Kuwahara, and Csika´sz-Nagy devised a signaling pathway composed of a protein (P) that possesses two possible states (unmodified P or modified P*) and can be modified by an incoming stimulus E. Furthermore, while the unmodified form, P, is permitted to enter or leave the system, P* is only allowed to leave (i.e. it is not generated elsewhere). After varying the parameters of this system, the researchers discovered that the modification of P to P* can shift between a graded response and an ultrasensitive response via the modification of the exit rates of P and P* relative to each other. The transition from an ultrasensitive response to E and a graded response to E was generated when the two rates went from highly similar to highly dissimilar, irrespective of the kinetics of the conversion from P to P* itself. This finding suggests at least two things: 1) the simplifying assumption that the levels of signaling molecules stay constant in a system can severely limit the understanding of ultrasensitivity's complexity; and 2) it may be possible to induce or inhibit ultrasensitivity artificially by regulating the rates of the entry and exit of signaling molecules occupying a system of interest.\n\nIt has been shown that the integration of a given synthetic ultrasensitive module with upstream and downstream components often alters the its information-processing capabilities. This effects must be taken into account in the design process.\n", "id": "33785040", "title": "Ultrasensitivity"}
{"url": "https://en.wikipedia.org/wiki?curid=25211094", "text": "Knockout moss\n\nA knockout moss is a moss plant in which one or more specific genes are deleted or inactivated (\"knocked out\") by gene targeting. After deletion of a gene, the knockout moss has lost the trait encoded by this gene. Thus, the function of this gene can be inferred. This scientific approach is called reverse genetics as the scientist wants to unravel the function of a specific gene. In classical genetics the scientist starts with a phenotype of interest and searches for the gene that causes this phenotype. Knockout mosses are relevant for basic research in biology as well as in biotechnology.\n\nThe targeted deletion or alteration of genes relies on the integration of a DNA strand at a specific and predictable position into the genome of the host cell. This DNA strand must be engineered in such a way that both ends are identical to this specific gene locus. This is a prerequisite for being efficiently integrated via homologous recombination (HR). Basically, a knockout mouse is engineered in the same way.\nSo far, this method of gene targeting in land plants has been carried out in the mosses \"Physcomitrella patens\" and \"Ceratodon purpureus\", since in these non-seed plant species the efficiency of HR is several orders of magnitude higher than in seed plants.\n\nKnockout mosses are stored at and distributed by a specialized Biobank, the International Moss Stock Center.\n\nFor altering moss genes in a targeted way, the DNA-construct needs to be incubated together with moss protoplasts and with polyethylene glycol (PEG). As mosses are haploid organisms, the regenerating moss filaments (protonemata) can be directly assayed for gene targeting within 6 weeks when utilizing PCR-methods.\n\nThe first scientific publication about identification of the function of a hitherto unknown gene utilizing knockout moss appeared 1998 and was authored by Ralf Reski and coworkers. They deleted the ftsZ-gene and thus functionally identified the first gene pivotal for the division of an organelle in any eukaryote.\n\nBy multiple gene knockout \"Physcomitrella\" plants were engineered, that lack the plant-specific glycosylation of proteins, an important post-translational modification. These knockout mosses are used to produce complex biopharmaceuticals in the field of molecular farming.\n\nIn cooperation with the chemical company BASF Ralf Reski and coworkers established a collection of knockout mosses that is used for gene identification.\n", "id": "25211094", "title": "Knockout moss"}
{"url": "https://en.wikipedia.org/wiki?curid=34137696", "text": "Molecular breeding\n\nMolecular breeding is the application of molecular biology tools, often in plant breeding and animal breeding \n\nThe areas of molecular breeding include:\n\nDevelopment of SNPs has revolutionized the molecular breeding process as it helps to create dense markers. Another area that is developing is genotyping by sequencing.\n\n\n\n\nTransfer of genes make it possible for horizontal transfer of genes from one organism to another. Thus plants can receive genes from humans or algae or any other organism. This provides limitless opportunity in breeding crop plants.\n", "id": "34137696", "title": "Molecular breeding"}
{"url": "https://en.wikipedia.org/wiki?curid=611074", "text": "Point mutation\n\nA point mutation is a genetic mutation where a single nucleotide base is changed, inserted or deleted from a sequence of DNA or RNA. Point mutations have a variety of effects on the downstream protein product—consequences that are moderately predictable based upon the specifics of the mutation. These consequences can range from benign (e.g. synonymous mutations) to catastrophic (e.g. frameshift mutations), with regard to protein production, composition, and function.\n\nPoint mutation is a random SNP (single-nucleotide polymorphism) mutation in the deoxyribonucleic acid (DNA) that occurs at one point. Point mutations usually take place during DNA replication. DNA replication occurs when one double-stranded DNA molecule creates two single strands of DNA, each of which is a template for the creation of the complementary strand. A single point mutation can change the whole DNA sequence. Changing one purine or pyrimidine may change the amino acid that the nucleotides code for.\n\nPoint mutations may arise from spontaneous mutations that occur during DNA replication. The rate of mutation may be increased by mutagens. Mutagens can be physical, such as radiation from UV rays, X-rays or extreme heat, or chemical (molecules that misplace base pairs or disrupt the helical shape of DNA). Mutagens associated with cancers are often studied to learn about cancer and its prevention.\n\nThere are multiple ways for point mutations to occur. First, ultraviolet (UV) light and higher-frequency light are capable of ionizing electrons, which in turn can affect DNA. Reactive oxygen molecules with free radicals, which are a byproduct of cellular metabolism, can also be very harmful to DNA. These reactants can lead to both single-stranded DNA breaks and double-stranded DNA breaks. Third, bonds in DNA eventually degrade, which creates another problem to keep the integrity of DNA to a high standard. There can also be replication errors that lead to substitution, insertion, or deletion mutations.\n\nIt was previously believed that these mutations happened completely by chance, with no regard for their effects on the organisms. Recently, there have been studies suggesting that these mutations occur in response to environmental challenges. That is to say, they are more likely to occur when they are advantageous to the organism, rather than when they are neutral or disadvantageous. When cells were deprived of a certain amino acid, tryptophan, for prolonged periods of time, point mutations in trp operon reverted to tryptophan, leading to an advantageous result, more frequently than under normal conditions when the mutations were neutral. In addition, the tryptophan mutation rate was unaffected when the cells were deprived of another amino acid, cysteine, further suggesting that the mutation rate was specific to situations in which the mutation was advantageous.\n\nIn 1959 Ernst Freese coined the terms \"transitions\" or \"transversions\" to categorize different types of point mutations. Transitions are replacement of a purine base with another purine or replacement of a pyrimidine with another pyrimidine. Transversions are replacement of a purine with a pyrimidine or vice versa. There is a systematic difference in mutation rates for transitions (Alpha) and transversions (Beta). Transition mutations are about ten times more common than transversions.\n\nNonsense mutations include stop-gain, and start-loss. Stop-gain is a mutation that results in a premature termination codon (\"a stop was gained\"), which signals the end of translation. This interruption causes the protein to be abnormally shortened. The number of amino acids lost mediates the impact on the protein's functionality and whether it will function whatsoever. Stop-loss is a mutation in the original termination codon (\"a stop was lost\"), resulting in abnormal extension of a protein's carboxyl terminus. Start-gain creates a AUG start codon upstream of the original start site. If the new AUG is near the original start site, in-frame within the processed transcript and downstream to a ribosomal binding site, it can be used to initiate translation. The likely effect is additional amino acids added to the amino terminus of the original protein. Frame-shift mutations are also possible in start-gain mutations, but typically do not affect translation of the original protein. Start-loss is a point mutation in a transcript's AUG start codon, resulting in the reduction or elimination of protein production.\n\nMissense mutations code for a different amino acid. A missense mutation changes a codon so that a different protein is created, a non-synonymous change. Conservative mutations result in an amino acid change. However, the properties of the amino acid remain the same (e.g., hydrophobic, hydrophilic, etc.). At times, a change to one amino acid in the protein is not detrimental to the organism as a whole. Most proteins can withstand one or two point mutations before their function changes. Non-conservative mutations result in an amino acid change that has different properties than the wild type. The protein may lose its function, which can result in a disease in the organism. For example, sickle-cell disease is caused by a single point mutation (a missense mutation) in the beta-hemoglobin gene that converts a GAG codon into GUG, which encodes the amino acid valine rather than glutamic acid. The protein may also exhibit a \"gain of function\" or become activated, such is the case with the mutation changing a valine to glutamic acid in the braf gene; this leads to an activation of the RAF protein which causes unlimited proliferative signalling in cancer cells. These are both examples of a non-conservative (missense) mutation.\n\nSilent mutations code for the same amino acid (a \"synonymous substitution\"). A silent mutation has no effect on the functioning of the protein. A single nucleotide can change, but the new codon specifies the same amino acid, resulting in an unmutated protein. This type of change is called synonymous change, since the old and new codon code for the same amino acid. This is possible because 64 codons specify only 20 amino acids. Different codons can lead to differential protein expression levels, however.\n\nSometimes the term \"point mutation\" is used to describe insertions or deletions of a single base pair (which has more of an adverse effect on the synthesized protein due to the nucleotides' still being read in triplets, but in different frames: a mutation called a frameshift mutation).\n\nPoint mutations that occur in non-coding sequences are most often without consequences, although there are exceptions. If the mutated base pair is in the promoter sequence of a gene, then the expression of the gene may change. Also, if the mutation occurs in the splicing site of an intron, then this may interfere with correct splicing of the transcribed pre-mRNA.\n\nBy altering just one amino acid, the entire peptide may change, thereby changing the entire protein. If the protein functions in cellular reproduction then this single point mutation can change the entire process of cellular reproduction for this organism.\n\nPoint germline mutations can lead to beneficial as well as harmful traits or diseases. This leads to adaptations based on the environment where the organism lives. An advantageous mutation can create an advantage for that organism and lead to the trait's being passed down from generation to generation, improving and benefiting the entire population. The scientific theory of evolution is greatly dependent on point mutations in cells. The theory explains the diversity and history of living organisms on Earth. In relation to point mutations, it states that beneficial mutations allow the organism to thrive and reproduce, thereby passing its positively affected mutated genes on to the next generation. On the other hand, harmful mutations cause the organism to die or be less likely to reproduce in a phenomenon known as natural selection.\n\nThere are different short-term and long-term effects that can arise from mutations. Smaller ones would be a halting of the cell cycle at numerous points. This means that a codon coding for the amino acid glycine may be changed to a stop codon, causing the proteins that should have been produced to be deformed and unable to complete their intended tasks. Because the mutations can affect the DNA and thus the chromatin, it can prohibit mitosis from occurring due to the lack of a complete chromosome. Problems can also arise during the processes of transcription and replication of DNA. These all prohibit the cell from reproduction and thus lead to the death of the cell. Long-term effects can be a permanent changing of a chromosome, which can lead to a mutation. These mutations can be either beneficial or detrimental. Cancer is an example of how they can be detrimental.\n\nOther effects of point mutations, or single nucleotide polymorphisms in DNA, depend on the location of the mutation within the gene. For example, if the mutation occurs in the region of the gene responsible for coding, the amino acid sequence of the encoded protein may be altered, causing a change in the function, protein localization, stability of the protein or protein complex. Many methods have been proposed to predict the effects of missense mutations on proteins. Machine learning algorithms train their models to distinguish known disease-associated from neutral mutations whereas other methods do not explicitly train their models but almost all methods exploit the evolutionary conservation assuming that changes at conserved positions tend to be more deleterious. While majority of methods provide a binary classification of effects of mutations into damaging and benign, a new level of annotation is needed to offer an explanation of why and how these mutations damage proteins.\n\nMoreover, if the mutation occurs in the region of the gene where transcriptional machinery binds to the protein, the mutation can affect the binding of the transcription factors because the short nucleotide sequences recognized by the transcription factors will be altered. Mutations in this region can affect rate of efficiency of gene transcription, which in turn can alter levels of mRNA and, thus, protein levels in general.\n\nPoint mutations can have several effects on the behavior and reproduction of a protein depending on where the mutation occurs in the amino acid sequence of the protein. If the mutation occurs in the region of the gene that is responsible for coding for the protein, the amino acid may be altered. This slight change in the sequence of amino acids can cause a change in the function, activation of the protein meaning how it binds with a given enzyme, where the protein will be located within the cell, or the amount of free energy stored within the protein.\n\nIf the mutation occurs in the region of the gene where transcriptional machinery binds to the protein, the mutation can affect the way in which transcription factors bind to the protein. The mechanisms of transcription bind to a protein through recognition of short nucleotide sequences. A mutation in this region may alter these sequences and, thus, change the way the transcription factors bind to the protein. Mutations in this region can affect the efficiency of gene transcription, which controls both the levels of mRNA and overall protein levels.\n\nA defect in the cystic fibrosis transmembrane conductance regulator (CFTR) gene causes cystic fibrosis (CF). A protein made by this gene controls the movement of the water and salt in and out of the body's cells. Genes in people with CF incorrectly code proteins. This causes thick, sticky mucus and very salty sweat.\n\nPoint mutations in multiple tumor suppressor proteins cause cancer. For instance, point mutations in Adenomatous Polyposis Coli promote tumorigenesis. A novel assay, Fast parallel proteolysis (FASTpp), might help swift screening of specific stability defects in individual cancer patients.\n\nNeurofibromatosis is caused by point mutations in the Neurofibromin 1 or Neurofibromin 2 gene.\n\nSickle-cell anemia is caused by a point mutation in the β-globin chain of hemoglobin, causing the hydrophilic amino acid glutamic acid to be replaced with the hydrophobic amino acid valine at the sixth position.\n\nThe β-globin gene is found on the short arm of chromosome 11. The association of two wild-type α-globin subunits with two mutant β-globin subunits forms hemoglobin S (HbS). Under low-oxygen conditions (being at high altitude, for example), the absence of a polar amino acid at position six of the β-globin chain promotes the non-covalent polymerisation (aggregation) of hemoglobin, which distorts red blood cells into a sickle shape and decreases their elasticity.\n\nHemoglobin is a protein found in red blood cells, and is responsible for the transportation of oxygen through the body. There are two subunits that make up the hemoglobin protein: beta-globins and alpha-globins.\nBeta-hemoglobin is created from the genetic information on the HBB, or \"hemoglobin, beta\" gene found on chromosome 11p15.5. A single point mutation in this polypeptide chain, which is 147 amino acids long, results in the disease known as Sickle Cell Anemia.\nSickle-Cell Anemia is an autosomal recessive disorder that affects 1 in 500 African Americans, and is one of the most common blood disorders in the United States. The single replacement of the sixth amino acid in the beta-globin, glutamic acid, with valine results in deformed red blood cells. These sickle-shaped cells cannot carry nearly as much oxygen as normal red blood cells and they get caught more easily in the capillaries, cutting off blood supply to vital organs. The single nucleotide change in the beta-globin means that even the smallest of exertions on the part of the carrier results in severe pain and even heart attack. Below is a chart depicting the first thirteen amino acids in the normal and abnormal sickle cell polypeptide chain.\n\nSequence for Normal Hemoglobin\nSequence for Sickle Cell Hemoglobin\nThe cause of Tay–Sachs disease is a genetic defect that is passed from parent to child. This genetic defect is located in the HEXA gene, which is found on chromosome 15.\n\nThe HEXA gene makes part of an enzyme called beta-hexosaminidase A, which plays a critical role in the nervous system. This enzyme helps break down a fatty substance called GM2 ganglioside in nerve cells.\nMutations in the HEXA gene disrupt the activity of beta-hexosaminidase A, preventing the breakdown of the fatty substances. As a result, the fatty substances accumulate to deadly levels in the brain and spinal cord. The buildup of GM2 ganglioside causes progressive damage to the nerve cells. This is the cause of the signs and symptoms of Tay-Sachs disease.\n\nPeople who are colorblind have mutations in their genes that cause a loss of either red or green cones, and they therefore have a hard time distinguishing between colors. There are three kinds of cones in the human eye: red, green, and blue.\nNow researchers have discovered that some people with the gene mutation that causes colorblindness lose an entire set of \"color\" cones with no change to the clearness of their vision overall.\n\nIn molecular biology, repeat induced point-mutation or RIP is a process by which DNA accumulates G:C to A:T transition mutations. Genomic evidence indicates that RIP occurs or has occurred in a variety of fungi while experimental evidence indicates that RIP is active in \"Neurospora crassa\", \"Podospora anserina\", \"Magnaporthe grisea\", \"Leptosphaeria maculans\", \"Gibberella zeae\" and \"Nectria haematococca\". In \"Neurospora crassa\", sequences mutated by RIP are often methylated \"de novo\".\n\nRIP occurs during the sexual stage in haploid nuclei after fertilization but prior to meiotic DNA replication. In \"Neurospora crassa\", repeat sequences of at least 400 base pairs in length are vulnerable to RIP. Repeats with as low as 80% nucleotide identity may also be subject to RIP. Though the exact mechanism of repeat recognition and mutagenesis are poorly understood, RIP results in repeated sequences undergoing multiple transition mutations.\n\nThe RIP mutations do not seem to be limited to repeated sequences. Indeed, for example, in the phytopathogenic fungus \"L. maculans\", RIP mutations are found in single copy regions, adjacent to the repeated elements. These regions are either non-coding regions or genes encoding small secreted proteins including avirulence genes.\nThe degree of RIP within these single copy regions was proportional to their proximity to repetitive elements.\n\nRep and Kistler have speculated that the presence of highly repetitive regions containing transposons, may promote mutation of resident effector genes. So the presence of effector genes within such regions is suggested to promote their adaptation and diversification when exposed to strong selection pressure.\n\nAs RIP mutation is traditionally observed to be restricted to repetitive regions and not single copy regions, Fudal \"et al.\" suggested that leakage of RIP mutation might occur within a relatively short distance of a RIP-affected repeat. Indeed, this has been reported in \"N. crassa\" whereby leakage of RIP was detected in single copy sequences at least 930 bp from the boundary of neighbouring duplicated sequences.\nTo elucidate the mechanism of detection of repeated sequences leading to RIP may allow to understand how the flanking sequences may also be affected.\n\nRIP causes G:C to A:T transition mutations within repeats, however, the mechanism that detects the repeated sequences is unknown. RID is the only known protein essential for RIP. It is a DNA methyltransferease-like protein, that when mutated or knocked out results in loss of RIP. Deletion of the \"rid\" homolog in \"Aspergillus nidulans\", \"dmtA\", results in loss of fertility while deletion of the \"rid\" homolog in \"Ascobolus immersens\", \"masc1\", results in fertility defects and loss of methylation induced premeiotically (MIP).\n\nRIP is believed to have evolved as a defense mechanism against transposable elements, which resemble parasites by invading and multiplying within the genome.\nRIP creates multiple missense and nonsense mutations in the coding sequence. This hypermutation of G-C to A-T in repetitive sequences eliminates functional gene products of the sequence (if there were any to begin with). In addition, many of the C-bearing nucleotides become methylated, thus decreasing transcription.\n\nBecause RIP is so efficient at detecting and mutating repeats, fungal biologists often use it as a tool for mutagenesis. A second copy of a single-copy gene is first transformed into the genome. The fungus must then mate and go through its sexual cycle to activate the RIP machinery. Many different mutations within the duplicated gene are obtained from even a single fertilization event so that inactivated alleles, usually due to nonsense mutations, as well as alleles containing missense mutations can be obtained.\n\nThe cellular reproduction process of meiosis was discovered by Oscar Hertwig in 1876. Mitosis was discovered several years later in 1882 by Walther Flemming.\n\nHertwig studied sea urchins, and noticed that each egg contained one nucleus prior to fertilization and two nuclei after. This discovery proved that one spermatozoon could fertilize an egg, and therefore proved the process of meiosis. Hermann Fol continued Hertwig’s research by testing the effects of injecting several spermatozoa into an egg, and found that the process did not work with more than one spermatozoon.\n\nFlemming began his research of cell division starting in 1868. The study of cells was an increasingly popular topic in this time period. By 1873, Schneider had already begun to describe the steps of cell division. Flemming furthered this description in 1874 and 1875 as he explained the steps in more detail. He also argued with Schneider’s findings that the nucleus separated into rod-like structures by suggesting that the nucleus actually separated into threads that in turn separated. Flemming concluded that cells replicate through cell division, to be more specific mitosis.\n\nMatthew Meselson and Franklin Stahl are credited with the discovery of DNA replication. Watson and Crick acknowledged that the structure of DNA did indicate that there is some form of replicating process. However, there was not a lot of research done on this aspect of DNA until after Watson and Crick. People considered all possible methods of determining the replication process of DNA, but none were successful until Meselson and Stahl. Meselson and Stahl introduced a heavy isotope into some DNA and traced its distribution. Through this experiment, Meselson and Stahl were able to prove that DNA reproduces semi-conservatively.\n", "id": "611074", "title": "Point mutation"}
{"url": "https://en.wikipedia.org/wiki?curid=34217148", "text": "Vectors in gene therapy\n\nGene therapy utilizes the delivery of DNA into cells, which can be accomplished by several methods, summarized below. The two major classes of methods are those that use recombinant viruses (sometimes called biological nanoparticles or viral vectors) and those that use naked DNA or DNA complexes (non-viral methods).\n\nAll viruses bind to their hosts and introduce their genetic material into the host cell as part of their replication cycle. This genetic material contains basic 'instructions' of how to produce more copies of these viruses, hacking the body's normal production machinery to serve the needs of the virus. The host cell will carry out these instructions and produce additional copies of the virus, leading to more and more cells becoming infected. Some types of viruses insert their genome into the host's cytoplasm, but do not actually enter the cell. Others penetrate the cell membrane disguised as protein molecules and enter the cell.\n\nThere are two main types of virus infection: lytic and lysogenic. Shortly after inserting its DNA, viruses of the lytic cycle quickly produce more viruses, burst from the cell and infect more cells. Lysogenic viruses integrate their DNA into the DNA of the host cell and may live in the body for many years before responding to a trigger. The virus reproduces as the cell does and does not inflict bodily harm until it is triggered. The trigger releases the DNA from that of the host and employs it to create new viruses.\n\nThe genetic material in retroviruses is in the form of RNA molecules, while the genetic material of their hosts is in the form of DNA. When a retrovirus infects a host cell, it will introduce its RNA together with some enzymes, namely reverse transcriptase and integrase, into the cell. This RNA molecule from the retrovirus must produce a DNA copy from its RNA molecule before it can be integrated into the genetic material of the host cell. The process of producing a DNA copy from an RNA molecule is termed reverse transcription. It is carried out by one of the enzymes carried in the virus, called reverse transcriptase. After this DNA copy is produced and is free in the nucleus of the host cell, it must be incorporated into the genome of the host cell. That is, it must be inserted into the large DNA molecules in the cell (the chromosomes). This process is done by another enzyme carried in the virus called integrase.\n\nNow that the genetic material of the virus has been inserted, it can be said that the host cell has been modified to contain new genes. If this host cell divides later, its descendants will all contain the new genes. Sometimes the genes of the retrovirus do not express their information immediately.\n\nOne of the problems of gene therapy using retroviruses is that the integrase enzyme can insert the genetic material of the virus into any arbitrary position in the genome of the host; it randomly inserts the genetic material into a chromosome. If genetic material happens to be inserted in the middle of one of the original genes of the host cell, this gene will be disrupted (insertional mutagenesis). If the gene happens to be one regulating cell division, uncontrolled cell division (i.e., cancer) can occur. This problem has recently begun to be addressed by utilizing zinc finger nucleases or by including certain sequences such as the beta-globin locus control region to direct the site of integration to specific chromosomal sites.\n\nGene therapy trials using retroviral vectors to treat X-linked severe combined immunodeficiency (X-SCID) represent the most successful application of gene therapy to date. More than twenty patients have been treated in France and Britain, with a high rate of immune system reconstitution observed. Similar trials were restricted or halted in the USA when leukemia was reported in patients treated in the French X-SCID gene therapy trial. To date, four children in the French trial and one in the British trial have developed leukemia as a result of insertional mutagenesis by the retroviral vector. All but one of these children responded well to conventional anti-leukemia treatment. Gene therapy trials to treat SCID due to deficiency of the Adenosine Deaminase (ADA) enzyme (one form of SCID) continue with relative success in the USA, Britain, Ireland, Italy and Japan.\n\nAdenoviruses are viruses that carry their genetic material in the form of double-stranded DNA. They cause respiratory, intestinal, and eye infections in humans (especially the common cold). When these viruses infect a host cell, they introduce their DNA molecule into the host. The genetic material of the adenoviruses is not incorporated (transient) into the host cell's genetic material. The DNA molecule is left free in the nucleus of the host cell, and the instructions in this extra DNA molecule are transcribed just like any other gene. The only difference is that these extra genes are not replicated when the cell is about to undergo cell division so the descendants of that cell will not have the extra gene.\nAs a result, treatment with the adenovirus will require readministration in a growing cell population although the absence of integration into the host cell's genome should prevent the type of cancer seen in the SCID trials. This vector system has been promoted for treating cancer and indeed the first gene therapy product to be licensed to treat cancer, Gendicine, is an adenovirus. Gendicine, an adenoviral p53-based gene therapy was approved by the Chinese food and drug regulators in 2003 for treatment of head and neck cancer. Advexin, a similar gene therapy approach from Introgen, was turned down by the US Food and Drug Administration (FDA) in 2008.\n\nConcerns about the safety of adenovirus vectors were raised after the 1999 death of Jesse Gelsinger while participating in a gene therapy trial. Since then, work using adenovirus vectors has focused on genetically crippled versions of the virus.\n\nThe viral vectors described above have natural host cell populations that they infect most efficiently. Retroviruses have limited natural host cell ranges, and although adenovirus and adeno-associated virus are able to infect a relatively broader range of cells efficiently, some cell types are refractory to infection by these viruses as well. Attachment to and entry into a susceptible cell is mediated by the protein envelope on the surface of a virus. Retroviruses and adeno-associated viruses have a single protein coating their membrane, while adenoviruses are coated with both an envelope protein and fibers that extend away from the surface of the virus. The envelope proteins on each of these viruses bind to cell-surface molecules such as heparin sulfate, which localizes them upon the surface of the potential host, as well as with the specific protein receptor that either induces entry-promoting structural changes in the viral protein, or localizes the virus in endosomes wherein acidification of the lumen induces this refolding of the viral coat. In either case, entry into potential host cells requires a favorable interaction between a protein on the surface of the virus and a protein on the surface of the cell.\nFor the purposes of gene therapy, one might either want to limit or expand the range of cells susceptible to transduction by a gene therapy vector. To this end, many vectors have been developed in which the endogenous viral envelope proteins have been replaced by either envelope proteins from other viruses, or by chimeric proteins. Such chimera would consist of those parts of the viral protein necessary for incorporation into the virion as well as sequences meant to interact with specific host cell proteins. Viruses in which the envelope proteins have been replaced as described are referred to as pseudotyped viruses. For example, the most popular retroviral vector for use in gene therapy trials has been the lentivirus Simian immunodeficiency virus coated with the envelope proteins, G-protein, from Vesicular stomatitis virus. This vector is referred to as VSV G-pseudotyped lentivirus, and infects an almost universal set of cells. This tropism is characteristic of the VSV G-protein with which this vector is coated. Many attempts have been made to limit the tropism of viral vectors to one or a few host cell populations. This advance would allow for the systemic administration of a relatively small amount of vector. The potential for off-target cell modification would be limited, and many concerns from the medical community would be alleviated. Most attempts to limit tropism have used chimeric envelope proteins bearing antibody fragments. These vectors show great promise for the development of \"magic bullet\" gene therapies.\n\nA replication-competent vector called ONYX-015 is used in replicating tumor cells. It was found that in the absence of the E1B-55Kd viral protein, adenovirus caused very rapid apoptosis of infected, p53(+) cells, and this results in dramatically reduced virus progeny and no subsequent spread. Apoptosis was mainly the result of the ability of EIA to inactivate p300. In p53(-) cells, deletion of E1B 55kd has no consequence in terms of apoptosis, and viral replication is similar to that of wild-type virus, resulting in massive killing of cells.\n\nA replication-defective vector deletes some essential genes. These deleted genes are still necessary in the body so they are replaced with either a helper virus or a DNA molecule.\n\nReplication-defective vectors always contain a “transfer construct”. The transfer construct carries the gene to be transduced or “transgene”. The transfer construct also carries the sequences which are necessary for the general functioning of the viral genome: packaging sequence, repeats for replication and, when needed, priming of reverse transcription. These are denominated cis-acting elements, because they need to be on the same piece of DNA as the viral genome and the gene of interest. Trans-acting elements are viral elements, which can be encoded on a different DNA molecule. For example, the viral structural proteins can be expressed from a different genetic element than the viral genome.\n\nThe Herpes simplex virus is a human neurotropic virus. This is mostly examined for gene transfer in the nervous system. The wild type HSV-1 virus is able to infect neurons and evade the host immune response, but may still become reactivated and produce a lytic cycle of viral replication. Therefore, it is typical to use mutant strains of HSV-1 that are deficient in their ability to replicate. Though the latent virus is not transcriptionally apparent, it does possess neuron specific promoters that can continue to function normally. Antibodies to HSV-1 are common in humans, however complications due to herpes infection are somewhat rare. Caution for rare cases of encephalitis must be taken and this provides some rationale to using HSV-2 as a viral vector as it generally has tropism for neuronal cells innervating the urogenital area of the body and could then spare the host of severe pathology in the brain.\n\nNon-viral methods present certain advantages over viral methods, with simple large scale production and low host immunogenicity being just two. Previously, low levels of transfection and expression of the gene held non-viral methods at a disadvantage; however, recent advances in vector technology have yielded molecules and techniques with transfection efficiencies similar to those of viruses.\n\nThis is the simplest method of non-viral transfection. Clinical trials carried out of intramuscular injection of a naked DNA plasmid have occurred with some success; however, the expression has been very low in comparison to other methods of transfection. In addition to trials with plasmids, there have been trials with naked PCR product, which have had similar or greater success. Cellular uptake of naked DNA is generally inefficient. Research efforts focusing on improving the efficiency of naked DNA uptake have yielded several novel methods, such as electroporation, sonoporation, and the use of a \"gene gun\", which shoots DNA coated gold particles into the cell using high pressure gas.\n\nElectroporation is a method that uses short pulses of high voltage to carry DNA across the cell membrane. This shock is thought to cause temporary formation of pores in the cell membrane, allowing DNA molecules to pass through. Electroporation is generally efficient and works across a broad range of cell types. However, a high rate of cell death following electroporation has limited its use, including clinical applications.\n\nMore recently a newer method of electroporation, termed electron-avalanche transfection, has been used in gene therapy experiments. By using a high-voltage plasma discharge, DNA was efficiently delivered following very short (microsecond) pulses. Compared to electroporation, the technique resulted in greatly increased efficiency and less cellular damage.\n\nThe use of particle bombardment, or the gene gun, is another physical method of DNA transfection. In this technique, DNA is coated onto gold particles and loaded into a device which generates a force to achieve penetration of the DNA into the cells, leaving the gold behind on a \"stopping\" disk.\n\nSonoporation uses ultrasonic frequencies to deliver DNA into cells. The process of acoustic cavitation is thought to disrupt the cell membrane and allow DNA to move into cells.\n\nIn a method termed magnetofection, DNA is complexed to magnetic particles, and a magnet is placed underneath the tissue culture dish to bring DNA complexes into contact with a cell monolayer.\n\nHydrodynamic delivery involves rapid injection of a high volume of a solution into vasculature (such as into the inferior vena cava, bile duct, or tail vein). The solution contains molecules that are to be inserted into cells, such as DNA plasmids or siRNA, and transfer of these molecules into cells is assisted by the elevated hydrostatic pressure caused by the high volume of injected solution.\n\nThe use of synthetic oligonucleotides in gene therapy is to deactivate the genes involved in the disease process. There are several methods by which this is achieved. One strategy uses antisense specific to the target gene to disrupt the transcription of the faulty gene. Another uses small molecules of RNA called siRNA to signal the cell to cleave specific unique sequences in the mRNA transcript of the faulty gene, disrupting translation of the faulty mRNA, and therefore expression of the gene. A further strategy uses double stranded oligodeoxynucleotides as a decoy for the transcription factors that are required to activate the transcription of the target gene. The transcription factors bind to the decoys instead of the promoter of the faulty gene, which reduces the transcription of the target gene, lowering expression. Additionally, single stranded DNA oligonucleotides have been used to direct a single base change within a mutant gene. The oligonucleotide is designed to anneal with complementarity to the target gene with the exception of a central base, the target base, which serves as the template base for repair. This technique is referred to as oligonucleotide mediated gene repair, targeted gene repair, or targeted nucleotide alteration.\n\nTo improve the delivery of the new DNA into the cell, the DNA must be protected from damage and positively charged. Initially, anionic and neutral lipids were used for the construction of lipoplexes for synthetic vectors. However, in spite of the facts that there is little toxicity associated with them, that they are compatible with body fluids and that there was a possibility of adapting them to be tissue specific; they are complicated and time consuming to produce so attention was turned to the cationic versions.\n\nCationic lipids, due to their positive charge, were first used to condense negatively charged DNA molecules so as to facilitate the encapsulation of DNA into liposomes. Later it was found that the use of cationic lipids significantly enhanced the stability of lipoplexes. Also as a result of their charge, cationic liposomes interact with the cell membrane, endocytosis was widely believed as the major route by which cells uptake lipoplexes. Endosomes are formed as the results of endocytosis, however, if genes can not be released into cytoplasm by breaking the membrane of endosome, they will be sent to lysosomes where all DNA will be destroyed before they could achieve their functions. It was also found that although cationic lipids themselves could condense and encapsulate DNA into liposomes, the transfection efficiency is very low due to the lack of ability in terms of “endosomal escaping”. However, when helper lipids (usually electroneutral lipids, such as DOPE) were added to form lipoplexes, much higher transfection efficiency was observed. Later on, it was figured out that certain lipids have the ability to destabilize endosomal membranes so as to facilitate the escape of DNA from endosome, therefore those lipids are called fusogenic lipids. Although cationic liposomes have been widely used as an alternative for gene delivery vectors, a dose dependent toxicity of cationic lipids were also observed which could limit their therapeutic usages.\n\nThe most common use of lipoplexes has been in gene transfer into cancer cells, where the supplied genes have activated tumor suppressor control genes in the cell and decrease the activity of oncogenes. Recent studies have shown lipoplexes to be useful in transfecting respiratory epithelial cells.\n\nPolymersomes are synthetic versions of liposomes (vesicles with a lipid bilayer), made of amphiphilic block copolymers. They can encapsulate either hydrophilic or hydrophobic contents and can be used to deliver cargo such as DNA, proteins, or drugs to cells. Advantages of polymersomes over liposomes include greater stability, mechanical strength, blood circulation time, and storage capacity.\n\nComplexes of polymers with DNA are called polyplexes. Most polyplexes consist of cationic polymers and their fabrication is based on self-assembly by ionic interactions. One important difference between the methods of action of polyplexes and lipoplexes is that polyplexes cannot directly release their DNA load into the cytoplasm. As a result, co-transfection with endosome-lytic agents such as inactivated adenovirus was required to facilitate nanoparticle escape from the endocytic vesicle made during particle uptake. However, a better understanding of the mechanisms by which DNA can escape from endolysosomal pathway, i.e. proton sponge effect, has triggered new polymer synthesis strategies such as incorporation of protonable residues in polymer backbone and has revitalized research on polycation-based systems.\n\nDue to their low toxicity, high loading capacity, and ease of fabrication, polycationic nanocarriers demonstrate great promise compared to their rivals such as viral vectors which show high immunogenicity and potential carcinogenicity, and lipid-based vectors which cause dose dependence toxicity. Polyethyleneimine and chitosan are among the polymeric carriers that have been extensively studies for development of gene delivery therapeutics. Other polycationic carriers such as poly(beta-amino esters) and polyphosphoramidate are being added to the library of potential gene carriers. In addition to the variety of polymers and copolymers, the ease of controlling the size, shape, surface chemistry of these polymeric nano-carriers gives them an edge in targeting capability and taking advantage of enhanced permeability and retention effect.\n\nA dendrimer is a highly branched macromolecule with a spherical shape. The surface of the particle may be functionalized in many ways and many of the properties of the resulting construct are determined by its surface.\n\nIn particular it is possible to construct a cationic dendrimer, i.e. one with a positive surface charge. When in the presence of genetic material such as DNA or RNA, charge complimentarity leads to a temporary association of the nucleic acid with the cationic dendrimer. On reaching its destination the dendrimer-nucleic acid complex is then taken into the cell via endocytosis.\n\nIn recent years the benchmark for transfection agents has been cationic lipids. Limitations of these competing reagents have been reported to include: the lack of ability to transfect some cell types, the lack of robust active targeting capabilities, incompatibility with animal models, and toxicity. Dendrimers offer robust covalent construction and extreme control over molecule structure, and therefore size. Together these give compelling advantages compared to existing approaches.\n\nProducing dendrimers has historically been a slow and expensive process consisting of numerous slow reactions, an obstacle that severely curtailed their commercial development. The Michigan-based company Dendritic Nanotechnologies discovered a method to produce dendrimers using kinetically driven chemistry, a process that not only reduced cost by a magnitude of three, but also cut reaction time from over a month to several days. These new \"Priostar\" dendrimers can be specifically constructed to carry a DNA or RNA payload that transfects cells at a high efficiency with little or no toxicity.\n\nInorganic nanoparticles, such as gold, silica, iron oxide (ex. magnetofection) and calcium phosphates have been shown to be capable of gene delivery. Some of the benefits of inorganic vectors is in their storage stability, low manufacturing cost and often time, low immunogenicity, and resistance to microbial attack. Nanosized materials less than 100 nm have been shown to efficiently trap the DNA or RNA and allows its escape from the endosome without degradation. Inorganics have also been shown to exhibit improved in vitro transfection for attached cell lines due to their increased density and preferential location on the base of the culture dish. Quantum dots have also been used successfully and permits the coupling of gene therapy with a stable fluorescence marker. Engineered organic nanoparticles are also under development, which could be used for co-delivery of genes and therapeutic agents.\n\nCell-penetrating peptides (CPPs), also known as peptide transduction domains (PTDs), are short peptides (< 40 amino acids) that efficiently pass through cell membranes while being covalently or non-covalently bound to various molecules, thus facilitating these molecules’ entry into cells. Cell entry occurs primarily by endocytosis but other entry mechanisms also exist. Examples of cargo molecules of CPPs include nucleic acids, liposomes, and drugs of low molecular weight.\n\nCPP cargo can be directed into specific cell organelles by incorporating localization sequences into CPP sequences. For example, nuclear localization sequences are commonly used to guide CPP cargo into the nucleus. For guidance into mitochondria, a mitochondrial targeting sequence can be used; this method is used in protofection (a technique that allows for foreign mitochondrial DNA to be inserted into cells' mitochondria).\n\nDue to every method of gene transfer having shortcomings, there have been some hybrid methods developed that combine two or more techniques. Virosomes are one example; they combine liposomes with an inactivated HIV or influenza virus. This has been shown to have more efficient gene transfer in respiratory epithelial cells than either viral or liposomal methods alone. Other methods involve mixing other viral vectors with cationic lipids or hybridising viruses.\n\n", "id": "34217148", "title": "Vectors in gene therapy"}
{"url": "https://en.wikipedia.org/wiki?curid=34253886", "text": "Sticky and blunt ends\n\nDNA ends refer to the properties of the end of DNA molecules, which may be sticky ends (cohesive ends), blunt ends or in other forms. The concept is used in molecular biology, especially in cloning or when subcloning inserts DNA into vector DNA. Such ends may be generated by restriction enzymes that cut the DNA – a staggered cut generates two sticky ends, while a straight cut generate blunt ends. \n\nA sticky or cohesive end has protruding single-stranded strands with unpaired nucleotides called overhangs, each overhang can anneal with another complementary one to form base pairs. The two complementary cohesive ends of DNA can anneal together via hydrogen bonding, the stability of these paired ends depends on the melting temperature of the paired overhangs. DNA ligase can join two adjacent strands of DNA by forming a covalent bond between the sugar-phosphate moieties of adjacent nucleotides to join the two together via a phosphodiester bond in a process called ligation. The blunt ends however do not have such protruding strands, and therefore cannot anneal together, and consequently ligation between blunt ends is less efficient. \n\nA single-stranded non-circular DNA molecule has two non-identical ends, the 3' end and the 5' end (usually pronounced \"three prime end\" and \"five prime end\"). The numbers refer to the numbering of carbon atoms in the deoxyribose, which is a sugar forming an important part of the backbone of the DNA molecule. In the backbone of DNA the 5' carbon of one deoxyribose is linked to the 3' carbon of another by a phosphate group. The 5' carbon of this deoxyribose is again linked to the 3' carbon of the next, and so forth.\n\nWhen a molecule of DNA is double stranded, as DNA usually is, the two strands run in opposite directions. Therefore, one end of the molecule will have the 3' end of strand 1 and the 5' end of strand 2, and vice versa in the other end. However, the fact that the molecule is two stranded allows numerous different variations.\n\nThe simplest DNA end of a double stranded molecule is called a \"blunt end\". In a blunt-ended molecule both strands terminate in a base pair. Blunt ends are not always desired in biotechnology since when using a DNA ligase to join two molecules into one, the yield is significantly lower with blunt ends. When performing subcloning, it also has the disadvantage of potentially inserting the insert DNA in the opposite orientation desired. On the other hand, blunt ends are always compatible with each other. Here is an example of a small piece of blunt-ended DNA:\n\nNon-blunt ends are created by various \"overhangs\". An overhang is a stretch of unpaired nucleotides in the end of a DNA molecule. These unpaired nucleotides can be in either strand, creating either 3' or 5' overhangs. These overhangs are in most cases palindromic.\n\nThe simplest case of an overhang is a single nucleotide. This is most often adenosine and is created as a 3' overhang by some DNA polymerases. Most commonly this is used in cloning PCR products created by such an enzyme. The product is joined with a linear DNA molecule with 3' thymine overhangs. Since adenine and thymine form a base pair, this facilitates the joining of the two molecules by a ligase, yielding a circular molecule. Here is an example of an A-overhang:\n\nLonger overhangs are called \"cohesive ends\" or \"sticky ends\". They are most often created by restriction endonucleases when they cut DNA. Very often they cut the two DNA strands four base pairs from each other, creating a four-base 5' overhang in one molecule and a complementary 5' overhang in the other. These ends are called cohesive since they are easily joined back together by a ligase. Also, since different restriction endonucleases usually create different overhangs, it is possible to cut a piece of DNA with two different enzymes and then join it with another DNA molecule with ends created by the same enzymes. Since the overhangs have to be complementary in order for the ligase to work, the two molecules can only join in one orientation. This is often highly desirable in molecular biology.\n\nFor example, these two \"sticky\" ends are compatible:\n\nThey can form complementary base pairs in the overhang region:\n\nAcross from each single strand of DNA, we typically see adenine pair with thymine, and cytosine pair with guanine to form a parallel complementary strand as described below. Two nucleotide sequences which correspond to each other in this manner are referred to as complementary:\n\nA frayed end refers to a region of a double stranded (or other multi-stranded) DNA molecule near the end with a significant proportion of non-complementary sequences; that is, a sequence where nucleotides on the adjacent strands do not match up correctly:\n\nThe term \"frayed\" is used because the incorrectly matched nucleotides tend to avoid bonding, thus appearing similar to the strands in a fraying piece of rope.\n\nAlthough non-complementary sequences are also possible in the middle of double stranded DNA, mismatched regions away from the ends are not referred to as \"frayed\".\n\nRonald W. Davis first discovered sticky ends as the product of the action of EcoRI, the restriction endonuclease.\n\nSticky end links are different in their stability. Free energy of formation can be measured to estimate stability. Free energy approximations can be made for different sequences from data related to oligonucleotide UV thermal denaturation curves. Also predictions from molecular dynamics simulations show that some sticky end links are much stronger in stretch than the others.\n\n", "id": "34253886", "title": "Sticky and blunt ends"}
{"url": "https://en.wikipedia.org/wiki?curid=34275662", "text": "SABIO-Reaction Kinetics Database\n\nSABIO-RK (System for the Analysis of Biochemical Pathways - Reaction Kinetics) is a web-accessible database storing information about biochemical reactions and their kinetic properties.\n\nSABIO-RK comprises a reaction-oriented representation of quantitative information on reaction dynamics based on a given selected publication.\nThis comprises all available kinetic parameters together with their corresponding rate equations,\nas well as kinetic law and parameter types and experimental and environmental conditions under which the kinetic data were determined.\nAdditionally, SABIO-RK contains information about the underlying biochemical reactions and pathways including their reaction participants,\ncellular location and detailed information about the enzymes catalysing the reactions.\n\nThe data stored in SABIO-RK in a comprehensive manner is extracted manually from literature. This includes reactions, their participants (substrates, products), modifiers (inhibitors, activators, cofactors), catalyst details (e.g. EC enzyme classification, protein complex composition, wild type / mutant information), kinetic parameters together with corresponding rate equation, biological sources (organism, tissue, cellular location), environmental conditions (pH, temperature, buffer) and reference details. Data are adapted, normalized and annotated to controlled vocabularies, ontologies and external data sources including KEGG, UniProt, ChEBI, PubChem, NCBI, and PubMed.\nCurrently SABIO-RK contains more than 52.000 curated single entries, for example 42% of them are related to the kinetic law type Michaelis-Menten kinetics, more than 14% of the entries contain diverse types of inhibitions. Kinetic parameters in SABIO-RK include more than 41.700 velocity constants (V, k, rate constants), more than 42.500 K values (including S_half for Hill equations) and about 11.800 inhibition constants (Ki, IC50). Kinetic data are available for about 850 organisms, 6.700 different reactions and about 1.480 enzymes catalysing these reactions (as of December 2015).\nSeveral tools, databases and workflows in Systems Biology make use of SABIO-RK biochemical reaction data by integration into their framework including\nSYCAMORE,\nMeMo-RK,\nCellDesigner,\nPeroxisomeDB,\nTaverna workflows\nor tools like KineticsWizard software for data capture and analysis.\nAdditionally, SABIO-RK is part of MIRIAM registry, a set of guidelines for the annotation and curation of computational models\n\nThe usage of SABIO-RK is free of charge. Commercial users need a license.\nSABIO-RK offers several ways for data access:\n\nPathways can be exported to SBML. A prototype for export to BioPAX/SBPAX is being prepared for release.\n\n", "id": "34275662", "title": "SABIO-Reaction Kinetics Database"}
{"url": "https://en.wikipedia.org/wiki?curid=34458189", "text": "Rule-based modeling\n\nRule-based modeling is a modeling approach that uses a set of rules that indirectly specifies a mathematical model. The rule-set can either be translated into a model such as Markov chains or differential equations, or be treated using tools that directly work on the rule-set in place of a translated model, as the latter is typically much bigger. Rule-based modeling is especially effective in cases where the rule-set is significantly simpler than the model it implies, meaning that the model is a repeated manifestation of a limited number of patterns. An important domain where this is often the case is biochemical models of living organisms. Groups of mutually corresponding substances are subject to mutually corresponding interactions.\n\nEarly efforts to use rule-based modeling in simulation of biochemical systems include the stochastic simulation systems StochSim\nA widely used tool for rule-based modeling of biochemical networks is BioNetGen It is released under the GNU GPL, version 3. BioNetGen includes a language to describe chemical substances, including the states they can assume and the bindings they can undergo. These rules can be used to create a reaction network model or to perform computer simulations directly on the rule set. The biochemical modeling framework Virtual Cell includes a BioNetGen interpreter.\n\nA close alternative is the Kappa language. Another alternative is BioChemical Space language.\n", "id": "34458189", "title": "Rule-based modeling"}
{"url": "https://en.wikipedia.org/wiki?curid=34491593", "text": "Ankyrin-G binding motif of KCNQ2-3\n\nIn molecular biology, the ankyrin-G binding motif of KCNQ2-3 is a protein motif found in the potassium channels KCNQ2 and KCNQ3.\n\nInteractions with ankyrin-G (ankyrin-3) are crucial to the localisation of voltage-gated sodium channels (VGSCs) at the axon initial segment and for neurons to initiate action potentials. This conserved 9-amino acid motif ((V/A)P(I/L)AXXE(S/D)D) is required for ankyrin-G binding and functions to localise sodium channels to a variety of 'excitable' membrane domains both inside and outside of the nervous system. This motif has also been identified in the potassium channel 6TM proteins KCNQ2 and KCNQ3 that correspond to the M channels that exert a crucial influence over neuronal excitability. KCNQ2/KCNQ3 channels are preferentially localised to the surface of axons both at the axonal initial segment and more distally, and this axonal initial segment targeting of surface KCNQ channels is mediated by these ankyrin-G binding motifs of KCNQ2 and KCNQ3. KCNQ3 is a major determinant of M channel localisation to the AIS, rather than KCNQ2. Phylogenetic analysis reveals that anchor motifs evolved sequentially in chordates (NaV channel) and jawed vertebrates (KCNQ2/3).\n", "id": "34491593", "title": "Ankyrin-G binding motif of KCNQ2-3"}
{"url": "https://en.wikipedia.org/wiki?curid=34581395", "text": "Toeprinting assay\n\nThe toeprinting assay, also known as the primer extension inhibition assay, is a method used in molecular biology that allows you to examine the interactions between messenger RNA and ribosomes or other RNA-binding proteins. It is different from the more commonly used DNA footprinting assay. The toeprinting assay has been utilized to examine the formation of the translation initiation complex. To do a toeprint assay, you need the mRNA of interest, ribosomes, a DNA primer, free nucleotides, and reverse transcriptase, among other reagents. Reverse transcriptase is an enzyme that catalyzes the formation of DNA from RNA; it is used to generate cDNA because RNA is unstable and difficult to study experimentally. Normally, under these conditions, the reverse transcriptase would simply create a complete cDNA copy of the mRNA of interest. However, when the correct reagents and experimental conditions are used, reverse transcriptase will be blocked by any bound ribosomes, resulting in shorter cDNA fragments called toeprints when the results are observed on a sequencing gel. A schematic of a toeprinting assay can be found here: https://www.researchgate.net/figure/41124064_fig2_A-Schematic-of-the-primer-extension-inhibition-toeprint-assay-illustrating-how.\n", "id": "34581395", "title": "Toeprinting assay"}
{"url": "https://en.wikipedia.org/wiki?curid=35126108", "text": "RK2 plasmid\n\nThe RK2 Plasmid is a broad-host-range plasmid belonging to the incP incompatibility group It is notable for its ability to replicate in a wide variety of single-celled organisms, which makes it suitable as a genetic engineering tool. It is capable of transfer, replication, and maintenance in most genera of Gram-negative bacteria. RK2 may sometimes be referred to as pRK2, which is also the name of another, unrelated plasmid. The IncP-1 plasmid group (IncP plasmids in \"Escherichia coli\") of which RK2 is a part has been described as \"highly potent, self-transmissible, selfish DNA molecules with a complicated regulatory circuit\"\n\nRK 2 was first isolated in connection with an outbreak of antibotic-resistant \"Pseudomonas aeruginosa\" and \"Klebsiella aerogenes\" in Birmingham in 1969, as one of a family of plasmids implicated in transfer of Ampicillin resistance between bacterial strains. Plasmids in the IncP-1 subgroup has been isolated from wastewater, agricultural soil, and hospitals.\n\nRK2 is approximately 60 kbp long and contains genes for replication, maintenance, conjugation and antibiotic resistance. The resistance genes confer resistance to the antibiotics kanamycin, ampicillin and tetracycline. In addition, RK2 contains a set of potentially lethal (to the cell) genes, called \"kil\" genes, and a set of complementary transcriptional repressor genes, called \"kor\" (short for \"kil-override\") genes, which inactivate the \"kil\" genes. The \"kil\" and \"kor\" genes together are suspected to play a role in the broad host range of RK2.\n\nThe essential replication system in RK2 consists of an origin of replication, \"oriV\", and a gene, \"trfA\", whose gene product, the TrfA protein, binds to and activates \"oriV\". In \"Escherichia coli\", replication proceeds unidirectionally from \"oriV\" after activation by TrfA. In E. coli, multiple plasmid copies appear to cluster together, creating a few multiplasmid clusters in each cell. The copy number of RK2 is about 4-7 per cell in \"E. coli\" and 3 in \"P. aeruginosa\".\n\nSeveral minimal derivatives of RK2 have been prepared. In these plasmids most of the genes have been removed, leaving only genes essential for replication and one or more selectable markers. One such \"mini-replicon\" is the plasmid PFF1, which is 5873 basepairs long.\n\nPFF1 consists of an origin of replication, oriV, an origin of transfer, oriT, a gene coding for plasmid replication proteins, trfA, and two antibiotic resistance genes, \"bla\" and \"cat\", which confer resistance to Ampicillin and Chloramphenicol, respectively. Minimal plasmids such as PFF1 are useful for studying the basic mechanisms of plasmid replication and copy number regulation, as there are less superfluous genetic elements which might affect the processes being studied. Several mutants of PFF1 which affect the copy number of the plasmid have been identified. Two such mutants, PFF1cop254D and PFF1cop271C, increase the copy number of PFF1 in \"E. coli\" from approximately 39-40 to about 501 and 113 plasmids per cell, respectively. An increase in copy number is useful for genetic engineering applications to increase the production yield of recombinant protein.\n\n", "id": "35126108", "title": "RK2 plasmid"}
{"url": "https://en.wikipedia.org/wiki?curid=35322254", "text": "Samp1\n\nSamp1, is an inner nuclear membrane protein in mammals. Samp1 is known to interact with SUN2 and lamin A/C, and is believed to be involved in the stabilizing of the LINC complex during cell mitosis, facilitating the anchoring to the lamina. Lamin A/C is required for samp1 presence at the inner nuclear membrane. Samp1 is homologous to the \"S. Pombe\" inner nuclear membrane protein Ima1.\n", "id": "35322254", "title": "Samp1"}
{"url": "https://en.wikipedia.org/wiki?curid=21688106", "text": "Clone manager\n\nClone Manager is a commercial bioinformatics software work suite of Sci-Ed, that supports molecular biologists with data management and allows to perform certain \"in silico\" preanalysis.\n\n\n", "id": "21688106", "title": "Clone manager"}
{"url": "https://en.wikipedia.org/wiki?curid=32104707", "text": "Cellular noise\n\nCellular noise is random variability in quantities arising in cellular biology. For example, cells which are genetically identical, even within the same tissue, are often observed to have different expression levels of proteins, different sizes and structures. These apparently random differences can have important biological and medical consequences.\n\nCellular noise was originally, and is still often, examined in the context of gene expression levels – either the concentration or copy number of the products of genes within and between cells. As gene expression levels are responsible for many fundamental properties in cellular biology, including cells' physical appearance, behaviour in response to stimuli, and ability to process information and control internal processes, the presence of noise in gene expression has profound implications for many processes in cellular biology.\n\nThe most frequent quantitative definition of noise is the coefficient of variation:\n\nwhere formula_2 is the noise in a quantity formula_3, formula_4 is the mean value of formula_3 and formula_6 is the standard deviation of formula_3. This measure is dimensionless, allowing a relative comparison of the importance of noise, without necessitating knowledge of the absolute mean.\n\nOther quantities often used for mathematical convenience are the Fano factor:\n\nand the normalized variance:\n\nCellular noise is often investigated in the framework of \"intrinsic\" and \"extrinsic\" noise. Intrinsic noise refers to variation in identically-regulated quantities within a single cell: for example, the intra-cell variation in expression levels of two identically-controlled genes. Extrinsic noise refers to variation in identically-regulated quantities between different cells: for example, the cell-to-cell variation in expression of a given gene.\n\nIntrinsic and extrinsic noise levels are often compared in dual reporter studies, in which the expression levels of two identically-regulated genes (often fluorescent reporters like GFP and YFP) are plotted for each cell in a population.\n\n\"Note\": These lists are illustrative, not exhaustive, and identification of noise sources is an active and expanding area of research.\n\n\nNote that extrinsic noise can affect levels and types of intrinsic noise: for example, extrinsic differences in the mitochondrial content of cells lead, through differences in ATP levels, to some cells transcribing faster than others, affecting the rates of gene expression and the magnitude of intrinsic noise across the population.\n\n\"Note\": These lists are illustrative, not exhaustive, and identification of noise effects is an active and expanding area of research.\n\n\nAs many quantities of cell biological interest are present in discrete copy number within the cell (single DNAs, dozens of mRNAs, hundreds of proteins), tools from discrete stochastic mathematics are often used to analyse and model cellular noise. In particular, master equation treatments – where the probabilities formula_10 of observing a system in a state formula_11 at time formula_12 are linked through ODEs – have proved particularly fruitful. A canonical model for noise gene expression, where the processes of DNA activation, transcription and translation are all represented as Poisson processes with given rates, gives a master equation which may be solved exactly (with generating functions) under various assumptions or approximated with stochastic tools like Van Kampen's system size expansion.\n\nNumerically, the Gillespie algorithm or stochastic simulation algorithm is often used to create realisations of stochastic cellular processes, from which statistics can be calculated.\n\nThe problem of inferring the values of parameters in stochastic models (parametric inference) for biological processes, which are typically characterised by sparse and noisy experimental data, is an active field of research, with methods including Bayesian MCMC and approximate Bayesian computation proving adaptable and robust . Regarding the two-state model, a moment-based method was described for parameters inference from mRNAs distributions .\n", "id": "32104707", "title": "Cellular noise"}
{"url": "https://en.wikipedia.org/wiki?curid=2311903", "text": "Endogenous retrovirus\n\nEndogenous retroviruses (ERVs) are endogenous viral elements in the genome that closely resemble and can be derived from retroviruses. They are abundant in the genomes of jawed vertebrates, and they comprise up to 5–8% of the human genome (lower estimates of ~1%). ERVs are a subclass of a type of gene called a transposon, which can be packaged and moved within the genome to serve a vital role in gene expression and in regulation. They are distinguished as retrotransposons, which are Class I elements. Researchers have suggested that retroviruses evolved from a type of transposable gene called a retrotransposon, which includes ERVs; these genes can mutate and instead of moving to another location in the genome they can become exogenous or pathogenic. This means that not all ERVs may have originated as an insertion by a retrovirus but that some may have been the source for the genetic information in the retroviruses they resemble. When integration of viral DNA occurs in the germ-line, it can give rise to an ERV, which can later become fixed in the gene pool of the host population.\n\nThe replication cycle of a retrovirus entails the insertion (\"integration\") of a DNA copy of the viral genome into the nuclear genome of the host cell. Most retroviruses infect somatic cells, but occasional infection of germline cells (cells that produce eggs and sperm) can also occur. Rarely, retroviral integration may occur in a germline cell that goes on to develop into a viable organism. This organism will carry the inserted retroviral genome as an integral part of its own genome—an \"endogenous\" retrovirus (ERV) that may be inherited by its offspring as a novel allele. Many ERVs have persisted in the genome of their hosts for millions of years. However, most of these have acquired inactivating mutations during host DNA replication and are no longer capable of producing virus. ERVs can also be partially excised from the genome by a process known as recombinational deletion, in which recombination between the identical sequences that flank newly integrated retroviruses results in deletion of the internal, protein-coding regions of the viral genome.\n\nThe general retrovirus genome consists of three genes vital for the invasion, replication, escape, and spreading of its viral genome. These three genes are gag (encodes for structural proteins for the viral core), pol(encodes for reverse transcriptase, integrase, and protease), and env (encodes for coat proteins for the virus's exterior). These viral proteins are encoded as polyproteins. In order to carry out their life cycle, the retrovirus relies heavily on the host cell's machinery. Protease degrades peptide bonds of the viral polyproteins, making the separate proteins functional. Reverse transcriptase functions to synthesize viral DNA from the viral RNA in the host cell's cytoplasm before it entering the nucleus. Integrase guides the integration of viral DNA into the host genome.\n\nEndogenous retroviruses can play an active role in shaping genomes. Most studies in this area have focused on the genomes of humans and higher primates, but other vertebrates, such as mice and sheep, have also been studied in depth. The long terminal repeat (LTR) sequences that flank ERV genomes frequently act as alternate promoters and enhancers, often contributing to the transcriptome by producing tissue-specific variants. In addition, the retroviral proteins themselves have been co-opted to serve novel host functions, particularly in reproduction and development. Recombination between homologous retroviral sequences has also contributed to gene shuffling and the generation of genetic variation. Furthermore, in the instance of potentially antagonistic effects of retroviral sequences, repressor genes have co-evolved to combat them.\n\nSolo LTRs and LTRs associated with complete retroviral sequences have been shown to act as transcriptional elements on host genes. Their range of action is mainly by insertion into the 5' UTRs of protein coding genes; however, they have been known to act upon genes up to 70–100 kb away. The majority of these elements are inserted in the sense direction to their corresponding genes, but there has been evidence of LTRs acting in the antisense direction and as a bidirectional promoter for neighboring genes. In a few cases, the LTR functions as the major promoter for the gene. For example, in humans AMY1C has a complete ERV sequence in its promoter region; the associated LTR confers salivary specific expression of the digestive enzyme amylase. Also, the primary promoter for bile acid-CoA:amino acid N-acyltransferase (BAAT), which codes for an enzyme that is integral in bile metabolism, is of LTR origin.\n\nThe insertion of a solo ERV-9 LTR may have produced a functional open reading frame (ORF), causing the rebirth of the human immunity related GTPase gene (IRGM). ERV insertions have also been shown to generate alternative splice sites either by direct integration into the gene, as with the human leptin hormone receptor, or driven by the expression of an upstream LTR, as with the phospholipase A-2 like protein.\n\nMost of the time, however, the LTR functions as one of many alternate promoters, often conferring tissue-specific expression related to reproduction and development. In fact, 64% of known LTR-promoted transcription variants are expressed in reproductive tissues. For example, the gene CYP19 codes for aromatase P450, an important enzyme for estrogen synthesis, that is normally expressed in the brain and reproductive organs of most mammals. However, in primates, an LTR-promoted transcriptional variant confers expression to the placenta and is responsible for controlling estrogen levels during pregnancy. Furthermore, the neuronal apoptosis inhibitory protein (NAIP), normally widespread, has an LTR of the HERV-P family acting as a promoter that confers expression to the testis and prostate. Other proteins, such as nitric oxide synthase 3 (NOS3), interleukin-2 receptor B (IL2RB), and another mediator of estrogen synthesis, HSD17B1, are also alternatively regulated by LTRs that confer placental expression, but their specific functions are not yet known. The high degree of reproductive expression is thought to be an aftereffect of the method by which they were endogenized; however, this also may be due to a lack of DNA methylation in germ-line tissues.\n\nThe best-characterized instance of placental protein expression comes not from an alternatively promoted host gene but from a complete co-option of a retroviral protein. Retroviral fusogenic env proteins, which play a role in the entry of the virion into the host cell, have had an important impact on the development of mammalian placenta. In mammals, intact env proteins called syncytins are responsible for the formation and function of syncytiotrophoblasts. These multinucleated cells are mainly responsible for maintaining nutrient exchange and separating the fetus from the mother's immune system. It has been suggested that the selection and fixation of these proteins for this function have played a critical role in the evolution of viviparity.\n\nIn addition, the insertion of ERVs and their respective LTRs have the potential to induce chromosomal rearrangement due to recombination between viral sequences at inter-chromosomal loci. These rearrangements have been shown to induce gene duplications and deletions that largely contribute to genome plasticity and dramatically change the dynamic of gene function. Furthermore, retroelements in general are largely prevalent in rapidly evolving, mammal-specific gene families whose function is largely related to the response to stress and external stimuli. In particular, both human class I and class II MHC genes have a high density of HERV elements as compared to other multi-locus-gene families. It has been shown that HERVs have contributed to the formation of extensively duplicated duplicon blocks that make up the HLA class 1 family of genes. More specifically, HERVs primarily occupy regions within and between the break points between these blocks, suggesting that considerable duplication and deletions events, typically associated with unequal crossover, facilitated their formation. The generation of these blocks, inherited as immunohaplotypes, act as a protective polymorphism against a wide range of antigens that may have imbued humans with an advantage over other primates.\n\nFinally, the insertion of ERVs or ERV elements into genic regions of host DNA, or overexpression of their transcriptional variants, has a much higher potential to produce deleterious effects than positive ones. Their appearance into the genome has created a host-parasite co-evolutionary dynamic that proliferated the duplication and expansion of repressor genes. The most clear-cut example of this involves the rapid duplication and proliferation of tandem zinc-finger genes in mammal genomes. Zinc-finger genes, particularly those that include a KRAB domain, exist in high copy number in vertebrate genomes, and their range of functions are limited to transcriptional roles. It has been shown in mammals, however, that the diversification of these genes was due to multiple duplication and fixation events in response to new retroviral sequences or their endogenous copies to repress their transcription. The characteristic of placentas being very evolutionary distinct organs between different species has been suggested to result from the co-option of ERV enhancers. Regulatory mutations, instead of mutations in genes that encode for hormones and growth factors, support the known evolution of placental morphology, especially since the majority of hormone and growth factor genes are expressed in response to pregnancy, not during placental development. Researchers studied the regulatory landscape of placental development between the rat and mouse, two closely related species. This was done by mapping all regulatory elements of the rat trophoblast stem cells (TSCs) and comparing them to their orthologs in mouse TSCs. TSCs were observed because they reflect the initial cells that develop in the fetal placenta. Regardless of their tangible similarities, enhancer and repressed regions were mostly species-specific. However, most promoter sequences were conserved between mouse and rat. In conclusion to their study, researchers proposed that ERVs influenced species-specific placental evolution through mediation of placental growth, immunosuppression, and cell fusion.\n\nAnother example of ERV exploiting cellular mechanisms is p53, a tumor suppressor gene (TSG). DNA damage and cellular stress induces the p53 pathway, which results in cell apoptosis. Using chromatin immunoprecipitation with sequencing, thirty-percent of all p53-binding sites were located within copies of a few primate-specific ERV families. A study suggested that this benefits retroviruses because p53's mechanism provides a rapid induction of transcription, which leads to the exit of viral RNA from the host cell.\n\nThe majority of ERVs that occur in vertebrate genomes are ancient, inactivated by mutation, and have reached genetic fixation in their host species. For these reasons, they are extremely unlikely to have negative effects on their hosts except under unusual circumstances. Nevertheless, it is clear from studies in birds and non-human mammal species including mice, cats and koalas, that younger (i.e., more recently integrated) ERVs can be associated with disease. This has led researchers to propose a role for ERVs in several forms of human cancer and autoimmune disease, although conclusive evidence is lacking.\n\nIn humans, ERVs have been proposed to be involved in multiple sclerosis (MS). A specific association between MS and the ERVWE1, or \"syncytin\", gene, which is derived from an ERV insertion, has been reported, along with the presence of an \"MS-associated retrovirus\" (MSRV), in patients with the disease.\nHuman ERVs (HERVs) have also been implicated in ALS.\n\nIn 2004 it was reported that antibodies to HERVs were found in greater frequency in the sera of people with schizophrenia. Additionally, the cerebrospinal fluid of people with recent onset schizophrenia contained levels of a retroviral marker, reverse transcriptase, four times higher than control subjects. Researchers continue to look at a possible link between HERVs and schizophrenia, with the additional possibility of a triggering infection inducing schizophrenia.\n\nERVs have been found to be associated to disease not only through disease-causing relations, but also through immunity. The frequency of ERVs in long terminal repeats (LTRs) likely correlates to viral adaptations to take advantage of immunity signaling pathways that promote viral transcription and replication. A study done in 2016 investigated the benefit of ancient viral DNA integrated into a host through gene regulation networks induced by interferons, a branch of innate immunity. These cytokines are first to respond to viral infection and are also important in immunosurveillance for malignant cells. ERVs are predicted to act as cis-regulatory elements, but much of the adaptive consequences of this for certain physiological functions is still unknown. There is data that supports the general role of ERVs in the regulation of human interferon response, specifically to interferon-gamma (IFNG). For example, interferon-stimulated genes were found to be greatly enriched with ERVs bound by signal transducer and activator of transcription (STAT1) and/or Interferon regulatory factor (IRF1) in CD14+ macrophages.\n\nAnother idea proposed was that ERVs from the same family played a role in recruiting multiple genes into the same network of regulation. It was found that MER41 elements provided addition redundant regulatory enhancement to the genes located near STAT1 binding sites.\n\nFor humans, porcine endogenous retroviruses (PERVs) pose a concern when using porcine tissues and organs in xenotransplantion, the transplanting of living cells, tissues, and organs from an organism of one species to an organism of different species. Although pigs are generally the most suitable donors to treat human organ diseases due practical, financial, safety, and ethical reasons, PERVs cannot be removed from pigs due to its viral nature of integrating into the host genome and being passed into offspring. The consequences of cross-species transmission remains unexplored and has very dangerous potential.\n\nResearchers indicated that infection of human tissues by PERVs is very possible, especially in immunosuppressed individuals. An immunosuppressed condition could potentially permit a more rapid and tenacious replication of viral DNA, and would later on have less difficulty adapting to human-to-human transmission. Although known infectious pathogens present in the donor organ/tissue can be eliminated by breeding pathogen-free herds, unknown retroviruses can be present in the donor. These retroviruses are often latent and asymptomatic in the donor, but can become active in the recipient. Some examples of endogenous viruses that can infect and multiply in human cells are from baboons (BaEV), cats (RD114), and mice.\n\nThere are three different classes of PERVs, PERV-A, PERV-B, and PERV-C. PERV-A and PERV-B are polytropic and can infect human cells in vitro, while PERV-C is ecotropic and does not replicate on human cells. The major differences between the classes is in the receptor binding domain of the \"env\" protein and the long terminal repeats (LTRs) that influence the replication of each class. PERV-A and PERV-B display LTRs that have repeats in the U3 region. However, PERV-A and PERV-C show repeatless LTRs. Researchers found that PERVs in culture actively adapted to the repeat structure of their LTR in order to match the best replication performance a host cell could perform. At the end of their study, researchers concluded that repeatless PERV LTR evolved from the repeat-harboring LTR. This was likely to have occurred from insertional mutation and was proven through use of data on LTR and \"env\"/Env. It is thought that the generation of repeatless LTRs could be reflective of an adaptation process of the virus, changing from an exogenous to an endogenous lifestyle.\n\nA clinical trial study performed in 1999 sampled 160 patients who were treated with different living pig tissues and observed no evidence of a persistent PERV infection in 97% of the patients for whom a sufficient amount of DNA was available to PCR for amplification of PERV sequences. This study stated that retrospective studies are limited to find the true incidence of infection or associated clinical symptoms, however. It suggested using closely monitored prospective trials, which would provide a more complete and detailed evaluation of the possible cross-species PERV transmission and a comparison of the PERV.\n\nHuman endogenous retroviruses (HERV) proviruses comprise a significant part of the human genome, with approximately 98,000 ERV elements and fragments making up 5–8%. According to a study published in 2005, no HERVs capable of replication had been identified; all appeared to be defective, containing major deletions or nonsense mutations. This is because most HERVs are merely traces of original viruses, having first integrated millions of years ago.\n\nHuman endogenous retroviruses were discovered by accident using a couple of different experiments. Human genomic libraries were screened under low-stringency conditions using probes from animal retroviruses, allowing the isolation and characterization of multiple, though defective, proviruses, that represented various families. Another experiment depended on oligonucleotides with homology to viral primer binding sites.\n\nHERVs are classified based on their homologies to animal retroviruses. Families belong to Class I are similar in sequence to mammalian type C retroviruses. Families belonging to Class II show homology to mammalian type B and D retroviruses. For both classes, if homologies appear well conserved in the \"gag\", \"pol\", and \"env\" gene, they are grouped into a superfamily. There more Class I families known to exist.\n\nThere are two proposals for how HERVs became fixed in the human genome. The first assumes that sometime during human evolution, exogenous progenitors of HERV inserted themselves into germ line cells and then replicated along with the host's genes using and exploiting the host's cellular mechanisms. Because of their distinct genomic structure, HERVs were subjected to many rounds of amplification and transposition, which lead to a widespread distribution of retroviral DNA. The second hypothesis claims the continuous evolution of retro-elements from more simple structured ancestors.\n\nNevertheless, one family of viruses has been active since the divergence of humans and chimpanzees. This family, termed HERV-K (HML2), makes up less than 1% of HERV elements but is one of the most studied. There are indications it has even been active in the past few hundred thousand years, e.g., some human individuals carry more copies of HML2 than others. Traditionally, age estimates of HERVs are performed by comparing the 5' and 3' LTR of a HERV; however, this method is only relevant for full-length HERVs. A recent method, called cross-sectional dating, uses variations within a single LTR to estimate the ages of HERV insertions. This method is more precise in estimating HERV ages and can be used for any HERV insertions. Cross-sectional dating has been used to suggest that two members of HERV-K(HML2), HERV-K106 and HERV-K116, were active in the last 800,000 years and that HERV-K106 may have infected modern humans 150,000 years ago. However, the absence of known infectious members of the HERV-K(HML2) family, and the lack of elements with a full coding potential within the published human genome sequence, suggests to some that the family is less likely to be active at present. In 2006 and 2007, researchers working independently in France and the US recreated functional versions of HERV-K(HML2).\n\nMER41.AIM2 is an HERV that regulates the transcription of AIM2 (Absent in Melanoma 2) which encodes for a sensor of foreign cytosolic DNA. This acts as a binding site for AIM2, meaning that it is necessary for the transcription of AIM2. Researchers had shown this by deleting MER41.AIM2 in HeLa cells using CRISPR/Cas9, leading to an undetectable transcript level of AIM2 in modified HeLa cells. The control cells, which still contained the MER41.AIM2 ERV, were observed with normal amounts of AIM2 transcript. In terms of immunity, researchers concluded that MER41.AIM2 is necessary for an inflammatory response to infection.\n\nImmunological studies have shown some evidence for T cell immune responses against HERVs in HIV-infected individuals. The hypothesis that HIV induces HERV expression in HIV-infected cells led to the proposal that a vaccine targeting HERV antigens could specifically eliminate HIV-infected cells. The potential advantage of this novel approach is that, by using HERV antigens as surrogate markers of HIV-infected cells, it could circumvent the difficulty inherent in directly targeting notoriously diverse and fast-mutating HIV antigens.\n\nThere are a few classes of human endogenous retroviruses that still have intact open reading frames. For example, the expression of HERV-K, a biologically active family of HERV, produces proteins found in placenta. Furthermore, the expression of the envelope genes of HERV-W (ERVW-1)and HERV-FRD (ERVFRD-1) produces syncytins which are important for the generation of the syncytiotrophoblast cell layer during placentogenesis by inducing cell-cell fusion. The HUGO Gene Nomenclature Committee (HGNC) approves gene symbols for transcribed human ERVs.\n\nExample: A porcine ERV (PERV) Chinese-born minipig isolate, PERV-A-BM, was sequenced completely and along with different breeds and cell lines in order to understand its genetic variation and evolution. The observed number of nucleotide substitutions and among the different genome sequences helped researchers determine an estimate age that PERV-A-BM was integrated into its host genome, which was found to be of an evolutionary age earlier than the European-born pigs isolates.\n\nThis technique is used to find histone marks indicative of promoters and enhancers, which are binding sites for DNA proteins, and repressed regions and trimethylation. DNA methylation has been shown to be vital to maintain silencing of ERVs in mouse somatic cells, while histone marks are vital for the same purpose in embryonic stem cells (ESCs) and early embryogenesis.\n\nBecause most HERVs have no function, are selectively neutral, and are very abundant in primate genomes, they easily serve as phylogenetic markers for linkage analysis. They can be exploited by comparing the integration site polymorphisms or the evolving, proviral, nucleotide sequences of orthologs. To estimate of when integration occurred, researchers used distances from each phylogenetic tree to find the rate of molecular evolution at each particular locus. It is also useful that ERVs are rich in many animal species genomes (i.e. plants, insects, mollusks, fish, rodents, domestic pets, and livestock) because its application can be used to answer a variety of phylogenetic questions.\n\nThis is accomplished by comparing the different HERV from different evolutionary periods. For example, this study was done for different hominoids, which ranged from humans to apes and to monkeys. This is difficult to do with PERV because of the large diversity present.\n\nResearchers could analyze individual epigenomes and transcriptomes to study the reactivation of dormant transposable elements through epigenetic release and their potential associations with human disease and exploring the specifics of gene regulatory networks.\n\nLittle is known about an effective way to overcoming hyperacute rejection (HAR), which follows the activation of complement initiated by xenoreactive antibodies recognizing galactosyl-alpha1-3galatosyl (alpha-Gal) antigens on the donor epithelium.\n\nBecause retroviruses are able to recombine with each other and with other endogenous DNA sequences, it would be beneficial for gene therapy to explore the potential risks HERVs can cause, if any. Also, this ability of HERVs to recombine can be manipulated for site-directed integration by including HERV sequences in retroviral vectors.\n\nResearchers believe that RNA and proteins encoded for by HERV genes should continue to be explored for putative function in cell physiology and in pathological conditions. This would make sense to examine in order to more deeply define the biological significance of protein's synthesized.\n\n\n\n", "id": "2311903", "title": "Endogenous retrovirus"}
{"url": "https://en.wikipedia.org/wiki?curid=35155103", "text": "Conserved signature indels\n\nConserved signature inserts and deletions (CSIs) in protein sequences provide an important category of molecular markers for understanding phylogenetic relationships. CSIs, brought about by rare genetic changes, provide useful phylogenetic markers that are generally of defined size and they are flanked on both sides by conserved regions to ensure their reliability. While indels can be arbitrary inserts or deletions, CSIs are defined as only those protein indels that are present within conserved regions of the protein.\n\nThe CSIs that are restricted to a particular clade or group of species, generally provide good phylogenetic markers of common evolutionary descent. Due to the rarity and highly specific nature of such changes, it is less likely that they could arise independently by either convergent or parallel evolution (i.e. homoplasy) or synapomorphy. Other confounding factors such as differences in evolutionary rates at different sites or among different species also generally do not affect the interpretation of a CSI. By determining the presence or absence of CSIs in an out-group species, one can infer whether the ancestral form of the CSI was an insert or deletion and this can be used to develop a rooted phylogenetic relationship among organisms.\n\nMost CSIs that have been identified have been found to exhibit high predictive value and they generally retain the specificity for the originally identified clades of species. Therefore, based upon their presence or absence, it should be possible to identify both known and even previously unknown species belonging to these groups in different environments.\n\nGroup specific CSIs are commonly shared by different species belonging to a particular Taxon(e.g. genus, family, class, order, phylum) but they are not present in other groups. These CSIs were most likely introduced in an ancestor of the group of species before the members of the taxa diverged. They provide molecular means for distinguishing members of a particular taxon from all other organisms.\n\nFigure 1 shows an example of 5aa CSI found in all species belonging to the taxon X. This is a distinctive characteristic of this taxon as it is not found in any other species. This signature was likely introduced in a common ancestor of the species from this taxon. Similarly other group-specific signatures (not shown) could be shared by either A1 and A2 or B1 and B2, etc., or even by X1 and X2 or by X3 and X4, etc. The groups A, B, C, D and X, in this diagram could correspond to various bacterial or Eukaryotic phyla.\n\nGroup specific CSIs have been used in the past to determine the phylogenetic relationship of a number of bacterial phyla and subgroups within it. For example a 3 amino acid insert was uniquely shared by members of the phylum Thermotogae in the essential 50S ribosomal protein L7/L12, within a highly conserved region (82-124 amino acid). This is not present in any other bacteria species and could be used to characterize members of the phylum Thermotogae from all other bacteria. Group specific CSIs were also used to characterize subgroups within the phylum Thermotogae.\n\nMain-line CSIs are those in which a conserved insert or deletion is shared by several major phyla, but absent from other phyla.\n\nFigure 2 shows an example of 5aa CSI found in a conserved region that is commonly present in the species belonging to phyla X, Y and Z, but it is absent in other phyla (A, B and C). This signature indicates a specific relationship of taxa X, Y and Z and also A, B and C. Based upon the presence or absence of such an indel, in out-group species (viz. Archaea), it can be inferred whether the indel is an insert or a deletion, and which of these two groups A, B, C or X, Y, Z is ancestral.\n\nMain-line CSIs have been used in the past to determine the phylogenetic relationship of a number of bacterial phyla. The large CSI of about 150-180 amino acids within a conserved region of Gyrase B (between amino acids 529-751), is commonly shared between various Proteobacteria, Chlamydiales, Planctomycetes and Aquificales species. This CSI is absent in other ancestral bacterial phyla as well as Archaea. Similarly a large CSI of about 100 amino acids in RpoB homologs (between amino acids 919-1058) is present in various species belonging to Proteobacteria, Bacteroidetes-Chlorobi, Chlamydiales, Planctomycetes and Aquificales. This CSI is absent in other ancestral bacterial phyla as well as Archaea. In both cases one can infer that the groups lacking the CSI are ancestral.\n\nA key issue in bacterial phylogeny is to understand how different bacterial species are related to each other and their branching order from a common ancestor. Currently most phylogenetic trees are based on 16S rRNA or other genes/proteins. These trees are not always able to resolve key phylogenetic questions with a high degree of certainty. However in recent years the discovery and analyses of conserved indels (CSIs) in many universally distributed proteins have aided in this quest. The genetic events leading to them are postulated to have occurred at important evolutionary branch points and their species distribution patterns provide valuable information regarding the branching order and interrelationships among different bacterial phyla.\n\nRecently the phylogenetic relationship of the group Thermotogae was characterized based on the CSI approach. Previously no biochemical or molecular markers were known that could clearly distinguish the species of this phylum from all other bacteria. More than 60 CSIs that were specific for the entire Thermotogae phylum or its different subgroups were discovered. 18 CSIs are uniquely present in various Thermotogae species and provide molecular markers for the phylum. Additionally there were many CSIs that were specific for various thermotogae subgroups. 12 CSIs were specific for a clade consisting of various Thermotoga species except Tt. Lettingae. 14CSIs were specific for a clade consisting of the Fervidobacterium and Thermosipho genera and 18 CSIs were specific for the genus Thermosiphon.\n\nLastly 16 CSIs were reported that were shared by either some or all Thermotogae species or some species from other taxa such as Archaea, Aquificae, Firmicutes, Proteobacteria, Deinococcus, Fusobacteria, Dictyoglomus, Chloroflexi and eukaryotes. The shared presence of some of these CSIs could be due to lateral gene transfer (LGT) between these groups. However the number of CSIs that are commonly shared with other taxa is much smaller than those that are specific for Thermotogae and they do not exhibit any specific pattern. Hence they have no significant effect on the distinction of Thermotogae.\n\nMesophillic Crenarchaeotes were recently placed into a new phylum of Archaea called the Thaumarchaeota. However there are very few molecular markers that can distinguish this group of archaea from the phylum Crenarchaeota. A detailed phylogenetic study using the CSI approach was conducted to distinguish these phyla in molecular terms. 6 CSIs were uniquely found in various Thaumarchaeota, namely \"Cenarchaeum symbiosum\", \"Nitrosopumilus maritimus\" and a number of uncultured marine crenarchaeotes. 3 CSIs were found that were commonly shared between species belonging to Thaumarchaeota and Crenarchaeota. Additionally, a number of CSIs were found that are specific for different orders of Crenarchaeota- 3 CSIs for Sulfolobales, 5 CSIs for Thermoproteales, lastly 2 CSIs common for Sulfolobales and Desulfurococcales. The signatures described provide novel means for distinguishing Crenarchaeota and Thaumarchaeota, additionally they could be used as a tool for the classification and identification of related species.\n\nThe members of the order Pasteurellales are currently distinguished mainly based on their position in the branching of the 16srRNA tree. There are currently very few molecular markers known that can distinguish members of this order from other bacteria. A CSI approach was recently used to elucidate the phylogenetic relationships between the species in this order; more than 40 CSIs were discovered that were uniquely shared by all or most of the species. Two major clades are formed within this Pasteurellales: Clade I, encompassing Aggregatibacter, Pasteurella, Actinobacillus succinogenes, Mannheimia succiniciproducens, Haemophilus influenzae and Haemophilus somnus, was supported by 13 CSIs. Clade II, encompassing Actinobacillus pleuropneumoniae, Actinobacillus minor, Haemophilus ducreyi, Mannheimia haemolytica and Haemophilus parasuis, was supported by 9 CSIs. Based on these results, it was proposed that Pasteurellales be divided from its current one family into two different ones. Additionally, the signatures described would provide novel means of identifying undiscovered Pasteurellales species.\n\nThe class Gammaproteobacteria forms one of the largest groups of bacteria. It is currently distinguished from other bacteria solely by 16s rRNA-based phylogenetic trees. No molecular characteristics unique to the class or its different subgroups are known. A detailed CSI-based study was conducted to better understand the phylogeny of this class. Firstly, a phylogenetic tree based on concatenated sequences of a number of universally-distributed proteins was created. The branching order of the different orders of the class Gammaproteobacteria (from most recent to the earliest diverging) was: Enterobacteriales >Pasteurellales >Vibrionales, Aeromonadales >Alteromonadales >Oceanospirillales, Pseudomonadales >Chromatiales, Legionellales, Methylococcales, Xanthomonadales, Cardiobacteriales, Thiotrichales. Additionally, 4 CSIs were discovered that were unique to most species of the class Gammaproteobacteria. A 2 aa deletion in AICAR transformylase was uniquely shared by all gammaproteobacteria except for Francisella tularensis. A 4 aa deletion in RNA polymerase b-subunit and a 1 aa deletion in ribosomal protein L16 were found uniquely in various species belonging to the orders Enterobacteriales, Pasteurellales, Vibrionales, Aeromonadales and Alteromonadales, but were not found in other gammaproteobacteria. Lastly, a 2 aa deletion in leucyl-tRNA synthetase was commonly present in the above orders of the class Gammaproteobacteria and in some members of the order Oceanospirillales. Another CSI based study has also identified 4 CSIs that are exclusive to the order Xanthomonadales. Taken together, these two facts show that Xanthomonadales is a monophyletic group that is ancestral to other Gammaproteobacteria, which further shows that Xanthomonadales is an independent subdivision, and constitutes one of the deepest-branching lineages within the Gammaproteobacteria clade.\n\nThe exact phylogenetic relationship between plants, animals and fungi is not well understood. A small CSI-based study was conducted to elucidate this relationship. Four CSIs were used to place animals and fungi together as a monophyletic group, and exclude plants. These CSIs were found in two essential cellular proteins, elongation factor l and enolase. However, traditionally, this specific relationship between fungi and animals has not been supported.\n", "id": "35155103", "title": "Conserved signature indels"}
{"url": "https://en.wikipedia.org/wiki?curid=28327688", "text": "WikiPathways\n\nWikiPathways is a community resource for contributing and maintaining content dedicated to biological pathways. Any registered WikiPathways user can contribute, and anybody can become a registered user. Contributions are monitored by a group of admins, but the bulk of peer review, editorial curation, and maintenance is the responsibility of the user community. WikiPathways is built using MediaWiki software, a custom graphical pathway editing tool (PathVisio) and integrated BridgeDb databases covering major gene, protein, and metabolite systems.\n\nEach article at WikiPathways is dedicated to a particular pathway. Many types of molecular pathways are covered, including metabolic, signaling, regulatory, etc. and the supported species include human, mouse, zebrafish, fruit fly, C. elegans, yeast, rice and arabidopsis, as well as bacteria and plant species. Using a search feature, one can locate a particular pathway by name, by the genes and proteins it contains, or by the text displayed in its description. The pathway collection can also be browsed with combinations of species names and ontology-based categories.\n\nIn addition to the pathway diagram, each pathway page also includes a description, bibliography, pathway version history and list of component genes and proteins with linkouts to public resources. For individual pathway nodes, users can access a list of other pathways with that node. Pathway changes can be monitored by displaying previous revisions or by viewing differences between specific revisions. Using the pathway history one can also revert to a previous revision of a pathway.\nPathways can also be tagged with ontology terms from three major BioPortal ontologies (Pathway, Disease and Cell Type).\n\nThe pathway content at WikiPathways is freely available for download in a several data and image formats. WikiPathways is completely open access and open source. All content is available under Creative Commons-Attribution 3.0 license terms. All source code for WikiPathways and the PathVisio editor is available under the Apache License, Version 2.0.\n\nIn addition to various primary data formats (e.g. GPML, BioPAX, Reactome, KEGG, and RDF), WikiPathways supports a variety of ways to integrate and interact with pathway content. These include directed link-outs, image maps, RSS feeds and deep web services.\n\nWikiPathways content is used to annotate and cross-link Wikipedia articles covering various genes, proteins, metabolites and pathways. Here are a few examples:\n\n", "id": "28327688", "title": "WikiPathways"}
{"url": "https://en.wikipedia.org/wiki?curid=3525700", "text": "Organotroph\n\nAn organotroph is an organism that obtains hydrogen or electrons from organic substrates. This term is used in microbiology to classify and describe organisms based on how they obtain electrons for their respiration processes. Some organotrophs such as animals and many bacteria, are also heterotrophs. Organotrophs can be either anaerobic or aerobic.\n\nAntonym: Lithotroph, Adjective: Organotrophic.\n\nThe term was suggested in 1946 by Lwoff and collaborators.\n\n\n", "id": "3525700", "title": "Organotroph"}
{"url": "https://en.wikipedia.org/wiki?curid=36096691", "text": "GC box\n\nIn molecular biology, a GC box, also known as a GSG box, is a distinct pattern of nucleotides found in the promoter region of some eukaryotic genes. The GC box is upstream of the TATA box and approximately 110 bases upstream from the transcription initiation site. It has a consensus sequence GGGCGG which is position dependent and orientation independent. The GC elements are bound by transcription factors and have similar functions to enhancers. Some known GC box-binding proteins include Sp1, Krox/Egr, Wilms' tumor, MIGI, and CREA.\n\nThe GC box is commonly the binding site for Zinc finger proteins. An Alpha helix section of the protein corresponds with a major groove in the DNA. Zinc-fingers bind to triplet base pair sequences, with residue 21 binding to the first base pair, residue 18 binding to the second base pair, and residue 15 binding to the third base pair. The triplet base pairs can either be a GGG or a GCG. If residue 18 is a histidine, it will bind to a G, and if residue 18 is a glutamic acid, it will bind to a C. GC box-binding zinc-finger have between 2 and 4 fingers, making them interact with base pair sequences that are 6 to 8 base pairs in length.\n", "id": "36096691", "title": "GC box"}
{"url": "https://en.wikipedia.org/wiki?curid=36128950", "text": "Macromolecular assembly\n\nThe term macromolecular assembly (MA) refers to massive chemical structures such as viruses and non-biologic nanoparticles, cellular organelles and membranes and ribosomes, etc. that are complex mixtures of polypeptide, polynucleotide, polysaccharide or other polymeric macromolecules. They are generally of more than one of these types, and the mixtures are defined spatially (i.e., with regard to their chemical shape), and with regard to their underlying chemical composition and structure. Macromolecules are found in living and nonliving things, and are composed of many hundreds or thousands of atoms held together by covalent bonds; they are often characterized by repeating units (i.e., they are polymers). Assemblies of these can likewise be biologic or non-biologic, though the MA term is more commonly applied in biology, and the term supramolecular assembly is more often applied in non-biologic contexts (e.g., in supramolecular chemistry and nanotechnology). MAs of macromolecules are held in their defined forms by non-covalent intermolecular interactions (rather than covalent bonds), and can be in either non-repeating structures (e.g., as in the ribosome (image) and cell membrane architectures), or in repeating linear, circular, spiral, or other patterns (e.g., as in actin filaments and the flagellar motor, image). The process by which MAs are formed has been termed molecular self-assembly, a term especially applied in non-biologic contexts. A wide variety of physical/biophysical, chemical/biochemical, and computational methods exist for the study of MA; given the scale (molecular dimensions) of MAs, efforts to elaborate their composition and structure and discern mechanisms underlying their functions are at the forefront of modern structure science.\n\nThe complexes of macromolecules that are referred to as MAs occur ubiquitously in nature, where they are involved in the construction of viruses and all living cells. In addition, they play fundamental roles in all basic life processes (protein translation, cell division, vesicle trafficking, intra- and inter-cellular exchange of material between compartments, etc.). In each of these roles, complex mixtures of become organized in specific structural and spatial ways. While the individual macromolecules are held together by a combination of covalent bonds and \"intra\"molecular non-covalent forces (i.e., associations between parts within each molecule, via charge-charge interactions, van der Waals forces, and dipole-dipole interactions such as hydrogen bonds), by definition MAs themselves are held together solely via the noncovalent forces, except now exerted \"between\" molecules (i.e., intermolecular interactions).\n\nThe images above give an indication of the compositions and scale (dimensions) associated with MAs, though these just begin to touch on the complexity of the structures; in principle, each living cell is composed of MAs, but is itself an MA as well. In the examples and other such complexes and assemblies, MAs are each often millions of daltons in molecular weight (megadaltons, i.e., millions of times the weight of a single, simple atom), though still having measurable component ratios (stoichiometries) at some level of precision. As alluded to in the image legends, when properly prepared, MAs or component subcomplexes of MAs can often be crystallized for study by protein crystallography and related methods, or studied by other physical methods (e.g., spectroscopy, microscopy).\n\nVirus structures were among the first studied MAs; other biologic examples include ribosomes (partial image above), proteasomes, and translation complexes (with protein and nucleic acid components), procaryotic and eukaryotic transcription complexes, and nuclear and other biological pores that allow material passage between cells and cellular compartments. Biomembranes are also generally considered MAs, though the requirement for structural and spatial definition is modified to accommodate the inherent molecular dynamics of membrane lipids, and of proteins within lipid bilayers.\n\nThe study of MA structure and function is challenging, in particular because of their megadalton size, but also because of their complex compositions and varying dynamic natures. Most have had standard chemical and biochemical methods applied (methods of protein purification and centrifugation, chemical and electrochemical characterization, etc.). In addition, their methods of study include modern proteomic approaches, computational and atomic-resolution structural methods (e.g., X-ray crystallography), small-angle X-ray scattering (SAXS) and small-angle neutron scattering (SANS), force spectroscopy, and transmission electron microscopy and cryo-electron microscopy. Aaron Klug was recognized with the 1982 Nobel Prize in Chemistry for his work on structural elucidation using electron microscopy, in particular for protein-nucleic acid MAs including the tobacco mosaic virus (a structure containing a 6400 base ssRNA molecule and >2000 coat protein molecules). The crystallization and structure solution for the ribosome, MW ~ 2.5 MDa, an example of part of the protein synthetic 'machinery' of living cells, was object of the 2009 Nobel Prize in Chemistry awarded to Venkatraman Ramakrishnan, Thomas A. Steitz, and Ada E. Yonath.\n\nFinally, biology is not the sole domain of MAs. The fields of supramolecular chemistry and nanotechnology each have areas that have developed to elaborate and extend the principles first demonstrated in biologic MAs. Of particular interest in these areas has been elaborating the fundamental processes of molecular machines, and extending known machine designs to new types and processes.\n\n\n", "id": "36128950", "title": "Macromolecular assembly"}
{"url": "https://en.wikipedia.org/wiki?curid=36890786", "text": "Capillary electrochromatography\n\nCapillary electrochromatography (CEC) is a chromatographic technique in which the mobile phase is driven through the chromatographic bed by electroosmosis. Capillary electrochromatography is a combination of two analytical techniques, high-performance liquid chromatography and capillary electrophoresis. Capillary electrophoresis aims to separate analytes on the basis of their mass-to-charge ratio by passing a high voltage across ends of a capillary tube, which is filled with the analyte. High-performance liquid chromatography separates analytes by passing them, under high pressure, through a column filled with stationary phase. The interactions between the analytes and the stationary phase and mobile phase lead to the separation of the analytes. In capillary electrochromatography capillaries, packed with HPLC stationary phase, are subjected to a high voltage. Separation is achieved by electrophoretic migration of solutes and differential partitioning.\n\nCapillary electrochromatography (CEC) combines the principles used in HPLC and CE. The mobile phase is driven across the chromatographic bed using electroosmosis instead of pressure (as in HPLC). Electroosmosis is the motion of liquid induced by an applied potential across a porous material, capillary tube, membrane or any other fluid conduit. Electroosmotic flow is caused by the Coulomb force induced by an electric field on net mobile electric charge in a solution. Under alkaline conditions, the surface silanol groups of the fused silica will become ionised leading to a negatively charged surface. This surface will have a layer of positively charged ions in close proximity which are relatively immobilised. This layer of ions is called the Stern layer. The thickness of the double layer is given by the formula:\n\nwhere ε is the relative permittivity of the medium, ε is the permittivity of vacuum, R is the universal gas constant, T is the absolute temperature, c is the molar concentration, and F is the Faraday constant\n\nWhen an electric field is applied to the fluid (usually via electrodes placed at inlets and outlets), the net charge in the electrical double layer is induced to move by the resulting Coulomb force. The resulting flow is termed electroosmotic flow. In CEC positive ions of the electrolyte added along with the analyte accumulate in the electrical double layer of the particles of the column packing on application of an electric field they move towards the cathode and drag the liquid mobile phase with them.\n\nThe relationship between the linear velocity u of the liquid in the capillary and the applied electric field is given by the Smoluchowski equation as \n\nwhere ζ is the potential across the Stern layer (zeta potential), E is the electric field strength, and η is the viscosity of the solvent.\n\nSeparation of components in CEC is based on interactions between the stationary phase and differential electrophoretic migration of solutes.\n\nThe components of a capillary electrochromatograph are a sample vial, source and destination vials, a packed capillary, electrodes, a high voltage power supply, a detector, and a data output and handling device. The source vial, destination vial and capillary are filled with an electrolyte such as an aqueous buffer solution. The capillary is packed with stationary phase. To introduce the sample, the capillary inlet is placed into a vial containing the sample and then returned to the source vial (sample is introduced into the capillary via capillary action, pressure, or siphoning). The migration of the analytes is then initiated by an electric field that is applied between the source and destination vials and is supplied to the electrodes by the high-voltage power supply. The analytes separate as they migrate due to their electrophoretic mobility, and are detected near the outlet end of the capillary. The output of the detector is sent to a data output and handling device such as an integrator or computer. The data is then displayed as an electropherogram, which reports detector response as a function of time. Separated chemical compounds appear as peaks with different migration times in an electropherogram.\n\nAvoiding the use of pressure to introduce the mobile phase into the column, results in a number of important advantages. Firstly, the pressure driven flow rate across a column depends directly on the square of the particle diameter and inversely on the length of the column. This restricts the length of the column and size of the particle, particle size is seldom less than 3 micrometer and the length of the column is restricted to 25 cm. Electrically driven flow rate is independent of length of column and size. A second advantage of using electroosmosis to pass the mobile phase into the column is the plug-like flow velocity profile of EOF, which reduces the solute dispersion in the column, increasing column efficiency.\n\n\n", "id": "36890786", "title": "Capillary electrochromatography"}
{"url": "https://en.wikipedia.org/wiki?curid=127511", "text": "DNA sequencer\n\nA DNA sequencer is a scientific instrument used to automate the DNA sequencing process. Given a sample of DNA, a DNA sequencer is used to determine the order of the four bases: G (guanine), C (cytosine), A (adenine) and T (thymine). This is then reported as a text string, called a read. Some DNA sequencers can be also considered optical instruments as they analyze light signals originating from fluorochromes attached to nucleotides.\n\nThe first automated DNA sequencer, invented by Lloyd M. Smith, was introduced by Applied Biosystems in 1987. It used the Sanger sequencing method, a technology which formed the basis of the “first generation” of DNA sequencers and enabled the completion of the human genome project in 2001. This first generation of DNA sequencers are essentially automated electrophoresis systems that detect the migration of labelled DNA fragments. Therefore, these sequencers can also be used in the genotyping of genetic markers where only the length of a DNA fragment(s) needs to be determined (e.g. microsatellites, AFLPs).\n\nThe Human Genome Project catalysed the development of cheaper, high throughput and more accurate platforms known as Next Generation Sequencers (NGS) to sequence the human genome. These include the 454, SOLiD and Illumina DNA sequencing platforms. Next generation sequencing machines have increased the rate of DNA sequence substantially compared with previous Sanger methods. DNA samples can be prepared automatically in as little as 90 mins, while a human genome can be sequenced at 15 times coverage in a matter of days.\n\nMore recent, third-generation DNA sequencers such as SMRT and Oxford Nanopore measure the addition of nucleotides to a single DNA molecule in real time.\n\nBecause of limitations in DNA sequencer technology these reads are short compared to the length of a genome therefore the reads must be assembled into longer contigs. The data may also contain errors, caused by limitations in the DNA sequencing technique or by errors during PCR amplification. DNA sequencer manufacturers use a number of different methods to detect which DNA bases are present. The specific protocols applied in different sequencing platforms have an impact in the final data that is generated. Therefore, comparing data quality and cost across different technologies can be a daunting task. Each manufacturer provides their own ways to inform sequencing errors and scores. However, errors and scores between different platforms cannot always be compared directly. Since these systems rely on different DNA sequencing approaches, choosing the best DNA sequencer and method will typically depend on the experiment objectives and available budget.\n\nThe first DNA sequencing methods were developed by Gilbert (1973) and Sanger (1975). Gilbert introduced a sequencing method based on chemical modification of DNA followed by cleavage at specific bases whereas Sanger’s technique is based on dideoxynucleotide chain termination. The Sanger method became popular due to its increased efficiency and low radioactivity. The first automated DNA sequencer was the AB370A, introduced in 1986 by Applied Biosystems. The AB370A was able to sequence 96 samples simultaneously, 500 kilobases per day, and reaching read lengths up to 600 bases. This was the beginning of the “first generation” of DNA sequencers, which implemented Sanger sequencing, fluorescent dideoxy nucleotides and polyacrylamide gel sandwiched between glass plates - slab gels. The next major advance was the release in 1995 of the AB310 which utilized a linear polymer in a capillary in place of the slab gel for DNA strand separation by electrophoresis. These techniques formed the base for the completion of the human genome project in 2001. The human genome project catalysed the development of cheaper, high throughput and more accurate platforms known as Next Generation Sequencers (NGS). In 2005, 454 Life Sciences released the 454 sequencer, followed by Solexa Genome Analyzer and SOLiD (Supported Oligo Ligation Detection) by Agencourt in 2006. Applied Biosystems acquired Agencourt in 2006, and in 2007, Roche bought 454 Life Sciences, while Illumina purchased Solexa. Ion Torrent entered the market in 2010 and was acquired by Life Technologies (now Thermo Fisher Scientific). These are still the most common NGS systems due to their competitive cost, accuracy, and performance.\n\nMore recently, a third generation of DNA sequencers was introduced. The sequencing methods applied by these sequencers do not require DNA amplification (polymerase chain reaction – PCR), which speeds up the sample preparation before sequencing and reduces errors. In addition, sequencing data is collected from the reactions caused by the addition of nucleotides in the complementary strand in real time. Two companies introduced different approaches in their third-generation sequencers. Pacific Biosciences sequencers utilize a method called Single-molecule real-time (SMRT), where sequencing data is produced by light (captured by a camera) emitted when a nucleotide is added to the complementary strand by enzymes containing fluorescent dyes. Oxford Nanopore Technologies is another company developing third-generation sequencers data using electronic systems based on nanopore sensing technologies.\n\nDNA sequencers have been developed, manufactured, and sold by the following companies, among others.\n\nThe 454 DNA sequencer was the first next-generation sequencer to become commercially successful. It was developed by 454 Life Sciences and purchased by Roche in 2007. 454 utilizes the detection of pyrophosphate released by the DNA polymerase reaction when adding a nucleotide to the template strain.\n\nRoche currently manufactures two systems based on their pyrosequencing technology: the GS FLX+ and the GS Junior System. The GS FLX+ System promises read lengths of approximately 1000 base pairs while the GS Junior System promises 400 base pair reads. A predecessor to GS FLX+, the 454 GS FLX Titanium system was released in 2008, achieving an output of 0.7G of data per run, with 99.9% accuracy after quality filter, and a read length of up to 700bp. In 2009, Roche launched the GS Junior, a bench top version of the 454 sequencer with read length up to 400bp, and simplified library preparation and data processing.\n\nOne of the advantages of 454 systems is their running speed, Manpower can be reduced with automation of library preparation and semi-automation of emulsion PCR. A disadvantage of the 454 system is that it is prone to errors when estimating the number of bases in a long string of identical nucleotides. This is referred to as a homopolymer error and occurs when there are 6 or more identical bases in row. Another disadvantage is that the price of reagents is relatively more expensive compared with other next-generation sequencers.\n\nIn 2013 Roche announced that they would be shutting down development of 454 technology and phasing out 454 machines completely in 2016.\n\nRoche produces a number of software tools which are optimised for the analysis of 454 sequencing data. GS Run Processor converts raw images generated by a sequencing run into intensity values. The process consists of two main steps: image processing and signal processing. The software also applies normalization, signal correction, base-calling and quality scores for individual reads. The software outputs data in Standard Flowgram Format (or SFF) files to be used in data analysis applications (GS De Novo Assembler, GS Reference Mapper or GS Amplicon Variant Analyzer). GS De Novo Assembler is a tool for \"de novo\" assembly of whole-genomes up to 3GB in size from shotgun reads alone or combined with paired end data generated by 454 sequencers. It also supports de novo assembly of transcripts (including analysis), and also isoform variant detection. GS Reference Mapper maps short reads to a reference genome, generating a consensus sequence. The software is able to generate output files for assessment, indicating insertions, deletions and SNPs. Can handle large and complex genomes of any size. Finally, the GS Amplicon Variant Analyzer aligns reads from amplicon samples against a reference, identifying variants (linked or not) and their frequencies. It can also be used to detect unknown and low-frequency variants. It includes graphical tools for analysis of alignments.\n\nIllumina produces a number of next-generation sequencing machines using technology acquired from Manteia Predictive Medicine and developed by Solexa. Illumina makes a number of next generation sequencing machines using this technology including the HiSeq, Genome Analyzer IIx, MiSeq and the HiScanSQ, which can also process microarrays.\n\nThe technology leading to these DNA sequencers was first released by Solexa in 2006 as the Genome Analyzer. Illumina purchased Solexa in 2007. The Genome Analyzer uses a sequencing by synthesis method. The first model produced 1G per run. During the year 2009 the output was increased from 20G per run in August to 50G per run in December. In 2010 Illumina released the HiSeq 2000 with an output of 200 and then 600G per run which would take 8 days. At its release the HiSeq 2000 provided one of the cheapest sequencing platforms at $0.02 per million bases as costed by the Beijing Genomics Institute.\n\nIn 2011 Illumina released a benchtop sequencer called the MiSeq. At its release the MiSeq could generate 1.5G per run with paired end 150bp reads. A sequencing run can be performed in 10 hours when using automated DNA sample preparation.\n\nThe Illumina HiSeq uses two software tools to calculate the number and position of DNA clusters to assess the sequencing quality: the HiSeq control system and the real-time analyzer. These methods help to assess if nearby clusters are interfering with each other.\n\nLife Technologies (now Thermo Fisher Scientific) produces DNA sequencers under the Applied Biosystems and Ion Torrent brands. Applied Biosystems makes the SOLiD next-generation sequencing platform, and Sanger-based DNA sequencers such as the 3500 Genetic Analyzer. Under the Ion Torrent brand, Applied Biosystems produces four next-generation sequencers: the Ion PGM System, Ion Proton System, Ion S5 and Ion S5xl systems.. The company is also believed to be developing their new capillary DNA sequencer called SeqStudio that will be released early 2018. \n\nSOLiD systems was acquired by Applied Biosystems in 2006. SOLiD applies sequencing by ligation and dual base encoding. The first SOLiD system was launched in 2007, generating reading lengths of 35bp and 3G data per run. After five upgrades, the 5500xl sequencing system was released in 2010, considerably increasing read length to 85bp, improving accuracy up to 99.99% and producing 30G per 7-day run.\n\nThe limited read length of the SOLiD has remained a significant shortcoming and has to some extent limited its use to experiments where read length is less vital such as resequencing and transcriptome analysis and more recently ChIP-Seq and methylation experiments. The DNA sample preparation time for SOLiD systems has become much quicker with the automation of sequencing library preparations such as the Tecan system.\n\nThe colour space data produced by the SOLiD platform can be decoded into DNA bases for further analysis, however software that considers the original colour space information can give more accurate results. Life Technologies has released BioScope, a data analysis package for resequencing, ChiP-Seq and transcriptome analysis. It uses the MaxMapper algorithm to map the colour space reads.\n\nBeckman Coulter (now Danaher) has previously manufactured chain termination and capillary electrophoresis-based DNA sequencers under the model name CEQ, including the CEQ 8000. The company now produces the GeXP Genetic Analysis System, which uses dye terminator cycle sequencing. This method uses a thermocycler in much the same way as PCR to denature, anneal, and extend DNA fragments, amplifying the sequenced fragments.\n\nPacific Biosciences produces a sequencing system named the PacBio RS using a single molecule real time sequencing, or SMRT, method. This system can produce read lengths of multiple thousands of base pairs, though with a high rate of errors. These errors are corrected using optimized assembly strategies. Scientists have reported 99.9999% accuracy with these strategies. In 2015, the company launched a new sequencing instrument, the Sequel System, that has increased capacity and a lower price.\n\nOxford Nanopore Technologies has begun shipping early versions of its nanopore sequencing MinION sequencer to selected labs. The device is four inches long and gets power from a USB port. MinION decodes DNA directly as the molecule is drawn at the rate of 30 bases/second through a nanopore suspended in a membrane. Changes in electric current indicate which base is present. It is 60 to 85 percent accurate, compared with 99.9 percent in conventional machines. Even inaccurate results may prove useful because it produces “long reads.” In one case a continuous 79,000 base strand was read. PromethION is another (unreleased) product that will use as many as 100,000 pores in parallel, more suitable for high volume sequencing.\n", "id": "127511", "title": "DNA sequencer"}
{"url": "https://en.wikipedia.org/wiki?curid=1749134", "text": "Pharming (genetics)\n\nPharming, a portmanteau of \"farming\" and \"pharmaceutical\", refers to the use of genetic engineering to insert genes that code for useful pharmaceuticals into host animals or plants that would otherwise not express those genes, thus creating a genetically modified organism (GMO). Pharming is also known as molecular farming, molecular pharming or biopharming.\n\nThe products of pharming are recombinant proteins or their metabolic products. Recombinant proteins are most commonly produced using bacteria or yeast in a bioreactor, but pharming offers the advantage to the producer that it does not require expensive infrastructure, and production capacity can be quickly scaled to meet demand, at greatly reduced cost.\n\nThe first recombinant plant-derived protein (PDP) was human serum albumin, initially produced in 1990 in transgenic tobacco and potato plants. Open field growing trials of these crops began in the United States in 1992 and have taken place every year since. While the United States Department of Agriculture has approved planting of pharma crops in every state, most testing has taken place in Hawaii, Nebraska, Iowa, and Wisconsin.\n\nIn the early 2000s, the pharming industry was robust. Proof of concept has been established for the production of many therapeutic proteins, including antibodies, blood products, cytokines, growth factors, hormones, recombinant enzymes and human and veterinary vaccines. By 2003 several PDP products for the treatment of human diseases were under development by nearly 200 biotech companies, including recombinant gastric lipase for the treatment of cystic fibrosis, and antibodies for the prevention of dental caries and the treatment of non-Hodgkin's lymphoma.\n\nSeveral proteins were brought to market as research and bioproduction reagents, mostly by Sigma-Aldrich. ProdiGene struck agreements with Sigma to distribute ProdiGene's corn-produced aprotinin, trypsin, beta-glucuronidase (GUS), and avidin. Large Scale Biology and SIgma agreed that Sigma would distribute LSBC's tobacco-produced aprotinin. Sigma also agreed to distribute Ventria's rise-produced Lactoferrin and Lysozyme.\n\nHowever, in late 2002, just as ProdiGene was ramping up production of trypsin for commercial launch it was discovered that volunteer plants (left over from the prior harvest) of one of their GM corn products were harvested with the conventional soybean crop later planted in that field. ProdiGene was fined $250,000 and ordered by the USDA to pay over $3 million in cleanup costs. This raised a furor and set the pharming field back, dramatically. Many companies went bankrupt as companies faced difficulties getting permits for field trials and investors fled. In reaction, APHIS introduced more strict regulations for pharming field trials in the US in 2003. In 2005, Anheuser-Busch threatened to boycott rice grown in Missouri because of plans by Ventria Bioscience to grow pharm rice in the state. A compromise was reached, but Ventria withdrew its permit to plant in Missouri due to unrelated circumstances.\n\nThe industry has slowly recovered, by focusing on pharming in simple plants grown in bioreactors and on growing GM crops in greenhouses. Some companies and academic groups have continued with open-field trials of GM crops that produce drugs. In 2006 Dow AgroSciences received USDA approval to market a vaccine for poultry against Newcastle disease, produced in plant cell culture – the first plant-produced vaccine approved in the U.S.\n\nMilk is presently the most mature system to produce recombinant proteins from transgenic organisms. Blood, egg white, seminal plasma, and urine are other theoretically possible systems, but all have drawbacks. Blood, for instance, as of 2012 cannot store high levels of stable recombinant proteins, and biologically active proteins in blood may alter the health of the animals. Expression in the milk of a mammal, such as a cow, sheep, or goat, is a common application, as milk production is plentiful and purification from milk is relatively easy. Hamsters and rabbits have also been used in preliminary studies because of their faster breeding.\n\nOne approach to this technology is the creation of a transgenic mammal that can produce the biopharmaceutical in its milk (or blood or urine). Once an animal is produced, typically using the pronuclear microinjection method, it becomes efficacious to use cloning technology to create additional offspring that carry the favorable modified genome. In February 2009 the US FDA granted marketing approval for the first drug to be produced in genetically modified livestock. The drug is called ATryn, which is antithrombin protein purified from the milk of genetically modified goats. Marketing permission was granted by the European Medicines Agency in August 2006.\n\nAs indicated above, some mammals typically used for food production (such as goats, sheep, pigs, and cows) have been modified to produce non-food products, a practice sometimes called pharming. Use of genetically modified goats has been approved by the FDA and EMA to produce ATryn, i.e. recombinant antithrombin, an anticoagulant protein drug. These products \"produced by turning animals into drug-manufacturing 'machines' by genetically modifying them\" are sometimes termed biopharmaceuticals.\n\nThe patentability of such biopharmaceuticals and their process of manufacture is uncertain. Probably, the biopharmaceuticals themselves so made are unpatentable, assuming that they are chemically identical to the preexisting drugs that they imitate. Several 19th century United States Supreme Court decisions hold that a previously known natural product manufactured by artificial means cannot be patented. An argument can be made for the patentability of the process for manufacturing a biopharmaceutical, however, because genetically modifying animals so that they will produce the drug is dissimilar to previous methods of manufacture; moreover, one Supreme Court decision seems to hold open that possibility.\n\nOn the other hand, it has been suggested that the recent Supreme Court decision in \"Mayo v. Prometheus\" may create a problem in that, in accordance with the ruling in that case, \"it may be said that such and such genes manufacture this protein in the same way they always did in a mammal, they produce the same product, and the genetic modification technology used is conventional, so that the steps of the process 'add nothing to the laws of nature that is not already present. If the argument prevailed in court, the process would also be ineligible for patent protection. This issue has not yet been decided in the courts.\n\nPlant-made pharmaceuticals (PMPs), also referred to as pharming, is a sub-sector of the biotechnology industry that involves the process of genetically engineering plants so that they can produce certain types of therapeutically important proteins and associated molecules such as peptides and secondary metabolites. The proteins and molecules can then be harvested and used to produce pharmaceuticals.\n\nRecently, several non-crop plants such as the duckweed \"Lemna minor\" or the moss \"Physcomitrella patens\" have shown to be useful for the production of biopharmaceuticals. These frugal organisms can be cultivated in bioreactors (as opposed to being grown in fields), secrete the transformed proteins into the growth medium and, thus, substantially reduce the burden of protein purification in preparing recombinant proteins for medical use. In addition, both species can be engineered to cause secretion of proteins with human patterns of glycosylation, an improvement over conventional plant gene-expression systems. Biolex Therapeutics developed a duckweed-based expression platform; it sold that business to Synthon and declared bankruptcy in 2012.\n\nAdditionally, an Israeli company, Protalix, has developed a method to produce therapeutics in cultured transgenic carrot or tobacco cells. Protalix and its partner, Pfizer, received FDA approval to market its drug, taliglucerase alfa (Elelyso), treatment for Gaucher's disease, in 2012.\n\nArabidopsis is often used as a model organism to study gene expression in plants, while actual production may be carried out in maize, rice, potatoes, tobacco, flax or safflower. The advantage of rice and flax is that they are self-pollinating, and thus gene flow issues (see below) are avoided. However, human error could still result in pharm crops entering the food supply. Using a minor crop such as safflower or tobacco, avoids the greater political pressures and risk to the food supply involved with using staple crops such as beans or rice.\n\nThe regulation of genetic engineering concerns the approaches taken by governments to assess and manage the risks associated with the development and release of genetically modified crops. There are differences in the regulation of GM crops – including those used for pharming – between countries, with some of the most marked differences occurring between the USA and Europe. Regulation varies in a given country depending on the intended use of the products of the genetic engineering. For example, a crop not intended for food use is generally not reviewed by authorities responsible for food safety.\n\nThere are controversies around GMOs generally on several levels, including whether making them is ethical, issues concerning intellectual property and market dynamics; environmental effects of GM crops; and GM crops' role in industrial agricultural more generally. There are also specific controversies around pharming.\n\nPlants do not carry pathogens that might be dangerous to human health. Additionally, on the level of pharmacologically active proteins, there are no proteins in plants that are similar to human proteins. On the other hand, plants are still sufficiently closely related to animals and humans that they are able to correctly process and configure both animal and human proteins. Their seeds and fruits also provide sterile packaging containers for the valuable therapeutics and guarantee a certain storage life.\n\nGlobal demand for pharmaceuticals is at unprecedented levels. Expanding the existing microbial systems, although feasible for some therapeutic products, is not a satisfactory option on several grounds. Many proteins of interest are too complex to be made by microbial systems or by protein synthesis. These proteins are currently being produced in animal cell cultures, but the resulting product is often prohibitively expensive for many patients. For these reasons, science has been exploring other options for producing proteins of therapeutic value.\n\nThese pharmaceutical crops could become extremely beneficial in developing countries. The World Health Organization estimates that nearly 3 million people die each year from vaccine preventable disease, mostly in Africa. Diseases such as measles and hepatitis lead to deaths in countries where the people cannot afford the high costs of vaccines, but pharm crops could help solve this problem.\n\nWhile molecular farming is one application of genetic engineering, there are concerns that are unique to it. In the case of genetically modified (GM) foods, concerns focus on the safety of the food for human consumption. In response, it has been argued that the genes that enhance a crop in some way, such as drought resistance or pesticide resistance, are not believed to affect the food itself. Other GM foods in development, such as fruits designed to ripen faster or grow larger, are believed not to affect humans any differently from non-GM varieties.\n\nIn contrast, molecular farming is not intended for crops destined for the food chain. It produces plants that contain physiologically active compounds that accumulate in the plant’s tissues. Considerable attention is focused, therefore, on the restraint and caution necessary to protect both consumer health and environmental biodiversity.\n\nThe fact that the plants are used to produce drugs alarms activists. They worry that once production begins, the altered plants might find their way into the food supply or cross-pollinate with conventional, non-GM crops. These concerns have historical validation from the ProdiGene incident, and from the StarLink incident, in which GMO corn accidentally ended up in commercial food products. Activists also are concerned about the power of business. According to the Canadian Food Inspection Agency, in a recent report, says that U.S. demand alone for biotech pharmaceuticals is expanding at 13 percent annually and to reach a market value of $28.6 billion in 2004. Pharming is expected to be worth $100 billion globally by 2020.\n\n\"Please note that this list is by no means exhaustive.\"\n\nProjects known to be abandoned\n\n\n\n", "id": "1749134", "title": "Pharming (genetics)"}
{"url": "https://en.wikipedia.org/wiki?curid=584617", "text": "Small interfering RNA\n\nSmall interfering RNA (siRNA), sometimes known as short interfering RNA or silencing RNA, is a class of double-stranded RNA molecules, 20-25 base pairs in length, similar to miRNA, and operating within the RNA interference (RNAi) pathway. It interferes with the expression of specific genes with complementary nucleotide sequences by degrading mRNA after transcription, preventing translation.\n\nsiRNA can also act in RNAi-related pathways as an antiviral mechanism or play a role in the shaping of the chromatin structure of a genome. siRNAs and their role in post-transcriptional gene silencing (PTGS) were first discovered in plants by David Baulcombe's group at the Sainsbury Laboratory in Norwich, England and reported in \"Science\" in 1999. Thomas Tuschl and colleagues soon reported in \"Nature\" that synthetic siRNAs could induce RNAi in mammalian cells. This discovery led to a surge in interest in harnessing RNAi for biomedical research and drug development. Significant developments in siRNA therapies have been made with both organic (carbon based) and inorganic (non-carbon based) nanoparticles, such as these which have been successful in drug delivery to the brain, offering promising methods of delivery into human subjects. However, significant barriers to successful siRNA therapies remain, the most significant of which is off-targeting.\n\nsiRNAs have a well-defined structure: a short (usually 20 to 24-bp) double-stranded RNA (dsRNA) with phosphorylated 5' ends and hydroxylated 3' ends with two overhanging nucleotides.\nThe Dicer enzyme catalyzes production of siRNAs from long dsRNAs and small hairpin RNAs. siRNAs can also be introduced into cells by transfection. Since in principle any gene can be knocked down by a synthetic siRNA with a complementary sequence, siRNAs are an important tool for validating gene function and drug targeting in the post-genomic era.\n\nThe mechanism by which siRNA causes gene silencing through repression of transcription occurs as follows:\n\nsiRNA is also similar to miRNA, however, miRNAs are derived from shorter stemloop RNA products, typically silence genes by repression of translation, and have broader specificity of action, while siRNAs typically work by cleaving the mRNA before translation, and have 100% complementarity, thus very tight target specificity.\n\nGene knockdown by transfection of exogenous siRNA is often unsatisfactory because the effect is only transient, especially in rapidly dividing cells. This may be overcome by creating an expression vector for the siRNA. The siRNA sequence is modified to introduce a short loop between the two strands. The resulting transcript is a short hairpin RNA (shRNA), which can be processed into a functional siRNA by Dicer in its usual fashion.. Typical transcription cassettes use an RNA polymerase III promoter (e.g., U6 or H1) to direct the transcription of small nuclear RNAs (snRNAs) (U6 is involved in gene splicing; H1 is the RNase component of human RNase P). It is theorized that the resulting siRNA transcript is then processed by Dicer.\n\nThe gene knockdown efficiency can also be improved by using cell squeezing.\nThe activity of siRNAs in RNAi is largely dependent on its binding ability to the RNA-induced silencing complex (RISC). Binding of the duplex siRNA to RISC is followed by unwinding and cleavage of the sense strand with endonucleases. The remaining anti-sense strand-RISC complex can then bind to target mRNAs for initiating transcriptional silencing.\n\nIt has been found that dsRNA can also activate gene expression, a mechanism that has been termed \"small RNA-induced gene activation\" or RNAa. It has been shown that dsRNAs targeting gene promoters induce potent transcriptional activation of associated genes. RNAa was demonstrated in human cells using synthetic dsRNAs, termed \"small activating RNAs\" (saRNAs). It is currently not known whether RNAa is conserved in other organisms.\n\nThe siRNA induced post transcriptional gene silencing starts with the assembly of the RNA-induced silencing complex (RISC). The complex silences certain gene expression by cleaving the mRNA molecules coding the target genes. To begin the process, one of the two siRNA strands, the guide strand, will be loaded into the RISC while the other strand, the passenger strand, is degraded. Certain Dicer enzymes may be responsible of loading the guide strand into RISC. Then, the siRNA scans for and direct RISC to perfectly complementary sequence on the mRNA molecules. The cleavage of the mRNA molecules is thought to be catalyzed by the Piwi domain of Argonaute proteins of the RISC. The mRNA molecule is then cut precisely by cleaving the phosphodiester bond between the target nucleotides which are paired to siRNA residues 10 and 11, counting from the 5’end. This cleavage results in mRNA fragments that are further degraded by cellular exonucleases. The 5’ fragment is degraded from its 3’ end by exosome while the 3’ fragment is degraded from its 5’ end by 5' -3' exoribonuclease 1(XRN1). Dissociation of the target mRNA strand from RISC after the cleavage allow more mRNA to be silenced. This dissociation process is likely to be promoted by extrinsic factors driven by ATP hydrolysis.\n\nSometimes cleavage of the target mRNA molecule does not occur. In some cases, the endonucleolytic cleavage of the phosphodiester backbone may be suppressed by mismatches of siRNA and target mRNA near the cleaving site. Other times, the Argonaute proteins of the RISC lack endonuclease activity even when the target mRNA and siRNA are perfectly paired. In such cases, gene expression will be silenced by miRNA induced mechanism instead.\nPiwi-interacting RNAs are responsible for the silencing of transposons and are not siRNAs.\n\nBecause RNAi intersects with a number of other pathways, it is not surprising that on occasion nonspecific effects are triggered by the experimental introduction of an siRNA. When a mammalian cell encounters a double-stranded RNA such as an siRNA, it may mistake it as a viral by-product and mount an immune response. Furthermore, because structurally related microRNAs modulate gene expression largely via incomplete complementarity base pair interactions with a target mRNA, the introduction of an siRNA may cause unintended off-targeting.\n\nIntroduction of too much siRNA can result in nonspecific events due to activation of innate immune responses. Most evidence to date suggests that this is probably due to activation of the dsRNA sensor PKR, although retinoic acid-inducible gene I (RIG-I) may also be involved. The induction of cytokines via toll-like receptor 7 (TLR7) has also been described. One promising method of reducing the nonspecific effects is to convert the siRNA into a microRNA. MicroRNAs occur naturally, and by harnessing this endogenous pathway it should be possible to achieve similar gene knockdown at comparatively low concentrations of resulting siRNAs. This should minimize nonspecific effects.\n\nOff-targeting is another challenge to the use of siRNAs as a gene knockdown tool. Here, genes with incomplete complementarity are inadvertently downregulated by the siRNA (in effect, the siRNA acts as a miRNA), leading to problems in data interpretation and potential toxicity. This, however, can be partly addressed by designing appropriate control experiments, and siRNA design algorithms are currently being developed to produce siRNAs free from off-targeting. Genome-wide expression analysis, e.g., by microarray technology, can then be used to verify this and further refine the algorithms. A 2006 paper from the laboratory of Dr. Khvorova implicates 6- or 7-basepair-long stretches from position 2 onward in the siRNA matching with 3'UTR regions in off-targeted genes.\n\nPlain RNAs may be poor immunogens, but antibodies can easily be created against RNA-protein complexes. Many autoimmune diseases see these types of antibodies. There haven’t yet been reports of antibodies against siRNA bound to proteins. Some methods for siRNA delivery adjoin polyethylene glycol (PEG) to the oligonucleotide reducing excretion and improving circulating half-life. However recently a large Phase III trial of PEGylated RNA aptamer against factor IX had to be discontinued by Regado Biosciences because of a severe anaphylactic reaction to the PEG part of the RNA. This reaction led to death in some cases and raises significant concerns about siRNA delivery when PEGylated oligonucleotides are involved.\n\nsiRNAs have been chemically modified to enhance their therapeutic properties, such as enhanced activity, increased serum stability, fewer off-targets and decreased immunological activation. A detailed database of all such chemical modifications is manually curated as siRNAmod in scientific literature.\n\nGiven the ability to knock down, in essence, any gene of interest, RNAi via siRNAs has generated a great deal of interest in both basic and applied biology.\n\nOne of the biggest challenges to siRNA and RNAi based therapeutics is intracellular delivery. Delivery of siRNA via nanoparticles has shown promise. siRNA oligos in vivo are vulnerable to degradation by plasma and tissue nucleases and have shown only mild effectiveness in localized delivery sites, such as the human eye. Delivering pure DNA to target organisms is challenging because its large size and structure prevents it from diffusing readily across membranes. siRNA oligos circumvent this problem due to their small size of 21-23 oligos. This allows delivery via nano-scale delivery vehicles called nanovectors.\n\nA good nanovector for siRNA delivery should protect siRNA from degradation, enrich siRNA in the target organ and facilitate the cellular uptake of siRNA. The three main groups of siRNA nanovectors are: lipid based, non-lipid organic-based, and inorganic. Lipid based nanovectors are excellent for delivering siRNA to solid tumors, but other cancers may require different non-lipid based organic nanovectors such as cyclodextrin based nanoparticles.\n\nsiRNAs delivered via lipid based nanoparticles have been shown to have therapeutic potential for central nervous system (CNS) disorders. Central nervous disorders are not uncommon, but the blood brain barrier (BBB) often blocks access of potential therapeutics to the brain. siRNAs that target and silence efflux proteins on the BBB surface have been shown to create an increase in BBB permeability. siRNA delivered via lipid based nanoparticles is able to cross the BBB completely.\n\nA huge difficulty in siRNA delivery is the problem of off-targeting. Since genes are read in both directions, there exists a possibility that even if the intended antisense siRNA strand is read and knocks out the target mRNA, the sense siRNA strand may target another protein involved in another function.\n\nPhase I results of the first two therapeutic RNAi trials (indicated for age-related macular degeneration, aka AMD) reported at the end of 2005 that siRNAs are well tolerated and have suitable pharmacokinetic properties.\n\nIn a phase 1 clinical trial, 41 patients with advanced cancer metastasised to liver were administered RNAi delivered through lipid nanoparticles. The RNAi targeted two genes encoding key proteins in the growth of the cancer cells, vascular endothelial growth factor, (VEGF), and kinesin spindle protein (KSP). The results showed clinical benefits, with the cancer either stabilized after six months, or regression of metastasis in some of the patients. Pharmacodynamic analysis of biopsy samples from the patients revealed the presence of the RNAi constructs in the samples, proving that the molecules reached the intended target.\n\nProof of concept trials have indicated that Ebola-targeted siRNAs may be effective as post-exposure prophylaxis in humans, with 100% of non-human primates surviving a lethal dose of Zaire Ebolavirus, the most lethal strain.\n\nDelivering siRNA intracellular continues to be a challenge. There are three main techniques of delivery for siRNA that differ on efficiency and toxicity.\n\nIn this technique siRNA first must be designed against the target gene. Once the siRNA is configured against the gene it has to be effectively delivered through a transfection protocol. Delivery is usually done by cationic liposomes, polymer nanoparticles, and lipid conjugation. This method is advantageous because it can deliver siRNA to most types of cells, has high efficiency and reproducibility, and is offered commercially. The most common commercial reagents for transfection of siRNA are Lipofectamine and Neon Transfection. However it is not compatible with all cell types, and has low in vivo efficiency.\n\nElectrical pulses are also used to intracellularly deliver siRNA into cells. The cell membrane is made of phospholipids which makes it susceptible to an electric field. When quick but powerful electrical pulses are initiated the lipid molecules reorient themselves, while undergoing thermal phase transitions because of heating. This results in the making of hydrophilic pores and localized perturbations in the lipid bilayer cell membrane also causing a temporary loss of semipermeability. This allows for the escape of many intracellular contents, such as ions and metabolites as well as the simultaneous uptake of drugs, molecular probes, and nucleic acids. For cells that are difficult to transfect electroporation is advantageous however cell death is more probable under this technique.\n\nThis method has been used to deliver siRNA targeting VEGF into the xenografted tumors in nude mice, which resulted in a significant suppression of tumor growth.\n\nThe gene silencing effects of transfected designed siRNA are generally transient, but this difficulty can be overcome through an RNAi approach. Delivering this siRNA from DNA templates can be done through several recombinant viral vectors based on retrovirus, adeno-associated virus, adenovirus, and lentivirus. The latter is the most efficient virus that stably delivers siRNA to target cells as it can transduce nondividing cells as well as directly target the nucleus. These specific viral vectors have been synthesized to effectively facilitate siRNA that is not viable for transfection into cells. Another aspect is that in some cases synthetic viral vectors can integrate shRNA into the cell genome which allows for stable expression of siRNA and long-term gene knockdown. This technique is advantageous because it is in vivo and effective for difficult to transfect cell. However problems arise because it can trigger antiviral responses in some cell types leading to mutagenic and immunogenic effects.\n\nThis method has potential use in gene silencing of the central nervous system for the treatment of Huntington’s disease.\n\n\n", "id": "584617", "title": "Small interfering RNA"}
{"url": "https://en.wikipedia.org/wiki?curid=25758", "text": "RNA\n\nRibonucleic acid (RNA) is a polymeric molecule essential in various biological roles in coding, decoding, regulation, and expression of genes. RNA and DNA are nucleic acids, and, along with lipids, proteins and carbohydrates, constitute the four major macromolecules essential for all known forms of life. Like DNA, RNA is assembled as a chain of nucleotides, but unlike DNA it is more often found in nature as a single-strand folded onto itself, rather than a paired double-strand. Cellular organisms use messenger RNA (mRNA) to convey genetic information (using the letters G, U, A, and C to denote the nitrogenous bases guanine, uracil, adenine, and cytosine) that directs synthesis of specific proteins. Many viruses encode their genetic information using an RNA genome.\n\nSome RNA molecules play an active role within cells by catalyzing biological reactions, controlling gene expression, or sensing and communicating responses to cellular signals. One of these active processes is protein synthesis, a universal function where RNA molecules direct the assembly of proteins on ribosomes. This process uses transfer RNA (tRNA) molecules to deliver amino acids to the ribosome, where ribosomal RNA (rRNA) then links amino acids together to form proteins.\n\nLike DNA, most biologically active RNAs, including mRNA, tRNA, rRNA, snRNAs, and other non-coding RNAs, contain self-complementary sequences that allow parts of the RNA to fold and pair with itself to form double helices. Analysis of these RNAs has revealed that they are highly structured. Unlike DNA, their structures do not consist of long double helices, but rather collections of short helices packed together into structures akin to proteins.\nIn this fashion, RNAs can achieve chemical catalysis (like enzymes). For instance, determination of the structure of the ribosome—an enzyme that catalyzes peptide bond formation—revealed that its active site is composed entirely of RNA.\n\nEach nucleotide in RNA contains a ribose sugar, with carbons numbered 1' through 5'. A base is attached to the 1' position, in general, adenine (A), cytosine (C), guanine (G), or uracil (U). Adenine and guanine are purines, cytosine and uracil are pyrimidines. A phosphate group is attached to the 3' position of one ribose and the 5' position of the next. The phosphate groups have a negative charge each, making RNA a charged molecule (polyanion). The bases form hydrogen bonds between cytosine and guanine, between adenine and uracil and between guanine and uracil. However, other interactions are possible, such as a group of adenine bases binding to each other in a bulge,\nor the GNRA tetraloop that has a guanine–adenine base-pair.\nAn important structural feature of RNA that distinguishes it from DNA is the presence of a hydroxyl group at the 2' position of the ribose sugar. The presence of this functional group causes the helix to mostly adopt the A-form geometry, although in single strand dinucleotide contexts, RNA can rarely also adopt the B-form most commonly observed in DNA. The A-form geometry results in a very deep and narrow major groove and a shallow and wide minor groove. A second consequence of the presence of the 2'-hydroxyl group is that in conformationally flexible regions of an RNA molecule (that is, not involved in formation of a double helix), it can chemically attack the adjacent phosphodiester bond to cleave the backbone.\nRNA is transcribed with only four bases (adenine, cytosine, guanine and uracil), but these bases and attached sugars can be modified in numerous ways as the RNAs mature. Pseudouridine (Ψ), in which the linkage between uracil and ribose is changed from a C–N bond to a C–C bond, and ribothymidine (T) are found in various places (the most notable ones being in the TΨC loop of tRNA). Another notable modified base is hypoxanthine, a deaminated adenine base whose nucleoside is called inosine (I). Inosine plays a key role in the wobble hypothesis of the genetic code.\n\nThere are more than 100 other naturally occurring modified nucleosides. The greatest structural diversity of modifications can be found in tRNA, while pseudouridine and nucleosides with 2'-O-methylribose often present in rRNA are the most common. The specific roles of many of these modifications in RNA are not fully understood. However, it is notable that, in ribosomal RNA, many of the post-transcriptional modifications occur in highly functional regions, such as the peptidyl transferase center and the subunit interface, implying that they are important for normal function.\n\nThe functional form of single-stranded RNA molecules, just like proteins, frequently requires a specific tertiary structure. The scaffold for this structure is provided by secondary structural elements that are hydrogen bonds within the molecule. This leads to several recognizable \"domains\" of secondary structure like hairpin loops, bulges, and internal loops. Since RNA is charged, metal ions such as Mg are needed to stabilise many secondary and tertiary structures.\n\nThe naturally occurring enantiomer of RNA is -RNA composed of -ribonucleotides. All chirality centers are located in the -ribose. By the use of -ribose or rather -ribonucleotides, -RNA can be synthesized. -RNA is much more stable against degradation by RNase.\n\nLike other structured biopolymers such as proteins, one can define topology of a folded RNA molecule. This is often done based on arrangement of intra-chain contacts within a folded RNA, termed as circuit topology.\n\nSynthesis of RNA is usually catalyzed by an enzyme—RNA polymerase—using DNA as a template, a process known as transcription. Initiation of transcription begins with the binding of the enzyme to a promoter sequence in the DNA (usually found \"upstream\" of a gene). The DNA double helix is unwound by the helicase activity of the enzyme. The enzyme then progresses along the template strand in the 3’ to 5’ direction, synthesizing a complementary RNA molecule with elongation occurring in the 5’ to 3’ direction. The DNA sequence also dictates where termination of RNA synthesis will occur.\n\nPrimary transcript RNAs are often modified by enzymes after transcription. For example, a poly(A) tail and a 5' cap are added to eukaryotic pre-mRNA and introns are removed by the spliceosome.\n\nThere are also a number of RNA-dependent RNA polymerases that use RNA as their template for synthesis of a new strand of RNA. For instance, a number of RNA viruses (such as poliovirus) use this type of enzyme to replicate their genetic material. Also, RNA-dependent RNA polymerase is part of the RNA interference pathway in many organisms.\n\nMessenger RNA (mRNA) is the RNA that carries information from DNA to the ribosome, the sites of protein synthesis (translation) in the cell. The coding sequence of the mRNA determines the amino acid sequence in the protein that is produced. However, many RNAs do not code for protein (about 97% of the transcriptional output is non-protein-coding in eukaryotes).\n\nThese so-called non-coding RNAs (\"ncRNA\") can be encoded by their own genes (RNA genes), but can also derive from mRNA introns. The most prominent examples of non-coding RNAs are transfer RNA (tRNA) and ribosomal RNA (rRNA), both of which are involved in the process of translation. There are also non-coding RNAs involved in gene regulation, RNA processing and other roles. Certain RNAs are able to catalyse chemical reactions such as cutting and ligating other RNA molecules, and the catalysis of peptide bond formation in the ribosome; these are known as ribozymes.\n\nAccording to the length of RNA chain, RNA includes small RNA and long RNA. Usually, small RNAs are shorter than 200 nt in length, and long RNAs are greater than 200 nt long. Long RNAs, also called large RNAs, mainly include long non-coding RNA (lncRNA) and mRNA. Small RNAs mainly include 5.8S ribosomal RNA (rRNA), 5S rRNA, transfer RNA (tRNA), microRNA (miRNA), small interfering RNA (siRNA), small nucleolar RNA (snoRNAs), Piwi-interacting RNA (piRNA), tRNA-derived small RNA (tsRNA) and small rDNA-derived RNA (srRNA).\n\nMessenger RNA (mRNA) carries information about a protein sequence to the ribosomes, the protein synthesis factories in the cell. It is coded so that every three nucleotides (a codon) corresponds to one amino acid. In eukaryotic cells, once precursor mRNA (pre-mRNA) has been transcribed from DNA, it is processed to mature mRNA. This removes its introns—non-coding sections of the pre-mRNA. The mRNA is then exported from the nucleus to the cytoplasm, where it is bound to ribosomes and translated into its corresponding protein form with the help of tRNA. In prokaryotic cells, which do not have nucleus and cytoplasm compartments, mRNA can bind to ribosomes while it is being transcribed from DNA. After a certain amount of time the message degrades into its component nucleotides with the assistance of ribonucleases.\n\nTransfer RNA (tRNA) is a small RNA chain of about 80 nucleotides that transfers a specific amino acid to a growing polypeptide chain at the ribosomal site of protein synthesis during translation. It has sites for amino acid attachment and an anticodon region for codon recognition that binds to a specific sequence on the messenger RNA chain through hydrogen bonding.\n\nRibosomal RNA (rRNA) is the catalytic component of the ribosomes. Eukaryotic ribosomes contain four different rRNA molecules: 18S, 5.8S, 28S and 5S rRNA. Three of the rRNA molecules are synthesized in the nucleolus, and one is synthesized elsewhere. In the cytoplasm, ribosomal RNA and protein combine to form a nucleoprotein called a ribosome. The ribosome binds mRNA and carries out protein synthesis. Several ribosomes may be attached to a single mRNA at any time. Nearly all the RNA found in a typical eukaryotic cell is rRNA.\n\nTransfer-messenger RNA (tmRNA) is found in many bacteria and plastids. It tags proteins encoded by mRNAs that lack stop codons for degradation and prevents the ribosome from stalling.\n\nSeveral types of RNA can downregulate gene expression by being complementary to a part of an mRNA or a gene's DNA. MicroRNAs (miRNA; 21-22 nt) are found in eukaryotes and act through RNA interference (RNAi), where an effector complex of miRNA and enzymes can cleave complementary mRNA, block the mRNA from being translated, or accelerate its degradation.\nWhile small interfering RNAs (siRNA; 20-25 nt) are often produced by breakdown of viral RNA, there are also endogenous sources of siRNAs. siRNAs act through RNA interference in a fashion similar to miRNAs. Some miRNAs and siRNAs can cause genes they target to be methylated, thereby decreasing or increasing transcription of those genes. Animals have Piwi-interacting RNAs (piRNA; 29-30 nt) that are active in germline cells and are thought to be a defense against transposons and play a role in gametogenesis.\n\nMany prokaryotes have CRISPR RNAs, a regulatory system similar to RNA interference. Antisense RNAs are widespread; most downregulate a gene, but a few are activators of transcription. One way antisense RNA can act is by binding to an mRNA, forming double-stranded RNA that is enzymatically degraded. There are many long noncoding RNAs that regulate genes in eukaryotes, one such RNA is Xist, which coats one X chromosome in female mammals and inactivates it.\n\nAn mRNA may contain regulatory elements itself, such as riboswitches, in the 5' untranslated region or 3' untranslated region; these cis-regulatory elements regulate the activity of that mRNA. The untranslated regions can also contain elements that regulate other genes.\n\nMany RNAs are involved in modifying other RNAs.\nIntrons are spliced out of pre-mRNA by spliceosomes, which contain several small nuclear RNAs (snRNA), or the introns can be ribozymes that are spliced by themselves.\nRNA can also be altered by having its nucleotides modified to nucleotides other than A, C, G and U.\nIn eukaryotes, modifications of RNA nucleotides are in general directed by small nucleolar RNAs (snoRNA; 60–300 nt), found in the nucleolus and cajal bodies. snoRNAs associate with enzymes and guide them to a spot on an RNA by basepairing to that RNA. These enzymes then perform the nucleotide modification. rRNAs and tRNAs are extensively modified, but snRNAs and mRNAs can also be the target of base modification. RNA can also be methylated.\n\nLike DNA, RNA can carry genetic information. RNA viruses have genomes composed of RNA that encodes a number of proteins. The viral genome is replicated by some of those proteins, while other proteins protect the genome as the virus particle moves to a new host cell. Viroids are another group of pathogens, but they consist only of RNA, do not encode any protein and are replicated by a host plant cell's polymerase.\n\nReverse transcribing viruses replicate their genomes by reverse transcribing DNA copies from their RNA; these DNA copies are then transcribed to new RNA. Retrotransposons also spread by copying DNA and RNA from one another, and telomerase contains an RNA that is used as template for building the ends of eukaryotic chromosomes.\n\nDouble-stranded RNA (dsRNA) is RNA with two complementary strands, similar to the DNA found in all cells. dsRNA forms the genetic material of some viruses (double-stranded RNA viruses). Double-stranded RNA such as viral RNA or siRNA can trigger RNA interference in eukaryotes, as well as interferon response in vertebrates.\n\nRecently, it was shown that there is a single stranded covalently closed, \"i.e.\" circular form of RNA expressed throughout the animal and plant kingdom (see circRNA). circRNAs are thought to arise via a \"back-splice\" reaction where the spliceosome joins a downstream donor to an upstream acceptor splice site. So far the function of circRNAs is largely unknown, although for few examples a microRNA sponging activity has been demonstrated.\n\nResearch on RNA has led to many important biological discoveries and numerous Nobel Prizes. Nucleic acids were discovered in 1868 by Friedrich Miescher, who called the material 'nuclein' since it was found in the nucleus. It was later discovered that prokaryotic cells, which do not have a nucleus, also contain nucleic acids. The role of RNA in protein synthesis was suspected already in 1939. Severo Ochoa won the 1959 Nobel Prize in Medicine (shared with Arthur Kornberg) after he discovered an enzyme that can synthesize RNA in the laboratory. However, the enzyme discovered by Ochoa (polynucleotide phosphorylase) was later shown to be responsible for RNA degradation, not RNA synthesis. In 1956 Alex Rich and David Davies hybridized two separate strands of RNA to form the first crystal of RNA whose structure could be determined by X-ray crystallography.\n\nThe sequence of the 77 nucleotides of a yeast tRNA was found by Robert W. Holley in 1965, winning Holley the 1968 Nobel Prize in Medicine (shared with Har Gobind Khorana and Marshall Nirenberg).\nIn 1967, Carl Woese hypothesized that RNA might be catalytic and suggested that the earliest forms of life (self-replicating molecules) could have relied on RNA both to carry genetic information and to catalyze biochemical reactions—an RNA world.\n\nDuring the early 1970s, retroviruses and reverse transcriptase were discovered, showing for the first time that enzymes could copy RNA into DNA (the opposite of the usual route for transmission of genetic information). For this work, David Baltimore, Renato Dulbecco and Howard Temin were awarded a Nobel Prize in 1975.\nIn 1976, Walter Fiers and his team determined the first complete nucleotide sequence of an RNA virus genome, that of bacteriophage MS2.\n\nIn 1977, introns and RNA splicing were discovered in both mammalian viruses and in cellular genes, resulting in a 1993 Nobel to Philip Sharp and Richard Roberts.\nCatalytic RNA molecules (ribozymes) were discovered in the early 1980s, leading to a 1989 Nobel award to Thomas Cech and Sidney Altman. In 1990, it was found in \"Petunia\" that introduced genes can silence similar genes of the plant's own, now known to be a result of RNA interference.\n\nAt about the same time, 22 nt long RNAs, now called microRNAs, were found to have a role in the development of \"C. elegans\".\nStudies on RNA interference gleaned a Nobel Prize for Andrew Fire and Craig Mello in 2006, and another Nobel was awarded for studies on the transcription of RNA to Roger Kornberg in the same year. The discovery of gene regulatory RNAs has led to attempts to develop drugs made of RNA, such as siRNA, to silence genes.\n\nIn March 2015, complex DNA and RNA nucleotides, including uracil, cytosine and thymine, were reportedly formed in the laboratory under outer space conditions, using starter chemicals, such as pyrimidine, an organic compound commonly found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), is one of the most carbon-rich compounds found in the Universe and may have been formed in red giants or in interstellar dust and gas clouds.\n\n\n", "id": "25758", "title": "RNA"}
{"url": "https://en.wikipedia.org/wiki?curid=38180687", "text": "Base calling\n\nBase calling is the process of assigning bases (nucleobases) to chromatogram peaks. One computer program for accomplishing this job is Phred base-calling, which is a widely used basecalling software program by both academic and commercial DNA sequencing laboratories because of its high base calling accuracy.\n", "id": "38180687", "title": "Base calling"}
{"url": "https://en.wikipedia.org/wiki?curid=37959176", "text": "Nb.BbvCI\n\nNb.BbvCI is a nicking endonuclease used to cut one strand of double-stranded DNA. It has been successfully used to incorporate fluorochrome-labeled nucleotides into specific spots of a DNA sequence via nick translation.\n", "id": "37959176", "title": "Nb.BbvCI"}
{"url": "https://en.wikipedia.org/wiki?curid=1726883", "text": "Intergenic region\n\nAn Intergenic region (IGR) is a stretch of DNA sequences located between genes. Intergenic regions are a subset of noncoding DNA. Occasionally some intergenic DNA acts to control genes nearby, but most of it has no currently known function. It is one of the DNA sequences sometimes referred to as junk DNA, though it is only one phenomenon labeled such and in scientific studies today, the term is less used. Recently DNA fragments in intergenic regions were known as \"dark matter\" or \"dark matter transcripts\".\n\n\"Inter\"genic regions are different from \"intra\"genic regions (or introns), which are short, non-coding regions that are found \"within\" genes, especially within the genes of eukaryotic organisms.\n\nAccording to the ENCODE project's study of the human genome, due to \"both the expansion of genic regions by the discovery of new isoforms and the identification of novel intergenic transcripts, there has been a marked increase in the number of intergenic regions (from 32,481 to 60,250) due to their fragmentation and a decrease in their lengths (from 14,170 bp to 3,949 bp median length)\"\n\nScientists have now artificially synthesized proteins from intergenic regions.\n\nHistorically intergenic regions have sometimes been called junk DNA suggesting that they have no function. However, it has been known for a long time that these regions do contain functionally important elements such as promoters and enhancers. Also intergenic regions may contain as yet unidentified genes such as noncoding RNAs. Though little is known about them, they are thought to have regulatory functions. In recent years the ENCODE project has been studying intergenic regions in humans in more detail.\n\nIn humans, intergenic regions comprise about 75% of the genome, whereas this number is much less in bacteria (15%) and yeast (30%) \n\nIn \"Plasmodium falciparum\", many intergenic regions have an AT content of 90% \n\n\n", "id": "1726883", "title": "Intergenic region"}
{"url": "https://en.wikipedia.org/wiki?curid=38316608", "text": "Scissile bond\n\nIn molecular biology, a scissile bond is a covalent chemical bond that can be broken by an enzyme. Examples would be the cleaved bond in the self-cleaving hammerhead ribozyme or the peptide bond of a substrate cleaved by a peptidase.\n", "id": "38316608", "title": "Scissile bond"}
{"url": "https://en.wikipedia.org/wiki?curid=38324409", "text": "DNA digital data storage\n\nDNA digital data storage refers to any process to store digital data in the base sequence of DNA. This technology uses artificial DNA made using commercially available oligonucleotide synthesis machines for storage and DNA sequencing machines for retrieval. This type of storage system is more compact than current magnetic tape or hard drive storage systems due to the data density of the DNA. Currently it was reported that in 1 gram of DNA 215 petabytes (215 million gigabytes) could be stored. It also has the capability for longevity, as long as the DNA is held in cold, dry and dark conditions, as is shown by the study of woolly mammoth DNA from up to 60,000 years ago, and for resistance to obsolescence, as DNA is a universal and fundamental data storage mechanism in biology. These features have led to researchers involved in their development to call this method of data storage \"apocalypse-proof\" because \"after a hypothetical global disaster, future generations might eventually find the stores and be able to read them.\" It is, however, a slow process, as the DNA needs to be sequenced in order to retrieve the data, and so the method is intended for uses with a low access rate such as long-term archival of large amounts of scientific data.\n\nEarly writing on the idea and general considerations about the possibility of recording, storage and retrieval of information on DNA molecules was made by Mikhail Neiman and published in 1964–65 in the \"Radiotekhnika\" journal, USSR. Neiman wrote that he had in turn taken the idea from an interview with the pioneering American cyberneticist Norbert Wiener published in 1964.\n\nAmong early examples of DNA data storage, in 2007 a device was created at the University of Arizona, using addressing molecules to encode mismatch sites within a DNA strand. These mismatches were then able to be read out by performing a restriction digest, thereby recovering the data. This system has a number of advantages over other methods. Firstly, unlike other methods in which bespoke molecules are synthesised for each new DNA encoding, a common set of molecules could be used to encode any arbitrary data. DNA synthesis is currently expensive, and laborious, so this means that this investment can be used to encode many different sets of data, using the same set of DNA molecules. The encoded DNA created here is also \"bio-compatible\", meaning that, in principle it can be readily inserted into, and propagated within, an organism.\n\nOn August 16, 2012, the journal \"Science\" published research by George Church and colleagues at Harvard University, in which DNA was encoded with digital information that included an HTML draft of a 53,400 word book written by the lead researcher, eleven JPG images and one JavaScript program. Multiple copies for redundancy were added and 5.5 petabits can be stored in each cubic millimeter of DNA. The researchers used a simple code where bits were mapped one-to-one with bases, which had the shortcoming that it led to long runs of the same base, the sequencing of which is error-prone. This research result showed that besides its other functions, DNA can also be another type of storage medium such as hard drives and magnetic tapes.\n\nAn improved system was reported in the journal \"Nature\" in January 2013, in an article led by researchers from the European Bioinformatics Institute (EBI) and submitted at around the same time as the paper of Church and colleagues. Over five million bits of data, appearing as a speck of dust to researchers, and consisting of text files and audio files, were successfully stored and then perfectly retrieved and reproduced. Encoded information consisted of all 154 of Shakespeare's sonnets, a twenty-six-second audio clip of the \"I Have a Dream\" speech by Martin Luther King, the well known paper on the structure of DNA by James Watson and Francis Crick, a photograph of EBI headquarters in Hinxton, United Kingdom, and a file describing the methods behind converting the data. All the DNA files reproduced the information between 99.99% and 100% accuracy. The main innovations in this research were the use of an error-correcting encoding scheme to ensure the extremely low data-loss rate, as well as the idea of encoding the data in a series of overlapping short oligonucleotides identifiable through a sequence-based indexing scheme. Also, the sequences of the individual strands of DNA overlapped in such a way that each region of data was repeated four times to avoid errors. Two of these four strands were constructed backwards, also with the goal of eliminating errors. The costs per megabyte were estimated at $12,400 to encode data and $220 for retrieval. However, it was noted that the exponential decrease in DNA synthesis and sequencing costs, if it continues into the future, should make the technology cost-effective for long-term data storage within about ten years.\n\nThe long-term stability of data encoded in DNA was reported in February 2015, in an article by researches from ETH Zurich. By adding redundancy via Reed–Solomon error correction coding and by encapsulating the DNA within silica glass spheres via Sol-gel chemistry, the researchers predict error-free information recovery after up to 1 million years at -18 °C and 2000 years if stored at 10 °C. By adding the possibility of being able to handle errors, the research team could reduce the cost of DNA synthesis down to ~$500/MB by choosing a more error-prone DNA synthesis method. In a news article in the New Scientist the team stated that if they are able to further decrease the cost they would store an archive version of Wikipedia in DNA.\n\nAlso, a group of researchers, led by Boise State University is working toward a better way to store digital information using nucleic acid memory (NAM). They suggest that the global flash memory market is predicted to reach $30.2 billion this year, potentially growing to $80.3 billion by 2025. They estimated that by 2040, the demand for global memory will exceed the projected supply of silicon (the raw material used to store flash memory), and that nucleic acid memory has a retention time far exceeding electronic memory. They have discussed the longevity of the DNA materials through first principle theoretical calculations that is published as commentary research article. According to their claims \"With information retention times that range from thousands to millions of years, volumetric density 10 times greater than flash memory and energy of operation 10 times less, we believe that DNA used as a memory-storage material in nucleic acid memory (NAM) products promises a viable and compelling alternative to electronic memory.\" and \"Given exponentially increasing demands for safeguarded information worldwide, and the long retention times for DNA (ranging from thousands to millions of years), NAM can store the world's information for future generations using far less space and energy. NAM could thus be used as a time capsule for massive, infrequently accessed records in scientific, financial, governmental, historical, genealogical, personal and genetic domains.\".\n\nThe above methods of DNA storage had the disadvantage that the whole strand of synthetic DNA has to be sequenced in order to retrieve only one of several data sets that were previously encoded. In April 2016 researchers at the University of Washington published an encoding, storage, retrieval and decoding method that enables random access of any one of the data sets \n\nIn March 2017, Dr. Yaniv Erlich and Dina Zielinski of Columbia University and the New York Genome Center published a method known as DNA Fountain which allows perfect retrieval of information from a density of 215 petabytes per gram of DNA. The technique approaches the Shannon capacity of DNA storage, achieving 85% of the theoretical limit. Using this method, they were also able to perfectly retrieve an operating system called KolibriOS, the French movie Arrival of a Train at La Ciotat, a $50 Amazon gift card, a computer virus, a Pioneer plaque and a study by Claude Shannon, all with a total of 2.14 megabytes. A process which allows 2.18 × 10 retrievals using the original DNA sample was also tested, being able to perfectly decode the data. The method is however not ready for large-scale use, as it costs $7000 to synthesize 2 megabytes of data and another $2000 to read it.\n\nIn September 2017, researchers at the University of Washington, Microsoft, and Twist Bioscience have saved audio recordings of Deep Purple's \"Smoke On The Water\" and Miles Davis' “Tutu” on DNA. The original recordings of the songs were part of the Montreux Jazz Festival archive, which is also where the new DNA-encoded versions will live.\n\n\n", "id": "38324409", "title": "DNA digital data storage"}
{"url": "https://en.wikipedia.org/wiki?curid=32170937", "text": "BSD domain\n\nIn molecular biology, the BSD domain is an approximately 60-amino-acid-long protein domain named after the BTF2-like transcription factors, synapse-associated proteins and DOS2-like proteins in which it is found. It is also found in several hypothetical proteins. The BSD domain occurs in one or two copies in a variety of species ranging from primal protozoan to human. It can be found associated with other domains such as the BTB domain or the U-box in multidomain proteins. The function of the BSD domain is as yet unknown.\n\nSecondary structure prediction indicates the presence of three predicted alpha helices, which probably form a three-helical bundle in small |domains. The third predicted helix contains neighbouring phenylalanine and tryptophan residues – less common amino acids that are invariant in all the BSD domains identified and that are the most striking sequence features of the domain.\n\nSome proteins known to contain one or two BSD domains are listed below:\n\nBSTA Promotes mTORC2-Mediated Phosphorylation of Akt1 to Suppress Expression of FoxC2 and Stimulate Adipocyte Differentiation.\nYao Y, Suraokar M, Darnay BG, Hollier BG, Shaiken TE, Asano T, Chen CH, Chang BH, Lu Y, Mills GB, Sarbassov D, Mani SA, Abbruzzese JL, Reddy SA.\nSci Signal. 2013 Jan 8;6(257):ra2. doi: 10.1126/scisignal.2003295.\n", "id": "32170937", "title": "BSD domain"}
{"url": "https://en.wikipedia.org/wiki?curid=32170974", "text": "Cfr10I/Bse634I\n\nIn molecular biology, the Cfr10I/Bse634I family of restriction endonucleases includes the type II restriction endonucleases Cfr10I and Bse634I. They exhibit a conserved tetrameric architecture that is of functional importance, wherein two dimers are arranged, back-to-back, with their putative DNA-binding clefts facing opposite directions. These clefts are formed between two monomers that interact, mainly via hydrophobic interactions supported by a few hydrogen bonds, to form a U-shaped dimer. Each monomer is folded to form a compact alpha-beta structure, whose core is made up of a five-stranded mixed beta-sheet. The monomer may be split into separate N-terminal and C-terminal subdomains at a hinge located in helix alpha3. Both Cfr10I and Bse634I recognise the double-stranded sequence RCCGGY and cleave after the purine R.\n", "id": "32170974", "title": "Cfr10I/Bse634I"}
{"url": "https://en.wikipedia.org/wiki?curid=38495892", "text": "Chlororespiration\n\nChlororespiration is a process in plant chloroplasts which is thought to involve a respiratory electron transport chain within the thylakoid membrane. It is thought to involve a proton pumping respiratory dehydrogenase (the Ndh complex), and Immutans, an oxidase similar to the mitochondrial alternative oxidase, which reduces the plastoquinone pool. This process may function to balance ATP: reductant levels in the chloroplast at night, when cyclic electron transfer around Photosystem I cannot occur.\n\nChlororespiration: an adaptation to nitrogen deficiency in Chlamydomonas reinhardtii. http://www.wikigenes.org/e/ref/e/11607187.html\n", "id": "38495892", "title": "Chlororespiration"}
{"url": "https://en.wikipedia.org/wiki?curid=38380954", "text": "Biotransducer\n\nElectrochemical biosensors contain a biorecognition element that selectively reacts with the target analyte and produces an electrical signal that is proportional to the analyte concentration. In general, there are several approaches that can be used to detect electrochemical changes during a biorecognition event and these can be classified as follows: amperometric, potentiometric, impedance, and conductometric.\n\nAmperometric transducers detect change in current as a result of electrochemical oxidation or reduction. Typically, the bioreceptor molecule is immobilized on the working electrode (commonly gold, carbon, or platinum). The potential between the working electrode and the reference electrode (usually Ag/AgCl) is fixed at a value and then current is measured with respect to time. The applied potential is the driving force for the electron transfer reaction. The current produced is a direct measure of the rate of electron transfer. The current reflects the reaction occurring between the bioreceptor molecule and analyte and is limited by the mass transport rate of the analyte to the electrode.\n\nPotentiometric sensors measure a potential or charge accumulation of an electrochemical cell. The transducer typically comprises an ion selective electrode (ISE) and a reference electrode. The ISE features a membrane that selectively interacts with the charged ion of interest, causing the accumulation of a charge potential compared to the reference electrode. The reference electrode provides a constant half-cell potential that is unaffected by analyte concentration. A high impedance voltmeter is used to measure the electromotive force or potential between the two electrodes when zero or no significant current flows between them. The potentiometric response is governed by the Nernst equation in that the potential is proportional to the logarithm of the concentration of the analyte.\n\nElectrochemical impedance spectroscopy (EIS) involves measuring resistive and capacitive changes caused by a biorecognition event. Typically, a small amplitude sinsusoidal electrical stimulus is applied, causing current to flow through the biosensor. The frequency is varied over a range to obtain the impedance spectrum. The resistive and capacitive components of impedance are determined from in phase and out of phase current responses. Typically, a conventional three-electrode system is made specific to the analyte by immobilizing a biorecognition element to the surface. A voltage is applied and the current is measured. The interfacial impedance between the electrode and solution changes as a result of the analyte binding. An impedance analyzer can be used to control and apply the stimulus as well as measure the impedance changes.\n\nConductometric sensing involves measuring the change in conductive properties of the sample solution or a medium. The reaction between the biomolecule and analyte changes the ionic species concentration, leading to a change in the solution electrical conductivity or current flow. Two metal electrodes are separated at a certain distance and an AC potential is applied across the electrodes, causing a current flow between the electrodes. During a biorecognition event the ionic composition changes, using an ohmmeter the change in conductance can be measured.\n\nOptical biotransducers, used in optical biosensors for signal transduction, use photons in order to collect information about analyte. These are highly sensitive, highly specific, small in size and cost effective.\n\nThe detection mechanism of optical biotransducer depends upon the enzyme system that converts analyte into products which are either oxidized or reduced at the working electrode.\n\nEvanescent field detection principle is most commonly used in an optical biosensor system as the transduction principle . This principle is one of the most sensitive detection methods. It enables the detection of fluorophores exclusively in the close proximity of the optical fiber.\nElectronic biosensing offers significant advantages over optical, biochemical and biophysical methods, in terms of high sensitivity and new sensing mechanisms, high spatial resolution for localized detection, facile integration with standard wafer-scale semiconductor processing and label-free, real-time detection in a nondestructive manner [6].\n\nDevices based on field-effect transistors (FETs) have attracted great attention because they can directly translate the interactions between target biological molecules and the FET surface into readable electrical signals. In a FET, current flows along the channel which is connected to the source and the drain. The channel conductance between the source and the drain is switched on and off by gate electrode that is capacitively coupled through a thin dielectric layer [6].\n\nIn FET-based biosensors, the channel is in direct contact with the environment, and this gives better control over the surface charge. This improves the sensitivity of surface FET-based biosensors as biological events occurring at the channel surface could result in the surface potential variation of the semiconductor channel and then modulate the channel conductance. In addition to ease of on-chip integration of device arrays and the cost-effective device fabrication, the surface ultrasensitivity of FET-based biosensors makes it an attractive alternative to existing biosensor technologies[6].\n\nGravimetric biosensors use the basic principle of a response to a change in mass. Most gravimetric biosensors use thin piezoelectric quartz crystals, either as resonating crystals (QCM), or as bulk/surface acoustic wave (SAW) devices. In the majority of these the mass response is inversely proportional to the crystal thickness. Thin polymer films are also used in which biomolecules can be added to the surface with known surface mass. Acoustic waves can be projected to the thin film to produce an oscillatory device, which then follows an equation that is nearly identical to the Sauerbrey equation used in the QCM method. Biomolecules, such as proteins or antibodies can bind and its change in mass gives a measureable signal proportional to the presence of the target analyte in the sample.\n\nPyroelectric biosensors generate an electric current as a result of a temperature change. This differential induces a polarization in the substance, producing a dipole moment in the direction of the temperature gradient. The result is a net voltage across the material. This net voltage can be calculated by the following equation.\n\nformula_1\n\nformula_2\n\nwhere V = Voltage,\nω = angular frequency of the modulated incident,\nP = pyroelectric coefficient,\nL = film thickness,\nε = film dielectric constant,\nA = area of film,\nr = resistance of the film,\nC = capacitance of the film,\nτE = electrical time constant of the detector output.\n", "id": "38380954", "title": "Biotransducer"}
{"url": "https://en.wikipedia.org/wiki?curid=28616", "text": "Shotgun sequencing\n\nIn genetics, shotgun sequencing is a method used for sequencing long DNA strands. It is named by analogy with the rapidly expanding, quasi-random firing pattern of a shotgun.\n\nThe chain termination method of DNA sequencing (or \"Sanger sequencing\" for its developer Frederick Sanger) can only be used for fairly short strands of 100 to 1000 base pairs. Longer sequences are subdivided into smaller fragments that can be sequenced separately, and subsequently they are re-assembled to give the overall sequence. Two principal methods are used for this: primer walking (or \"chromosome walking\") which progresses through the entire strand piece by piece, and shotgun sequencing, which is a faster but more complex process that uses random fragments.\n\nIn shotgun sequencing, DNA is broken up randomly into numerous small segments, which are sequenced using the chain termination method to obtain \"reads\". Multiple overlapping reads for the target DNA are obtained by performing several rounds of this fragmentation and sequencing. Computer programs then use the overlapping ends of different reads to assemble them into a continuous sequence.\n\nShotgun sequencing was one of the precursor technologies that was responsible for enabling full genome sequencing.\n\nFor example, consider the following two rounds of shotgun reads:\nIn this extremely simplified example, none of the reads cover the full length of the original sequence, but the four reads can be assembled into the original sequence using the overlap of their ends to align and order them. In reality, this process uses enormous amounts of information that are rife with ambiguities and sequencing errors. Assembly of complex genomes is additionally complicated by the great abundance of repetitive sequences, meaning similar short reads could come from completely different parts of the sequence.\n\nMany overlapping reads for each segment of the original DNA are necessary to overcome these difficulties and accurately assemble the sequence. For example, to complete the Human Genome Project, most of the human genome was sequenced at 12X or greater \"coverage\"; that is, each base in the final sequence was present on average in 12 different reads. Even so, current methods have failed to isolate or assemble reliable sequence for approximately 1% of the (euchromatic) human genome, as of 2004.\n\nThe first genome sequenced by shotgun sequencing was that of cauliflower mosaic virus, published in 1981. However, whole genome shotgun sequencing for small (4000- to 7000-base-pair) genomes had been suggested already in 1979.\n\nBroader application benefited from pairwise end sequencing, known colloquially as \"double-barrel shotgun sequencing\". As sequencing projects began to take on longer and more complicated DNA sequences, multiple groups began to realize that useful information could be obtained by sequencing both ends of a fragment of DNA. Although sequencing both ends of the same fragment and keeping track of the paired data was more cumbersome than sequencing a single end of two distinct fragments, the knowledge that the two sequences were oriented in opposite directions and were about the length of a fragment apart from each other was valuable in reconstructing the sequence of the original target fragment.\n\nHistory. The first published description of the use of paired ends was in 1990 as part of the sequencing of the human HGPRT locus, although the use of paired ends was limited to closing gaps after the application of a traditional shotgun sequencing approach. The first theoretical description of a pure pairwise end sequencing strategy, assuming fragments of constant length, was in 1991. At the time, there was community consensus that the optimal fragment length for pairwise end sequencing would be three times the sequence read length. In 1995 Roach et al. introduced the innovation of using fragments of varying sizes, and demonstrated that a pure pairwise end-sequencing strategy would be possible on large targets. The strategy was subsequently adopted by The Institute for Genomic Research (TIGR) to sequence the genome of the bacterium \"Haemophilus influenzae\" in 1995, and then by Celera Genomics to sequence the \"Drosophila melanogaster\" (fruit fly) genome in 2000, and subsequently the human genome.\n\nTo apply the strategy, a high-molecular-weight DNA strand is sheared into random fragments, size-selected (usually 2, 10, 50, and 150 kb), and cloned into an appropriate vector. The clones are then sequenced from both ends using the chain termination method yielding two short sequences. Each sequence is called an \"end-read\" or \"read\" and two reads from the same clone are referred to as \"mate pairs\". Since the chain termination method usually can only produce reads between 500 and 1000 bases long, in all but the smallest clones, mate pairs will rarely overlap.\n\nThe original sequence is reconstructed from the reads using sequence assembly software. First, overlapping reads are collected into longer composite sequences known as \"contigs\". Contigs can be linked together into \"scaffolds\" by following connections between mate pairs. The distance between contigs can be inferred from the mate pair positions if the average fragment length of the library is known and has a narrow window of deviation. Depending on the size of the gap between contigs, different techniques can be used to find the sequence in the gaps. If the gap is small (5-20kb) then the use of PCR to amplify the region is required, followed by sequencing. If the gap is large (>20kb) then the large fragment is cloned in special vectors such as BAC (Bacterial artificial chromosomes) followed by sequencing of the vector.\n\nProponents of this approach argue that it is possible to sequence the whole genome at once using large arrays of sequencers, which makes the whole process much more efficient than more traditional approaches. Detractors argue that although the technique quickly sequences large regions of DNA, its ability to correctly link these regions is suspect, particularly for genomes with repeating regions. As sequence assembly programs become more sophisticated and computing power becomes cheaper, it may be possible to overcome this limitation.\n\nCoverage (read depth or depth) is the average number of reads representing a given nucleotide in the reconstructed sequence. It can be calculated from the length of the original genome (\"G\"), the number of reads(\"N\"), and the average read length(\"L\") as formula_1. For example, a hypothetical genome with 2,000 base pairs reconstructed from 8 reads with an average length of 500 nucleotides will have 2x redundancy. This parameter also enables one to estimate other quantities, such as the percentage of the genome covered by reads (sometimes also called coverage). A high coverage in shotgun sequencing is desired because it can overcome errors in base calling and assembly. The subject of DNA sequencing theory addresses the relationships of such quantities.\n\nSometimes a distinction is made between \"sequence coverage\" and \"physical coverage\". Sequence coverage is the average number of times a base is read (as described above). Physical coverage is the average number of times a base is read or spanned by mate paired reads.\n\nAlthough shotgun sequencing can in theory be applied to a genome of any size, its direct application to the sequencing of large genomes (for instance, the human genome) was limited until the late 1990s, when technological advances made practical the handling of the vast quantities of complex data involved in the process. Historically, full-genome shotgun sequencing was believed to be limited by both the sheer size of large genomes and by the complexity added by the high percentage of repetitive DNA (greater than 50% for the human genome) present in large genomes. It was not widely accepted that a full-genome shotgun sequence of a large genome would provide reliable data. For these reasons, other strategies that lowered the computational load of sequence assembly had to be utilized before shotgun sequencing was performed.\nIn hierarchical sequencing, also known as top-down sequencing, a low-resolution physical map of the genome is made prior to actual sequencing. From this map, a minimal number of fragments that cover the entire chromosome are selected for sequencing. In this way, the minimum amount of high-throughput sequencing and assembly is required.\n\nThe amplified genome is first sheared into larger pieces (50-200kb) and cloned into a bacterial host using BACs or PACs. Because multiple genome copies have been sheared at random, the fragments contained in these clones have different ends, and with enough coverage (see section above) finding a scaffold of BAC contigs that covers the entire genome is theoretically possible. This scaffold is called a tiling path. Once a tiling path has been found, the BACs that form this path are sheared at random into smaller fragments and can be sequenced using the shotgun method on a smaller scale.\n\nAlthough the full sequences of the BAC contigs is not known, their orientations relative to one another are known. There are several methods for deducing this order and selecting the BACs that make up a tiling path. The general strategy involves identifying the positions of the clones relative to one another and then selecting the least number of clones required to form a contiguous scaffold that covers the entire area of interest. The order of the clones is deduced by determining the way in which they overlap. Overlapping clones can be identified in several ways. A small radioactively or chemically labeled probe containing a sequence-tagged site (STS) can be hybridized onto a microarray upon which the clones are printed. In this way, all the clones that contain a particular sequence in the genome are identified. The end of one of these clones can then be sequenced to yield a new probe and the process repeated in a method called chromosome walking.\n\nAlternatively, the BAC library can be restriction-digested. Two clones that have several fragment sizes in common are inferred to overlap because they contain multiple similarly spaced restriction sites in common. This method of genomic mapping is called restriction fingerprinting because it identifies a set of restriction sites contained in each clone. Once the overlap between the clones has been found and their order relative to the genome known, a scaffold of a minimal subset of these contigs that covers the entire genome is shotgun-sequenced.\n\nBecause it involves first creating a low-resolution map of the genome, hierarchical shotgun sequencing is slower than whole-genome shotgun sequencing, but relies less heavily on computer algorithms than whole-genome shotgun sequencing. The process of extensive BAC library creation and tiling path selection, however, make hierarchical shotgun sequencing slow and labor-intensive. Now that the technology is available and the reliability of the data demonstrated, and the speed and cost efficiency of whole-genome shotgun sequencing has made it the primary method for genome sequencing.\n\nThe classical shotgun sequencing was based on the Sanger sequencing method: this was the most advanced technique for sequencing genomes from about 1995–2005. The shotgun strategy is still applied today, however using other sequencing technologies, called next-generation sequencing. These technologies produce shorter reads (anywhere from 25–500bp) but many hundreds of thousands or millions of reads in a relatively short time (on the order of a day).\nThis results in high coverage, but the assembly process is much more computationally intensive. These technologies are vastly superior to Sanger sequencing due to the high volume of data and the relatively short time it takes to sequence a whole genome.\n\n\n", "id": "28616", "title": "Shotgun sequencing"}
{"url": "https://en.wikipedia.org/wiki?curid=38679864", "text": "Jumping library\n\nJumping libraries or junction-fragment libraries are collections of genomic DNA fragments generated by chromosome jumping. These libraries allow us to analyze large areas of the genome and overcome distance limitations in common cloning techniques.\nA jumping library clone is composed of two stretches of DNA that are usually located many kilobases away from each other. The stretch of DNA located between these two “ends” is deleted by a series of biochemical manipulations carried out at the start of this cloning technique.\n\nChromosome jumping (or chromosome hopping) was first described in 1984 by Collins and Weissman.\nAt the time, cloning techniques allowed for generation of clones of limited size (up to 240kb), and cytogenetic techniques allowed for mapping such clones to a small region of a particular chromosome to a resolution of around 5-10Mb. Therefore, a major gap remained in resolution between available technologies, and no methods were available for mapping larger areas of the genome.\n\nThis technique is an extension of “chromosome walking” that allows for larger “steps” along the chromosome. \nIf we desire to take steps of length N kb, we first require very high molecular weight DNA. Once isolated, we partially digest it with a frequent-cutting restriction enzyme (such as MboI or BamHI). Next, obtained fragments are selected for size which should be around N kb in length. DNA must then be ligated at low concentration to favour ligation into circles rather than formation of multimers. A DNA marker (such as the amber suppressor tRNA gene supF) can be included at this time point within the covalently linked circle to allow for selection of junction fragments. Circles are subsequently fully digested with a second restriction enzyme (such as EcoRI) to generate a large number of fragments. Such fragments are ligated into vectors (such as a λ vector) which should be selected for using the DNA marker introduced earlier. The remaining fragments thus represent our library of junction fragments, or “jumping library”.\nThe next step is to screen this library with a probe that represents a “starting point” of the desired “chromosome hop”, i.e. determining the location of the genome that is being interrogated. Clones obtained from this final selection step will consist of DNA that is homologous to our probe, separated by our DNA marker from another DNA sequence that was originally located N kb away (thus being called “jumping”).\nBy generating several libraries of different N values, we should eventually be able to map the entire genome and move from one location to another, while controlling direction, by any value of N desired.\n\nThe original technique of chromosome jumping was developed in the laboratories of Collins and Weissman at Yale University in New Haven, U.S. and the laboratories of Poustka and Lehrach at the European Molecular Biology Laboratory in Heidelberg, Germany.\n\nCollins and Weissman’s method described above encountered some early limitations. The main concern was with avoiding non-circularized fragments. Two solutions were suggested: either screening junction fragments with a given probe or adding a second size-selection step after the ligation to separate single circular clones (monomers) from clones ligated to each other (multimers). The authors also suggested that other markers such as the λ cos site or antibiotic resistance genes should be considered (instead of the amber suppressor tRNA gene) to facilitate selection of junction clones.\n\nPoustka and Lehrach suggested that full digestion with rare-cutting restrictions enzymes (such as NotI) should be used for the first step of the library construction instead of partial digestion with a frequently cutting restriction enzyme. This would significantly reduce the number of clones from millions to thousands. However, this could create problems with circularizing the DNA fragments since these fragments would be very long, and would also lose the flexibility in choice of end points that one gets in partial digests. One suggestion for overcoming these problems would be to combine the two methods, i.e. to construct a jumping library from DNA fragments digested partially with a commonly cutting restriction enzyme and completely with a rare cutting restriction enzyme and circularizing them into plasmids cleaved with both enzymes. Several of these “combination” libraries were completed in 1986.\n\nIn 1991, Zabarovsky et al. proposed a new approach for construction of jumping libraries. This approach included the use of two separate λ vectors for library construction, and a partial filling-in reaction that removes the need for a selectable marker. This filling-in reaction worked by destroying the specific cohesive ends (resulting from restriction digests) of the DNA fragments that were nonligated and noncircularized, thus preventing them from cloning into the vectors, in a more energy-efficient and accurate manner. Furthermore, this improved technique required less DNA to start with, and also produced a library that could be transferred into a plasmid form, making it easier to store and replicate. Using this new approach, they successfully constructed a human NotI jumping library from a lymphoblastoid cell line and a human chromosome 3-specific NotI jumping library from a human chromosome 3 and mouse hybrid cell line.\n\nSecond-generation or \"Next-Gen\" (NGS) techniques have evolved radically: the sequencing capacity has increased more than ten thousandfold and the cost has dropped by over one million fold since 2007(National Human Genome Research Institute). NGS has revolutionized the Genetic field in many ways.\n\nA library is often prepared by random fragmentation of DNA and ligation of common adaptor sequences. \nHowever, the generated short reads challenge the identification of structural variants, such as indels, translocations, and duplication. Large regions of simple repeats can further complicate the alignment. Alternatively, jumping library can be used with NGS for the mapping of structural variation and scaffolding of de novo assemblies. \nJumping Libraries can be categorized according to the length of the incorporated DNA fragments.\n\n3 kb genomic DNA fragments are ligated with biotinylate ends and circularized. The circular segments are then sheared into small fragments and the biotinylated fragments are selected by affinity assay for paired-end sequencing.\n\nThere are two issues related to Short-jump libraries. First, a read can pass through the biotinylated circularization junction and reduce the effective read length. Second, reads from non-jumped fragments (i.e. fragments without the circularization junction) are sequenced and reduce genomic coverage. It has been reported that non-jumped fragments range from 4% to 13%, depending on the size of selection. The first problem might be solved by shearing circles into a larger size and select for those larger fragments. The second problem can be addressed by using Custom Barcoded Jumping Library.\n\nThis specific jumping library uses adaptors containing markers for fragment selection in combination with barcodes for multiplexing. The protocol was developed by Talkowski et al. and based on mate-pair library preparation for SOLiD sequencing. The selected DNA fragment size is 3.5 – 4.5 kb. Two adaptors were involved: one containing an EcoP15I recognition site and an AC overhang; the other containing a GT overhang, a biotinylated thymine, and an oligo barcode. The circularized DNA was digested and the fragments with biotynylated adaptors were selected for(See Figure 3). The EcoP15I recognition site and barcode help to distinguish junction fragments from nonjump fragments. These targeted fragments should contain 25 to 27bp of genomic DNA, the EcoP15I recognition site, the overhang, and the barcode.\n\nThis library construction process is similar to that of Short-jump library except that the condition is optimized for longer fragments(5 kb).\n\nThis library construction process is also similar to that of Short-jump library except that transfection using the E. coli vector is required for amplification of large (40 kb) DNA fragments. In addition, the Fosmids can be modified to facilitate the conversion into jumping library compatible with certain Next Generation Sequencers.\n\nThe segments resulting from circularization during constructing jumping library are cleaved, and DNA fragments with markers will be enriched and subjected to paired-end sequencing. \nThese DNA fragments are sequenced from both ends and generate pairs of reads. The genomic distance between the reads in each pair is approximately known and used for the assembly process.\nFor example, a DNA clone generated by random fragmentation is about 200 bp, and a read from each end is around 180bp, overlapping each other. \nThis should be distinguished from mate-pair sequencing, which is basically a combination of Next Generation Sequencing with jumping libraries.\n\nDifferent assembly tools have been developed to handle jumping library data. One example is DELLY. DELLY was developed to discover genomic structural variants and “integrates short insert paired-ends, long-range mate-pairs and split-read alignments” to detect rearrangements at sequence level.\n\nAn example of joint development of new experimental design and algorithm development is demonstrated by the ALLPATHS-LG assembler.\n\nWhen used for detection of genetic and genomic changes, jumping clones require validation by Sanger sequencing.\n\nIn the early days, chromosome walking from genetically linked DNA markers was used to identify and clone disease genes. However, the large molecular distance between known markers and the gene of interest was complicating the cloning process. In 1987, a human chromosome jumping library was constructed to clone the cystic fibrosis gene. Cystic Fibrosis is an autosomal recessive disease affecting 1 in 2000 Caucasians. This was the first disease in which the usefulness of the jumping libraries was demonstrated. Met oncogene was a marker tightly linked to the cystic fibrosis gene on human chromosome 7, and the library was screened for a jumping clone starting at this marker. The cystic fibrosis gene was determined to localize 240kb downstream of the met gene. Chromosome jumping helped reduce the mapping “steps” and bypass the highly repetitive regions in the mammalian genome. Chromosome jumping also allowed the production of probes required for faster diagnosis of this and other diseases.\n\nBalanced chromosomal rearrangements can have a significant contribution to diseases, as demonstrated by the studies of leukemia. However, many of them are undetected by chromosomal microarray. Karyotyping and FISH can identify balanced translocations and inversions but are labor-intensive and provide low resolution (small genomic changes are missed).\n\nA jumping library NGS combined approach can be applied to identify such genomic changes. For example, Slade et al. applied this method to fine map a de novo balanced translocation in a child with Wilms' tumor. For this study, 50 million reads were generated, but only 11.6% of these could be mapped uniquely to the reference genome, which represents approximately a sixfold coverage.\n\nTalkowski et al. compared different approaches to detect balanced chromosome alterations, and showed that modified jumping library in combination with next generation DNA sequencing is an accurate method for mapping chromosomal breakpoints. Two varieties of jumping libraries (short-jump libraries and custom barcoded jumping libraries) were tested and compared to standard sequencing libraries. \nFor standard NGS, 200-500bp fragments are generated. About 0.03% -0.54% of fragments represent chimeric pairs, which are pairs of end-reads that are mapped to two different chromosomes. Therefore, very few fragments cover the breakpoint area.\nWhen using short-jump libraries with fragments of 3.2–3.8kb, the percentage of chimeric pairs increased to 1.3%. \nWith Custom Barcoded Jumping Libraries, the percentage of chimeric pairs further increased to 1.49%.\n\nConventional cytogenetic testing cannot offer the gene-level resolution required to predict the outcome of a pregnancy and whole genome deep sequencing is not practical for routine prenatal diagnosis. Whole-genome jumping library could complement conventional prenatal testing. This novel method was successfully applied to idenfity a case of CHARGE syndrome.\n\nIn metagenomics, regions of the genomes that are shared between strains are typically longer than the reads. This complicates the assembly process and makes reconstructing individual genomes for a species a daunting task. Chimeric pairs that are mapped far apart in the genome can facilitate the de novo assembly process. By using a longer-jump library, Ribeiro et al. demonstrated that the assemblies of bacterial genomes were of high quality while reducing both cost and time.\n\nThe cost of sequencing has dropped dramatically during the past few years while the cost of construction of jumping libraries has not. Therefore,as newsequencing technologies and bioinformatic tools are developed, jumping libraries may become redundant.\n\n\n", "id": "38679864", "title": "Jumping library"}
{"url": "https://en.wikipedia.org/wiki?curid=38255399", "text": "Exposome\n\nThe exposome encompasses the \"totality\" of human environmental (i.e. non-genetic) exposures from conception onwards, complementing the genome, first proposed in 2005 by a cancer epidemiologist. The concept of the exposome and how to assess it has led to lively discussions with varied views. As of 2016 it may not be possible to measure or model the full exposome, several European projects such as HELIX, EXPOsOMICS, and HEALS and the U.S. initiative HERCULES have started to make first attempts.\n\nThe exposome encompasses the \"totality\" of human environmental (i.e. non-genetic) exposures from conception onwards, complementing the genome. It was first proposed in 2005 by a cancer epidemiologist, in an article entitled \"Complementing the genome with an \"exposome\": the outstanding challenge of environmental exposure measurement in molecular epidemiology\". The concept of the exposome and how to assess it has led to lively discussions with varied views in 2010,2012 and 2014, \n\nIn his 2005 article Wild stated, \"At its most complete, the exposome encompasses life-course environmental exposures (including lifestyle factors), from the prenatal period onwards.\" The concept was first proposed to draw attention to the need for better and more complete environmental exposure data for causal research, in order to balance the investment in genetics. Per Wild even incomplete versions of the exposome could be useful to epidemiology. In 2012 Wild outlined methods, including personal sensors, biomarkers and 'omics' technologies, to better define the exposome. He described three overlapping domains within the exposome:\nIn late 2013 this definition was explained in greater depth in the first book on the exposome. \nIn 2014, the same author revised the definition to include the body's response with its endogenous metabolic processes which alter processing of chemicals.\n\nFor complex disorders specific genetic causes appear to only account for 10-30% of the disease incidence, but there has been no standard or systematic way to measure the influence of environmental exposures. Some studies into the interaction of genetic and environmental factors in the incidence of diabetes have demonstrated that \"environment-wide association studies\" (EWAS, or exposome-wide association studies) may be feasible. However, it is not clear what data sets are most appropriate to represent the value of \"E\".\n\nIn July 2017, saliva was suggested as a practical specimen to measure the human exposome, and because it is easy to collect, to analyse it repeatedly in longitudinal EWAS. The authors found concentrations of 1,233 chemicals and 169 metabolites had been detected in saliva per their literature and saliva–metabolome database review, which fit into 49 metabolic pathways.\n\nAs of 2016 it may not be possible to measure or model the full exposome, but several European projects have started to make first attempts.\nIn 2012, the European Commission awarded two large-grants to pursue exposome-related research.\n\nThe HELIX project at the Barcelona-based Centre for Research in Environmental Epidemiology will attempt to develop an early life exposome, noting that the first exposures occur during development. It will build upon six existing birth cohorts across Europe and measure the exposome at key prenatal and early childhood time points, through the use of GIS, personal sensors, biomarkers and omics platforms. \nA second project, Exposomics, based at Imperial College London will use smartphones which utilize GPS and environmental sensors to assess exposures. \nIn late 2013, a major initiative called the \"Health and Environment-Wide Associations based on Large Scale population Surveys\" or HEALS began. Touted as the largest environmental health-related study in Europe, HEALS proposes to adopt a paradigm defined by interactions between DNA sequence, epigenetic DNA modifications, gene expression and environmental factors.\n\nIn December 2011, the US National Academy of Sciences hosted a meeting entitled \"Emerging Technologies for Measuring Individual Exposomes.\" A Centers for Disease Control and Prevention overview \"Exposome and Exposomics\" outlines the three priority areas for researching the occupational exposome as identified by the National Institute for Occupational Safety and Health. The National Institutes of Health (NIH) has invested in technologies supporting exposome-related research, including biosensors, and supports research on gene-environment interactions. In May, 2013, the National Institute of Environmental Health Sciences (NIEHS) awarded a Core Center Grant to Emory University´s exposome project HERCULES.\n\nThe idea of a Human Exposome Project, analogous to the Human Genome Project, has been proposed and discussed in numerous scientific meetings, but as of 2017 no such project exists. Given the lack of clarity on how science would go about pursuing such a project, support has been lacking. Reports on the issue include:\n\n\nThe concept of exposome has contributed to the 2010 proposal of a new paradigm in disease phenotype, \"the unique disease principle\": Every individual has a unique disease process different from any other individual, considering uniqueness of the exposome and its unique influence on molecular pathologic processes including alterations in the interactome. This principle was first described in neoplastic diseases as \"the unique tumor principle\". Based on this unique disease principle, the interdisciplinary field of molecular pathological epidemiology (MPE) integrates molecular pathology and epidemiology.\n\n", "id": "38255399", "title": "Exposome"}
{"url": "https://en.wikipedia.org/wiki?curid=38731303", "text": "Boom method\n\nBoom method (Boom nucleic acid extraction method) is a solid phase extraction method for isolating nucleic acid from a biological sample. This method is characterized by \"absorbing the nucleic acids (NA) to the silica beads\".\n\nBoom method (Boom nucleic acid extraction method)\nis a solid phase extraction method for \"isolating nucleic acid (NA)\nfrom biological samples. Essential of this method is the use of silica beads, capable of binding the NA in the presence of a chaotropic substance according to the effect.\nThis method is one of the most widespread methods for isolating nucleic acids from biological samples and is known as a simple, rapid, and reliable method for the small-scale purification of NA from biological sample.\n\nThis method is said to have been developed and invented by Willem R. Boom et al. around 1990.\nHowever, the aforementioned chaotropic effect itself was already known and already reported by Vogelstein and Gillespie before the development of the BOOM method.\nSo the contribution of Boom et al. may be the optimization of the method to complex starting materials, such as body fluids and other biological starting materials, and provides a short step procedure according to the Boom et al. US5234809. After the Boom et al. was filed,\nsimilar applications have also been filed by other parties.\n\nIn a narrow sense, the word \"silica\" meant SiO crystals; however, other forms of silica particles are available.\nEspecially amorphous silicon oxide and glass powder, alkylsilica, aluminum silicate (zeolite), or, activated\nsilica with -NH, are suitable as nucleic acid binding solid phase material according to this method.\nToday, the embodyments of Boom method, characterized by \"utilizing the magnetic beads (silica beads are magnetic beads)\" is widely used. In such method, silica beads are captured by magnetic beads collector, such as Tajima pipette, Pick pen(R),\nQuad Pole collector, and so on.\n\nThe process for isolating nucleic acid from starting material of Boom method are essentially consist of following 4 steps (See Fig. 1).\n(a) Lysing and/or Homogenizing the starting material.<br>\nLysate of starting material is obtained by for example a detergent\nin the presence of protein degrading enzymes.<br><br>\n\n(b) Mixing chaotropic substance and silica beads into the starting material.<br>\nMixing the Starting material, a chaotropic substance to bind the NA to silica beads, lysate of starting material of (a) is mixed with sufficiently large amounts of chaotropic substance.\nAccording to the chaotropic effect, releasing-NA will be bound to\nthe silica beads almost instantaneously. In this way, silica-nucleic\nacid complexes are formed. The reasons why NA and silica form\nbonds are to be described in the following section (Basic principles).<br><br>\n\n(c) Washing silica beads <br>\nIn this step, silica beads of (b) are washed several times to remove contaminants.\nProcess of washing of the silica-nucleic acid complexes (silica beads) typically consists of following steps,\n\n(d)Separating the bonded nucleic acids <br>\nSeparating the bonded nucleic acids from the silica beads. Pure NA are eluted into buffer by decreasing the concentration of chaotropic substance.\nNA presented in the washed (and preferably dried) silica-nucleic\nacid complexes is eluted into elution buffer such as TE buffer,\naqua bidest, ... , and so on. The selection of the elution buffer is\nco-determined by the contemplated use of the isolated NA.\nIn this way, pure NA are isolated from the starting material.\n\nBy the alteration of the experimental condition, especially by alteration of the composition of reagents (chaotropic substance, wash buffer, and so on) we can realize more specific isolation. For example, some composition of reagents are suitable for obtaining long ds DNA, some composition of reagents are suitable for short ss RNA, and so on.\n\nStarting material are for example, whole blood, blood serum, buffy coat, urine, feces, cerebrospinal fluid, sperm, saliva, tissues, cell cultures, food products, vaccines. and..., so various Starting biological material are available.\n\nOf cause optimization of procedure according to the starting material, species of desired nucleic acid (DNA/RNA, Linear/ circular, ds/ss, long/short) are required.\n\nToday, the assay characterized by using silica coated magnetic beads seems to be the most common. Therefore, in this article, \"silica beads\" are intended to mean silica coated magnetic beads unless stated otherwise.\n\nThe silica coated beads coated various magnetic particles (magnetic carrier) with silica\nare often used.\nMaghemite particle (γ-FeO) and magnetite particle (FeO),\nas well as an intermediate iron oxide particle thereof, are most suitable\nas magnetic carrier.\n\nGenerally, the quality of the magnetic beads are characterized by\nfollowing parameters:\n\nHere, \"ease of collection\" is defined and compared by\n\"magnetic beads are collected by not less than X wt % (～90wt %) within T seconds(～ 3 seconds) in the presence of a magnetic field of Y gauss (～3000 gauss) when it is dispersed in an amount of at least Z mg (～20 mg) in W mL (～1 mL) of an aqueous solution of a sample containing a biological substance\" and, capture ability are defined and compared by\n\"binding with at least A μg (～0.4μg) of the biological substance per B mg (～1 mg) thereof\nwhen it is dispersed in an amount of at least Z mg (～20 mg) in W mL (～1 mL) of an aqueous solution of a sample containing a biological substance\".\nThe principle of this method is based on the nucleic acid-binding properties of silica particles or diatoms in the presence of this chaotropic agent, which are according to the chaotropic effect.\n\nPut simply, the chaotropic effect is where a chaotropic anion in an aqueous solution disturbs the structure of water, and weakens the hydrophobic interaction.\nIn a broad sense, \"chaotropic agent\" stands for any substance\ncapable of altering the secondary, tertiary and/or quaternary\nstructure of proteins and nucleic acids, but leaving at least the\nprimary structure intact.\n\nAqueous solution of chaotropic salt is a chaotropic agent. Chaotropic anion increase the entropy of the system by interfering with inter molecular interactions mediated by non-covalent forces such as hydrogen bonds, van der Waals forces, and hydrophobic effects.\nExamples thereof are aqueous solution of:\nthiocyanate ion, iodine ion, perchlorate ion,\nnitrate ion, bromine ion,\nchlorine ion, acetate ion, fluorine ion, and\nsulfate ion, or mutual combinations therewith.\nAccording to the original method of Boom method, the chaotropic\nguanidinium salt employed IS preferably guanidinium\nthiocyanate (GuSCN).\nAccording to the chaotropic effect,\nin the presence of the chaotropic agent, hydration water of NA are taken from the phosphodiester bond of the phosphate group of the skeleton of a NA. Thereby, the phosphate group becomes \"exposed\" and hydrophobic interaction between silica and exposed phosphate group are formed.\n\nNucleic acid extraction apparatus based on the Tajima pipette (see Fig. 2) are one of the most widespread instruments to perform the Boom method.\n\nThe Tajima pipette was invented by Hideji Tajima, founder and president of\nPrecision System Sciences (PSS) Inc., a Japanese manufacturer of precision and measuring instruments.\nTajima pipette is a Core Technology of PSS Inc.\nPSS Inc. provides OEM product based on this technology (for example MagNA Pure(R) )\nfor several leading reagent manufacturers such as Hoffmann-La Roche, Life Technologies, ... and so on.\nAfter Tajima et al. was filed, similar applications such as have also been filed by other parties.\n\nThe Tajima pipette performs magnetic particle control method and procedure, which can separate magnetic particles combined with a target substance from the liquid by magnetic force and suspend them in a liquid.\n\nThe pipette itself is an apparatus comprising following members (see Fig. 2).\n\nA nucleic acid extraction apparatus incorporating Tajima pipettes typically consists of:\n\n(a) Capturing of the magnetic beads.<br>\nDuring this suction process,\nwhen the magnetic field are applied to the separation region of piper tip, from outside of pipette tip, by the magnet arranged on the outside of the pipette tip,\nas liquid containing magnetic beads passes through a separation region of the pipette tip,\nthe magnetic particles are attracted to and arrested to the inner wall of tile separation region of pipette tip.\n\nSubsequently, when that solution are discharged under the conditions of has been kept the magnetic field,\nmagnetic particles only are left in the inside of pipette tip.\nIn this way magnetic particles are separated from liquid.\n\nIn accordance with Tajima,\nthe preferable suction height of the mixture liquid is such that\n\nAt this time, because the magnetic particles are wet, they stay attached to the inner surface of the\nseparation region of the liquid passage of the pipette tip. If the pipette tip P is moved or\ntransported, the magnetic particles will not come off easily.\n\n(b) Re-suspension of the captured magnetic beads.<br>\n\nAfter the magnetic particles are arrested by above mentioned manner (a),\nwe can do the re-suspension process.\n\nRe-suspension of the captured magnetic beads are in detail, consists of the following steps.\nOf cause, we consider that, the state in which that magnetic material has been captured\nby above mention way.\n\nAn example of the operations of the nucleic acid extraction apparatus which incorporates\nTajima pipette are typically as shown in Fig. 1.\n\nExamples of other type of method of the magnetic particle capturing device are as follows.\n", "id": "38731303", "title": "Boom method"}
{"url": "https://en.wikipedia.org/wiki?curid=1158125", "text": "DNA sequencing\n\nDNA sequencing is the process of determining the precise order of nucleotides within a DNA molecule. It includes any method or technology that is used to determine the order of the four bases—adenine, guanine, cytosine, and thymine—in a strand of DNA. The advent of rapid DNA sequencing methods has greatly accelerated biological and medical research and discovery.\n\nKnowledge of DNA sequences has become indispensable for basic biological research, and in numerous applied fields such as medical diagnosis, biotechnology, forensic biology, virology and biological systematics. The rapid speed of sequencing attained with modern DNA sequencing technology has been instrumental in the sequencing of complete DNA sequences, or genomes of numerous types and species of life, including the human genome and other complete DNA sequences of many animal, plant, and microbial species.\nThe first DNA sequences were obtained in the early 1970s by academic researchers using laborious methods based on two-dimensional chromatography. Following the development of fluorescence-based sequencing methods with a DNA sequencer, DNA sequencing has become easier and orders of magnitude faster.\n\nDNA sequencing may be used to determine the sequence of individual genes, larger genetic regions (i.e. clusters of genes or operons), full chromosomes or entire genomes, of any organism. DNA sequencing is also the most efficient way to sequence RNA or proteins (via their open reading frames). In fact, DNA sequencing has become a key technology in many areas of biology and other sciences such as medicine, forensics, or anthropology.\n\nSequencing is used in molecular biology to study genomes and the proteins they encode. Information obtained using sequencing allows researchers to identify changes in genes, associations with diseases and phenotypes, and identify potential drug targets.\n\nSince DNA is an informative macromolecule in terms of transmission from one generation to another, DNA sequencing is used in evolutionary biology to study how different organisms are related and how they evolved.\n\nThe field of metagenomics involves identification of organisms present in a body of water, sewage, dirt, debris filtered from the air, or swab samples from organisms. Knowing which organisms are present in a particular environment is critical to research in ecology, epidemiology, microbiology, and other fields. Sequencing enables researchers to determine which types of microbes may be present in a microbiome, for example.\n\nMedical technicians may sequence genes (or, theoretically, full genomes) from patients to determine if there is risk of genetic diseases. This is a form of genetic testing, though some genetic tests may not involve DNA sequencing.\n\nDNA sequencing may be used along with DNA profiling methods for forensic identification and paternity testing. DNA testing has evolved tremendously in the last few decades to ultimately link a DNA print to what is under investigation. The DNA patterns in fingerprint, saliva, hair follicles, etc. uniquely separate each living organism from one another. Testing DNA is a technique which can detect specific genomes in a DNA strand to produce a unique and individualized pattern. Every living organism ever created has a one of a kind DNA pattern, which can be determined through DNA testing. It's extremely rare that two people have exactly the same DNA pattern, therefore DNA testing is highly successful.\n\nThe canonical structure of DNA has four bases: thymine (T), adenine (A), cytosine (C), and guanine (G). DNA sequencing is the determination of the physical order of these bases in a molecule of DNA. However, there are many other bases that may be present in a molecule. In some viruses (specifically, bacteriophage), cytosine may be replaced by hydroxy methyl or hydroxy methyl glucose cytosine. In mammalian DNA, variant bases with methyl groups or phosphosulfate may be found. Depending on the sequencing technique, a particular modification, e.g., the 5mC (5 methyl cytosine) common in humans, may or may not be detected.\n\nDeoxyribonucleic acid (DNA) was first discovered and isolated by Friedrich Miescher in 1869, but it remained understudied for many decades because proteins, rather than DNA, were thought to hold the genetic blueprint to life. This situation changed after 1944 as a result of some experiments by Oswald Avery, Colin MacLeod, and Maclyn McCarty demonstrating that purified DNA could change one strain of bacteria into another. This was the first time that DNA was shown capable of transforming the properties of cells.\n\nIn 1953, James Watson and Francis Crick put forward their double-helix model of DNA, based on crystallized X-ray structures being studied by Rosalind Franklin. According to the model, DNA is composed of two strands of nucleotides coiled around each other, linked together by hydrogen bonds and running in opposite directions. Each strand is composed of four complementary nucleotides – adenine (A), cytosine (C), guanine (G) and thymine (T) – with an A on one strand always paired with T on the other, and C always paired with G. They proposed such a structure allowed each strand to be used to reconstruct the other, an idea central to the passing on of hereditary information between generations.\n\nThe foundation for sequencing proteins was first laid by the work of Frederick Sanger who by 1955 had completed the sequence of all the amino acids in insulin, a small protein secreted by the pancreas. This provided the first conclusive evidence that proteins were chemical entities with a specific molecular pattern rather than a random mixture of material suspended in fluid. Sanger's success in sequencing insulin greatly electrified x-ray crystallographers, including Watson and Crick who by now were trying to understand how DNA directed the formation of proteins within a cell. Soon after attending a series of lectures given by Frederick Sanger in October 1954, Crick began to develop a theory which argued that the arrangement of nucleotides in DNA determined the sequence of amino acids in proteins which in turn helped determine the function of a protein. He published this theory in 1958.\n\nRNA sequencing was one of the earliest forms of nucleotide sequencing. The major landmark of RNA sequencing is the sequence of the first complete gene and the complete genome of Bacteriophage MS2, identified and published by Walter Fiers and his coworkers at the University of Ghent (Ghent, Belgium), in 1972 and 1976. Traditional RNA sequencing methods require the creation of a cDNA molecule which must be sequenced.\n\nThe first method for determining DNA sequences involved a location-specific primer extension strategy established by Ray Wu at Cornell University in 1970. DNA polymerase catalysis and specific nucleotide labeling, both of which figure prominently in current sequencing schemes, were used to sequence the cohesive ends of lambda phage DNA. Between 1970 and 1973, Wu, R Padmanabhan and colleagues demonstrated that this method can be employed to determine any DNA sequence using synthetic location-specific primers. Frederick Sanger then adopted this primer-extension strategy to develop more rapid DNA sequencing methods at the MRC Centre, Cambridge, UK and published a method for \"DNA sequencing with chain-terminating inhibitors\" in 1977. Walter Gilbert and Allan Maxam at Harvard also developed sequencing methods, including one for \"DNA sequencing by chemical degradation\". In 1973, Gilbert and Maxam reported the sequence of 24 basepairs using a method known as wandering-spot analysis. Advancements in sequencing were aided by the concurrent development of recombinant DNA technology, allowing DNA samples to be isolated from sources other than viruses.\n\nThe first full DNA genome to be sequenced was that of bacteriophage φX174 in 1977. Medical Research Council scientists deciphered the complete DNA sequence of the Epstein-Barr virus in 1984, finding it contained 172,282 nucleotides. Completion of the sequence marked a significant turning point in DNA sequencing because it was achieved with no prior genetic profile knowledge of the virus.\n\nA non-radioactive method for transferring the DNA molecules of sequencing reaction mixtures onto an immobilizing matrix during electrophoresis was developed by Pohl and co-workers in the early 1980s. Followed by the commercialization of the DNA sequencer \"Direct-Blotting-Electrophoresis-System GATC 1500\" by GATC Biotech, which was intensively used in the framework of the EU genome-sequencing programme, the complete DNA sequence of the yeast \"Saccharomyces cerevisiae\" chromosome II. Leroy E. Hood's laboratory at the California Institute of Technology announced the first semi-automated DNA sequencing machine in 1986. This was followed by Applied Biosystems' marketing of the first fully automated sequencing machine, the ABI 370, in 1987 and by Dupont's Genesis 2000 which used a novel fluorescent labeling technique enabling all four dideoxynucleotides to be identified in a single lane. By 1990, the U.S. National Institutes of Health (NIH) had begun large-scale sequencing trials on \"Mycoplasma capricolum\", \"Escherichia coli\", \"Caenorhabditis elegans\", and \"Saccharomyces cerevisiae\" at a cost of US$0.75 per base. Meanwhile, sequencing of human cDNA sequences called expressed sequence tags began in Craig Venter's lab, an attempt to capture the coding fraction of the human genome. In 1995, Venter, Hamilton Smith, and colleagues at The Institute for Genomic Research (TIGR) published the first complete genome of a free-living organism, the bacterium \"Haemophilus influenzae\". The circular chromosome contains 1,830,137 bases and its publication in the journal Science marked the first published use of whole-genome shotgun sequencing, eliminating the need for initial mapping efforts.\n\nBy 2001, shotgun sequencing methods had been used to produce a draft sequence of the human genome.\n\nSeveral new methods for DNA sequencing were developed in the mid to late 1990s and were implemented in commercial DNA sequencers by the year 2000. Together these were called the \"next-generation\" or \"second-generation\" sequencing methods.\n\nOn October 26, 1990, Roger Tsien, Pepi Ross, Margaret Fahnestock and Allan J Johnston filed a patent describing stepwise (\"base-by-base\") sequencing with removable 3' blockers on DNA arrays (blots and single DNA molecules).\nIn 1996, Pål Nyrén and his student Mostafa Ronaghi at the Royal Institute of Technology in Stockholm published their method of pyrosequencing.\n\nOn April 1, 1997, Pascal Mayer and Laurent Farinelli submitted patents to the World Intellectual Property Organization describing DNA colony sequencing. The DNA sample preparation and random surface-PCR arraying methods described in this patent, coupled to Roger Tsien et al.'s \"base-by-base\" sequencing method, is now implemented in Illumina's Hi-Seq genome sequencers.\n\nLynx Therapeutics published and marketed Massively parallel signature sequencing (MPSS), in 2000. This method incorporated a parallelized, adapter/ligation-mediated, bead-based sequencing technology and served as the first commercially available \"next-generation\" sequencing method, though no DNA sequencers were sold to independent laboratories.\n\nThe large quantities of data produced by DNA sequencing have also required development of new methods and programs for sequence analysis. Phil Green and Brent Ewing of the University of Washington described their phred quality score for sequencer data analysis in 1998.\n\nAllan Maxam and Walter Gilbert published a DNA sequencing method in 1977 based on chemical modification of DNA and subsequent cleavage at specific bases. Also known as chemical sequencing, this method allowed purified samples of double-stranded DNA to be used without further cloning. This method's use of radioactive labeling and its technical complexity discouraged extensive use after refinements in the Sanger methods had been made.\n\nMaxam-Gilbert sequencing requires radioactive labeling at one 5' end of the DNA and purification of the DNA fragment to be sequenced. Chemical treatment then generates breaks at a small proportion of one or two of the four nucleotide bases in each of four reactions (G, A+G, C, C+T). The concentration of the modifying chemicals is controlled to introduce on average one modification per DNA molecule. Thus a series of labeled fragments is generated, from the radiolabeled end to the first \"cut\" site in each molecule. The fragments in the four reactions are electrophoresed side by side in denaturing acrylamide gels for size separation. To visualize the fragments, the gel is exposed to X-ray film for autoradiography, yielding a series of dark bands each corresponding to a radiolabeled DNA fragment, from which the sequence may be inferred.\n\nThe chain-termination method developed by Frederick Sanger and coworkers in 1977 soon became the method of choice, owing to its relative ease and reliability. When invented, the chain-terminator method used fewer toxic chemicals and lower amounts of radioactivity than the Maxam and Gilbert method. Because of its comparative ease, the Sanger method was soon automated and was the method used in the first generation of DNA sequencers.\n\nSanger sequencing is the method which prevailed from the 1980s until the mid-2000s. Over that period, great advances were made in the technique, such as fluorescent labelling, capillary electrophoresis, and general automation. These developments allowed much more efficient sequencing, leading to lower costs. The Sanger method, in mass production form, is the technology which produced the first human genome in 2001, ushering in the age of genomics. However, later in the decade, radically different approaches reached the market, bringing the cost per genome down from $100 million in 2001 to $10,000 in 2011.\n\n Large-scale sequencing often aims at sequencing very long DNA pieces, such as whole chromosomes, although large-scale sequencing can also be used to generate very large numbers of short sequences, such as found in phage display. For longer targets such as chromosomes, common approaches consist of cutting (with restriction enzymes) or shearing (with mechanical forces) large DNA fragments into shorter DNA fragments. The fragmented DNA may then be cloned into a DNA vector and amplified in a bacterial host such as \"Escherichia coli\". Short DNA fragments purified from individual bacterial colonies are individually sequenced and assembled electronically into one long, contiguous sequence. Studies have shown that adding a size selection step to collect DNA fragments of uniform size can improve sequencing efficiency and accuracy of the genome assembly. In these studies, automated sizing has proven to be more reproducible and precise than manual gel sizing.\n\nThe term \"\"de novo\" sequencing\" specifically refers to methods used to determine the sequence of DNA with no previously known sequence. \"De novo\" translates from Latin as \"from the beginning\". Gaps in the assembled sequence may be filled by primer walking. The different strategies have different tradeoffs in speed and accuracy; shotgun methods are often used for sequencing large genomes, but its assembly is complex and difficult, particularly with sequence repeats often causing gaps in genome assembly.\n\nMost sequencing approaches use an \"in vitro\" cloning step to amplify individual DNA molecules, because their molecular detection methods are not sensitive enough for single molecule sequencing. Emulsion PCR isolates individual DNA molecules along with primer-coated beads in aqueous droplets within an oil phase. A polymerase chain reaction (PCR) then coats each bead with clonal copies of the DNA molecule followed by immobilization for later sequencing. Emulsion PCR is used in the methods developed by Marguilis et al. (commercialized by 454 Life Sciences), Shendure and Porreca et al. (also known as \"Polony sequencing\") and SOLiD sequencing, (developed by Agencourt, later Applied Biosystems, now Life Technologies). Emulsion PCR is also used in the GemCode and Chromium platforms developed by 10x Genomics.\n\nShotgun sequencing is a sequencing method designed for analysis of DNA sequences longer than 1000 base pairs, up to and including entire chromosomes. This method requires the target DNA to be broken into random fragments. After sequencing individual fragments, the sequences can be reassembled on the basis of their overlapping regions.\n\nAnother method for \"in vitro\" clonal amplification is bridge PCR, in which fragments are amplified upon primers attached to a solid surface and form \"DNA colonies\" or \"DNA clusters\". This method is used in the Illumina Genome Analyzer sequencers. Single-molecule methods, such as that developed by Stephen Quake's laboratory (later commercialized by Helicos) are an exception: they use bright fluorophores and laser excitation to detect base addition events from individual DNA molecules fixed to a surface, eliminating the need for molecular amplification.\n\nHigh-throughput (formerly \"next-generation\") sequencing applies to genome sequencing, genome resequencing, transcriptome profiling (RNA-Seq), DNA-protein interactions (ChIP-sequencing), and epigenome characterization. Resequencing is necessary, because the genome of a single individual of a species will not indicate all of the genome variations among other individuals of the same species.\n\nThe high demand for low-cost sequencing has driven the development of high-throughput sequencing technologies that parallelize the sequencing process, producing thousands or millions of sequences concurrently. High-throughput sequencing technologies are intended to lower the cost of DNA sequencing beyond what is possible with standard dye-terminator methods. In ultra-high-throughput sequencing as many as 500,000 sequencing-by-synthesis operations may be run in parallel.\n\nThe first of the high-throughput sequencing technologies, massively parallel signature sequencing (or MPSS), was developed in the 1990s at Lynx Therapeutics, a company founded in 1992 by Sydney Brenner and Sam Eletr. MPSS was a bead-based method that used a complex approach of adapter ligation followed by adapter decoding, reading the sequence in increments of four nucleotides. This method made it susceptible to sequence-specific bias or loss of specific sequences. Because the technology was so complex, MPSS was only performed 'in-house' by Lynx Therapeutics and no DNA sequencing machines were sold to independent laboratories. Lynx Therapeutics merged with Solexa (later acquired by Illumina) in 2004, leading to the development of sequencing-by-synthesis, a simpler approach acquired from Manteia Predictive Medicine, which rendered MPSS obsolete. However, the essential properties of the MPSS output were typical of later high-throughput data types, including hundreds of thousands of short DNA sequences. In the case of MPSS, these were typically used for sequencing cDNA for measurements of gene expression levels.\n\nThe Polony sequencing method, developed in the laboratory of George M. Church at Harvard, was among the first high-throughput sequencing systems and was used to sequence a full \"E. coli\" genome in 2005. It combined an in vitro paired-tag library with emulsion PCR, an automated microscope, and ligation-based sequencing chemistry to sequence an \"E. coli\" genome at an accuracy of >99.9999% and a cost approximately 1/9 that of Sanger sequencing. The technology was licensed to Agencourt Biosciences, subsequently spun out into Agencourt Personal Genomics, and eventually incorporated into the Applied Biosystems SOLiD platform. Applied Biosystems was later acquired by Life Technologies, now part of Thermo Fisher Scientific.\n\nA parallelized version of pyrosequencing was developed by 454 Life Sciences, which has since been acquired by Roche Diagnostics. The method amplifies DNA inside water droplets in an oil solution (emulsion PCR), with each droplet containing a single DNA template attached to a single primer-coated bead that then forms a clonal colony. The sequencing machine contains many picoliter-volume wells each containing a single bead and sequencing enzymes. Pyrosequencing uses luciferase to generate light for detection of the individual nucleotides added to the nascent DNA, and the combined data are used to generate sequence reads. This technology provides intermediate read length and price per base compared to Sanger sequencing on one end and Solexa and SOLiD on the other.\n\nSolexa, now part of Illumina, was founded by Shankar Balasubramanian and David Klenerman in 1998, and developed a sequencing method based on reversible dye-terminators technology, and engineered polymerases. The reversible terminated chemistry concept was invented by Bruno Canard and Simon Sarfati at the Pasteur Institute in Paris. It was developed internally at Solexa by those named on the relevant patents. In 2004, Solexa acquired the company Manteia Predictive Medicine in order to gain a massivelly parallel sequencing technology invented in 1997 by Pascal Mayer and Laurent Farinelli. It is based on \"DNA Clusters\" or \"DNA colonies\", which involves the clonal amplification of DNA on a surface. The cluster technology was co-acquired with Lynx Therapeutics of California. Solexa Ltd. later merged with Lynx to form Solexa Inc.\n\nIn this method, DNA molecules and primers are first attached on a slide or flow cell and amplified with polymerase so that local clonal DNA colonies, later coined \"DNA clusters\", are formed. To determine the sequence, four types of reversible terminator bases (RT-bases) are added and non-incorporated nucleotides are washed away. A camera takes images of the fluorescently labeled nucleotides. Then the dye, along with the terminal 3' blocker, is chemically removed from the DNA, allowing for the next cycle to begin. Unlike pyrosequencing, the DNA chains are extended one nucleotide at a time and image acquisition can be performed at a delayed moment, allowing for very large arrays of DNA colonies to be captured by sequential images taken from a single camera.\n\nDecoupling the enzymatic reaction and the image capture allows for optimal throughput and theoretically unlimited sequencing capacity. With an optimal configuration, the ultimately reachable instrument throughput is thus dictated solely by the analog-to-digital conversion rate of the camera, multiplied by the number of cameras and divided by the number of pixels per DNA colony required for visualizing them optimally (approximately 10 pixels/colony). In 2012, with cameras operating at more than 10 MHz A/D conversion rates and available optics, fluidics and enzymatics, throughput can be multiples of 1 million nucleotides/second, corresponding roughly to 1 human genome equivalent at 1x coverage per hour per instrument, and 1 human genome re-sequenced (at approx. 30x) per day per instrument (equipped with a single camera).\n\nApplied Biosystems' (now a Life Technologies brand) SOLiD technology employs sequencing by ligation. Here, a pool of all possible oligonucleotides of a fixed length are labeled according to the sequenced position. Oligonucleotides are annealed and ligated; the preferential ligation by DNA ligase for matching sequences results in a signal informative of the nucleotide at that position. Before sequencing, the DNA is amplified by emulsion PCR. The resulting beads, each containing single copies of the same DNA molecule, are deposited on a glass slide. The result is sequences of quantities and lengths comparable to Illumina sequencing. This sequencing by ligation method has been reported to have some issue sequencing palindromic sequences.\n\nIon Torrent Systems Inc. (now owned by Life Technologies) developed a system based on using standard sequencing chemistry, but with a novel, semiconductor based detection system. This method of sequencing is based on the detection of hydrogen ions that are released during the polymerisation of DNA, as opposed to the optical methods used in other sequencing systems. A microwell containing a template DNA strand to be sequenced is flooded with a single type of nucleotide. If the introduced nucleotide is complementary to the leading template nucleotide it is incorporated into the growing complementary strand. This causes the release of a hydrogen ion that triggers a hypersensitive ion sensor, which indicates that a reaction has occurred. If homopolymer repeats are present in the template sequence, multiple nucleotides will be incorporated in a single cycle. This leads to a corresponding number of released hydrogens and a proportionally higher electronic signal.\n\nDNA nanoball sequencing is a type of high throughput sequencing technology used to determine the entire genomic sequence of an organism. The company Complete Genomics uses this technology to sequence samples submitted by independent researchers. The method uses rolling circle replication to amplify small fragments of genomic DNA into DNA nanoballs. Unchained sequencing by ligation is then used to determine the nucleotide sequence. This method of DNA sequencing allows large numbers of DNA nanoballs to be sequenced per run and at low reagent costs compared to other high-throughput sequencing platforms. However, only short sequences of DNA are determined from each DNA nanoball which makes mapping the short reads to a reference genome difficult. This technology has been used for multiple genome sequencing projects and is scheduled to be used for more.\n\nHeliscope sequencing is a method of single-molecule sequencing developed by Helicos Biosciences. It uses DNA fragments with added poly-A tail adapters which are attached to the flow cell surface. The next steps involve extension-based sequencing with cyclic washes of the flow cell with fluorescently labeled nucleotides (one nucleotide type at a time, as with the Sanger method). The reads are performed by the Heliscope sequencer. The reads are short, averaging 35 bp. In 2009 a human genome was sequenced using the Heliscope, however in 2012 the company went bankrupt.\n\nSMRT sequencing is based on the sequencing by synthesis approach. The DNA is synthesized in zero-mode wave-guides (ZMWs) – small well-like containers with the capturing tools located at the bottom of the well. The sequencing is performed with use of unmodified polymerase (attached to the ZMW bottom) and fluorescently labelled nucleotides flowing freely in the solution. The wells are constructed in a way that only the fluorescence occurring by the bottom of the well is detected. The fluorescent label is detached from the nucleotide upon its incorporation into the DNA strand, leaving an unmodified DNA strand. According to Pacific Biosciences (PacBio), the SMRT technology developer, this methodology allows detection of nucleotide modifications (such as cytosine methylation). This happens through the observation of polymerase kinetics. This approach allows reads of 20,000 nucleotides or more, with average read lengths of 5 kilobases. In 2015, Pacific Biosciences announced the launch of a new sequencing instrument called the Sequel System, with 1 million ZMWs compared to 150,000 ZMWs in the PacBio RS II instrument. SMRT sequencing is referred to as \"third-generation\" or \"long-read\" sequencing.\n\nThe DNA passing through the nanopore changes its ion current. This change is dependent on the shape, size and length of the DNA sequence. Each type of the nucleotide blocks the ion flow through the pore for a different period of time. The method does not require modified nucleotides and is performed in real time. Nanopore sequencing is referred to as \"third-generation\" or \"long-read\" sequencing, along with SMRT sequencing.\n\nEarly industrial research into this method was based on a technique called 'Exonuclease sequencing', where the readout of electrical signals occurring at nucleotides passing by alpha-hemolysin pores covalently bound with cyclodextrin. However the subsequently commercial method, 'strand sequencing' sequencing DNA bases in an intact strand.\n\nTwo main areas of nanopore sequencing in development are solid state nanopore sequencing, and protein based nanopore sequencing. Protein nanopore sequencing utilizes membrane protein complexes such as ∝-Hemolysin, MspA (Mycobacterium Smegmatis Porin A) or CssG, which show great promise given their ability to distinguish between individual and groups of nucleotides. In contrast, solid-state nanopore sequencing utilizes synthetic materials such as silicon nitride and aluminum oxide and it is preferred for its superior mechanical ability and thermal and chemical stability. The fabrication method is essential for this type of sequencing given that the nanopore array can contain hundreds of pores with diameters smaller than eight nanometers.\n\nThe concept originated from the idea that single stranded DNA or RNA molecules can be electrophoretically driven in a strict linear sequence through a biological pore that can be less than eight nanometers, and can be detected given that the molecules release an ionic current while moving through the pore. The pore contains a detection region capable of recognizing different bases, with each base generating various time specific signals corresponding to the sequence of bases as they cross the pore which are then evaluated. Precise control over the DNA transport through the pore is crucial for success. Various enzymes such as exonucleases and polymerases have been used to moderate this process by positioning them near the pore’s entrance.\n\nDNA sequencing methods currently under development include reading the sequence as a DNA strand transits through nanopores (a method that is now commercial but subsequent generations such as solid-state nanopores are still in development), and microscopy-based techniques, such as atomic force microscopy or transmission electron microscopy that are used to identify the positions of individual nucleotides within long DNA fragments (>5,000 bp) by nucleotide labeling with heavier elements (e.g., halogens) for visual detection and recording.\nThird generation technologies aim to increase throughput and decrease the time to result and cost by eliminating the need for excessive reagents and harnessing the processivity of DNA polymerase.\n\nAnother approach uses measurements of the electrical tunnelling currents across single-strand DNA as it moves through a channel. Depending on its electronic structure, each base affects the tunnelling current differently, allowing differentiation between different bases.\n\nThe use of tunnelling currents has the potential to sequence orders of magnitude faster than ionic current methods and the sequencing of several DNA oligomers and micro-RNA has already been achieved.\n\n\"Sequencing by hybridization\" is a non-enzymatic method that uses a DNA microarray. A single pool of DNA whose sequence is to be determined is fluorescently labeled and hybridized to an array containing known sequences. Strong hybridization signals from a given spot on the array identifies its sequence in the DNA being sequenced.\n\nThis method of sequencing utilizes binding characteristics of a library of short single stranded DNA molecules (oligonucleotides), also called DNA probes, to reconstruct a target DNA sequence. Non-specific hybrids are removed by washing and the target DNA is eluted. Hybrids are re-arranged such that the DNA sequence can be reconstructed. The benefit of this sequencing type is its ability to capture a large number of targets with a homogenous coverage. A large number of chemicals and starting DNA is usually required. However, with the advent of solution-based hybridization, much less equipment and chemicals are necessary.\n\nMass spectrometry may be used to determine DNA sequences. Matrix-assisted laser desorption ionization time-of-flight mass spectrometry, or MALDI-TOF MS, has specifically been investigated as an alternative method to gel electrophoresis for visualizing DNA fragments. With this method, DNA fragments generated by chain-termination sequencing reactions are compared by mass rather than by size. The mass of each nucleotide is different from the others and this difference is detectable by mass spectrometry. Single-nucleotide mutations in a fragment can be more easily detected with MS than by gel electrophoresis alone. MALDI-TOF MS can more easily detect differences between RNA fragments, so researchers may indirectly sequence DNA with MS-based methods by converting it to RNA first.\n\nThe higher resolution of DNA fragments permitted by MS-based methods is of special interest to researchers in forensic science, as they may wish to find single-nucleotide polymorphisms in human DNA samples to identify individuals. These samples may be highly degraded so forensic researchers often prefer mitochondrial DNA for its higher stability and applications for lineage studies. MS-based sequencing methods have been used to compare the sequences of human mitochondrial DNA from samples in a Federal Bureau of Investigation database and from bones found in mass graves of World War I soldiers.\n\nEarly chain-termination and TOF MS methods demonstrated read lengths of up to 100 base pairs. Researchers have been unable to exceed this average read size; like chain-termination sequencing alone, MS-based DNA sequencing may not be suitable for large \"de novo\" sequencing projects. Even so, a recent study did use the short sequence reads and mass spectroscopy to compare single-nucleotide polymorphisms in pathogenic \"Streptococcus\" strains.\n\nIn microfluidic Sanger sequencing the entire thermocycling amplification of DNA fragments as well as their separation by electrophoresis is done on a single glass wafer (approximately 10 cm in diameter) thus reducing the reagent usage as well as cost. In some instances researchers have shown that they can increase the throughput of conventional sequencing through the use of microchips. Research will still need to be done in order to make this use of technology effective.\n\nThis approach directly visualizes the sequence of DNA molecules using electron microscopy. The first identification of DNA base pairs within intact DNA molecules by enzymatically incorporating modified bases, which contain atoms of increased atomic number, direct visualization and identification of individually labeled bases within a synthetic 3,272 base-pair DNA molecule and a 7,249 base-pair viral genome has been demonstrated.\n\nThis method is based on use of RNA polymerase (RNAP), which is attached to a polystyrene bead. One end of DNA to be sequenced is attached to another bead, with both beads being placed in optical traps. RNAP motion during transcription brings the beads in closer and their relative distance changes, which can then be recorded at a single nucleotide resolution. The sequence is deduced based on the four readouts with lowered concentrations of each of the four nucleotide types, similarly to the Sanger method. A comparison is made between regions and sequence information is deduced by comparing the known sequence regions to the unknown sequence regions.\n\nA method has been developed to analyze full sets of protein interactions using a combination of 454 pyrosequencing and an \"in vitro\" virus mRNA display method. Specifically, this method covalently links proteins of interest to the mRNAs encoding them, then detects the mRNA pieces using reverse transcription PCRs. The mRNA may then be amplified and sequenced. The combined method was titled IVV-HiTSeq and can be performed under cell-free conditions, though its results may not be representative of \"in vivo\" conditions.\n\nThe success of any DNA sequencing protocol relies upon the DNA or RNA sample extraction and preparation from the biological material of interest.\n\nAccording to the sequencing technology to be used, the samples resulting from either the DNA or the RNA extraction require further preparation. For Sanger sequencing, either cloning procedures or PCR are required prior to sequencing. In the case of next-generation sequencing methods, library preparation is required before processing. Assessing the quality and quantity of nucleic acids both after extraction and after library preparation identifies degraded, fragmented, and low-purity samples and yields high-quality sequencing data.\n\nIn October 2006, the X Prize Foundation established an initiative to promote the development of full genome sequencing technologies, called the Archon X Prize, intending to award $10 million to \"the first Team that can build a device and use it to sequence 100 human genomes within 10 days or less, with an accuracy of no more than one error in every 100,000 bases sequenced, with sequences accurately covering at least 98% of the genome, and at a recurring cost of no more than $10,000 (US) per genome.\"\n\nEach year the National Human Genome Research Institute, or NHGRI, promotes grants for new research and developments in genomics. 2010 grants and 2011 candidates include continuing work in microfluidic, polony and base-heavy sequencing methodologies.\n\nThe sequencing technologies described here produce raw data that needs to be assembled into longer sequences such as complete genomes (sequence assembly). There are many computational challenges to achieve this, such as the evaluation of the raw sequence data which is done by programs and algorithms such as Phred and Phrap. Other challenges have to deal with repetitive sequences that often prevent complete genome assemblies because they occur in many places of the genome. As a consequence, many sequences may not be assigned to particular chromosomes. The production of raw sequence data is only the beginning of its detailed bioinformatical analysis. Yet new methods for sequencing and correcting sequencing errors were developed.\n\nSometimes, the raw reads produced by the sequencer are correct and precise only in a fraction of their length. Using the entire read may introduce artifacts in the downstream analyses like genome assembly, snp calling, or gene expression estimation. Two classes of trimming programs have been introduced, based on the window-based or the running-sum classes of algorithms. This is a partial list of the trimming algorithms currently available, specifying the algorithm class they belong to:\n\nHuman genetics have been included within the field of bioethics since the early 1970s and the growth in the use of DNA sequencing (particularly high-throughput sequencing) has introduced a number of ethical issues. One key issue is the ownership of an individual's DNA and the data produced when that DNA is sequenced. Regarding the DNA molecule itself, the leading legal case on this topic, \"Moore v. Regents of the University of California\" (1990) ruled that individuals have no property rights to discarded cells or any profits made using these cells (for instance, as a patented cell line). However, individuals have a right to informed consent regarding removal and use of cells. Regarding the data produced through DNA sequencing, \"Moore\" gives the individual no rights to the information derived from their DNA.\n\nAs DNA sequencing becomes more widespread, the storage, security and sharing of genomic data has also become more important. For instance, one concern is that insurers may use an individual's genomic data to modify their quote, depending on the perceived future health of the individual based on their DNA. In May 2008, the Genetic Information Nondiscrimination Act (GINA) was signed in the United States, prohibiting discrimination on the basis of genetic information with respect to health insurance and employment. In 2012, the US Presidential Commission for the Study of Bioethical Issues reported that existing privacy legislation for DNA sequencing data such as GINA and the Health Insurance Portability and Accountability Act were insufficient, noting that whole-genome sequencing data was particularly sensitive, as it could be used to identify not only the individual from which the data was created, but also their relatives.\n\nEthical issues have also been raised by the increasing use of genetic variation screening, both in newborns, and in adults by companies such as 23andMe. It has been asserted that screening for genetic variations can be harmful, increasing anxiety in individuals who have been found to have an increased risk of disease. For example, in one case noted in \"Time\", doctors screening an ill baby for genetic variants chose not to inform the parents of an unrelated variant linked to dementia due to the harm it would cause to the parents. However, a 2011 study in \"The New England Journal of Medicine\" has shown that individuals undergoing disease risk profiling did not show increased levels of anxiety.\n\n\n", "id": "1158125", "title": "DNA sequencing"}
{"url": "https://en.wikipedia.org/wiki?curid=38959571", "text": "Transcriptor\n\nA transcriptor is a transistor-like device composed of DNA and RNA rather than a semiconducting material such as silicon. Prior to its invention in 2013, the transcriptor was considered an important component to build biological computers.\n\nTo function, a modern computer needs three different capabilities: It must be able to store information, transmit information between components, and possess a basic system of logic. Prior to March 2013, scientists had successfully demonstrated the ability to store and transmit data using biological components made of proteins and DNA. Simple two-terminal logic gates had been demonstrated, but required multiple layers of inputs and thus were impractical due to scaling difficulties.\n\nOn March 28, 2013, a team of bioengineers from Stanford University led by Drew Endy announced that they had created the biological equivalent of a transistor, which they named a \"transcriptor\". That is, they created a three-terminal device with a logic system that can control other components. The transcriptor regulates the flow of RNA polymerase across a strand of DNA using special combinations of enzymes to control movement. According to project member Jerome Bonnet, \"The choice of enzymes is important. We have been careful to select enzymes that function in bacteria, fungi, plants and animals, so that bio-computers can be engineered within a variety of organisms.\" \n\nTranscriptors can replicate traditional AND, OR, NOR, NAND, XOR, and XNOR gates with equivalents, which Endy dubbed \"Boolean Integrase Logic (BIL) gates\", in a single-layer process (i.e., without requiring multiple instances of the simpler gates to build up more complex ones). Like a traditional transistor, a transcriptor can amplify an input signal. A group of transcriptors can do almost any type of computing, including counting and comparison.\n\nStanford dedicated the BIL gate's design to the public domain, which may speed its adoption. According to Endy, other researchers were already using the gates to reprogram metabolism when the Stanford team published its research.\n\nComputing by transcriptor is still very slow; it can take a few hours between receiving an input signal and generating an output. Endy doubted that biocomputers would ever be as fast as traditional computers, but added that is not the goal of his research. \"We're building computers that will operate in a place where your cellphone isn't going to work\", he said. Medical devices with built-in biological computers could monitor, or even alter, cell behavior from inside a patient's body. \"ExtremeTech\" writes:\nUC Berkeley biochemical engineer Jay Keasling said the transcriptor \"clearly demonstrates the power of synthetic biology and could revolutionize how we compute in the future\".\n\n", "id": "38959571", "title": "Transcriptor"}
{"url": "https://en.wikipedia.org/wiki?curid=339838", "text": "Molecular genetics\n\nMolecular genetics is the field of biology that studies the structure and function of genes at a molecular level and thus employs methods of both molecular biology and genetics. The study of chromosomes and gene expression of an organism can give insight into heredity, genetic variation, and mutations. This is useful in the study of developmental biology and in understanding and treating genetic diseases. \n\nGene amplification is a procedure in which a certain gene or DNA sequence is replicated many times in a process called DNA replication.\n\n\n\nIn separation and detection, DNA and mRNA are isolated from cells and then detected simply by the isolation. Cell cultures are also grown to provide a constant supply of cells ready for isolation.\n\n\n\nFirst, laboratories use a normal cellular modification of mRNA that adds up to 200 adenine nucleotides to the end of the molecule (poly(A) tail). Once this has been added, the cell is ruptured and its cell contents are exposed to synthetic beads that are coated with thymine string nucleotides. Because Adenine and Thymine pair together in DNA, the poly(A) tail and synthetic beads are attracted to one another, and once they bind in this process the cell components can be washed away without removing the mRNA. Once the mRNA has been isolated, reverse transcriptase is employed to convert it to single-stranded DNA, from which a stable double-stranded DNA is produced using DNA polymerase. Complementary DNA (cDNA) is much more stable than mRNA and so, once the double-stranded DNA has been produced it represents the expressed DNA sequence scientists look for.\n\n\nThis technique is used to identify which genes or genetic mutations produce a certain phenotype. A mutagen is very often used to accelerate this process. Once mutants have been isolated, the mutated genes can be molecularly identified.\n\nForward saturation genetics is a method for treating organisms with a mutagen, then screens the organism's offspring for particular phenotypes. This type of genetic screening is used to find and identify all the genes involved in a trait.\n\n\nA mutation in a gene can cause encoded proteins and the cells that rely on those proteins to malfunction. Conditions related to gene mutations are called genetic disorders. However, altering a patient's genes can sometimes be used to treat or cure a disease as well. Gene therapy can be used to replace a mutated gene with the correct copy of the gene, to inactivate or knockout the expression of a malfunctioning gene, or to introduce a foreign gene to the body to help fight disease. Major diseases that can be treated with gene therapy include viral infections, cancers, and inherited disorders, including immune system disorders.\n\nGene therapy delivers a copy of the missing, mutated, or desired gene via a modified virus or vector to the patient's target cells so that a functional form of the protein can then be produced and incorporated into the body. These vectors are often siRNA. Treatment can be either in vivo or ex vivo. The therapy has to be repeated several times for the infected patient to continually be relieved, as repeated cell division and cell death slowly randomizes the body's ratio of functional-to-mutant genes. Gene therapy is an appealing alternative to some drug-based approaches, because gene therapy repairs the underlying genetic defect using the patients own cells with minimal side effects. Gene therapies are still in development and mostly used in research settings. All experiments and products are controlled by the U.S. FDA and the NIH. \n\nClassical gene therapies usually require efficient transfer of cloned genes into the disease cells so that the introduced genes are expressed at sufficiently high levels to change the patient's physiology. There are several different physicochemical and biological methods that can be used to transfer genes into human cells. The size of the DNA fragments that can be transferred is very limited, and often the transferred gene is not a conventional gene. Horizontal gene transfer is the transfer of genetic material from one cell to another that is not its offspring. Artificial horizontal gene transfer is a form of genetic engineering.\n\nThe Human Genome Project is a molecular genetics project that began in the 1990s and was projected to take fifteen years to complete. However, because of technological advances the progress of the project was advanced and the project finished in 2003, taking only thirteen years. The project was started by the U.S. Department of Energy and the National Institutes of Health in an effort to reach six set goals. These goals included:\n\nThe project was worked on by eighteen different countries including the United States, Japan, France, Germany, and the United Kingdom. The collaborative effort resulted in the discovery of the many benefits of molecular genetics. Discoveries such as molecular medicine, new energy sources and environmental applications, DNA forensics, and livestock breeding, are only a few of the benefits that molecular genetics can provide.\n\n\n", "id": "339838", "title": "Molecular genetics"}
{"url": "https://en.wikipedia.org/wiki?curid=1138554", "text": "SR protein\n\nSR proteins are a conserved family of proteins involved in RNA splicing. SR proteins are named because they contain a protein domain with long repeats of serine and arginine amino acid residues, whose standard abbreviations are \"S\" and \"R\" respectively. SR proteins are ~200-600 amino acids in length and composed of two domains, the RNA recognition motif (RRM) region and the RS domain. SR proteins are more commonly found in the nucleus than the cytoplasm, but several SR proteins are known to shuttle between the nucleus and the cytoplasm.\n\nSR proteins were discovered in the 1990s in \"Drosophila\" and in amphibian oocytes, and later in humans. In general, metazoans appear to have SR proteins and unicellular organisms lack SR proteins.\n\nSR proteins are important in constitutive and alternative pre-mRNA splicing, mRNA export, genome stabilization, nonsense-mediated decay, and translation. SR proteins alternatively splice pre-mRNA by preferentially selecting different splice sites on the pre-mRNA strands to create multiple mRNA transcripts from one pre-mRNA transcript. Once splicing is complete the SR protein may or may not remain attached to help shuttle the mRNA strand out of the nucleus. As RNA Polymerase II is transcribing DNA into RNA, SR proteins attach to newly made pre-mRNA to prevent the pre-mRNA from binding to the coding DNA strand to increase genome stabilization. Topoisomerase I and SR proteins also interact to increase genome stabilization. SR proteins can control the concentrations of specific mRNA that is successfully translated into protein by selecting for nonsense-mediated decay codons during alternative splicing. SR proteins can alternatively splice NMD codons into its own mRNA transcript to auto-regulate the concentration of SR proteins. Through the mTOR pathway and interactions with polyribosomes, SR proteins can increase translation of mRNA.\n\nAtaxia telangiectasia, neurofibromatosis type 1, several cancers, HIV-1, and spinal muscular atrophy have all been linked to alternative splicing by SR proteins.\n\nSR proteins were discovered independently through the use of two different monoclonal antibodies. The first antibody, mAb104 found SR proteins in the nucleus of amphibian oocytes. The mAb104 antibody binds to a phosphoepitope on the C-terminal domain of SR proteins. mAb104 also binds to active sites of RNA polymerase II transcription. This antibody allowed identification of four SR proteins (SRp20, SRp40, SRp55 and SRp75) and demonstrated their conservation among vertebrates and invertebrates. The second antibody, B52 was used in \"Drosophila\". B52 is closely related to the splicing factor SF2/ASF and bound to both RNA and DNA in \"Drosophila\". The discovery of SR proteins in \"Drosophila\" revealed three SR proteins, SWAP (suppressor-of-white-apricot), Tra and Tra-2 (transformer and transformer-2 respectively).\n\nThe following is a list of 12 human genes encoding SR proteins involved in splicing:\n\nSR proteins are characterized by an RS domain and at least one RNA recognition motif (RRM). The RRM is typically located near the N-terminus. The RS domain is located near the C-terminal end of an SR protein. RS domains regulate protein-protein interactions of SR proteins. Based on sequence analysis, SR proteins are suspected to be intrinsically disordered proteins resulting in an unstructured RS domain. Eight unphosphorylated repeats of arginine and serine in the RS domain take a helical form with arginine on the outside to reduce charge and in a phosphorylated state, the eight repeats of arginine and serine form a 'claw' shape.\n\nSR proteins can have more than one RRM domain. The second RRM domain is called the RNA recognition motif homolog (RRMH). RRM domains are located near the N-terminus end of SR proteins. The RRM domain mediates the RNA interactions of the SR proteins by binding to exon splicing enhancer sequences. The RRMH usually has weaker interactions with RNA compared to the RRM domain. From NMR, the RRM domain of SRSF1, an SR protein, has a RNA binding fold structure. The RRM domain may also protect the phosphorylated RS domain, which suggests that the RS domain fits into the RRM domain.\n\nSR proteins can be found in both the cytosol and in nuclear speckles in the nucleus. SR proteins are mostly found in the nucleus. Localization depends on the phosphorylation of the RS domain of the SR protein. Phosphorylation of the RS domain causes the SR proteins to enter and remain in the nucleus. Partial dephosphorylation of the RS domain causes the SR proteins to leave the nucleus and SR proteins with unphosphorylated RS domains are found in the cytosol.\n\nSR proteins are located in two different types of nuclear speckles, interchromatin granule clusters and perichromatin fibrils. Interchromatin granule clusters are for the storage and reassembly of pre-mRNA splicing proteins. Perichromatin fibrils are areas of gene transcription and where SR proteins associate with RNA polymerase II for co-transcriptional splicing.\n\nTwo protein kinases are thought to play a role in the localization of SR proteins in the nucleus. SR protein kinase 1 (SRPK1) binds to and phosphorylates 10-12 serine residues on the N-terminal portion of the RS domain of SR proteins located in the cytosol. SR proteins can translocate into the nucleus after the serines are phosphorylated. The phosphorylated SR protein moves into the nucleus and relocates to a nuclear speckle. The second protein kinase, CLK1, then phosphorylates the remaining serines on the RS domain of the SR protein causing it to translocate out of the nuclear speckle and become associated with RNA polymerase II for co-transcriptional splicing of RNA.\n\nMovement of SR proteins out of the nucleus is controlled by a different mechanism. SR proteins that do not leave the nucleus are called nonshuttling SR proteins and those that do leave the nucleus are called shuttling SR proteins. SRp20 (SFRS3) and 9G8 (SFRS7) are two examples of mammalian shuttling SR proteins. Both recognize and bind poly-A RNA to transport RNA. Most SR proteins that do not shuttle out of the nucleus with an RNA transcript have nuclear retention signals. Shuttling SR proteins associate with the nuclear export factor TAP for export out of the nucleus. Methylation of arginine residues in the RRM may also contribute to the export of SR proteins out of the nucleus.\n\nSR proteins have been shown to have roles in alternative and constitutive splicing resulting in differential gene expression and also play a part in mRNA export, genome stabilization, non-sense mediated decay, and translation.\n\nThe first step for SR proteins to begin alternative splicing of an RNA transcript is for SR proteins to bind on to the carboxyl-terminal domain (CTD) of the largest subunit of RNA polymerase II. The CTD is made of the conserved repeating heptapeptide sequence YSPTSPS. Different steps of transcription have different levels of phosphorylation of the CTD of RNA polymerase II. Before initiation of transcription, the CTD has low levels of phosphorylation, but it is subsequently hyperphosphorylated during initiation and elongation. The RS domain of SR proteins interacts with the hyperphosphorylated CTD during elongation of transcription.\n\nRNA polymerase II moves from initiation to elongation once P-TEFb kinase phosphorylates Ser5 and Ser2 on RNA polymerase II. SR proteins interact with CDK9, the kinase component of P-TEFb leading to the phosphorylation of Ser2. SR proteins bind to the phosphorylated Ser2 on the CTD. The positioning of SR proteins on the RNA polymerase II allows the SR proteins to \"see\" the new RNA transcript first. SR proteins then moves from the RNA polymerase II to the pre-mRNA transcript.\n\nOnce on the new RNA transcript, SR proteins can then stimulate the formation of the spliceosome. SR proteins promote the binding of U1 snRNP and U2AF snRNP to the new RNA transcript to being the formation of the spliceosome. SR proteins also help U2 recognize and bind to the branch site of the intron that is to be excised. Later in spliceosome formation, SR proteins help recruit U4/U6 and U5 snRNPs.\n\nSR proteins are important for selecting splice sites for alternative splicing. SR proteins recognize intron and exon enhancers and silencers. SR proteins combine with SR-like proteins to select exon splicing enhancers on RNA transcripts causing U2 snRNP to bind to the upstream, adjacent branch site causing spliceosome assembly at the specific 3' site selected by the SR proteins.\nSR proteins' alternative splicing promoting activities are in contrast to those of hnRNPs. hnRNPs bind to exon splicing silencers, ESS, and inhibit the inclusion of exons, thus hnRNPs are splicing repressors. SR proteins and hnRNPs compete for binding to ESEs and ESSs sequences in exons. Binding is based on concentrations of SR proteins and hnRNPs in cells. If the cell has a high concentration of SR proteins then SR proteins are more likely to bind to ESEs compared to hnRNPs binding to ESS. If the cell has a high concentration of hnRNPs then hnRNPs can outcompete SR proteins for ESSs compared to ESEs.\n\nSR proteins may work in an antagonistic fashion, competing with each other to bind to exonic splicing enhancers. Some evidence suggests that selection of the mRNA splicing variant depends upon the relative ratios of SR proteins. SR proteins appear to be redundant. Experiments have shown that knocking down SR proteins with RNAi shows no detectable phenotype in \"C. elegans\". After knocking down one specific SR protein another different SR protein can make up for the lost function of the SR protein that was knocked down. Specific SR proteins' activities are important for specific tissues and developmental stages.\n\nSR proteins select alternative upstream 3' splice sites by recruiting U2AF and U2AF to specific ESE pyrimidine sequences in the exon of the pre-mRNA transcript.\n\nSR proteins can also alternatively select different downstream 5' splice sites by binding to ESE upstream of the splice site. The suspected mechanism is that alternative 5' splice sites are chosen when SR proteins bind to upstream ESE and interacts with U1-70K and together recruit U1 to the 5' splice site.\n\nIn constitutive splicing SR proteins bind to U2AF and U1-70K to bridge the gap between the two components of the spliceosome to mark the 3' and 5' splice sites. Constitutively spliced exons have many different SR protein binding sequences that act as constitutive splicing enhancers. The difference between alternative and constitutive splicing is that during alternative splicing the splice site choice is regulated.\n\nExon independent roles of SR proteins are called exon independent because it is not known if SR proteins must bind to exons in order for them to perform exon independent activities. SR proteins can bind to U1 and U2AF while they are bound to the 3' and 5' splice sites at the same time without binding to the pre-mRNA transcript. The SR protein thus creates a bridge across the intron in what is called a cross-intron interaction. SR proteins also recruit the tri-snRNP molecule U4/U6·U5 to the maturing spliceosome complex by interacting with RS domains in the tri-snRNP. SR proteins might be able to bind directly to the 5' splice site and recruit the U1 complex of the spliceosome.\n\nSR proteins can be either shuttling SR proteins or nonshuttling SR proteins. Some SR proteins associate with RNA export factor TAP, a nuclear export factor, to shuttle RNA out of the nucleus. The shuttling property of the SR protein is determined by the phosphorylation status of the RS domain. When hyperphosphorylated, SR proteins bind to pre-mRNA transcripts, but SR proteins become partially dephosphorylated during transcription allowing them to interact with NXF1. Thus the phosphorylation of the RS domain determines if the SR proteins stays with the RNA transcript after co-transcription splicing and while the mRNP matures. If the RS domain remains phosphorylated, then the SR protein will not shuttle from the nucleus to the cytosol. The phosphorylated SR protein will be sorted away from the mRNA transcript further preventing shuttling of the phosphorylated SR proteins. If the RS domain becomes partially dephosphorylated then the SR protein will shuttle out of the nucleus into the cytosol. The methylation and charge of arginine residues in the RRM domain also contributes to the export of SR proteins associated with mRNA.\n\nSR proteins can increase genome stability by preventing the formation of R loops in the DNA strand that is actively being transcribed during transcription. SR protein SC35 has the ability to bind to the largest subunit of RNA polymerase II at the phosphorylated C-terminal domain. Once RNA polymerase II begins making the new RNA strand, SR proteins move from the C-terminal domain of the RNA polymerase II to the new RNA stand. The movement of SR proteins from the RNA polymerase II to the new RNA strand prevents the new RNA strand, which is complementary to the template DNA strand, from binding to the template DNA strand thus preventing R loops.\n\nSR proteins can also stabilize DNA during transcription through an interaction with Topoisomerase I. When Topoisomerase I, Topo I, reduces supercoiling caused by transcription when it is bound to DNA. When Topo I is not bound to DNA it can phosphorylate the SR protein SF2/ASF. Topo I and SF2/ASF interact when SF2/ASF is hypophosphorylated during transcription elongation. SR proteins can become hypophosphorylated during elongation decreasing their affinity for RNA polymerase II causing SR proteins to move to Topo I. When Topo I complexes with SF2/ASF, it can no longer undo the supercoiling of DNA causing elongation to pause. Topo I phosphorylates S2F/ASF increasing the SR proteins affinity for RNA poly II moving S2F/ASF from the Topo I back to RNA poly II allowing elongation to continue.\n\nSR proteins can alternatively splice pre-mRNA transcripts to include nonsense-mediated decay (NMD) codons in the mRNA. The most common method of an NMD response in cells is alternative splicing. If a pre-mRNA transcript has a duplicated 5' splice site and SR proteins are over expressed then NMD can be upregulated. The splice variant with the NMD codon is chosen more often during splicing and the cell is more sensitive to NMD further down stream during translation. It is estimated that close to 30% of alternatively spliced mRNA are degraded by NMD. SR protein concentrations in cells can be auto-regulated by NMD codons in SR proteins pre-mRNA. For example, SC35 SR protein can alternatively splice a SC35 pre-mRNA to include a NMD codon in the mRNA. The location of SR protein binding on a pre-mRNA strand and which SR proteins are binding determine the NMD activity of a cell.\n\nSR proteins can indirectly and directly influence translation. SR proteins SF2/ASF alternatively splices the transcript of MNK2. MNK2 is a kinase that initiates translation. High levels of SF2/ASF produce an isoform of MNK2 that increases cap-dependent translation by promoting phosphorylation of MAPK-independent eIF4E. SF2/ASF recruits components of the mTOR pathway, specifically S6K1. SF2/ASF creates an oncogenic form of S6K1 to increase the prevalence of cap-dependent translation. SF2/ASF can also interact with polyribosomes to directly influence translation of mRNA into protein by recruiting component of the mTOR pathway. SF2/ASF increases the phosphorylation of rpS6 and eIF4B by S6K1. 9G8 increases the translation of unspliced mRNA with a constitutive transport sequence.\n\nGenetic diversity is increased by the alternative splicing activities of SR proteins, but splicing can also result in mutations in mRNA strands. Mutations in pre-mRNA can affect the correct splice site selection for SR proteins. Mutations in mRNA, because of nonsense-associated altered splicing by SR proteins, have been linked to ataxia telangiectasia, neurofibromatosis type 1, several cancers, HIV-1, and spinal muscular atrophy.\n\nSeveral SR proteins have been implicated in cancer. Elevated levels of SF2/ASF, SC35, and SRp20 have all been associated with breast and ovarian cancer development. SF2/ASF is also upregulated in lung, kidney, and liver tumors. SFRS1, the gene that codes for SF2/ASF, is a known proto-oncogene. Mutations in the ESE sequence of BRCA1 have been linked to irregular exon skipping because SF2/ASF cannot recognize the ESE.\n\nThree SR proteins have been implicated in HIV-1, SRp75, SF2/ASF, and SRp40. All three SR proteins are important for alternatively splicing the viral pre-mRNA. HIV can also change the concentrations of specific SR proteins in the cell. New drug treatments for HIV infections are looking to target specific SR proteins to prevent the virus from replicating in cells. One treatment works by blocking SR proteins from selecting 3' splice sites for an important HIV-1 regulatory protein.\n\nSpina muscular atrophy is caused by a transition from cytosine to thymine. The transition mutation results in exon 7 being skipped during splicing. The exon could be skipped for two reasons. The first is that the mutation prevents SF2/ASF from recognizing the correct ESE. The second is that the mutation creates an ESS for an hnRNP to bind and block splicing of the exon.\n\n", "id": "1138554", "title": "SR protein"}
{"url": "https://en.wikipedia.org/wiki?curid=9933420", "text": "DNA repair protein XRCC4\n\nDNA repair protein XRCC4 also known as X-ray repair cross-complementing protein 4 or XRCC4 is a protein that in humans is encoded by the XRCC4 gene. In addition to humans, the XRCC4 protein is also expressed in many other metazoans, fungi and in plants. The X-ray repair cross-complementing protein 4 is one of several core proteins involved in the non-homologous end joining (NHEJ) pathway to repair DNA double strand breaks (DSBs).\n\nNHEJ requires two main components to achieve successful completion. The first component is the cooperative binding and phosphorylation of artemis by the catalytic subunit of the DNA-dependent protein kinase (DNA-PKcs). Artemis cleaves the ends of damaged DNA to prepare it for ligation. The second component involves the bridging of DNA to DNA Ligase IV (LigIV), by XRCC4, with the aid of Cernunos-XLF. DNA-PKcs and XRCC4 are anchored to Ku70 / Ku80 heterodimer, which are bound to the DNA ends.\n\nSince XRCC4 is the key protein that enables interaction of LigIV to damaged DNA and therefore ligation of the ends, mutations in the XRCC4 gene were found to cause embryonic lethality in mice and developmental inhibition and immunodeficiency in humans. Furthermore, certain mutations in the XRCC4 gene are associated with an increased risk of cancer.\n\nDSBs are mainly caused by free radicals generated from ionizing radiation in the environment and from by-products released continually during cellular metabolism. DSBs that are not efficiently repaired may result in the loss of important protein coding genes and regulatory sequences required for gene expression necessary for the life of a cell. DSBs that cannot rely on a newly copied sister chromosome generated by DNA replication to fill in the gap will go into the NHEJ pathway. This method of repair is essential as it is a last resort to prevent loss of long stretches of the chromosome. NHEJ is also used to repair DSBs generated during V(D)J recombination when gene regions are rearranged to create the unique antigen binding sites of antibodies and T-cell receptors.\n\nDNA damage occurs very frequently and is generated from exposure to a variety of both exogenous and endogenous genotoxic sources. One of these include ionizing radiation, such as γ radiation and X-rays, which ionize the deoxyribose groups in the DNA backbone and can induce DSBs. Reactive oxygen species, ROS, such as superoxide (O), hydrogen peroxide (HO), hydroxyl radicals (HO), and singlet oxygen (O), can also produce DSBs as a result of ionizing radiation as well as cellular metabolic processes that are naturally occurring. DSBs can also be caused by the action of DNA polymerase while attempting to replicate DNA over a nick that was introduced as a result of DNA damage.\n\nThere are many types of DNA damage, but DSBs, in particular, are the most harmful as both strands are completely disjointed from the rest of the chromosome. If an efficient repair mechanism does not exist, the ends of the DNA can eventually degrade, leading to a permanent loss of sequence. A double-stranded gap in DNA will also prevent replication from proceeding, resulting in an incomplete copy of that specific chromosome, targeting the cell for apoptosis. As with all DNA damage, DSBs can introduce new mutations that can ultimately lead to cancer.\n\nThere are two methods for repairing DSBs depending on when the damage occurs during mitosis. If the DSB occurs after DNA replication has completed proceeding S phase of the cell cycle, the DSB repair pathway will use homologous recombination by pairing with the newly synthesized daughter strand to repair the break. However, if the DSB is generated prior to synthesis of the sister chromosome, then the template sequence that is required will be absent. For this circumstance, the NHEJ pathway provides a solution for repairing the break and is the main system used to repair DSBs in humans and multicellular eukaryotes. During NHEJ, very short stretches of complementary DNA, 1 bp or more at a time, are hybridized together, and the overhangs are removed. As a result, this specific region of the genome is permanently lost and the deletion can lead to cancer and premature aging.\n\nThe human XRCC4 gene is located on chromosome 5, specifically at 5q14.2. This gene contains eight exons and three mRNA transcript variants, which encode two different protein isoforms. Transcript variant 1, mRNA, RefSeq NM_003401.3, is 1688 bp long and is the shortest out of the three variants. It is missing a short sequence in the 3’ coding region as compared to variant 2. Isoform 1 contains 334 amino acids. Transcript variant 2, mRNA, RefSeq NM_022406, is 1694 bp long and encodes the longest isoform 2, which contains 336 amino acids. Transcript variant 3, RefSeq NM_022550.2, is 1735 bp and is the longest, but it also encodes for the same isoform 1 as variant 1. It contains an additional sequence in the 5’UTR of the mRNA transcript and lacks a short sequence in the 3’ coding region as compared to variant 2.\n\nXRCC4 protein is a tetramer that resembles the shape of a dumbbell containing two globular ends separated by a long, thin stalk. The tetramer is composed of two dimers, and each dimer is made up of two similar subunits. The first subunit (L) contains amino acid residues 1 – 203 and has a longer stalk than the second subunit (S) which contains residues 1 – 178.\n\nThe globular N-terminal domains of each subunit are identical. They are made up of two, antiparallel beta sheets that face each other in a beta sandwich-like structure (i.e., a \"flattened\" beta barrel) and are separated by two alpha helices on one side. The N-terminus begins with one beta sheet composed of strands 1, 2, 3, and 4, followed by a helix-turn-helix motif of the two alpha helices, αA and αB, which continues into strands 5, 6, 7, and ending with one alpha-helical stalk at the C-terminus. αA and αB are perpendicular to one another, and because one end of αB is partially inserted between the two beta sheets, it causes them to flare out away from each other. The beta sandwich structure is held together through three hydrogen bonds between antiparallel strands 4 and 7 and one hydrogen bond between strands 1 and 5.\n\nThe two helical stalks between subunits L and S intertwine with a single left-handed crossover into a coiled-coil at the top, near the globular domains forming a palm tree configuration. This region interacts with the two alpha helices of the second dimer in an opposite orientation to form a four-helix bundle and the dumbbell-shaped tetramer.\n\nIn order for XRCC4 to be sequestered from the cytoplasm to the nucleus to repair a DSB during NHEJ or to complete V(D)J recombination, post-translational modification at lysine 210 with a small ubiquitin-related modifier (SUMO), or sumoylation, is required. SUMO modification of diverse types of DNA repair proteins can be found in topoisomerases, base excision glycosylase TDG, Ku70/80, and BLM helicase. A common conserved motif is typically found to be a target of SUMO modification, ΨKXE (where Ψ is a bulky, hydrophobic amino acid). In the case of the XRCC4 protein, the consensus sequence surrounding lysine 210 is IKQE. Chinese hamster ovary cells, CHO, that express the mutated form of XRCC4 at K210 cannot be modified with SUMO, fail recruitment to the nucleus and instead accumulate in the cytoplasm. Furthermore, these cells are radiation sensitive and do not successfully complete V(D)J recombination.\n\nUpon generation of a DSB, Ku proteins will move through the cytoplasm until they find the site of the break and bind to it. Ku recruits XRCC4 and Cer-XLF and both of these proteins interact cooperatively with one another through specific residues to form a nucleoprotein pore complex that wraps around DNA. Cer-XLF is a homodimer that is very similar to XRCC4 in the structure and size of its N-terminal and C-terminal domains. Residues arginine 64, leucine 65, and leucine 115 in Cer-XLF interact with lysines 65 and 99 in XRCC4 within their N-terminal domains. Together they form a filament bundle that wraps around DNA in an alternating pattern. Hyper-phosphorylation of the C-terminal alpha helical domains of XRCC4 by DNA-PKcs facilitates this interaction. XRCC4 dimer binds to a second dimer on an adjacent DNA strand to create a tetramer for DNA bridging early on in NHEJ. Prior to ligation, Lig IV binds to the C-terminal stalk of XRCC4 at the site of the break and displaces the second XRCC4 dimer. The BRCT2 domain of Lig IV hydrogen bonds with XRCC4 at this domain through multiple residues and introduces a kink in the two alpha helical tails. The helix-loop-helix clamp connected to the BRCT-linker also makes extensive contacts.\n\nThe process of NHEJ involves XRCC4 and a number of tightly coupled proteins acting in concert to repair the DSB. The system begins with the binding of one heterodimeric protein called Ku70/80 to each end of the DSB to maintain them close together in preparation for ligation and prevent their degradation. Ku70/80 then sequesters one DNA-dependent protein kinase catalytic subunit (DNA-PKcs) to the DNA ends to enable the binding of Artemis protein to one end of each DNA-PKcs. One end of the DNA-PKcs joins to stabilize the proximity of the DSB and allow very short regions of DNA complementarity to hybridize. DNA-PKcs then phosphorylates Artemis at a serine/threonine to activate its exonuclease activity and cleave nucleotides at the single strand tails that are not hybridized in a 5’ to 3’ direction. Two XRCC4 proteins are post-translationally modified for recognition and localization to Ku70/80 (5). The two XRCC4 proteins dimerize together and bind to Ku70/80 at the ends of the DNA strands to promote ligation. XRCC4 then forms a strong complex with DNA ligase IV, LigIV, which is enhanced by Cernunnos XRCC4-like factor, Cer-XLF. Cer-XLF only binds to XRCC4 without direct interaction with LigIV. LigIV then joins the DNA ends by catalyzing a covalent phosphodiester bond.\n\nV(D)J recombination is the rearrangement of multiple, distinct gene segments in germ-line DNA to produce the unique protein domains of immune cells, B cells and T cells, that will specifically recognize foreign antigens such as viruses, bacteria, and pathogenic eukaryotes. B cells produce antibodies that are secreted into the bloodstream and T cells produce receptors that once translated are transported to the outer lipid bilayer of the cell. Antibodies are composed of two light and two heavy chains. The antigen binding site consists of two variable regions, VL and VH. The remainder of the antibody structure is made up of constant regions, CL, CH, CH2 and CH3. The Kappa locus in the mouse encodes an antibody light chain and contains approximately 300 gene segments for the variable region, V, four J segments than encode a short protein region, and one constant, C, segment. To produce a light chain with one unique type of VL, when B cells are differentiating, DNA is rearranged to incorporate a unique combination of the V and J segments. RNA splicing joins the recombined region with the C segment. The heavy chain gene also contain numerous diversity segments, D, and multiple constant segments, Cμ, Cδ, Cγ, Cε, Cα. Recombination occurs in a specific region of the gene that is located between two conserved sequence motifs called recombination signal sequences. Each motif is flanked by a 7 bp and 9 bp sequence that is separated by a 12 bp spacer, referred to as class 1, or a 23 bp spacer, referred to as class 2. A recombinase made up of RAG1 and RAG2 subunits always cleave between these two sites. The cleavage results in two hairpin structures for the V and J segments, respectively, and the non-coding region, are now separated from the V and J segments by a DSB. The hairpin coding region goes through the process of NHEJ where the closed end is cleaved and repaired. The non-coding region is circularized and degraded. Thus, NHEJ is also important in the development of the immune system via its role in V(D)J recombination.\n\nRecent studies have shown an association between XRCC4 and potential susceptibility to a variety of pathologies. The most frequently observed linkage is between XRCC4 mutations and susceptibility to cancers such as bladder cancer, breast cancer, and lymphomas. Studies have also pointed to a potential linkage between XRCC4 mutation and endometriosis. Autoimmunity is also being studied in this regard. Linkage between XRCC4 mutations and certain pathologies may provide a basis for diagnostic biomarkers and, eventually, potential development of new therapeutics.\n\nXRCC4 polymorphisms have been linked to a risk of susceptibility for cancers such as bladder cancer, breast cancer, prostate cancer, hepatocellular carcinoma, lymphomas, and multiple myeloma. With respect to bladder cancer, for example, the link between XRCC4 and risk of cancer susceptibility was based on hospital-based case-control histological studies of gene variants of both XRCC4 and XRCC3 and their possible association with risk for urothelial bladder cancer. The linkage with risk for urothelial bladder cancer susceptibility was shown for XRCC4, but not for XRCC3 With regard to breast cancer, the linkage with \"increased risk of breast cancer\" was based on an examination of functional polymorphisms of the XRCC4 gene carried out in connection with a meta-analysis of five case-control studies . There is also at least one hospital-based case-control histological study indicating that polymorphisms in XRCC4 may have an \"influence\" on prostate cancer susceptibility. Conditional (CD21-cre-mediated) deletion of the XRCC4 NHEJ gene in p53-deficient peripheral mouse B cells resulted in surface Ig-negative B-cell lymphomas, and these lymphomas often had a \"reciprocal chromosomal translocation\" fusing IgH to Myc (and also had \"large chromosomal deletions or translocations\" involving IgK or IgL, with IgL \"fusing\" to oncogenes or to IgH). XRCC4- and p53-deficient pro-B lymphomas \"routinely activate c-myc by gene amplification\"; and furthermore, it should be noted that XRCC4- and p53-deficient peripheral B-cell lymphomas \"routinely ectopically activate\" a single copy of c-myc. Indeed, in view of the observation by some that “DNA repair enzymes are correctives for DNA damage induced by carcinogens and anticancer drugs”, it should not be surprising that “SNPs in DNA repair genes may play an important part” in cancer susceptibility. In addition to the cancers identified above, XRCC4 polymorphisms have been identified as having a potential link to various additional cancers such as oral cancer, lung cancer, gastric cancer, and gliomas.\n\nDeclining ability to repair DNA double-strand breaks by NHEJ may be a significant factor in the aging process. Li et al. found that, in humans, the efficiency of NHEJ repair declines from age 16 to 75 years. Their study indicated that decreased expression of XRCC4 and other NHEJ proteins drives an age-associated decline in NHEJ efficiency and fidelity. They suggested that the age related decline in expression of XRCC4 may contribute to cellular senescence.\n\nBased on the findings that (1) several polypeptides in the NHEJ pathway are \"potential targets of autoantibodies\" and (2) \"one of the autoimmune epitopes in XRCC4 coincides with a sequence that is a nexus for radiation-induced regulatory events\", it has been suggested that exposure to DNA double-strand break-introducing agents \"may be one of the factors\" mediating autoimmune responses.\n\nThere has been speculation that \"XRCC4 codon 247*A and XRCC4 promoter -1394*T related genotypes and alleles . . . might be associated with higher endometriosis susceptibilities and pathogenesis\".\n\nIn view of the possible associations of XRCC4 polymorphisms with risk of cancer susceptibility (see discussion above), XRCC4 could be used as a biomarker for cancer screening, particularly with respect to prostate cancer, breast cancer, and bladder cancer. In fact, XRCC4 polymorphisms were specifically identified as having the potential to be novel useful markers for \"primary prevention and anticancer intervention\" in the case of urothelial bladder cancer.\n\nIn view of the role of XRCC4 in DNA double-strand break repair, the relationship between impaired XRCC4 function and the radiosensitization of tumor cells has been investigated. For instance, it has been reported that \"RNAi-mediated targeting of noncoding and coding sequences in DNA repair gene messages efficiently radiosensitizes human tumor cells\".\n\nThere has been discussion in the literature comcerning the potential role of XRCC4 in the development of novel therapeutics. For instance, Wu \"et al.\" have suggested that since the XRCC4 gene is \"critical in NHEJ\" and is \"positively associated with cancer susceptibility\", some XRCC4 SNPs such as G-1394T (rs6869366) \"may serve as a common SNP for detecting and predict[ing] various cancers (so far for breast, gastric and prostate cancers . . .)\"; and, although further investigation is needed, \"they may serve as candidate targets for personalized anticancer drugs\". The possibility of detecting endometriosis on this basis has also been mentioned, and this may also possibly lead to the eventual development of treatments. In evaluating further possibilites for anticancer treatments, Wu \"et al\". also commented on the importance of “co-treatments of DNA-damaging agents and radiation”. Specifically, Wu \"et al\". noted that the “balance between DNA damage and capacity of DNA repair mechanisms determines the final therapeutic outcome” and “the capacity of cancer cells to complete DNA repair mechanisms is important for therapeutic resistance and has a negative impact upon therapeutic efficacy”, and thus theorized that “[p]harmacological inhibition of recently detected targets of DNA repair with several small-molecule compounds . . . . has the potential to enhance the cytotoxicity of anticancer agents”.\n\nIn humans, mutations in the XRCC4 gene cause microcephalic primordial dwarfism, a phenotype characterized by marked microcephaly, facial dysmorphism, developmental delay and short stature. Although immunoglobulin junctional diversity is impaired, these individuals do not show a recognizable immunological phenotype. In contrast to individuals with a LIG4 mutation, pancytopenia resulting in bone marrow failure is not observed in individuals with XRCC4 deficiency. At the cellular level, disruption of XRCC4 induces hypersensitivity to agents that induce double-strand breaks, defective double-strand break repair and increased apoptosis after induction of DNA damage.\n\nAnti-XRCC4 antibodies include Alexa Fluor anti-XRCC4 mouse monoclonal antibody ab118008 (4H9), anti-XRCC4 rabbit polyclonal antibody ab157147 (N-terminal), and rabbit polyclonal anti-XRCC4 antibody ab145 (ChIP Grade) (all available from Abcam; Cambridge, MA, USA); phosphospecific antibodies to pS260 and pS318 in XRCC4, raised in sheep against the phosphopeptides: Ser260: SIISSLDVTD and Ser318: AENMSLETLR (phosphoserines underlined); and SAB2102728 (Sigma) anti-XRCC4 rabbit polyclonal antibody (available from Sigma-Aldrich; St. Louis, MO, USA). Antibodies to XRCC4 can have a variety of uses, including use in immunoassays to conduct research in areas such as DNA damage and repair, non-homologous end joining, transcription factors, epigenetics and nuclear signaling.\n\nResearch carried out in the 1980s revealed that a Chinese hamster ovary (CHO) cell mutant called XR-1 was \"extremely sensitive\" with regard to being killed by gamma rays during the G1 portion of the cell cycle but, in the same research studies, showed \"nearly normal resistance\" to gamma-ray damage during the late S phase; and in the course of this research, XR-1’s cell-cycle sensitivity was correlated with its inability to repair DNA double-strand breaks produced by ionizing radiation and restriction enzymes. In particular, in a study using somatic cell hybrids of XR-1 cells and human fibroblasts, Giaccia \"et al.\" (1989) showed that the XR-1 mutation was a recessive mutation; and in follow-up to this work, Giaccia \"et al.\" (1990) carried out further studies examining the XR-1 mutation (again using somatic cell hybrids formed between XR-1 and human fibroblasts) and were able to map the human complementing gene to chromosome 5 using chromosome-segregation analysis. Giaccia \"et al\", tentatively assigned this human gene the name “XRCC4” (an abbreviation of “X-ray-complementing Chinese hamster gene 4”) and determined that (a) the newly named XRCC4 gene biochemically restored the hamster defect to normal levels of resistance to gamma-ray radiation and bleomycin and (b) the XRCC4 gene restored the proficiency to repair DNA DSBs. Based on these findings, Giaccia \"et al.\" proposed that XRCC4 ― as a single gene― was responsible for the XR-1 phenotype.\n\n", "id": "9933420", "title": "DNA repair protein XRCC4"}
{"url": "https://en.wikipedia.org/wiki?curid=734256", "text": "Molecular modelling\n\nMolecular modelling encompasses all methods, theoretical and computational, used to model or mimic the behaviour of molecules. The methods are used in the fields of computational chemistry, drug design, computational biology and materials science to study molecular systems ranging from small chemical systems to large biological molecules and material assemblies. The simplest calculations can be performed by hand, but inevitably computers are required to perform molecular modelling of any reasonably sized system. The common feature of molecular modelling methods is the atomistic level description of the molecular systems. This may include treating atoms as the smallest individual unit (a molecular mechanics approach), or explicitly modelling electrons of each atom (a quantum chemistry approach).\n\nMolecular mechanics is one aspect of molecular modelling, as it involves the use of classical mechanics (Newtonian mechanics) to describe the physical basis behind the models. Molecular models typically describe atoms (nucleus and electrons collectively) as point charges with an associated mass. The interactions between neighbouring atoms are described by spring-like interactions (representing chemical bonds) and Van der Waals forces. The Lennard-Jones potential is commonly used to describe the latter. The electrostatic interactions are computed based on Coulomb's law. Atoms are assigned coordinates in Cartesian space or in internal coordinates, and can also be assigned velocities in dynamical simulations. The atomic velocities are related to the temperature of the system, a macroscopic quantity. The collective mathematical expression is termed a potential function and is related to the system internal energy (U), a thermodynamic quantity equal to the sum of potential and kinetic energies. Methods which minimize the potential energy are termed energy minimization methods (e.g., steepest descent and conjugate gradient), while methods that model the behaviour of the system with propagation of time are termed molecular dynamics.\n\nThis function, referred to as a potential function, computes the molecular potential energy as a sum of energy terms that describe the deviation of bond lengths, bond angles and torsion angles away from equilibrium values, plus terms for non-bonded pairs of atoms describing van der Waals and electrostatic interactions. The set of parameters consisting of equilibrium bond lengths, bond angles, partial charge values, force constants and van der Waals parameters are collectively termed a force field. Different implementations of molecular mechanics use different mathematical expressions and different parameters for the potential function. The common force fields in use today have been developed by using high level quantum calculations and/or fitting to experimental data. The method, termed energy minimization, is used to find positions of zero gradient for all atoms, in other words, a local energy minimum. Lower energy states are more stable and are commonly investigated because of their role in chemical and biological processes. A molecular dynamics simulation, on the other hand, computes the behaviour of a system as a function of time. It involves solving Newton's laws of motion, principally the second law, formula_3. Integration of Newton's laws of motion, using different integration algorithms, leads to atomic trajectories in space and time. The force on an atom is defined as the negative gradient of the potential energy function. The energy minimization method is useful to obtain a static picture for comparing between states of similar systems, while molecular dynamics provides information about the dynamic processes with the intrinsic inclusion of temperature effects.\n\nMolecules can be modelled either in vacuum, or in the presence of a solvent such as water. Simulations of systems in vacuum are referred to as \"gas-phase\" simulations, while those that include the presence of solvent molecules are referred to as \"explicit solvent\" simulations. In another type of simulation, the effect of solvent is estimated using an empirical mathematical expression; these are termed \"implicit solvation\" simulations.\n\nMost force fields are distance-dependent, making the most convenient expression for these Cartesian coordinates. Yet the comparatively rigid nature of bonds which occur between specific atoms, and in essence, defines what is meant by the designation \"molecule\", make an internal coordinate system the most logical representation. In some fields the IC representation (bond length, angle between bonds, and twist angle of the bond as shown in the figure) is termed the Z-matrix or torsion angle representation. Unfortunately, continuous motions in Cartesian space often require discontinuous angular branches in internal coordinates, making it relatively hard to work with force fields in the internal coordinate representation, and conversely a simple displacement of an atom in Cartesian space may not be a straight line trajectory due to the prohibitions of the interconnected bonds. Thus, it is very common for computational optimizing programs to flip back and forth between representations during their iterations. This can dominate the calculation time of the potential itself and in long chain molecules introduce cumulative numerical inaccuracy. While all conversion algorithms produce mathematically identical results, they differ in speed and numerical accuracy. Currently, the fastest and most accurate torsion to Cartesian conversion is the Natural Extension Reference Frame (NERF) method.\n\nMolecular modelling methods are now used routinely to investigate the structure, dynamics, surface properties, and thermodynamics of inorganic, biological, and polymeric systems. The types of biological activity that have been investigated using molecular modelling include protein folding, enzyme catalysis, protein stability, conformational changes associated with biomolecular function, and molecular recognition of proteins, DNA, and membrane complexes.\n\n\n", "id": "734256", "title": "Molecular modelling"}
{"url": "https://en.wikipedia.org/wiki?curid=102352", "text": "Polyacrylamide gel electrophoresis\n\nPolyacrylamide gel electrophoresis (PAGE), describes a technique widely used in biochemistry, forensics, genetics, molecular biology and biotechnology to separate biological macromolecules, usually proteins or nucleic acids, according to their electrophoretic mobility. Mobility is a function of the length, conformation and charge of the molecule. Polyacrylamide gel electrophoresis is a powerful tool used to analyze RNA samples. When polyacrylamide gel electrophoresis is denatured, it provides information on the sample composition of the RNA species.\nHydration of acrylonitrile results in formation of acrylamide molecule (CHNO). Acrylamide monomer is in a powder state before addition of water. Acrylamide is toxic to human nervous system, therefore all safety measures must be followed while working with this substance. Acrylamide is soluble in water and upon addition of water it polymerizes resulting in formation of polyacrylamide. It is useful to make polyacrylamide gel because size pore can be regulated. Upon increase of acrylamide concentration the pore size decrease after polymerization. Polyacrylamide gel with small pores helps to examine smaller molecules better since the small molecules can enter the pores and travel through the gel. Large molecules get trapped at the beginning of the concentrated gel since they are too big to enter the pores of the gel.\n\nAs with all forms of gel electrophoresis, molecules may be run in their native state, preserving the molecules' higher-order structure. This method is called native-PAGE. Alternatively, a chemical denaturant may be added to remove this structure and turn the molecule into an unstructured molecule whose mobility depends only on its length and mass-to-charge ratio. This procedure is called SDS-PAGE. Sodium dodecyl sulfate polyacrylamide gel electrophoresis (SDS-PAGE) is a method of separating molecules based on the difference of their molecular weight. At the pH at which gel electrophoresis is carried out the SDS molecules are negatively charged and bind to proteins in a set ratio, approximately one molecule of SDS for every 2 amino acids. In this way, the detergent provides all proteins with a uniform charge-to-mass ratio. By binding to the proteins the detergent destroys their secondary, tertiary and/or quaternary structure denaturing them and turning them into negatively charged linear poly peptide chains. When subjected to an electric field in PAGE, the negatively charged poly peptide chains travel toward the anode with different mobility. Their mobility, or the distance traveled by molecules, is inversely proportional to the logarithm of their molecular weight. By comparing the relative ratio of the distance traveled by each protein to the length of the gel (Rf) one can make conclusions about the relative molecular weight of the proteins, where the length of the gel is determined by the distance traveled by a small molecule like a tracking dye.\n\nFor nucleic acids, urea is the most commonly used denaturant. For proteins, sodium dodecyl sulfate (SDS) is an anionic detergent applied to protein samples to coat proteins in order to impart two negative charges (from every SDS molecule) to every two amino acids of the denatured protein. 2-Mercaptoethanol may also be used to disrupt the disulfide bonds found between the protein complexes, which helps further denature the protein. In most proteins, the binding of SDS to the polypeptide chain impart an even distribution of charge per unit mass, thereby resulting in a fractionation by approximate size during electrophoresis. Proteins that have a greater hydrophobic content – for instance, many membrane proteins, and those that interact with surfactants in their native environment – are intrinsically harder to treat accurately using this method, due to the greater variability in the ratio of bound SDS. Procedurally, using both Native and SDS-PAGE together can be used to purify and to separate the various subunits of the protein. Native-PAGE keeps the oligomeric form intact and will show a band on the gel that is representative of the level of activity. SDS-PAGE will denature and separate the oligomeric form into its monomers, showing bands that are representative of their molecular weights. These bands can be used to assess the purity of and identify the protein.\n\nSamples may be any material containing proteins or nucleic acids. These may be biologically derived, for example from prokaryotic or eukaryotic cells, tissues, viruses, environmental samples, or purified proteins. In the case of solid tissues or cells, these are often first broken down mechanically using a blender (for larger sample volumes), using a homogenizer (smaller volumes), by sonicator or by using cycling of high pressure, and a combination of biochemical and mechanical techniques – including various types of filtration and centrifugation – may be used to separate different cell compartments and organelles prior to electrophoresis. Synthetic biomolecules such as oligonucleotides may also be used as analytes.\n\nThe sample to analyze is optionally mixed with a chemical denaturant if so desired, usually SDS for proteins or urea for nucleic acids. SDS is an anionic detergent that denatures secondary and non–disulfide–linked tertiary structures, and additionally applies a negative charge to each protein in proportion to its mass. Urea breaks the hydrogen bonds between the base pairs of the nucleic acid, causing the constituent strands to anneal. Heating the samples to at least 60 °C further promotes denaturation.\n\nIn addition to SDS, proteins may optionally be briefly heated to near boiling in the presence of a reducing agent, such as dithiothreitol (DTT) or 2-mercaptoethanol (beta-mercaptoethanol/BME), which further denatures the proteins by reducing disulfide linkages, thus overcoming some forms of tertiary protein folding, and breaking up quaternary protein structure (oligomeric subunits). This is known as reducing SDS-PAGE.\n\nA tracking dye may be added to the solution. This typically has a higher electrophoretic mobility than the analytes to allow the experimenter to track the progress of the solution through the gel during the electrophoretic run.\n\nThe gels typically consist of acrylamide, bisacrylamide, the optional denaturant (SDS or urea), and a buffer with an adjusted pH. The solution may be degassed under a vacuum to prevent the formation of air bubbles during polymerization. Alternatively, butanol may be added to the resolving gel (for proteins) after it is poured, as butanol removes bubbles and makes the surface smooth.\n\nA source of free radicals and a stabilizer, such as ammonium persulfate and TEMED are added to initiate polymerization. The polymerization reaction creates a gel because of the added bisacrylamide, which can form cross-links between two acrylamide molecules. The ratio of bisacrylamide to acrylamide can be varied for special purposes, but is generally about 1 part in 35. The acrylamide concentration of the gel can also be varied, generally in the range from 5% to 25%. Lower percentage gels are better for resolving very high molecular weight molecules, while much higher percentages of acrylamide are needed to resolve smaller proteins. The average pore diameter of polyacrylamide gels is determined by the total concentration of acrylamides (% T with T = Total concentration of acrylamide and bisacrylamide) and the concentration of the cross-linker bisacrylamide (%C with C = bisacrylamide concentration).<ref name=\"DOI10.1016/S0021-9673(00)95641-3\">Reinhard Rüchel, Russell L. Steere, Eric F. Erbe: \"Transmission-electron microscopic observations of freeze-etched polyacrylamide gels.\" In: \"Journal of Chromatography A.\" 166, 1978, S. 563, .</ref> The pore size is reduced reciprocally to the %T. Concerning %C, a concentration of 5 % produces the smallest pores, since the influence of bisacrylamide on the pore size has a parabola-shape with a vertex at 5 %.\n\nGels are usually polymerized between two glass plates in a gel caster, with a comb inserted at the top to create the sample wells. After the gel is polymerized the comb can be removed and the gel is ready for electrophoresis.\n\nVarious buffer systems are used in PAGE depending on the nature of the sample and the experimental objective. The buffers used at the anode and cathode may be the same or different.\n\nAn electric field is applied across the gel, causing the negatively charged proteins or nucleic acids to migrate across the gel away from the negative electrode (which is the cathode being that this is an electrolytic rather than galvanic cell) and towards the positive electrode (the anode). Depending on their size, each biomolecule moves differently through the gel matrix: small molecules more easily fit through the pores in the gel, while larger ones have more difficulty. The gel is run usually for a few hours, though this depends on the voltage applied across the gel; migration occurs more quickly at higher voltages, but these results are typically less accurate than at those at lower voltages. After the set amount of time, the biomolecules have migrated different distances based on their size. Smaller biomolecules travel farther down the gel, while larger ones remain closer to the point of origin. Biomolecules may therefore be separated roughly according to size, which depends mainly on molecular weight under denaturing conditions, but also depends on higher-order conformation under native conditions. The gel mobility is defined as the rate of migration traveled with a voltage gradient of 1V/cm and has units of cm/sec/V. For analytical purposes, the relative mobility of biomolecules, \"R\", the ratio of the distance the molecule traveled on the gel to the total travel distance of a tracking dye is plotted versus the molecular weight of the molecule (or sometimes the log of MW, or rather the M, molecular radius). Such typically linear plots represent the standard markers or calibration curves that are widely used for the quantitative estimation of a variety of biomolecular sizes.\n\nCertain glycoproteins, however, behave anomalously on SDS gels. Additionally, the analysis of larger proteins ranging from 250,000 to 600,000 Da is also reported to be problematic due to the fact that such polypeptides move improperly in the normally used gel systems.\n\nFollowing electrophoresis, the gel may be stained (for proteins, most commonly with Coomassie Brilliant Blue R-250 or autoradiography; for nucleic acids, ethidium bromide; or for either, silver stain), allowing visualization of the separated proteins, or processed further (e.g. Western blot). After staining, different species biomolecules appear as distinct bands within the gel. It is common to run molecular weight size markers of known molecular weight in a separate lane in the gel to calibrate the gel and determine the approximate molecular mass of unknown biomolecules by comparing the distance traveled relative to the marker.\n\nFor proteins, SDS-PAGE is usually the first choice as an assay of purity due to its reliability and ease. The presence of SDS and the denaturing step make proteins separate, approximately based on size, but aberrant migration of some proteins may occur. Different proteins may also stain differently, which interferes with quantification by staining. PAGE may also be used as a preparative technique for the purification of proteins. For example, quantitative preparative native continuous polyacrylamide gel electrophoresis (QPNC-PAGE) is a method for separating native metalloproteins in complex biological matrices.\n\nPolyacrylamide gel (PAG) had been known as a potential embedding medium for sectioning tissues as early as 1964, and two independent groups employed PAG in electrophoresis in 1959. It possesses several electrophoretically desirable features that make it a versatile medium. It is a synthetic, thermo-stable, transparent, strong, chemically relatively inert gel, and can be prepared with a wide range of average pore sizes. The pore size of a gel and the reproducibility in gel pore size are determined by three factors, the total amount of acrylamide present (%T) (T = Total concentration of acrylamide and bisacrylamide monomer), the amount of cross-linker (%C) (C = bisacrylamide concentration), and the time of polymerization of acrylamide (cf. QPNC-PAGE). Pore size decreases with increasing %T; with cross-linking, 5%C gives the smallest pore size. Any increase or decrease in %C from 5% increases the pore size, as pore size with respect to %C is a parabolic function with vertex as 5%C. This appears to be because of non-homogeneous bundling of polymer strands within the gel. This gel material can also withstand high voltage gradients, is amenable to various staining and destaining procedures, and can be digested to extract separated fractions or dried for autoradiography and permanent recording.\n\nPolyacrylamide gels are composed of a stacking gel and separating gel. Stacking gels have a higher porosity relative to the separating gel, and allow for proteins to migrate in a concentrated area. Additionally, stacking gels usually have a pH of 6.8, since the neutral glycine molecules allow for faster protein mobility. Separating gels have a pH of 8.8, where the anionic glycine slows down the mobility of proteins. Separating gels allow for the separation of proteins and have a relatively lower porosity. Here, the proteins are separated based on size (in SDS-PAGE) and size/ charge (Native PAGE).\n\nChemical buffer stabilizes the pH value to the desired value within the gel itself and in the electrophoresis buffer. The choice of buffer also affects the electrophoretic mobility of the buffer counterions and thereby the resolution of the gel. The buffer should also be unreactive and not modify or react with most proteins. Different buffers may be used as cathode and anode buffers, respectively, depending on the application. Multiple pH values may be used within a single gel, for example in DISC electrophoresis. Common buffers in PAGE include Tris, Bis-Tris, or imidazole.\n\nCounterion balance the intrinsic charge of the buffer ion and also affect the electric field strength during electrophoresis. Highly charged and mobile ions are often avoided in SDS-PAGE cathode buffers, but may be included in the gel itself, where it migrates ahead of the protein. In applications such as DISC SDS-PAGE the pH values within the gel may vary to change the average charge of the counterions during the run to improve resolution. Popular counterions are glycine and tricine. Glycine has been used as the source of trailing ion or slow ion because its pKa is 9.69 and mobility of glycinate are such that the effective mobility can be set at a value below that of the slowest known proteins of net negative charge in the pH range. The minimum pH of this range is approximately 8.0.\n\nAcrylamide (CHNO; mW: 71.08) when dissolved in water, slow, spontaneous autopolymerization of acrylamide takes place, joining molecules together by head on tail fashion to form long single-chain polymers. The presence of a free radical-generating system greatly accelerates polymerization. This kind of reaction is known as vinyl addition polymerisation. A solution of these polymer chains becomes viscous but does not form a gel, because the chains simply slide over one another. Gel formation requires linking various chains together. Acrylamide is carcinogenic, a neurotoxin, and a reproductive toxin. It is also essential to store acrylamide in a cool dark and dry place to reduce autopolymerisation and hydrolysis.\n\nBisacrylamide (\"N\",\"N\"′-Methylenebisacrylamide) (CHNO; mW: 154.17) is the most frequently used cross linking agent for polyacrylamide gels. Chemically it can be thought of as two acrylamide molecules coupled head to head at their non-reactive ends. Bisacrylamide can crosslink two polyacrylamide chains to one another, thereby resulting in a gel.\n\nSodium dodecyl sulfate (SDS) (CHNaOS; mW: 288.38) (only used in denaturing protein gels) is a strong detergent agent used to denature native proteins to individual polypeptides. This denaturation, which is referred to as reconstructive denaturation, is not accomplished by the total linearization of the protein, but instead, through a conformational change to a combination of random coil and α helix secondary structures. When a protein mixture is heated to 100 °C in presence of SDS, the detergent wraps around the polypeptide backbone. It binds to polypeptides in a constant weight ratio of 1.4 g SDS/g of polypeptide. In this process, the intrinsic charges of polypeptides become negligible when compared to the negative charges contributed by SDS. Thus polypeptides after treatment become rod-like structures possessing a uniform charge density, that is same net negative charge per unit weight. The electrophoretic mobilities of these proteins is a linear function of the logarithms of their molecular weights. Without SDS, different proteins with similar molecular weights would migrate differently due to differences in mass-charge ratio, as each protein has an isoelectric point and molecular weight particular to its primary structure. This is known as native PAGE. Adding SDS solves this problem, as it binds to and unfolds the protein, giving a near uniform negative charge along the length of the polypeptide.\n\nUrea (CO(NH); mW: 60.06) is a chaotropic agent that increases the entropy of the system by interfering with intramolecular interactions mediated by non-covalent forces such as hydrogen bonds and van der Waals forces. Macromolecular structure is dependent on the net effect of these forces, therefore it follows that an increase in chaotropic solutes denatures macromolecules,\n\nAmmonium persulfate (APS) (NHSO; mW: 228.2) is a source of free radicals and is often used as an initiator for gel formation. An alternative source of free radicals is riboflavin, which generated free radicals in a photochemical reaction.\n\nTEMED (\"N\", \"N\", \"N\"′, \"N\"′-tetramethylethylenediamine) (CHN; mW: 116.21) stabilizes free radicals and improves polymerization. The rate of polymerisation and the properties of the resulting gel depend on the concentrations of free radicals. Increasing the amount of free radicals results in a decrease in the average polymer chain length, an increase in gel turbidity and a decrease in gel elasticity. Decreasing the amount shows the reverse effect. The lowest catalytic concentrations that allow polymerisation in a reasonable period of time should be used. APS and TEMED are typically used at approximately equimolar concentrations in the range of 1 to 10 mM.\n\nThe following chemicals and procedures are used for processing of the gel and the protein samples visualized in it.\n\nTracking dye; as proteins and nucleic acids are mostly colorless, their progress through the gel during electrophoresis cannot be easily followed. Anionic dyes of a known electrophoretic mobility are therefore usually included in the PAGE sample buffer. A very common tracking dye is Bromophenol blue (BPB, 3',3\",5',5\" tetrabromophenolsulfonphthalein). This dye is coloured at alkali and neutral pH and is a small negatively charged molecule that moves towards the anode. Being a highly mobile molecule it moves ahead of most proteins. As it reaches the anodic end of the electrophoresis medium electrophoresis is stopped. It can weakly bind to some proteins and impart a blue colour. Other common tracking dyes are xylene cyanol, which has lower mobility, and Orange G, which has a higher mobility.\n\nLoading aids; most PAGE systems are loaded from the top into wells within the gel. To ensure that the sample sinks to the bottom of the gel, sample buffer is supplemented with additives that increase the density of the sample. These additives should be non-ionic and non-reactive towards proteins to avoid interfering with electrophoresis. Common additives are glycerol and sucrose.\n\nCoomassie Brilliant Blue R-250 (CBB)(CHNNaOS; mW: 825.97) is the most popular protein stain. It is an anionic dye, which non-specifically binds to proteins. The structure of CBB is predominantly non-polar, and it is usually used in methanolic solution acidified with acetic acid. Proteins in the gel are fixed by acetic acid and simultaneously stained. The excess dye incorporated into the gel can be removed by destaining with the same solution without the dye. The proteins are detected as blue bands on a clear background. As SDS is also anionic, it may interfere with staining process. Therefore, large volume of staining solution is recommended, at least ten times the volume of the gel.\n\nEthidium bromide (EtBr) is a popular nucleic acid stain. EtBr allows one to easily visualize DNA or RNA on a gel as EtBr fluoresces an orange color under UV light. Ethidium bromide binds nucleic acid chains through the process of Intercalation. While Ethidium bromide is a popular stain it is important to exercise caution when using EtBr as it is a known carcinogen. Because of this fact, many researchers opt to use stains such as SYBR Green and SYBR Safe which are safer alternatives to EtBr. EtBr is used by simply adding it to the gel mixture. Once the gel has run, the gel may be viewed through the use of a photo-documentation system.\n\nSilver staining is used when more sensitive method for detection is needed, as classical Coomassie Brilliant Blue staining can usually detect a 50 ng protein band, Silver staining increases the sensitivity typically 10-100 fold more. This is based on the chemistry of photographic development. The proteins are fixed to the gel with a dilute methanol solution, then incubated with an acidic silver nitrate solution. Silver ions are reduced to their metallic form by formaldehyde at alkaline pH. An acidic solution, such as acetic acid stops development. Silver staining was introduced by Kerenyi and Gallyas as a sensitive procedure to detect trace amounts of proteins in gels. The technique has been extended to the study of other biological macromolecules that have been separated in a variety of supports. Many variables can influence the colour intensity and every protein has its own staining characteristics; clean glassware, pure reagents and water of highest purity are the key points to successful staining. Silver staining was developed in the 14th century for colouring the surface of glass. It has been used extensively for this purpose since the 16th century. The colour produced by the early silver stains ranged between light yellow and an orange-red. Camillo Golgi perfected the silver staining for the study of the nervous system. Golgi's method stains a limited number of cells at random in their entirety.\n\nAutoradiography, also used for protein band detection post gel electrophoresis, uses radioactive isotopes to label proteins, which are then detected by using X-ray film.\n\nWestern blotting is a process by which proteins separated in the acrylamide gel are electrophoretically transferred to a stable, manipulable membrane such as a nitrocellulose, nylon, or PVDF membrane. It is then possible to apply immunochemical techniques to visualise the transferred proteins, as well as accurately identify relative increases or decreases of the protein of interest.\n\n\n", "id": "102352", "title": "Polyacrylamide gel electrophoresis"}
{"url": "https://en.wikipedia.org/wiki?curid=39900181", "text": "Light-oxygen-voltage-sensing domain\n\nA Light-oxygen-voltage-sensing domain (LOV domain) is a protein sensor used by a large variety of higher plants, microalgae, fungi and bacteria to sense environmental conditions. In higher plants, they are used to control phototropism, chloroplast relocation, and stomatal opening, whereas in fungal organisms, they are used for adjusting the circadian temporal organization of the cells to the daily and seasonal periods.\n\nCommon to all LOV proteins is the blue-light sensitive flavin chromophore, which in the signaling state is covalently linked to the protein core via an adjacent cysteine residue. LOV domains are e.g. encountered in phototropins, which are blue-light-sensitive protein complexes regulating a great diversity of biological processes in higher plants as well as in micro-algae. Phototropins are composed of two LOV domains, each containing a non-covalently bound flavin mononucleotide (FMN) chromophore in its dark-state form, and a C-terminal Ser-Thr kinase. \n\nUpon blue-light absorption, a covalent bond between the FMN chromophore and an adjacent reactive cysteine residue of the apo-protein is formed in the LOV2 domain. This subsequently mediates the activation of the kinase, which induces a signal in the organism through phototropin autophosphorylation. \n\nWhile the photochemical reactivity of the LOV2 domain has been found to be essential for the activation of the kinase, the in vivo functionality of the LOV1 domain within the protein complex still remains unclear. \n\nIn case of the fungus \"Neurospora crassa\", the circadian clock is controlled by two light-sensitive domains, known as the white-collar-complex (WCC) and the LOV domain vivid (VVD-LOV). WCC is primarily responsible for the light-induced transcription on the control-gene frequency (FRQ) under day-light conditions, which drives the expression of VVD-LOV and governs the negative feedback loop onto the circadian clock. By contrast, the role of VVD-LOV is mainly modulatory and does not directly affect FRQ. \n\nLOV domains have been found to control gene expression through DNA binding and\nto be involved in redox-dependent regulation, like e.g. in the bacterium \"Rhodobacter sphaeroides\". Notably, LOV-based optogenetic tools have been gaining wide popularity in recent years to control a myriad of cellular events, including cell motility, subcellular organelle distribution, formation of membrane contact sites, and protein degradation.\n\n", "id": "39900181", "title": "Light-oxygen-voltage-sensing domain"}
{"url": "https://en.wikipedia.org/wiki?curid=39993849", "text": "BIOPAN\n\nBIOPAN is a multi-user research program by the European Space Agency (ESA) designed to investigate the effect of the space environment on biological material. The experiments in BIOPAN are exposed to solar and cosmic radiation, the space vacuum and weightlessness, or a selection thereof. Optionally, the experiment temperature can be stabilized. BIOPAN hosts astrobiology, radiobiology and materials science experiments.\n\nThe BIOPAN facility is installed on the external surface of Russian Foton descent capsules protruding from the thermal blanket that envelops the satellite.\n\nThe BIOPAN program started in the early nineties with an ESA contract for the a joint development by Kayser-Threde and Kayser Italia. It was based on the heritage of a low-tech Russian exposure container called KNA (\"Kontejner Nauchnoj Apparatury\"). The BIOPAN facilities are installed on the external surface of Foton descent capsules. It has a motor-driven hinged lid, which opens 180° in Earth orbit to expose the experiment samples to the harsh space environment. For re-entry, the closed facility is protected with an Ablative heat shield.\n\nThe BIOPAN facilities are equipped with thermometers, UV sensors, a radiometer, a pressure sensor and an active radiation dosimeter. Data acquired by the sensors is stored by BIOPAN throughout each mission and can be accessed after flight. The possibility of overheating during atmospheric re-entry was acknowledged early during the development, therefore, a quite massive heat shield was designed for it. While the total weight of BIOPAN is close to 27 kg, including the experiments, the heat shield is responsible for 12 kg of that figure.\n\nThe BIOPAN electronics consists of the following units: signal acquisition board, microcontroller board with its flight software, memory board and EGSE.\n\nThe missions flown so far are:\n\n", "id": "39993849", "title": "BIOPAN"}
{"url": "https://en.wikipedia.org/wiki?curid=21647820", "text": "Whole genome sequencing\n\nWhole genome sequencing (also known as WGS, full genome sequencing, complete genome sequencing, or entire genome sequencing) is the process of determining the complete DNA sequence of an organism's genome at a single time. This entails sequencing all of an organism's chromosomal DNA as well as DNA contained in the mitochondria and, for plants, in the chloroplast.\n\nWhole genome sequencing has largely been used as a research tool, but is currently being introduced to clinics. In the future of personalized medicine, whole genome sequence data will be an important tool to guide therapeutic intervention. The tool of gene sequencing at SNP level is also used to pinpoint functional variants from association studies and improve the knowledge available to researchers interested in evolutionary biology, and hence may lay the foundation for predicting disease susceptibility and drug response.\n\nWhole genome sequencing should not be confused with DNA profiling, which only determines the likelihood that genetic material came from a particular individual or group, and does not contain additional information on genetic relationships, origin or susceptibility to specific diseases. In addition, whole genome sequencing should not be confused with methods that sequence specific subsets of the genome - such methods include whole exome sequencing (1% of the genome) or SNP genotyping (<0.1% of the genome). \n\nAs of 2017 there were no complete genomes for any mammals, including humans. Between 4% to 9% of the human genome, mostly satellite DNA, had not been sequenced.\n\nThe DNA sequencing methods used in the 1970s and 1980s were manual, for example Maxam-Gilbert sequencing and Sanger sequencing. The shift to more rapid, automated sequencing methods in the 1990s finally allowed for sequencing of whole genomes.\n\nThe first organism to have its entire genome sequenced was \"Haemophilus influenzae\" in 1995. After it, the genomes of other bacteria and some archaea were first sequenced, largely due to their small genome size. \"H. influenzae\" has a genome of 1,830,140 base pairs of DNA. In contrast, eukaryotes, both unicellular and multicellular such as \"Amoeba dubia\" and humans (\"Homo sapiens\") respectively, have much larger genomes (see C-value paradox). \"Amoeba dubia\" has a genome of 700 billion nucleotide pairs spread across thousands of chromosomes. Humans contain fewer nucleotide pairs (about 3.2 billion in each germ cell - note the exact size of the human genome is still being revised) than \"A. dubia\" however their genome size far outweighs the genome size of individual bacteria.\n\nThe first bacterial and archaeal genomes, including that of \"H. influenzae\", were sequenced by Shotgun sequencing. In 1996 the first eukaryotic genome (\"Saccharomyces cerevisiae\") was sequenced. \"S. cerevisiae\", a model organism in biology has a genome of only around 12 million nucleotide pairs, and was the first \"unicellular\" eukaryote to have its whole genome sequenced. The first \"multicellular\" eukaryote, and animal, to have its whole genome sequenced was the nematode worm: \"Caenorhabditis elegans\" in 1998. Eukaryotic genomes are sequenced by several methods including Shotgun sequencing of short DNA fragments and sequencing of larger DNA clones from DNA libraries such as bacterial artificial chromosomes (BACs) and yeast artificial chromosomes (YACs).\n\nIn 1999, the entire DNA sequence of human chromosome 22, the shortest human autosome, was published. By the year 2000, the second animal and second invertebrate (yet first insect) genome was sequenced - that of the fruit fly \"Drosophila melanogaster\" - a popular choice of model organism in experimental research. The first plant genome - that of the model organism \"Arabidopsis thaliana\" - was also fully sequenced by 2000. By 2001, a draft of the entire human genome sequence was published. The genome of the laboratory mouse \"Mus musculus\" was completed in 2002.\n\nIn 2004, the Human Genome Project published an incomplete version of the human genome.\n\nCurrently thousands of genomes have been wholly or partially sequenced.\n\nAlmost any biological sample containing a full copy of the DNA—even a very small amount of DNA or ancient DNA—can provide the genetic material necessary for full genome sequencing. Such samples may include saliva, epithelial cells, bone marrow, hair (as long as the hair contains a hair follicle), seeds, plant leaves, or anything else that has DNA-containing cells.\nThe genome sequence of a single cell selected from a mixed population of cells can be determined using techniques of \"single cell genome sequencing\". This has important advantages in environmental microbiology in cases where a single cell of a particular microorganism species can be isolated from a mixed population by microscopy on the basis of its morphological or other distinguishing characteristics. In such cases the normally necessary steps of isolation and growth of the organism in culture may be omitted, thus allowing the sequencing of a much greater spectrum of organism genomes.\n\nSingle cell genome sequencing is being tested as a method of preimplantation genetic diagnosis, wherein a cell from the embryo created by in vitro fertilization is taken and analyzed before embryo transfer into the uterus. After implantation, cell-free fetal DNA can be taken by simple venipuncture from the mother and used for whole genome sequencing of the fetus.\n\nSequencing of nearly an entire human genome was first accomplished in 2000 partly through the use of shotgun sequencing technology. While full genome shotgun sequencing for small (4000–7000 base pair) genomes was already in use in 1979, broader application benefited from pairwise end sequencing, known colloquially as \"double-barrel shotgun sequencing\". As sequencing projects began to take on longer and more complicated genomes, multiple groups began to realize that useful information could be obtained by sequencing both ends of a fragment of DNA. Although sequencing both ends of the same fragment and keeping track of the paired data was more cumbersome than sequencing a single end of two distinct fragments, the knowledge that the two sequences were oriented in opposite directions and were about the length of a fragment apart from each other was valuable in reconstructing the sequence of the original target fragment.\n\nThe first published description of the use of paired ends was in 1990 as part of the sequencing of the human HPRT locus, although the use of paired ends was limited to closing gaps after the application of a traditional shotgun sequencing approach. The first theoretical description of a pure pairwise end sequencing strategy, assuming fragments of constant length, was in 1991. In 1995 the innovation of using fragments of varying sizes was introduced, and demonstrated that a pure pairwise end-sequencing strategy would be possible on large targets. The strategy was subsequently adopted by The Institute for Genomic Research (TIGR) to sequence the entire genome of the bacterium \"Haemophilus influenzae\" in 1995, and then by Celera Genomics to sequence the entire fruit fly genome in 2000, and subsequently the entire human genome. Applied Biosystems, now called Life Technologies, manufactured the automated capillary sequencers utilized by both Celera Genomics and The Human Genome Project.\n\nWhile capillary sequencing was the first approach to successfully sequence a nearly full human genome, it is still too expensive and takes too long for commercial purposes. Since 2005 capillary sequencing has been progressively displaced by high-throughput (formerly \"next-generation\") sequencing technologies such as Illumina dye sequencing, pyrosequencing, and SMRT sequencing. All of these technologies continue to employ the basic shotgun strategy, namely, parallelization and template generation via genome fragmentation.\n\nOther technologies are emerging, including nanopore technology. Though nanopore sequencing technology is still being refined, its portability and potential capability of generating long reads are of relevance to whole-genome sequencing applications.\n\nIn principle, full genome sequencing can provide the raw nucleotide sequence of an individual organism's DNA. However, further analysis must be performed to provide the biological or medical meaning of this sequence, such as how this knowledge can be used to help prevent disease. Methods for analysing sequencing data are being developed and refined.\n\nBecause sequencing generates a lot of data (for example, there are approximately six billion base pairs in each human diploid genome), its output is stored electronically and requires a large amount of computing power and storage capacity.\n\nWhile analysis of WGS data can be slow, it is possible to speed up this step by using dedicated hardware.\n\nA number of public and private companies are competing to develop a full genome sequencing platform that is commercially robust for both research and clinical use, including\nIllumina,\nKnome,\nSequenom,\n454 Life Sciences,\nPacific Biosciences,\nComplete Genomics,\nHelicos Biosciences,\nGE Global Research (General Electric),\nAffymetrix,\nIBM,\nIntelligent Bio-Systems,\nLife Technologies and\nOxford Nanopore Technologies.\nThese companies are heavily financed and backed by venture capitalists, hedge funds, and investment banks.\n\nA commonly-referenced commercial target for sequencing cost is the $1,000 genome.\n\nIn October 2006, the X Prize Foundation, working in collaboration with the J. Craig Venter Science Foundation, established the Archon X Prize for Genomics, intending to award $10 million to \"the first team that can build a device and use it to sequence 100 human genomes within 10 days or less, with an accuracy of no more than one error in every 1,000,000 bases sequenced, with sequences accurately covering at least 98% of the genome, and at a recurring cost of no more than $1,000 per genome\".\n\nThe Archon X Prize for Genomics was cancelled in 2013, before its official start date.\n\nIn 2007, Applied Biosystems started selling a new type of sequencer called SOLiD System. The technology allowed users to sequence 60 gigabases per run.\n\nIn June 2009, Illumina announced that they were launching their own Personal Full Genome Sequencing Service at a depth of 30× for $48,000 per genome.\n\nIn August 2009, the founder of Helicos Biosciences, Stephen Quake, stated that using the company's Single Molecule Sequencer he sequenced his own full genome for less than $50,000.\n\nIn November 2009, Complete Genomics published a peer-reviewed paper in \"Science\" demonstrating its ability to sequence a complete human genome for $1,700.\n\nIn May 2011, Illumina lowered its Full Genome Sequencing service to $5,000 per human genome, or $4,000 if ordering 50 or more.\nHelicos Biosciences, Pacific Biosciences, Complete Genomics, Illumina, Sequenom, ION Torrent Systems, Halcyon Molecular, NABsys, IBM, and GE Global appear to all be going head to head in the race to commercialize full genome sequencing.\n\nWith sequencing costs declining, a number of companies began claiming that their equipment would soon achieve the $1,000 genome: these companies included Life Technologies in January 2012, Oxford Nanopore Technologies in February 2012 and Illumina in February 2014.\n\nHowever, as of 2015, the NHGRI estimates the cost of obtaining a whole-genome sequence at around $1,500.\n\nFull genome sequencing provides information on a genome that is orders of magnitude larger than by DNA arrays, the previous leader in genotyping technology.\n\nFor humans, DNA arrays currently provide genotypic information on up to one million genetic variants, while full genome sequencing will provide information on all six billion bases in the human genome, or 3,000 times more data. Because of this, full genome sequencing is considered a disruptive innovation to the DNA array markets as the accuracy of both range from 99.98% to 99.999% (in non-repetitive DNA regions) and their consumables cost of $5000 per 6 billion base pairs is competitive (for some applications) with DNA arrays ($500 per 1 million basepairs).\n\nWhole genome sequencing has established the mutation frequency for whole human genomes. The mutation frequency in the whole genome between generations for humans (parent to child) is about 70 new mutations per generation. An even lower level of variation was found comparing whole genome sequencing in blood cells for a pair of monozygotic (identical twins) 100-year-old centenarians. Only 8 somatic differences were found, though somatic variation occurring in less than 20% of blood cells would be undetected.\n\nIn the specifically protein coding regions of the human genome, it is estimated that there are about 0.35 mutations that would change the protein sequence between parent/child generations (less than one mutated protein per generation).\n\nIn cancer, mutation frequencies are much higher, due to genome instability. This frequency can further depend on patient age, exposure to DNA damaging agents (such as UV-irradiation or components of tobacco smoke) and the activity/inactivity of DNA repair mechanisms. Furthermore, mutation frequency can vary between cancer types: in germline cells, mutation rates occur at approximately 0.023 mutations per megabase, but this number is much higher in breast cancer (1.18-1.66 somatic mutations per Mb), in lung cancer (17.7) or in melanomas (~33).\n\nThe distribution of somatic mutations across the human genome is very uneven, such that the gene-rich, early-replicating regions receive fewer mutations than gene-poor, late-replicating heterochromatin, likely due to differential DNA repair activity. In particular, the histone modification H3K9me3 is associated with high, and H3K36me3 with low mutation frequencies.\n\nIn research, whole-genome sequencing can be used in a Genome-Wide Association Study (GWAS) - a project aiming to determine the genetic variant or variants associated with a disease or some other phenotype.\n\nIn 2009, Illumina released its first whole genome sequencers that were approved for clinical as opposed to research-only use and doctors at academic medical centers began quietly using them to try to diagnose what was wrong with people whom standard approaches had failed to help. The price to sequence a genome at that time was US$19,500, which was billed to the patient but usually paid for out of a research grant; one person at that time had applied for reimbursement from their insurance company. For example, one child had needed around 100 surgeries by the time he was three years old, and his doctor turned to whole genome sequencing to determine the problem; it took a team of around 30 people that included 12 bioinformatics experts, three sequencing technicians, five physicians, two genetic counsellors and two ethicists to identify a rare mutation in the XIAP that was causing widespread problems.\n\nCurrently available newborn screening for childhood diseases allows detection of rare disorders that can be prevented or better treated by early detection and intervention. Specific genetic tests are also available to determine an etiology when a child's symptoms appear to have a genetic basis. Full genome sequencing, in addition has the potential to reveal a large amount of information (such as carrier status for autosomal recessive disorders, genetic risk factors for complex adult-onset diseases, and other predictive medical and non-medical information) that is currently not completely understood, may not be clinically useful to the child during childhood, and may not necessarily be wanted by the individual upon reaching adulthood.\n\nDue to recent cost reductions (see above) whole genome sequencing has become a realistic application in DNA diagnostics. In 2013, the 3Gb-TEST consortium obtained funding from the European Union to prepare the health care system for these innovations in DNA diagnostics. Quality assessment schemes, Health technology assessment and guidelines have to be in place. The 3Gb-TEST consortium has identified the analysis and interpretation of sequence data as the most complicated step in the diagnostic process. At the Consortium meeting in Athens in September 2014, the Consortium coined the word \"genotranslation\" for this crucial step. This step leads to a so-called \"genoreport\". Guidelines are needed to determine the required content of these reports.\n\nGenomes2People (G2P), an initiative of Brigham and Women's Hospital and Harvard Medical School was created in 2011 to examine the integration of genomic sequencing into clinical care of adults and children. G2P's director, Robert C. Green, had previously led the REVEAL study — Risk Evaluation and Education for Alzheimer’s Disease – a series of clinical trials exploring patient reactions to the knowledge of their genetic risk for Alzheimer’s.\n\nThe introduction of whole genome sequencing may have ethical implications. On one hand, genetic testing can potentially diagnose preventable diseases, both in the individual undergoing genetic testing and in their relatives. On the other hand, genetic testing has potential downsides such as genetic discrimination, loss of anonymity, and psychological impacts such as discovery of non-paternity.\n\nSome ethicists insist that the privacy of individuals undergoing genetic testing must be protected. Indeed, privacy issues can be of particular concern when minors undergo genetic testing. Illumina's CEO, Jay Flatley, claimed in February 2009 that \"by 2019 it will have become routine to map infants' genes when they are born\". This potential use of genome sequencing is highly controversial, as it runs counter to established ethical norms for predictive genetic testing of asymptomatic minors that have been well established in the fields of medical genetics and genetic counseling. The traditional guidelines for genetic testing have been developed over the course of several decades since it first became possible to test for genetic markers associated with disease, prior to the advent of cost-effective, comprehensive genetic screening.\n\nWhen an individual undergoes whole genome sequencing, they reveal information about not only their own DNA sequences, but also about probable DNA sequences of their close genetic relatives. This information can further reveal useful predictive information about relatives' present and future health risks. Hence, there are important questions about what obligations, if any, are owed to the family members of the individuals who are undergoing genetic testing. In Western/European society, tested individuals are usually encouraged to share important information on any genetic diagnoses with their close relatives, since the importance of the genetic diagnosis for offspring and other close relatives is usually one of the reasons for seeking a genetic testing in the first place. Nevertheless, a major ethical dilemma can develop when the patients refuse to share information on a diagnosis that is made for serious genetic disorder that is highly preventable and where there is a high risk to relatives carrying the same disease mutation. Under such circumstances, the clinician may suspect that the relatives would rather know of the diagnosis and hence the clinician can face a conflict of interest with respect to patient-doctor confidentiality.\n\nPrivacy concerns can also arise when whole genome sequencing is used in scientific research studies. Researchers often need to put information on patient's genotypes and phenotypes into public scientific databases, such as locus specific databases. Although only anonymous patient data are submitted to locus specific databases, patients might still be identifiable by their relatives in the case of finding a rare disease or a rare missense mutation.\n\nThe first nearly complete human genomes sequenced were two Americans of predominantly Northwestern European ancestry in 2007 (J. Craig Venter at 7.5-fold coverage, and James Watson at 7.4-fold). This was followed in 2008 by sequencing of an anonymous Han Chinese man (at 36-fold), a Yoruban man from Nigeria (at 30-fold), and a female caucasian Leukemia patient (at 33 and 14-fold coverage for tumor and normal tissues). Steve Jobs was among the first 20 people to have their whole genome sequenced, reportedly for the cost of $100,000. , there were 69 nearly complete human genomes publicly available. In November 2013, a Spanish family made their personal genomics data publicly available under a Creative Commons public domain license. The work was led by Manuel Corpas and the data obtained by direct-to-consumer genetic testing with 23andMe and the Beijing Genomics Institute). This is believed to be the first such public genomics dataset for a whole family.\n\n\n", "id": "21647820", "title": "Whole genome sequencing"}
{"url": "https://en.wikipedia.org/wiki?curid=24950378", "text": "Cell–cell interaction\n\nCell–cell interaction refers to the direct interactions between cell surfaces that play a crucial role in the development and function of multicellular organisms.\nThese interactions allow cells to communicate with each other in response to changes in their microenvironment. This ability to send and receive signals is essential for the survival of the cell. Interactions between cells can be stable such as those made through cell junctions. These junctions are involved in the communication and organization of cells within a particular tissue. Others are transient or temporary such as those between cells of the immune system or the interactions involved in tissue inflammation. These types of intercellular interactions are distinguished from other types such as those between cells and the extracellular matrix. The loss of communication between cells can result in uncontrollable cell growth and cancer.\n\nStable cell-cell interactions are required for cell adhesion within a tissue and controlling the shape and function of cells. These stable interactions involve cell junctions which are multiprotein complexes that provide contact between neighboring cells. Cell junctions allow for the preservation and proper functioning of epithelial cell sheets. These junctions are also important in the organization of tissues where cells of one type can only adhere to cells of the same tissue rather than to a different tissue.\n\nTight junctions are multi-protein complexes that hold cells of a same tissue together and prevent movement of water and water-soluble molecules between cells. In epithelial cells, they function also to separate the extracellular fluid surrounding their apical and basolateral membranes. These junctions exist as a continuous band located just below the apical surface between the membranes of neighboring epithelial cells. The tight junctions on adjacent cells line up so as to produce a seal between different tissues and body cavities. For example, the apical surface of gastrointestinal epithelial cells serve as a selective permeable barrier that separates the external environment from the body. The permeability of these junctions is dependent on a variety of factors including protein makeup of that junction, tissue type and signaling from the cells.\n\nTight junctions are made up of many different proteins. The four main transmembrane proteins are occludin, claudin, junctional adhesion molecules (JAMs) and tricellulins. The extracellular domains of these proteins form the tight junction barrier by making homophilic (between proteins of the same kind) and heterophilic interactions (between different types of proteins) with the protein domains on adjacent cells. Their cytoplasmic domains interact with the cell cytoskeleton to anchor them.\n\nOf the three types of anchoring junctions, only two are involved in cell-cell interactions: adherens junctions and desmosomes. Both are found in many types of cells. Adjacent epithelial cells are connected by adherens junctions on their lateral membranes. They are located just below tight junctions. Their function is to give shape and tension to cells and tissues and they are also the site of cell-cell signaling. Adherens junctions are made of cell adhesion molecules from the cadherin family. There are over 100 types of cadherins, corresponding to the many different types of cells and tissues with varying anchoring needs. The most common are E-, N- and P-cadherins. In the adherens junctions of epithelial cells, E-cadherin is the most abundant.\n\nDesmosomes also provide strength and durability to cells and tissues and are located just below adherens junctions. They are sites of adhesion and do not encircle the cell. They are made of two specialized cadherins, desmoglein and desmocollin. These proteins have extracellular domains that interact with each other on adjacent cells. On the cytoplasmic side, plakins form plaques which anchor the desmosomes to intermediate filaments composed of keratin proteins. Desmosomes also play a role in cell-cell signaling.\n\nGap junctions are the main site of cell-cell signaling or communication that allow small molecules to diffuse between adjacent cells. In vertebrates, gap junctions are composed of transmembrane proteins called connexins. They form hexagonal pores or channels through which ions, sugars, and other small molecules can pass. Each pore is made of 12 connexin molecules; 6 form a hemichannel on one cell membrane and interact with a hemichannel on an adjacent cell membrane. The permeability of these junctions is regulated by many factors including pH and Ca2+ concentration.\n\nReceptor proteins on the cell surface have the ability to bind specific signaling molecules secreted by other cells. Cell signaling allows cells to communicate with adjacent cells, nearby cells (paracrine) and even distant cells (endocrine). This binding induces a conformational change in the receptor which, in turn, elicits a response in the corresponding cell. These responses include changes in gene expression and alterations in cytoskeleton structure. The extracellular face of the plasma membrane has a variety of proteins, carbohydrates, and lipids which project outward and act as signals. Direct contact between cells allows the receptors on one cell to bind the small molecules attached to the plasma membrane of different cell. In eukaryotes, many of the cells during early development communicate through direct contact.\n\nSynaptic signaling, an integral part of nervous system activity, occurs between neurons and target cells. These target cells can also be neurons or other cell types (i.e. muscle or gland cells). Protocadherins, a member of the cadherin family, mediate the adhesion of neurons to their target cells at synapses otherwise known as synaptic junctions. In order to for communication to occur between a neuron and its target cell, a wave of depolarization travels the length of the neuron and causes neurotransmitters to be released into the synaptic junction. These neurotransmitters bind and activate receptors on the post-synaptic neuron thereby transmitting the signal to the target cell. Thus, a post-synaptic membrane belongs to the membrane receiving the signal, while a pre-synaptic membrane is the source of the neurotransmitter. In a neuromuscular junction, a synapse is formed between a motor neuron and muscle fibers. In vertebrates, acetylcholine released from the motor neuron acts as a neurotransmitter which depolarizes the muscle fiber and causes muscle contraction. A neuron’s ability to receive and integrate simultaneous signals from the environment and other neurons allows for complex animal behavior.\n\nPlant cells are surrounded by cell walls which are barriers for cell-cell communication. This barrier is overcome by specialized junctions called plasmodesmata. They are similar to gap junctions, connecting the cytosol of adjacent cells. Small molecules (<1000 Da), such as ions, amino acids, and sugars, can diffuse freely through plasmodesmata. These small molecules include signaling molecule and transcription factors. The size of the channel is also regulated to allow molecules up to 10,000 Da in size. The permeability of these channels is dependent on many factors, including Ca2+ concentration. An increase in cytosolic Ca2+ concentration will reversibly limit passage through the plasmodesmata. Unlike gap junctions, the cell membranes of adjacent cells merge to form a continuous channel called an annulus. Additionally, within the channel, there is an extension of the endoplasmic reticulum, called a desmotubule, which spans between the cells. The cell-cell interactions facilitated by plasmodesmata play an important role in development of plant cells and tissues and defense against viral infection.\n\nLeukocytes or white blood cells destroy abnormal cells and also provide protection against bacteria and other foreign matter. These interactions are transitory in nature but are crucial as an immediate immune response. To fight infection, leukocytes must move from the blood into the affected tissues. This movement into tissues is called extravasation. It requires successive forming and breaking of cell-cell interactions between the leukocytes and the endothelial cells that line blood vessels. These cell-cell interactions are mediated mainly by a group of Cell Adhesion Molecules (CAMs) called selectins.\n\nT helper cells, central to the immune system, interact with other leukocytes by releasing signals known as cytokines which activate and stimulate the proliferation of B cells and killer T cells.T helper cells also directly interact with macrophages, cells that engulf foreign matter and display antigens on its surface. T-helper cells that possess the appropriate receptors can bind to these antigens and proliferate resulting in T-helper cells that have the ability to identify the same antigens.\n\nCoagulation or blood clotting relies on, in addition to the production of fibrin, interactions between platelets. When the endothelium or the lining of a blood vessel is damaged, connective tissue including collagen fibers is locally exposed. Initially, platelets stick to the exposed connective tissue through specific cell-surface receptors. This is followed by platelet activation and aggregation in which platelets become firmly attached and release chemicals that recruit neighboring platelets to the site of vascular injury. A meshwork of fibrin then forms around this aggregation of platelets to increase the strength of the clot.\n\nBacterial populations interact in a similar manner to cells in tissue. They communicate through physical interactions and signaling molecules such as homoserine lactones and peptides as a means to control metabolism and regulate growth . A common example and one of the most studied forms of bacterial cell interactions is biofilm. Biofilm is a cell aggregate that can be attached to biological or abiotic surfaces. Bacteria form biofilms to adapt to various environments such as changes in substrate availability. For example, the formation of biofilm increases a bacterial cell's resistance to antibiotics compared to cells which are not part of the aggregate.\n\nCancer can result from the loss of cell-cell interaction. In normal cells, growth is controlled by contact inhibition in which contact with neighboring cells causes a stunt in cell growth. Contact inhibition is thought to be mediated by cadherins, proteins that play an important role in cell adhesion. This inhibition prevents cells from piling up on top of one another and forming mounds. However, in cancerous cells where expression of E-cadherin is lost, contact inhibition is lost and results in uncontrolled growth or proliferation, tumor formation, and metastasis.\n\nIn order for pathogenic bacteria to invade a cell, communication with the host cell is required. The first step for invading bacteria is usually adhesion to host cells. Strong anchoring, a characteristic that determines virulence, prevents the bacteria from being washed away before infection occurs. Bacterial cells can bind to many host cell surface structures such as glycolipids and glycoproteins which serve as attachment receptors. Once attached, the bacteria begin to interact with the host to disrupt its normal functioning and disrupt or rearrange its cytoskeleton. Proteins on the bacteria surface can interact with protein receptors on the host thereby affecting signal transduction within the cell. Alterations to signaling are favorable to bacteria because these alterations provide conditions under which the pathogen can invade. Many pathogens have Type III secretion systems which can directly inject protein toxins into the host cells. These toxins ultimately lead to rearrangement of the cytoskeleton and entry of the bacteria.\n\nCell–cell interactions are highly specific and are tightly regulated. Genetic defects and dysregulation of these interactions can cause many different diseases. Dysregulation that leads to leukocyte migration into healthy tissues can cause conditions such as acute respiratory distress syndrome and some types of arthritis. The autoimmune disease pemphigus vulgaris results from autoantibodies to desmoglein and other normal body proteins. The autoantibodies disrupt the adhesion between epithelial cells. This causes blisters of the skin and mucous membranes. Mutations in the connexin genes cause 8 human diseases including heart malformations and neurosensory deafness.\n", "id": "24950378", "title": "Cell–cell interaction"}
{"url": "https://en.wikipedia.org/wiki?curid=40378416", "text": "Plasmidome\n\nAn environment's plasmidome refers to the plasmids present in it.\nThe term is a portmanteau of the two English words Plasmid and Kingdom.\nIn biological research, plasmidome may refer to the actual plasmids that were found and isolated from a certain microorganism by means of culturing isolated microorganism and investigating the plasmids it possesses or by taking an environmental sample and performing a metagenomic survey using next generation sequencing methods in order to reveal and characterize plasmid genomes that belong to that environment.\n\n", "id": "40378416", "title": "Plasmidome"}
{"url": "https://en.wikipedia.org/wiki?curid=34368888", "text": "Myokine\n\nA myokine is one of several hundred cytokines or other small proteins (~5–20 kDa) and proteoglycan peptides that are produced and released by muscle cells (myocytes) in response to muscular contractions. They have autocrine, paracrine and/or endocrine effects; their systemic effects occur at picomolar concentrations.\n\nReceptors for myokines are found on muscle, fat, liver, pancreas, bone, heart, immune, and brain cells. The location of these receptors reflect the fact that myokines have multiple functions. Foremost, they are involved in exercise-associated metabolic changes, as well as in the metabolic changes following training adaptation. They also participate in tissue regeneration and repair, maintenance of healthy bodily functioning, immunomodulation; and cell signaling, expression and differentiation.\n\nThe present definition of the term myokine is attributed to Dr. Bente Klarlund Pedersen et al., who suggested its use in 2003.\nIn 2008, the first myokine, myostatin, was identified. The gp130 receptor cytokine IL-6 (Interleukin 6) was the first myokine found to be secreted into the blood stream in response to muscle contractions.\n\nThere is an emerging understanding of skeletal muscle as a secretory organ, and of myokines as mediators of physical fitness through the practice of regular physical exercise (aerobic exercise and strength training), as well as new awareness of the anti-inflammatory and thus disease prevention aspects of exercise. Different muscle fiber types -slow twitch muscle fibers, oxidative muscle fibers, intermediate twitch muscle fibers, and fast twitch muscle fibers - release different clusters of myokines during contraction. This implies that variation of exercise types, particularly aerobic training/endurance training and muscle contraction against resistance (strength training) may offer differing myokine-induced benefits. This topic has been discussed by fitness training specialists.\n\n\"Some myokines exert their effects within the muscle itself. Thus, myostatin, LIF, IL-6 and IL-7 are involved in muscle hypertrophy and myogenesis, whereas BDNF and IL-6 are involved in AMPK-mediated fat oxidation. IL-6 also appears to have systemic effects on the liver, adipose tissue and the immune system, and mediates crosstalk between intestinal L cells and pancreatic islets. Other myokines include the osteogenic factors IGF-1 and FGF-2; FSTL-1, which improves the endothelial function of the vascular system; and the PGC-1alpha-dependent myokine irisin, which drives brown fat-like development. Studies in the past few years suggest the existence of yet unidentified factors, secreted from muscle cells, which may influence cancer cell growth and pancreas function. Many proteins produced by skeletal muscle are dependent upon contraction; therefore, physical inactivity probably leads to an altered myokine response, which could provide a potential mechanism for the association between sedentary behaviour and many chronic diseases.\"\n\nHeart muscle is subject to two kinds of stress: physiologic stress, i.e. exercise; and pathologic stress, i.e. disease related. Likewise, the heart has two potential responses to either stress: cardiac hypertrophy, which is a normal, physiologic, adaptive growth; or cardiac remodeling, which is an abnormal, pathologic, maladaptive growth. Upon being subjected to either stress, the heart \"chooses\" to turn on one of the responses and turn off the other. If it has chosen the abnormal path, i.e. remodeling, exercise can reverse this choice by turning off remodeling and turning on hypertrophy. The mechanism for reversing this choice is the microRNA miR-222 in cardiac muscle cells, which exercise up-regulates via unknown myokines. miR-222 represses genes involved in fibrosis and cell-cycle control.\n\nImmunomodulation and immunoregulation were a particular focus of early myokine research, as, according to Dr. Bente Klarlund Pedersen and her colleagues, \"the interactions between exercise and the immune system provided a unique opportunity to evaluate the role of underlying endocrine and cytokine mechanisms.\"\n\nBoth aerobic exercise and strength training (resistance exercise) attenuate myostatin expression, and myostatin inactivation potentiates the beneficial effects of endurance exercise on metabolism.\n\nAerobic exercise provokes a systemic cytokine response, including, for example, IL-6, IL-1 receptor antagonist (IL-1ra), and IL-10 (Interleukin 10). IL-6 was serendipitously discovered as a myokine because of the observation that it increased in an exponential fashion proportional to the length of exercise and the amount of muscle mass engaged in the exercise. This increase is followed by the appearance of IL-1ra and the anti-inflammatory cytokine IL-10. In general, the cytokine response to exercise and sepsis differs with regard to TNF-α. Thus, the cytokine response to exercise is not preceded by an increase in plasma-TNF-α. Following exercise, the basal plasma IL-6 concentration may increase up to 100-fold, but less dramatic increases are more frequent. The exercise-induced increase of plasma IL-6 occurs in an exponential manner and the peak IL-6 level is reached at the end of the exercise or shortly thereafter. It is the combination of mode, intensity, and duration of the exercise that determines the magnitude of the exercise-induced increase of plasma IL-6.\n\nIL-6 had previously been classified as a proinflammatory cytokine. Therefore, it was first thought that the exercise-induced IL-6 response was related to muscle damage. However, it has become evident that eccentric exercise is not associated with a larger increase in plasma IL-6 than exercise involving concentric “nondamaging” muscle contractions. This finding clearly\ndemonstrates that muscle damage is not required to provoke an increase in plasma IL-6 during exercise. As a matter of fact, eccentric exercise may result in a delayed peak and a much slower decrease of plasma IL-6 during recovery.\n\nIL-6, among an increasing number of other recently identified myokines, thus remains an important topic of myokine research. It appears in muscle tissue and in the circulation during exercise at levels up to one hundred times basal rates, as noted, and is seen as having a beneficial impact on health and bodily functioning in most circumstances. P. Munoz-Canoves et al. write: \"It appears consistently in the literature that IL-6, produced locally by different cell types, has a positive impact on the proliferative capacity of muscle stem cells. This physiological mechanism functions to provide enough muscle progenitors in situations that require a high number of these cells, such as during the processes of muscle regeneration and hypertrophic growth after an acute stimulus. IL-6 is also the founding member of the myokine family of muscle-produced cytokines. Indeed, muscle-produced IL-6 after repeated contractions also has important autocrine and paracrine benefits, acting as a myokine, in regulating energy metabolism, controlling, for example, metabolic functions and stimulating glucose production. It is important to note that these positive effects of IL-6 and other myokines are normally associated with its transient production and short-term action.\"\n\nInterleukin-15 may play a significant role in the reduction of the volume of visceral (intra-abdominal) fat. IL-15 may accumulate within the muscle as a consequence of regular training. There is a negative association between plasma IL-15 concentration and trunk fat mass, but not limb fat mass.\n\nBrain-derived neurotrophic factor (BDNF) is also a myokine, though BDNF produced by contracting muscle is not released into circulation. Rather, BDNF produced in skeletal muscle appears to enhance the oxidation of fat. Skeletal muscle activation through exercise also contributes to an increase in BDNF secretion in the brain. A beneficial effect of BDNF on neuronal function has been noted in multiple studies. Dr. Pedersen writes, \"Neurotrophins are a family of structurally related growth factors, including brain-derived neurotrophic factor (BDNF), which exert many of their effects on neurons primarily through Trk receptor tyrosine kinases. Of these, BDNF and its receptor TrkB are most widely and abundantly expressed in the brain. However, recent studies show that BDNF is also expressed in non-neurogenic tissues, including skeletal muscle. BDNF has been shown to regulate neuronal development and to modulate synaptic plasticity. BDNF plays a key role in regulating survival, growth and maintenance of neurons, and BDNF has a bearing on learning and memory. However, BDNF has also been identified as a key component of the hypothalamic pathway that controls body mass and energy homeostasis.\n\n\"Most recently, we have shown that BDNF appears to be a major player not only in central metabolic pathways but also as a regulator of metabolism in skeletal muscle. Hippocampal samples from Alzheimer’s disease donors show decreased BDNF expression and individuals with Alzheimer’s disease have low plasma levels of BDNF. Also, patients with major depression have lower levels of serum BDNF than normal control subjects. Other studies suggest that plasma BDNF is a biomarker of impaired memory and general cognitive function in ageing women and a low circulating BDNF level was recently shown to be an independent and robust biomarker of mortality risk in old women. Interestingly, low levels of circulating BDNF are also found in obese individuals and those with type 2 diabetes. In addition, we have demonstrated that there is a cerebral output of BDNF and that this is inhibited during hyperglycaemic clamp conditions in humans. This last finding may explain the concomitant finding of low circulating levels of BDNF in individuals with type 2 diabetes, and the association between low plasma BDNF and the severity of insulin resistance.\n\n\"BDNF appears to play a role in both neurobiology and metabolism. Studies have demonstrated that physical exercise may increase circulating BDNF levels in humans. To identify whether the brain is a source of BDNF during exercise, eight volunteers rowed for 4 h while simultaneous blood samples were obtained from the radial artery and the internal jugular vein. To further identify the putative cerebral region(s) responsible for BDNF release, mouse brains were dissected and analysed for BDNF mRNA expression following treadmill exercise. In humans, a BDNF release from the brain was observed at rest and increased 2- to 3-fold during exercise. Both at rest and during exercise, the brain contributed 70–80% of the circulating BDNF, while this contribution decreased following 1 h of recovery. In mice, exercise induced a 3- to 5-fold increase in BDNF mRNA expression in the hippocampus and cortex, peaking 2 h after the termination of exercise. These results suggest that the brain is a major but not the sole contributor to circulating BDNF. Moreover, the importance of the cortex and hippocampus as sources of plasma BDNF becomes even more prominent in the response to exercise.”\n\nWith respect to studies of exercise and brain function, a 2010 report is of particular interest. Erickson et al. have shown that the volume of the anterior hippocampus increased by 2% in response to aerobic training in a randomized controlled trial with 120 older adults. The authors also summarize several previously-established research findings relating to exercise and brain function: (1) Aerobic exercise training increases grey and white matter volume in the prefrontal cortex of older adults and increases the functioning of key nodes in the executive control network. (2) Greater amounts of physical activity have been associated with sparing of prefrontal and temporal brain regions over a 9-y period, which reduces the risk for cognitive impairment. (3) Hippocampal and medial temporal lobe volumes are larger in higher-fit older adults (larger hippocampal volumes have been demonstrated to mediate improvements in spatial memory). (4) Exercise training increases cerebral blood volume and perfusion of the hippocampus.\n\nRegarding the 2010 study, the authors conclude: \"We also demonstrate that increased hippocampal volume is associated with greater serum levels of BDNF, a mediator of neurogenesis in the dentate gyrus. Hippocampal volume declined in the control group, but higher preintervention fitness partially attenuated the decline, suggesting that fitness protects against volume loss. Caudate nucleus and thalamus volumes were unaffected by the intervention. These theoretically important findings indicate that aerobic exercise training is effective at reversing hippocampal volume loss in late adulthood, which is accompanied by improved memory function.\"\n\nSeldin, Peterson, Byerly, Wei and Wong clarify that most myokines are also secreted by non-muscle cells. In 2012, they reported: \"The expression of all myokines described to date is not restricted to skeletal muscle; they are generally expressed by a variety of cell types, and most are, in fact, expressed at much higher levels by nonmuscle tissues. Prior to (our) study, no myokine has been discovered to be preferentially expressed by skeletal muscle. While characterizing the metabolic function of the C1q/TNF-related protein (CTRP) family of proteins we recently uncovered, we identified myonectin (CTRP15) as a novel member of the family on the basis of sequence homology in the shared C1q domain, the signature that defines this protein family... (Myonectin) is a novel nutrient-responsive myokine secreted by skeletal muscle to regulate whole-body fatty acid metabolism... Circulating levels of myonectin were tightly regulated by the metabolic state; fasting suppressed, but refeeding dramatically increased, its mRNA and serum levels. Although mRNA and circulating levels of myonectin were reduced in a diet-induced obese state, voluntary exercise increased its expression and circulating levels. Accordingly, myonectin transcript was up-regulated by compounds (forskolin, epinephrine, ionomycin) that raise cellular cAMP or calcium levels... In vitro results of myonectin expression in myotubes suggest that exercise-induced rises in intracellular calcium levels may also up-regulate myonectin expression in intact skeletal muscle... Consistent with enhanced mRNA expression in skeletal muscle of mice subjected to voluntary exercise, circulating levels of myonectin also increased, suggesting a potential role of myonectin in exercise-induced physiology.\n\n\"Given that exercise induces myonectin expression in skeletal muscle, we next addressed whether short- and long-term changes in nutritional/metabolic state also regulate myonectin expression and circulating levels. Surprisingly, an overnight fast greatly suppressed myonectin expression, but a 2-h refeeding period (following an overnight fast) dramatically up-regulated its mRNA expression in skeletal muscle. Intriguingly, refeeding induced myonectin mRNA expression to a much greater extent in soleus than in plantaris muscle fiber of both male and female mice (data not shown), suggesting that myonectin expression may be regulated differentially depending on muscle fiber type. Consistent with the mRNA data, fasting reduced, but refeeding substantially increased, circulating levels of myonectin... As compared with mice fed an isocaloric matched low-fat diet, mice fed a high-fat diet had lower myonectin mRNA and serum levels, suggesting that obesity-induced alteration in energy balance may be linked to dysregulation of myonectin-mediated processes in the obese state... A relatively modest rise in serum myonectin levels was sufficient to lower (by >30%) nonesterified free fatty acid (NEFA) levels over time relative to vehicle-injected controls. However, no significant difference was observed in serum triacylglycerol levels between the two groups of mice. These data suggest a potential role of myonectin in regulating systemic fatty acid metabolism... Treatment of adipocytes with recombinant myonectin (5 micrograms/ml) also enhanced fatty acid uptake to the same extent as insulin... To determine whether myonectin-mediated enhancement of lipid uptake is specific to adipocytes, we also tested the effect of myonectin on lipid uptake in rat H4IIE hepatocytes. We observed a modest (>25%) but consistent increase in fatty acid uptake into hepatocytes stimulated with myonectin (5 micrograms/ml), an effect similar to cells treated with a saturating dose of insulin (50 nM)... Together, these results indicate that myonectin promotes lipid uptake into adipocytes and hepatocytes via transcriptional up-regulation of genes involved in fatty acid uptake...\n\n\"We provide the first characterization of myonectin, with in vitro and in vivo evidence that it is a novel myokine with important metabolic function. Unlike the other CTRPs characterized to date, myonectin (CTRP15) is expressed and secreted predominantly by skeletal muscle... (Our) results suggest that myonectin is a nutrient-responsive metabolic regulator secreted by skeletal muscle in response to changes in cellular energy state resulting from glucose or fatty acid fluxes. Many metabolically relevant secreted proteins (e.g. adiponectin, leptin, resistin, and RBP) and the signaling pathways they regulate in tissues are known to be dysregulated in the condition of obesity. The reduction in expression and circulating levels of myonectin in the obese state may represent yet another component of the complex metabolic circuitry dysregulated by excess caloric intake. Although exercise has long been known to have profound positive impacts on systemic insulin sensitivity and energy balance, the underlying mechanisms remain incompletely understood. That voluntary exercise dramatically increases the expression and circulating levels of myonectin to promote fatty acid uptake into cells may underlie one of the beneficial effects of physical exercise... A modest rise in the circulating levels of myonectin resulting from recombinant protein administration is sufficient to lower serum NEFA without altering serum triglyceride levels. Unlike CTRP1, CTRP3 and CTRP12, injection of recombinant myonectin into mice appears to have no glucose-lowering effect. Reduction in circulating NEFA is not due to suppression of adipose tissue lipolysis; rather, it results from increased fatty acid uptake by adipocytes and hepatocytes. Although the myonectin-mediated enhancement of lipid uptake in vitro appears modest (25–50%), in fact, the magnitude of this effect is comparable with cells stimulated with 50 nM insulin, a saturating dose that leads to maximum increase in fatty acid uptake... In accordance with myonectin mediating its metabolic effect through a transcriptional mechanism, a reduction in circulating NEFA in mice occurred only 2 h after recombinant protein injection, a lag period presumably required for mRNA and protein synthesis.\"\n\nDecorin is an example of a proteoglycan which functions as a myokine. Kanzleiter et al have established that this myokine is secreted during muscular contraction against resistance, and plays a role in muscle growth. They reported on July 1, 2014: \"The small leucine-rich proteoglycan decorin has been described as a myokine for some time. However, its regulation and impact on skeletal muscle (had) not been investigated in detail. In (our recent) study, we report decorin to be differentially expressed and released in response to muscle contraction using different approaches. Decorin is released from contracting human myotubes, and circulating decorin levels are increased in response to acute resistance exercise in humans. Moreover, decorin expression in skeletal muscle is increased in humans and mice after chronic training. Because decorin directly binds myostatin, a potent inhibitor of muscle growth, we investigated a potential function of decorin in the regulation of skeletal muscle growth. In vivo overexpression of decorin in murine skeletal muscle promoted expression of the pro-myogenic factor Mighty, which is negatively regulated by myostatin. We also found Myod1 and follistatin to be increased in response to decorin overexpression. Moreover, muscle-specific ubiquitin ligases atrogin1 and MuRF1, which are involved in atrophic pathways, were reduced by decorin overexpression. In summary, our findings suggest that decorin secreted from myotubes in response to exercise is involved in the regulation of muscle hypertrophy and hence could play a role in exercise-related restructuring processes of skeletal muscle.\"\n\nIrisin is a cleaved version of FNDC5. Boström and coworkers named the cleaved product irisin, after the Greek messenger goddess Iris. FNDC5 was initially discovered in 2002 by two independent groups of researchers.\n\nRana et al. reported in January 2014 that irisin (fibronectin type III domain-containing protein 5 or FNDC5), a recently described myokine hormone produced and secreted by acutely exercising skeletal muscles, is thought to bind white adipose tissue cells via undetermined receptors. Irisin has been reported to promote a brown adipose tissue-like phenotype upon white adipose tissue by increasing cellular mitochondrial density and expression of uncoupling protein-1, thereby increasing adipose tissue energy expenditure via thermogenesis. This is considered important, because excess visceral adipose tissue in particular distorts the whole body energy homeostasis, increases the risk of cardiovascular disease and raises exposure to a milieu of adipose tissue-secreted hormones (adipokines) that promote inflammation and cellular aging. The authors enquired whether the favorable impact of irisin on white adipose tissue might be associated with maintenance of telomere length, a well-established genetic marker in the aging process. They conclude that these data support the view that irisin may have a role in the modulation not only of energy balance but also the aging process.\n\nHowever, exogenous irisin may aid in heightening energy expenditure, and thus in reducing obesity. Boström et al. reported on December 14, 2012: \"Since the conservation of calories would likely provide an overall survival advantage for mammals, it appears paradoxical that exercise would stimulate the secretion of a polypeptide hormone that increases thermogenesis and energy expenditure. One explanation for the increased irisin expression with exercise in mouse and man may have evolved as a consequence of muscle contraction during shivering. Muscle secretion of a hormone that activates adipose thermogenesis during this process might provide a broader, more robust defense against hypothermia. The therapeutic potential of irisin is obvious. Exogenously administered irisin induces the browning of subcutaneous fat and thermogenesis, and it presumably could be prepared and delivered as an injectable polypeptide. Increased formation of brown or beige/brite fat has been shown to have anti-obesity, anti-diabetic effects in multiple murine models, and adult humans have significant deposits of UCP1-positive brown fat. (Our data show) that even relatively short treatments of obese mice with irisin improves glucose homeostasis and causes a small weight loss. Whether longer treatments with irisin and/or higher doses would cause more weight loss remains to be determined. The worldwide, explosive increase in obesity and diabetes strongly suggests exploring the clinical utility of irisin in these and related disorders. Another potentially important aspect of this work relates to other beneficial effects of exercise, especially in some diseases for which no effective treatments exist. The clinical data linking exercise with health benefits in many other diseases suggests that irisin could also have significant effects in these disorders.\"\n\nWhile the murine findings reported by Boström et al. appear encouraging, other researchers have questioned whether irisin operates in a similar manner in humans. For example, Timmons et al. noted that over 1,000 genes are upregulated by exercise and examined how expression of FNDC5 was affected by exercise in ~200 humans. They found that it was upregulated only in highly active elderly humans, casting doubt on the conclusions of Boström et al. Further discussion of this issue can be found in the Wikipedia entry for irisin under the \"function\" heading.\n\nA novel myokine Osteonectin, or SPARC, plays a vital role in bone mineralization, cell-matrix interactions, and collagen binding. Osteonectin inhibits tumorigenesis in mice. Osteonectin can be classed as a myokine, as it was found that even a single bout of exercise increased its expression and secretion in skeletal muscle in both mice and humans.\n\n", "id": "34368888", "title": "Myokine"}
{"url": "https://en.wikipedia.org/wiki?curid=40471305", "text": "MG-RAST\n\nMG-RAST is an open source web application server that suggests automatic phylogenetic and functional analysis of metagenomes. It is also one of the biggest repositories for metagenomic data. The name is an abbreviation of \"Metagenomic Rapid Annotations using Subsystems Technology\".\nThe pipeline automatically produces functional assignments to the sequences that belong to the metagenome by performing sequence comparisons to databases in both\nnucleotide and amino-acid levels. The applications supplies phylogenetic and functional assignments of the metagenome being analysed, as well as tools for comparing different metagenomes. It also provides a RESTful API for programmatic access.\n\nThe server was created and maintained by Argonne National Laboratory from the University of Chicago. In December 29 of 2016, the system had analyzed 60 terabase-pairs of data from more than 150,000 data sets. Among the analyzed data sets, more than 23,000 are available to the public.\n\nCurrently, the computational resources are provided by the DOE Magellan cloud at Argonne National Laboratory, Amazon EC2 Web services, and a number of traditional clusters.\n\nMG-RAST has been developed as an effort to have a free, public resource for the analysis and the storage of metagenome sequence data. The service removes one of the primary bottlenecks in metagenome analysis: the availability of high-performance computing for annotating data.\n\nMetagenomic and metatranscriptomic studies involve the processing of large datasets and therefore they can require computationally expensive analysis. Nowadays, scientists are able to generate such volumes of data because, in the recent years, the sequencing costs have reduced dramatically. This fact has shifted the limiting factor to the computing costs:for instance, a recent study of the University of Maryland, estimated a cost of more than $5 million per terabase using their CLOVR metagenome analysis pipeline. As the size and number of sequence datasets continue to increase, costs related to their analysis will continue to rise.\n\nAdditionally, MG-RAST also works as a repository tool for metagenomic data. Metadata collection and interpretation is vital for genomic and metagenomic studies, and challenges in this regard include the exchange, curation, and distribution of this information. The MG-RAST system has been an early adopter of the minimal checklist standards and the expanded biome-specific environmental packages devised by the Genomics Standards Consortium, and provides an easy-to-use uploader for metadata capture at the time of data submission.\n\nThe MG-RAST application offers automated quality control, annotation, comparative analysis and archiving service of metagenomic and amplicon sequences using a combination of several bioinformatics tools. The application was built to analyze metagenomic data, but it also supports amplicon (16S, 18S, and ITS) sequences and metatranscriptome (RNA-seq) sequences processing. By now, MG-RAST is not capable of predicting coding regions from eukaryotes and therefore it is of limited use for eukaryotic metagenomes analysis.\n\nThe pipeline of MG-RAST can be divided into five stages:\n\nIncludes steps for quality control and artifacts removal. Firstly, low-quality regions are trimmed using SolexaQA and reads showing inappropriate lengths are removed. A dereplication step is included in the case of metagenome and metatranscriptome datasets processing. Subsequently, DRISEE (Duplicate Read Inferred Sequencing Error Estimation) is used to assess the sample sequencing error based on Artificial Duplicate Reads (ADRs) measuring. And finally, the pipeline offers the possibility of screening the reads using Bowtie aligner and removing the reads showing matches close to model organisms genomes (including fly, mouse, cow and human).\n\nMG-RAST identifies gene sequences by using a machine learning approach: FragGeneScan. Ribosomal RNA sequences are identified through an initial BLAT search against a reduced version of SILVA database.\n\nIn order to identify the putative functions and annotation of the genes, MG-RAST builds clusters of proteins at 90% identity level using the UCLUST implementation in QIIME. The longest sequence of each cluster will be selected for a similarity analysis. The similarity analysis is computed through sBLAT (in which BLAT algorithm is parallelized using OpenMP). The search is computed against a protein database derived from the M5nr, which provides nonredundant integration of sequences from GenBank, SEED, IMG, UniProt, KEGG and eggNOGs databases.\n\nThe reads associated to rRNA sequences are clustered at 97% identity. The longest sequence of each cluster is picked as representative and will be used for a BLAT search against the M5rna database, which integrates SILVA, Greengenes and RDP.\n\nThe data is integrated into a number of data products. The most important ones are the abundance profiles, which represent a pivoted and aggregated version of the similarity files.\n\nFinally, the obtained abundance profiles are loaded into the respective databases.\n\nBesides metagenome analysis, MT-RAST can also be used for data discovery. The visualization or comparison of metagenomes profiles and data sets can be implemented in a wide variety of modes; the web interface allows to select data based on criteria like composition, sequences quality, functionality or sample type and offers several ways to compute statistical inferences and ecological analyses. The profiles for the metagenomes can be visualized and compared by using barcharts, trees, spreadsheet-like tables,heatmaps, PCoA, rarefaction plots, circular recruitment plot,and KEGG maps.\n\n\n", "id": "40471305", "title": "MG-RAST"}
{"url": "https://en.wikipedia.org/wiki?curid=40482234", "text": "Panomics\n\n\"Panomics\" or 'pan-omics' refers to the range of molecular biology technologies including genomics, proteomics, metabolomics, transcriptomics, and so forth, or the integration of their combined use. Systems biology approaches are often based upon the use of panomic analysis data. \n\nThe American Society of Clinical Oncology (ASCO) defines panomics as referring to \"the interaction of all biological\nfunctions within a cell and with other body functions, combining data collected by targeted tests ... and global assays (such as genome sequencing) with other patient-specific information.\"\n\n", "id": "40482234", "title": "Panomics"}
{"url": "https://en.wikipedia.org/wiki?curid=40427167", "text": "Recombinase Polymerase Amplification\n\nRecombinase polymerase amplification (RPA) is a single tube, isothermal alternative to the polymerase chain reaction (PCR). By adding a reverse transcriptase enzyme to an RPA reaction it can detect RNA as well as DNA, without the need for a separate step to produce cDNA. Because it is isothermal, RPA can use much simpler equipment than PCR, which requires a thermal cycler. Operating best at temperatures of 37–42 °C and still working, albeit more slowly, at room temperature means RPA reactions can in theory be run quickly simply by holding a tube. This makes RPA an excellent candidate for developing low-cost, rapid, point-of-care molecular tests. A recent international quality assessment of molecular detection of Rift Valley fever virus performed as well as the best RT-PCR tests, detecting less concentrated samples missed by some PCR tests and an RT_LAMP test.\nRPA was developed and launched by TwistDx Ltd. (formerly known as ASM Scientific Ltd), a biotechnology company based in Cambridge, UK.\n\nThe RPA process employs three core enzymes – a recombinase, a single-stranded DNA-binding protein (SSB) and strand-displacing polymerase. Recombinases are capable of pairing oligonucleotide primers with homologous sequence in duplex DNA. SSB bind to displaced strands of DNA and prevent the primers from being displaced. Finally, the strand displacing polymerase begins DNA synthesis where the primer has bound to the target DNA. By using two opposing primers, much like PCR, if the target sequence is indeed present, an exponential DNA amplification reaction is initiated. There is no other sample manipulation such as thermal or chemical melting is required to initiate amplification. At optimal temperatures (37–42 °C), the reaction progresses rapidly and results in specific DNA amplification from just a few target copies to detectable levels, typically within 10 minutes, for rapid detection of viral genomic DNA or RNA, pathogenic bacterial genomic DNA, as well as short length aptamer DNA.\n\nThe three core RPA enzymes can be supplemented by further enzymes to provide extra functionality. Addition of exonuclease III allows the use of an exo probe for real-time, fluorescence detection akin to real-time PCR. Addition of endonuclease IV means that a nfo probe can be used for lateral flow strip detection of successful amplification. If a reverse transcriptase that works at 37–42 °C is added then RNA can be reverse transcribed and the cDNA produced amplified all in one step. Currently only the TwistAmp exo version of RPA is available with the reverse transcriptase included, although users can simply supplement other TwistAmp reactions with a reverse transcriptase to produce the same effect.\nAs with PCR, all forms of RPA reactions can be multiplexed by the addition of further primer/probe pairs, allowing the detection of multiple analytes or an internal control in the same tube.\n\nRPA is one of several isothermal nucleic acid amplification techniques to be developed as a molecular diagnostic technique, frequently with the objective of simplifying the laboratory instrumentation required relative to PCR. A partial list of other isothermal amplification techniques include LAMP, NASBA, helicase-dependent amplification (HDA), and nicking enzyme amplification reaction (NEAR). The techniques differ in the specifics of primer design and reaction mechanism, and in some cases (like RPA) make use of cocktails of two or more enzymes. Like RPA, many of these techniques offer rapid amplification times with the potential for simplified instrumentation, and reported resistance to substances in unpurified samples that are known to inhibit PCR. With respect to amplification time, it should be noted that modern thermocyclers with rapid temperature ramps can reduce PCR amplification times to less than 30 minutes, particularly for short amplicons using dual-temperature cycling rather than the conventional three-temperature protocols. In addition, the demands of sample prep (including lysis and extraction of DNA or RNA, if necessary) should be considered as part of the overall time and complexity inherent to the technique. These requirements vary according to the technique as well as to the specific target and sample type.\n\nCompared to PCR, the guidelines for primer and probe design for RPA are less established, and may take a certain degree of trial and error, although recent results indicate that standard PCR primers can work as well. The general principle of a discrete amplicon bounded by a forward and reverse primer with an (optional) internal fluorogenic probe is similar to PCR. PCR primers may be used directly in RPA, but their short length means that recombination rates are low and RPA will not be especially sensitive or fast. Typically 30–38 base primers are needed for efficient recombinase filament formation and RPA performance. This is in contrast to some other techniques such as LAMP which use a larger number of primers subject to additional design constraints. Although the original 2006 report of RPA describes a functional set of reaction components, the current (proprietary) formulation of the TwistAmp kit is \"substantially different\" and is available only from the TwistDx supplier. This is in comparison to reaction mixtures for PCR which are available from many suppliers, or LAMP or NASBA for which the composition of the reaction mixture is freely published, allowing researchers to create their own customized \"kits\" from inexpensive ingredients.\n\nPublished scientific literature generally lacks detailed comparison of the performance of isothermal amplification techniques such as RPA, HDA, and LAMP relative to each other, often rather comparing a single isothermal technique to a \"gold standard\" PCR assay. This makes it difficult to judge the merits of these techniques independently from the claims of the manufacturers, inventors, or proponents. Furthermore, performance characteristics of any amplification technique are difficult to decouple from primer design: a \"good\" primer set for one target for RPA may give faster amplification or more sensitive detection than a \"poor\" LAMP primer set for the same target, but the converse may be true for different primer sets for a different target. An exception is a recent study comparing RT-qPCR, RT-LAMP, and RPA for detection of Schmallenberg virus and bovine viral diarrhea virus, which effectively makes the point that each amplification technique has strengths and weaknesses, which may vary by the target, and that the properties of the available amplification techniques need to be evaluated in combination with the requirements for each application. As with PCR and any other amplification technique, there is obviously a publication bias, with poorly performing primer sets rarely deemed worthy of reporting.\n", "id": "40427167", "title": "Recombinase Polymerase Amplification"}
{"url": "https://en.wikipedia.org/wiki?curid=22833956", "text": "Molecular models of DNA\n\nMolecular models of DNA structures are representations of the molecular geometry and topology of deoxyribonucleic acid (DNA) molecules using one of several means, with the aim of simplifying and presenting the essential, physical and chemical, properties of DNA molecular structures either \"in vivo\" or \"in vitro\". These representations include closely packed spheres (CPK models) made of plastic, metal wires for \"skeletal models\", graphic computations and animations by computers, artistic rendering. Computer molecular models also allow animations and molecular dynamics simulations that are very important for understanding how DNA functions \"in vivo\".\n\nThe more advanced, computer-based molecular models of DNA involve molecular dynamics simulations and quantum mechanics computations of vibro-rotations, delocalized molecular orbitals (MOs), electric dipole moments, hydrogen-bonding, and so on. \"DNA molecular dynamics modeling\" involves simulating deoxyribonucleic acid (DNA) molecular geometry and topology changes with time as a result of both intra- and inter- molecular interactions of DNA. Whereas molecular models of DNA molecules such as closely packed spheres (CPK models) made of plastic or metal wires for \"skeletal models\" are useful representations of static DNA structures, their usefulness is very limited for representing complex DNA dynamics. Computer molecular modeling allows both animations and molecular dynamics simulations that are very important to understand how DNA functions \"in vivo\".\n\nFrom the very early stages of structural studies of DNA by X-ray diffraction and biochemical means, molecular models such as the Watson-Crick nucleic acid double helix model were successfully employed to solve the 'puzzle' of DNA structure, and also find how the latter relates to its key functions in living cells. The first high quality X-ray diffraction patterns\nof A-DNA were reported by Rosalind Franklin and Raymond Gosling in 1953. Rosalind Franklin made the critical observation that DNA exists in two distinct forms, A and B, and produced the sharpest pictures of both through X-ray diffraction technique. The first calculations of the Fourier transform of an atomic helix were reported one year earlier by Cochran, Crick and Vand, and were followed in 1953 by the computation of the Fourier transform of a coiled-coil by Crick.\n\nStructural information is generated from X-ray diffraction studies of oriented DNA fibers with the help of molecular models of DNA that are combined with crystallographic and mathematical analysis of the X-ray patterns.\n\nThe first reports of a double helix molecular model of B-DNA structure were made by James Watson and Francis Crick in 1953. That same year, Maurice F. Wilkins,\nA. Stokes and H.R. Wilson, reported the first X-ray patterns\nof \"in vivo\" B-DNA in partially oriented salmon sperm heads.\n\nThe development of the first correct double helix molecular model of DNA by Crick and Watson may not have been possible without the biochemical evidence for the nucleotide base-pairing ([A---T]; [C---G]), or Chargaff's rules. Although such initial studies of DNA structures with the help of molecular models were essentially static, their consequences for explaining the \"in vivo\" functions of DNA were significant in the areas of protein biosynthesis and the quasi-universality of the genetic code. Epigenetic transformation studies of DNA \"in vivo\" were however much slower to develop despite their importance for embryology, morphogenesis and cancer research. Such chemical dynamics and biochemical reactions of DNA are much more complex than the molecular dynamics of DNA physical interactions with water, ions and proteins/enzymes in living cells.\n\nAn old standing dynamic problem is how DNA \"self-replication\" takes place in living cells that should involve transient uncoiling of supercoiled DNA fibers. Although DNA consists of relatively rigid, very large elongated biopolymer molecules called \"fibers\" or chains (that are made of repeating nucleotide units of four basic types, attached to deoxyribose and phosphate groups), its molecular structure \"in vivo\" undergoes dynamic configuration changes that involve dynamically attached water molecules and ions. Supercoiling, packing with histones in chromosome structures, and other such supramolecular aspects also involve \"in vivo\" DNA topology which is even more complex than DNA molecular geometry, thus turning molecular modeling of DNA into an especially challenging problem for both molecular biologists and biotechnologists. Like other large molecules and biopolymers, DNA often exists in multiple stable geometries (that is, it exhibits conformational isomerism) and configurational, quantum states which are close to each other in energy on the potential energy surface of the DNA molecule.\n\nSuch varying molecular geometries can also be computed, at least in principle, by employing \"ab initio\" quantum chemistry methods that can attain high accuracy for small molecules, although claims that acceptable accuracy can be also achieved for polynuclelotides, and DNA conformations, were recently made on the basis of vibrational circular dichroism (VCD) spectral data. Such quantum geometries define an important class of \"ab initio\" molecular models of DNA which exploration has barely started, especially related to results obtained by VCD in solutions. More detailed comparisons with such \"ab initio\" quantum computations are in principle obtainable through 2D-FT NMR spectroscopy and relaxation studies of polynucleotide solutions or specifically labeled DNA, as for example with deuterium labels.\n\nIn an interesting twist of roles, the DNA molecule was proposed to be used for quantum computing via DNA. Both DNA nanostructures and DNA computing biochips have been built.\n\nThe chemical structure of DNA is insufficient to understand the complexity of the 3D structures of DNA. In contrast, animated molecular models allow one to visually explore the three-dimensional (3D) structure of DNA. The DNA model shown (far right) is a space-filling, or CPK, model of the DNA double helix. Animated molecular models, such as the wire, or skeletal, type shown at the top of this article, allow one to visually explore the three-dimensional (3D) structure of DNA. Another type of DNA model is the space-filling, or CPK, model.\n\nThe hydrogen bonding dynamics and proton exchange is very different by many orders of magnitude between the two systems of fully hydrated DNA and water molecules in ice. Thus, the DNA dynamics is complex, involving nanosecond and several tens of picosecond time scales, whereas that of liquid ice is on the picosecond time scale, and that of proton exchange in ice is on the millisecond time scale. The proton exchange rates in DNA and attached proteins may vary from picosecond to nanosecond, minutes or years, depending on the exact locations of the exchanged protons in the large biopolymers.\n\nA simple harmonic oscillator 'vibration' is only an oversimplified dynamic representation of the longitudinal vibrations of the DNA intertwined helices which were found to be anharmonic rather than harmonic as often assumed in quantum dynamic simulations of DNA.\n\nThe structure of DNA shows a variety of forms, both double-stranded and single-stranded. The mechanical properties of DNA, which are directly related to its structure, are a significant problem for cells. Every process which binds or reads DNA is able to use or modify the mechanical properties of DNA for purposes of recognition, packaging and modification. The extreme length (a chromosome may contain a 10 cm long DNA strand), relative rigidity and helical structure of DNA has led to the evolution of histones and of enzymes such as topoisomerases and helicases to manage a cell's DNA. The properties of DNA are closely related to its molecular structure and sequence, particularly the weakness of the hydrogen bonds and electronic interactions that hold strands of DNA together compared to the strength of the bonds within each strand.\n\nExperimental methods which can directly measure the mechanical properties of DNA are relatively new, and high-resolution visualization in solution is often difficult. Nevertheless, scientists have uncovered large amount of data on the mechanical properties of this polymer, and the implications of DNA's mechanical properties on cellular processes is a topic of active current research.\n\nThe DNA found in many cells can be macroscopic in length: a few centimetres long for each human chromosome. Consequently, cells must compact or \"package\" DNA to carry it within them. In eukaryotes this is carried by spool-like proteins named histones, around which DNA winds. It is the further compaction of this DNA-protein complex which produces the well known mitotic eukaryotic chromosomes.\n\nIn the late 1970s, alternate non-helical models of DNA structure were briefly considered as a potential solution to problems in DNA replication in plasmids and chromatin. However, the models were set aside in favor of the double-helical model due to subsequent experimental advances such as X-ray crystallography of DNA duplexes, and later the nucleosome core particle, and the discovery of topoisomerases. Such non-double-helical models are not currently accepted by the mainstream scientific community.\n\nAfter DNA has been separated and purified by standard biochemical methods, one has a sample in a jar much like in the figure at the top of this article. Below are the main steps involved in generating structural information from X-ray diffraction studies of oriented DNA fibers that are drawn from the hydrated DNA sample with the help of molecular models of DNA that are combined with crystallographic and mathematical analysis of the X-ray patterns.\n\nA paracrystalline lattice, or paracrystal, is a molecular or atomic lattice with significant amounts (e.g., larger than a few percent) of partial disordering of molecular arrangements. Limiting cases of the paracrystal model are nanostructures, such as glasses, liquids, etc., that may possess only local ordering and no global order. A simple example of a paracrystalline lattice is shown in the following figure for a silica glass:\n\nLiquid crystals also have paracrystalline rather than crystalline structures.\n\nHighly hydrated B-DNA occurs naturally in living cells in such a paracrystalline state, which is a dynamic one despite the relatively rigid DNA double helix stabilized by parallel hydrogen bonds between the nucleotide base-pairs in the two complementary, helical DNA chains (see figures). For simplicity most DNA molecular models omit both water and ions dynamically bound to B-DNA, and are thus less useful for understanding the dynamic behaviors of B-DNA \"in vivo\". The physical and mathematical analysis of X-ray and spectroscopic data for paracrystalline B-DNA is thus far more complex than that of crystalline, A-DNA X-ray diffraction patterns. The paracrystal model is also important for DNA technological applications such as DNA nanotechnology. Novel methods that combine X-ray diffraction of DNA with X-ray microscopy in hydrated living cells are now also being developed.\n\nThere are various uses of DNA molecular modeling in Genomics and Biotechnology research applications, from DNA repair to PCR and DNA nanostructures. Two-dimensional DNA junction arrays have been visualized by Atomic force microscopy.\n\nDNA molecular modeling has various uses in genomics and biotechnology, with research applications ranging from DNA repair to PCR and DNA nanostructures. These include computer molecular models of molecules as varied as RNA polymerase, an E. coli, bacterial DNA primase template suggesting very complex dynamics at the interfaces between the enzymes and the DNA template, and molecular models of the mutagenic, chemical interaction of potent carcinogen molecules with DNA. These are all represented in the gallery below.\n\nTechnological application include a DNA biochip and DNA nanostructures designed for DNA computing and other dynamic applications of DNA nanotechnology.\nThe image at right is of self-assembled DNA nanostructures. The DNA \"tile\" structure in this image consists of four branched junctions oriented at 90° angles. Each tile consists of nine DNA oligonucleotides as shown; such tiles serve as the primary \"building block\" for the assembly of the DNA nanogrids shown in the AFM micrograph.\n\nQuadruplex DNA may be involved in certain cancers. Images of quadruplex DNA are in the gallery below.\n\n\n\n\n\n\n\n\n\n", "id": "22833956", "title": "Molecular models of DNA"}
{"url": "https://en.wikipedia.org/wiki?curid=241191", "text": "DNA–DNA hybridization\n\nDNA–DNA hybridization generally refers to a molecular biology technique that measures the degree of genetic similarity between pools of DNA sequences. It is usually used to determine the genetic distance between two organisms. This has been used extensively in phylogeny and taxonomy.\n\nThe DNA of one organism is labeled, then mixed with the unlabeled DNA to be compared against. The mixture is incubated to allow DNA strands to dissociate and renewal forming hybrid double-stranded DNA. Hybridized sequences with a high degree of similarity will bind more firmly, and require more energy to separate them: i.e. they separate when heated at a higher temperature than dissimilar sequences, a process known as \"DNA melting\".\n\nTo assess the melting profile of the hybridized DNA, the double-stranded DNA is bound to a column and the mixture is heated in small steps. At each step, the column is washed; sequences that melt become single-stranded and wash off the column. The temperatures at which labeled DNA comes off the column reflects the amount of similarity between sequences (and the self-hybridization sample serves as a control). These results are combined to determine the degree of genetic similarity between organisms.\n\nWhen several species are compared that way, the similarity values allow the species to be arranged in a phylogenetic tree; it is therefore one possible approach to carrying out molecular systematics. Charles Sibley and Jon Ahlquist, pioneers of the technique, used DNA–DNA hybridization to examine the phylogenetic relationships of avians (the Sibley–Ahlquist taxonomy) and primates.\n\nDNA–DNA hybridization is the gold standard to distinguish bacterial species, with a similarity value greater than 70% indicating that the compared strains belong to distinct species.\nIn 2014, a threshold of 79% similarity has been suggested to separate bacterial subspecies.<ref name=\"doi:10.1186/1944-3277-9-2\"></ref>\n\nCritics argue that the technique is inaccurate for comparison of closely related species, as any attempt to measure differences between orthologous sequences between organisms is overwhelmed by the hybridization of paralogous sequences within an organism's genome. DNA sequencing and computational comparisons of sequences is now generally the method for determining genetic distance, although the technique is still used in microbiology to help identify bacteria.\n\nThe modern approach is to carry out DNA–DNA hybridization \"in silico\" using completely or partially sequenced genomes.<ref name=\"doi10.1186/1471-2105-14-60\"></ref> The GGDC developed at DSMZ is the most accurate known tool for calculating DDH-analogous values. Among other algorithmic improvements, it solves the problem with paralogous sequences by carefully filtering them from the matches between the two genome sequences.\n\n", "id": "241191", "title": "DNA–DNA hybridization"}
{"url": "https://en.wikipedia.org/wiki?curid=394765", "text": "Yeast artificial chromosome\n\nYeast artificial chromosomes (YACs) are genetically engineered chromosomes derived from the DNA of the yeast, \"Saccharomyces cerevisiae\", which is then ligated into a bacterial plasmid. By inserting large fragments of DNA, from 100–1000 kb, the inserted sequences can be cloned and physically mapped using a process called chromosome walking. This is the process that was initially used for the Human Genome Project, however due to stability issues, YACs were abandoned for the use of Bacterial artificial chromosomes (BAC). Beginning with the initial research of the Rankin et al., Strul et al., and Hsaio et al., the inherently fragile chromosome was stabilized by discovering the necessary autonomously replicating sequence (ARS); a refined YAC utilizing this data was described in 1983 by Murray et al. The primary components of a YAC are the ARS, centromere, and telomeres from \"S. cerevisiae\". Additionally, selectable marker genes, such as antibiotic resistance and a visible marker, are utilized to select transformed yeast cells. Without these sequences, the chromosome will not be stable during extracellular replication, and would not be distinguishable from colonies without the vector.\n\nA YAC is built using an initial circular DNA plasmid, which is typically cut into a linear DNA molecule using restriction enzymes; DNA ligase is then used to ligate a DNA sequence or gene of interest into the linearized DNA, forming a single large, circular piece of DNA. The basic generation of linear yeast artificial chromosomes can be broken down into 6 main steps:\n\n1. Ligation of selectable marker into plasmid vector: this allows for the differential selection of colonies with, or without the marker gene\nAn antibiotic resistance gene allows the YAC vector to be amplified and selected for in \"E. coli\" by rescuing the ability of mutant E. coli to synthesize leucine in the presence of the necessary components within the growth medium. \"TRP1\" and \"URA3\" genes are other YAC vector cloning site for foreign DNA is located within the \"SUP4\" gene. This gene compensates for a mutation in the yeast host cell that causes the accumulation of red pigment. The host cells are normally red, and those transformed with YAC only, will form colorless colonies. Cloning of a foreign DNA fragment into the YAC causes insertional inactivation of the gene, restoring the red color. Therefore, the colonies that contain the foreign DNA fragment are red.\n\n2. Ligation of necessary centromeric sequences for mitotic stability \n3. Ligation of Autonomously Replicating Sequences (ARS) providing an origin of replication to undergo mitotic replication\nAllows the plasmid to replicate extrachromosomally, but renders the plasmid highly mitotically unstable, and easily lost without the centromeric sequences.\n\n4. Ligation of artificial telomeric sequences to convert circular plasmid into a linear piece of DNA \n\n5. Insertion of DNA sequence to be amplified (up to 1000kb)\n\n6. Transformation yeast colony \n\nIn March 2014, Jef Boeke of the Langone Medical Centre at New York University, published that his team has synthesized one of the \"S. cerevisiae\" 16 yeast chromosomes, the chromosome III, that he named synIII. The procedure involved replacing the genes in the original chromosome with synthetic versions and the finished synthesized chromosome was then integrated into a yeast cell. It required designing and creating 273,871 base pairs of DNA - fewer than the 316,667 pairs in the original chromosome.\n\nYeast expression vectors, such as YACs, YIps (yeast integrating plasmids), and YEps (yeast episomal plasmids), have an advantage over bacterial artificial chromosomes (BACs) in that they can be used to express eukaryotic proteins that require posttranslational modification. By being able to insert large fragments of DNA, YACs can be utilized to clone and assemble the entire genomes of an organism. With the insertion of a YAC into yeast cells, they can be propagated as linear artificial chromosomes, cloning the inserted regions of DNA in the process. With this completed, two process can be used to obtain a sequenced genome, or region of interest:\n\n1. Physical Mapping\n\n2. Chromosome Walking\n\nThis is significant in that it allows for the detailed mapping of specific regions of the genome. Whole human chromosomes have been examined, such as the X chromosome, generating the location of genetic markers for numerous genetic disorders and traits.\n\nYACs are significantly less stable than BACs, producing \"chimeric effects\": artifacts where the sequence of the cloned DNA actually corresponds not to a single genomic region but to multiple regions. Chimerism may be due to either co-ligation of multiple genomic segments into a single YAC, or recombination of two or more YACs transformed in the same host Yeast cell. The incidence of chimerism may be as high as 50%. Other artifacts are deletion of segments from a cloned region, and rearrangement of genomic segments (such as inversion). In all these cases, the sequence as determined from the YAC clone is different from the original, natural sequence, leading to inconsistent results and errors in interpretation if the clone's information is relied upon. Due to these issues, the Human Genome Project ultimately abandoned the use of YACs and switched to bacterial artificial chromosomes, where the incidence of these artifacts is very low. In addition to stability issues, specifically the relatively frequent occurrence of chimeric events, YACs proved to be inefficient when generating the minimum tiling path covering the entire human genome. Generating the clone libraries is time consuming. Also, due to the nature of the reliance on sequence tagged sites (STS) as a reference point when selecting appropriate clones, there are large gaps that need further generation of libraries to span. It is this additional hindrance that drove the project to utilize BACs instead. This is due to two factors:\n\n1) BACs are much quicker to generate, and when generating redundant libraries of clones, this is essential\n\n2) BACs allow more dense coverage with STSs, resulting in more complete and efficient minimum tiling paths generated in silico.\n\nHowever, it is possible to utilize both approaches, as was demonstrated when the genome of the nematode, C. elegans. There majority of the genome was tiled with BACs, and the gaps filled in with YACs.\n\n\n", "id": "394765", "title": "Yeast artificial chromosome"}
{"url": "https://en.wikipedia.org/wiki?curid=6798584", "text": "Acid guanidinium thiocyanate-phenol-chloroform extraction\n\nAcid guanidinium thiocyanate-phenol-chloroform extraction (abbreviated AGPC) is a liquid–liquid extraction technique in biochemistry. It is widely used in molecular biology for isolating RNA (as well as DNA and protein in some cases). This method may take longer than a column-based system such as the silica-based purification, but has higher purity and the advantage of high recovery of RNA: an RNA column is typically unsuitable for purification of short (<200 nucleotides) RNA species, such as siRNA, miRNA, gRNA and tRNA.\n\nIt was originally devised by Piotr Chomczynski and Nicoletta Sacchi and published in 1987. The reagent is sold by Sigma-Aldrich by the name TRI Reagent; by Invitrogen under the name TRIzol; by Bioline as Trisure; and by Tel-Test as STAT-60.\n\nThis method relies on phase separation by centrifugation of a mixture of the aqueous sample and a solution containing water-saturated phenol and chloroform, resulting in an upper aqueous phase and a lower organic phase (mainly phenol). Guanidinium thiocyanate, a chaotropic agent, is added to the organic phase to aid in the denaturation of proteins (such as those that strongly bind nucleic acids or those that degrade RNA). The nucleic acids (RNA and/or DNA) partition into the aqueous phase, while protein partitions into the organic phase. The pH of the mixture determines which nucleic acids get purified. Under acidic conditions (pH 4-6), DNA partitions into the organic phase while RNA remains in the aqueous phase. Under neutral conditions (pH 7-8), both DNA and RNA partition into the aqueous phase. In a last step, the nucleic acids are recovered from the aqueous phase by precipitation with 2-propanol. The 2-propanol is then washed with ethanol and the pellet briefly air-dried and dissolved in TE buffer or RNAse free water.\n\nGuanidinium thiocyanate denatures proteins, including RNases, and separates rRNA from ribosomal proteins, while phenol, isopropanol and water are solvents with poor solubility. In the presence of chloroform or BCP (bromochloropropane), these solvents separate entirely into two phases that are recognized by their color: a clear, upper aqueous phase (containing the nucleic acids) and a lower phase (containing the proteins dissolved in phenol and the lipids dissolved in chloroform).\nOther denaturing chemicals such as 2-mercaptoethanol and sarcosine may also be used. \nThe major downside is that phenol and chloroform are both hazardous and inconvenient materials, and the extraction is often laborious, so in recent years many companies now offer alternative ways to isolate DNA.\n\n\n\n", "id": "6798584", "title": "Acid guanidinium thiocyanate-phenol-chloroform extraction"}
{"url": "https://en.wikipedia.org/wiki?curid=40798564", "text": "Biomolecular complex\n\nBiomolecular complex, also called macromolecular complex or biomacromolecular complex, is any biological complex made of more than one molecule of protein, RNA, DNA,\n\nlipids, carbohydrates. The interactions between these biomolecules are non-covalent.\n\nExamples:\n\nThe biomacromolecular complexes are studied structurally by X-ray crystallography, NMR spectroscopy of proteins, cryo-electron microscopy and successive single particle analysis, and electron tomography.\n\nThe atomic structure models obtained by X-ray crystallography and biomolecular NMR spectroscopy can be docked into the much larger structures of biomolecular complexes obtained by lower resolution techniques like electron microscopy, electron tomography, and small-angle X-ray scattering.\n\n", "id": "40798564", "title": "Biomolecular complex"}
{"url": "https://en.wikipedia.org/wiki?curid=914098", "text": "Amplicon\n\nAn amplicon is a piece of DNA or RNA that is the source and/or product of natural or artificial amplification or replication events. It can be formed using various methods including polymerase chain reactions (PCR), ligase chain reactions (LCR), or natural gene duplication. In this context, \"amplification\" refers to the production of one or more copies of a genetic fragment or target sequence, specifically the amplicon. As the product of an amplification reaction, amplicon is used interchangeably with common laboratory terms, such as PCR product.\n\nArtificial amplification is used in research, forensics, and medicine for purposes that include detection and quantification of infectious agents, identification of human remains, and extracting genotypes from human hair.\n\nNatural gene duplication is implicated in several forms of human cancer including primary mediastinal B cell lymphoma and Hodgkin's lymphoma. Amplicons in this context can refer both to sections of chromosomal DNA that have been excised, amplified, and reinserted elsewhere in the genome, and to extrachromasomal DNA known as double minutes, each of which can be composed of one or more genes. Amplification of the genes encoded by these amplicons generally increases transcription of those genes and ultimately the volume of associated proteins.\n\nAmplicons in general are direct repeat (head-to-tail) or inverted repeat (head-to-head or tail-to-tail) genetic sequences, and can be either linear or circular in structure. Circular amplicons consist of imperfect inverted duplications annealed into a circle and are thought to arise from precursor linear amplicons.\n\nDuring artificial amplification, amplicon length is dictated by the experimental goals.\n\nAnalysis of amplicons has been made possible by the development of amplification methods such as PCR, and increasingly by cheaper and more high-throughput technologies for DNA sequencing or next-generation sequencing, such as ion semiconductor sequencing, popularly referred to as the brand of the developer, Ion Torrent.\n\nDNA sequencing technologies such as next-generation sequencing have made it possible to study amplicons in genome biology and genetics, including cancer genetics research, phylogenetic research, and human genetics.\n\nIrrespective of the approach used to amplify the amplicons, some technique must be used to quantitate the amplified product. Generally, these techniques incorporate a capture step and a detection step, although how these steps are incorporated depends on the individual assay.\n\nExamples include the Amplicor HIV-1 Monitor Assay (RT-PCR), which has the capacity to recognize HIV in plasma; the HIV-1 QT (NASBA), which is used to measure plasma viral load by amplifying a segment of the HIV RNA; and Transcription Mediated Amplification, which employs a hybridization protection assay to distinguish \"Chlamydia trachomatis\" infections. Various detection and capture steps are involved in each approach to assess the amplification product, or amplicon.\n\nPCR can be used to determine sex from a human DNA sample. The loci of Alu element insertion is selected, amplified and evaluated in terms of size of the fragment. The sex assay utilizes AluSTXa for the X chromosome, AluSTYa for the Y chromosome, or both AluSTXa and AluSTYa, to reduce the possibility of error to a negligible quantity. The inserted chromosome yields a large fragment when the homologous region is amplified. The males are distinguished as having two DNA amplicons present, while females have only a single amplicon. The kit adapted for carrying out the method includes a pair of primers to amplify the locus and optionally polymerase chain reaction reagents.\n\nLCR can be used to diagnose tuberculosis. The sequence containing protein antigen B is targeted by four oligonucleotide primers—two for the sense strand, and two for the antisense strand. The primers bind adjacent to one another, forming a segment of double stranded DNA that once separated, can serve as a target for future rounds of replication. In this instance, the product can be detected via the microparticle enzyme immunoassay (MEIA).\n\n", "id": "914098", "title": "Amplicon"}
{"url": "https://en.wikipedia.org/wiki?curid=167544", "text": "Transcription (biology)\n\nTranscription is the first step of gene expression, in which a particular segment of DNA is copied into RNA (especially mRNA) by the enzyme RNA polymerase.\nBoth DNA and RNA are nucleic acids, which use base pairs of nucleotides as a complementary language. During transcription, a DNA sequence is read by an RNA polymerase, which produces a complementary, antiparallel RNA strand called a primary transcript.\n\nTranscription proceeds in the following general steps:\n\nThe stretch of DNA transcribed into an RNA molecule is called a \"transcription unit\" and encodes at least one gene. If the gene encodes a protein, the transcription produces messenger RNA (mRNA); the mRNA, in turn, serves as a template for the protein's synthesis through translation. Alternatively, the transcribed gene may encode for either non-coding RNA (such as microRNA), ribosomal RNA (rRNA), transfer RNA (tRNA), or other enzymatic RNA molecules called ribozymes. Overall, RNA helps synthesize, regulate, and process proteins; it therefore plays a fundamental role in performing functions within a cell.\n\nIn virology, the term may also be used when referring to mRNA synthesis from an RNA molecule (i.e., RNA replication). For instance, the genome of a negative-sense single-stranded RNA (ssRNA -) virus may be template for a positive-sense single-stranded RNA (ssRNA +). This is because the positive-sense strand contains the information needed to translate the viral proteins for viral replication afterwards. This process is catalyzed by a viral RNA replicase.\n\nA DNA transcription unit encoding for a protein may contain both a \"coding sequence\", which will be translated into the protein, and \"regulatory sequences\", which direct and regulate the synthesis of that protein. The regulatory sequence before (\"upstream\" from) the coding sequence is called the five prime untranslated region (5'UTR); the sequence after (\"downstream\" from) the coding sequence is called the three prime untranslated region (3'UTR).\n\nAs opposed to DNA replication, transcription results in an RNA complement that includes the nucleotide uracil (U) in all instances where thymine (T) would have occurred in a DNA complement.\n\nOnly one of the two DNA strands serve as a template for transcription. The antisense strand of DNA is read by RNA polymerase from the 3' end to the 5' end during transcription (3' → 5'). The complementary RNA is created in the opposite direction, in the 5' → 3' direction, matching the sequence of the sense strand with the exception of switching uracil for thymine. This directionality is because RNA polymerase can only add nucleotides to the 3' end of the growing mRNA chain. This use of only the 3' → 5' DNA strand eliminates the need for the Okazaki fragments that are seen in DNA replication. This also removes the need for an RNA primer to initiate RNA synthesis, as is the case in DNA replication.\n\nThe \"non\"-template (sense) strand of DNA is called the coding strand, because its sequence is the same as the newly created RNA transcript (except for the substitution of uracil for thymine). This is the strand that is used by convention when presenting a DNA sequence.\n\nTranscription has some proofreading mechanisms, but they are fewer and less effective than the controls for copying DNA. As a result, transcription has a lower copying fidelity than DNA replication. \n\nTranscription is divided into \"initiation\", \"promoter escape\", \"elongation,\" and \"termination\".\n\nTranscription begins with the binding of RNA polymerase, together with one or more general transcription factor, to a specific DNA sequence referred to as a \"promoter\" to form an RNA polymerase-promoter \"closed complex\". In the \"closed complex\" the promoter DNA is still fully double-stranded.\n\nRNA polymerase, assisted by one or more general transcription factors, then unwinds approximately 14 base pairs of DNA to form an RNA polymerase-promoter \"open complex\". In the \"open complex\" the promoter DNA is partly unwound and single-stranded. The exposed, single-stranded DNA is referred to as the \"transcription bubble.\"\n\nRNA polymerase, assisted by one or more general transcription factors, then selects a transcription start site in the transcription bubble, binds to an initiating NTP and an extending NTP (or a short RNA primer and an extending NTP) complementary to the transcription start site sequence, and catalyzes bond formation to yield an initial RNA product.\n\nIn bacteria, RNA polymerase holoenzyme consists of five subunits: 2 α subunits, 1 β subunit, 1 β' subunit, and 1 ω subunit. In bacteria, there is one general RNA transcription factor: sigma. RNA polymerase core enzyme binds to the bacterial general transcription factor sigma to form RNA polymerase holoenzyme and then binds to a promoter.\n( RNA polymerase is called a holoenzyme when sigma subunit is attached to the core enzyme which is consist of 2 α subunits, 1 β subunit, 1 β' subunit only ) \n\nIn archaea and eukaryotes, RNA polymerase contains subunits homologous to each of the five RNA polymerase subunits in bacteria and also contains additional subunits. In archaea and eukaryotes, the functions of the bacterial general transcription factor sigma are performed by multiple general transcription factors that work together. In archaea, there are three general transcription factors: TBP, TFB, and TFE. In eukaryotes, in RNA polymerase II-dependent transcription, there are six general transcription factors: TFIIA, TFIIB (an ortholog of archaeal TFB), TFIID (a multisubunit factor in which the key subunit, TBP, is an ortholog of archaeal TBP), TFIIE (an ortholog of archaeal TFE), TFIIF, and TFIIH. In archaea and eukaryotes, the RNA polymerase-promoter closed complex is usually referred to as the \"preinitiation complex.\"\n\nTranscription initiation is regulated by additional proteins, known as activators and repressors, and, in some cases, associated coactivators or corepressors, which modulate formation and function of the transcription initiation complex.\n\nAfter the first bond is synthesized, the RNA polymerase must escape the promoter. During this time there is a tendency to release the RNA transcript and produce truncated transcripts. This is called abortive initiation, and is common for both eukaryotes and prokaryotes. Abortive initiation continues to occur until an RNA product of a threshold length of approximately 10 nucleotides is synthesized, at which point promoter escape occurs and a transcription elongation complex is formed.\n\nMechanistically, promoter escape occurs through DNA scrunching, providing the energy needed to break interactions between RNA polymerase holoenzyme and the promoter.\n\nIn bacteria, upon and following promoter clearance, the σ factor is released according to a stochastic model.\n\nIn eukaryotes, at an RNA polymerase II-dependent promoter, upon promoter clearance, TFIIH phosphorylates serine 5 on the carboxy terminal domain of RNA polymerase II, leading to the recruitment of capping enzyme (CE). The exact mechanism of how CE induces promoter clearance in eukaryotes is not yet known.\n\nOne strand of the DNA, the \"template strand\" (or noncoding strand), is used as a template for RNA synthesis. As transcription proceeds, RNA polymerase traverses the template strand and uses base pairing complementarity with the DNA template to create an RNA copy. Although RNA polymerase traverses the template strand from 3' → 5', the coding (non-template) strand and newly formed RNA can also be used as reference points, so transcription can be described as occurring 5' → 3'. This produces an RNA molecule from 5' → 3', an exact copy of the coding strand (except that thymines are replaced with uracils, and the nucleotides are composed of a ribose (5-carbon) sugar where DNA has deoxyribose (one fewer oxygen atom) in its sugar-phosphate backbone).\n\nmRNA transcription can involve multiple RNA polymerases on a single DNA template and multiple rounds of transcription (amplification of particular mRNA), so many mRNA molecules can be rapidly produced from a single copy of a gene. The characteristic elongation rates in prokaryotes and eukaryotes are about 10-100 nts/sec. In eukaryotes, however, nucleosomes act as major barriers to transcribing polymerases during transcription elongation. In these organisms, the pausing induced by nucleosomes can be regulated by transcription elongation factors such as TFIIS.\n\nElongation also involves a proofreading mechanism that can replace incorrectly incorporated bases. In eukaryotes, this may correspond with short pauses during transcription that allow appropriate RNA editing factors to bind. These pauses may be intrinsic to the RNA polymerase or due to chromatin structure.\n\nBacteria use two different strategies for transcription termination – Rho-independent termination and Rho-dependent termination. In Rho-independent transcription termination, RNA transcription stops when the newly synthesized RNA molecule forms a G-C-rich hairpin loop followed by a run of Us. When the hairpin forms, the mechanical stress breaks the weak rU-dA bonds, now filling the DNA–RNA hybrid. This pulls the poly-U transcript out of the active site of the RNA polymerase, terminating transcription. In the \"Rho-dependent\" type of termination, a protein factor called \"Rho\" destabilizes the interaction between the template and the mRNA, thus releasing the newly synthesized mRNA from the elongation complex.\n\nTranscription termination in eukaryotes is less well understood than in bacteria, but involves cleavage of the new transcript followed by template-independent addition of adenines at its new 3' end, in a process called polyadenylation.\n\nTranscription inhibitors can be used as antibiotics against, for example, pathogenic bacteria (antibacterials) and fungi (antifungals). An example of such an antibacterial is rifampicin, which inhibits Bacterial transcription of DNA into mRNA by inhibiting DNA-dependent RNA polymerase by binding its beta-subunit. 8-Hydroxyquinoline is an antifungal transcription inhibitor. The effects of histone methylation may also work to inhibit the action of transcription.\n\nIn vertebrates, the majority of gene promoters contain a CpG island with numerous CpG sites. When many of a gene's promoter CpG sites are methylated the gene becomes inhibited (silenced). Colorectal cancers typically have 3 to 6 driver mutations and 33 to 66 hitchhiker or passenger mutations. However, transcriptional inhibition (silencing) may be of more importance than mutation in causing progression to cancer. For example, in colorectal cancers about 600 to 800 genes are transcriptionally inhibited by CpG island methylation (see regulation of transcription in cancer). Transcriptional repression in cancer can also occur by other epigenetic mechanisms, such as altered expression of microRNAs. In breast cancer, transcriptional repression of BRCA1 may occur more frequently by over-expressed microRNA-182 than by hypermethylation of the BRCA1 promoter (see Low expression of BRCA1 in breast and ovarian cancers).\n\nActive transcription units are clustered in the nucleus, in discrete sites called transcription factories or euchromatin. Such sites can be visualized by allowing engaged polymerases to extend their transcripts in tagged precursors (Br-UTP or Br-U) and immuno-labeling the tagged nascent RNA. Transcription factories can also be localized using fluorescence in situ hybridization or marked by antibodies directed against polymerases. There are ~10,000 factories in the nucleoplasm of a HeLa cell, among which are ~8,000 polymerase II factories and ~2,000 polymerase III factories. Each polymerase II factory contains ~8 polymerases. As most active transcription units are associated with only one polymerase, each factory usually contains ~8 different transcription units. These units might be associated through promoters and/or enhancers, with loops forming a \"cloud\" around the factor.\n\nA molecule that allows the genetic material to be realized as a protein was first hypothesized by François Jacob and Jacques Monod. Severo Ochoa won a Nobel Prize in Physiology or Medicine in 1959 for developing a process for synthesizing RNA \"in vitro\" with polynucleotide phosphorylase, which was useful for cracking the genetic code. RNA synthesis by RNA polymerase was established \"in vitro\" by several laboratories by 1965; however, the RNA synthesized by these enzymes had properties that suggested the existence of an additional factor needed to terminate transcription correctly.\n\nIn 1972, Walter Fiers became the first person to actually prove the existence of the terminating enzyme.\n\nRoger D. Kornberg won the 2006 Nobel Prize in Chemistry \"for his studies of the molecular basis of eukaryotic transcription\".\n\nTranscription can be measured and detected in a variety of ways:\n\nSome viruses (such as HIV, the cause of AIDS), have the ability to transcribe RNA into DNA. HIV has an RNA genome that is \"reverse transcribed\" into DNA. The resulting DNA can be merged with the DNA genome of the host cell. The main enzyme responsible for synthesis of DNA from an RNA template is called reverse transcriptase.\n\nIn the case of HIV, reverse transcriptase is responsible for synthesizing a complementary DNA strand (cDNA) to the viral RNA genome. The enzyme ribonuclease H then digests the RNA strand, and reverse transcriptase synthesises a complementary strand of DNA to form a double helix DNA structure (\"cDNA\"). The cDNA is integrated into the host cell's genome by the enzyme integrase, which causes the host cell to generate viral proteins that reassemble into new viral particles. In HIV, subsequent to this, the host cell undergoes programmed cell death, or apoptosis of T cells. However, in other retroviruses, the host cell remains intact as the virus buds out of the cell.\n\nSome eukaryotic cells contain an enzyme with reverse transcription activity called telomerase. Telomerase is a reverse transcriptase that lengthens the ends of linear chromosomes. Telomerase carries an RNA template from which it synthesizes a repeating sequence of DNA, or \"junk\" DNA. This repeated sequence of DNA is called a telomere and can be thought of as a \"cap\" for a chromosome. It is important because every time a linear chromosome is duplicated, it is shortened. With this \"junk\" DNA or \"cap\" at the ends of chromosomes, the shortening eliminates some of the non-essential, repeated sequence rather than the protein-encoding DNA sequence, that is farther away from the chromosome end.\n\nTelomerase is often activated in cancer cells to enable cancer cells to duplicate their genomes indefinitely without losing important protein-coding DNA sequence. Activation of telomerase could be part of the process that allows cancer cells to become \"immortal\". The immortalizing factor of cancer via telomere lengthening due to telomerase has been proven to occur in 90% of all carcinogenic tumors \"in vivo\" with the remaining 10% using an alternative telomere maintenance route called ALT or Alternative Lengthening of Telomeres.\n\n\n", "id": "167544", "title": "Transcription (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=41047668", "text": "Phenol–chloroform extraction\n\nPhenol–chloroform extraction is a liquid-liquid extraction technique in molecular biology used to purify nucleic acids and eliminate proteins and lipids. \n\nAqueous samples, lysed cells, or homogenised tissue are mixed with equal volumes of a phenol:chloroform mixture. After mixing, the mixture is centrifuged and two distinct phases are formed, because the phenol:chloroform mixture is immiscible with water. The aqueous phase is on top because it is less dense than the organic phase (phenol:chloroform). The proteins and hydrophobic lipids will partition into the lower organic phase while the nucleic acids (as well as other contaminants such as salts, sugars, etc.) remain in the upper aqueous phase. The upper aqueous phase is pipetted off and care is taken to avoid pipetting any of the organic phase or material at the interface. This procedure is often performed multiple times to increase the purity of the DNA.\n\nIf the mixture is acidic, DNA will precipitate into the organic phase while RNA remains in the aqueous phase due to DNA being more readily neutralised than RNA.\n\n", "id": "41047668", "title": "Phenol–chloroform extraction"}
{"url": "https://en.wikipedia.org/wiki?curid=40882801", "text": "Propidium monoazide\n\nPropidium monoazide (PMA) is a photoreactive DNA-binding dye that preferentially binds to dsDNA. It is used to detect viable microorganisms by qPCR. Visible light (high power halogen lamps or specific LED devices) induces a photoreaction of the chemical that will lead to a covalent bond with PMA and the dsDNA. This will render the DNA non-amplificable by PCR. Dead microorganisms lose their capability to maintain their membranes, which leaves the \"naked\" DNA in the cytosol ready to react with PMA. Living organisms don't react to the PMA, as they have an intact cell membrane. After treatment with the chemical, only the DNA from living bacteria is usable in qPCR, allowing to obtain only the amplified DNA of living organisms. This is helpful in determining which pathogens are active in specific samples. The main use of PMA is in Viability PCR but the same principle can be applied in flow cytometry or fluorescence microscopy.\n\nPMA was developed at Biotium, Inc. as an improvement on ethidium monoazide (EMA). PMA provides better discrimination between live and dead bacteria because it is excluded from live cells more efficiently than EMA.\n", "id": "40882801", "title": "Propidium monoazide"}
{"url": "https://en.wikipedia.org/wiki?curid=41578185", "text": "Broadly neutralizing HIV-1 antibodies\n\nBroadly Neutralizing HIV-1 Antibodies (bNAbs) are neutralizing antibodies which neutralize multiple HIV-1 viral strains. bNAbs are unique in that they target conserved epitopes of the virus, meaning the virus may mutate, but the targeted epitopes will still exist. In contrast, non-bNAbs are specific for individual viral strains with unique epitopes. The discovery of bNAbs has led to an important area of research, namely, discovery of a vaccine, not only limited to HIV, but also other rapidly mutating viruses like Influenza, etc.\n\nThe following table shows the characteristics of various HIV-1 bNAbs\n\nRecent years have seen an increase in HIV-1 bNAb discovery.\n\nOnline databases like bNAber and LANL constantly report and update the discovery of new HIV bNAbs.\n\n", "id": "41578185", "title": "Broadly neutralizing HIV-1 antibodies"}
{"url": "https://en.wikipedia.org/wiki?curid=41719778", "text": "Epitranscriptomics\n\nEpitranscriptomics describes an aspect of molecular genetics, or a study thereof, that depends on biochemical modifications of RNA. By analogy to the term epigenetics, described as \"functionally relevant changes to the genome that do not involve a change in the nucleotide sequence\", epitranscriptomics can be defined as a functionally relevant changes to the transcriptome that do not involve a change in the ribonucleotide sequence. The epitranscriptome, therefore, is defined as the ensemble of such functionally relevant changes.\n\nThere are several types of RNA modifications that impact gene expression. These modifications happen to all types of cellular RNA including, but not limited to, ribosomal RNA(rRNA), transfer RNA (tRNA), messenger RNA (mRNA), and small nuclear RNA (snRNA). There are more than one hundred documented RNA modifications, which are described in a database maintained by the RNA Institute at the University at Albany. The most common and well-understood mRNA modification at present is the N-Methyladenosine (mA), which has been observed to occur an average of three times in every mRNA molecule.\n\nThe relative youth of this field means there is still much progress to be made in characterizing all modifications to the transcriptome and elucidating their mechanisms of action. Once these questions are answered and biologists have a better sense of the amount of variation in RNA modification, the focus will turn to each modification’s biological function. This has already been investigated in a select few proteins such as adenosine deaminase, which acts on RNA (ADAR). ADAR has been shown to affect antibody production and the innate immune system as well as transcripts encoding important receptors for the central nervous system. This plurality in function has caused some scientists to speculate that the epitransciptome may be even more expansive than the better defined epigenome.\n\nIt has been recently discovered that some modification like RNA methylation, may play an active role in regulating some biological process. Also that reversible modifications might occur on tRNA, rRNA, and mRNAs to regulate gene expression and affect biological process.\n\nModifying an RNA molecule requires a significant amount of energy from the cell This high energy cost must mean that the cell deems these modifications to be very important. There are three known types of RNA modifications. rRNA modifications, changes that help with molecular recognition, and mRNA modifications.\n\nrRNA changes take place in areas of high translational activity, such as the PTC(peptidyl transferase center). Some modifications include pseudouridines, 2′-\"O\"-methylations on backbone sugars, and methylated bases. It is not well known what the biological effects of these modifications are on the rRNA molecule but the hypothesis is that they help stabilize the structure and function of the ribosome especially during ribosome replication. Some functions of these modifications is that the 2' O methyl does not allow the hydrolysis of the phosphate backbone of the rRNA. It also allows for more base stacking forces. Which creates a more stable secondary structure. These modifications are required around the PTC to help with translational efficiency.\n\nThe known purposes for this kind of modification is to allow for a bigger difference between the tRNA molecules so that tRNA is distinguished from elongator tRNA and tRNA stability. Some studies have shown the modifications of tRNA can be a dynamic and adaptive to the changes for the environment. Examples include the tRNA methyltranserfase (Trm4) which methylates a cysteine group in response to the depletion of nutrients in the body.\n\nBecause mRNA is the essential code that changes our biological information from DNA to protein, through transcription and translation, the epitranscriptomic modifications that occur on the mRNA must be nonmutagenic and thus most modifications are methylations. The four sites on mRNA for methylation are \"N7\"-methylguanine (at the 5′ cap), \"N6\"-methyl adenosine, 5-methylcytosine, and 2′-\"O\"-methylation. The cap in mRNA is very important it helps the to recognition by cap-binding proteins to initiate translation, and prevents the degradation of mRNA in the cytosol.\n\nThe most common modification of eukaryotic mRNA. mA is applied by a mA methyltransferase complex post-transcriptionally . It was only until recent years that scientists have been able to examine and try to identify the purpose of the mRNA modifications. It has been shown that three members of the human YTH domain family proteins show higher binding affinities to methylated mRNA. The YTH protein, YTHDF2, affects mRNA by directing methylated mRNA from the translational pool to mRNA decay sites. It was also found that the mRNA that was methylated has a shorter half-life and that it is filled with regulatory sequences. This suggests that the methylation has a big control on the effect of gene expression. Some mA demethylases, such as FTO and AlkBH5, were discovered. Their function and knockout study reveals the affects they have on the body. FTO has correlations with body weight and disease, while \"Alkbh5\" knockout mice have impaired fertility. These demethylations serve as proof that the mA methylation that occurs on mRNA is very important for bodily function.\n\nAn epigenetic modification that is known to occur in rRNA and tRNA, but it has been recently discovered that there are many mC sites in mRNA. Genes that deal with energy and lipid metabolism are found near these sites which may indicate mC involvement in this type of regulation.\n\nIs involved in differentiating between self and non-self mRNA. Without 2′-\"O\"-methylation activity the immune system triggers higher levels of type 1 interferon activity. It is also seen that the 2′-\"O\"-methylation occurs in plants and animals to help protect the 3' end of miRNA(microRNA) from polyadenylation.\n\nmA describes the methylation of the nitrogen at position 6 of the adenosine base within mRNA. Discovered in 1974, mA is the most abundant eukaryotic mRNA modification. The term \"epitranscriptome\" was coined following transcriptome-wide mappings of mA sites, but does not necessarily exclude other post-transcriptional mRNA modifications.\n\nmA methylation regulates nuclear export of mature mRNA and mRNA stability. How, and in response to what stimulus, the cell endogeneously regulates the level of mA methylation remains unclear at present.\n\nThis mA modification has a notable effect on translational dynamics. As expected, because it is just a modified adenosine base, mA base-pairs with uridine during decoding. However, the adenosine’s methylation hinders tRNA accommodation and translation elongation. When an mA-modified codon interacts with its cognate tRNA, it acts more like a near-cognate codon interaction instead of the cognate codon interaction. This can be seen in that the delay in the tRNA accommodation, which is dependent upon both the position of the mA in the mRNA codons and on how accurate the translation is. To summarize, translation-elongation dynamics are slower for codons with mA and different locations of these modified nucleotides in the mRNA codons affect decoding dynamics in different ways.\n\nThe mA modification affects tRNA incorporation’s distinct steps in different ways. It slows down GTP hydrolysis by EF-Tu 12-fold, delays tRNA accommodation two-fold, and it slows down peptidyl transfer two-fold. It also causes a 1.5-fold increase in the amount of GTP hydrolyzed per peptidyl transfer, which indicates that a lot of proofreading is required. Overall, this mA modification leads to a kinetic loss of a factor of 18.\n\nThe terms “eraser” and “reader” have been associated with RNA modification. “Eraser” is a general term to describe an enzyme that de-methylates mA. Changes that mutate the gene encoding the “eraser” enzyme lead to obesity and cancer. “Reader” proteins are involved in gene expression where there are abundant mA; “reader” proteins have a higher propensity to bind with greater affinity, while the de-methylated form has been reported to have a decreased binding affinity.\n\nmRNA are subject to layers of regulatory gene expression. One known mechanism involves the formation of RNA stem-loops. Stem-loops occur when complementary bases within a single-stranded RNA molecule form Watson-Crick base pairs on the stem while forming an unpaired end or loop. Stem-loops do not have one definite function, but a plethora of functions. In the case of the mA regulatory mechanism, it is involved in alternative splicing. These stem and loop structures are subject to alterations regarding changes in pH, temperature, ion concentrations, binding nature of proteins and also nucleic acids.\n\nmA has been observed to be located within the loops opposite of the HNRNPC binding site. HNRNPC is single-stranded RNA binding protein where it participates in post-transcriptional regulation, specifically alternative splicing. HNRNPC protein binds to its site (uridine rich region on the stem loop) when methylated adenosine is present.The HNRNPC binding site on the mRNA consists of an abundance of uridine nucleotides. Methylated adenosine residues destabilize the hairpin structure, elongating the uridine nucleotide stretch, causing the binding site to be more accessible for efficient HNRNPC protein binding.\n\nEvidence supporting this claim identified that decreased mA levels in the transcriptome lead to significantly reduced HNRNPC binding, thus alternative splicing is co-regulated by methylation and HNRNPC binding activity. However, the mA modification does not directly cause protein binding. Rather, it alters the loop structure to regulate gene expression by acting as a switch that exposes the HNRNPC region. It is essentially a two-step, co-regulatory mechanism prevalent in biochemistry in controlling alternative splicing. Note that demethylase enzyme can indeed “erase” the methyl group, thus inhibiting alternative splicing which is the reverse of the two-step regulation mechanism.\n\n mRNA capping in eukaryotes have shown to exist and have been studied extensively regarding the 5’ 7-methylguanylate cap and its poly-A tail. It provides RNA stability, processing, localization and translational efficiency. Recently, researchers have proven a similar mechanism in RNA pre-processing in prokaryotes. In prokaryotic (E.coli) mRNA, the 5’ is capped with nicotinamide adenine dinucleotide (NAD+ or NADH) or 3’-desphospho-coenzyme A (dpCoA). It was previously thought that the NAD+,NADH, dpCoA modification occurs after transcription analogously with the 7-methylguanylate cap; however, recently it has been shown that modifications are incorporated during transcription initiation by acting as a non-canonical initiating nucleotides (NCIN) for \"de novo\" transcription by cellular RNA polymerase. RppH and NudC pyrophosphohydrolases cleaves specific phosphate bonds to eliminate the cap modification. ATP was initially known to cap an RNA product in prokaryotes while NAD+, NADH and dpCoA was still being studied. RppH specifically cleaves 5’-triphosphate and 5’-diphosphate RNAs to 5’-monophosphate RNA products which only targets the ATP capped RNAs. On the other hand, NudC cleaves 5’-(NADH,NAD+,dpCoA) capped RNAs to 5’-monophosphate RNAs but does not cleave ATP capped RNAs. This discovery of two specific hydrolases that target specific products lead to the discovery of NCIN-mediated transcription.\nBoth prokaryotes and eukaryotes transcripts have been observed with x-ray crystallography to have a NCIN-capped RNA product in addition to being observed under \"in vivo\" condition. The efficiency of NCIN capping is largely influenced by promoter sequence especially the -35 box and -10 box upstream of the transcription start site. Studies so far have defined the mechanism and structural basis of NCIN mediated capping but the function of these caps are still to be discovered. Recent consensus is that NCIN-mediated \"ab initio\" capping occurs in all organisms.\n\nPseudouridine is a modified nucleoside found within non-coding RNAs (ncRNA). It increases the function of tRNA and rRNA by stabilizing the structure. Though mRNAs are not known for containing pseudouridine, the artificial process of pseudouridylation has an effect on the function of mRNA: it changes the genetic code by making non-canonical base pairing possible in the ribosome decoding center. Pseudouridine also provides more hydrogen bonding opportunity and makes the RNA backbone more rigid.\n\nA certain paper looks at pseudouridylation in yeast and human RNAs using pseudo-seq, a process that utilizes a single-nucleotide-resolution method for pseudouridine identification. It identifies the known modification sites as well as other sites in ncRNAs in addition to the many pseudouridylated sites in mRNA. Other methods outside os pseudo-seq that have been utilized for pseudouridine identification and detection range from pseudouridine-seq, PSI-seq, and CeU-seq.\n\nThere are more than 100 classes of RNA modifications that have been found, a majority of which are in tRNA and rRNA while only three modified nucleotides have been discovered inside a coding sequence of mRNA: mA, 5-methylcytosine (mC), and inosine. The relevance of pseudouridine in rRNA has been documented where pseudouridylation is required for ribosome biogenesis and translational accuracy. The importance of pseudouridylation additionally seen in snRNA where specific pseudouridine residues have been located and identified as a needed component of pre-mRNA splicing. Previous research has focused specifically at the modifications of U2 spliceosomal RNA which are required for snRNP assembly and pre-mRNA splicing. U2 in comparison to other spliceosomal snRNAs has the most extensive modifications, including 13 pseudouridines as well as a 5' trim ethyl guanosine (TMG) cap and others. It is a mixture of modifications involving that of the pseudouridine residues that assist in the complete assembly of the 17S U2 snRNP particle needed for splicing.\n\nPseudouridylation has been found to be inducible in response to stress and differentiation in the cell, giving reason to believe that pseudouridylation may act as an important regulator mechanism for RNA function. Much of the regulation in regards to pseudouridylation is regulated through the environment. In yeast this may be nutrient deprivation and in humans it is the serum starvation. When looking at yeast, research has utilized perturbing pseudouridine synthases deletion strains grown to high density and identified mRNA targets for each PUS protein. Results came back showing that most mRNA targets showed increase modification during post-diauxic growth. The pseudo-seq method identified 96 pseudouridines in 89 mRNAs, similar to yeast the growth of pseudouridine was regulated by cellular growth state. This approach provides an analysis of RNA pseudouridylation with single-nucleotide resolution and shows endogenous mRNAs are specifically pseudouridylated in a highly regulated manner in yeast and human cells. mRNA pseudouridylation could also bring a change in translation initiation efficiency, RNA localization, and other processes all cause pseuduridine stabilizes RNA structure.\n\n\n", "id": "41719778", "title": "Epitranscriptomics"}
{"url": "https://en.wikipedia.org/wiki?curid=714053", "text": "Transduction (genetics)\n\nTransduction is the process by which foreign DNA is introduced into a cell by a virus or viral vector. An example is the viral transfer of DNA from one bacterium to another. Transduction does not require physical contact between the cell donating the DNA and the cell receiving the DNA (which occurs in conjugation), and it is DNase resistant (transformation is susceptible to DNase). Transduction is a common tool used by molecular biologists to stably introduce a foreign gene into a host cell's genome (both bacterial and mammalian cells).\n\nWhen viruses, including bacteriophages (viruses that infect bacteria), infect bacterial cells, their normal mode of reproduction is to harness the replicational, transcriptional, and translation machinery of the host bacterial cell to make numerous virions, or complete viral particles, including the viral DNA or RNA and the protein coat.\n\nTransduction was discovered by Norton Zinder and Joshua Lederberg at the University of Wisconsin–Madison in 1952 in Salmonella.\n\nTransduction happens through either the lytic cycle or the lysogenic cycle.\nIf the lysogenic cycle is adopted, the phage chromosome is integrated (by covalent bonds) into the bacterial chromosome, where it can remain dormant for thousands of generations. If the lysogen is induced (by UV light for example), the phage genome is excised from the bacterial chromosome and initiates the lytic cycle, which culminates in lysis of the cell and the release of phage particles. The lytic cycle leads to the production of new phage particles which are released by lysis of the host.\"\n\nThe packaging of bacteriophage DNA has low fidelity and small pieces of bacterial DNA, together with the bacteriophage genome, may become packaged into the bacteriophage genome. At the same time, some phage genes are left behind in the bacterial chromosome.\n\nThere are generally three types of recombination events that can lead to this incorporation of bacterial DNA into the viral DNA, leading to two modes of recombination.\n\nGeneralized transduction is the process by which any bacterial gene may be transferred to another bacterium via a bacteriophage, and very rarely a small number of phages carry the donor(bacterial genome)genome,(1 phage in 10.000 ones carry the donor genome). In essence, this is the packaging of bacterial DNA into a viral envelope. This may occur in two main ways, recombination and headful packaging.\n\nIf bacteriophages undertake the lytic cycle of infection upon entering a bacterium, the virus will take control of the cell’s machinery for use in replicating its own viral DNA. If by chance bacterial chromosomal DNA is inserted into the viral capsid which is usually used to encapsulate the viral DNA, the mistake will lead to \"generalized transduction\".\n\nIf the virus replicates using 'headful packaging', it attempts to fill the nucleocapsid with genetic material. If the viral genome results in spare capacity, viral packaging mechanisms may incorporate bacterial genetic material into the new virion.\n\nThe new virus capsule now loaded with part bacterial DNA continues to infect another bacterial cell. This bacterial material may become recombined into another bacterium upon infection.\n\nWhen the new DNA is inserted into this recipient cell it can fall to one of three fates\n\n\n\"Specialized transduction\" is the process by which a \"restricted\" set of bacterial genes is transferred to another bacterium. The genes that get transferred (donor genes) depend on where the phage genome is located on the chromosome. Specialized transduction occurs when the prophage excises imprecisely from the chromosome so that bacterial genes lying adjacent to the prophage are included in the excised DNA. The excised DNA is then packaged into a new virus particle, which then delivers the DNA to a new bacterium, where the donor genes can be inserted into the recipient chromosome or remain in the cytoplasm, depending on the nature of the bacteriophage.\n\nWhen the partially encapsulated phage material infects another cell and becomes a \"prophage\" (is covalently bonded into the infected cell's chromosome), the partially coded prophage DNA is called a \"heterogenote\".\n\nAn example of specialized transduction is λ phage in \"Escherichia coli\".\n\nTransduction with viral vectors can be used to insert or modify genes in mammalian cells. It is often used as a tool in basic research and is actively researched as a potential means for gene therapy. In these cases, a plasmid is constructed in which the genes to be transferred are flanked by viral sequences that are used by viral proteins to recognize and package the viral genome into viral particles. This plasmid is inserted (usually by transfection) into a producer cell together with other plasmids (DNA constructs) that carry the viral genes required for formation of infectious virions. In these producer cells, the viral proteins expressed by these packaging constructs bind the sequences on the DNA/RNA (depending on the type of viral vector) to be transferred and insert it into viral particles. For safety, none of the plasmids used contains all the sequences required for virus formation, so that simultaneous transfection of multiple plasmids is required to get infectious virions. Moreover, only the plasmid carrying the sequences to be transferred contains signals that allow the genetic materials to be packaged in virions, so that none of the genes encoding viral proteins are packaged. Viruses collected from these cells are then applied to the cells to be altered. The initial stages of these infections mimic infection with natural viruses and lead to expression of the genes transferred and (in the case of lentivirus/retrovirus vectors) insertion of the DNA to be transferred into the cellular genome. However, since the transferred genetic material does not encode any of the viral genes, these infections do not generate new viruses (the viruses are \"replication-deficient\").\n\n\n\n", "id": "714053", "title": "Transduction (genetics)"}
{"url": "https://en.wikipedia.org/wiki?curid=41759070", "text": "Long interspersed nuclear element\n\nLong interspersed nuclear elements (LINEs) (also known as Long interspersed nucleotide elements or Long interspersed elements) are a group of non-LTR (long terminal repeat) retrotransposons which are widespread in the genome of many eukaryotes. They make up around 21.1% of the human genome. LINEs make up a family of transposons, where each LINE is about 7000 base pairs long. LINEs are transcribed into mRNA and translated into protein that acts as a reverse transcriptase. The reverse transcriptase makes a DNA copy of the LINE RNA that can be integrated into the genome at a new site. The only abundant LINE in humans is LINE-1. Our genome contains an estimated 10 truncated and 4 × 10 full-length LINE-1 elements. Due to the accumulation of random mutations, the sequence of many LINES has degenerated to the extent that they are no longer transcribed or translated. Comparisons of LINE DNA sequences can be used to date transposon insertion in the genome.\n\nThe first description of an approximately 6.4 kb long LINE-derived sequence was published by J. Adams \"et al.\" in 1980.\n\nBased on structural features and the phylogeny of its key enzyme, the reverse transcriptase (RT), LINEs are grouped into five main groups, called L1, RTE, R2, I and Jockey, which can be subdivided into at least 28 clades.\n\nIn plant genomes, so far only LINEs of the L1 and RTE clade have been reported. Whereas L1 elements diversify into several subclades, RTE-type LINEs are highly conserved, often constituting a single family.\n\nIn fungi, Tad, L1, CRE, Deceiver and Inkcap-like elements have been identified, with Tad-like elements appearing exclusively in fungal genomes.\n\nAll LINEs encode a least one protein, ORF2, which contains an RT and an endonuclease (EN) domains. Except for the evolutionary ancient R2 and RTE superfamilies, LINEs usually encode for another protein named ORF1. LINE elements are relatively rare compared to LTR-retrotransposons in plants, fungi or insects, but are dominant in vertebrates and especially in mammals, where they represent around 20% of the genome.\n\nThe LINE-1/L1-element is the only element that is still active in the human genome today. It is found in all mammals.\n\nRemnants of L2 and L3 elements are found in the human genome. It is estimated, that L2 and L3 elements were active ~200-300 million years ago. Unlike L1 elements, L2 and L3 elements lack flanking target site duplications.\n\nIn the first human genome draft the fraction of LINE elements of the human genome was given as 21% and their copy number as 850,000. Of these, L1, L2 and L3 elements made up 516,000, 315,000 and 37,000 copies, respectively. The non-autonomous SINE elements which depend on L1 elements for their proliferation make up 13% of the human genome and have a copy number of around 1.5 million. Recent estimates show the typical human genome contains on average 100 L1 elements with potential for mobilization, however there is a fair amount of variation and some individuals may contain a larger number of active L1 elements, making these individuals more prone to L1-induced mutagenesis.\n\nIncreased L1 copy numbers have also been found in the brains of people with schizophrenia, indicating that LINE elements may play a role in some neuronal diseases.\n\nLINE elements propagate by a so-called target primed reverse transcription mechanism (TPRT), which was first described for the R2 element from the silkworm \"Bombyx mori.\"\n\nORF2 (and ORF1 when present) proteins primarily associate in cis with their encoding mRNA, forming a ribonucleoprotein (RNP) complex, likely composed of two ORF2s and an unknown number of ORF1 trimers. The complex is transported back into the nucleus, where the ORF2 endonuclease domain opens the DNA (at TTAAAA hexanucleotide motifs in mammals). Thus, a 3'OH group is freed for the reverse transcriptase to prime reverse transcription of the LINE RNA transcript. Following the reverse transcription the target strand is cleaved and the newly created cDNA is integrated\n\nNew insertions create short TSDs, and the majority of new inserts are severely 5’-truncated (average insert size of 900pb in humans) and often inverted (Szak et al., 2002). Because they lack their 5’UTR, most of new inserts are non functional.\n\nIt has been shown that host cells regulate L1 retrotransposition activity, for example through epigenetic silencing.\nFor example, the RNA interference (RNAi) mechanism of small interfering RNAs derived from L1 sequences can cause suppression of L1 retrotransposition.\n\nIn plant genomes, epigenetic modification of LINEs can lead to expression changes of nearby genes and even to phenotypic changes: In the oil palm genome, methylation of a Karma-type LINE underlies the somaclonal, 'mantled' variant of this plant, responsible for drastic yield loss.\n\nHuman APOBEC3C mediated restriction of LINE-1 elements were reported and it is due to the interaction between A3C with the ORF1p that affects the reverse transcriptase activity.\n\nA historic example of L1 conferred disease is Haemophilia A, which is cased by insertional mutagenesis, aberrant splicing, recombination, there are nearly 100 examples of known diseases caused by retroelement insertions that includes cancer and neurological disorders Correlations between L1 mobilization and oncogenesis have been reported for epithelial cell cancer (carcinoma). Shift work sleep disorder is associated with increased cancer risk because light exposure at night reduces melatonin, a hormone that has been shown to reduce L1-induced genome instability.\n", "id": "41759070", "title": "Long interspersed nuclear element"}
{"url": "https://en.wikipedia.org/wiki?curid=42243737", "text": "Min System\n\nThe Min System is a mechanism composed of three proteins MinC, MinD, and MinE used by \"E. coli\" as a means of properly localizing the septum prior to cell division. Each component participates in generating a dynamic oscillation of FtsZ protein inhibition between the two bacterial poles to precisely specify the mid-zone of the cell, allowing the cell to accurately divide in two. This system is known to function in conjunction with a second negative regulatory system, the nucleoid occlusion system (NO), to ensure proper spatial and temporal regulation of chromosomal segregation and division.\n\nThe initial discovery of this family of proteins is attributed to Adler et al. (1967). First identified as \"E. coli\" mutants that could not produce a properly localized septum, resulting in the generation of minicells\n\ndue to mislocalized cell division occurring near the bacterial poles. This caused miniature vesicles to pinch off, void of essential molecular constituents permitting it to exist as a viable bacterial cell. Minicells are achromosomal cells that are products of aberrant cell division, and contain RNA and protein, but little or no chromosomal DNA. This finding led to the identification of three interacting proteins involved in a dynamic system of localizing the mid-zone of the cell for properly controlled cell division.\n\nThe Min proteins prevent the FtsZ ring from being placed anywhere but near the mid cell and are hypothesized to be involved in a spatial regulatory mechanism that links size increases prior to cell division to FtsZ polymerization in the middle of the cell.\n\nOne model of Z-ring formation permits its formation only after a certain spatial signal that tells the cell that it is big enough to divide.\n\nThe MinCDE system prevents FtsZ polymerization near certain parts of the plasma membrane. MinD localizes to the membrane only at cell poles and contains an ATPase and an ATP-binding domain. MinD is only able to bind to the membrane when in its ATP-bound conformation. Once anchored, the protein polymerizes, resulting in clusters of MinD. These clusters bind and then activate another protein called MinC, which has activity only when bound by MinD.\n\nMinC serves as a FtsZ inhibitor that prevents FtsZ polymerization. The high concentration of a FtsZ polymerization inhibitor at the poles prevents FtsZ from initiating division at anywhere but the mid-cell.\nMinE is involved in preventing the formation of MinCD complexes in the middle of the cell. MinE forms a ring near each cell pole. This ring is not like the Z-ring. Instead, it catalyzes the release of MinD from the membrane by activating MinD’s ATPase. This hydrolyzes the MinD’s bound ATP, preventing it from anchoring itself to the membrane.\n\nMinE prevents the MinD/C complex from forming in the center but allows it to stay at the poles. Once the MinD/C complex is released, MinC becomes inactivated. This prevents MinC from deactivating FtsZ. As a consequence, this activity imparts regional specificity to Min localization.\n\nThus, FtsZ can form only in the center, where the concentration of the inhibitor MinC is minimal. Mutations that prevent the formation of MinE rings result in the MinCD zone extending well beyond the polar zones, preventing FtsZ to polymerize and to perform cell division.\n\nMinD requires a nucleotide exchange step to re-bind to ATP so that it can re-associate with the membrane after MinE release. The time lapse results in a periodicity of Min association that may yield clues to a temporal signal linked to a spatial signal.\nIn vivo observations show that the oscillation of Min proteins between cell poles occurs approximately every 50 seconds.\n\nOscillation of Min proteins, however, is not necessary for all bacterial cell division systems. \"Bacillus subtilis\" has been shown to have static concentrations of MinC and MinD at the cell poles.\n\nThis system still links cell size to the ability to form a septum via FtsZ and divide.\n\nThe dynamic behavior of Min proteins has been reconstituted in vitro using artificial lipid bilayers, with varying lipid composition and different confinement geometry as mimics for the cell membrane. The first pattern to be reconstituted were spiraling waves of MinD chased by MinE, followed by the reconstitution of waves of all three proteins, MinD, MinE and MinC. Importantly, MinD and MinE can self-organize into a wide variety of patterns depending on the reaction conditions.\n\nAdditional study is required to elucidate the extent of temporal and spatial signaling permissible by this biological function. These \"in vitro\" systems offered unprecedented access to features such as residence times and molecular motility.\n", "id": "42243737", "title": "Min System"}
{"url": "https://en.wikipedia.org/wiki?curid=42177410", "text": "DNA base flipping\n\nDNA base flipping, or nucleotide flipping, is a mechanism in which a single nucleotide base, or nucleobase, is rotated outside the nucleic acid double helix. This occurs when a nucleic acid-processing enzyme needs access to the base to perform work on it, such as its excision for replacement with another base during DNA repair. It was first observed in 1994 using X-ray crystallography in a methyltransferase enzyme catalyzing methylation of a cytosine base in DNA. Since then, it has been shown to be used by different enzymes in many biological processes such as DNA methylation, various DNA repair mechanisms, and DNA replication. It can also occur in RNA double helices or in the DNA:RNA intermediates formed during RNA transcription.\n\nDNA base flipping occurs by breaking the hydrogen bonds between the bases and unstacking the base from its neighbors. This could occur through an active process, where an enzyme binds to the DNA and then facilitates rotation of the base, or a passive process, where the \nbase rotates out spontaneously, and this state is recognized and bound by an enzyme. It can be detected using \nX-ray crystallography, NMR spectroscopy, fluorescence spectroscopy, or hybridization probes.\n\nBase flipping was first observed in 1994 when researchers Klimasauskas, Kumar, Roberts, and Cheng used X-ray crystallography to view an intermediate step in the chemical reaction of a methyltransferase bound to DNA. The methyltransferase they used was the C5-cytosine methyltransferase from \"Haemophilus haemolyticus\" (M. HhaI). This enzyme recognizes a specific sequence of the DNA (5'-GCGC-3') and methylates the first cytosine base of the sequence at its C5 location. Upon crystallization of the M. HhaI-DNA complex, they saw the target cytosine base was rotated completely out of the double helix and was positioned in the active site of the M. HhaI. It was held in place by numerous interactions between the M. HhaI and DNA. \nThe authors theorized that base flipping was a mechanism used by many other enzymes, such as helicases, recombination enzymes, RNA polymerases, DNA polymerases, and Type II topoisomerases. Much research has been done in the years subsequent to this discovery and it has been found that base flipping is a mechanism used in many of the biological processes the authors suggest.\n\nDNA nucleotides are held together with hydrogen bonds, which are relatively weak and can be easily broken. Base flipping occurs on a millisecond timescale by breaking the hydrogen bonds between bases and unstacking the base from its neighbors. The base is rotated out of the double helix by 180 degrees., typically via the major groove, and into the active site of an enzyme. This opening leads to small conformational changes in the DNA backbone which are quickly stabilized by the increased enzyme-DNA interactions. Studies looking at the free-energy profiles of base flipping have shown that the free-energy barrier to flipping can be lowered by 17 kcal/mol for M.HhaI in the closed conformation.\n\nThere are two mechanisms of DNA base flipping: active and passive. In the active mechanism, an enzyme binds to the DNA and then actively rotates the base, while in the passive mechanism a damaged base rotates out spontaneously first, then is recognized and bound by the enzyme. Research has demonstrated both mechanisms: uracil-DNA glycosylase follows the passive mechanism and Tn10 transposase follows the active mechanism.\n\nFurthermore, studies have shown that DNA base flipping is used by many different enzymes in a variety biological processes such as DNA methylation, various DNA repair mechanisms, RNA transcription and DNA replication.\n\nDNA can have mutations that cause a base in the DNA strand to be damaged. To ensure genetic integrity of the DNA, enzymes need to repair any damage. There are many types of DNA repair. Base excision repair utilizes base flipping to flip the damaged base out of the double helix and into the specificity pocket of a glycosylase which hydrolyzes the glycosidic bond and removes the base. DNA glycosylases interact with DNA, flipping bases to determine a mismatch. An example of base excision repair occurs when a cytosine base is deaminated and becomes a uracil base. This causes a U:G mispair which is detected by Uracil DNA glycosylase. The uracil base is flipped out into the glycosylase active pocket where it is removed from the DNA strand. Base flipping is used to repair mutations such as 8-Oxoguanine (oxoG) and thymine dimers created by UV radiation.\n\nDNA replication and RNA transcription both make use of base flipping. DNA polymerase is an enzyme that carries out replication. It can be thought of as a hand that grips the DNA single strand template. As the template passes across the palm region of the polymerase, the template bases are flipped out of the helix and away from the dNTP binding site. During transcription, RNA polymerase catalyzes RNA synthesis. During the initiation phase, two bases in the -10 element flip out from the helix and into two pockets in RNA polymerase. These new interactions stabilize the -10 element and promote the DNA strands to separate or melt.\n\nBase flipping occurs during latter stages of recombination. RecA is a protein that promotes strand invasion during homologous recombination. Base flipping has been proposed as the mechanism by which RecA can enable a single strand to recognize homology in duplex DNA. Other studies indicate that it is also involved in V(D)J Recombination.\n\nDNA methylation is the process in which a methyl group is added to either a cytosine or adenine. This process causes the activation or inactivation of gene expression, thereby resulting in gene regulation in eukaryotic cells. DNA methylation process is also known to be involved in certain types of cancer formation. In order for this chemical modification to occur, it is necessary that the target base flips out of the DNA double helix to allow the methyltransferases to catalyze the reaction.\n\nRestriction endonucleases, also known as restriction enzymes are enzymes that cleave the sugar-phosphate backbone of the DNA at specific nucleotides sequences that are usually four to six nucleotides long. Studies performed by Horton and colleagues have shown that the mechanism by which these enzymes cleave the DNA involves base flipping as well as bending the DNA and the expansion of the minor groove. In 2006, Horton and colleagues, x-ray crystallography evidence was presented showing that the restriction endonuclease HinP1I utilizes base flipping in order to recognize its target sequence. This enzyme is known to cleave the DNA at the palindromic tetranucleotide sequence G↓CGC.\n\nX-ray crystallography is a technique that measures the angles and intensities of crystalline atoms in order to determine the atomic and molecular structure of the crystal of interest. Crystallographers are then able to produce and three-dimensional picture where the positions of the atoms, chemical bonds as well as other important characteristics can be determined. Klimasaukas and colleagues used this technique to observe the first base flipping phenomenon, in which their experimental procedure involved several steps: \nDuring purification, Haemophilus haemolyticus methyltransferase was overexpressed and purified using a high salt back-extraction step to selectively solubilize M.HhaI, followed by fast protein liquid chromatography (FPLC) as done previously by Kumar and colleagues. Authors utilized a Mono-Q anion exchange column to remove the small quantity of proteinaceous materials and unwanted DNA prior to the crystallization step. Once M.HhaI was successfully purified, the sample was then grown using a method that mixes the solution containing the complex at a temperature of 16 °C and the hanging-drop vapor diffusion technique to obtain the crystals. Authors were then able to collect the x-ray data according to a technique used by Cheng and colleagues in 1993. This technique involved the measurement of the diffraction intensities on a FAST detector, where the exposure times for 0.1° rotation were 5 or 10 seconds. For the structure determination and refinement, Klimasaukas and colleagues used the molecular replacement of the refined apo structure described by Cheng and colleagues in 1993 where the search models X-PLOR, MERLOT, and TRNSUM were used to solve the rotation and translation functions. This part of the study involves the use of a variety of software and computer algorithms to solve the structures and characteristics of the crystal of interest.\n\nNMR spectroscopy is a technique that has been used over the years to study important dynamic aspects of base flipping. This technique allows researchers to determine the physical and chemical properties of atoms and other molecules by utilizing the magnetic properties of atomic nuclei. In addition, NMR can provide a variety of information including structure, reaction states, chemical environment of the molecules, and dynamics. During the DNA base flipping discovery experiment, researchers utilized NMR spectroscopy to investigate the enzyme-induced base flipping of HhaI methyltransferase. In order to accomplish this experiment, two 5-fluorocytosine residues were incorporated into the target and the reference position with the DNA substrate so the F chemical shift analysis could be performed. Once the F chemical shift analysis was evaluated, it was then concluded that the DNA complexes existed with multiple forms of the target 5-fluorocytosine along the base flipping pathway.\n\nFluorescence spectroscopy is a technique that is used to assay a sample using a fluorescent probe. DNA nucleotides themselves are not good candidates for this technique because they do not readily re-emit light upon light excitation. A fluorescent marker is needed to detect base flipping. 2-Aminopurine is a base that is structurally similar to adenine, but is very fluorescent when flipped out from the DNA duplex. It is commonly used to detect base flipping and has an excitation at 305‑320 nm and emission at 370 nm so that it well separated from the excitations of proteins and DNA. Other fluorescent probes used to study DNA base flipping are 6MAP (4‑amino‑6‑methyl‑7(8H)‑pteridone) and Pyrrolo‑C (3-[β-D-2-ribofuranosyl]-6-methylpyrrolo[2,3-d]pyrimidin-2(3H)-one). Time-resolved fluorescence spectroscopy is also employed to provide a more detailed picture of the extent of base flipping as well as the conformational dynamics occurring during base flipping.\n\nHybridization probes can be used to detect base flipping. This technique uses a molecule that has a complementary sequence to the sequence you would like to detect such that it binds to a single-strand of the DNA or RNA. Several hybridization probes have been used to detect base flipping. Potassium permanganate is used to detect thymine residues that have been flipped out by cytosine-C5 and adenine-N6 methyltransferases. Chloroacetaldehyde is used to detect cytosine residues flipped out by the HhaI DNA cytosine-5 methyltransferase (M. HhaI).\n", "id": "42177410", "title": "DNA base flipping"}
{"url": "https://en.wikipedia.org/wiki?curid=42318629", "text": "Minigene\n\nA minigene is a minimal gene fragment that includes an exon and the control regions necessary for the gene to express itself in the same way as a wild type gene fragment. This is a minigene in its most basic sense. More complex minigenes can be constructed containing multiple exons and intron(s). Minigenes provide a valuable tool for researchers evaluating splicing patterns both \"in vivo\" and \"in vitro\" biochemically assessed experiments. Specifically, minigenes are used as splice reporter vectors (also called exon-trapping vectors) and act as a probe to determine which factors are important in splicing outcomes. They can be constructed to test the way both cis-regulatory elements (RNA effects) and trans-regulatory elements (associated proteins/splicing factors) affect gene expression.\n\nMinigenes were first described as the somatic assembly of DNA segments and consisted of DNA regions known to encode the protein and the flanking regions required to express the protein. The term was first used in a paper in 1977 to describe the cloning of two minigenes that were designed to express a peptide.\n\nRNA splicing was discovered in the late 1970s through the study of adenoviruses that invade mammals and replicate inside them. Researchers identified RNA molecules that contained sequences from noncontiguous parts of the virus’s genome. This discovery led to the conclusion that regulatory mechanisms existed which affected mature RNA and the genes it expresses. Using minigenes as a splice reporting vector to explore the effects of RNA splicing regulation naturally followed and remains the major use of minigenes to date.\n\nIn order to provide a good minigene model, the gene fragment should have all of the necessary elements to ensure it exhibits the same alternative splicing (AS) patterns as the wild type gene, i.e., the length of the fragment must include all upstream and downstream sequences which can affect its splicing. Therefore, most minigene designs begin with a thorough \"in silico\" analysis of the requirements of the experiment before any \"wet\" lab work is conducted. With the advent of Bioinformatics and widespread use of computers, several good programs now exist for the identification of cis-acting control regions that affect the splicing outcomes of a gene and advanced programs can even consider splicing outcomes in various tissue types. Differences in minigenes are usually reflected in the final size of the fragment, which is in turn a reflection of the complexity of the minigene itself. The number of foreign DNA elements (exon and introns) inserted into the constitutive exons and introns of a given fragment varies with the type of experiment and the information being sought. A typical experiment might involve wild type minigenes which are expected to express genes normally in a comparison run against genetically engineered allelic variations which replace the wild-type gene and have been cloned into the same flanking sequences as the original fragment. These types of experiments help to determine the effect of various mutations on pre-mRNA splicing.\n\nOnce a suitable genomic fragment is chosen (Step 1), the exons and introns of the fragment can be inserted and amplified, along with the flanking constitutive exons and introns of the original gene, by PCR. The primers for PCR can be chosen so that they leave \"sticky ends\" at 3' sense and anti-sense strands (Step 2). These \"sticky-ends\" can be easily incorporated into a TOPO Vector by ligation into a commercially available source which has ligase already attached to it at the sight of incorporation (Step 3). The subsequent TOPO Vectors can be transfected into E.coli cells (Step 4). After incubation, total RNA can be extracted from the bacterial colonies and analyzed using RT-PCR to quantify ratios of exon inclusion/exclusion (step 5). The minigene can be transfected into different cell types with various splicing factors to test trans-acting elements (Step 6). The expressed genes or the proteins they encode can be analyzed to evaluate splicing components and their effects via a variety of methods including hybridization or size-exclusion chromatography.\n\nRNA splicing errors have been estimated to occur in a third of genetic diseases. To understand pathogenesis and identify potential targets of therapeutic intervention in these diseases, explicating the splicing elements involved is essential. Determining the complete set of components involved in splicing presents many challenges due to the abundance of alternative splicing, which occurs in most human genes, and the specificity in which splicing is carried out \"in vivo\". Splicing is distinctly conducted from cell type to cell type and across different stages of cellular development. Therefore, it is critical that any \"in vitro\" or bioinformatic assumptions about splicing regulation are confirmed \"in vivo\". Minigenes are used to elucidate \"cis\"-regulatory elements, \"trans\"-regulatory elements and other regulators of pre-mature RNA splicing \"in vivo\". Minigenes have been applied to the study of a diverse array of genetic diseases due to the aforementioned abundance of alternatively spliced genes and the specificity and variation observed in splicing regulation. The following are examples of minigene use in various diseases. While it is not an exhaustive list, it does provide a better understanding of how minigenes are utilized.\n\nRNA splicing errors can have drastic effects on how proteins function, including the hormones secreted by the endocrine system. These effects on hormones have been identified as the cause of many endocrine disorders including thyroid-related pathological conditions, rickets, hyperinsulinemic hypoglycemia and congenital adrenal hyperplasia. One specific example of a splicing error causing an endocrine disease that has been studied using minigenes is a type of growth hormone deficiency called isolated growth hormone deficiency (IGHD), a disease that results in growth failure. IGHD type II is an autosomal dominant form caused by a mutation in the intervening sequence (IVS) adjacent to exon 3 of the gene encoding growth hormone 1, the GH-1 gene. This mutated form of IVS3 causes exon 3 to be skipped in the mRNA product. The mRNA (-E3) encodes a truncated form of hGH that then inhibits normal hGH secretion. Minigenes were used to determine that a point mutation within an intron splice enhancer (ISE) embedded in IVS3 was to blame for the skipping of E3. Moreover, it was determined that the function of the ISE is influenced by a nearby transposable AC element, revealing that this particular splicing error is caused by a trans-acting factor.\n\nAccumulation of tau protein is associated with neurodegenerative diseases including Alzheimer's and Parkinson's diseases as well as other tauopathies. Tau protein isoforms are created by alternative splicing of exons 2, 3 and 10. The regulation of tau splicing is specific to stage of development, physiology and location. Errors in tau splicing can occur in both exons and introns and, depending on the error, result in changes to protein structure or loss of function. Aggregation of these abnormal tau proteins correlates directly with pathogenesis and disease progression. Minigenes have been used by several researchers to help understand the regulatory components responsible for mRNA splicing of the TAU gene.\n\nCancer is a complex, heterogeneous disease that can be hereditary or the result of environmental stimuli. Minigenes are used to help oncologists understand the roles pre-mRNA splicing plays in different cancer types. Of particular interest are cancer specific genetic mutations that disrupt normal splicing events, including those affecting spliceosome components and RNA-binding proteins such as heterogeneous nuclear ribonucleoparticules (hnRNP), serine/arginine-rich (SR) proteins and small ribonucleoproteins (snRNP). Proteins encoded by aberrantly spliced pre-mRNAs are functionally different and contribute to the characteristic anomalies exhibited by cancer cells, including their ability to proliferate, invade and undergo angiogenesis, and metastasis. Minigenes help researchers identify genetic mutations in cancer that result in splicing errors and determine the downstream effects those splicing errors have on gene expression. Using knowledge obtained from studies employing minigenes, oncologists have proposed tests designed to detect products of abnormal gene expression for diagnostic purposes. Additionally, the prospect of using minigenes as a cancer immunotherapy is being explored.\n\n\n\n", "id": "42318629", "title": "Minigene"}
{"url": "https://en.wikipedia.org/wiki?curid=42480333", "text": "Paritaprevir\n\nParitaprevir (previously known as ABT-450) is an acylsulfonamide inhibitor of the NS3-4A serine protease manufactured by Abbott Laboratories that shows promising results as a treatment of hepatitis C. When given in combination with ritonavir and ribavirin for 12 weeks, the rate of sustained virologic response at 24 weeks after treatment has been estimated to be 95% for those with hepatitis C virus genotype 1. Resistance to treatment with paritaprevir is uncommon, because it targets the binding site, but has been seen to arise due to mutations at positions 155 and 168 in NS3.\n\nParitaprevir is a component of Viekira Pak and Technivie.\n", "id": "42480333", "title": "Paritaprevir"}
{"url": "https://en.wikipedia.org/wiki?curid=42632611", "text": "Abortive initiation\n\nAbortive initiation, also known as abortive transcription, is an early process of genetic transcription in which RNA polymerase binds to a DNA promoter and enters into cycles of synthesis of short mRNA transcripts which are released before the transcription complex leaves the promoter. This process occurs in both eukaryotes and prokaryotes. Abortive initiation is typically studied in the T3 and T7 RNA polymerases in bacteriophages, and in \"E. coli\".\n\nAbortive initiation occurs prior to promoter clearance. \n\nAbortive initiation is a normal process of transcription and occurs both \"in vitro\" and \"in vivo\". After each nucleotide-addition step in initial transcription, RNA polymerase, stochastically, can proceed on the pathway toward promoter escape (productive initiation) or can release the RNA product and revert to the RNA polymerase-promoter open complex (abortive initiation). During this early stage of transcription, RNA polymerase enters a phase during which dissociation of the transcription complex energetically competes with the elongation process. Abortive cycling is not caused by strong binding between the initiation complex and the promoter.\n\n For many years, the mechanism by which RNA polymerase moves along the DNA strand during abortive initiation remained elusive. It had been observed that RNA polymerase did not escape from the promoter during transcription initiation, so it was unknown how the enzyme could read the DNA strand to transcribe it without moving downstream. Within the last decade, studies have revealed that abortive initiation involves \"DNA scrunching,\" in which RNA polymerase remains stationary while it unwinds and pulls downstream DNA into the transcription complex to pass the nucleotides through the polymerase active site, thereby transcribing the DNA without moving. This causes the unwound DNA to accumulate within the enzyme, hence the name DNA \"scrunching\". In abortive initiation, RNA polymerase re-winds and ejects the downstream portion of the unwound DNA, releasing the RNA, and reverting to the RNA polymerase-promoter open complex; in contrast, in productive initiation, RNA polymerase re-winds and ejects the upstream portion of the unwound DNA, breaking RNA polymerase-promoter interactions, escaping the promoter, and forming a transcription elongation complex\n\nA 2006 paper that demonstrated the involvement of DNA scrunching in initial transcription proposed the idea that the stress incurred during DNA scrunching provides the driving force for both abortive initiation and productive initiation. A companion paper published the same year confirmed that detectable DNA scrunching occurs in 80% of transcription cycles, and is actually estimated to be 100%, given the limitation of the ability to detect rapid scrunching (20% of scrunches have a duration of less than 1 second).\n\nA 2016 paper showed that DNA scrunching also occurs before RNA synthesis during transcription start site selection.\n\nThere are no widely accepted functions for the resulting truncated RNA transcripts. However, a study in 1981 found evidence that there was a relationship between the amount of abortive transcripts produced and the time until long RNA strands are successfully produced. When RNA polymerase undergoes abortive transcription in the presence of ATP, UTP, and GTP, a complex is formed that has a much lower capacity for abortive recycling and a much higher rate of synthesis of the full-length RNA transcript. A study in 2010 did find evidence supporting that these truncated transcripts inhibit termination of RNA synthesis by a RNA hairpin-dependent intrinsic terminator.\n\n", "id": "42632611", "title": "Abortive initiation"}
{"url": "https://en.wikipedia.org/wiki?curid=1570968", "text": "Isopropyl β-D-1-thiogalactopyranoside\n\nIsopropyl β--1-thiogalactopyranoside (IPTG) is a molecular biology reagent. This compound is a molecular mimic of allolactose, a lactose metabolite that triggers transcription of the \"lac\" operon, and it is therefore used to induce protein expression where the gene is under the control of the lac operator.\n\nIPTG, unlike allolactose, is not hydrolyzable by β-galactosidase. Therefore, its concentration remains constant during an experiment. For induction, a sterile, filtered 1 M solution of IPTG is typically added by 1:1000 dilution into an exponentially growing bacterial culture, to give a final concentration of 1 mM. However, different concentrations of IPTG may also be used.\n\nLike allolactose, IPTG binds to the lac repressor and releases the tetrameric repressor from the lac operator in an allosteric manner, thereby allowing the transcription of genes in the lac operon, such as the gene coding for beta-galactosidase, a hydrolase enzyme that catalyzes the hydrolysis of β-galactosides into monosaccharides. But unlike allolactose, the sulfur (S) atom creates a chemical bond which is non-hydrolyzable by the cell, preventing the cell from metabolizing or degrading the inducer.\n\nIPTG uptake by \"E. coli\" can be independent of the action of lactose permease, since other transport pathways are also involved. At low concentration, IPTG enters cells through lactose permease, but at high concentrations (typically used for protein induction), IPTG can enter the cells independently of lactose permease.\n\nIPTG is an effective inducer of protein expression in the concentration range of 100 to 3.0 . Concentration used depends on the strength of induction required, as well as the genotype of cells or plasmid used. If \"lacI\", a mutant that over-produces the lac repressor, is present, then a higher concentration of IPTG may be necessary.\n\nIn blue-white screen, IPTG is used together with X-gal. Blue-white screen allows colonies that have been transformed with the recombinant plasmid rather than a non-recombinant one to be identified in cloning experiments.\n\n\n", "id": "1570968", "title": "Isopropyl β-D-1-thiogalactopyranoside"}
{"url": "https://en.wikipedia.org/wiki?curid=7842233", "text": "Aminoallyl nucleotide\n\nAminoallyl nucleotide is a nucleotide with a modified base containing an allylamine. They are used in post-labeling of nucleic acids by fluorescence detection in microarray. They are reactive with N-Hydroxysuccinimide ester group which helps attach a fluorescent dye to the primary amino group on the nucleotide. These nucleotides are known as 5-(3-aminoallyl)-nucleotides since the aminoallyl group is usually attached to carbon 5 of the pyrimidine ring of uracil or cytosine. The primary amine group in the aminoallyl moiety is aliphatic and thus more reactive compared to the amine groups that are directly attached to the rings (aromatic) of the bases. Common names of aminoallyl nucleosides are initially abbreviated with aa- or AA- to indicate aminoallyl. The 5-carbon sugar is indicated with or without the lowercase \"d\" indicating deoxyribose if included or ribose if not. Finally the nitrogenous base and number of phosphates are indicated (i.e. aa-UTP = aminoallyl uridine triphosphate).\n\nThe goal of combining fluorescence and nucleic acids has been to provide a non-isotopic tag that is detectable to study DNA or RNA. This type of labeling allows scientists to study DNA or RNA in their structure, function, or formation with other nucleic acids. The first base modification for fluorescent labeling occurred in 1971 with a 4-thiouridine and 4-thiouracil. This research along with others, which included various types of direct and non-direct labeling via: analogs, addition via enzymes, or other methods made labeling of nucleotides much safer for scientist to study DNA.\n\nAs instrumentation and technologies become more advanced in the field of DNA microarray, better reagents and techniques will be needed to further scientific studies. Fluorescent labeling with Cy3 was shown to be more insufficient and skew results; the method of aminoallyl nucleotide incorporation was opted instead. Using aminoallyl nucleotides as indirect fluorescent labeling seemed to nullify the sensitivity issues seen in cyanine-labeling.\n\nAminoallyl nucleosides can be synthesized via Heck coupling as shown in the image below.\n\nIn the image above, on the left is a modified nucleoside with an iodine (the iodine is added via electrophilic halogenation) in the fifth carbon in the pyrimidine ring. Its formation can be associated with a reaction with an allylamine and various reagents via heck coupling are able to remove the halogen group from the base and add the allylamine to become the aminoallyl nucleoside shown on the right. The product on the right is then used to in molecular biology in RNA synthesis.\n\nOther reactions include using a single pot synthesis with other halogens.\n\nThe primary amine on the aminoallyl nucleotide reacts with amino-reactive dyes such as a cyanine and patented dyes which contain a reactive leaving group, such as a succinimidyl ester (NHS).The amine groups directly attached to the ring of the base are not affected. These nucleotides are used for labeling DNA.\n\nAminoallyl NTPs are used for indirect DNA labeling in PCR, nick translation, primer extensions and cDNA synthesis. These labeled NTPs are helpful because of their application in molecular biology labs where they do not have the capacity to handle radioactive material. For example, 5-(3-Aminoallyl)-Uridine(AA-UTPs) are more effective for high density labeling of DNA than pre-labeling the DNA. After the enzymatic addition of the NTPs, amine reactant fluorescent dyes can be added for detection of the DNA molecule. When incorporated into DNA or RNA molecules by DNA/RNA polymerase, 5-(3-aminoallyl)-UTP provide a reactive group for the addition of other chemical groups. Thus aminoallyl modified DNA or RNA can be labeled with any compound which has an amine-reactive group.\naa-NTPs incorporated into DNA/RNA in combination with a secondary dye coupling reagents can probe for an array analysis.\n\ncDNA relies on aminoallyl labeling for detection purposes. Although direct labeling of dNTP is the quickest and cheapest method of fluorescent labeling, it is disadvantageous as the sequence allows for only one modified nucleotide for use. Another disadvantage of direct labeling is the bulky nucleotides, however this can be overcome by indirect labeling using aminoallyl modified nucleotides. An easy way to check for labeling success is the color;Good labeling will result in visible blue (Cy5) or red (Cy3) color in the final material.\n\nAnother process which uses aminoallyl labeling is NASBA ( Nucleic Acid Sequence Based Amplification), a highly sensitive technique for amplifying RNA. In this specific case, the aaUTP modified RNAs were tagged with fluorescent market Cy3. NASBA combined with aminoallyl-UTP labeling is very useful for many different areas of microbial diagnostics including environmental monitoring, bio threat detection, industrial process monitoring and clinical microbiology. DNA microarray is another method which utilizes specifically AA-NTP's making DNA microarray testing quicker and cheaply.\n\nPost-synthesis labeling avoids the problems found in direct enzymatic incorporation of Cy-labeled dNTPs by generating probes with equal labeling effectiveness. With indirect labeling, amine-modified NTPs are incorporated during reverse transcription, RNA amplification, or PCR. Amino allyl-NTPs are incorporated with similar efficiency as unmodified NTPs during polymerization.\n\nConcerns with labeling:\nThe amine group, in aminoallyl-modified nucleotide, is reactive with dyes such as the cyanine series, or other patented dyes. A problem arises when the dyes react with buffering agents which are necessary for the proper storage of the nucleotides. However, a carbonate buffer can be used to overcome this problem.\n\n\n", "id": "7842233", "title": "Aminoallyl nucleotide"}
{"url": "https://en.wikipedia.org/wiki?curid=42735534", "text": "Nonribosomal Code\n\nThe nonribosomal code refers to key amino acid residues and their positions within the primary sequence of an adenylation domain of a nonribosomal peptide synthetase used to predict substrate specificity and thus (partially) the final product. Analogous to the nonribosomal code is prediction of peptide composition by DNA/RNA codon reading, which is well supported by the central dogma of molecular biology and accomplished using the genetic code simply by following the DNA codon table or RNA codon table. However, prediction of natural product/secondary metabolites by the nonribosomal code is not as concrete as DNA/RNA codon-to-amino acid and much research is still needed to have a broad-use code. The increasing number of sequenced genomes and high-throughput prediction software has allowed for better elucidation of predicted substrate specificity and thus natural products/secondary metabolites. Enzyme characterization by, for example, ATP-pyrophosphate exchange assays for substrate specificity, \"in silico\" substrate-binding pocket modelling and structure-function mutagenesis (\"in vitro\" tests or \"in silico\" modelling) helps support predictive algorithms. Much research has been done on bacteria and fungi, with prokaryotic bacteria having easier-to-predict products.\n\nThe nonribosomal peptide synthetase (NRPS), a multi-modular enzyme complex, minimally contains repeating, tri-domains (adenylation (A), peptidyl carrier protein (PCP) and lastly codensation(C)). The adenylation domain (A) is the focus for substrate specificity since it is the initiating and substrate recognition domain. In one example, adenylation substrate-binding pocket (defined by 10 residue within) alignments led to clusters giving rise to defined specificity (i.e. the residues of the enzyme pocket can predict nonribosomal peptide sequence). \"In silico\" mutations of substrate-determining residues also led to varying or relaxed specificity. Additionally, the NRPS collinearity principle/rule dictates that given the order of adenylation domains (and substrate-specificity code) throughout the NRPS one can predict the amino acid sequence of the produced small peptide. NRPS, NRPS-like or NRPS-PKS complexes also exist and have domain variations, additions and/or exclusions.\n\n\nModular Peptide Synthetases Involved in Nonribosomal Peptide Synthesis 1997\nThe enduracidin biosynthetic gene cluster from Streptomyces fungicidicus 2006\nCharacterisation of the ArmA adenylation domain implies a more diverse secondary metabolism in the genus Armillaria 2011\nCharacterization of the atromentin biosynthesis genes and enzymes in the homobasidiomycete Tapinella panuoides 2008\nSpecificity prediction of adenylation domains in nonribosomal peptide synthetases (NRPS) using transductive support vector machines (TSVMs) 2005\nThe nonribosomal code 1999\nThe specificity-conferring code of adenylation domains in nonribosomal peptide synthetases 1999\n", "id": "42735534", "title": "Nonribosomal Code"}
{"url": "https://en.wikipedia.org/wiki?curid=42848638", "text": "Transcription-mediated amplification\n\nTranscription-mediated amplification (TMA) is an isothermal, single-tube nucleic acid amplification system utilizing two enzymes, RNA polymerase and reverse transcriptase, to rapidly amplify the target RNA/DNA, enabling the simultaneous detection of multiple pathogenic organisms in a single tube. TMA technology allows a clinical laboratory to perform nucleic acid test (NAT) assays for blood screening with fewer steps, less processing time, and faster results.\nIt is used in molecular biology, forensics, and medicine for the rapid identification and diagnosis of pathogenic organisms, \nIn contrast to similar techniques such as polymerase chain reaction and ligase chain reaction, this method involves RNA transcription (via RNA polymerase) and DNA synthesis (via reverse transcriptase) to produce an RNA amplicon (the source or product of amplification) from a target nucleic acid. This technique can be used to target both RNA and DNA.\n\nTranscription-mediated amplification has several advantages compared to other amplification methods including:\n\n\nFrom: http://www.gen-probe.com\n", "id": "42848638", "title": "Transcription-mediated amplification"}
{"url": "https://en.wikipedia.org/wiki?curid=42630261", "text": "G-Less Cassette\n\nThe G-Less Cassette transcription assay is a method used in molecular biology to determine promoter strength \"in vitro\". The technique involves quantification of an mRNA product with the use of a plasmid. The G-less cassette is part of a pre-constructed vector, usually containing a multiple cloning site (MCS) upstream of the cassette. For this reason, promoters of interest can be inserted directly into the MCS to ultimately measure the accuracy and efficiency of a promoter in recruiting transcription machinery.\n\nA plasmid containing a gene lacking guanine residues in the sense strand of the DNA is located downstream of a MCS. After the promoter is inserted into the MCS, transcription proceeds with the addition of radiolabeled UTP, CTP, and ATP (as well as non-radiolabeled/cold nucleotides) and continues until the end of the G-less cassette is reached and guanine residues are once again apparent in the sense strand of the DNA. The absence of GTP \"in vitro\" results in transcription being prematurely terminated at the first guanine residue in the sense strand following the cassette. Gel electrophoresis is performed on the transcription products and the amount of radioactivity is quantified by autoradiography or phosphorimaging to determine the strength of the promoter of interest.\n\nThe G-less cassette technique is used to determine promoter strength beyond basal levels of transcription (i.e. in the presence of transcription activators or transcription factors). For example, to measure the effects of a TATA box consensus sequence modification in \"Saccharomyces cerevisiae\" in the presence of TFIID, G-less cassettes were implemented to measure the relative strength of each promoter.\n\nThe G-less assay can be performed on a circular plasmid to measure levels of transcription. A circular plasmid provides a more efficient template in many systems when compared to other assays such as runoff transcription, in which a cleaved end is required. This method generates radiolabeled transcripts very efficiently because it bypasses the unnecessary process of performing other indirect mRNA product measurements. The promoter is inserted into a circular plasmid containing the G-less cassette, which will generate a transcript of a certain length that omits random and nonspecific transcription throughout the plasmid. Most crude systems, such as HeLa nuclear extracts, are used because they contain low amounts of contaminating GTP that lead to background transcription and may occasionally cause random transcription to read through the G-less cassette.\n", "id": "42630261", "title": "G-Less Cassette"}
{"url": "https://en.wikipedia.org/wiki?curid=1323803", "text": "Rapid amplification of cDNA ends\n\nRapid amplification of cDNA ends (RACE) is a technique used in molecular biology to obtain the full length sequence of an RNA transcript found within a cell. RACE results in the production of a cDNA copy of the RNA sequence of interest, produced through reverse transcription, followed by PCR amplification of the cDNA copies (see RT-PCR). The amplified cDNA copies are then sequenced and, if long enough, should map to a unique genomic region. RACE is commonly followed up by cloning before sequencing of what was originally individual RNA molecules. A more high-throughput alternative which is useful for identification of novel transcript structures, is to sequence the RACE-products by next generation sequencing technologies.\n\nRACE can provide the sequence of an RNA transcript from a small known sequence within the transcript to the 5' end (5' RACE-PCR) or 3' end (3' RACE-PCR) of the RNA. This technique is sometimes called \"one-sided PCR\" or \"anchored PCR\".\n\nThe first step in RACE is to use reverse transcription to produce a cDNA copy of a region of the RNA transcript. In this process, an unknown end portion of a transcript is copied using a known sequence from the center of the transcript. The copied region is bounded by the known sequence, at either the 5' or 3' end.\n\nThe protocols for 5' or 3' RACES differ slightly. 5' RACE-PCR begins using mRNA as a template for a first round of cDNA synthesis (or reverse transcription) reaction using an \"anti-sense\" (reverse) oligonucleotide primer that recognizes a known sequence in the middle of the gene of interest; the primer is called a \"gene specific primer\" (GSP). The primer binds to the mRNA, and the enzyme reverse transcriptase adds base pairs to the 3' end of the primer to generate a specific single-stranded cDNA product; this is the reverse complement of the mRNA. Following cDNA synthesis, the enzyme terminal deoxynucleotidyl transferase (TdT) is used to add a string of identical nucleotides, known as a homopolymeric tail, to the 3' end of the cDNA. (There are some other ways to add the 3'-terminal sequence for the first strand of the de novo cDNA synthesis which are much more efficient than homopolymeric tailing, but the sense of the method remains the same). \nA PCR reaction is then carried out, which uses a second anti-sense gene specific primer (GSP2) that binds to the known sequence, and a sense (forward) universal primer (UP) that binds the homopolymeric tail added to the 3' ends of the cDNAs to amplify a cDNA product from the 5' end.\n\n3' RACE-PCR uses the natural polyA tail that exists at the 3' end of all eukaryotic mRNAs for priming during reverse transcription, so this method does not require the addition of nucleotides by TdT. cDNAs are generated using an Oligo-dT-adaptor primer (a primer with a short sequence of deoxy-thymine nucleotides) that complements the polyA stretch and adds a special adaptor sequence to the 5' end of each cDNA. PCR is then used to amplify 3' cDNA from a known region using a sense GSP, and an anti-sense primer complementary to the adaptor sequence.\n\nThe cDNA molecules generated by RACE can be sequenced using high-throughput sequencing technologies (also called, RACE-seq). High-throughput sequencing characterization of RACE fragments is highly time-efficient, more sensitive, less costly and technically feasible compared to traditional characterization of RACE fragments with molecular cloning followed by Sanger sequencing of a few clones.\n\nRACE can be used to amplify unknown 5' (5'-RACE) or 3' (3'-RACE) parts of RNA molecules where part of the RNA sequence is known and targeted by a gene-specific primer. Combined with high-throughput sequencing for characterization of these amplified RACE products, it is possible to apply the approach to characterize any types of coding or non-coding RNA-molecules.\n\nThe idea of combining RACE with high-throughput sequencing was first introduced in 2009 as Deep-RACE to perform mapping of Transcription start sites (TSS) of 17 genes in a single cell-line. For example, In a study from 2014 to accurately map cleavage sites of target RNA directed by synthetic siRNAs, the approach was first named RACE-seq. Further, the methodology was used to characterize full-length unknown parts of novel transcripts and fusion transcripts in colorectal cancer. \nIn another study aiming to characterize unknown transcript structures of lncRNAs, RACE was used in combination with semi-long 454 sequencing.\n\n\n", "id": "1323803", "title": "Rapid amplification of cDNA ends"}
{"url": "https://en.wikipedia.org/wiki?curid=3689668", "text": "Protein-fragment complementation assay\n\nA Protein-fragment complementation assay, or PCA, is a method for the identification of protein–protein interactions, especially in the field of proteomics. In the PCA, the proteins of interest (\"bait\" and \"prey\") are each covalently linked to fragments of a third protein (e.g. DHFR, which acts as a \"reporter\"). Interaction between the bait and the prey proteins brings the fragments of the reporter protein in close enough proximity to allow them to form a functional reporter protein whose activity can be measured. This principle can be applied to many different reporter proteins and is also the basis for the yeast two-hybrid system, an archetypical PCA assay.\n\nAny protein that can be split into two parts and reconstituted non-covalently may be used in a PCA. The two parts just have to be brought together by other interacting proteins fused to them (often called \"bait\" and \"prey\" because a bait protein can be used to find a prey protein, see figure). The protein that produces a detectable readout is called \"reporter\". Usually enzymes which confer resistance to antibiotics, such as Dihydrofolate reductase or Beta-lactamase, or proteins that give colorimetric or fluorescent signals are used as reporters. When fluorescent proteins are reconstituted the PCA is called Bimolecular fluorescence complementation assay. The following proteins have been used in split protein PCAs:\n\n", "id": "3689668", "title": "Protein-fragment complementation assay"}
{"url": "https://en.wikipedia.org/wiki?curid=34858148", "text": "ChIP-exo\n\nChIP-exo is a chromatin immunoprecipitation based method for mapping the locations at which a protein of interest (transcription factor) binds to the genome. It is a modification of the ChIP-seq protocol, improving the resolution of binding sites from hundreds of base pairs to almost one base pair. It employs the use of exonucleases to degrade strands of the protein-bound DNA in the 5'-3' direction to within a small number of nucleotides of the protein binding site. The nucleotides of the exonuclease-treated ends are determined using some combination of DNA sequencing, microarrays, and PCR. These sequences are then mapped to the genome to identify the locations on the genome at which the protein binds.\n\nChromatin immunoprecipitation (ChIP) techniques have been in use since 1984 to detect protein-DNA interactions. There have been many variations on ChIP to improve the quality of results. One such improvement, ChIP-on-chip (ChIP-chip), combines ChIP with microarray technology. This technique has limited sensitivity and specificity, especially \"in vivo\" where microarrays are constrained by thousands of proteins present in the nuclear compartment, resulting in a high rate of false positives. Next came ChIP-sequencing (ChIP-seq), which combines ChIP with high-throughput sequencing. However, the heterogeneous nature of sheared DNA fragments maps binding sites to within ±300 base pairs, limiting specificity. Secondly, contaminating DNA presents a grave problem since so few genetic loci are cross-linked to the protein of interest, making any non-specific genomic DNA a significant source of background noise.\n\nTo address these problems, Rhee and Pugh revised the classic nuclease protection assay to develop ChIP-exo. This new ChIP technique relies on a lambda exonuclease that degrades only, and all, unbound double-stranded DNA in the 5′-3′ direction. Briefly, a protein of interest (engineering one with an epitope tag can be useful for immunoprecipitation) is crosslinked in vivo to its natural binding locations across a genome using formaldehyde. Cells are then collected, broken open, and the chromatin sheared and solubilized by sonication. An antibody is then used to immunoprecipitate the protein of interest, along with the crosslinked DNA. DNA PCR adaptors are then ligated to the ends, which serve as a priming point for second strand DNA synthesis after the exonuclease digestion. Lambda exonuclease then digests double DNA strands from the 5′ end until digestion is blocked at the border of the protein-DNA covalent interaction. Most contaminating DNA is degraded by the addition of a second single-strand specific exonuclease. After the cross-linking is reversed, the primers to the PCR adaptors are extended to form double stranded DNA, and a second adaptor is ligated to 5′ ends to demarcate the precise location of exonuclease digestion cessation. The library is then amplified by PCR, and the products are identified by high throughput sequencing. This method allows for resolution of up to a single base pair for any protein binding site within any genome, which is a much higher resolution than either ChIP-chip or ChIP-seq.\n\nChIP-exo has been shown to give up to single base pair resolution in identifying protein binding locations. This is in contrast to ChIP-seq which can locate a protein's binding site only to with ±300 base pairs.\n\nContamination of non-protein-bound DNA fragments can result in a high rate of false positives and negatives in ChIP experiments. The addition of exonucleases to the process not only improves resolution of binding-site calling, but removes contaminating DNA from the solution before sequencing.\n\nProteins that are inefficiently bound to a nucleotide fragment are more likely to be detected by ChIP-exo. This has allowed, for example, the recognition of more CTCF transcription factor binding sites than previously discovered.\n\nDue to the higher resolution and reduced background, less depth of sequencing coverage is needed when using ChIP-exo.\n\nIf a protein-DNA complex has multiple locations of cross-linking within a single binding event, then it can appear as though there are multiple distinct binding events. This likely results from these proteins being denatured and cross-linking at one of the available binding sites within the same event. The exonuclease would then stop at one of the bound sites, depending on which site the protein is cross-linked to.\n\nAs with any ChIP-based method, a suitable antibody for the protein of interest needs to be available in order to use this technique.\n\nRhee and Pugh introduce ChIP-exo by performing analyses on a small collection of transcription factors: Reb1, Gal4, Phd1, Rap1 in yeast and CTCF in human. Reb1 sites were often found in clusters and these clusters had ~10-fold higher occupancy than expected. Secondary sites in clusters were found ~40 bp from a primary binding site. Binding motifs of Gal4 showed a strong preference for three of the four nucleotides, suggesting a negative interaction between Gal4 and the excluded nucleotide. Phd1 recognizes three different motifs which explains previous reports of the ambiguity of Phd1's binding motif. Rap1 was found to recognize four motifs. Ribosomal protein genes bound by this protein had a tendency to use a particular motif with a stronger consensus sequence. Other genes often used clusters of weaker consensus motifs, possibly to achieve a similar occupancy. Binding motifs of CTCF employed four \"modules\". Half of the bound CTCF sites used modules 1 and 2, while the rest used some combination of the four. It is believed that CTCF uses its zinc fingers to recognize different combinations of these modules.\n\nRhee and Pugh analyzed pre-initiation complex (PIC) structure and organization in \"Saccharomyces\" genomes. Using ChIP-exo, they were able to, among other discoveries, precisely identify TATA-like features in promoters reported to be TATA-less.\n\n\n", "id": "34858148", "title": "ChIP-exo"}
{"url": "https://en.wikipedia.org/wiki?curid=43478312", "text": "ZMapp\n\nZMapp is an experimental biopharmaceutical drug comprising three chimeric monoclonal antibodies under development as a treatment for Ebola virus disease. Two of the three components were originally developed at the Public Health Agency of Canada's National Microbiology Laboratory (NML), and the third at the U.S. Army Medical Research Institute of Infectious Diseases; the cocktail was optimized by Gary Kobinger, the recently departed branch chief of the NML and is undergoing further development under license by Mapp Biopharmaceutical. Zmapp was first used on humans during the 2014 West Africa Ebola virus outbreak, having only been previously tested on animals and not yet subjected to a randomized controlled trial. In February 2015, the first clinical trial of ZMapp commenced, taking place in both Liberia and the United States, and was concluded in January 2016.\nThe drug is composed of three monoclonal antibodies (mAbs), initially harvested from mice exposed to Ebola virus proteins, that have been chimerized with human constant regions. The components are chimeric monoclonal antibody c13C6 from a previously existing antibody cocktail called \"MB-003\" and two chimeric mAbs from a different antibody cocktail called ZMab, c2G4 and c4G7. ZMapp is manufactured in the tobacco plant \"Nicotiana benthamiana\" in the bioproduction process known as \"pharming\" by Kentucky BioProcessing, a subsidiary of Reynolds American.\n\nLike intravenous immunoglobulin therapy, ZMapp contains a mixture of neutralizing antibodies that confer passive immunity to an individual, enhancing the normal immune response, and is designed to be administered after exposure to the Ebola virus. Such antibodies have been used in the treatment and prevention of various infectious diseases and are intended to attack the virus by interfering with its surface and neutralizing it to prevent further damage.\n\nTwo of the drug's three components were originally developed at the Public Health Agency of Canada's National Microbiology Laboratory (NML), and a third at the U.S. Army Medical Research Institute of Infectious Diseases; the cocktail was optimized by Gary Kobinger, then branch chief of the NML, and is undergoing further development by Leaf Biopharmaceutical (LeafBio, Inc.), a San Diego-based arm of Mapp Biopharmaceutical. LeafBio created ZMapp in collaboration with its parent and Defyrus Inc., each of which had licensed its own cocktail of antibodies, called MB-003 and ZMab.\n\nMB-003 is a cocktail of three humanized or human–mouse chimeric mAbs: c13C6, h13F6 and c6D8. A study published in September 2012 found that rhesus macaques infected with Ebola virus (EBOV) survived when receiving MB-003 (mixture of 3 chimeric monoclonal antibodies) one hour after infection. When treated 24 or 48 hours after infection, four of six animals survived and had little to no viremia and few, if any, clinical symptoms.\n\nMB-003 was created by scientists at the U.S. Army Medical Research Institute of Infectious Diseases, Gene Olinger and Jamie Pettitt in collaboration with Mapp Biopharmaceutical with years of funding from US government agencies including the National Institute of Allergy and Infectious Disease, Biomedical Advanced Research and Development Authority, and the Defense Threat Reduction Agency.\n\nZMAb is a mixture of three mouse mAbs: m1H3, m2G4 and m4G7. A study published in November 2013 found that EBOV-infected macaque monkeys survived after being given a therapy with a combination of three EBOV surface glycoprotein (EBOV-GP)-specific monoclonal antibodies (ZMAb) within 24 hours of infection. The authors concluded that post-exposure treatment resulted in a robust immune response, with good protection for up to 10 weeks and some protection at 13 weeks. ZMab was created by the NML and licensed to Defyrus, a Toronto-based biodefense company, with further funding by the Public Health Agency of Canada.\n\nA 2014 paper described how Mapp and its collaborators, including investigators at Public Health Agency of Canada, Kentucky BioProcessing, and the National Institute of Allergy and Infectious Diseases, first chimerized the three antibodies comprising ZMAb, then tested combinations of MB-003 and the chimeric ZMAb antibodies in guinea pigs and then primates to determine the best combination, which turned out to be c13C6 from MB-003 and two chimeric mAbs from ZMAb, c2G4 and c4G7. This is ZMapp.\n\nIn an experiment also published in the 2014 paper, 21 rhesus macaque primates were infected with the Kikwit Congolese variant of EBOV. Three primates in the control arm were given a non-functional antibody, and the 18 in the treatment arm were divided into three groups of six. All primates in the treatment arm received three doses of ZMapp, spaced 3 days apart. The first treatment group received its first dose on 3rd day after being infected; the second group on the 4th day after being infected, and the third group, on the 5th day after being infected. All three primates in the control group died; all 18 primates in the treatment arm survived. Mapp then went on to show that ZMapp inhibits replication of a Guinean strain of EBOV in cell cultures.\n\nMapp remains involved in the production of the drug through its contracts with Kentucky BioProcessing, a subsidiary of Reynolds American. To produce the drug, genes coding for the chimeric mAbs were inserted into viral vectors, and tobacco plants are infected with the viral vector encoding for the antibodies, using \"Agrobacterium\" cultures. Subsequently, antibodies are extracted and purified from the plants. Once the genes encoding the chimeric mAbs are in hand, the entire tobacco production cycle is believed to take a few months. The development of these production methods was funded by the U.S. Defense Advanced Research Projects Agency as part of its bio-defense efforts following the 9/11 terrorist attacks.\n\nZMapp was first used during the 2014 West Africa Ebola Virus outbreak, having not previously undergone any human clinical trials to determine its efficacy or potential risks. By October 2014, the United States Food and Drug Administration had approved the use of several experimental drugs, including ZMapp, to be used on patients infected with Ebola virus. The use of such drugs during the epidemic was also deemed ethical by the World Health Organization. In 2014, a limited supply of ZMapp was used to treat 7 individuals infected with the Ebola virus; of these 2 died. The outcome is not considered to be statistically significant. Mapp announced in August 2014, that supplies of ZMapp had been exhausted.\n\nThe lack of drugs and unavailability of experimental treatment in the most affected regions of the West African Ebola virus outbreak spurred some controversy. The fact that the drug was first given to Americans and a European and not to Africans, according to the \"Los Angeles Times\", \"provoked outrage, feeding into African perceptions of Western insensitivity and arrogance, with a deep sense of mistrust and betrayal still lingering over the exploitation and abuses of the colonial era\". Salim S. Abdool Karim, the director of an AIDS research center in South Africa, placed the issue in the context of the history of exploitation and abuses. Responding to a question on how people might have reacted if ZMapp and other drugs had first been used on Africans, he said \"It would have been the front-page screaming headline: 'Africans used as guinea pigs for American drug company's medicine.\n\nIn early August, the World Health Organization called for convening a panel of medical authorities \"to consider whether experimental drugs should be more widely released.\" In a statement, Peter Piot (co-discoverer of the Ebola virus); Jeremy Farrar, the director of the Wellcome Trust; and David Heymann of the Chatham House Center on Global Health Security, called for the release of experimental drugs for affected African nations.\n\nAt an August 6, 2014 press conference, Barack Obama, the President of the United States, was questioned regarding whether the cocktail should be fast-tracked for approval or be made available to sick patients outside of the United States. He responded, \"I think we've got to let the science guide us. I don't think all the information's in on whether this drug is helpful.\"\n\nThe National Institutes of Health announced on 27 February 2015 the commencement of a randomized controlled trial of ZMapp to be conducted in Liberia and the United States. From March 2015 through November 2015, 72 individuals infected with the Ebola virus were enrolled in the trial. The trial was concluded on 29 January 2017, having failed to reach its enrollment goal of 200 due to the waning of the Ebola outbreak. As a result, although a 40% lower risk of death was calculated for those who received ZMapp, the difference was not statistically significant and ultimately it could not be determined whether the use of ZMapp was superior to the optimized standard of care alone. However, ZMapp was found to be safe and well-tolerated.\n", "id": "43478312", "title": "ZMapp"}
{"url": "https://en.wikipedia.org/wiki?curid=43492779", "text": "Golden Gate Cloning\n\nGolden Gate cloning or Golden Gate assembly is a molecular cloning method that allows a researcher to simultaneously and directionally assemble multiple DNA fragments into a single piece using Type IIs restriction enzymes and T4 DNA ligase. This assembly is performed \"in vitro\". Most commonly used Type IIS enzymes include BsaI, BsmBI, and BbsI.\n\nUnlike standard Type II restriction enzymes like EcoRI and BamHI, these enzymes cut DNA outside of their recognition sites and, therefore, can create non-palindromic overhangs. Since 256 potential overhang sequences are possible, multiple fragments of DNA can be assembled by using combinations of overhang sequences. In practice, this means that Golden Gate cloning is typically scarless. Additionally, because the final product does not have a Type II restriction enzyme recognition site, the correctly-ligated product cannot be cut again by the restriction enzyme, meaning the reaction is essentially irreversible.\n\nA typical thermal cycler protocol oscillates between 37 °C (optimal for restriction enzymes) and 16 °C (optimal for ligases) many times. While this technique can be used for a single insert, researchers have used Golden Gate cloning to assemble many pieces of DNA simultaneously.\n\nScar sequences are common in multiple segment DNA assembly. In the multisegment assembly method Gateway, segments are added into the donor with additional att sequences, which overlap in those added segments, and this results in the segments separated by the att sequences. In BioBrick assembly, an eight-nucleotide scar sequence, which codes for a tyrosine and a stop codon, is left between every segment added into the plasmid.\n\nGolden Gate assembly uses type II restriction enzymes cutting outside their recognition sequences. Also, the same type II restriction enzyme can generate copious different overhangs on the inserts and the vector, for instance, BsaI creates 256 four-basepair overhangs. If the overhangs are carefully designed, the segments are ligated without scar sequences between them, and the final construct can be quasi-scarless, where the restriction enzyme sites remain on both sides of the insert. As additional segments can be inserted into the vectors without scars within an open reading frame, Golden Gate is widely used in protein engineering.\n\nAlthough Golden Gate Cloning speeds up multisegment cloning, careful design of donor and recipient plasmids is required. In each cloning step, Golden Gate Cloning can assemble up to nine fragments and only requires homology in type II restriction enzyme sites so that the DNA fragments can be ligated seamlessly. After the fragments are ligated, the product will not have the original type IIS restriction site and will not be redigested in ligation reaction afterwards. Meanwhile, the original restriction sites, which are not ligated, can be redigested so that they can add more fragments into the plasmid. If the DNA fragments are well-designed to be compatible to one another, they can be ligated in a linear order in one step.\n\nRestriction enzyme DNA assembly has cloning standards to minimize the change in cloning efficiency and the function of the plasmid, which can be caused by compatibility of the restriction sites on the insert and those on the vector.\n\nGolden Gate assembly's cloning standards have two tiers. First-tier Golden Gate assembly constructs the single-gene construct by adding in genetic elements such as promoter, open reading frames, and terminators. Then, second-tier Golden Gate assembly combine several constructs made in first-tier assembly to make a multigene construct. To achieve second-tier assembly, modular cloning(MoClo) system and GoldenBraid2.0 standard are used.\n\nMoClo utilizes a parallel approach, where all constructs from tier-one(level 0 modules) have restriction sites for BpiI on both sides of the inserts. The vector(also known as \"destination vector\"), where genes will be added, has an outward-facing BsaI restriction site with a drop-out screening cassette. LacZ is a common screening cassette, where it is replaced by the multigene construct on the destination vector. Each tier-one construct and the vector have different overhangs on them yet complementary to the overhang of the next segment, and this determines the layout of the final multigene construct. Golden Gate cloning usually starts with level 0 modules. However, if the level 0 module is too large, cloning will start from level -1 fragments, which have to be sequenced, to help cloning the large construct. If starting from level -1 fragments, the level 0 modules do not need to be sequenced again, whereas if starting from level 0 modules, the modules must be sequenced.\n\nLevel 0 modules are the base for MoClo system, where they contain genetic elements like a promoter, a 5' untranslated region (UTR), a coding sequence, and a terminator. For the purpose of Golden Gate Cloning, the internal sequences of level 0 modules should not contain type IIS restriction enzymes sites for BsaI, BpiI, and Esp3I while surrounded by two BsaI restriction sites in inverted orientation. Level 0 modules without type IIS restriction sites flanking can add the BsaI sites during the process of Golden Gate cloning.\n\nIf the level 0 modules contains any unwanted restriction site, they can be mutated in silico by removing one nucleotide from the type IIS restriction site. In this process, one needs to make sure that the introduced mutation will not affect the genetic function encoded by the sequence of interest. A silent mutation in the coding sequence is preferred, for it neither changes the protein sequence nor the function of the gene of interest.\n\nLevel -1 fragments are used to help cloning large level 0 modules. To clone level -1 fragments, blunt-end cloning with restriction ligation can be used. The vector used in cloning level -1 fragments cannot contain type IIS restriction site BpiI that is used for the following assembly step. Moreover, the vector should also have a different selection marker from the destination vector in next assembly step, for example, if spectinomycin resistance is used in level 0 modules, level -1 fragments should have another antibiotic resistance like ampicillin. \n\nThe level 1 destination vector determines the position and orientation of each gene in the final construct. There are fourteen available level 1 vectors, which differ only by the sequence of the flanking fusion sites while being identical in the internal fusion sites. Hence, all vectors can assemble the same level 0 parts.\n\nAs all level 1 vectors are binary plasmids, they are used for \"Agrobacterium\" mediated temporary expression in plants.\n\nLevel 2 vectors have two inverted BpiI sites from the insertion of level 1 modules. The upstream fusion site is compatible to a gene cloned in level 1 vector while the downstream fusion site has a universal sequence. Each cloning allows 2-6 genes to be inserted in the same vector.\n\nAdding more genes in one cloning step is not recommended, for this would result in incorrect constructs. On one hand, this can induce more restriction sites in the construct, where this open construct allows additional genes be added. On the other hand, this can also eliminate restriction sites, where this close construct stop the further addition of genes.\n\nTherefore, constructs of more than six genes need successive cloning steps, which requires end-linkers containing BsaI or BsmBI internal restriction sites and blue or purple markers. Each cloning step needs to alternate the restriction site and the marker. Furthermore, two restriction enzymes are needed, where BpiI is used for releasing level 1 modules from level 1 constructs and BsaI/BsmBI is for digesting and opening the recipient level 2-n plasmid. When screening, the correct colonies should alternate from blue to purple every cloning step, but if a \"closed\" end-linker is used, the colonies will be white. \n\nIn standard Golden Gate Cloning, the restriction sites from the previous tier construct cannot be reused. To add more genes to the construct, restriction sites of a different type IIS restriction enzyme need to be added to the destination vector. As going up the hierarchy level, this requires an indefinite amount of destination plasmids to be designed.\n\nGoldenBraid overcomes the problem of designing numerous destination vectors by having a double loop, which is the \"braid,\" to allow binary assembly of multiple constructs. There are two levels of destination plasmids, level α and level Ω. Each level of plasmids can be used as entry plasmids for the other level of plasmids for multiple times because both levels of plasmids have different type IIS restriction sites that are in inverted orientation. For counterselection, the two levels of plasmids differ in their antibiotic resistance markers.\n", "id": "43492779", "title": "Golden Gate Cloning"}
{"url": "https://en.wikipedia.org/wiki?curid=43889944", "text": "Internal control region\n\nAn internal control region is a sequence of DNA located with the coding region of eukaryotic genes that binds regulatory elements such as activators or repressors. This region can recruit RNA Polymerase or contribute to splicing.\n\n\n", "id": "43889944", "title": "Internal control region"}
{"url": "https://en.wikipedia.org/wiki?curid=42671200", "text": "Epigenetic regulation of transposable elements in the plant kingdom\n\nTransposable elements (transposons, TEs, 'jumping genes') are short strands of repetitive DNA that can self-replicate and translocate within the eukaryotic genome, and are generally perceived as parasitic in nature. Their transcription can lead to the production of dsRNAs (double-stranded RNAs), which resemble retroviruses transcripts. While most host cellular RNA has a singular, unpaired sense strand, dsRNA possesses sense and anti-sense transcripts paired together, and this difference in structure allows an host organism to detect dsRNA production, and thereby the presence of transposons. Plants lack distinct divisions between somatic cells and reproductive cells, and also have, generally, larger genomes than animals, making them an intriguing case-study kingdom to be used in attempting to better understand the epigenetics function of transposable elements.\n\nTransposons vary in their structure and manner of proliferation, both of which help to define their classification. Each class contains autonomous elements, a sub-variety distinguished by the ability to self-proliferate, and also non-autonomous elements, which lack that ability.\n\nAlso known as retrotransposons, these employ a strategy of self-copying via RNA transcriptase and subsequently inserting themselves into a new site within the host genome. The presence or absence of transcriptase (the enzyme which allows for self-copying) within the coding of the transposon defines class I elements as autonomous or non-autonomous. Class I transposons can take the form of:\nCuriously, retrotransposons have been discovered to be the predominant form of transpositional element in plants with large genomes, such as maize and wheat, potentially indicating rapid success of this class of transposon in the creation of hybrids, such as wheat, peppermint and, in the distant past, maize. Plant hybridization often creates polyploids, with double, triple, quadruple or more the number of chromosomes present in the parent generation. Polyploid hybrids seem to be particularly susceptible to genetic intrusion by retrotransposons, as supported by a study in sunflower hybridization, which showed that the hybridized flowers possessed genomes that were about 50% larger than that of their parents, with the majority of this increase linked to the amplification of a single retrotransposon class\n\nAlso known as Transposable element#Class II (DNA transposons) DNA transposons], these employ a strategy by which the transposon is excised from its position via transposase, and re-integrated elsewhere in the genome. These can be identified by the following:\nThose DNA transposons lacking the coding necessary to synthesize transposase function non-autonomously, likely piggy-backing off of the machinery generated by neighboring transposons of the same class. An example of this would be MITEs, miniature inverted repeat transposable elements, which, while having both TIRs and TSDs, cannot produce transposase. These are particularly prevalent in plants, and are thought to be derived from deletions in the more autonomous DNA transposons. Similarly, these types of transposons can become non-autonomous by capturing or replicating pieces of host DNA.\n\nAnother variety of transposons, discovered in 2001, which can also potentially capture host DNA. Helitrons are thought to replicate via a \"rolling circle\", in which transposase links the helitron to two distinct regions of the genome at once, using a helicase, ligase, and nuclease in the process to unravel the strands involved, replicate the helitron, and subsequently ligate the replicated material into the new site. During this process, it is thought that the helitrons often encode for the surrounding DNA and integrate this into their own material. Non-autuonomous helitrons may lack a transposase, a helicase, a ligase, or a nuclease. All are thought to be necessary for this complex process of transposition.\n\nDue to their invasive nature, and their potentially disruptive production of ncRNAs, most transposons are dangerous to plants and metazoans alike. Given the lack of distinction between germ-line and somatic cells in the plant kingdom, this is doubly so, since alterations to the genetic and epigenetic code will be more easily inherited. While transposable elements may affect any number of different cell-types in an animal, be a skin cell, a liver cell, a brain cell, these changes are not heritable, due to the fact that an animal inherits only a parents gametic genetic code. In plants, however, there is no such distinction; a flower develops from a meristem, which is a form of somatic cell, and which will pass down to the flower, and thus to the offspring, any genetic or epigenetic alteration. Since each meristem will have developed differently, each different flower from each meristem of the same plant will potentially possess different modifications. In contrast to animals, however, plants do not undergo chromatin remodeling between generations, making the maintenance and inheritance of silencing an entirely different process. There are distinct and identifiable mechanisms on the maintenance of transposon inactivation in plants but, unfortunately, there is significantly less information on initiation of said inactivation.\n\nThough the effects of transposition can sometimes manifest phenotypically, and indeed, this effect led to their discovery, transposons can be difficult for the cellular machinery to detect. Many TEs contain stretches of genuine coding DNA, copied from the host, and there is no distinct structure, code, or identifying characteristic of any kind that would allow a cell to recognize the full range of transposable elements with accuracy. Even besides coding for functional proteins or RNAs, some transposons, like class II elements, contain code copied from the nearby strand, allowing them to blend in. Given that the preceding is true, it must be that transposons are recognized more by their effect than their structure. Thus, cell machinery, as detailed in the next section, exists that is capable of detecting transcripts that are atypical of host genomes, such as:\nand, more specifically:\n\nSilencing of transposon transcripts can vary in completeness of silencing as well as in duration of alteration. Plants employ a number of methods, which range from elimination of transcripts to complete epigenetic silencing. In general, these can be sorted into two 'strategies':\n\nIn general, initiation of transposon silencing has yet to be fully explained. For example, there have been recorded incidences of spontaneous silencing in maize, which carries a high number of transposons (~85% of the genome), though the mechanism by which this occurs is unknown. While it is known that heritable methylation occurs, and must occur with frequency, and must be initiated, triggered by some distinct factor, the only known example of this is in the case of Mu killer (Muk), a gene in maize that silences MuDR, a class II autonomous transposable element. Muk encodes a natural inverted derivative of the transposase coding sequence in MuDR, which, when transcribed, forms a dsRNA that is subsequently cut into siRNA, which renders MUDR incapable of 'cutting and pasting' itself by way of RNAi interference of the transposase. Muk also engages RNAi-Directed Methylation to create a stable and heritable suppression.\n\nThough transposable elements were discovered due in large part to their deleterious effects, epigenetic research has shown that they may be, in some cases, beneficial to the host organism.(1,5) This research indicates that the distinction between those two aspects, mutualist and parasite, may be harder to accurately describe than was once thought.\n\nThe primary mutualistic interaction between transposon and host organism is in the formation of epialleles. True to the name, an epiallele is a kind of epigenetic mutant of a certain allelic type that produces distinct morphological differences from the wild type. The predominant research into this subject has been conducted on Arabidopsis thaliana, which has the dual disadvantages of being both TE-poor and an overly genetically stable organism. The manner of formation of epialleles is somewhat unclear, but it is thought to be due to the fact that some transposable elements, in stealing pieces of genetic code from their host organism, blend in so well as to confuse the host cellular machinery into thinking that its own genes are the transposons, which leads to epigenetic silencing of certain alleles, forming an epiallele. Some examples of this are:\n\nThere is also evidence to suggest that trasposons play a more general role than was previously thought in the formation of miRNAs as well as in the silencing of centromeres.\n\nThough the majority of information on transposons is in relation to their parasitic effect, it is sometimes unclear as to how exactly they hurt the host organism. To clarify, there are several ways in which a negative effect can be produced by transposable elements.\n\nAny one of these can have an extreme or minimal effect, depending on what systems the mutation affects. For example, if a transposon were to interrupt the coding for the enzyme which allows for seeds to digest the nourishing endosperm, then the seed would fail to propagate at all, meaning that the mutation was, in essence, fatal. As a counter example, a transposon could be inserted into a non-coding region (which is likely the remnant of a now inactive transposon) and have no effect at all.\n\nVery little is known about the initiation of epigenetic silencing of transposable elements, and aside from the rare exception to this rule, as in the gene Muk, present as an initiator of regulatory epigenetic modification in maize, there are many other unclear aspects of how transposons are regulated in plant genomes. Might they be a first step in evolution that we never knew about? [1] Might they be, simply, a kink in the chain of genetic coding, one that will eventually be worked out? Again, given the lack of information it is hard to say. Future research into this field will see the changing of our conceptions of transposons and their role in eukaryote development, one way or another.\n", "id": "42671200", "title": "Epigenetic regulation of transposable elements in the plant kingdom"}
{"url": "https://en.wikipedia.org/wiki?curid=43858552", "text": "High-performance Integrated Virtual Environment\n\nThe High-performance Integrated Virtual Environment (HIVE) is a distributed computing environment used for biological research, including analysis of Next Generation Sequencing (NGS) data, post market data, adverse events, metagenomic data, etc.\n\nHIVE is a massively parallel distributed computing environment where the distributed storage library and the distributed computational powerhouse are linked seamlessly. The system is both robust and flexible due to maintaining both storage and the metadata database on the same network. The distributed storage layer of software is the key component for file and archive management and is the backbone for the deposition pipeline. The data deposition back-end allows automatic uploads and downloads of external datasets into HIVE data repositories. The metadata database can be used to maintain specific information about extremely large files ingested into the system (big data) as well as metadata related to computations run on the system. This metadata then allows details of a computational pipeline to be brought up easily in the future in order to validate or replicate experiments. Since the metadata is associated with the computation, it stores the parameters of any computation in the system eliminating manual record keeping.\n\nDifferentiating HIVE from other object oriented databases is that HIVE implements a set of unified APIs to search, view, and manipulate data of all types. The system also facilitates a highly secure hierarchical access control and permission system, allowing determination of data access privileges in a finely granular manner without creating a multiplicity of rules in the security subsystem. The security model, designed for sensitive data, provides comprehensive control and auditing functionality in compliance with HIVE's designation as a FISMA Moderate system.\n\n\n\n\n\n\n\nFDA launched HIVE Open Source as a platform to support end to end needs for NGS analytics. \nhttps://github.com/FDA/fda-hive\n\nHIVE biocompute harmonization platform is at the core of High-throughput Sequencing Computational Standards for Regulatory Sciences (HTS-CSRS) project. Its mission is to provide the scientific community with a framework to harmonize biocomputing, promote interoperability, and verify bioinformatics protocols (https://hive.biochemistry.gwu.edu/htscsrs). For more information, see the project description on the FDA Extramural Research page (https://www.fda.gov/ScienceResearch/SpecialTopics/RegulatoryScience/ucm491893.htm\nSub-clusters of scalable high performance high density compute cores are there to serve as a powerhouse for extra-large distributed parallelized computations of NGS algorithmics. System is extremely scalable and has deployment instances ranging from a single HIVE in a box appliance to massive enterprise level systems of thousands of compute units.\n\n\n\n\n", "id": "43858552", "title": "High-performance Integrated Virtual Environment"}
{"url": "https://en.wikipedia.org/wiki?curid=43628051", "text": "TKM-Ebola\n\nTKM-Ebola was an experimental antiviral drug for Ebola disease that was developed by Arbutus Biopharma (formerly Tekmira Pharmaceuticals Corp.) in Vancouver, Canada. The drug candidate was formerly known as Ebola-SNALP.\n\nTKM-Ebola is a combination of small interfering RNAs targeting three of the seven proteins in Ebola virus: Zaire Ebola L polymerase, Zaire Ebola membrane-associated protein (VP24), and Zaire Ebola polymerase complex protein (VP35). By down-regulating these three protein, TKM-Ebola inhibit virus replication that eliminate the infection. The drug was effective in rhesus monkeys infected with Ebola. After the Ebola outbreak in West Africa in 2014, the new variant responsible for was isolated from several Ebola virus family and the specific genomic sequence was determined. The company re-designed TKM-Ebola and termed as \"TKM-Ebola-Guinea\".\n\nIn January 2014, Tekmira started a Phase I clinical trial of TKM-Ebola to assess its safety in healthy people with a dose of 0.24 mg/kg/day for seven day treatment. The FDA put the trial on clinical hold in July 2014 to assess results, after some subjects had flu-like responses. In August, the FDA changed the status to \"partial hold\", allowing the drug to be used under Expanded access in people infected with Ebola but with the Phase I trial still suspended. In April 2015 the FDA allowed the study to resume at a lower dose.\n\nA Phase II trial started on 11 March 2015 in Sierra Leone, West Africa and stopped enrolling new subjects on 19 June 2015 after it appeared not to work. In July 2015 the company announced it was suspending development of the drug, changing its name to Arbutus, and changing its focus to developing treatments for Hepatitis B virus.\n\n", "id": "43628051", "title": "TKM-Ebola"}
{"url": "https://en.wikipedia.org/wiki?curid=44260712", "text": "SNP annotation\n\nSingle nucleotide polymorphism plays an important role in genome wide association studies because they act as primary biomarker. SNPs are currently the marker of choice due to their large numbers in virtually all populations of individuals. The location of these biomarkers can be tremendously important in terms of predicting functional significance, genetic mapping and population genetics. Each SNP represents a nucleotide change between two individuals at a defined location. SNPs are the most common genetic variant found in all individual with one SNP every 100–300 bp in some species. Since there is a massive number of SNPs on the genome, there is a clear need to prioritize SNPs according to their potential effect in order to expedite genotyping and analysis.\nAnnotating large numbers of SNPs is a difficult and complex process, which need computational methods to handle such a large dataset. Many tools available have been developed for SNP annotation in different organism, some of them are optimized for use with organisms densely sampled for SNPs (such as humans), but there are currently few tools available that are species non-specific or support non-model organism data. The majority of SNPs annotation tools provide computationally predicted putative deleterious effects of SNPs. These tools examine whether a SNP resides in functional genomic regions such as exons, splice sites, or transcription regulatory sites, and predict the potential corresponding functional effects that the SNP may have using a variety of machine-learning approaches. But the tools and systems that prioritize functionally significant SNPs, suffer from few limitations: First, they examine the putative deleterious effects of SNPs with respect to a single biological function that provide only partial information about the functional significance of SNPs. Second, current systems classify SNPs into deleterious or neutral group.\n\nFor SNP annotation many genetic and genomic information are used. Based on different feature used by the annotation tool, the SNP annotation can be classified into this category.\n\nGenomic information from surrounding genomic elements is among the most useful information for interpreting the biological function of an observed variant. Information from a known gene is used as a reference to indicate whether the observed variant resides in or near a gene and if it has the potential to disrupt the protein sequence and its function. Gene based annotation is based on the fact that non-synonymous mutations can alter the protein sequence and that splice site mutation may disrupt the transcript splicing pattern.\n\nKnowledge base annotation is done based on the information of gene attribute, protein function and its metabolism. In this type of annotation more emphasis is given to genetic variation that disrupts the protein function domain, protein-protein interaction and biological pathway. The non-coding region of genome contain many important regulatory elements including promoter, enhancer and insulator, any kind of change in this regulatory region can change the functionality of that protein. The mutation in DNA can change the RNA sequence and then influence the RNA secondary structure, RNA binding protein recognition and miRNA binding activity.\n\nThis method mainly identifies variant function based on the information whether the variant loci are in the known functional region that harbor genomic or epigenomic signals. The function of non-coding variants are extensive in terms of the affected genomic region and they involve in almost all processes of gene regulation from transcriptional to post translational level \n\nTranscriptional gene regulation process depends on many spatial and temporal factors in the nucleus such as global or local chromatin states, nucleosome positioning, TF binding, enhancer/promoter activities. Variant that alter the function of any of these biological processes may alter the gene regulation and cause phenotypic abnormality. Genetic variants that located in distal regulatory region can affect the binding motif of TFs, chromatin regulators and other distal transcriptional factors, which disturb the interaction between enhancer/silencer and its target gene.\n\nAlternative splicing is one of the most important components that show functional complexity of genome. Modified splicing has significant effect on the phenotype that is relevance to disease or drug metabolism. A change in splicing can be caused by modifying any of the components of the splicing machinery such as splice sites or splice enhancers or silencers. Modification in the alternative splicing site can lead to a different protein form which will show a different function. Humans use an estimated 100,000 different proteins or more, so some genes must be capable of coding for a lot more than just one protein. Alternative splicing occurs more frequently than was previously thought and can be hard to control; genes may produce tens of thousands of different transcripts, necessitating a new gene model for each alternative splice.\n\nMutations in the untranslated region (UTR) affect many post-transcriptional regulation. Distinctive structural features are required for many RNA molecules and cis-acting regulatory elements to execute effective functions during gene regulation. SNVs can alter the secondary structure of RNA molecules and then disrupt the proper folding of RNAs, such as tRNA/mRNA/lncRNA folding and miRNA binding recognition regions.\n\nSingle nucleotide variant can also affect the cis-acting regulatory elements in mRNA’s to inhibit/promote the translation initiation. Change in the synonymous codons region due to mutation may affect the translation efficiency because of codon usage biases. The translation elongation can also be retarded by mutations along the ramp of ribosomal movement. In the post-translational level, genetic variants can contribute to proteostasis and amino acid modifications. However, mechanisms of variant effect in this field are complicated and there are only a few tools available to predict variant’s effect on translation related modifications.\n\nNon-synonymous is the variant in exons that change the amino acid sequence encoded by the gene, including single base changes and non frameshift indels. It has been extremely investigated the function of non-synonymous variants on protein and many algorithms have been developed to predict the deleteriousness and pathogenesis of single nucleotide variants (SNVs). Classical bioinformatics tools, such as SIFT, Polyphen and MutationTaster, successfully predict the functional consequence of non-synonymous substitution.\n\nComparative genomics approaches were used to predict the function-relevant variants under the assumption that the functional genetic locus should be conserved across different species at an extensive phylogenetic distance. On the other hand, some adaptive traits and the population differences are driven by positive selections of advantageous variants, and these genetic mutations are functionally relevant to population specific phenotypes. Functional prediction of variants’ effect in different biological processes is pivotal to pinpoint the molecular mechanism of diseases/traits and direct the experimental validation.\n\nTo annotate large number of available NGS data, currently a large number of SNPs annotation tools is available. Some of them are specific to some specific SNPs annotation. Some of the available SNPs annotation tools are as follows SNPeff, VEP, ANNOVAR, FATHMM, PhD-SNP, PolyPhen-2, SuSPect, F-SNP, AnnTools, SeattleSeq, SNPit, SCAN, Snap, SNPs&GO, LS-SNP, Snat, TREAT, TRAMS, Maviant, MutationTaster, SNPdat, Snpranker, NGS – SNP, SVA, VARIANT, SIFT, PhD-SNP and FAST-SNP. Function and approach used in SNPs annotation tools are listed below\n\nVariant annotation tools use machine learning algorithms for prediction of variant. Different annotation tools use different algorithms. Common algorithms include:\n\n\nA large number of variant annotation tools are available for variant annotation but in some cases the prediction by the tools does not agree since the way the rules have been defined differ slightly between each application. It is frankly impossible to perform a perfect comparison of the tools. Not all the tools have same input and output and function. Here is a table of major annotation tools and it's functional area.\n\nSource: S. Pabinger et al., 2012 \n\nThe next generation of SNP annotation webservers can take advantage of the growing amount of data in core bioinformatics resources and use intelligent agents to fetch data from different sources as needed. From a user’s point of view, it is more efficient to submit a set of SNPs and receive results in a single step, which makes meta-servers the most attractive choice. However, if SNP annotation tools deliver heterogeneous data covering sequence, structure, regulation, pathways, etc., they must also provide frameworks for integrating data into a decision algorithm(s), and quantitative confidence measures so users can assess which data are relevant and which are not.\n", "id": "44260712", "title": "SNP annotation"}
{"url": "https://en.wikipedia.org/wiki?curid=43671366", "text": "Electro-switchable biosurface\n\nAn electro-switchable biosurface is a biosensor that can be used in conjunction with alternating or fixed electrical potentials in order to affect change in the structure and position (movement) of charged biomolecules such as DNA, RNA or oligopeptides bound to the biosurface. This is especially pronounced when the biomolecule has rigidity in its structure such as double stranded DNA.\n\nIn turn, the changes caused by electrical potentials can be used to affect the biological function of the biomolecule by revealing or changing the access to molecular targets.\n\nThe biomolecule (for instance double stranded DNA) bound to the biosurface can be used to tether a label free target in a label free interaction model. By applying a variable or alternating potential to the biosurface, the biomolecules can be moved systematically. In turn the movement can be measured in real time using time-resolved fluorescence spectroscopy of the fluorescence quenching of an attached fluorescent marker molecule. By analyzing the speed of the biomolecule as it is dragged through the buffer solution by the electric field, the binding of a ligand can be measured in real time. Since the speed of the biomolecule and a bound ligand depend on their hydrodynamic friction the method can be used to measure the size of the biomolecule and/or of the ligand. This can also be directly applied to detect conformational changes of the biomolecule and/or the ligand. It competes with technologies including surface plasmon resonance.\n\nRecently, electrical stimulus has also been utilized to influence the binding between a protein and its ligand which could be incorporated into smart surfaces for controlled molecular release.\n\n", "id": "43671366", "title": "Electro-switchable biosurface"}
{"url": "https://en.wikipedia.org/wiki?curid=11555", "text": "Fluorescence\n\nFluorescence is the emission of light by a substance that has absorbed light or other electromagnetic radiation. It is a form of luminescence. In most cases, the emitted light has a longer wavelength, and therefore lower energy, than the absorbed radiation. The most striking example of fluorescence occurs when the absorbed radiation is in the ultraviolet region of the spectrum, and thus invisible to the human eye, while the emitted light is in the visible region, which gives the fluorescent substance a distinct color that can only be seen when exposed to UV light. Fluorescent materials cease to glow immediately when the radiation source stops, unlike phosphorescence, where they continue to emit light for some time after.\n\nFluorescence has many practical applications, including mineralogy, gemology, medicine, chemical sensors (fluorescence spectroscopy), fluorescent labelling, dyes, biological detectors, cosmic-ray detection, and, most commonly, fluorescent lamps. Fluorescence also occurs frequently in nature in some minerals and in various biological states in many branches of the animal kingdom.\n\nAn early observation of fluorescence was described in 1560 by Bernardino de Sahagún and in 1565 by Nicolás Monardes in the infusion known as \"lignum nephriticum\" (Latin for \"kidney wood\"). It was derived from the wood of two tree species, \"Pterocarpus indicus\" and \"Eysenhardtia polystachya\". The chemical compound responsible for this fluorescence is matlaline, which is the oxidation product of one of the flavonoids found in this wood.\n\nIn 1819, Edward D. Clarke and in 1822 René Just Haüy described fluorescence in fluorites, Sir David Brewster described the phenomenon for chlorophyll in 1833 and Sir John Herschel did the same for quinine in 1845.\n\nIn his 1852 paper on the \"Refrangibility\" (wavelength change) of light, George Gabriel Stokes described the ability of fluorspar and uranium glass to change invisible light beyond the violet end of the visible spectrum into blue light. He named this phenomenon \"fluorescence\" : \"I am almost inclined to coin a word, and call the appearance \"fluorescence\", from fluor-spar [i.e., fluorite], as the analogous term \"opalescence\" is derived from the name of a mineral.\" The name was derived from the mineral fluorite (calcium difluoride), some examples of which contain traces of divalent europium, which serves as the fluorescent activator to emit blue light. In a key experiment he used a prism to isolate ultraviolet radiation from sunlight and observed blue light emitted by an ethanol solution of quinine exposed by it.\n\nFluorescence occurs when an orbital electron of a molecule, atom, or nanostructure, relaxes to its ground state by emitting a photon from an excited singlet state:\n\nExcitation: formula_1\n\nFluorescence (emission): formula_2\n\nHere formula_3 is a generic term for photon energy with h = Planck's constant and formula_4 = frequency of light. The specific frequencies of exciting and emitted light are dependent on the particular system.\n\nS is called the ground state of the fluorophore (fluorescent molecule), and S is its first (electronically) excited singlet state.\n\nA molecule in S can relax by various competing pathways. It can undergo \"non-radiative\" relaxation in which the excitation energy is dissipated as heat (vibrations) to the solvent. Excited organic molecules can also relax via conversion to a triplet state, which may subsequently relax via phosphorescence, or by a secondary non-radiative relaxation step.\n\nRelaxation from S can also occur through interaction with a second molecule through fluorescence quenching. Molecular oxygen (O) is an extremely efficient quencher of fluorescence just because of its unusual triplet ground state.\n\nIn most cases, the emitted light has a longer wavelength, and therefore lower energy, than the absorbed radiation; this phenomenon is known as the Stokes shift. However, when the absorbed electromagnetic radiation is intense, it is possible for one electron to absorb two photons; this two-photon absorption can lead to emission of radiation having a shorter wavelength than the absorbed radiation. The emitted radiation may also be of the same wavelength as the absorbed radiation, termed \"resonance fluorescence\".\n\nMolecules that are excited through light absorption or via a different process (e.g. as the product of a reaction) can transfer energy to a second 'sensitized' molecule, which is converted to its excited state and can then fluoresce.\n\nThe fluorescence quantum yield gives the efficiency of the fluorescence process. It is defined as the ratio of the number of photons emitted to the number of photons absorbed.\n\nThe maximum fluorescence quantum yield is 1.0 (100%); each photon absorbed results in a photon emitted. Compounds with quantum yields of 0.10 are still considered quite fluorescent. Another way to define the quantum yield of fluorescence, is by the rate of excited state decay:\n\nwhere formula_7 is the rate constant of spontaneous emission of radiation and\n\nis the sum of all rates of excited state decay. Other rates of excited state decay are caused by mechanisms other than photon emission and are, therefore, often called \"non-radiative rates\", which can include:\ndynamic collisional quenching, near-field dipole-dipole interaction (or resonance energy transfer), internal conversion, and intersystem crossing. Thus, if the rate of any pathway changes, both the excited state lifetime and the fluorescence quantum yield will be affected.\n\nFluorescence quantum yields are measured by comparison to a standard. The quinine salt \"quinine sulfate\" in a sulfuric acid solution is a common fluorescence standard.\n\nThe fluorescence lifetime refers to the average time the molecule stays in its excited state before emitting a photon. Fluorescence typically follows first-order kinetics:\n\nwhere formula_10 is the concentration of excited state molecules at time formula_11, formula_12 is the initial concentration and formula_13 is the decay rate or the inverse of the fluorescence lifetime. This is an instance of exponential decay. Various radiative and non-radiative processes can de-populate the excited state. In such case the total decay rate is the sum over all rates:\n\nwhere formula_15 is the total decay rate, formula_16 the radiative decay rate and formula_17 the non-radiative decay rate. It is similar to a first-order chemical reaction in which the first-order rate constant is the sum of all of the rates (a parallel kinetic model). If the rate of spontaneous emission, or any of the other rates are fast, the lifetime is short. For commonly used fluorescent compounds, typical excited state decay times for photon emissions with energies from the UV to near infrared are within the range of 0.5 to 20 nanoseconds. The fluorescence lifetime is an important parameter for practical applications of fluorescence such as fluorescence resonance energy transfer and fluorescence-lifetime imaging microscopy.\n\nThe Jablonski diagram describes most of the relaxation mechanisms for excited state molecules. The diagram alongside shows how fluorescence occurs due to the relaxation of certain excited electrons of a molecule.\n\nFluorophores are more likely to be excited by photons if the transition moment of the fluorophore is parallel to the electric vector of the photon. The polarization of the emitted light will also depend on the transition moment. The transition moment is dependent on the physical orientation of the fluorophore molecule. For fluorophores in solution this means that the intensity and polarization of the emitted light is dependent on rotational diffusion. Therefore, anisotropy measurements can be used to investigate how freely a fluorescent molecule moves in a particular environment.\n\nFluorescence anisotropy can be defined quantitatively as\nwhere formula_19 is the emitted intensity parallel to polarization of the excitation light and formula_20 is the emitted intensity perpendicular to the polarization of the excitation light.\n\nStrongly fluorescent pigments often have an unusual appearance which is often described colloquially as a \"neon color.\" This phenomenon was termed \"Farbenglut\" by Hermann von Helmholtz and \"fluorence\" by Ralph M. Evans. It is generally thought to be related to the high brightness of the color relative to what it would be as a component of white. Fluorescence shifts energy in the incident illumination from shorter wavelengths to longer (such as blue to yellow) and thus can make the fluorescent color appear brighter (more saturated) than it could possibly be by reflection alone.\n\nThere are several general rules that deal with fluorescence. Each of the following rules has exceptions but they are useful guidelines for understanding fluorescence (these rules do not necessarily apply to two-photon absorption).\n\nKasha's rule dictates that the quantum yield of luminescence is independent of the wavelength of exciting radiation. This occurs because excited molecules usually decay to the lowest vibrational level of the excited state before fluorescence emission takes place. The Kasha–Vavilov rule does not always apply and is violated severely in many simple molecules. A somewhat more reliable statement, although still with exceptions, would be that the fluorescence spectrum shows very little dependence on the wavelength of exciting radiation.\n\nFor many fluorophores the absorption spectrum is a mirror image of the emission spectrum. This is known as the mirror image rule and is related to the Franck–Condon principle which states that electronic transitions are vertical, that is energy changes without distance changing as can be represented with a vertical line in Jablonski diagram. This means the nucleus does not move and the vibration levels of the excited state resemble the vibration levels of the ground state.\n\nIn general, emitted fluorescent light has a longer wavelength and lower energy than the absorbed light. This phenomenon, known as Stokes shift, is due to energy loss between the time a photon is absorbed and when it is emitted. The causes and magnitude of Stokes shift can be complex and are dependent on the fluorophore and its environment. However, there are some common causes. It is frequently due to non-radiative decay to the lowest vibrational energy level of the excited state. Another factor is that the emission of fluorescence frequently leaves a fluorophore in a higher vibrational level of the ground state.\n\nThere are many natural compounds that exhibit fluorescence, and they have a number of applications. Some deep-sea animals, such as the greeneye, use fluorescence.\n\nBiofluorescence is the absorption of electromagnetic wavelengths from the visible light spectrum by fluorescent proteins in a living organism, and the reemission of that light at a lower energy level. This causes the light that is re-emitted to be a different color than the light that is absorbed. Stimulating light excites an electron, raising energy to an unstable level. This instability is unfavorable, so the energized electron is returned to a stable state almost as immediately as it becomes unstable. This return to stability corresponds with the release of excess energy in the form of fluorescent light. This emission of light is only observable when the stimulant light is still providing light to the organism/object and is typically yellow, pink, orange, red, green, or purple. Biofluorescence is often confused with the following forms of biotic light, bioluminescence and biophosphorescence.\n\nBioluminescence differs from biofluorescence in that it is the natural production of light by chemical reactions within an organism, whereas biofluorescence is the absorption and reemission of light from the environment.\n\nBiophosphorescence is similar to biofluorescence in its requirement of light wavelengths as a provider of excitation energy. The difference here lies in the relative stability of the energized electron. Unlike with biofluorescence, here the electron retains stability, emitting light that continues to “glow-in-the-dark” even long after the stimulating light source has been removed.\n\nPigment cells that exhibit fluorescence are called fluorescent chromatophores, and function somatically similar to regular chromatophores. These cells are dendritic, and contain pigments called fluorosomes. These pigments contain fluorescent proteins which are activated by K+ (potassium) ions, and it is their movement, aggregation, and dispersion within the fluorescent chromatophore that cause directed fluorescence patterning. Fluorescent cells are innervated the same as other chromatphores, like melanophores, pigment cells that contain melanin. Short term fluorescent patterning and signaling is controlled by the nervous system. Fluorescent chromatophores can be found in the skin (e.g. in fish) just below the epidermis, amongst other chromatophores.\n\nEpidermal fluorescent cells in fish also respond to hormonal stimuli by the α–MSH and MCH hormones much the same as melanophores. This suggests that fluorescent cells may have color changes throughout the day that coincide with their circadian rhythm. Fish may also be sensitive to cortisol induced stress responses to environmental stimuli, such as interaction with a predator or engaging in a mating ritual.\n\nIt is suspected by some scientists that GFPs and GFP like proteins began as electron donors activated by light. These electrons were then used for reactions requiring light energy. Functions of fluorescent proteins, such as protection from the sun, conversion of light into different wavelengths, or for signaling are thought to have evolved secondarily.\n\nThe incidence of fluorescence across the tree of life is widespread, and has been studied most extensively in a phylogenetic sense in fish. The phenomenon appears to have evolved multiple times in multiple taxa such as in the anguilliformes (eels), gobioidei (gobies and cardinalfishes), and tetradontiformes (triggerfishes), along with the other taxa discussed later in the article. Fluorescence is highly genotypically and phenotypically variable even within ecosystems, in regards to the wavelengths emitted, the patterns displayed, and the intensity of the fluorescence. Generally, the species relying upon camouflage exhibit the greatest diversity in fluorescence, likely because camouflage is one of the most common uses of fluorescence.\n\nCurrently, relatively little is known about the functional significance of fluorescence and fluorescent proteins. However, it is suspected that biofluorescence may serve important functions in signaling and communication, mating, lures, camouflage, UV protection and antioxidation, photoacclimation, dinoflagellate regulation, and in coral health.\n\nWater absorbs light of long wavelengths, so less light from these wavelengths reflects back to reach the eye. Therefore, warm colors from the visual light spectrum appear less vibrant at increasing depths. Water scatters light of shorter wavelengths, meaning cooler colors dominate the visual field in the photic zone. Light intensity decreases 10 fold with every 75 m of depth, so at depths of 75 m, light is 10% as intense as it is on the surface, and is only 1% as intense at 150 m as it is on the surface. Because the water filters out the wavelengths and intensity of water reaching certain depths, different proteins, because of the wavelengths and intensities of light they are capable of absorbing, are better suited to different depths. Theoretically, some fish eyes can detect light as deep as 1000 m. At these depths of the aphotic zone, the only sources of light are organisms themselves, giving off light through chemical reactions in a process called bioluminescence.\n\nFluorescence is simply defined as the absorption of electromagnetic radiation at one wavelength and its reemission at another, lower energy wavelength. Thus any type of fluorescence depends on the presence of external sources of light. Biologically functional fluorescence is found in the photic zone, where there is not only enough light to cause biofluorescence, but enough light for other organisms to detect it. The visual field in the photic zone is naturally blue, so colors of fluorescence can be detected as bright reds, oranges, yellows, and greens. Green is the most commonly found color in the biofluorescent spectrum, yellow the second most, orange the third, and red is the rarest. Fluorescence can occur in organisms in the aphotic zone as a byproduct of that same organism’s bioluminescence. Some biofluorescence in the aphotic zone is merely a byproduct of the organism’s tissue biochemistry and does not have a functional purpose. However, some cases of functional and adaptive significance of biofluorescence in the aphotic zone of the deep ocean is an active area of research.\n\nBony fishes living in shallow water, due to living in a colorful environment, generally have good color vision. Thus, in shallow-water fishes, red, orange, and green fluorescence most likely serves as a means of communication with conspecifics, especially given the great phenotypic variance of the phenomenon.\n\nMany fish that exhibit biofluorescence, such as sharks, lizardfish, scorpionfish, wrasses, and flatfishes, also possess yellow intraocular filters. Yellow intraocular filters in the lenses and cornea of certain fishes function as long-pass filters, thus enabling the species that possess them to visualize and potentially exploit fluorescence to enhance visual contrast and patterns that are unseen to other fishes and predators that lack this visual specialization. Fishes that possess the necessary yellow intraocular filters for visualizing biofluorescence potentially exploit a light signal from members of it or a similar functional role. Biofluorescent patterning was especially prominent in cryptically patterned fishes possessing complex camouflage, and that many of these lineages also possess yellow long-pass intraocular filters that could enable visualization of such patterns.\n\nAnother adaptive use of fluorescence is to generate red light from the ambient blue light of the photic zone to aid vision. Red light can only be seen across short distances due to attenuation of red light wavelengths by water. Many fish species that fluoresce are small, group-living, or benthic/aphotic, and have conspicuous patterning. This patterning is caused by fluorescent tissue and is visible to other members of the species, however the patterning is invisible at other visual spectra. These intraspecific fluorescent patterns also coincide with intra-species signaling. The patterns present in ocular rings to indicate directionality of an individual’s gaze, and along fins to indicate directionality of an individual’s movement. Current research suspects that this red fluorescence is used for private communication between members of the same species. Due to the prominence of blue light at ocean depths, red light and light of longer wavelengths are muddled, and many predatory reef fish have little to no sensitivity for light at these wavelengths. Fish such as the fairy wrasse that have developed visual sensitivity to longer wavelengths are able to display red fluorescent signals that give a high contrast to the blue environment and are conspicuous to conspecifics in short ranges, yet are relatively invisible to other common fish that have reduced sensitivities to long wavelengths. Thus, fluorescence can be used as adaptive signaling and intra-species communication in reef fish.\n\nAdditionally, it is suggested that fluorescent tissues that surround an organism’s eyes are used to convert blue light from the photic zone or green bioluminescence in the aphotic zone into red light to aid vision.\n\nFluorescence serves a wide variety of functions in coral. Fluorescent proteins in corals may contribute to photosynthesis by converting otherwise unusable wavelengths of light into ones for which the coral’s symbiotic algae are able to conduct photosynthesis. Also, the proteins may fluctuate in number as more or less light becomes available as a means of photoacclimation. Similarly, these fluorescent proteins may possess antioxidant capacities to eliminate oxygen radicals produced by photosynthesis. Finally, through modulating photosynthesis, the fluorescent proteins may also serve as a means of regulating the activity of the coral’s photosynthetic algal symbionts.\n\n\"Alloteuthis subulata\" and \"Loligo vulgaris\", two types of nearly transparent squid, have fluorescent spots above their eyes. These spots reflect incident light, which may serve as a means of camouflage, but also for signaling to other squids for schooling purposes.\n\nAnother, well-studied example of biofluorescence in the ocean is the hydrozoan \"Aequorea victoria\". This jellyfish lives in the photic zone off the west coast of North America and was identified as a carrier of green fluorescent protein (GFP) by Osamu Shimomura. The gene for these green fluorescent proteins has been isolated and is scientifically significant because it is widely used in genetic studies to indicate the expression of other genes.\n\nSeveral species of mantis shrimp, which are stomatopod crustaceans, including \"Lysiosquillina glabriuscula\", have yellow fluorescent markings along their antennal scales and carapace (shell) that males present during threat displays to predators and other males. The display involves raising the head and thorax, spreading the striking appendages and other maxillipeds, and extending the prominent, oval antennal scales laterally, which makes the animal appear larger and accentuates its yellow fluorescent markings. Furthermore, as depth increases, mantis shrimp fluorescence accounts for a greater part of the visible light available. During mating rituals, mantis shrimp actively fluoresce, and the wavelength of this fluorescence matches the wavelengths detected by their eye pigments.\n\n\"Siphonophorae\" is an order of marine animals from the phylum Hydrozoa that consist of a specialized medusoid and polyp zooid. Some siphonophores, including the genus Erenna that live in the aphotic zone between depths of 1600 m and 2300 m, exhibit yellow to red fluorescence in the photophores of their tentacle-like tentilla. This fluorescence occurs as a by-product of bioluminescence from these same photophores. The siphonophores exhibit the fluorescence in a flicking pattern that is used as a lure to attract prey.\n\nThe predatory deep-sea dragonfish \"Malacosteus niger\", the closely related \"Aristostomias\" genus and the species \"Pachystomias microdon\" are capable of harnessing the blue light emitted from their own bioluminescence to generate red biofluorescence from suborbital photophores. This red fluorescence is invisible to other animals, which allows these dragonfish extra light at dark ocean depths without attracting or signaling predators.\n\nThe Polka-dot tree frog, widely found in the Amazon was discovered to be the first fluorescent amphibian in 2017. The frog is pale green with dots in white, yellow or light red. The fluorescence of the frog was discovered unintentionally in Buenos Aires, Argentina. The fluorescence was traced to a new compound found in the lymph and skin glads. The main fluorescent compound is Hyloin-L1 and it gives a blue-green glow when exposed to violet or ultra violet light. Scientists behind the discovery say that the fluorescence can be used for communication. They also think that about 100 or 200 species of frogs are likely to be fluorescent.\n\nSwallowtail (\"Papilio\") butterflies have complex systems for emitting fluorescent light. Their wings contain pigment-infused crystals that provide directed fluorescent light. These crystals function to produce fluorescent light best when they absorb radiance from sky-blue light (wavelength about 420 nm). The wavelengths of light that the butterflies see the best correspond to the absorbance of the crystals in the butterfly's wings. This likely functions to enhance the capacity for signaling.\n\nParrots have fluorescent plumage that may be used in mate signaling. A study using mate-choice experiments on budgerigars (\"Melopsittacus undulates\") found compelling support for fluorescent sexual signaling, with both males and females significantly preferring birds with the fluorescent experimental stimulus. This study suggests that the fluorescent plumage of parrots is not simply a by-product of pigmentation, but instead an adapted sexual signal. Considering the intricacies of the pathways that produce fluorescent pigments, there may be significant costs involved. Therefore, individuals exhibiting strong fluorescence may be honest indicators of high individual quality, since they can deal with the associated costs.\n\nSpiders fluoresce under UV light and possess a huge diversity of fluorophores. Remarkably, spiders are the only known group in which fluorescence is “taxonomically widespread, variably expressed, evolutionarily labile, and probably under selection and potentially of ecological importance for intraspecific and interspecific signaling.” A study by Andrews et al. (2007) reveals that fluorescence has evolved multiple times across spider taxa, with novel fluorophores evolving during spider diversification. In some spiders, ultraviolet cues are important for predator-prey interactions, intraspecific communication, and camouflaging with matching fluorescent flowers. Differing ecological contexts could favor inhibition or enhancement of fluorescence expression, depending upon whether fluorescence helps spiders be cryptic or makes them more conspicuous to predators. Therefore, natural selection could be acting on expression of fluorescence across spider species.\n\nScorpions also fluoresce.\n\nThe \"Mirabilis jalapa\" flower contains violet, fluorescent betacyanins and yellow, fluorescent betaxanthins. Under white light, parts of the flower containing only betaxanthins appear yellow, but in areas where both betaxanthins and betacyanins are present, the visible fluorescence of the flower is faded due to internal light-filtering mechanisms. Fluorescence was previously suggested to play a role in pollinator attraction, however, it was later found that the visual signal by fluorescence is negligible compared to the visual signal of light reflected by the flower.\n\nChlorophyll fluoresces a weak red under ultraviolet light.\n\nGemstones, minerals, may have a distinctive fluorescence or may fluoresce differently under short-wave ultraviolet, long-wave ultraviolet, visible light, or X-rays.\n\nMany types of calcite and amber will fluoresce under shortwave UV, longwave UV and visible light. Rubies, emeralds, and diamonds exhibit red fluorescence under long-wave UV, blue and sometimes green light; diamonds also emit light under X-ray radiation.\n\nFluorescence in minerals is caused by a wide range of activators. In some cases, the concentration of the activator must be restricted to below a certain level, to prevent quenching of the fluorescent emission. Furthermore, the mineral must be free of impurities such as iron or copper, to prevent quenching of possible fluorescence. Divalent manganese, in concentrations of up to several percent, is responsible for the red or orange fluorescence of calcite, the green fluorescence of willemite, the yellow fluorescence of esperite, and the orange fluorescence of wollastonite and clinohedrite. Hexavalent uranium, in the form of the uranyl cation, fluoresces at all concentrations in a yellow green, and is the cause of fluorescence of minerals such as autunite or andersonite, and, at low concentration, is the cause of the fluorescence of such materials as some samples of hyalite opal. Trivalent chromium at low concentration is the source of the red fluorescence of ruby. Divalent europium is the source of the blue fluorescence, when seen in the mineral fluorite. Trivalent lanthanides such as terbium and dysprosium are the principal activators of the creamy yellow fluorescence exhibited by the yttrofluorite variety of the mineral fluorite, and contribute to the orange fluorescence of zircon. Powellite (calcium molybdate) and scheelite (calcium tungstate) fluoresce intrinsically in yellow and blue, respectively. When present together in solid solution, energy is transferred from the higher-energy tungsten to the lower-energy molybdenum, such that fairly low levels of molybdenum are sufficient to cause a yellow emission for scheelite, instead of blue. Low-iron sphalerite (zinc sulfide), fluoresces and phosphoresces in a range of colors, influenced by the presence of various trace impurities.\n\nCrude oil (petroleum) fluoresces in a range of colors, from dull-brown for heavy oils and tars through to bright-yellowish and bluish-white for very light oils and condensates. This phenomenon is used in oil exploration drilling to identify very small amounts of oil in drill cuttings and core samples.\n\nOrganic solutions such anthracene or stilbene, dissolved in benzene or toluene, fluoresce with ultraviolet or gamma ray irradiation. The decay times of this fluorescence are of the order of nanoseconds, since the duration of the light depends on the lifetime of the excited states of the fluorescent material, in this case anthracene or stilbene.\n\nScintillation is defined a flash of light produced in a transparent material by the passage of a particle (an electron, an alpha particle, an ion, or a high-energy photon). Stilbene and derivatives are used in scintillation counters to detect such particles. Stilbene is also one of the gain mediums used in dye lasers.\n\nFluorescence is observed in the atmosphere when the air is under energetic electron bombardment. In cases such as the natural aurora, high-altitude nuclear explosions, and rocket-borne electron gun experiments, the molecules and ions formed have a fluorescent response to light.\n\n\nThe common fluorescent lamp relies on fluorescence. Inside the glass tube is a partial vacuum and a small amount of mercury. An electric discharge in the tube causes the mercury atoms to emit mostly ultraviolet light. The tube is lined with a coating of a fluorescent material, called the \"phosphor\", which absorbs the ultraviolet and re-emits visible light. Fluorescent lighting is more energy-efficient than incandescent lighting elements. However, the uneven spectrum of traditional fluorescent lamps may cause certain colors to appear different than when illuminated by incandescent light or daylight. The mercury vapor emission spectrum is dominated by a short-wave UV line at 254 nm (which provides most of the energy to the phosphors), accompanied by visible light emission at 436 nm (blue), 546 nm (green) and 579 nm (yellow-orange). These three lines can be observed superimposed on the white continuum using a hand spectroscope, for light emitted by the usual white fluorescent tubes. These same visible lines, accompanied by the emission lines of trivalent europium and trivalent terbium, and further accompanied by the emission continuum of divalent europium in the blue region, comprise the more discontinuous light emission of the modern trichromatic phosphor systems used in many compact fluorescent lamp and traditional lamps where better color rendition is a goal.\n\nFluorescent lights were first available to the public at the 1939 New York World's Fair. Improvements since then have largely been better phosphors, longer life, and more consistent internal discharge, and easier-to-use shapes (such as compact fluorescent lamps). Some high-intensity discharge (HID) lamps couple their even-greater electrical efficiency with phosphor enhancement for better color rendition.\n\nWhite light-emitting diodes (LEDs) became available in the mid-1990s as LED lamps, in which blue light emitted from the semiconductor strikes phosphors deposited on the tiny chip. The combination of the blue light that continues through the phosphor and the green to red fluorescence from the phosphors produces a net emission of white light.\n\nGlow sticks sometimes utilize fluorescent materials to absorb light from the chemiluminescent reaction and emit light of a different color.\n\nMany analytical procedures involve the use of a fluorometer, usually with a single exciting wavelength and single detection wavelength. Because of the sensitivity that the method affords, fluorescent molecule concentrations as low as 1 part per trillion can be measured.\n\nFluorescence in several wavelengths can be detected by an array detector, to detect compounds from HPLC flow. Also, TLC plates can be visualized if the compounds or a coloring reagent is fluorescent. Fluorescence is most effective when there is a larger ratio of atoms at lower energy levels in a Boltzmann distribution. There is, then, a higher probability of excitement and release of photons by lower-energy atoms, making analysis more efficient.\n\nUsually the setup of a fluorescence assay involves a light source, which may emit many different wavelengths of light. In general, a single wavelength is required for proper analysis, so, in order to selectively filter the light, it is passed through an excitation monochromator, and then that chosen wavelength is passed through the sample cell. After absorption and re-emission of the energy, many wavelengths may emerge due to Stokes shift and various electron transitions. To separate and analyze them, the fluorescent radiation is passed through an emission monochromator, and observed selectively by a detector.\n\nFluorescence in the life sciences is used generally as a non-destructive way of tracking or analysis of biological molecules by means of the fluorescent emission at a specific frequency where there is no background from the excitation light, as relatively few cellular components are naturally fluorescent (called intrinsic or autofluorescence).\nIn fact, a protein or other component can be \"labelled\" with an extrinsic fluorophore, a fluorescent dye that can be a small molecule, protein, or quantum dot, finding a large use in many biological applications.\n\nThe quantification of a dye is done with a spectrofluorometer and finds additional applications in:\n\n\n\nFingerprints can be visualized with fluorescent compounds such as ninhydrin or DFO (1,8-Diazafluoren-9-one). Blood and other substances are sometimes detected by fluorescent reagents, like fluorescein. Fibers, and other materials that may be encountered in forensics or with a relationship to various collectibles, are sometimes fluorescent.\n\nFluorescent penetrant inspection is used to find cracks and other defects on the surface of a part. Dye tracing, using fluorescent dyes, is used to find leaks in liquid and gas plumbing systems.\n\nFluorescent colors are frequently used in signage, particularly road signs. Fluorescent colors are generally recognizable at longer ranges than their non-fluorescent counterparts, with fluorescent orange being particularly noticeable. This property has led to its frequent use in safety signs and labels.\n\nFluorescent compounds are often used to enhance the appearance of fabric and paper, causing a \"whitening\" effect. A white surface treated with an optical brightener can emit more visible light than that which shines on it, making it appear brighter. The blue light emitted by the brightener compensates for the diminishing blue of the treated material and changes the hue away from yellow or brown and toward white. Optical brighteners are used in laundry detergents, high brightness paper, cosmetics, high-visibility clothing and more.\n\n", "id": "11555", "title": "Fluorescence"}
{"url": "https://en.wikipedia.org/wiki?curid=44637603", "text": "Protein mimetic\n\nA protein mimetic is a molecule such as a peptide, a modified peptide or any other molecule that biologically mimics the action or activity of some other protein. \nProtein mimetics are commonly used in drug design and discovery.\n\nThere are a number of different distinct classes of protein mimetics.\n\n", "id": "44637603", "title": "Protein mimetic"}
{"url": "https://en.wikipedia.org/wiki?curid=45458060", "text": "End-sequence profiling\n\nEnd-sequence profiling (ESP) (sometimes “Paired-end mapping (PEM)”) is a method based on sequence-tagged connectors developed to facilitate \"de novo\" genome sequencing to identify high-resolution copy number and structural aberration such as inversion and translocation. \nBriefly, the target genomic DNA is isolated and partially digested with restriction enzymes into large fragments. Following size-fractionation, the fragments is cloned into plasmids to construct artificial chromosome such as bacterial artificial chromosomes (BAC) which will be sequenced and compared to the reference genome. The differences, including orientation and length variations between constructed chromosomes and the reference genome, will suggest copy number and structural aberration.\n\nBefore analyzing target genome structural aberration and copy number variation (CNV) with ESP, the target genome is usually amplified and conserved with artificial chromosome construction. The classic strategy to construct an artificial chromosome is bacterial artificial chromosome (BAC). Basically, the target chromosome is randomly digested and inserted into plasmids which are transformed and cloned in bacteria. The size of fragments inserted is 150–350 kb. Another commonly used artificial chromosome is fosmid. The difference between BAC and fosmids is the size of the DNA inserted. Fosmids can only hold 40 kb DNA fragments, which allows a more accurate breakpoint determination.\n\nEnd sequence profiling (ESP) can be used to detect structural variations such as insertions, deletions, and chromosomal rearrangement. Compare to other methods that look at chromosomal abnormalities, ESP is particularly useful to identify copy neutral abnormalities such as inversions and translocations that would not be apparent when looking at copy number variation. From the BAC library, both ends of the inserted fragments are sequenced using a sequencing platform. Detection of variations is then achieved by mapping the sequenced reads onto a reference genome.\n\nInversions and translocations are relatively easy to detect by an invalid pair of sequenced-end. For instance, a translocation can be detected if the paired-ends are mapped onto different chromosomes on the reference genome. Inversion can be detected by divergent orientation of the reads, where the insert will have two plus-end or two minus-end.\nIn the case of an insertion or a deletion, mapping of the paired-end is consistent with the reference genome. But the read are disconcordant in apparent size. The apparent size is the distance of the BAC sequenced-ends mapped in the reference genome. If a BAC has an insert of length (l), a concordant mapping will show a fragment of size (l) in the reference genome. If the paired-ends are closer than distance (l), an insertion is suspected in the sampled DNA. A distance of (l< µ-3σ) can be used as a cut-off to detect an insertion, where µ is the mean length of the insert and σ is the standard deviation. In case of a deletion, the paired-ends are mapped further away in the reference genome compared to the expected distance (l> µ-3σ).\n\nIn some cases, discordant reads can also indicate a CNV for example in sequences repeats. For larger CNV, the density of the reads will vary accordingly to the copy number. An increase of copy numbers will be reflected by increasing mapping of the same region on the reference genome.\n\nESP was first developed and published in 2003 by Dr. Collins and his colleagues in University of California, San Francisco. Their study revealed the chromosome rearrangements and CNV of MCF7 human cancer cells at a 150kb resolution, which is much more accurate compared to both CGH and spectral karyotyping at that time. In 2007, Dr. Snyder and his group improved the ESP to 3kb resolution by sequencing both pairs of 3-kb DNA fragments without BAC construction. Their approach is able to identify deletions, inversions, insertions with an average breakpoint resolution of 644bp, which close to the resolution of polymerase chain reaction (PCR).\n\nVarious bioinformatics tools can be used to analyze end-sequence profiling. Common ones include BreakDancer, PEMer, Variation Hunter, common LAW, GASV, and Spanner. ESP can be used to map structural variation at high-resolution in disease tissue. This technique is mainly used on tumor samples from different cancer types. Accurate identification of copy neutral chromosomal abnormalities is particularly important as translocation can lead to fusion proteins, chimeric proteins, or misregulated proteins that can be seen in tumors. This technique can also be used in evolution studies by identifying large structural variation between different populations. Similar methods are being developed for various applications. For example, a barcoded Illumina paired-end sequencing (BIPES) approach was used to assess microbial diversity by sequencing the 16S V6 tag.\n\nResolution of structural variation detection by ESP has been increased to the similar level as PCR, and can be further improved by selection of more evenly sized DNA fragments. ESP can be applied for either with or without constructed artificial chromosome. With BAC, precious samples can be immortalized and conserved, which is particularly important for small quantity of smalls which are planned for extensive analyses. Furthermore, BACs carrying rearranged DNA fragments can be directly transfected \"in vitro\" or \"in vivo\" to analyze the function of these arrangements. However, BAC construction is still expensive and labor-intensive. Researchers should be really careful to choose which strategy they need for particular project. Because ESP only looks at short paired-end sequences, it has the advantage of providing useful information genome-wide without the need for large-scale sequencing. Approximately 100-200 tumors can be sequenced at a resolution greater than 150kb when compared to sequencing an entire genome.\n\n\n", "id": "45458060", "title": "End-sequence profiling"}
{"url": "https://en.wikipedia.org/wiki?curid=27460882", "text": "List of restriction enzyme cutting sites: Bd–Bp\n\nThis article contains a list of the most studied restriction enzymes whose names start with Bd to Bp inclusive. It contains approximately 100 enzymes.\n\nThe following information is given:\n", "id": "27460882", "title": "List of restriction enzyme cutting sites: Bd–Bp"}
{"url": "https://en.wikipedia.org/wiki?curid=27460930", "text": "List of restriction enzyme cutting sites: Bsa–Bso\n\nThis article contains a list of the most studied restriction enzymes whose names start with Bsa to Bso inclusive. It contains approximately 90 enzymes.\nThe following information is given:\n", "id": "27460930", "title": "List of restriction enzyme cutting sites: Bsa–Bso"}
{"url": "https://en.wikipedia.org/wiki?curid=27456459", "text": "List of restriction enzyme cutting sites: Bsp–Bss\n\nThis article contains a list of the most studied restriction enzymes whose names start with Bsp to Bss inclusive. It contains approximately 180 enzymes.\n\nThe following information is given:\n", "id": "27456459", "title": "List of restriction enzyme cutting sites: Bsp–Bss"}
{"url": "https://en.wikipedia.org/wiki?curid=27456863", "text": "List of restriction enzyme cutting sites: Bst–Bv\n\nThis article contains a list of the most studied restriction enzymes whose names start with Bst to Bv inclusive. It contains approximately 200 enzymes.\n\nThe following information is given:\n", "id": "27456863", "title": "List of restriction enzyme cutting sites: Bst–Bv"}
{"url": "https://en.wikipedia.org/wiki?curid=27460950", "text": "List of restriction enzyme cutting sites: C–D\n\nThis article contains a list of the most studied restriction enzymes whose names start with C to D inclusive. It contains approximately 80 enzymes.\n\nThe following information is given:\n", "id": "27460950", "title": "List of restriction enzyme cutting sites: C–D"}
{"url": "https://en.wikipedia.org/wiki?curid=27460957", "text": "List of restriction enzyme cutting sites: E–F\n\nThis article contains a list of the most studied restriction enzymes whose names start with E to F inclusive. It contains approximately 110 enzymes.\n\nThe following information is given:\n", "id": "27460957", "title": "List of restriction enzyme cutting sites: E–F"}
{"url": "https://en.wikipedia.org/wiki?curid=27459241", "text": "List of restriction enzyme cutting sites: G–K\n\nThis article contains a list of the most studied restriction enzymes whose names start with G to K inclusive. It contains approximately 90 enzymes.\n\nThe following information is given:\n", "id": "27459241", "title": "List of restriction enzyme cutting sites: G–K"}
{"url": "https://en.wikipedia.org/wiki?curid=27457008", "text": "List of restriction enzyme cutting sites: L–N\n\nThis article contains a list of the most studied restriction enzymes whose names start with L to N inclusive. It contains approximately 120 enzymes.\n\nThe following information is given:\n", "id": "27457008", "title": "List of restriction enzyme cutting sites: L–N"}
{"url": "https://en.wikipedia.org/wiki?curid=27458835", "text": "List of restriction enzyme cutting sites: O–R\n\nThis article contains a list of the most studied restriction enzymes whose names start with O to R inclusive. It contains approximately 130 enzymes.\nThe following information is given:\n", "id": "27458835", "title": "List of restriction enzyme cutting sites: O–R"}
{"url": "https://en.wikipedia.org/wiki?curid=27458759", "text": "List of restriction enzyme cutting sites: T–Z\n\nThis article contains a list of the most studied restriction enzymes whose names start with T to Z inclusive. It contains approximately 70 enzymes.\n\nThe following information is given:\n", "id": "27458759", "title": "List of restriction enzyme cutting sites: T–Z"}
{"url": "https://en.wikipedia.org/wiki?curid=25416659", "text": "Mitochondrial replacement therapy\n\nMitochondrial replacement (MRT, sometimes called mitochondrial donation) is a special form of in vitro fertilisation in which the future baby's mitochondrial DNA comes from a third party. This technique is used in cases when mothers carry genes for mitochondrial diseases. The two most common techniques in mitochondrial donation are pronuclear transfer and maternal spindle transfer.\n\nIn 2015 MRT was made legal in the United Kingdom and in 2016 the first regulations were issued there, clearing the way for procedures to begin. In February 2016, the US National Academy of Sciences issued a report describing technologies then current and the surrounding ethical issues.\n\nMitochondrial replacement therapy performed via pronuclear transfer or maternal spindle transfer was approved for use in the UK in December 2016 to prevent the transmission of mitochondrial diseases from mother to child; it could only be performed in clinics licensed by the UK's Human Fertilisation and Embryology Authority (HFEA), only for people individually approved the HFEA, for whom preimplantation genetic diagnosis is unlikely to be helpful, and only with informed consent that the risks and benefits are not well understood.\n\nRelevant mutations are found in about 0.5% of the population and disease affects around one in 5000 individuals (0.02%) — the percentage of people affected is much smaller because cells contain many mitochondria, only some of which carry mutations, and the number of mutated mitochondria need to reach a threshold in order to affect the entire cell, and many cells need to be affected for the person to show disease.\n\nThe average number of births per year among women at risk for transmitting mtDNA disease is estimated to approximately 150 in the United Kingdom and 800 in the United States.\n\nPrior to the development of MRT, and in places where it is not legal or feasible, the reproductive options for women who are at risk for transmitting mtDNA disease and who want to prevent transmission were using an egg from another woman, adoption, or childlessness.\n\nMRT is a form of in vitro fertilization and involves removing eggs from a woman, removing sperm from a man, fertilizing the egg with the sperm, allowing the fertilized egg to form a blastocyst, and then implanting the blastocyst. MRT involves an additional egg from a third person, and manipulating both the recipient egg and the donor egg.\n\nAs of 2016 there were three MRT techniques: maternal spindle transfer (MST), pronuclear transfer (PNT), and more recently as of that date, polar body transfer (PBT).\nIn MST, an oocyte is removed from the recipient, and when it is in the metaphase II stage of cell division, the spindle-chromosome complex is removed; some of cytoplasm comes with it, so some mitochondria are likely included. The spindle-chromosome complex is inserted into a donor oocyte from which the nucleus has already been removed. This egg is fertilized with sperm, and allowed to form a blastocyst, which can then be investigated with preimplantation genetic diagnosis to check for mitochondrial mutations, prior to being implanted in the recipient's uterus.\n\nIn pronuclear transfer, an oocyte is removed from the recipient, and fertilized with sperm. The donor oocyte is fertilized with sperm from the same person. The male and female pronuclei are removed from each fertilized egg prior to their fusing, and the pronuclei from the recipient's fertilized egg are inserted into the fertilized egg from the donor. As with MST, a small amount of cytoplasm from the donor egg may be transferred, and as with MST, the fertilized egg is allowed to form a blastocyst, which can then be investigated with preimplantation genetic diagnosis to check for mitochondrial mutations, prior to being implanted in the recipient's uterus.\nIn polar body transfer, a polar body (a small cell with very little cytoplasm that is created when an egg cell divides) from the recipient is used in its entirety, instead of using nuclear material extracted from the recipient's normal egg; this can be used in either MST or PNT. This technique was first published in 2014 and as of 2015 it had not been consistently replicated, but is considered promising as there is a greatly reduced chance for transmitting mitochondria from the recipient because polar bodies contain very few mitochondria, and it does not involve extracting material from the recipient's egg.\nCytoplasmic transfer was originally developed in the 1980s in the course of basic research conducted with mice to study the role that parts of the cell outside of the nucleus played in embryonic development. In this technique, cytoplasm including proteins, mRNA, mitochondria and other organelles, is taken from a donor egg, and injected into the recipient egg, resulting in a mixture of mitochondrial genetic material. This technique started to be used in the late 1990s to \"boost\" the eggs of older women who wanted to conceive but were having problems and led to the birth of about 30 babies. Concerns were raised that the mixture of genetic material and proteins could cause problems with respect to epigenetic clashes, or differences in the ability of the recipient and donor materials to effect the development process, or due to the injection of the donor material. After three children born through the technique were found to have developmental disorders (2 cases of Turner's syndrome and one case of pervasive developmental disorder (an autism spectrum disorder), the FDA banned the procedure until a clinical trial could prove its safety. As of 2015 that study had not been conducted, but the procedure was still being used in other countries.\n\nMRT is a form of assisted reproductive technology that involves preimplantation genetic screening of the mother, preimplantation genetic diagnosis after the egg is fertilized, and in vitro fertilization. It has all the risks of those procedures.\n\nIn addition, both procedures used in MRT entail their own risks. On one level, the procedures physically disrupt two oocytes, removing nuclear genetic material from the recipient egg or fertilized egg, and inserting the nuclear genetic material into the donor unfertilized or fertilized egg; the manipulations for both procedures may cause various forms of damage that were not well understood as of 2016.\n\nMaternal mitochondria will be carried over to the donor egg; as of 2016 it was estimated that using techniques current in the UK, maternal mitochondria will comprise only around 2% or less of mitochondria in the resulting egg, a level that was considered safe by the HFEA and within the limits of mitochondrial variation that most people have.\n\nBecause MRT procedures involve actions at precise times during egg development and fertilization, and involves manipulating eggs, there is a risk that eggs may mature abnormally or that fertilization may happen abnormally; as of 2016 the HFEA judged that laboratory techniques in the UK had been well enough developed to manage these risks to proceed cautiously with making MRT available.\n\nBecause mitochondria in the final egg will come from a third party, different from the two parties whose DNA is in the nucleus, and because nuclear DNA codes for genes that make some of the proteins and mRNA used by mitochondria, there is a theoretical risk of adverse \"mito-nuclear\" interactions. While this theoretical risk could possibly be managed by attempting to match the haplotype of the donor and the recipient, as of 2016 there was no evidence that was an actual risk.\n\nFinally, there is a risk of epigenetic modification to DNA in the nucleus and mitochondria, caused by the procedure itself, or by mito-nuclear interactions. As of 2016 these risks appeared to be minimal but were being monitoring by long-term study of children born from the procedure.\n\nIn the United States in 1996 embryologist Jacques Cohen and others at the Institute for Reproductive Medicine and Science, Saint Barnabas Medical Center in Livingston, New Jersey first used cytoplasmic transfer in a human assisted reproduction procedure. In 1997 the first baby was born using this procedure (Emma Ott). In 2001, Cohen and others reported that 10 single babies, twins, and a quadruplet at his New Jersey clinic and a further six children in Israel had been born using his technique. Using modifications of his procedure, a baby had been born at Eastern Virginia Medical School, five children at the Lee Women's Hospital Infertility Clinic in Taichung, Taiwan. twins in Naples, Italy and a twins in India. In total as of 2016, 30-50 children worldwide had been reported to have been born using cytoplasmic transfer.\n\nIn 2002, the US Food and Drug Administration (FDA) asked a Biological Response Modifiers Advisory Committee Meeting to advise on the technique of cytoplasmic transfer to Treat Infertility. This committee felt that there were risks at the time of inadvertent transfer of chromosomes and enhanced survival of abnormal embryos. The FDA informed clinics that they considered the cytoplasmic transfer technique as a new treatment, and, as such, it would require an Investigational New Drug (IND) application. Cohen's clinic started the pre-IND application but the clinic then went private, funding for the application dried up, the application was abandoned, the research team disbanded, and the cytoplasmic transfer procedure fell out of favor. In 2016, 12 (out of the 13) parents of children born using cytoplasmic transfer at the Saint Barnabas Center participated in a limited follow-up inquiry via online questionnaire. Children whose ages then were 13-18 reported no major problems.\n\nIn 2009, a team in Japan published studies of mitochondrial donation. In the same year, a team led by scientists at Oregon Health & Science University published results of mitochondrial donation in monkeys; that team published an update reporting on the health of the monkeys born with the technique, as well as further work it had done on human embryos.\n\nHuman trials in oocytes in 2010 by Craven, et al. were successful in reducing transmission of mutated mtDNA. The results of the study found the mean tDNA carryover to stay under 2% in all of the experimental embryos. This was true for both the MI-SCC and PN transfer methods of MTR. This research did not extend past the blastocyst stage because of ethical concerns, but there are still concerns about whether or not results retrieved from the blastocyst stage are viable representations of whole embryos. Because of these speculations and to further the viability of MTR as a safe and effective technique, further research and clinical trials need to be initiated in order to test the efficacy of MTR in the long term in human patients.\n\nIn the United Kingdom, following animal experiments and the recommendations of a government commissioned expert committee, the Human Fertilisation and Embryology (Research Purposes) Regulations were passed in 2001 regulating and allowing research into human embryos. In 2004, the Newcastle University applied for a licence to develop pronuclear transfer to avoid the transmission of mitochondrial diseases, and was granted the license in 2005. Following further research by Newcastle and the Wellcome Foundation, scientific review, public consultations, and debate, the UK government recommended that mitochondrial donation be legalized in 2013. In 2015 parliament passed the Human Fertilisation and Embryology (Mitochondrial Donation) Regulations, which came into force on 29 October 2015, making human mitochondrial donation legal in the UK. The Human Fertilisation and Embryology Authority (HFEA) was authorized to licence and regulate medical centers which wanted to use human mitochondrial donation. The HFEA Safety Committee issued its fourth report in November 2016 recommending procedures under which HFEA should authorize MRT, the HFEA issued their regulations in December 2016 and granted their first licence (to Newcastle University) in March 2017.\n\nProfessor Douglass Turnbull, the driving force behind mitochondrial research at Newcastle University, was awarded a knighthood in 2016.\n\nIn 2016, John Zhang and his team at New Hope Fertility Center in New York, USA used the spindle transfer technique to help a Jordanian woman to give birth to a baby boy in Mexico where there was no law against using such a technique. The mother had Leigh disease and already had four miscarriages and two children who had died of the disease. Ukrainian doctors at the Nadiya clinic in Kiev also reported that they had used the pronuclear transfer method of mitichondrial donation to help two previously infertile women aged 34 and 29 become pregnant. The doctors first got approval from an ethical review board of the Ukrainian Association of Reproductive Medicine but there was no law in the Ukraine against mitochondrial donation. A baby boy was born to the 34-year-old woman in January 2017 and genetic test results were reported as normal.\n\nIn August 2017, in a letter to Darwin Life, Inc. and New Hope Fertility Center, the FDA warned that the technique should not be marketed in the U.S.\n\nAs of February 2016, the United States had no regulations governing mitochondrial donation, and Congress barred the FDA from evaluating any applications that involve implanting modified embryos into a woman.\n\nThe United Kingdom became the first country to legalize the procedure; the UK's chief medical officer recommended it be legalized in 2013; parliament passed The Human Fertilisation and Embryology (Mitochondrial Donation) Regulations in 2015, and the regulatory authority published regulations in 2016.\n\nDespite the promising outcomes of the two techniques, pronuclear transfer and spindle transfer, mitochondrial gene replacement raises ethical and social concerns.\n\nMitochondrial donation involves modification of the germline, and hence such modifications would be passed on to subsequent generations. Using human embryos for in vitro research is also controversial, as embryos are created specifically for research and the financial compensation of egg donors.\n\nImplications for identity is another ethical concern with psychological and emotional impacts on a child's life regarding of a person's sense of identity. It debates whether the genetic make-up of children born as a result of mitochondrial replacement affect their emotional well-being when they are aware that they are different from other healthy children conceived from two parents.\n\nOpponents argue that scientists are \"playing God\" and that children with three genetic parents may suffer both psychological and physical damage.\n\nOn the other hand, New York University researcher James Grifo, a critic of the American ban, has argued that society \"would never have made the advances in treating infertility that we have if these bans had been imposed 10 years\" earlier.\n\nOn February 3, 2016, a report was issued by the Institute of Medicine of the National Academies of Sciences, Engineering, and Medicine (commissioned by the U.S. Food and Drug Administration) addressing whether it is ethically permissible for clinical research into mitochondrial replacement techniques (MRT) to continue. The report, titled \"Mitochondrial Replacement Techniques: Ethical, Social, and Policy Considerations\" analyzes multiple facets of the arguments surrounding MRT and concludes that it is 'ethically permissible' to continue clinical investigations of MRT, so long as certain conditions are met. They recommended that initially it should only be used for male embryos to ensure that DNA with mitochondrial disease won't be passed on.\n", "id": "25416659", "title": "Mitochondrial replacement therapy"}
{"url": "https://en.wikipedia.org/wiki?curid=45414429", "text": "Nucleic acid hybridization\n\nIn molecular biology, hybridization (or hybridisation) is a phenomenon in which single-stranded deoxyribonucleic acid (DNA) or ribonucleic acid (RNA) molecules anneal to complementary DNA or RNA. Though a double-stranded DNA sequence is generally stable under physiological conditions, changing these conditions in the laboratory (generally by raising the surrounding temperature) will cause the molecules to separate into single strands. These strands are complementary to each other but may also be complementary to other sequences present in their surroundings. Lowering the surrounding temperature allows the single-stranded molecules to anneal or “hybridize” to each other.\n\nDNA replication and transcription of DNA into RNA both rely upon nucleotide hybridization, as do molecular biology techniques including Southern blots and Northern blots, the polymerase chain reaction (PCR), and most approaches to DNA sequencing.\n\nHybridization is a basic property of nucleotide sequences and is taken advantage of in numerous molecular biology techniques. Overall genetic relatedness of two species can be determined by hybridizing segments of their DNA (DNA-DNA hybridization). Due to sequence similarity between closely related organisms, higher temperatures are required to melt such DNA hybrids when compared to more distantly related organisms. A variety of different methods use hybridization to pinpoint the origin of a DNA sample, including the polymerase chain reaction (PCR). In another technique, short DNA sequences are hybridized to cellular mRNAs to identify expressed genes. Pharmaceutical drug companies are exploring the use of antisense RNA to bind to undesired mRNA, preventing the ribosome from translating the mRNA into protein.\nFluorescence in situ hybridization (FISH) is a laboratory method used to detect and locate a DNA sequence, often on a particular chromosome.\n\nResearchers Joseph Gall and Mary Lou Pardue realized in the 1960s that molecular hybridization could be used to identify the position of DNA sequences \"in situ\" (i.e., in their natural positions within a chromosome). In 1969, the two scientists published a paper demonstrating that radioactive copies of a ribosomal DNA sequence could be used to detect complementary DNA sequences in the nucleus of a frog egg. Since those original observations, many refinements have increased the versatility and sensitivity of the procedure to the extent that in situ hybridization is now considered an essential tool in cytogenetics.\n\n", "id": "45414429", "title": "Nucleic acid hybridization"}
{"url": "https://en.wikipedia.org/wiki?curid=45221774", "text": "Taxonomic list of viruses\n\nThis is a taxonomic list of viruses according to the most recent (2014) taxonomy release by the International Committee on Taxonomy of Viruses (ICTV), placed into the groups of the Baltimore classification system. Though not used by the ICTV, Baltimore classification, which groups viruses together based on how they produce mRNA, is used in conjunction with the ICTV's work in modern virus classification.\n\n", "id": "45221774", "title": "Taxonomic list of viruses"}
{"url": "https://en.wikipedia.org/wiki?curid=44290", "text": "DNA profiling\n\nDNA profiling (also called DNA fingerprinting, DNA testing, or DNA typing) is the process of determining an individual's DNA characteristics, called a DNA profile, that is very likely to be different in unrelated individuals, thereby being as unique to individuals as are fingerprints (hence the alternative name for the technique). DNA profiling with the aim of identifying not an individual but a species is called DNA barcoding.\n\nDNA profiling is most commonly used as a forensic technique in criminal investigations to identify an unidentified person or whose identity needs to be confirmed, or to place a person at a crime scene or to eliminate a person from consideration. DNA profiling has also been used to help clarify paternity, in immigration disputes, in parentage testing and in genealogical research or medical research. DNA fingerprinting has also been used in the study of animal and floral populations and in the fields of zoology, botany, and agriculture.\n\nThe modern process of DNA profiling was developed in 1984 by Sir Alec Jeffreys while working in the Department of Genetics at the University of Leicester.\n\nAlthough 99.9% of human DNA sequences are the same in every person, enough of the DNA is different that it is possible to distinguish one individual from another, unless they are monozygotic (\"identical\") twins. DNA profiling uses repetitive (\"repeat\") sequences that are highly variable, called variable number tandem repeats (VNTRs), in particular short tandem repeats (STRs), also known as microsatellites, and minisatellites. VNTR loci are very similar between closely related individuals, but are so variable that unrelated individuals are extremely unlikely to have the same VNTRs.\n\nDeveloped by Professor of Genetics Sir Alec Jeffreys, the process begins with a sample of an individual's DNA (typically called a \"reference sample\"). A common method of collecting a reference sample is the use of a buccal swab, which is easy, non-invasive and cheap. When this is not available (e.g. because a court order is needed but not obtainable) other methods may need to be used to collect a sample of blood, saliva, semen, vaginal lubrication, or other appropriate fluid or tissue from personal items (e.g. a toothbrush, razor) or from stored samples (e.g. banked sperm or biopsy tissue). Samples obtained from blood relatives (related by birth, not marriage) can provide an indication of an individual's profile, as could human remains that had been previously profiled.\n\nA reference sample is then analyzed to create the individual's DNA profile using one of a number of techniques, discussed below. The DNA profile is then compared against another sample to determine whether there is a genetic match.\n\nThe first methods for finding out genetics used for DNA profiling involved RFLP analysis. DNA is collected from cells, such as a blood sample, and cut into small pieces using a restriction enzyme (a restriction digest). This generates thousands of DNA fragments of differing sizes as a consequence of variations between DNA sequences of different individuals. The fragments are then separated on the basis of size using gel electrophoresis.\n\nThe separated fragments are then transferred to a nitrocellulose or nylon filter; this procedure is called a Southern blot. The DNA fragments within the blot are permanently fixed to the filter, and the DNA strands are denatured. Radiolabeled probe molecules are then added that are complementary to sequences in the genome that contain repeat sequences. These repeat sequences tend to vary in length among different individuals and are called variable number tandem repeat sequences or VNTRs. The probe molecules hybridize to DNA fragments containing the repeat sequences and excess probe molecules are washed away. The blot is then exposed to an X-ray film. Fragments of DNA that have bound to the probe molecules appear as fluoresent bands on the film.\n\nThe Southern blot technique is laborious, and requires large amounts of undegraded sample DNA. Also, Karl Brown's original technique looked at many minisatellite loci at the same time, increasing the observed variability, but making it hard to discern individual alleles (and thereby precluding paternity testing). These early techniques have been supplanted by PCR-based assays.\n\nDeveloped by Kary Mullis in 1983, a process was reported by which specific portions of the sample DNA can be amplified almost indefinitely (Saiki et al. 1985, 1988). This has revolutionized the whole field of DNA study. The process, polymerase chain reaction (PCR), mimics the biological process of DNA replication, but confines it to specific DNA sequences of interest. With the invention of the PCR technique, DNA profiling took huge strides forward in both discriminating power and the ability to recover information from very small (or degraded) starting samples.\n\nPCR greatly amplifies the amounts of a specific region of DNA. In the PCR process, the DNA sample is denatured into the separate individual polynucleotide strands through heating. Two oligonucleotide DNA primers are used to hybridize to two corresponding nearby sites on opposite DNA strands in such a fashion that the normal enzymatic extension of the active terminal of each primer (that is, the 3’ end) leads toward the other primer. PCR uses replication enzymes that are tolerant of high temperatures, such as the thermostable Taq polymerase. In this fashion, two new copies of the sequence of interest are generated. Repeated denaturation, hybridization, and extension in this fashion produce an exponentially growing number of copies of the DNA of interest. Instruments that perform thermal cycling are readily available from commercial sources. This process can produce a million-fold or greater amplification of the desired region in 2 hours or less.\n\nEarly assays such as the HLA-DQ alpha reverse dot blot strips grew to be very popular due to their ease of use, and the speed with which a result could be obtained. However, they were not as discriminating as RFLP analysis. It was also difficult to determine a DNA profile for mixed samples, such as a vaginal swab from a sexual assault victim.\n\nHowever, the PCR method was readily adaptable for analyzing VNTR, in particular STR loci. In recent years, research in human DNA quantitation has focused on new \"real-time\" quantitative PCR (qPCR) techniques. Quantitative PCR methods enable automated, precise, and high-throughput measurements. Inter-laboratory studies have demonstrated the importance of human DNA quantitation on achieving reliable interpretation of STR typing and obtaining consistent results across laboratories.\n\nThe system of DNA profiling used today is based on PCR and uses simple sequences or short tandem repeats (STR). This method uses highly polymorphic regions that have short repeated sequences of DNA (the most common is 4 bases repeated, but there are other lengths in use, including 3 and 5 bases). Because unrelated people almost certainly have different numbers of repeat units, STRs can be used to discriminate between unrelated individuals. These STR loci (locations on a chromosome) are targeted with sequence-specific primers and amplified using PCR. The DNA fragments that result are then separated and detected using electrophoresis. There are two common methods of separation and detection, capillary electrophoresis (CE) and gel electrophoresis.\n\nEach STR is polymorphic, but the number of alleles is very small. Typically each STR allele will be shared by around 5 - 20% of individuals. The power of STR analysis comes from looking at multiple STR loci simultaneously. The pattern of alleles can identify an individual quite accurately. Thus STR analysis provides an excellent identification tool. The more STR regions that are tested in an individual the more discriminating the test becomes.\n\nFrom country to country, different STR-based DNA-profiling systems are in use. In North America, systems that amplify the CODIS 20 core loci are almost universal, whereas in the United Kingdom the DNA-17 17 loci system (which is compatible with The National DNA Database) is in use, and Australia uses 18 core markers. Whichever system is used, many of the STR regions used are the same. These DNA-profiling systems are based on multiplex reactions, whereby many STR regions will be tested at the same time.\n\nThe true power of STR analysis is in its statistical power of discrimination. Because the 20 loci that are currently used for discrimination in CODIS are independently assorted (having a certain number of repeats at one locus does not change the likelihood of having any number of repeats at any other locus), the product rule for probabilities can be applied. This means that, if someone has the DNA type of ABC, where the three loci were independent, we can say that the probability of having that DNA type is the probability of having type A times the probability of having type B times the probability of having type C. This has resulted in the ability to generate match probabilities of 1 in a quintillion (1x10) or more. However, DNA database searches showed much more frequent than expected false DNA profile matches. Moreover, since there are about 12 million monozygotic twins on Earth, the theoretical probability is not accurate.\n\nIn practice, the risk of contaminated-matching is much greater than matching a distant relative, such as contamination of a sample from nearby objects, or from left-over cells transferred from a prior test. The risk is greater for matching the most common person in the samples: Everything collected from, or in contact with, a victim is a major source of contamination for any other samples brought into a lab. For that reason, multiple control-samples are typically tested in order to ensure that they stayed clean, when prepared during the same period as the actual test samples. Unexpected matches (or variations) in several control-samples indicates a high probability of contamination for the actual test samples. In a relationship test, the full DNA profiles should differ (except for twins), to prove that a person was not actually matched as being related to their own DNA in another sample.\n\nAnother technique, AmpFLP, or amplified fragment length polymorphism was also put into practice during the early 1990s. This technique was also faster than RFLP analysis and used PCR to amplify DNA samples. It relied on variable number tandem repeat (VNTR) polymorphisms to distinguish various alleles, which were separated on a polyacrylamide gel using an allelic ladder (as opposed to a molecular weight ladder). Bands could be visualized by silver staining the gel. One popular focus for fingerprinting was the D1S80 locus. As with all PCR based methods, highly degraded DNA or very small amounts of DNA may cause allelic dropout (causing a mistake in thinking a heterozygote is a homozygote) or other stochastic effects. In addition, because the analysis is done on a gel, very high number repeats may bunch together at the top of the gel, making it difficult to resolve. AmpFLP analysis can be highly automated, and allows for easy creation of phylogenetic trees based on comparing individual samples of DNA. Due to its relatively low cost and ease of set-up and operation, AmpFLP remains popular in lower income countries.\n\nUsing PCR technology, DNA analysis is widely applied to determine genetic family relationships such as paternity, maternity, siblingship and other kinships.\n\nDuring conception, the father’s sperm cell and the mother’s egg cell, each containing half the amount of DNA found in other body cells, meet and fuse to form a fertilized egg, called a zygote. The zygote contains a complete set of DNA molecules, a unique combination of DNA from both parents. This zygote divides and multiplies into an embryo and later, a full human being.\n\nAt each stage of development, all the cells forming the body contain the same DNA—half from the father and half from the mother. This fact allows the relationship testing to use all types of all samples including loose cells from the cheeks collected using buccal swabs, blood or other types of samples.\n\nThere are predictable inheritance patterns at certain locations (called loci) in the human genome, which have been found to be useful in determining identity and biological relationships. These loci contain specific DNA markers that scientists use to identify individuals. In a routine DNA paternity test, the markers used are short tandem repeats (STRs), short pieces of DNA that occur in highly differential repeat patterns among individuals.\n\nEach person’s DNA contains two copies of these markers—one copy inherited from the father and one from the mother. Within a population, the markers at each person’s DNA location could differ in length and sometimes sequence, depending on the markers inherited from the parents.\n\nThe combination of marker sizes found in each person makes up his/her unique genetic profile. When determining the relationship between two individuals, their genetic profiles are compared to see if they share the same inheritance patterns at a statistically conclusive rate.\n\nFor example, the following sample report from this commercial DNA paternity testing laboratory Universal Genetics signifies how relatedness between parents and child is identified on those special markers:\n\nThe partial results indicate that the child and the alleged father’s DNA match among these five markers. The complete test results show this correlation on 16 markers between the child and the tested man to enable a conclusion to be drawn as to whether or not the man is the biological father.\n\nEach marker is assigned with a Paternity Index (PI), which is a statistical measure of how powerfully a match at a particular marker indicates paternity. The PI of each marker is multiplied with each other to generate the Combined Paternity Index (CPI), which indicates the overall probability of an individual being the biological father of the tested child relative to a randomly selected man from the entire population of the same race. The CPI is then converted into a Probability of Paternity showing the degree of relatedness between the alleged father and child.\n\nThe DNA test report in other family relationship tests, such as grandparentage and siblingship tests, is similar to a paternity test report. Instead of the Combined Paternity Index, a different value, such as a Siblingship Index, is reported.\n\nThe report shows the genetic profiles of each tested person. If there are markers shared among the tested individuals, the probability of biological relationship is calculated to determine how likely the tested individuals share the same markers due to a blood relationship.\n\nRecent innovations have included the creation of primers targeting polymorphic regions on the Y-chromosome (Y-STR), which allows resolution of a mixed DNA sample from a male and female or cases in which a differential extraction is not possible. Y-chromosomes are paternally inherited, so Y-STR analysis can help in the identification of paternally related males. Y-STR analysis was performed in the Sally Hemings controversy to determine if Thomas Jefferson had sired a son with one of his slaves.\nThe analysis of the Y-chromosome yields weaker results than autosomal chromosome analysis. The Y male sex-determining chromosome, as it is inherited only by males from their fathers, is almost identical along the patrilineal line. This leads to a less precise analysis than if autosomal chromosomes were testing, because of the random matching that occurs between pairs of chromosomes as zygotes are being made.\n\nFor highly degraded samples, it is sometimes impossible to get a complete profile of the 13 CODIS STRs. In these situations, mitochondrial DNA (mtDNA) is sometimes typed due to there being many copies of mtDNA in a cell, while there may only be 1-2 copies of the nuclear DNA. Forensic scientists amplify the HV1 and HV2 regions of the mtDNA, and then sequence each region and compare single-nucleotide differences to a reference. Because mtDNA is maternally inherited, directly linked maternal relatives can be used as match references, such as one's maternal grandmother's daughter's son. In general, a difference of two or more nucleotides is considered to be an exclusion. Heteroplasmy and poly-C differences may throw off straight sequence comparisons, so some expertise on the part of the analyst is required. mtDNA is useful in determining clear identities, such as those of missing people when a maternally linked relative can be found. mtDNA testing was used in determining that Anna Anderson was not the Russian princess she had claimed to be, Anastasia Romanov.\n\nmtDNA can be obtained from such material as hair shafts and old bones/teeth. Control mechanism based on interaction point with data. This can be determined by tooled placement in sample.\n\nAn early application of a DNA database was the compilation of a Mitochondrial DNA Concordance, prepared by Kevin W. P. Miller and John L. Dawson at the University of Cambridge from 1996 to 1998 from data collected as part of Miller's PhD thesis. There are now several DNA databases in existence around the world. Some are private, but most of the largest databases are government-controlled. The United States maintains the largest DNA database, with the Combined DNA Index System (CODIS) holding over 9 million records as of 2011. The United Kingdom maintains the National DNA Database (NDNAD), which is of similar size, despite the UK's smaller population. The size of this database, and its rate of growth, are giving concern to civil liberties groups in the UK, where police have wide-ranging powers to take samples and retain them even in the event of acquittal. The Conservative–Liberal Democrat coalition partially addressed these concerns with part 1 of the Protection of Freedoms Act 2012, under which DNA samples must be deleted if suspects are acquitted or not charged, except in relation to certain (mostly serious and/or sexual) offences.\n\nThe U.S. Patriot Act of the United States provides a means for the U.S. government to get DNA samples from suspected terrorists. DNA information from crimes is collected and deposited into the CODIS database, which is maintained by the FBI. CODIS enables law enforcement officials to test DNA samples from crimes for matches within the database, providing a means of finding specific biological profiles associated with collected DNA evidence.\n\nWhen a match is made from a national DNA databank to link a crime scene to an offender having provided a DNA sample to a databank, that link is often referred to as a \"cold hit\". A cold hit is of value in referring the police agency to a specific suspect but is of less evidential value than a DNA match made from outside the DNA Databank.\n\nFBI agents cannot legally store DNA of a person not convicted of a crime. DNA collected from a suspect not later convicted must be disposed of and not entered into the database. In 1998, a man residing in the UK was arrested on accusation of burglary. His DNA was taken and tested, and he was later released. Nine months later, this man's DNA was accidentally and illegally entered in the DNA database. New DNA is automatically compared to the DNA found at cold cases and, in this case, this man was found to be a match to DNA found at a rape and assault case one year earlier. The government then prosecuted him for these crimes. During the trial the DNA match was requested to be removed from the evidence because it had been illegally entered into the database. The request was carried out.\n\nThe DNA collected from victims of rape is often stored for years until matched with the perpetrator's, usually when committing another crime. In 2014, Congress extended a bill that helps states deal with \"a backlog\" of unexamined evidence.\n\nIn the early days of the use of genetic fingerprinting as criminal evidence, juries were often swayed by spurious statistical arguments by defense lawyers along these lines: Given a match that had a 1 in 5 million probability of occurring by chance, the lawyer would argue that this meant that in a country of say 60 million people there were 12 people who would also match the profile. This was then translated to a 1 in 12 chance of the suspect's being the guilty one. This argument is not sound unless the suspect was drawn at random from the population of the country. In fact, a jury should consider how likely it is that an individual matching the genetic profile would also have been a suspect in the case for other reasons. Also, different DNA analysis processes can reduce the amount of DNA recovery if the procedures are not properly done. Therefore, the number of times a piece of evidence is sampled can\ndiminish the DNA collection efficiency. Another spurious statistical argument is based on the false assumption that a 1 in 5 million probability of a match automatically translates into a 1 in 5 million probability of innocence and is known as the prosecutor's fallacy.\n\nWhen using RFLP, the theoretical risk of a coincidental match is 1 in 100 billion (100,000,000,000), although the practical risk is actually 1 in 1000 because monozygotic twins are 0.2% of the human population. Moreover, the rate of laboratory error is almost certainly higher than this, and often actual laboratory procedures do not reflect the theory under which the coincidence probabilities were computed. For example, the coincidence probabilities may be calculated based on the probabilities that markers in two samples have bands in \"precisely\" the same location, but a laboratory worker may conclude that similar—but not precisely identical—band patterns result from identical genetic samples with some imperfection in the agarose gel. However, in this case, the laboratory worker increases the coincidence risk by expanding the criteria for declaring a match. Recent studies have quoted relatively high error rates, which may be cause for concern. In the early days of genetic fingerprinting, the necessary population data to accurately compute a match probability was sometimes unavailable. Between 1992 and 1996, arbitrary low ceilings were controversially put on match probabilities used in RFLP analysis rather than the higher theoretically computed ones. Today, RFLP has become widely disused due to the advent of more discriminating, sensitive and easier technologies.\n\nSince 1998, the DNA profiling system supported by The National DNA Database in the UK is the SGM+ DNA profiling system that includes 10 STR regions and a sex-indicating test. STRs do not suffer from such subjectivity and provide similar power of discrimination (1 in 10 for unrelated individuals if using a full SGM+ profile). Figures of this magnitude are not considered to be statistically supportable by scientists in the UK; for unrelated individuals with full matching DNA profiles a match probability of 1 in a billion is considered statistically supportable. However, with any DNA technique, the cautious juror should not convict on genetic fingerprint evidence alone if other factors raise doubt. Contamination with other evidence (secondary transfer) is a key source of incorrect DNA profiles and raising doubts as to whether a sample has been adulterated is a favorite defense technique. More rarely, chimerism is one such instance where the lack of a genetic match may unfairly exclude a suspect.\n\nIt is possible to use DNA profiling as evidence of genetic relationship, although such evidence varies in strength from weak to positive. Testing that shows no relationship is absolutely certain. Further, while almost all individuals have a single and distinct set of genes, ultra-rare individuals, known as \"chimeras\", have at least two different sets of genes. There have been two cases of DNA profiling that falsely suggested that a mother was unrelated to her children. This happens when two eggs are fertilized at the same time and fuse together to create one individual instead of twins.\n\nIn one case, a criminal planted fake DNA evidence in his own body: John Schneeberger raped one of his sedated patients in 1992 and left semen on her underwear. Police drew what they believed to be Schneeberger's blood and compared its DNA against the crime scene semen DNA on three occasions, never showing a match. It turned out that he had surgically inserted a Penrose drain into his arm and filled it with foreign blood and anticoagulants.\n\nThe functional analysis of genes and their coding sequences (open reading frames [ORFs]) typically requires that each ORF be expressed, the encoded protein purified, antibodies produced, phenotypes examined, intracellular localization determined, and interactions with other proteins sought. In a study conducted by the life science company Nucleix and published in the journal Forensic Science International, scientists found that an in vitro synthesized sample of DNA matching any desired genetic profile can be constructed using standard molecular biology techniques without obtaining any actual tissue from that person. Nucleix claims they can also prove the difference between non-altered DNA and any that was synthesized.\n\nIn the case of the Phantom of Heilbronn, police detectives found DNA traces from the same woman on various crime scenes in Austria, Germany, and France—among them murders, burglaries and robberies. Only after the DNA of the \"woman\" matched the DNA sampled from the burned body of a \"male\" asylum seeker in France did detectives begin to have serious doubts about the DNA evidence. It was eventually discovered that DNA traces were already present on the cotton swabs used to collect the samples at the crime scene, and the swabs had all been produced at the same factory in Austria. The company's product specification said that the swabs were guaranteed to be sterile, but not DNA-free.\n\nFamilial DNA searching (sometimes referred to as \"familial DNA\" or \"familial DNA database searching\") is the practice of creating new investigative leads in cases where DNA evidence found at the scene of a crime (forensic profile) strongly resembles that of an existing DNA profile (offender profile) in a state DNA database but there is not an exact match. After all other leads have been exhausted, investigators may use specially developed software to compare the forensic profile to all profiles taken from a state's DNA database to generate a list of those offenders already in the database who are most likely to be a very close relative of the individual whose DNA is in the forensic profile. To eliminate the majority of this list when the forensic DNA is a man's, crime lab technicians conduct Y-STR analysis. Using standard investigative techniques, authorities are then able to build a family tree. The family tree is populated from information gathered from public records and criminal justice records. Investigators rule out family members' involvement in the crime by finding excluding factors such as sex, living out of state or being incarcerated when the crime was committed. They may also use other leads from the case, such as witness or victim statements, to identify a suspect. Once a suspect has been identified, investigators seek to legally obtain a DNA sample from the suspect. This suspect DNA profile is then compared to the sample found at the crime scene to definitively identify the suspect as the source of the crime scene DNA.\n\nFamilial DNA database searching was first used in an investigation leading to the conviction of Jeffrey Gafoor of the murder of Lynette White in the United Kingdom on 4 July 2003. DNA evidence was matched to Gafoor's nephew, who at 14 years old had not been born at the time of the murder in 1988. It was used again in 2004 to find a man who threw a brick from a motorway bridge and hit a lorry driver, killing him. DNA found on the brick matched that found at the scene of a car theft earlier in the day, but there were no good matches on the national DNA database. A wider search found a partial match to an individual; on being questioned, this man revealed he had a brother, Craig Harman, who lived very close to the original crime scene. Harman voluntarily submitted a DNA sample, and confessed when it matched the sample from the brick. Currently, familial DNA database searching is not conducted on a national level in the United States, where states determine how and when to conduct familial searches. The first familial DNA search with a subsequent conviction in the United States was conducted in Denver, Colorado, in 2008, using software developed under the leadership of Denver District Attorney Mitch Morrissey and Denver Police Department Crime Lab Director Gregg LaBerge. California was the first state to implement a policy for familial searching under then Attorney General, now Governor, Jerry Brown. In his role as consultant to the Familial Search Working Group of the California Department of Justice, former Alameda County Prosecutor Rock Harmon is widely considered to have been the catalyst in the adoption of familial search technology in California. The technique was used to catch the Los Angeles serial killer known as the \"Grim Sleeper\" in 2010. It wasn't a witness or informant that tipped off law enforcement to the identity of the \"Grim Sleeper\" serial killer, who had eluded police for more than two decades, but DNA from the suspect's own son. The suspect's son was arrested and convicted in a felony weapons charge and swabbed for DNA last year. When his DNA was entered into the database of convicted felons, detectives were alerted to a partial match to evidence found at the \"Grim Sleeper\" crime scenes. David Franklin Jr., also known as the Grim Sleeper, was charged with ten counts of murder and one count of attempted murder. More recently, familial DNA, led to the arrest of 21-year-old Elvis Garcia on charges of sexual assault and false imprisonment of a woman in Santa Cruz in 2008. In March 2011 Virginia Governor Bob McDonnell announced that Virginia would begin using familial DNA searches. Other states are expected to follow.\n\nAt a press conference in Virginia on March 7, 2011, regarding the East Coast Rapist, Prince William County prosecutor Paul Ebert and Fairfax County Police Detective John Kelly said the case would have been solved years ago if Virginia had used familial DNA searching. Aaron Thomas, the suspected East Coast Rapist, was arrested in connection with the rape of 17 women from Virginia to Rhode Island, but familial DNA was not used in the case.\n\nCritics of familial DNA database searches argue that the technique is an invasion of an individual's 4th Amendment rights. Privacy advocates are petitioning for DNA database restrictions, arguing that the only fair way to search for possible DNA matches to relatives of offenders or arrestees would be to have a population-wide DNA database. Some scholars have pointed out that the privacy concerns surrounding familial searching are similar in some respects to other police search techniques, and most have concluded that the practice is constitutional. The Ninth Circuit Court of Appeals in \"United States v. Pool\" (vacated as moot) suggested that this practice is somewhat analogous to a witness looking at a photograph of one person and stating that it looked like the perpetrator, which leads law enforcement to show the witness photos of similar looking individuals, one of whom is identified as the perpetrator. Regardless of whether familial DNA searching was the method used to identify the suspect, authorities always conduct a normal DNA test to match the suspect's DNA with that of the DNA left at the crime scene.\n\nCritics also claim that racial profiling could occur on account of familial DNA testing. In the United States, the conviction rates of racial minorities are much higher than that of the overall population. It is unclear whether this is due to discrimination from police officers and the courts, as opposed to a simple higher rate of offence among minorities. Arrest-based databases, which are found in the majority of the United States, lead to an even greater level of racial discrimination. An arrest, as opposed to conviction, relies much more heavily on police discretion.\n\nFor instance, investigators with Denver District Attorney's Office successfully identified a suspect in a property theft case using a familial DNA search. In this example, the suspect's blood left at the scene of the crime strongly resembled that of a current Colorado Department of Corrections prisoner. Using publicly available records, the investigators created a family tree. They then eliminated all the family members who were incarcerated at the time of the offense, as well as all of the females (the crime scene DNA profile was that of a male). Investigators obtained a court order to collect the suspect's DNA, but the suspect actually volunteered to come to a police station and give a DNA sample. After providing the sample, the suspect walked free without further interrogation or detainment. Later confronted with an exact match to the forensic profile, the suspect pleaded guilty to criminal trespass at the first court date and was sentenced to two years probation.\n\nIn Italy a familiar DNA search has been done to solve the case of the murder of Yara Gambirasio whose body was found in the bush three months after her disappearance. A DNA trace was found on the underwear of the murdered teenage near and a DNA sample was requested from a person who lived near the municipality of Brembate di Sopra and a common male ancestor was found in the DNA sample of a young man not involved in the murder. After a long investigation the father of the supposed killer was identified as Giuseppe Guerinoni, a deceased man, but his two sons born from his wife were not related to the DNA samples found on the body of Yara. After three and a half years the DNA found on the underwear of the deceased girl was matched with Massimo Giuseppe Bosetti who was arrested and accused of the murder of the 13-year-old girl.In the summer of 2016 Bosetti was found guilty and sentenced to life by the Corte d'assise of Bergamo.\n\nPartial DNA matches are not searches themselves, but are the result of moderate stringency CODIS searches that produce a potential match that shares at least one allele at every locus. Partial matching does not involve the use of familial search software, such as those used in the UK and United States, or additional Y-STR analysis, and therefore often misses sibling relationships. Partial matching has been used to identify suspects in several cases in the UK and United States, and has also been used as a tool to exonerate the falsely accused. Darryl Hunt was wrongly convicted in connection with the rape and murder of a young woman in 1984 in North Carolina. Hunt was exonerated in 2004 when a DNA database search produced a remarkably close match between a convicted felon and the forensic profile from the case. The partial match led investigators to the felon’s brother, Willard E. Brown, who confessed to the crime when confronted by police. A judge then signed an order to dismiss the case against Hunt.\nIn Italy, partial matching has been used in the controversial murder of Yara Gambirasio Murder of Yara Gambirasio, a child found dead about a month after her presumed kidnapping. In this case, the partial match has been used as the only incriminating element against the defendant, Massimo Bossetti, who has been subsequently condemned for the murder (waiting appeal by the Italian Supreme Court).\n\nPolice forces may collect DNA samples without a suspect's knowledge, and use it as evidence. The legality of the practice has been questioned in Australia.\n\nIn the United States, it has been accepted, courts often ruling that there is no expectation of privacy, citing \"California v. Greenwood\" (1985), in which the Supreme Court held that the Fourth Amendment does not prohibit the warrantless search and seizure of garbage left for collection outside the curtilage of a home. Critics of this practice underline that this analogy ignores that \"most people have no idea that they risk surrendering their genetic identity to the police by, for instance, failing to destroy a used coffee cup. Moreover, even if they do realize it, there is no way to avoid abandoning one's DNA in public.\"\n\nThe United States Supreme Court ruled in \"Maryland v. King\" (2013) that DNA sampling of prisoners arrested for serious crimes is constitutional.\n\nIn the UK, the \"Human Tissue Act 2004\" prohibits private individuals from covertly collecting biological samples (hair, fingernails, etc.) for DNA analysis, but exempts medical and criminal investigations from the prohibition.\n\nEvidence from an expert who has compared DNA samples must be accompanied by evidence as to the sources of the samples and the procedures for obtaining the DNA profiles. The judge must ensure that the jury must understand the significance of DNA matches and mismatches in the profiles. The judge must also ensure that the jury does not confuse the match probability (the probability that a person that is chosen at random has a matching DNA profile to the sample from the scene) with the probability that a person with matching DNA committed the crime. In 1996 \"R v. Doheny\" Phillips LJ gave this example of a summing up, which should be carefully tailored to the particular facts in each case:\n\nJuries should weigh up conflicting and corroborative evidence, using their own common sense and not by using mathematical formulae, such as Bayes' theorem, so as to avoid \"confusion, misunderstanding and misjudgment\".\n\nIn \"R v Bates\", Moore-Bick LJ said:\n\nThere are state laws on DNA profiling in all 50 states of the United States. Detailed information on database laws in each state can be found at the National Conference of State Legislatures website.\n\nIn August 2009, scientists in Israel raised serious doubts concerning the use of DNA by law enforcement as the ultimate method of identification. In a paper published in the journal \"Forensic Science International: Genetics\", the Israeli researchers demonstrated that it is possible to manufacture DNA in a laboratory, thus falsifying DNA evidence. The scientists fabricated saliva and blood samples, which originally contained DNA from a person other than the supposed donor of the blood and saliva.\n\nThe researchers also showed that, using a DNA database, it is possible to take information from a profile and manufacture DNA to match it, and that this can be done without access to any actual DNA from the person whose DNA they are duplicating. The synthetic DNA oligos required for the procedure are common in molecular laboratories.\n\n\"The New York Times\" quoted the lead author, Daniel Frumkin, saying, \"You can just engineer a crime scene ... any biology undergraduate could perform this\". Frumkin perfected a test that can differentiate real DNA samples from fake ones. His test detects epigenetic modifications, in particular, DNA methylation. Seventy percent of the DNA in any human genome is methylated, meaning it contains methyl group modifications within a CpG dinucleotide context. Methylation at the promoter region is associated with gene silencing. The synthetic DNA lacks this epigenetic modification, which allows the test to distinguish manufactured DNA from genuine DNA.\n\nIt is unknown how many police departments, if any, currently use the test. No police lab has publicly announced that it is using the new test to verify DNA results.\n\n\nDNA testing is used to establish the right of succession to British titles.\n\nCases:\n\n\n\n", "id": "44290", "title": "DNA profiling"}
{"url": "https://en.wikipedia.org/wiki?curid=45598981", "text": "Gaseous signaling molecules\n\nGaseous signaling molecules are gaseous molecules that are either synthesised internally (endogenously) in the organism, tissue or cell or are received by the organism, tissue or cell from outside (say, from the atmosphere or hydrosphere, as in the case of oxygen) and that are used to transmit chemical signals which induce certain physiological or biochemical changes in the organism, tissue or cell. The term is applied to, for example, oxygen, carbon dioxide, nitric oxide, carbon monoxide, hydrogen sulfide, sulfur dioxide, nitrous oxide, hydrogen cyanide, ammonia, methane, hydrogen, ethylene, etc.\n\nMany, but not all, gaseous signaling molecules are called gasotransmitters.\n\nThe biological roles of each of the gaseous signaling molecules are in short outlined below.\n\nGasotransmitters is a subfamily of endogenous molecules of gases or gaseous signaling molecules, including NO, CO, HS. These particular gases share many common features in their production and function but carry on their tasks in unique ways, which differ from classical signaling molecules, in the human body. In 1981, it was first suggested from clinical work with nitrous oxide that a gas had a direct action at pharmacological receptors and thereby acted as a neurotransmitter. In vitro experiments confirmed these observations which were replicated at NIDA later.\nThe terminology and characterization criteria of “gasotransmitter” were firstly introduced in 2002. For one gas molecule to be categorized as a gasotransmitters, all of the following criteria should be met.\n\nIn 2011, a European Network on Gasotransmitters (ENOG) was formed. The aim of the network is to promote research on NO, CO and H2S in order to better understand the biology of gasotransmitters and to unravel the role of each mediator in health and disease. Moreover, the network aims to contribute to the translation of basic science knowledge in this area of research into therapeutic or diagnostic tools.\n\nCarbon dioxide is one of the mediators of local autoregulation of blood supply. If its levels are high, the capillaries expand to allow a greater blood flow to that tissue.\n\nBicarbonate ions are crucial for regulating blood pH. A person's breathing rate influences the level of CO in their blood. Breathing that is too slow or shallow causes respiratory acidosis, while breathing that is too rapid leads to hyperventilation, which can cause respiratory alkalosis.\n\nAlthough the body requires oxygen for metabolism, low oxygen levels normally do not stimulate breathing. Rather, breathing is stimulated by higher carbon dioxide levels.\n\nThe respiratory centers try to maintain an arterial CO pressure of 40 mm Hg. With intentional hyperventilation, the CO content of arterial blood may be lowered to 10–20 mm Hg (the oxygen content of the blood is little affected), and the respiratory drive is diminished. This is why one can hold one's breath longer after hyperventilating than without hyperventilating. This carries the risk that unconsciousness may result before the need to breathe becomes overwhelming, which is why hyperventilation is particularly dangerous before free diving.\n\nNO is one of the few gaseous signalling molecules known and is additionally exceptional due to the fact that it is a radical gas. It is a key vertebrate biological messenger, playing a role in a variety of biological processes. It is a known bioproduct in almost all types of organisms, ranging from bacteria to plants, fungi, and animal cells.\n\nNitric oxide, known as the 'endothelium-derived relaxing factor', or 'EDRF', is biosynthesized endogenously from L-arginine, oxygen, and NADPH by various nitric oxide synthase (NOS) enzymes. Reduction of inorganic nitrate may also serve to make nitric oxide. The endothelium (inner lining) of blood vessels uses nitric oxide to signal the surrounding smooth muscle to relax, thus resulting in vasodilation and increasing blood flow. Nitric oxide is highly reactive (having a lifetime of a few seconds), yet diffuses freely across membranes. These attributes make nitric oxide ideal for a transient paracrine (between adjacent cells) and autocrine (within a single cell) signaling molecule.\n\nIndependent of nitric oxide synthase, an alternative pathway, coined the nitrate-nitrite-nitric oxide pathway, elevates nitric oxide through the sequential reduction of dietary nitrate derived from plant-based foods. Nitrate-rich vegetables, in particular leafy greens, such as spinach and arugula, and beetroot, have been shown to increase cardioprotective levels of nitric oxide with a corresponding reduction in blood pressure in pre-hypertensive persons. For the body to generate nitric oxide through the nitrate-nitrite-nitric oxide pathway, the reduction of nitrate to nitrite occurs in the mouth, by commensal bacteria, an obligatory and necessary step. Monitoring nitric oxide status by saliva testing detects the bioconversion of plant-derived nitrate into nitric oxide. A rise in salivary levels is indicative of diets rich in leafy vegetables which are often abundant in anti-hypertensive diets such as the DASH diet.\n\nThe production of nitric oxide is elevated in populations living at high altitudes, which helps these people avoid hypoxia by aiding in pulmonary vasculature vasodilation. Effects include vasodilatation, neurotransmission, modulation of the hair cycle, production of reactive nitrogen intermediates and penile erections (through its ability to vasodilate). Nitroglycerin and amyl nitrite serve as vasodilators because they are converted to nitric oxide in the body. The vasodilating antihypertensive drug minoxidil contains an NO moiety and may act as an NO agonist. Likewise, Sildenafil citrate, popularly known by the trade name \"Viagra\", stimulates erections primarily by enhancing signaling through the nitric oxide pathway in the penis.\n\nNitric oxide (NO) contributes to vessel homeostasis by inhibiting vascular smooth muscle contraction and growth, platelet aggregation, and leukocyte adhesion to the endothelium. Humans with atherosclerosis, diabetes, or hypertension often show impaired NO pathways. A high salt intake was demonstrated to attenuate NO production in patients with essential hypertension, although bioavailability remains unregulated.\n\nNitric oxide is also generated by phagocytes (monocytes, macrophages, and neutrophils) as part of the human immune response. Phagocytes are armed with inducible nitric oxide synthase (iNOS), which is activated by interferon-gamma (IFN-γ) as a single signal or by tumor necrosis factor (TNF) along with a second signal. On the other hand, transforming growth factor-beta (TGF-β) provides a strong inhibitory signal to iNOS, whereas interleukin-4 (IL-4) and IL-10 provide weak inhibitory signals. In this way, the immune system may regulate the resources of phagocytes that play a role in inflammation and immune responses. Nitric oxide is secreted as free radicals in an immune response and is toxic to bacteria and intracellular parasites, including \"Leishmania\" and malaria; the mechanism for this includes DNA damage and degradation of iron sulfur centers into iron ions and iron-nitrosyl compounds.\n\nIn response, many bacterial pathogens have evolved mechanisms for nitric oxide resistance. Because nitric oxide might have proinflammatory actions in conditions like asthma, there has been increasing interest in the use of exhaled nitric oxide as a breath test in diseases with airway inflammation. Reduced levels of exhaled NO have been associated with exposure to air pollution in cyclists and smokers, but, in general, increased levels of exhaled NO are associated with exposure to air pollution.\n\nNitric oxide can contribute to reperfusion injury when an excessive amount produced during reperfusion (following a period of ischemia) reacts with superoxide to produce the damaging oxidant peroxynitrite. In contrast, inhaled nitric oxide has been shown to help survival and recovery from paraquat poisoning, which produces lung tissue-damaging superoxide and hinders NOS metabolism.\n\nIn plants, nitric oxide can be produced by any of four routes: (i) L-arginine-dependent nitric oxide synthase, (although the existence of animal NOS homologs in plants is debated), (ii) plasma membrane-bound nitrate reductase, (iii) mitochondrial electron transport chain, or (iv) non-enzymatic reactions. It is a signaling molecule, acts mainly against oxidative stress and also plays a role in plant pathogen interactions. Treating cut flowers and other plants with nitric oxide has been shown to lengthen the time before wilting.\n\nTwo important biological reaction mechanisms of nitric oxide are S-nitrosation of thiols, and nitrosylation of transition metal ions. S-nitrosation involves the (reversible) conversion of thiol groups, including cysteine residues in proteins, to form S-nitrosothiols (RSNOs). S-Nitrosation is a mechanism for dynamic, post-translational regulation of most or all major classes of protein. The second mechanism, nitrosylation, involves the binding of NO to a transition metal ion like iron or copper. In this function, NO is referred to as a nitrosyl ligand. Typical cases involve the nitrosylation of heme proteins like cytochromes, thereby disabling the normal enzymatic activity of the enzyme. Nitrosylated ferrous iron is particularly stable, as the binding of the nitrosyl ligand to ferrous iron (Fe(II)) is very strong. Hemoglobin is a prominent example of a heme protein that may be modified by NO by both pathways: NO may attach directly to the heme in the nitrosylation reaction, and independently form S-nitrosothiols by S-nitrosation of the thiol moieties.\n\nThere are several mechanisms by which NO has been demonstrated to affect the biology of living cells. These include oxidation of iron-containing proteins such as ribonucleotide reductase and aconitase, activation of the soluble guanylate cyclase, ADP ribosylation of proteins, protein sulfhydryl group nitrosylation, and iron regulatory factor activation. NO has been demonstrated to activate NF-κB in peripheral blood mononuclear cells, an important transcription factor in iNOS gene expression in response to inflammation.\n\nIt was found that NO acts through the stimulation of the soluble guanylate cyclase, which is a heterodimeric enzyme with subsequent formation of cyclic-GMP. Cyclic-GMP activates protein kinase G, which causes reuptake of Ca and the opening of calcium-activated potassium channels. The fall in concentration of Ca ensures that the myosin light-chain kinase (MLCK) can no longer phosphorylate the myosin molecule, thereby stopping the crossbridge cycle and leading to relaxation of the smooth muscle cell.\n\nNitrous oxide in biological systems can be formed by an enzymatic or non-enzymatic reduction of nitric oxide. In vitro studies have shown that endogenous nitrous oxide can be formed by the reaction between nitric oxide and thiol. Some authors have shown that this process of NO reduction to NO takes place in hepatocytes, specifically in their cytoplasm and mitochondria, and suggested that the NO can possibly be produced in mammalian cells. It is well known that NO is produced by some bacteria during process called denitrification.\n\nApart from its direct and indirect actions at opioid recepters, it was also shown that NO inhibits NMDA receptor-mediated activity and ionic currents and diminishes NMDA receptor-mediated excitotoxicity and neurodegeneration. Nitrous oxide also inhibits methionine synthase and slows the conversion of homocysteine to methionine, increases homocysteine concentration and decreases methionine concentration. This effect was shown in lymphocyte cell cultures and in human liver biopsy samples.\n\nNitrous oxide does not bind as a ligand to the heme and does not react with thiol-containing proteins. Nevertheless, studies have shown that nitrous oxide can reversibly and non-covalently \"insert\" itself into the inner structures of some heme-containing proteins such as hemoglobin, myoglobin, cytochrome oxidase and alter their structure and function. The ability of nitrous oxide to alter the structure and function of these proteins was demonstrated by shifts in infrared spectra of cysteine thiols of hemoglobin and by partial and reversible inhibition of cytochrome oxidase.\n\nEndogenous nitrous oxide can possibly play a role in modulating endogenous opioid and NMDA systems.\n\nCarbon monoxide is produced naturally by the human body as a signaling molecule. Thus, carbon monoxide may have a physiological role in the body, such as a neurotransmitter or a blood vessel relaxant. Because of carbon monoxide's role in the body, abnormalities in its metabolism have been linked to a variety of diseases, including neurodegenerations, hypertension, heart failure, and inflammation.\n\nFunctional Summary \n\nIn mammals, carbon monoxide is naturally produced by the action of heme oxygenase 1 and 2 on the heme from hemoglobin breakdown. This process produces a certain amount of carboxyhemoglobin in normal persons, even if they do not breathe any carbon monoxide.\n\nFollowing the first report that carbon monoxide is a normal neurotransmitter in 1993, as well as one of three gases that naturally modulate inflammatory responses in the body (the other two being nitric oxide and hydrogen sulfide), carbon monoxide has received a great deal of clinical attention as a biological regulator. In many tissues, all three gases are known to act as anti-inflammatories, vasodilators, and encouragers of neovascular growth. However, the issues are complex, as neovascular growth is not always beneficial, since it plays a role in tumor growth, and also the damage from \"wet\" macular degeneration, a disease for which smoking (a major source of carbon monoxide in the blood, several times more than natural production) increases the risk from 4 to 6 times.\n\nThere is a theory that, in some nerve cell synapses, when long-term memories are being laid down, the receiving cell makes carbon monoxide, which back-transmits to the transmitting cell, telling it to transmit more readily in future. Some such nerve cells have been shown to contain guanylate cyclase, an enzyme that is activated by carbon monoxide.\n\nStudies involving carbon monoxide have been conducted in many laboratories throughout the world for its anti-inflammatory and cytoprotective properties. These properties have potential to be used to prevent the development of a series of pathological conditions including ischemia reperfusion injury, transplant rejection, atherosclerosis, severe sepsis, severe malaria, or autoimmunity. Clinical tests involving humans have been performed, however the results have not yet been released.\n\nCarbon suboxide, CO, can be produced in small amounts in any biochemical process that normally produces carbon monoxide, CO, for example, during heme oxidation by heme oxygenase-1. It can also be formed from malonic acid. It has been shown that carbon suboxide in an organism can quickly polymerize into macrocyclic polycarbon structures with the common formula (CO) (mostly (CO) and (CO)), and that those macrocyclic compounds are potent inhibitors of Na/K-ATP-ase and Ca-dependent ATP-ase, and have digoxin-like physiological properties and natriuretic and antihypertensive actions. Those macrocyclic carbon suboxide polymer compounds are thought to be endogenous digoxin-like regulators of Na/K-ATP-ases and Ca-dependent ATP-ases, and endogenous natriuretics and antihypertensives. Other than that, some authors think also that those macrocyclic compounds of carbon suboxide can possibly diminish free radical formation and oxidative stress and play a role in endogenous anticancer protective mechanisms, for example in the retina.\n\nHydrogen sulfide is produced in small amounts by some cells of the mammalian body and has a number of biological signaling functions. (Only two other such gases are currently known: nitric oxide (NO) and carbon monoxide (CO).)\n\nThe gas is produced from cysteine by the enzymes cystathionine beta-synthase and cystathionine gamma-lyase. It acts as a relaxant of smooth muscle and as a vasodilator and is also active in the brain, where it increases the response of the NMDA receptor and facilitates long term potentiation, which is involved in the formation of memory.\n\nEventually the gas is converted to sulfite in the mitochondria by thiosulfate reductase, and the sulfite is further oxidized to thiosulfate and sulfate by sulfite oxidase. The sulfates are excreted in the urine.\n\nDue to its effects similar to nitric oxide (without its potential to form peroxides by interacting with superoxide), hydrogen sulfide is now recognized as potentially protecting against cardiovascular disease. The cardioprotective role effect of garlic is caused by catabolism of the polysulfide group in allicin to , a reaction that could depend on reduction mediated by glutathione.\n\nThough both nitric oxide (NO) and hydrogen sulfide have been shown to relax blood vessels, their mechanisms of action are different: while NO activates the enzyme guanylyl cyclase, activates ATP-sensitive potassium channels in smooth muscle cells. Researchers are not clear how the vessel-relaxing responsibilities are shared between nitric oxide and hydrogen sulfide. However, there exists some evidence to suggest that nitric oxide does most of the vessel-relaxing work in large vessels and hydrogen sulfide is responsible for similar action in smaller blood vessels.\n\nRecent findings suggest strong cellular crosstalk of NO and , demonstrating that the vasodilatatory effects of these two gases are mutually dependent. Additionally, reacts with intracellular S-nitrosothiols to form the smallest S-nitrosothiol (HSNO), and a role of hydrogen sulfide in controlling the intracellular S-nitrosothiol pool has been suggested.\n\nLike nitric oxide, hydrogen sulfide is involved in the relaxation of smooth muscle that causes erection of the penis, presenting possible new therapy opportunities for erectile dysfunction.\n\nHydrogen sulfide (H2S) deficiency can be detrimental to the vascular function after an acute myocardial infarction (AMI). AMIs can lead to cardiac dysfunction through two distinct changes; increased oxidative stress via free radical accumulation and decreased NO bioavailability. Free radical accumulation occurs due to increased electron transport uncoupling at the active site of endothelial nitric oxide synthase (eNOS), an enzyme involved in converting L-arginine to NO. During an AMI, oxidative degradation of tetrahydrobiopterin (BH4), a cofactor in NO production, limits BH4 availability and limits NO productionby eNOS. Instead, eNOS reacts with oxygen, another cosubstrates involved in NO production. The products of eNOS are reduced to superoxides, increasing free radical production and oxidative stress within the cells. A H2S deficiency impairs eNOS activity by limiting Akt activation and inhibiting Akt phosphorylation of the eNOSS1177 activation site. Instead, Akt activity is increased to phosphorylate the eNOST495 inhibition site, downregulating eNOS production of NO.\n\nH2S therapy uses a H2S donor, such as diallyl trisulfide (DATS), to increase the supply of H2S to an AMI patient. H2S donors reduce myocardial injury and reperfusion complications. Increased H2S levels within the body will react with oxygen to produce sulfane sulfur, a storage intermediate for H2S. H2S pools in the body attracts oxygen to react with excess H2S and eNOS to increase NO production. With increased use of oxygen to produce more NO, less oxygen is available to react with eNOS to produce superoxides during an AMI, ultimately lowering the accumulation of reactive oxygen species (ROS). Furthermore, decreased accumulation of ROS lowers oxidative stress in vascular smooth muscle cells, decreasing oxidative degeneration of BH4. Increased BH4 cofactor contributes to increased production of NO within the body. Higher concentrations of H2S directly increase eNOS activity through Akt activation to increase phosphorylation of the eNOSS1177 activation site, and decrease phosphorylation of the eNOST495 inhibition site. This phosphorylation process upregulates eNOS activity, catalyzing more conversion of L-arginine to NO. Increased NO production enables soluble guanylyl cyclase (sGC) activity, leading to an increased conversion of guanosine triphosphate (GTP) to 3’,5’-cyclic guanosine monophosphate (cGMP). In H2S therapy immediately following an AMI, increased cGMP triggers an increase in protein kinase G (PKG) activity. PKG reduces intracellular Ca2+ in vascular smooth muscle to increase smooth muscle relaxation and promote blood flow. PKG also limits smooth muscle cell proliferation, reducing intima thickening following AMI injury, ultimately decreasing myocardial infarct size.\n\nIn Alzheimer's disease the brain's hydrogen sulfide concentration is severely decreased. In a certain rat model of Parkinson's disease, the brain's hydrogen sulfide concentration was found to be reduced, and administering hydrogen sulfide alleviated the condition. In trisomy 21 (Down syndrome) the body produces an excess of hydrogen sulfide. Hydrogen sulfide is also involved in the disease process of type 1 diabetes. The beta cells of the pancreas in type 1 diabetes produce an excess of the gas, leading to the death of these cells and to a reduced production of insulin by those that remain.\n\nIn 2005, it was shown that mice can be put into a state of suspended animation-like hypothermia by applying a low dosage of hydrogen sulfide (81 ppm ) in the air. The breathing rate of the animals sank from 120 to 10 breaths per minute and their temperature fell from 37 °C to just 2 °C above ambient temperature (in effect, they had become cold-blooded). The mice survived this procedure for 6 hours and afterwards showed no negative health consequences. In 2006 it was shown that the blood pressure of mice treated in this fashion with hydrogen sulfide did not significantly decrease.\n\nA similar process known as hibernation occurs naturally in many mammals and also in toads, but not in mice. (Mice can fall into a state called clinical torpor when food shortage occurs). If the -induced hibernation can be made to work in humans, it could be useful in the emergency management of severely injured patients, and in the conservation of donated organs. In 2008, hypothermia induced by hydrogen sulfide for 48 hours was shown to reduce the extent of brain damage caused by experimental stroke in rats.\n\nAs mentioned above, hydrogen sulfide binds to cytochrome oxidase and thereby prevents oxygen from binding, which leads to the dramatic slowdown of metabolism. Animals and humans naturally produce some hydrogen sulfide in their body; researchers have proposed that the gas is used to regulate metabolic activity and body temperature, which would explain the above findings.\n\nTwo recent studies cast doubt that the effect can be achieved in larger mammals. A 2008 study failed to reproduce the effect in pigs, concluding that the effects seen in mice were not present in larger mammals. Likewise a paper by Haouzi et al. noted that there is no induction of hypometabolism in sheep, either.\n\nAt the February 2010 TED conference, Mark Roth announced that hydrogen sulfide induced hypothermia \"in humans\" had completed Phase I clinical trials. The clinical trials commissioned by the company he helped found, Ikaria, were however withdrawn or terminated by August 2011.\n\nThe role of sulfur dioxide in mammalian biology is not yet well understood. Sulfur dioxide blocks nerve signals from the pulmonary stretch receptors and abolishes the Hering–Breuer inflation reflex.\n\nIt was shown that endogenous sulfur dioxide plays a role in diminishing an experimental lung damage caused by oleic acid. Endogenous sulfur dioxide lowered lipid peroxidation, free radical formation, oxidative stress and inflammation during an experimental lung damage. Conversely, a successful lung damage caused a significant lowering of endogenous sulfur dioxide production, and an increase in lipid peroxidation, free radical formation, oxidative stress and inflammation. Moreover, blockade of an enzyme that produces endogenous SO significantly increased the amount of lung tissue damage in the experiment. Conversely, adding acetylcysteine or glutathione to the rat diet increased the amount of endogenous SO produced and decreased the lung damage, the free radical formation, oxidative stress, inflammation and apoptosis.\n\nIt is considered that endogenous sulfur dioxide plays a significant physiological role in regulating cardiac and blood vessel function, and aberrant or deficient sulfur dioxide metabolism can contribute to several different cardiovascular diseases, such as arterial hypertension, atherosclerosis, pulmonary arterial hypertension, stenocardia.\n\nIt was shown that in children with pulmonary arterial hypertension due to congenital heart diseases the level of homocysteine is higher and the level of endogenous sulfur dioxide is lower than in normal control children. Moreover, these biochemical parameters strongly correlated to the severity of pulmonary arterial hypertension. Authors considered homocysteine to be one of useful biochemical markers of disease severity and sulfur dioxide metabolism to be one of potential therapeutic targets in those patients.\n\nEndogenous sulfur dioxide also has been shown to lower the proliferation rate of endothelial smooth muscle cells in blood vessels, via lowering the MAPK activity and activating adenylyl cyclase and protein kinase A. Smooth muscle cell proliferation is one of important mechanisms of hypertensive remodeling of blood vessels and their stenosis, so it is an important pathogenetic mechanism in arterial hypertension and atherosclerosis.\n\nEndogenous sulfur dioxide in low concentrations causes endothelium-dependent vasodilation. In higher concentrations it causes endothelium-independent vasodilation and has a negative inotropic effect on cardiac output function, thus effectively lowering blood pressure and myocardial oxygen consumption. The vasodilating effects of sulfur dioxide are mediated via ATP-dependent calcium channels and L-type (\"dihydropyridine\") calcium channels. Endogenous sulfur dioxide is also a potent antiinflammatory, antioxidant and cytoprotective agent. It lowers blood pressure and slows hypertensive remodeling of blood vessels, especially thickening of their intima. It also regulates lipid metabolism.\n\nEndogenous sulfur dioxide also diminishes myocardial damage, caused by isoproterenol adrenergic hyperstimulation, and strengthens the myocardial antioxidant defense reserve.\n\nSulfur dioxide expelled from the gastrointestinal tract (through the process of flatulence) is thought to be one of the main constituents to the unpleasant smell colloquially referred to as a \"fart\".\n\nSome authors have shown that neurons can produce hydrogen cyanide upon activation of their opioid receptors by endogenous or exogenous opioids. They have also shown that neuronal production of HCN activates NMDA receptors and plays a role in signal transduction between neuronal cells (neurotransmission). Moreover, increased endogenous neuronal HCN production under opioids was seemingly needed for adequate opioid analgesia, as analgesic action of opioids was attenuated by HCN scavengers. They considered endogenous HCN to be a neuromodulator.\n\nIt was also shown that, while stimulating muscarinic cholinergic receptors in cultured pheochromocytoma cells \"increases\" HCN production, in a living organism (\"in vivo\") muscarinic cholinergic stimulation actually \"decreases\" HCN production.\n\nLeukocytes generate HCN during phagocytosis.\n\nThe vasodilatation, caused by sodium nitroprusside, has been shown to be mediated not only by NO generation, but also by endogenous cyanide generation, which adds not only toxicity, but also some additional antihypertensive efficacy compared to nitroglycerine and other non-cyanogenic nitrates which do not cause blood cyanide levels to rise.\n\nAmmonia also plays a role in both normal and abnormal animal physiology. It is biosynthesised through normal amino acid metabolism and is toxic in high concentrations. The liver converts ammonia to urea through a series of reactions known as the urea cycle. Liver dysfunction, such as that seen in cirrhosis, may lead to elevated amounts of ammonia in the blood (hyperammonemia). Likewise, defects in the enzymes responsible for the urea cycle, such as ornithine transcarbamylase, lead to hyperammonemia. Hyperammonemia contributes to the confusion and coma of hepatic encephalopathy, as well as the neurologic disease common in people with urea cycle defects and organic acidurias.\n\nAmmonia is important for normal animal acid/base balance. After formation of ammonium from glutamine, α-ketoglutarate may be degraded to produce two molecules of bicarbonate, which are then available as buffers for dietary acids. Ammonium is excreted in the urine, resulting in net acid loss. Ammonia may itself diffuse across the renal tubules, combine with a hydrogen ion, and thus allow for further acid excretion.\n\nSome authors have shown that endogenous methane is produced not only by the intestinal flora and then absorbed into the blood, but also is produced - in small amounts - by eukaryotic cells (during process of lipid peroxidation). And they have also shown that the endogenous methane production rises during an experimental mitochondrial hypoxia, for example, sodium azide intoxication. They thought that methane could be one of intercellular signals of hypoxia and stress.\n\nOther authors have shown that cellular methane production also rises during sepsis or bacterial endotoxemia, including an experimental imitation of endotoxemia by lipopolysaccharide (LPS) administration.\n\nSome other researchers have shown that methane, produced by the intestinal flora, is not fully \"biologically neutral\" to the intestine, and it participates in the normal physiologic regulation of peristalsis. And its excess causes not only belching, flatulence and belly pain, but also functional constipation.\n\nEthylene serves as a hormone in plants. It acts at trace levels throughout the life of the plant by stimulating or regulating the ripening of fruit, the opening of flowers, and the abscission (or shedding) of leaves.\nCommercial ripening rooms use \"catalytic generators\" to make ethylene gas from a liquid supply of ethanol. Typically, a gassing level of 500 to 2,000 ppm is used, for 24 to 48 hours. Care must be taken to control carbon dioxide levels in ripening rooms when gassing, as high temperature ripening () has been seen to produce CO levels of 10% in 24 hours.\n\nEthylene has been used since the ancient Egyptians, who would gash figs in order to stimulate ripening (wounding stimulates ethylene production by plant tissues). The ancient Chinese would burn incense in closed rooms to enhance the ripening of pears. In 1864, it was discovered that gas leaks from street lights led to stunting of growth, twisting of plants, and abnormal thickening of stems. In 1901, a Russian scientist named Dimitry Neljubow showed that the active component was ethylene. Sarah Doubt discovered that ethylene stimulated abscission in 1917. It wasn't until 1934 that Gane reported that plants synthesize ethylene. In 1935, Crocker proposed that ethylene was the plant hormone responsible for fruit ripening as well as senescence of vegetative tissues.\nEthylene is produced from essentially all parts of higher plants, including leaves, stems, roots, flowers, fruits, tubers, and seeds.\nEthylene production is regulated by a variety of developmental and environmental factors. During the life of the plant, ethylene production is induced during certain stages of growth such as germination, ripening of fruits, abscission of leaves, and senescence of flowers. Ethylene production can also be induced by a variety of external aspects such as mechanical wounding, environmental stresses, and certain chemicals including auxin and other regulators.\n\nEthylene is biosynthesized from the amino acid methionine to \"S\"-adenosyl--methionine (SAM, also called Adomet) by the enzyme Met Adenosyltransferase. SAM is then converted to 1-aminocyclopropane-1-carboxylic acid (ACC) by the enzyme ACC synthase (ACS). The activity of ACS determines the rate of ethylene production, therefore regulation of this enzyme is key for the ethylene biosynthesis. The final step requires oxygen and involves the action of the enzyme ACC-oxidase (ACO), formerly known as the ethylene forming enzyme (EFE). Ethylene biosynthesis can be induced by endogenous or exogenous ethylene. ACC synthesis increases with high levels of auxins, especially indole acetic acid (IAA) and cytokinins.\n\nEthylene is perceived by a family of five transmembrane protein dimers such as the ETR protein in \"Arabidopsis\". The gene encoding an ethylene receptor has been cloned in \"Arabidopsis thaliana\" and then in tomato. Ethylene receptors are encoded by multiple genes in the \"Arabidopsis\" and tomato genomes. Mutations in any of the gene family, which comprises five receptors in \"Arabidopsis\" and at least six in tomato, can lead to insensitivity to ethylene. DNA sequences for ethylene receptors have also been identified in many other plant species and an ethylene binding protein has even been identified in Cyanobacteria.\n\nEnvironmental cues such as flooding, drought, chilling, wounding, and pathogen attack can induce ethylene formation in plants. In flooding, roots suffer from lack of oxygen, or anoxia, which leads to the synthesis of 1-aminocyclopropane-1-carboxylic acid (ACC). ACC is transported upwards in the plant and then oxidized in leaves. The ethylene produced causes nastic movements (epinasty) of the leaves, perhaps helping the plant to lose water.\n\nEthylene in plant induces such responses:\n\nSmall amounts of endogenous ethylene are also produced in mammals, including humans, due to lipid peroxidation. Some of endogenous ethylene is then oxidized to ethylene oxide, which is able to alkylate DNA and proteins, including hemoglobin (forming a specific adduct with its N-terminal valine, N-hydroxyethyl-valine). Endogenous ethylene oxide, just as like environmental (exogenous) one, can alkylate guanine in DNA, forming an adduct 7-(2-hydroxyethyl)-guanine, and this poses an intrinsic carcinogenic risk. It is also mutagenic.\n", "id": "45598981", "title": "Gaseous signaling molecules"}
{"url": "https://en.wikipedia.org/wiki?curid=45627329", "text": "Virus Information Table\n\nViruses are extremely diverse in nature. Even their method of storing genetic information differs tremendously, from to negative sense RNA, as denoted in the Baltimore Classification System. Other features, such as entry and release details, or capsid structure may be important in classifying and understanding viruses.\n\nThis table is specific to the genus level, according to the 2014 ICTV report. Individual species may have slightly different details. Information regarding virus genera were obtained from VirusZone, a service provided by the Swiss Institute of Bioinformatics.\n\nTaxonomic List of Viruses\n", "id": "45627329", "title": "Virus Information Table"}
{"url": "https://en.wikipedia.org/wiki?curid=45668969", "text": "Microbial dark matter\n\nMicrobial dark matter\ncomprises the vast majority of microbial organisms (usually bacteria and archaea) that biologists are unable to culture in lab due to lack of knowledge or ability to supply the required growth conditions. It is hard to estimate the relative magnitude of the dark matter, but the accepted gross estimate is that less than one percent of microbial species in a given ecological niche is culturable. In recent years effort is being put to decipher more of the microbial dark matter by means of learning their genome DNA sequence from environmental samples and then by gaining insights to their metabolism from their sequenced genome, promoting the knowledge required for their cultivation.\n\n\n", "id": "45668969", "title": "Microbial dark matter"}
{"url": "https://en.wikipedia.org/wiki?curid=9731834", "text": "List of genera of viruses\n\nThis is an alphabetical list of genera of biological viruses. It includes all genera of virus listed by the International Committee on Taxonomy of Viruses (ICTV) 2014 report.\nFor a list of individual species, see List of virus species.\n\nFor a list of virus families and subfamilies, see List of virus families and subfamilies.\n\nFor a taxonomic list, see Taxonomic list of viruses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "9731834", "title": "List of genera of viruses"}
{"url": "https://en.wikipedia.org/wiki?curid=37315646", "text": "List of virus families and subfamilies\n\nThis is an alphabetical list of biological virus families and subfamilies; it includes those families and subfamilies listed by the ICTV 2014 report.\nFor a list of individual species, see List of virus species.\n\nFor a list of virus genera, see List of genera of viruses.\n\nFor a taxonomic list, see Taxonomic list of viruses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "37315646", "title": "List of virus families and subfamilies"}
{"url": "https://en.wikipedia.org/wiki?curid=42574504", "text": "Plasmid partition system\n\nA plasmid partition system is a mechanism that assures the stable transmission of plasmids during bacterial cell division. Each plasmid has its independent replication system which controls the number of copies of the plasmid in a cell. The higher the copy number is, the more likely the two daughter cells will contain the plasmid. Generally, each molecule of plasmid diffuses randomly, so the probability of having a plasmid-less daughter cell is 2, where N is the number of copies. For instance, if there are 2 copies of a plasmid in a cell, there is a 50% chance of having one plasmid-less daughter cell. However, high-copy number plasmids have a cost for the hosting cell. This metabolic burden is lower for low-copy plasmids, but those have a higher probability of plasmid loss after a few generations. To control vertical transmission of plasmids, in addition to controlled-replication systems, bacterial plasmids use different maintenance strategies, such as multimer resolution systems, post-segregational killing systems (addiction modules), and partition systems.\n\nPlasmid copies are paired around a centromere-like site and then separated in the two daughter cells. Partition systems involve three elements, organized in an auto-regulated operon: \n\nThe centromere-like DNA site is required in \"cis\" for plasmid stability. It often contains one or more inverted repeats which are recognized by multiple CBPs. This forms a nucleoprotein complex termed the partition complex. This complex recruits the motor protein, which is a nucleotide triphosphatase (NTPase). The NTPase uses energy from NTP binding and hydrolysis to directly or indirectly move and attach plasmids to specific host location (e.g. opposite bacterial cell poles).\n\nThe partition systems are divided in four types, based primarily on the type of NTPases:\n\nThis system is also used by most bacteria for chromosome segregation. \nType I partition systems are composed of an ATPase which contains Walker motifs and a CBP which is structurally distinct in type Ia and Ib. ATPases and CBP from type Ia are longer than the ones from type Ib, but both CBPs contain an arginine finger in their N-terminal part.\nParA proteins from different plasmids and bacterial species show 25 to 30% of sequence identity to the protein ParA of the plasmid P1.\nThe partition of type I system uses a \"diffusion-ratchet\" mechanism. This mechanism works as follows:\n\nIt should be noted that there are likely to be differences in the details of type I mechanisms.\n\nType 1 partition has been mathematically modelled with variations in the mechanism described above.\n\nThe CBP of this type consists in three domains:\n\nThe CBP of this type, also known as \"parG\" is composed of:\n\nFor this type, the \"parS\" site is called \"parC\".\n\nThis system is the best understood of the plasmid partition system.\nIt is composed of an actin-like ATPAse, ParM, and a CBP called ParR. The centromere like site, \"parC\" contains two sets of five 11 base pair direct repeats separated by the \"parMR\" promoter.\nThe amino-acid sequence identity can go down to 15% between ParM and other actin-like ATPase.\n\nThe mechanism of partition involved here is a pushing mechanism:\n\nThe filament of ParM is regulated by the polymerization allowed by the presence the partition complex (ParR-\"parC\"), and by the depolymerization controlled by the ATPase activity of ParM.\n\nThe type III partition system is the most recently discovered partition system. It is composed of tubulin-like GTPase termed TubZ, and the CBP is termed TubR.\nAmino-acid sequence identity can go down to 21% for TubZ proteins.\n\nThe mechanism is similar to a treadmill mechanism:\n\nThe net result being transport of partition complex to the cell pole.\n\nThe partition system of the plasmid R388 has been found within the \"stb\" operon. This operon is composed of three genes, \"stbA\", \"stbB\" and \"stbC\".\nThe StbA-\"stbDRs\" complex may be used to pair plasmid the host chromosome, using indirectly the bacterial partitioning system.\n\nStbA and StbB have opposite but connected effect related to conjugation.\n\nThis system has been proposed to be the type IV partition system. It is thought to be a derivative of the type I partition system, given the similar operon organization.\nThis system represents the first evidence for a mechanistic interplay between plasmid segregation and conjugation processes.\n\npSK1 is a plasmid from Staphylococcus aureus. This plasmid has a partition system determined by a single gene, \"par\", previously known as \"orf245\". This gene does not effect the plasmid copy number nor the grow rate (excluding its implication in a post-segregational killing system). A centromere-like binding sequence is present upstream of the \"par\" gene, and is composed of seven direct repeats and one inverted repeat.\n", "id": "42574504", "title": "Plasmid partition system"}
{"url": "https://en.wikipedia.org/wiki?curid=4250553", "text": "Gene\n\nA gene is a sequence of DNA or RNA which codes for a molecule that has a function. During gene expression, the DNA is first copied into RNA. The RNA can be directly functional or be the intermediate template for a protein that performs a function. The transmission of genes to an organism's offspring is the basis of the inheritance of phenotypic traits. These genes make up different DNA sequences called genotypes. Genotypes along with environmental and developmental factors determine what the phenotypes will be. Most biological traits are under the influence of polygenes (many different genes) as well as gene–environment interactions. Some genetic traits are instantly visible, such as eye color or number of limbs, and some are not, such as blood type, risk for specific diseases, or the thousands of basic biochemical processes that constitute life.\n\nGenes can acquire mutations in their sequence, leading to different variants, known as alleles, in the population. These alleles encode slightly different versions of a protein, which cause different phenotypical traits. Usage of the term \"having a gene\" (e.g., \"good genes,\" \"hair colour gene\") typically refers to containing a different allele of the same, shared gene. Genes evolve due to natural selection or survival of the fittest of the alleles.\n\nThe concept of a gene continues to be refined as new phenomena are discovered. For example, regulatory regions of a gene can be far removed from its coding regions, and coding regions can be split into several exons. Some viruses store their genome in RNA instead of DNA and some gene products are functional non-coding RNAs. Therefore, a broad, modern working definition of a gene is any discrete locus of heritable, genomic sequence which affect an organism's traits by being expressed as a functional product or by regulation of gene expression.\n\nThe term \"gene\" was introduced by Danish botanist, plant physiologist and geneticist Wilhelm Johannsen in 1905. It is inspired by the ancient Greek: γόνος, \"gonos\", that means offspring and procreation.\n\nThe existence of discrete inheritable units was first suggested by Gregor Mendel (1822–1884). From 1857 to 1864, in Brno (Czech Republic), he studied inheritance patterns in 8000 common edible pea plants, tracking distinct traits from parent to offspring. He described these mathematically as 2 combinations where n is the number of differing characteristics in the original peas. Although he did not use the term \"gene\", he explained his results in terms of discrete inherited units that give rise to observable physical characteristics. This description prefigured Wilhelm Johannsen's distinction between genotype (the genetic material of an organism) and phenotype (the visible traits of that organism). Mendel was also the first to demonstrate independent assortment, the distinction between dominant and recessive traits, the distinction between a heterozygote and homozygote, and the phenomenon of discontinuous inheritance.\n\nPrior to Mendel's work, the dominant theory of heredity was one of blending inheritance, which suggested that each parent contributed fluids to the fertilisation process and that the traits of the parents blended and mixed to produce the offspring. Charles Darwin developed a theory of inheritance he termed pangenesis, from Greek pan (\"all, whole\") and genesis (\"birth\") / genos (\"origin\"). Darwin used the term \"gemmule\" to describe hypothetical particles that would mix during reproduction.\n\nMendel's work went largely unnoticed after its first publication in 1866, but was rediscovered in the late 19th century by Hugo de Vries, Carl Correns, and Erich von Tschermak, who (claimed to have) reached similar conclusions in their own research. Specifically, in 1889, Hugo de Vries published his book \"Intracellular Pangenesis\", in which he postulated that different characters have individual hereditary carriers and that inheritance of specific traits in organisms comes in particles. De Vries called these units \"pangenes\" (\"Pangens\" in German), after Darwin's 1868 pangenesis theory.\n\nSixteen years later, in 1905, Wilhelm Johannsen introduced the term 'gene' and William Bateson that of 'genetics' while Eduard Strasburger, amongst others, still used the term 'pangene' for the fundamental physical and functional unit of heredity.\n\nAdvances in understanding genes and inheritance continued throughout the 20th century. Deoxyribonucleic acid (DNA) was shown to be the molecular repository of genetic information by experiments in the 1940s to 1950s. The structure of DNA was studied by Rosalind Franklin and Maurice Wilkins using X-ray crystallography, which led James D. Watson and Francis Crick to publish a model of the double-stranded DNA molecule whose paired nucleotide bases indicated a compelling hypothesis for the mechanism of genetic replication.\n\nIn the early 1950s the prevailing view was that the genes in a chromosome acted like discrete entities, indivisible by recombination and arranged like beads on a string. The experiments of Benzer using mutants defective in the rII region of bacteriophage T4 (1955-1959) showed that individual genes have a simple linear structure and are likely to be equivalent to a linear section of DNA.\n\nCollectively, this body of research established the central dogma of molecular biology, which states that proteins are translated from RNA, which is transcribed from DNA. This dogma has since been shown to have exceptions, such as reverse transcription in retroviruses. The modern study of genetics at the level of DNA is known as molecular genetics.\n\nIn 1972, Walter Fiers and his team at the University of Ghent were the first to determine the sequence of a gene: the gene for Bacteriophage MS2 coat protein. The subsequent development of chain-termination DNA sequencing in 1977 by Frederick Sanger improved the efficiency of sequencing and turned it into a routine laboratory tool. An automated version of the Sanger method was used in early phases of the Human Genome Project.\n\nThe theories developed in the early 20th century to integrate Mendelian genetics with Darwinian evolution are called the modern synthesis, a term introduced by Julian Huxley.\n\nEvolutionary biologists have subsequently modified this concept, such as George C. Williams' gene-centric view of evolution. He proposed an evolutionary concept of the gene as a unit of natural selection with the definition: \"that which segregates and recombines with appreciable frequency.\" In this view, the molecular gene \"transcribes\" as a unit, and the evolutionary gene \"inherits\" as a unit. Related ideas emphasizing the centrality of genes in evolution were popularized by Richard Dawkins.\n\nThe vast majority of living organisms encode their genes in long strands of DNA (deoxyribonucleic acid). DNA consists of a chain made from four types of nucleotide subunits, each composed of: a five-carbon sugar (2'-deoxyribose), a phosphate group, and one of the four bases adenine, cytosine, guanine, and thymine.\n\nTwo chains of DNA twist around each other to form a DNA double helix with the phosphate-sugar backbone spiralling around the outside, and the bases pointing inwards with adenine base pairing to thymine and guanine to cytosine. The specificity of base pairing occurs because adenine and thymine align to form two hydrogen bonds, whereas cytosine and guanine form three hydrogen bonds. The two strands in a double helix must therefore be complementary, with their sequence of bases matching such that the adenines of one strand are paired with the thymines of the other strand, and so on.\n\nDue to the chemical composition of the pentose residues of the bases, DNA strands have directionality. One end of a DNA polymer contains an exposed hydroxyl group on the deoxyribose; this is known as the 3' end of the molecule. The other end contains an exposed phosphate group; this is the 5' end. The two strands of a double-helix run in opposite directions. Nucleic acid synthesis, including DNA replication and transcription occurs in the 5'→3' direction, because new nucleotides are added via a dehydration reaction that uses the exposed 3' hydroxyl as a nucleophile.\n\nThe expression of genes encoded in DNA begins by transcribing the gene into RNA, a second type of nucleic acid that is very similar to DNA, but whose monomers contain the sugar ribose rather than deoxyribose. RNA also contains the base uracil in place of thymine. RNA molecules are less stable than DNA and are typically single-stranded. Genes that encode proteins are composed of a series of three-nucleotide sequences called codons, which serve as the \"words\" in the genetic \"language\". The genetic code specifies the correspondence during protein translation between codons and amino acids. The genetic code is nearly the same for all known organisms.\n\nThe total complement of genes in an organism or cell is known as its genome, which may be stored on one or more chromosomes. A chromosome consists of a single, very long DNA helix on which thousands of genes are encoded. The region of the chromosome at which a particular gene is located is called its locus. Each locus contains one allele of a gene; however, members of a population may have different alleles at the locus, each with a slightly different gene sequence.\n\nThe majority of eukaryotic genes are stored on a set of large, linear chromosomes. The chromosomes are packed within the nucleus in complex with storage proteins called histones to form a unit called a nucleosome. DNA packaged and condensed in this way is called chromatin. The manner in which DNA is stored on the histones, as well as chemical modifications of the histone itself, regulate whether a particular region of DNA is accessible for gene expression. In addition to genes, eukaryotic chromosomes contain sequences involved in ensuring that the DNA is copied without degradation of end regions and sorted into daughter cells during cell division: replication origins, telomeres and the centromere. Replication origins are the sequence regions where DNA replication is initiated to make two copies of the chromosome. Telomeres are long stretches of repetitive sequence that cap the ends of the linear chromosomes and prevent degradation of coding and regulatory regions during DNA replication. The length of the telomeres decreases each time the genome is replicated and has been implicated in the aging process. The centromere is required for binding spindle fibres to separate sister chromatids into daughter cells during cell division.\n\nProkaryotes (bacteria and archaea) typically store their genomes on a single large, circular chromosome. Similarly, some eukaryotic organelles contain a remnant circular chromosome with a small number of genes. Prokaryotes sometimes supplement their chromosome with additional small circles of DNA called plasmids, which usually encode only a few genes and are transferable between individuals. For example, the genes for antibiotic resistance are usually encoded on bacterial plasmids and can be passed between individual cells, even those of different species, via horizontal gene transfer.\nWhereas the chromosomes of prokaryotes are relatively gene-dense, those of eukaryotes often contain regions of DNA that serve no obvious function. Simple single-celled eukaryotes have relatively small amounts of such DNA, whereas the genomes of complex multicellular organisms, including humans, contain an absolute majority of DNA without an identified function. This DNA has often been referred to as \"junk DNA\". However, more recent analyses suggest that, although protein-coding DNA makes up barely 2% of the human genome, about 80% of the bases in the genome may be expressed, so the term \"junk DNA\" may be a misnomer.\n\nThe structure of a gene consists of many elements of which the actual protein coding sequence is often only a small part. These include DNA regions that are not transcribed as well as untranslated regions of the RNA.\n\nFlanking the open reading frame, genes contain a regulatory sequence that is required for their expression. First, genes require a promoter sequence. The promoter is recognized and bound by transcription factors and RNA polymerase to initiate transcription. The recognition typically occurs as a consensus sequence like the TATA box. A gene can have more than one promoter, resulting in messenger RNAs (mRNA) that differ in how far they extend in the 5' end. Highly transcribed genes have \"strong\" promoter sequences that form strong associations with transcription factors, thereby initiating transcription at a high rate. Others genes have \"weak\" promoters that form weak associations with transcription factors and initiate transcription less frequently. Eukaryotic promoter regions are much more complex and difficult to identify than prokaryotic promoters.\n\nAdditionally, genes can have regulatory regions many kilobases upstream or downstream of the open reading frame that alter expression. These act by binding to transcription factors which then cause the DNA to loop so that the regulatory sequence (and bound transcription factor) become close to the RNA polymerase binding site. For example, enhancers increase transcription by binding an activator protein which then helps to recruit the RNA polymerase to the promoter; conversely silencers bind repressor proteins and make the DNA less available for RNA polymerase.\n\nThe transcribed pre-mRNA contains untranslated regions at both ends which contain a ribosome binding site, terminator and start and stop codons. In addition, most eukaryotic open reading frames contain untranslated introns which are removed before the exons are translated. The sequences at the ends of the introns, dictate the splice sites to generate the final mature mRNA which encodes the protein or RNA product.\n\nMany prokaryotic genes are organized into operons, with multiple protein-coding sequences that are transcribed as a unit. The genes in an operon are transcribed as a continuous messenger RNA, referred to as a polycistronic mRNA. The term cistron in this context is equivalent to gene. The transcription of an operon’s mRNA is often controlled by a repressor that can occur in an active or inactive state depending on the presence of certain specific metabolites. When active, the repressor binds to a DNA sequence at the beginning of the operon, called the operator region, and represses transcription of the operon; when the repressor is inactive transcription of the operon can occur (see e.g. Lac operon). The products of operon genes typically have related functions and are involved in the same regulatory network.\n\nDefining exactly what section of a DNA sequence comprises a gene is difficult. Regulatory regions of a gene such as enhancers do not necessarily have to be close to the coding sequence on the linear molecule because the intervening DNA can be looped out to bring the gene and its regulatory region into proximity. Similarly, a gene's introns can be much larger than its exons. Regulatory regions can even be on entirely different chromosomes and operate \"in trans\" to allow regulatory regions on one chromosome to come in contact with target genes on another chromosome.\n\nEarly work in molecular genetics suggested the concept that one gene makes one protein. This concept (originally called the one gene-one enzyme hypothesis) emerged from an influential 1941 paper by George Beadle and Edward Tatum on experiments with mutants of the fungus Neurospora crassa. Norman Horowitz, an early colleague on the \"Neurospora\" research, reminisced in 2004 that “these experiments founded the science of what Beadle and Tatum called \"biochemical genetics\". In actuality they proved to be the opening gun in what became molecular genetics and all the developments that have followed from that.” The one gene-one protein concept has been refined since the discovery of genes that can encode multiple proteins by alternative splicing and coding sequences split in short section across the genome whose mRNAs are concatenated by trans-splicing.\n\nA broad operational definition is sometimes used to encompass the complexity of these diverse phenomena, where a gene is defined as a union of genomic sequences encoding a coherent set of potentially overlapping functional products. This definition categorizes genes by their functional products (proteins or RNA) rather than their specific DNA loci, with regulatory elements classified as \"gene-associated\" regions.\n\nIn all organisms, two steps are required to read the information encoded in a gene's DNA and produce the protein it specifies. First, the gene's DNA is \"transcribed\" to messenger RNA (mRNA). Second, that mRNA is \"translated\" to protein. RNA-coding genes must still go through the first step, but are not translated into protein. The process of producing a biologically functional molecule of either RNA or protein is called gene expression, and the resulting molecule is called a gene product.\n\nThe nucleotide sequence of a gene's DNA specifies the amino acid sequence of a protein through the genetic code. Sets of three nucleotides, known as codons, each correspond to a specific amino acid. The principle that three sequential bases of DNA code for each amino acid was demonstrated in 1961 using frameshift mutations in the rIIB gene of bacteriophage T4 (see Crick, Brenner et al. experiment).\n\nAdditionally, a \"start codon\", and three \"stop codons\" indicate the beginning and end of the protein coding region. There are 64 possible codons (four possible nucleotides at each of three positions, hence 4 possible codons) and only 20 standard amino acids; hence the code is redundant and multiple codons can specify the same amino acid. The correspondence between codons and amino acids is nearly universal among all known living organisms.\n\nTranscription produces a single-stranded RNA molecule known as messenger RNA, whose nucleotide sequence is complementary to the DNA from which it was transcribed. The mRNA acts as an intermediate between the DNA gene and its final protein product. The gene's DNA is used as a template to generate a complementary mRNA. The mRNA matches the sequence of the gene's DNA coding strand because it is synthesised as the complement of the template strand. Transcription is performed by an enzyme called an RNA polymerase, which reads the template strand in the 3' to 5' direction and synthesizes the RNA from 5' to 3'. To initiate transcription, the polymerase first recognizes and binds a promoter region of the gene. Thus, a major mechanism of gene regulation is the blocking or sequestering the promoter region, either by tight binding by repressor molecules that physically block the polymerase, or by organizing the DNA so that the promoter region is not accessible.\n\nIn prokaryotes, transcription occurs in the cytoplasm; for very long transcripts, translation may begin at the 5' end of the RNA while the 3' end is still being transcribed. In eukaryotes, transcription occurs in the nucleus, where the cell's DNA is stored. The RNA molecule produced by the polymerase is known as the primary transcript and undergoes post-transcriptional modifications before being exported to the cytoplasm for translation. One of the modifications performed is the splicing of introns which are sequences in the transcribed region that do not encode protein. Alternative splicing mechanisms can result in mature transcripts from the same gene having different sequences and thus coding for different proteins. This is a major form of regulation in eukaryotic cells and also occurs in some prokaryotes.\n\nTranslation is the process by which a mature mRNA molecule is used as a template for synthesizing a new protein. Translation is carried out by ribosomes, large complexes of RNA and protein responsible for carrying out the chemical reactions to add new amino acids to a growing polypeptide chain by the formation of peptide bonds. The genetic code is read three nucleotides at a time, in units called codons, via interactions with specialized RNA molecules called transfer RNA (tRNA). Each tRNA has three unpaired bases known as the anticodon that are complementary to the codon it reads on the mRNA. The tRNA is also covalently attached to the amino acid specified by the complementary codon. When the tRNA binds to its complementary codon in an mRNA strand, the ribosome attaches its amino acid cargo to the new polypeptide chain, which is synthesized from amino terminus to carboxyl terminus. During and after synthesis, most new proteins must fold to their active three-dimensional structure before they can carry out their cellular functions.\n\nGenes are regulated so that they are expressed only when the product is needed, since expression draws on limited resources. A cell regulates its gene expression depending on its external environment (e.g. available nutrients, temperature and other stresses), its internal environment (e.g. cell division cycle, metabolism, infection status), and its specific role if in a multicellular organism. Gene expression can be regulated at any step: from transcriptional initiation, to RNA processing, to post-translational modification of the protein. The regulation of lactose metabolism genes in \"E. coli\" (\"lac\" operon) was the first such mechanism to be described in 1961.\n\nA typical protein-coding gene is first copied into RNA as an intermediate in the manufacture of the final protein product. In other cases, the RNA molecules are the actual functional products, as in the synthesis of ribosomal RNA and transfer RNA. Some RNAs known as ribozymes are capable of enzymatic function, and microRNA has a regulatory role. The DNA sequences from which such RNAs are transcribed are known as non-coding RNA genes.\n\nSome viruses store their entire genomes in the form of RNA, and contain no DNA at all. Because they use RNA to store genes, their cellular hosts may synthesize their proteins as soon as they are infected and without the delay in waiting for transcription. On the other hand, RNA retroviruses, such as HIV, require the reverse transcription of their genome from RNA into DNA before their proteins can be synthesized. RNA-mediated epigenetic inheritance has also been observed in plants and very rarely in animals.\n\nOrganisms inherit their genes from their parents. Asexual organisms simply inherit a complete copy of their parent's genome. Sexual organisms have two copies of each chromosome because they inherit one complete set from each parent.\n\nAccording to Mendelian inheritance, variations in an organism's phenotype (observable physical and behavioral characteristics) are due in part to variations in its genotype (particular set of genes). Each gene specifies a particular trait with different sequence of a gene (alleles) giving rise to different phenotypes. Most eukaryotic organisms (such as the pea plants Mendel worked on) have two alleles for each trait, one inherited from each parent.\n\nAlleles at a locus may be dominant or recessive; dominant alleles give rise to their corresponding phenotypes when paired with any other allele for the same trait, whereas recessive alleles give rise to their corresponding phenotype only when paired with another copy of the same allele. If you know the genotypes of the organisms, you can determine which alleles are dominant and which are recessive. For example, if the allele specifying tall stems in pea plants is dominant over the allele specifying short stems, then pea plants that inherit one tall allele from one parent and one short allele from the other parent will also have tall stems. Mendel's work demonstrated that alleles assort independently in the production of gametes, or germ cells, ensuring variation in the next generation. Although Mendelian inheritance remains a good model for many traits determined by single genes (including a number of well-known genetic disorders) it does not include the physical processes of DNA replication and cell division.\n\nThe growth, development, and reproduction of organisms relies on cell division; the process by which a single cell divides into two usually identical daughter cells. This requires first making a duplicate copy of every gene in the genome in a process called DNA replication. The copies are made by specialized enzymes known as DNA polymerases, which \"read\" one strand of the double-helical DNA, known as the template strand, and synthesize a new complementary strand. Because the DNA double helix is held together by base pairing, the sequence of one strand completely specifies the sequence of its complement; hence only one strand needs to be read by the enzyme to produce a faithful copy. The process of DNA replication is semiconservative; that is, the copy of the genome inherited by each daughter cell contains one original and one newly synthesized strand of DNA.\n\nThe rate of DNA replication in living cells was first measured as the rate of phage T4 DNA elongation in phage-infected \"E. coli\" and found to be impressively rapid. During the period of exponential DNA increase at 37 °C, the rate of elongation was 749 nucleotides per second.\n\nAfter DNA replication is complete, the cell must physically separate the two copies of the genome and divide into two distinct membrane-bound cells. In prokaryotes (bacteria and archaea) this usually occurs via a relatively simple process called binary fission, in which each circular genome attaches to the cell membrane and is separated into the daughter cells as the membrane invaginates to split the cytoplasm into two membrane-bound portions. Binary fission is extremely fast compared to the rates of cell division in eukaryotes. Eukaryotic cell division is a more complex process known as the cell cycle; DNA replication occurs during a phase of this cycle known as S phase, whereas the process of segregating chromosomes and splitting the cytoplasm occurs during M phase.\n\nThe duplication and transmission of genetic material from one generation of cells to the next is the basis for molecular inheritance, and the link between the classical and molecular pictures of genes. Organisms inherit the characteristics of their parents because the cells of the offspring contain copies of the genes in their parents' cells. In asexually reproducing organisms, the offspring will be a genetic copy or clone of the parent organism. In sexually reproducing organisms, a specialized form of cell division called meiosis produces cells called gametes or germ cells that are haploid, or contain only one copy of each gene. The gametes produced by females are called eggs or ova, and those produced by males are called sperm. Two gametes fuse to form a diploid fertilized egg, a single cell that has two sets of genes, with one copy of each gene from the mother and one from the father.\n\nDuring the process of meiotic cell division, an event called genetic recombination or \"crossing-over\" can sometimes occur, in which a length of DNA on one chromatid is swapped with a length of DNA on the corresponding homologous non-sister chromatid. This can result in reassortment of otherwise linked alleles. The Mendelian principle of independent assortment asserts that each of a parent's two genes for each trait will sort independently into gametes; which allele an organism inherits for one trait is unrelated to which allele it inherits for another trait. This is in fact only true for genes that do not reside on the same chromosome, or are located very far from one another on the same chromosome. The closer two genes lie on the same chromosome, the more closely they will be associated in gametes and the more often they will appear together; genes that are very close are essentially never separated because it is extremely unlikely that a crossover point will occur between them. This is known as genetic linkage.\n\nDNA replication is for the most part extremely accurate, however errors (mutations) do occur. The error rate in eukaryotic cells can be as low as 10 per nucleotide per replication, whereas for some RNA viruses it can be as high as 10. This means that each generation, each human genome accumulates 1–2 new mutations. Small mutations can be caused by DNA replication and the aftermath of DNA damage and include point mutations in which a single base is altered and frameshift mutations in which a single base is inserted or deleted. Either of these mutations can change the gene by missense (change a codon to encode a different amino acid) or nonsense (a premature stop codon). Larger mutations can be caused by errors in recombination to cause chromosomal abnormalities including the duplication, deletion, rearrangement or inversion of large sections of a chromosome. Additionally, DNA repair mechanisms can introduce mutational errors when repairing physical damage to the molecule. The repair, even with mutation, is more important to survival than restoring an exact copy, for example when repairing double-strand breaks.\n\nWhen multiple different alleles for a gene are present in a species's population it is called polymorphic. Most different alleles are functionally equivalent, however some alleles can give rise to different phenotypic traits. A gene's most common allele is called the wild type, and rare alleles are called mutants. The genetic variation in relative frequencies of different alleles in a population is due to both natural selection and genetic drift. The wild-type allele is not necessarily the ancestor of less common alleles, nor is it necessarily fitter.\n\nMost mutations within genes are neutral, having no effect on the organism's phenotype (silent mutations). Some mutations do not change the amino acid sequence because multiple codons encode the same amino acid (synonymous mutations). Other mutations can be neutral if they lead to amino acid sequence changes, but the protein still functions similarly with the new amino acid (e.g. conservative mutations). Many mutations, however, are deleterious or even lethal, and are removed from populations by natural selection. Genetic disorders are the result of deleterious mutations and can be due to spontaneous mutation in the affected individual, or can be inherited. Finally, a small fraction of mutations are beneficial, improving the organism's fitness and are extremely important for evolution, since their directional selection leads to adaptive evolution.\n\nGenes with a most recent common ancestor, and thus a shared evolutionary ancestry, are known as homologs. These genes appear either from gene duplication within an organism's genome, where they are known as paralogous genes, or are the result of divergence of the genes after a speciation event, where they are known as orthologous genes, and often perform the same or similar functions in related organisms. It is often assumed that the functions of orthologous genes are more similar than those of paralogous genes, although the difference is minimal.\n\nThe relationship between genes can be measured by comparing the sequence alignment of their DNA. The degree of sequence similarity between homologous genes is called conserved sequence. Most changes to a gene's sequence do not affect its function and so genes accumulate mutations over time by neutral molecular evolution. Additionally, any selection on a gene will cause its sequence to diverge at a different rate. Genes under stabilizing selection are constrained and so change more slowly whereas genes under directional selection change sequence more rapidly. The sequence differences between genes can be used for phylogenetic analyses to study how those genes have evolved and how the organisms they come from are related.\n\nThe most common source of new genes in eukaryotic lineages is gene duplication, which creates copy number variation of an existing gene in the genome. The resulting genes (paralogs) may then diverge in sequence and in function. Sets of genes formed in this way compose a gene family. Gene duplications and losses within a family are common and represent a major source of evolutionary biodiversity. Sometimes, gene duplication may result in a nonfunctional copy of a gene, or a functional copy may be subject to mutations that result in loss of function; such nonfunctional genes are called pseudogenes.\n\n\"Orphan\" genes, whose sequence shows no similarity to existing genes, are less common than gene duplicates. Estimates of the number of genes with no homologs outside humans range from 18 to 60. Two primary sources of orphan protein-coding genes are gene duplication followed by extremely rapid sequence change, such that the original relationship is undetectable by sequence comparisons, and de novo conversion of a previously non-coding sequence into a protein-coding gene. De novo genes are typically shorter and simpler in structure than most eukaryotic genes, with few if any introns. Over long evolutionary time periods, de novo gene birth may be responsible for a significant fraction of taxonomically-restricted gene families.\n\nHorizontal gene transfer refers to the transfer of genetic material through a mechanism other than reproduction. This mechanism is a common source of new genes in prokaryotes, sometimes thought to contribute more to genetic variation than gene duplication. It is a common means of spreading antibiotic resistance, virulence, and adaptive metabolic functions. Although horizontal gene transfer is rare in eukaryotes, likely examples have been identified of protist and alga genomes containing genes of bacterial origin.\n\nThe genome is the total genetic material of an organism and includes both the genes and non-coding sequences.\n\nThe genome size, and the number of genes it encodes varies widely between organisms. The smallest genomes occur in viruses (which can have as few as 2 protein-coding genes), and viroids (which act as a single non-coding RNA gene). Conversely, plants can have extremely large genomes, with rice containing >46,000 protein-coding genes. The total number of protein-coding genes (the Earth's proteome) is estimated to be 5 million sequences.\n\nAlthough the number of base-pairs of DNA in the human genome has been known since the 1960s, the estimated number of genes has changed over time as definitions of genes, and methods of detecting them have been refined. Initial theoretical predictions of the number of human genes were as high as 2,000,000. Early experimental measures indicated there to be 50,000–100,000 \"transcribed\" genes (expressed sequence tags). Subsequently, the sequencing in the Human Genome Project indicated that many of these transcripts were alternative variants of the same genes, and the total number of protein-coding genes was revised down to ~20,000 with 13 genes encoded on the mitochondrial genome. With the GENCODE annotation project, that estimate has continued to fall to 19,000. Of the human genome, only 1–2% consists of protein-coding genes, with the remainder being 'noncoding' DNA such as introns, retrotransposons, and noncoding RNAs. Every multicellular organism has all its genes in each cell of its body but not every gene functions in every cell .\n\nEssential genes are the set of genes thought to be critical for an organism's survival. This definition assumes the abundant availability of all relevant nutrients and the absence of environmental stress. Only a small portion of an organism's genes are essential. In bacteria, an estimated 250–400 genes are essential for \"Escherichia coli\" and \"Bacillus subtilis\", which is less than 10% of their genes. Half of these genes are orthologs in both organisms and are largely involved in protein synthesis. In the budding yeast \"Saccharomyces cerevisiae\" the number of essential genes is slightly higher, at 1000 genes (~20% of their genes). Although the number is more difficult to measure in higher eukaryotes, mice and humans are estimated to have around 2000 essential genes (~10% of their genes). The synthetic organism, \"Syn 3\", has a minimal genome of 473 essential genes and quasi-essential genes (necessary for fast growth), although 149 have unknown function.\n\nEssential genes include Housekeeping genes (critical for basic cell functions) as well as genes that are expressed at different times in the organisms development or life cycle. Housekeeping genes are used as experimental controls when analysing gene expression, since they are constitutively expressed at a relatively constant level.\n\nGene nomenclature has been established by the HUGO Gene Nomenclature Committee (HGNC) for each known human gene in the form of an approved gene name and symbol (short-form abbreviation), which can be accessed through a database maintained by HGNC. Symbols are chosen to be unique, and each gene has only one symbol (although approved symbols sometimes change). Symbols are preferably kept consistent with other members of a gene family and with homologs in other species, particularly the mouse due to its role as a common model organism.\n\nGenetic engineering is the modification of an organism's genome through biotechnology. Since the 1970s, a variety of techniques have been developed to specifically add, remove and edit genes in an organism. Recently developed genome engineering techniques use engineered nuclease enzymes to create targeted DNA repair in a chromosome to either disrupt or edit a gene when the break is repaired. The related term synthetic biology is sometimes used to refer to extensive genetic engineering of an organism.\n\nGenetic engineering is now a routine research tool with model organisms. For example, genes are easily added to bacteria and lineages of knockout mice with a specific gene's function disrupted are used to investigate that gene's function. Many organisms have been genetically modified for applications in agriculture, industrial biotechnology, and medicine.\n\nFor multicellular organisms, typically the embryo is engineered which grows into the adult genetically modified organism. However, the genomes of cells in an adult organism can be edited using gene therapy techniques to treat genetic diseases.\n\n – A molecular biology textbook available free online through NCBI Bookshelf.\n\n\n", "id": "4250553", "title": "Gene"}
{"url": "https://en.wikipedia.org/wiki?curid=45552819", "text": "Nano differential scanning fluorimetry\n\nnanoDSF is a modified differential scanning fluorimetry method to determine protein stability employing intrinsic tryptophan or tyrosin fluorescence.\n\nProtein stability is typically addressed by thermal or chemical unfolding experiments. In thermal unfolding experiments, a linear temperature ramp is applied to unfold proteins, whereas chemical unfolding experiments use chemical denaturants in increasing concentrations.\nThe thermal stability of a protein is typically described by the 'melting temperature' or 'Tm', at which 50% of the protein population is unfolded, corresponding to the midpoint of the transition from folded to unfolded.\n\nIn contrast to conventional DSF methods, nanoDSF uses tryptophan or tyrosin fluorescence to monitor protein unfolding. Both the fluorescence intensity and the fluorescence maximum strongly depends on the close surroundings of the tryptophan. Therefore, the ratio of the fluorescence intensities at 350 nm and 330 nm is suitable to detect any changes in protein structure, for example due to protein unfolding.\n\nIts applications include antibody engineering, membrane protein research, quality control and formulation development.\n\n", "id": "45552819", "title": "Nano differential scanning fluorimetry"}
{"url": "https://en.wikipedia.org/wiki?curid=16598780", "text": "EXPOSE\n\nEXPOSE is a multi-user facility mounted outside the International Space Station dedicated to astrobiology. EXPOSE was developed by the European Space Agency (ESA) for long-term spaceflights and was designed to allow exposure of chemical and biological samples to outer space while recording data during exposure.\n\nThe results will contribute to our understanding of photobiological processes in simulated radiation climates of planets (e.g. early Earth, early and present Mars, and the role of the ozone layer in protecting the biosphere from harmful UV-B radiation), as well as studies of the probabilities and limitations for life to be distributed beyond its planet of origin. EXPOSE data support long-term \"in situ\" studies of microbes in artificial meteorites, as well as of microbial communities from special ecological niches. Some EXPOSE experiments investigated to what extent particular terrestrial organisms are able to cope with extraterrestrial environmental conditions. Others tested how organic molecules react when subjected for a prolonged period of time to unfiltered solar light.\n\nEXPOSE has several objectives, specific to each experiments, but all linked to the astrobiology domain. Their collective objective is to better understand the nature and evolution of organic matter present in extraterrestrial environments and their potential implications in astrobiology. These experiments mostly study molecules of cometary interest in order to understand the results of the Rosetta\nmission, the chemistry of Titan (Cassini–Huygens mission), or the organic chemistry of the Martian environment (Mars Science Laboratory and ExoMars project).\n\nWith the experiments onboard of the EXPOSE facilities, various aspects of astrobiology were investigated that could not be sufficiently approached by use of laboratory facilities on ground. The chemical set of experiments is designed to reach a better understanding of the role of interstellar, cometary and planetary chemistry in the origin of life. Comets and meteorites are interpreted as exogenous sources of prebiotic molecules on the early Earth. All data achieved from the astrobiological experiments on both EXPOSE missions will add to the understanding of the origin and evolution of life on Earth and on the possibility of its distribution in space or origin elsewhere.\n\nData obtained from the studies on complex organics of cometary interest will support the interpretation of \"in-situ\" data obtained from the Rosetta mission after landing on Comet 67P/Churyumov-Gerasimenko in 2014, and samples analyzed by the \"Curiosity\" and \"ExoMars\" rovers on Mars. Finally the chemical experiments will contribute to the understanding of the chemical processes on Saturn's moon Titan and possible analogies to the prebiotic chemistry on the early Earth.\n\nThe biology experiments used the full extraterrestrial spectrum of solar UV radiation and suitable cut-off filters to study both, the role of the ozone layer in protecting our biosphere and the likelihood of resistant terrestrial microorganisms (extremophiles) to survive in outer space. The latter studies will provide experimental data to the lithopanspermia hypothesis, and they will provide basic data to planetary protection issues. To get better insight into the habitability of Mars, one set of samples was exposed to simulated Martian conditions (UV-radiation climate, pressure, atmosphere), with and without a protective cover of simulated Martian soil. The biological test samples selected are hardy representatives of various branches of life.\n\nThere were three EXPOSE experiments completed between 2008 and 2015: EXPOSE-E, EXPOSE-R and EXPOSE-R2. <br>\nThe EXPOSE-E was launched on February 7, 2008 on board the Space Shuttle Atlantis and was mounted on the ISS European module \"Columbus\" to the European Technology Exposure Facility (EuTEF). EXPOSE-R was launched to the ISS on November 26, 2008 from Baikonur in Kazakhstan on board of a Progress capsule and was mounted on the ISS Russian module \"Zevzda\". EXPOSE-R2 was launched\n\nEXPOSE-E provided accommodation in three exposure trays for a variety of astrobiological test samples that were exposed to selected space conditions: either to space vacuum, solar electromagnetic radiation at >110 nm and cosmic radiation (trays 1 and 3) or to simulated Martian surface conditions (tray 2). The different experiments consisted in exposing solid molecules, gas mixtures or biological samples to the solar ultraviolet (UV) radiation, cosmic rays, vacuum and temperature fluctuations of outer space as the ISS repeatedly passed between areas of direct sunlight and the cold darkness of Earth's shadow.\n\nAt the end of the exposition period, EXPOSE-E was brought back to the ground in September 2009 as part of the Space Shuttle Discovery mission STS-128. EXPOSE-R was brought back in 2011 by a Soyuz spacecraft. From the landing site in Kazakhstan, the trays were returned via Moscow and distributed to scientists for further analysis in their laboratories.\n\nEXPOSE-R2 was launched on 24 July 2014, exposure was finished on April 2015, and was returned to Earth in early 2016 where it is still undergoing analyses.\n\nThe EXPOSE-E experiments are:\n\nThe search for organic molecules at the surface of Mars is a top priority of Mars exploration space missions. Therefore, a key step in interpretation of future data collected by these missions is to understand the preservation of organic matter in the Martian environment. A 1.5-year exposure to Mars-like surface UV radiation conditions in space resulted in complete degradation of the organic compounds (glycine, serine, phthalic acid, phthalic acid in the presence of a mineral phase, and mellitic acid). Their half-lives were between 50 and 150 h for Martian surface conditions.\n\nTo understand the chemical behavior of organic molecules in the space environment, amino acids and a dipeptide in pure form and embedded in meteorite powder were exposed to space conditions for 18 months; the samples were returned to Earth and analyzed in the laboratory for reactions caused by solar UV and cosmic radiation. The results show that resistance to irradiation is a function of the chemical nature of the exposed molecules and the wavelengths of the UV light. The most altered compounds were the dipeptide, aspartic acid, and aminobutyric acid. The most resistant were alanine, valine, glycine, and aminoisobutyric acid. The results also demonstrate the protective effect of meteorite powder, which reemphasizes the importance of exogenic contribution to the inventory of prebiotic organics on early Earth.\n\nBacterial endospores of the highly UV-resistant \"Bacillus subtilis\" strain MW01 were exposed to low-Earth orbit and simulated Martian surface conditions for 559 days. It was clearly shown that solar extraterrestrial UV radiation (λ ≥110 nm) as well as the Martian UV spectrum (λ ≥200 nm) was the most deleterious factor applied; in some samples only a few spore survivors were recovered from \"B. subtilis\" MW01 spores exposed in monolayers. However, \"if shielded from solar irradiation\", about 8% of MW01 spores survived, and 100% survived in simulated Martian conditions, compared to the laboratory controls.\n\nSpore-forming bacteria are of particular concern in the context of planetary protection because their tough endospores may withstand certain sterilization procedures as well as the harsh environments of outer space or planetary surfaces. To test their hardiness on a hypothetical mission to Mars, spores of \"Bacillus subtilis\" 168 and \"Bacillus pumilus\" SAFR-032 were exposed for 1.5 years to selected parameters of space. It was clearly shown that solar extraterrestrial UV radiation (λ ≥110 nm) as well as the Martian UV spectrum (λ ≥200 nm) was the most deleterious factor applied; in some samples only a few survivors were recovered from spores exposed in monolayers. Spores in multilayers survived better by several orders of magnitude. All other environmental parameters encountered did little harm to the spores, which showed about 50% survival or more. The data demonstrate the high chance of survival of spores on a Mars mission, if protected against solar irradiation. These results will have implications for planetary protection considerations.\n\nThe mutagenic efficiency of space was also studied in spores of \"Bacillus subtilis\" 168. The data show the unique mutagenic power of space and Martian surface conditions as a consequence of DNA injuries induced by solar UV radiation and space vacuum or the low pressure of Mars. Spores exposed to space demonstrated a much broader and more severe stress response than spores exposed to simulated Martian conditions.\n\nA comparative protein analysis (proteomics) of \"Bacillus pumilus\" SAFR-032 spores indicated that proteins conferring resistant traits (superoxide dismutase) were present in higher concentration in space-exposed spores when compared to controls. Also, the first-generation cells and spores derived from space-exposed samples exhibited elevated ultraviolet-C resistance when compared with their ground control counterparts. The data generated are important for calculating the probability and mechanisms of microbial survival in space conditions and assessing microbial contaminants as risks for forward contamination and \"in situ\" life detection.\n\n\nAfter 1.5 years in space, samples were retrieved, rehydrated and spread on different culture media. The only two organisms able to grow were isolated from a sample exposed to simulated Mars conditions beneath a 0.1% T Suprasil neutral density filter and from a sample exposed to space vacuum without solar radiation exposure, respectively. The two surviving organisms were identified as \"Stichococcus sp.\" (green algae) and \"Acarospora sp\". (lichenized fungal genus). Among other fungal spores tested were \"Cryomyces antarcticus\" and \"Cryomyces minteri\", and although 60% of the cells' DNA studied remained intact after the Mars-like conditions, less than 10% of the fungi were able to proliferate and form colonies after their return to Earth. According to the researchers, the studies provide experimental information on the possibility of eukaryotic life transfer from one planet to another by means of rocks and of survival in Mars environment.\n\nCryptoendolithic microbial communities and epilithic lichens have been considered as appropriate candidates for the scenario of lithopanspermia, which proposes a natural interplanetary exchange of organisms by means of rocks that have been impact ejected from their planet of origin. A 1.5 years exposure experiment in space was performed with a variety of rock-colonizing eukaryotic organisms. Selected organisms are known to cope with the environmental extremes of their natural habitats. It was found that some —but not all— of those most robust microbial communities from extremely hostile regions on Earth are also partially resistant to the even more hostile environment of outer space, including high vacuum, temperature fluctuation, the full spectrum of extraterrestrial solar electromagnetic radiation, and cosmic ionizing radiation. Although the reported experimental period of 1.5 years in space is not comparable with the time spans of thousands or millions of years believed to be required for lithopanspermia, the data provide first evidence of the differential hardiness of cryptoendolithic communities in space.\n\n\nThe plausibility that life was imported to Earth from elsewhere was tested by subjecting plant seeds to 1.5 years of exposure to solar UV, solar and galactic cosmic radiation, temperature fluctuations, and space vacuum outside the International Space Station. Of the 2100 exposed wild-type \"Arabidopsis thaliana\" and \"Nicotiana tabacum\" (tobacco) seeds, 23% produced viable plants after being returned to Earth. Germination was delayed in seeds shielded from solar light, yet full survival was attained, which indicates that longer space travel would be possible for seeds embedded in an opaque matrix. The team conclude that a naked, seed-like entity could have survived exposure to solar UV radiation during a hypothetical transfer from Mars to Earth, and even if seeds do not survive, components (e.g., their DNA) might survive transfer over cosmic distances.\n\nAs a consequence of the high shielding by the nearby ISS, the biological samples were predominantly exposed to galactic cosmic heavy ions, while electrons and a significant fraction of protons of the radiation belts and solar wind did not reach the samples.\n\n\nR3D measured ionizing and non-ionizing radiation as well as cosmic radiation reaching the biological samples located on the EXPOSE-E. Due to errors in data transmission or temporary termination of EXPOSE power, not all data could be acquired. Radiation was not constant during the mission. At regular intervals of about 2 months, low or almost no radiation was encountered. The radiation dose during the mission was 1823.98 MJ m−2 for PAR, 269.03 MJ m−2 for UVA, 45.73 MJ m−2 for UVB, or 18.28 MJ m−2 for UVC. Registered sunshine duration during the mission was about 152 days (about 27% of mission time). The surface of EXPOSE was most likely turned away from the Sun for considerably longer time.\nThe highest daily averaged absorbed dose rate of 426 μGy per day came from the 'South Atlantic Anomaly' (SAA) region of the inner radiation belt; galactic cosmic rays (GCR) delivered a daily absorbed dose rate of 91.1 μGy per day, and the outer radiation belt (ORB) source delivered 8.6 μGy per day.\n\nExpose-R (‘R’ stands for its mounting on the Russian module Zvezda) was mounted by Russian cosmonaut's extravehicular activity on March 11, 2009 and the exposure to outer space conditions continued for 682 days until January 21, 2011, when it was brought back to Earth by the last \"Discovery Shuttle\" flight STS 133 on March 9, 2011. EXPOSE-R was equipped with three trays housing eight experiments and 3 radiation dosimeters. Each tray was loaded with a variety of biological organisms including plant seeds and spores of bacteria, fungi and ferns that were exposed to the harsh space environment for about one and a half years. The ROSE (Response of Organisms to Space Environment) group of experiments are under the coordination of the German Aerospace Center (DLR) and has been composed of scientists from different European countries, from USA and from Japan. In its 8 experiments of biological and chemical content, more than 1200 individual samples were exposed to solar ultraviolet (UV) radiations, vacuum, cosmic rays or extreme temperature variations. In their different experiments, the involved scientists are studying the question of life's origin on Earth and the results of their experiments are contributing to different aspects of the evolution and distribution of life in the Universe.\n\nThe EXPOSE-R experiments are:\n\nPictures acquired during the spacewalk #27 on the final day of exposure indicated that many of the 75 small windows had turned brown. The brown film was clearly a deposit which had precipitated inside the windows during the spaceflight. The appearance of the brown film turned out to depend on two prerequisites: solar irradiation and vacuum. As the brown film should have impacted the quantity and quality of solar light that reached the test samples, affecting the core of the scientific goals, an investigation was started to identify the properties and the root cause of the colour change. The brown film contained hydrocarbons, so an inventory was made of materials contained inside Expose-R that could possibly have delivered the contaminating volatiles.\n\nThe true chemical identity was not established, but their origin may have been substances added to adhesives, plastics and printed circuit boards.\nSince not all the windows developed a contaminating brown film, some experiments were effectively exposed:\n\n\nA third mission, called EXPOSE-R2, was launched on 24 July 2014 aboard the Russian Progress M-24M, carrying 46 species of bacteria, fungi and arthropods, in 758 different samples that were exposed to different conditions, under different filters, and for various time periods. It was attached on 18 August 2014 to the exterior of the ISS on the Russian module Zvezda, and exposure was finished on 3 February 2016, and were stored inside the ISS until their return to Earth on 18 June 2016. Two main experiments (BIOMEX and BOSS) tested a desert strain of cyanobacterium called \"Chroococcidiopsis\" and \"Deinococcus geothermalis\", as well as bacteria, yeast (including Kombucha culture), archaea, algae, fungi, lichens and mosses, while the Biochip experiment will test affinity receptors to biomolecules. The organisms and organic compounds were exposed to partial and full space conditions for 12 to 18 months, and were returned to Earth in early 2016 for analyses.\n\n\n\n\n\n", "id": "16598780", "title": "EXPOSE"}
{"url": "https://en.wikipedia.org/wiki?curid=45663583", "text": "Human interactome\n\nThe human interactome is the set of protein–protein interactions (the interactome) that occur in human cells. The sequencing of reference genomes, in particular the Human Genome Project, has revolutionized human genetics, molecular biology, and clinical medicine. Genome-wide association study results have led to the association of genes with most Mendelian disorders, and over 140 000 germline mutations have been associated with at least one genetic disease. However, it became apparent that inherent to these studies is an emphasis on clinical outcome rather than a comprehensive understanding of human disease; indeed to date the most significant contributions of GWAS have been restricted to the “low-hanging fruit” of direct single mutation disorders, prompting a systems biology approach to genomic analysis. The connection between genotype and phenotype (how variation in genotype affects the disease or normal functioning of the cell and the human body) remain elusive, especially in the context of multigenic complex traits and cancer. To assign functional context to genotypic changes, much of recent research efforts have been devoted to the mapping of the networks formed by interactions of cellular and genetic components in humans, as well as how these networks are altered by genetic and somatic disease.\n\nWith the sequencing of the genomes of a diverse array or model organisms, it became clear that the number of genes does not correlate with the human perception of relative organism complexity – the human proteome contains some 20 000 genes, which is smaller than some species such as corn. A statistical approach to calculating the number of interactions in humans gives an estimate of around 650 000, one order of magnitude bigger than Drosophila and 3 times larger than C. Elegans. As of 2008, only about <0.3% of all estimated interactions among human proteins has been identified, although in recent years there has been exponential growth in discovery – as of 2015, over 210 000 unique human positive protein–protein interactions are currently catalogued, and bioGRID database contains almost 750 000 literature-curated PPI's for 30 model organisms, 300 000 of which are verified or predicted human physical or genetic protein–protein interactions, a 50% increase from 2013. The currently available information on the human interactome network originates from either literature-curated interactions, high-throughput experiments, or from potential interactions predicted from interactome data, whether through phylogenetic profiling (evolutionary similarity), statistical network inference, or text/literature mining methods.\n\nProtein–protein interactions are only the raw material for networks. To form useful interactome databases and create integrated networks, other types of data that can be combined with protein–protein interactions include information on gene expression and co-expression, cellular co-localization of proteins (based on microscopy), genetic information, metabolic and signalling pathways, and more. The end goal of unravelling human protein interactomes is ultimately to understand mechanisms of disease and uncover previously unknown disease genes. It has been found that proteins with a high number of interactions (outward edges) are significantly more likely to be hubs in modules that correlate with disease, probably because proteins with more interactions are involved in more biological functions. By mapping disease alterations to the human interactome, we can gain a much better understanding of the pathways and biological processes of disease.\n\nAnalysis of metabolic networks of proteins hearkens back to the 1940s, but it was not until the late 1990s and early 2000s that computational data-driven genomic analyses to predict functional context and networks of genetic associations appeared in earnest. Since then, the interactomes of many model organisms are considered to have been well characterized, notably the Saccharomyces cerevisiae Interactome and the Drosophila interactome.\n\nHigh throughput experimental approaches for discovering protein–protein interactions typically perform a version of the two-hybrid screening approach or tandem affinity purification followed by mass spectrometry. Information from experiments and literature curation are compiled into databases of protein interactions, such as DIP, and BioGRID. A more recent effort, HINT-KB, attempts to amalgamate most of the current PPI databases, but filtering systematically erroneous interactions as well as trying to correct for inherent sociological sampling biases in literature curated datasets.\n\nSmaller human interactome networks have been described in the specific context of important drivers of many different disorders, including neurodegenerative disorders, autism and other psychiatric disorders, and cancer. Cancer gene networks have been particularly well studied, due in part to large genome initiatives such as The Cancer Genome Atlas (TCGA). A large portion of the mutational landscape including intra-tumoural heterogeneity has been mapped for most common types of cancers (for example, breast cancer has been well studied, and many studies have also investigated the difference between active driver genes and passive passenger mutations in the context of cancer interaction networks.\n\nThe first attempts at large-scale integrative human interactome mapping occurred around 2005. Stetzl et al. used a protein matrix of 4500 baits and 5600 preys in a yeast two hybrid system to piece together the interactome, and Rual et al. performed a similar yeast-two hybrid study verified with co-affinity purification and correlation with other biological attributes, revealing more than 300 connections to 100 disease-associated proteins. Since those pioneering efforts, hundreds of similar studies have been conducted. Compiled databases such as UniHI provide platform for single entry. Futschik et al. performed a meta analysis of eight interactome maps and found that of 57 000 interacting proteins in total, there was a small (albeit statistically significant) overlap between the different databases, indicating considerable selection and detection biases.\n\nIn 2010, around 130 000 binary interactions in the interactome were described in the most popular databases, but many were verified with only one source. With the rapid development of high throughput methods, datasets still suffer from high rates of false positives and low coverage of the interactome. Tyagi et al. described a novel framework for incorporating structural complexes and binding interfaces for verification. This was part of much larger efforts for PPI verification; interaction networks are typically validated further by using a combination of coexpression profiles, protein structural information, Gene ontology terms, topological considerations, and colocalization before being considered “high-confidence”.\n\nA recent resource paper (November 2014) attempts to provide a more comprehensive proteome level map of the human interactome. It found vast uncharted territory in the human interactome, and used diverse methods to build a new interactome map correcting for curation bias, including probing all pairwise combinations of 13 000 protein products for interaction using Yeast two hybrid and co-affinity purification, in a massive coordinated effort across research labs in Canada and the United States. However, this still represents confirmation of but a fraction of expected interactions – around 30 000 of high confidence. Despite the coordinated efforts of many, the human interactome is still very much a work in progress.\n\n", "id": "45663583", "title": "Human interactome"}
{"url": "https://en.wikipedia.org/wiki?curid=19200", "text": "Molecular biology\n\nMolecular biology concerns the molecular basis of biological activity between biomolecules in the various systems of a cell, including the interactions between DNA, RNA, and proteins and their biosynthesis, as well as the regulation of these interactions. Writing in \"Nature\" in 1961, William Astbury described molecular biology as:\n\nResearchers in molecular biology use specific techniques native to molecular biology but increasingly combine these with techniques and ideas from genetics and biochemistry. There is not a defined line between these disciplines. The figure to the right is a schematic that depicts one possible view of the relationships between the fields:\n\n\nMuch of molecular biology is quantitative, and recently much work has been done at its interface with computer science in bioinformatics and computational biology. In the early 2000s, the study of gene structure and function, molecular genetics, has been among the most prominent sub-fields of molecular biology. Increasingly many other areas of biology focus on molecules, either directly studying interactions in their own right such as in cell biology and developmental biology, or indirectly, where molecular techniques are used to infer historical attributes of populations or species, as in fields in evolutionary biology such as population genetics and phylogenetics. There is also a long tradition of studying biomolecules \"from the ground up\" in biophysics.\n\nOne of the most basic techniques of molecular biology to study protein function is molecular cloning. In this technique, DNA coding for a protein of interest is cloned using polymerase chain reaction (PCR), and/or restriction enzymes into a plasmid ( expression vector). A vector has 3 distinctive features: an origin of replication, a multiple cloning site (MCS), and a selective marker usually antibiotic resistance. Located upstream of the multiple cloning site are the promoter regions and the transcription start site which regulate the expression of cloned gene.\nThis plasmid can be inserted into either bacterial or animal cells. Introducing DNA into bacterial cells can be done by transformation via uptake of naked DNA, conjugation via cell-cell contact or by transduction via viral vector. Introducing DNA into eukaryotic cells, such as animal cells, by physical or chemical means is called transfection. Several different transfection techniques are available, such as calcium phosphate transfection, electroporation, microinjection and liposome transfection. The plasmid may be integrated into the genome, resulting in a stable transfection, or may remain independent of the genome, called transient transfection.\n\nDNA coding for a protein of interest is now inside a cell, and the protein can now be expressed. A variety of systems, such as inducible promoters and specific cell-signaling factors, are available to help express the protein of interest at high levels. Large quantities of a protein can then be extracted from the bacterial or eukaryotic cell. The protein can be tested for enzymatic activity under a variety of situations, the protein may be crystallized so its tertiary structure can be studied, or, in the pharmaceutical industry, the activity of new drugs against the protein can be studied.\n\nPolymerase chain reaction (PCR) is an extremely versatile technique for copying DNA. In brief, PCR allows a specific DNA sequence to be copied or modified in predetermined ways. The reaction is extremely powerful and under perfect conditions could amplify one DNA molecule to become 1.07 billion molecules in less than two hours. The PCR technique can be used to introduce restriction enzyme sites to ends of DNA molecules, or to mutate particular bases of DNA, the latter is a method referred to as site-directed mutagenesis. PCR can also be used to determine whether a particular DNA fragment is found in a cDNA library. PCR has many variations, like reverse transcription PCR (RT-PCR) for amplification of RNA, and, more recently, quantitative PCR which allow for quantitative measurement of DNA or RNA molecules.\n\nGel electrophoresis is one of the principal tools of molecular biology. The basic principle is that DNA, RNA, and proteins can all be separated by means of an electric field and size. In agarose gel electrophoresis, DNA and RNA can be separated on the basis of size by running the DNA through an electrically charged agarose gel. Proteins can be separated on the basis of size by using an SDS-PAGE gel, or on the basis of size and their electric charge by using what is known as a 2D gel electrophoresis.\n\nThe terms \"northern\", \"western\" and \"eastern\" blotting are derived from what initially was a molecular biology joke that played on the term \"Southern blotting\", after the technique described by Edwin Southern for the hybridisation of blotted DNA. Patricia Thomas, developer of the RNA blot which then became known as the \"northern blot\", actually didn't use the term.\n\nNamed after its inventor, biologist Edwin Southern, the Southern blot is a method for probing for the presence of a specific DNA sequence within a DNA sample. DNA samples before or after restriction enzyme (restriction endonuclease) digestion are separated by gel electrophoresis and then transferred to a membrane by blotting via capillary action. The membrane is then exposed to a labeled DNA probe that has a complement base sequence to the sequence on the DNA of interest. Southern blotting is less commonly used in laboratory science due to the capacity of other techniques, such as PCR, to detect specific DNA sequences from DNA samples. These blots are still used for some applications, however, such as measuring transgene copy number in transgenic mice or in the engineering of gene knockout embryonic stem cell lines.\n\nThe northern blot is used to study the expression patterns of a specific type of RNA molecule as relative comparison among a set of different samples of RNA. It is essentially a combination of denaturing RNA gel electrophoresis, and a blot. In this process RNA is separated based on size and is then transferred to a membrane that is then probed with a labeled complement of a sequence of interest. The results may be visualized through a variety of ways depending on the label used; however, most result in the revelation of bands representing the sizes of the RNA detected in sample. The intensity of these bands is related to the amount of the target RNA in the samples analyzed. The procedure is commonly used to study when and how much gene expression is occurring by measuring how much of that RNA is present in different samples. It is one of the most basic tools for determining at what time, and under what conditions, certain genes are expressed in living tissues.\n\nIn western blotting, proteins are first separated by size, in a thin gel sandwiched between two glass plates in a technique known as SDS-PAGE. The proteins in the gel are then transferred to a polyvinylidene fluoride (PVDF), nitrocellulose, nylon, or other support membrane. This membrane can then be probed with solutions of antibodies. Antibodies that specifically bind to the protein of interest can then be visualized by a variety of techniques, including colored products, chemiluminescence, or autoradiography. Often, the antibodies are labeled with enzymes. When a chemiluminescent substrate is exposed to the enzyme it allows detection. Using western blotting techniques allows not only detection but also quantitative analysis. Analogous methods to western blotting can be used to directly stain specific proteins in live cells or tissue sections.\n\nThe eastern blotting technique is used to detect post-translational modification of proteins. Proteins blotted on to the PVDF or nitrocellulose membrane are probed for modifications using specific substrates.\n\nDNA microarray is a collection of spots attached to a solid support such as a microscope slide where each spot contains one or more single-stranded DNA oligonucleotide fragment. Arrays make it possible to put down large quantities of very small (100 micrometre diameter) spots on a single slide. Each spot has a DNA fragment molecule that is complementary to a single DNA sequence. A variation of this technique allows the gene expression of an organism at a particular stage in development to be qualified (expression profiling). In this technique the RNA in a tissue is isolated and converted to labeled cDNA. This cDNA is then hybridized to the fragments on the array and visualization of the hybridization can be done. Since multiple arrays can be made with exactly the same position of fragments they are particularly useful for comparing the gene expression of two different tissues, such as a healthy and cancerous tissue. Also, one can measure what genes are expressed and how that expression changes with time or with other factors. \nThere are many different ways to fabricate microarrays; the most common are silicon chips, microscope slides with spots of ~100 micrometre diameter, custom arrays, and arrays with larger spots on porous membranes (macroarrays). There can be anywhere from 100 spots to more than 10,000 on a given array. Arrays can also be made with molecules other than DNA.\n\nAllele-specific oligonucleotide (ASO) is a technique that allows detection of single base mutations without the need for PCR or gel electrophoresis. Short (20-25 nucleotides in length), labeled probes are exposed to the non-fragmented target DNA, hybridization occurs with high specificity due to the short length of the probes and even a single base change will hinder hybridization. The target DNA is then washed and the labeled probes that didn't hybridize are removed. The target DNA is then analyzed for the presence of the probe via radioactivity or fluorescence. In this experiment, as in most molecular biology techniques, a control must be used to ensure successful experimentation.\n\nIn molecular biology, procedures and technologies are continually being developed and older technologies abandoned. For example, before the advent of DNA gel electrophoresis (agarose or polyacrylamide), the size of DNA molecules was typically determined by rate sedimentation in sucrose gradients, a slow and labor-intensive technique requiring expensive instrumentation; prior to sucrose gradients, viscometry was used. Aside from their historical interest, it is often worth knowing about older technology, as it is occasionally useful to solve another new problem for which the newer technique is inappropriate.\n\nWhile molecular biology was established in the 1930s, the term was coined by Warren Weaver in 1938. Weaver was the director of Natural Sciences for the Rockefeller Foundation at the time and believed that biology was about to undergo a period of significant change given recent advances in fields such as X-ray crystallography.\n\nClinical research and medical therapies arising from molecular biology are partly covered under gene therapy. The use of molecular biology or molecular cell biology approaches in medicine is now called molecular medicine. Molecular biology also plays important role in understanding formations, actions, and regulations of various parts of cells which can be used to efficiently target new drugs, diagnosis disease, and understand the physiology of the cell.\n\n", "id": "19200", "title": "Molecular biology"}
{"url": "https://en.wikipedia.org/wiki?curid=46549364", "text": "TERRA (biology)\n\nTERRA in biology is an abbreviation for \"TElomeric Repeat-containing RNA\". TERRA is RNA that is transcribed from telomeres — the repeating 6-nucleotide sequences that cap the ends of chromosomes. TERRA functions with shelterin to inhibit telomere lengthening by telomerase enzyme. At least four factors contribute to telomere maintenance: telomerase, shelterin, TERRA and the CST Complex.\n\nTERRA can also regulate telomere length by increasing euchromatin formation. On the other hand, nonsense-mediated decay factor enrichment at telomeres may exist to prevent TERRA inhibition of telomerase. TERRA levels vary during the cell cycle, decreasing during S phase, and increasing in the transition from G2 phase to G1 phase.\n", "id": "46549364", "title": "TERRA (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=54888", "text": "Telomere\n\nA telomere is a region of repetitive nucleotide sequences at each end of a chromosome, which protects the end of the chromosome from deterioration or from fusion with neighboring chromosomes. Its name is derived from the Greek nouns telos (\"τέλος\") \"end\" and merοs (\"μέρος\", root: \"μερ-\") \"part\". For vertebrates, the sequence of nucleotides in telomeres is TTAGGG, with the complementary DNA strand being AATCCC, with a single-stranded TTAGGG overhang. This sequence of TTAGGG is repeated approximately 2,500 times in humans. In humans, average telomere length declines from about 11 kilobases at birth to less than 4 kilobases in old age, with the average rate of decline being greater in men than in women.\n\nDuring chromosome replication, the enzymes that duplicate DNA cannot continue their duplication all the way to the end of a chromosome, so in each duplication the end of the chromosome is shortened (this is because the synthesis of Okazaki fragments requires RNA primers attaching ahead on the lagging strand). The telomeres are disposable buffers at the ends of chromosomes which are truncated during cell division; their presence protects the genes before them on the chromosome from being truncated instead. The telomeres themselves are protected by a complex of shelterin proteins, as well as by the RNA that telomeric DNA encodes (TERRA).\n\nOver time, due to each cell division, the telomere ends become shorter. They are replenished by an enzyme, telomerase reverse transcriptase.\n\nIn 1933, Barbara McClintock, a distinguished American cytogeneticist and the first woman to receive an unshared Nobel Prize in Physiology or Medicine, observed that the chromosomes lacking end parts became \"sticky\" and hypothesised the existence of a special structure at the chromosome tip that would maintain chromosome stability. Similar observations were reported by Hermann Muller who coined the term \"telomere\".\n\nIn the early 1970s, Russian theorist Alexei Olovnikov first recognized that chromosomes could not completely replicate their ends. Building on this, and to accommodate Leonard Hayflick's idea of limited somatic cell division, Olovnikov suggested that DNA sequences are lost every time a cell/DNA replicates until the loss reaches a critical level, at which point cell division ends. However, Olovnikov's prediction was not widely known except by a handful of researchers studying cellular aging and immortalization.\n\nIn 1975–1977, Elizabeth Blackburn, working as a postdoctoral fellow at Yale University with Joseph G. Gall, discovered the unusual nature of telomeres, with their simple repeated DNA sequences composing chromosome ends. Blackburn, Carol Greider, and Jack Szostak were awarded the 2009 Nobel Prize in Physiology or Medicine for the discovery of how chromosomes are protected by telomeres and the enzyme telomerase.\n\nNevertheless, in the 1970s there was no recognition that the telomere-shortening mechanism normally limits cells to a fixed number of divisions, nor was there any animal study suggesting that this could be responsible for aging on the cellular level. There was also no recognition that the mechanism set a limit on lifespans.\n\nIt remained for a privately funded collaboration from biotechnology company Geron to isolate the genes for the RNA and protein component of human telomerase in order to establish the role of telomere shortening in cellular aging and telomerase reactivation in cell immortalization.\n\nTelomeres are repetitive nucleotide sequences located at the termini of linear chromosomes of most eukaryotic organisms. For vertebrates, the sequence of nucleotides in telomeres is TTAGGG. Most prokaryotes, having circular chromosomes rather than linear, do not have telomeres. Telomeres compensate for incomplete semi-conservative DNA replication at chromosomal ends. A protein complex known as shelterin serves to protect the ends of telomeres from being recognised as double-strand breaks by inhibiting homologous recombination (HR) and non-homologous end joining (NHEJ).\n\nIn most prokaryotes, chromosomes are circular and, thus, do not have ends to suffer premature replication termination. A small fraction of bacterial chromosomes (such as those in \"Streptomyces\", \"Agrobacterium\", and \"Borrelia\") are linear and possess telomeres, which are very different from those of the eukaryotic chromosomes in structure and functions. The known structures of bacterial telomeres take the form of proteins bound to the ends of linear chromosomes, or hairpin loops of single-stranded DNA at the ends of the linear chromosomes.\n\nWhile replicating DNA, the eukaryotic DNA replication enzymes (the DNA polymerase protein complex) cannot replicate the sequences present at the ends of the chromosomes (or more precisely the chromatid fibres). Hence, these sequences and the information they carry may get lost. This is the reason telomeres are so important in context of successful cell division: They \"cap\" the end-sequences and themselves get lost in the process of DNA replication. But the cell has an enzyme called telomerase, which carries out the task of adding repetitive nucleotide sequences to the ends of the DNA. Telomerase, thus, \"replenishes\" the telomere \"cap\" of the DNA. In most multicellular eukaryotic organisms, telomerase is active only in germ cells, some types of stem cells such as embryonic stem cells, and certain white blood cells. Telomerase can be reactivated and telomeres reset back to an embryonic state by somatic cell nuclear transfer. There are theories that claim that the steady shortening of telomeres with each replication in somatic (body) cells may have a role in senescence and in the prevention of cancer. This is because the telomeres act as a sort of time-delay \"fuse\", eventually running out after a certain number of cell divisions and resulting in the eventual loss of vital genetic information from the cell's chromosome with future divisions.\n\nTelomere length varies greatly between species, from approximately 300 base pairs in yeast to many kilobases in humans, and usually is composed of arrays of guanine-rich, six- to eight-base-pair-long repeats. Eukaryotic telomeres normally terminate with 3′ single-stranded-DNA overhang, which is essential for telomere maintenance and capping. Multiple proteins binding single- and double-stranded telomere DNA have been identified. These function in both telomere maintenance and capping. Telomeres form large loop structures called telomere loops, or T-loops. Here, the single-stranded DNA curls around in a long circle, stabilized by telomere-binding proteins. At the very end of the T-loop, the single-stranded telomere DNA is held onto a region of double-stranded DNA by the telomere strand disrupting the double-helical DNA, and base pairing to one of the two strands. This triple-stranded structure is called a displacement loop or D-loop.\n\nTelomere shortening in humans can induce replicative senescence, which blocks cell division. This mechanism appears to prevent genomic instability and development of cancer in human aged cells by limiting the number of cell divisions. However, shortened telomeres impair immune function that might also increase cancer susceptibility.<ref name=\"10.1002/ajhb.21127\"></ref> If telomeres become too short, they have the potential to unfold from their presumed closed structure. The cell may detect this uncapping as DNA damage and then either stop growing, enter cellular old age (senescence), or begin programmed cell self-destruction (apoptosis) depending on the cell's genetic background (p53 status). Uncapped telomeres also result in chromosomal fusions. Since this damage cannot be repaired in normal somatic cells, the cell may even go into apoptosis. Many aging-related diseases are linked to shortened telomeres. Organs deteriorate as more and more of their cells die off or enter cellular senescence.\n\nAt the very distal end of the telomere is a 300 base pair single-stranded portion, which forms the T-loop. This loop is analogous to a knot, which stabilizes the telomere, preventing the telomere ends from being recognized as break points by the DNA repair machinery. Should non-homologous end joining occur at the telomeric ends, chromosomal fusion will result. The T-loop is held together by several proteins, the most notable ones being TRF1, TRF2, POT1, TIN1, and TIN2, collectively referred to as the shelterin complex. In humans, the shelterin complex consists of six proteins identified as TRF1, TRF2, TIN2, POT1, TPP1, and RAP1.\n\nTelomeres shorten in part because of the \"end replication problem\" that is exhibited during DNA replication in eukaryotes only. Because DNA replication does not begin at either end of the DNA strand, but starts in the center, and considering that all known DNA polymerases move in the 5' to 3' direction, one finds a leading and a lagging strand on the DNA molecule being replicated.\n\nOn the leading strand, DNA polymerase can make a complementary DNA strand without any difficulty because it goes from 5' to 3'. However, there is a problem going in the other direction on the lagging strand. To counter this, short sequences of RNA acting as primers attach to the lagging strand a short distance ahead of where the initiation site was. The DNA polymerase can start replication at that point and go to the end of the initiation site. This causes the formation of Okazaki fragments. More RNA primers attach further on the DNA strand and DNA polymerase comes along and continues to make a new DNA strand.Eventually, the last RNA primer attaches, and DNA polymerase, RNA nuclease, and DNA ligase come along to convert the RNA (of the primers) to DNA and to seal the gaps in between the Okazaki fragments. But, in order to change RNA to DNA, there must be another DNA strand in front of the RNA primer. This happens at all the sites of the lagging strand, but it does not happen at the end where the last RNA primer is attached. Ultimately, that RNA is destroyed by enzymes that degrade any RNA left on the DNA. Thus, a section of the telomere is lost during each cycle of replication at the 5' end of the lagging strand's daughter.\n\nHowever, test-tube studies have shown that telomeres are highly susceptible to oxidative stress. There is evidence that oxidative stress-mediated DNA damage is an important determinant of telomere shortening. Telomere shortening due to free radicals explains the difference between the estimated loss per division because of the end-replication problem (c. 20 bp) and actual telomere shortening rates (50–100 bp), and has a greater absolute impact on telomere length than shortening caused by the end-replication problem. Population-based studies have also indicated an interaction between anti-oxidant intake and telomere length. In the Long Island Breast Cancer Study Project (LIBCSP), authors found a moderate increase in breast cancer risk among women with the shortest telomeres and lower dietary intake of beta carotene, vitamin C or E. These results suggest that cancer risk due to telomere shortening may interact with other mechanisms of DNA damage, specifically oxidative stress.\n\nTelomere shortening is associated with aging, mortality and aging-related diseases. In 2003, Richard Cawthon discovered that those with longer telomeres lead longer lives than those with short telomeres. However, it is not known whether short telomeres are just a sign of cellular age or actually contribute to the aging process themselves.\n\nA 2017 meta-analysis of 23 studies found that increased perceived psychological stress was associated with a very small decrease in telomere length- although there was also evidence of potential publication bias which when taken into account attenuated this effect and made it non-significant.\n\nThe phenomenon of limited cellular division was first observed by Leonard Hayflick, and is now referred to as the Hayflick limit.\nSignificant discoveries were subsequently made by a group of scientists organized at Geron Corporation by Geron's founder Michael D. West that tied telomere shortening with the Hayflick limit. The cloning of the catalytic component of telomerase enabled experiments to test whether the expression of telomerase at levels sufficient to prevent telomere shortening was capable of immortalizing human cells. Telomerase was demonstrated in a 1998 publication in \"Science\" to be capable of extending cell lifespan, and now is well-recognized as capable of immortalizing human somatic cells.\n\nIt is becoming apparent that reversing shortening of telomeres through temporary activation of telomerase may be a potent means to slow aging. The reason that this would extend human life is because it would extend the Hayflick limit. Three routes have been proposed to reverse telomere shortening: drugs, gene therapy, or metabolic suppression, so-called, torpor/hibernation. So far these ideas have not been proven in humans, but it has been demonstrated that telomere shortening is reversed in hibernation and aging is slowed (Turbill, et al. 2012 & 2013) and that hibernation prolongs life-span (Lyman et al. 1981). It has also been demonstrated that telomere extension has successfully reversed some signs of aging in laboratory mice and the nematode worm species \"Caenorhabditis elegans\". It has been hypothesized that longer telomeres and especially telomerase activation might cause increased cancer (e.g. Weinstein and Ciszek, 2002). However, longer telomeres might also protect against cancer, because short telomeres are associated with cancer. It has also been suggested that longer telomeres might cause increased energy consumption.\n\nTechniques to extend telomeres could be useful for tissue engineering, because they might permit healthy, noncancerous mammalian cells to be cultured in amounts large enough to be engineering materials for biomedical repairs.\n\nTwo recent studies on long-lived seabirds demonstrate that the role of telomeres is far from being understood. In 2003, scientists observed that the telomeres of Leach's storm-petrel (\"Oceanodroma leucorhoa\") seem to lengthen with chronological age, the first observed instance of such behaviour of telomeres. In 2006, Juola \"et al.\" reported that in another unrelated, long-lived seabird species, the great frigatebird (\"Fregata minor\"), telomere length did decrease until at least c. 40 years of age (i.e. probably over the entire lifespan), but the speed of decrease slowed down massively with increasing ages, and that rates of telomere length decrease varied strongly between individual birds. They concluded that in this species (and probably in frigatebirds and their relatives in general), telomere length could not be used to determine a bird's age sufficiently well. Thus, it seems that there is much more variation in the behavior of telomere length than initially believed.\n\nFurthermore, Gomes et al. found, in a study of the comparative biology of mammalian telomeres, that telomere length of different mammalian species correlates inversely, rather than directly, with lifespan, and they concluded that the contribution of telomere length to lifespan remains controversial. Harris et al. found little evidence that, in humans, telomere length is a significant biomarker of normal aging with respect to important cognitive and physical abilities. Gilley and Blackburn tested whether cellular senescence in paramecium is caused by telomere shortening, and found that telomeres were not shortened during senescence.\n\nA 2013 pilot study from UCSF took 35 men with localized early-stage prostate cancer and had 10 of them begin \"lifestyle changes that included: a plant-based diet (high in fruits, vegetables and unrefined grains, and low in fat and refined carbohydrates); moderate exercise (walking 30 minutes a day, six days a week); stress reduction (gentle yoga-based stretching, breathing, meditation)\" and also \"weekly group support\". When compared to the other 25 study participants, \"The group that made the lifestyle changes experienced a 'significant' increase in telomere length of approximately 10 percent. Further, the more people changed their behavior by adhering to the recommended lifestyle program, the more dramatic their improvements in telomere length.\" A 2014 study entitled \"Stand up for health – avoiding sedentary behaviour might lengthen your telomeres: secondary outcomes from a physical activity RCT in older people\" indicated somewhat contradictory results, stating, \"In the intervention group, there was a negative correlation between changes in time spent exercising and changes in telomere length (rho=-0.39, p=0.07). On the other hand, in the intervention group, telomere lengthening was significantly associated with reduced sitting time (rho=-0.68, p=0.02).\"\n\nKnown, up-to-date telomere nucleotide sequences are listed in Telomerase Database website.\n\nTelomeres are critical for maintaining genomic integrity and studies show that telomere dysfunction or shortening is commonly acquired during the process of tumor development. Short telomeres can lead to genomic instability, chromosome loss and the formation of non-reciprocal translocations; and telomeres in tumor cells and their precursor lesions are significantly shorter than surrounding normal tissue.\n\nObservational studies have found shortened telomeres in many cancers: including pancreatic, bone, prostate, bladder, lung, kidney, and head and neck. In addition, people with many types of cancer have been found to possess shorter leukocyte telomeres than healthy controls. Recent meta-analyses suggest 1.4 to 3.0 fold increased risk of cancer for those with the shortest vs. longest telomeres. However the increase in risk varies by age, sex, tumor type and differences in lifestyle factors.\n\nSome of the same lifestyle factors which increase risk of developing cancer have also been associated with shortened telomeres: including stress, smoking, physical inactivity and diet high in refined sugars Diet and physical activity influence inflammation and oxidative stress. These factors are thought to influence telomere maintenance. Psychologic stress has also been linked to accelerated cell aging, as reflected by decreased telomerase activity and short telomeres. It has been suggested that a combination of lifestyle modifications, including healthy diet, exercise and stress reduction, have the potential to increase telomere length, reverse cellular aging, and reduce the risk for aging-related diseases. In a recent clinical trial for early-stage prostate cancer patients, comprehensive lifestyle changes resulted in a short-term increase in telomerase activity and long-term modification in telomere length. Lifestyle modifications have the potential to naturally regulate telomere maintenance without promoting tumorigenesis, as traditional mechanisms of telomere lengthening involve the use of telomerase activating agents.\n\nCancer cells require a mechanism to maintain their telomeric DNA in order to continue dividing indefinitely (immortalization). A mechanism for telomere elongation or maintenance is one of the key steps in cellular immortalization and can be used as a diagnostic marker in the clinic. Telomerase, the enzyme complex responsible for elongating telomeres through the addition of telomere repeats to the ends of chromosomes, is activated in approximately 80% of tumors. However, a sizeable fraction of cancerous cells employ alternative lengthening of telomeres (ALT), a non-conservative telomere lengthening pathway involving the transfer of telomere tandem repeats between sister-chromatids.\n\nTelomerase is the natural enzyme that promotes telomere lengthening. It is active in stem cells, germ cells, hair follicles, and 90 percent of cancer cells, but its expression is low or absent in somatic cells. Telomerase functions by adding bases to the ends of the telomeres. Cells with sufficient telomerase activity are considered immortal in the sense that they can divide past the Hayflick limit without entering senescence or apoptosis. For this reason, telomerase is viewed as a potential target for anti-cancer drugs (such as Geron's Imetelstat currently in human clinical trials and telomestatin).\n\nStudies using knockout mice have demonstrated that the role of telomeres in cancer can both be limiting to tumor growth, as well as promote tumorigenesis, depending on the cell type and genomic context.\n\nTelomerase is a \"ribonucleoprotein complex\" composed of a protein component and an RNA primer sequence that acts to protect the terminal ends of chromosomes from being broken down by enzymes. The telomeres (and the actions of telomerase) are necessary because, during replication, DNA polymerase can synthesize DNA in only a 5' to 3' direction (each DNA strand having a polarity that is determined by the precise manner in which sugar molecules of the strand's \"backbone\" are linked together) and can do so only by adding nucleotides to RNA primers (that have already been placed at various points along the length of the DNA). The RNA strands are replaced with newly synthesized DNA, but DNA polymerase can only \"backfill\" deoxyribonucleotides if there is already DNA \"upstream\" from (i.e., located 5' to) the RNA primer. At the chromosome terminal, however, there is no nucleotide sequence in the 5' direction (and therefore no upstream RNA primer or DNA), so DNA polymerase cannot function and genetic sequence might be lost through chromosomal fraying. Chromosomal ends might also be processed as breaks in double-strand DNA with chromosome-to-chromosome telomere fusions resulting.\n\nTelomeres at the end of DNA prevent the chromosome from growing shorter during replications (with loss of genetic information) by employing \"telomerases\" to synthesize DNA at the chromosome terminal. These include a protein subgroup of specialized reverse transcriptase enzymes known as TERT (\"te\"lomerase \"r\"everse \"t\"ranscriptases) and are involved in synthesis of telomeres in humans and many other, but not all, organisms. Because DNA replication mechanisms are affected by oxidative stress and because TERT expression is very low in most types of human cell, telomeres shorten every time a cell divides. Among cell types characterized by extensive cell division (such as stem cells and certain white blood cells), however, TERT is expressed at higher levels and telomere shortening is partially or fully prevented.\n\nIn addition to its TERT protein component, telomerase also contains a piece of template RNA known as the TERC (\"te\"lomerase \"R\"NA \"c\"omponent) or TR (\"t\"elomerase \"R\"NA). In humans, this TERC telomere sequence is a repeating string of TTAGGG, between 3 and 20 kilobases in length. There are an additional 100-300 kilobases of telomere-associated repeats between the telomere and the rest of the chromosome. Telomere sequences vary from species to species, but, in general, one strand is rich in G with fewer Cs. These G-rich sequences can form four-stranded structures (G-quadruplexes), with sets of four bases held in plane and then stacked on top of each other, with either a sodium or a potassium ion between the planar quadruplexes.\n\nMammalian (and other) somatic cells without telomerase gradually lose telomeric sequences as a result of incomplete replication (Counter \"et al.\", 1992). As mammalian telomeres shorten, eventually cells reach their replicative limit and progress into senescence or old age. Senescence involves p53 and pRb pathways and leads to the halting of cell proliferation (Campisi, 2005). Senescence may play an important role in suppression of cancer emergence, although inheriting shorter telomeres probably does not protect against cancer. With critically shortened telomeres, further cell proliferation can be achieved by inactivation of p53 and pRb pathways. Cells entering proliferation after inactivation of p53 and pRb pathways undergo crisis. Crisis is characterized by gross chromosomal rearrangements and genome instability, and almost all cells die.\n\nAbout 5–10% of human cancers activate the alternative lengthening of telomeres (ALT) pathway, which relies on recombination-mediated elongation.\nRarely, cells emerge from crisis immortalized through telomere lengthening by either activated telomerase or ALT (Colgina and Reddel, 1999; Reddel and Bryan, 2003). The first description of an ALT cell line demonstrated that their telomeres are highly heterogeneous in length and predicted a mechanism involving recombination (Murnane et al., 1994). Subsequent studies have confirmed a role for recombination in telomere maintenance by ALT (Dunham et al., 2000), however the exact mechanism of this pathway is yet to be determined. ALT cells produce abundant T-circles, possible products of intratelomeric recombination and T-loop resolution (Tomaska \"et al.\", 2000; 2009; Cesare and Griffith, 2004; Wang \"et al.\", 2004).\n\nSince shorter telomeres are thought by some to be a cause of aging, this raises the question of why longer telomeres are not selected for to ameliorate these effects. A prominent explanation suggests that inheriting longer telomeres would cause increased cancer rates (e.g. Weinstein and Ciszek, 2002). However, a recent literature review and analysis suggests this is unlikely, because shorter telomeres and telomerase inactivation is more often associated with increased cancer rates, and the mortality from cancer occurs late in life when the force of natural selection is very low. An alternative explanation to the hypothesis that long telomeres are selected against due to their cancer promoting effects is the \"thrifty telomere\" hypothesis, which suggests that the cellular proliferation effects of longer telomeres causes increased energy expenditures. In environments of energetic limitation, shorter telomeres might be an energy sparing mechanism.\n\nIn a healthy female breast, a proportion of cells called luminal progenitors that line the milk ducts have proliferative and differentiation potential and most of them contain critically short telomeres with DNA damage foci. These cells are believed to be the possible common cellular loci where cancers of the breast involving telomere dysregulation may arise. The telomere shortening in these progenitors is not age dependent but is speculated to be basal to luminal epithelial differentiation program-dependent. Also, the telomerase activity is unusually high in these cells when isolated from younger women, but declines with age.\n\nSeveral techniques are currently employed to assess average telomere length in eukaryotic cells. One method is the Terminal Restriction Fragment (TRF) southern blot, which involves hybridization of a radioactive 32P-(TTAGGG)n oligonucleotide probe to Hinf / Rsa I digested genomic DNA embedded on a nylon membrane and subsequently exposed to autoradiographic film or phosphoimager screen. Another histochemical method, termed Q-FISH, involves fluorescent in situ hybridization (FISH). Q-FISH, however, requires significant amounts of genomic DNA (2-20 micrograms) and labor that renders its use limited in large epidemiological studies. Some of these impediments have been overcome with a Real-Time PCR assay for telomere length and Flow-FISH. Real-time PCR assay involves determining the Telomere-to-Single Copy Gene (T/S)ratio, which is demonstrated to be proportional to the average telomere length in a cell.\n\nAnother technique, referred to as single telomere elongation length analysis (STELA), was developed in 2003 by Duncan Baird. This technique allows investigations that can target specific telomere ends, which is not possible with TRF analysis. However, due to this technique's being PCR-based, telomeres larger than 25Kb cannot be amplified and there is a bias towards shorter telomeres.\n\nWhile multiple companies offer telomere length measurement services, the utility of these measurements for widespread clinical or personal use has been questioned by prominent scientists without financial interests in these companies. Nobel Prize winner Elizabeth Blackburn, who was the co-founder of one of these companies and has prominently promoted the clinical utility of telomere length measures, resigned from the company in June 2013 \"owing to an impending change in the control of Telome Health\".\n\n\n\n", "id": "54888", "title": "Telomere"}
{"url": "https://en.wikipedia.org/wiki?curid=37139319", "text": "Shelterin\n\nShelterin (also called telosome) is a protein complex known to protect telomeres in many eukaryotes from DNA repair mechanisms, as well as regulate telomerase activity. In mammals and other eukaryotes, telomeric DNA consists of double- and single-stranded TTAGGG repeats and a single-stranded, G-rich overhang. Subunits of shelterin bind to these regions and induce the formation of a t-loop, a cap structure that deters DNA-damage-sensing machinery from mistakenly repairing telomeres. The absence of shelterin causes telomere uncapping and thereby activates damage-signaling pathways that may lead to non-homologous end joining (NHEJ), homology directed repair (HDR), senescence, or apoptosis.\n\nShelterin has six subunits: TRF1, TRF2, POT1, RAP1, TIN2, and TPP1. They can operate in smaller subsets to regulate the length of or protect telomeres.\n\nThere are two main DNA-damage-signaling pathways that shelterin represses: the ATR kinase pathway, blocked by POT1, and the ATM kinase pathway, blocked by TRF2. In the ATR kinase pathway, ATR and ATRIP sense the presence of single-stranded DNA and induce a phosphorylation cascade that leads to cell cycle arrest. To prevent this signal, POT1 \"shelters\" the single-stranded region of telomeric DNA. The ATM kinase pathway, which starts from ATM and other proteins sensing double strand breaks, similarly ends with cell cycle arrest. TRF2 may also hide the ends of telomeres, just as POT1 hides the single-stranded regions. Another theory proposes the blocking of the signal downstream. This will lead to a dynamic instability of the cells over time.\n\nThe structure of the t-loop may prevent NHEJ. For NHEJ to occur, the Ku heterodimer must be able to bind to the ends of the chromosome. Another theory offers the mechanism proposed earlier: TRF2 hides the ends of telomeres.\n\nAt least four factors contribute to telomere maintenance in most eurkaryotes: telomerase, shelterin, TERRA and the CST Complex. \nFission yeast (Schizosaccharomyces pombe) has a shelterin complex for protection and maintenance of telomeres, but in budding yeast (Saccharomyces cerevisiae) this function is performed by the CST Complex. For fission yeast, Rap1 and Pot1 are conserved, but Tpz1 is an ortholog of TPP1 and Taz1 is an ortholog of TRF1 and TRF2.\n\nPlants contain a variety of telomere-protecting proteins which can resemble either shelterin or the CST Complex.\n\nThe fruit fly Drosophila melanogaster lacks both shelterin and telomerase, but instead uses retrotransposons to maintain telomeres.\n\nTIN2 can localize to mitochondria where it promotes glycolysis.\n\nRAP1 regulates transcription and affects NF-κB signaling.\n\n", "id": "37139319", "title": "Shelterin"}
{"url": "https://en.wikipedia.org/wiki?curid=2024835", "text": "Nanodomain\n\nIn molecular biology, a nanodomain, in reference to calcium signaling, is where highly localized Ca signals are associated with a single (or very few at most) ion channel(s) that extend over a few tens of nm from the channel pore. Larger signalling distances from the pore (>100 nm) mediated by a larger number of channels, are referred to as microdomains. Nanodomains are related to, but not the same as coupling distance, which explicitly refers to the distance between the calcium-binding proteins (or \"sensors\") that are located within a few nanometers of an open calcium channel. The coupling distance is particularly important in the temporality of pre-synaptic signalling. If Ca influx is seen in a nanodomain then one would also expect to see tighter coupling distances in order to expose the calcium sensor to high enough concentrations of Ca to cause vesicular exocytosis of neurotransmitters.\n\nNanodomains are thought to improve the temporal precision of fast exocytosis of vesicles due to two specific properties:\n\nIt has been found that single channels are able to cause vesicular release, however, the cooperativity of different calcium channels is synapse specific. The release driven by a single Ca channel minimizes the total Ca influx, overlapping domains can provide greater reliability and temporal fidelity.\n", "id": "2024835", "title": "Nanodomain"}
{"url": "https://en.wikipedia.org/wiki?curid=46978792", "text": "Nucleosome Repeat Length\n\nNucleosome Repeat Length (NRL) is the average distance between the centers of neighboring nucleosomes. NRL is an important physical chromatin property that determines its biological function. NRL can be determined genome-wide for the chromatin in a given cell type and state, or locally for a large enough genomic region containing several nucleosomes.\n\nIn chromatin neighboring nucleosomes are separated by the linker DNA and in many cases also by the linker histone H1 as well as non-histone proteins. Since the size of the nucleosome is typically fixed (146-147 base pairs), NRL is mostly determined by the size of the linker region between nucleosomes. Alternatively, partial DNA unwrapping from the histone octamer or partial disassembly of the histone octamer can decrease the effective nucleosome size and thus affect NRL.\n\nPast studies going back to 1970s showed that, in general, NRL is different for different species and even for different cell types of the same organism. In addition, recent publications reported NRL variations for different genomic regions of the same cell type.\n", "id": "46978792", "title": "Nucleosome Repeat Length"}
{"url": "https://en.wikipedia.org/wiki?curid=3224696", "text": "Photodegradation\n\nPhotodegradation is the alteration of materials by light. Typically, the term refers to the combined action of sunlight and air. Photodegradation is usually oxidation and hydrolysis. Often photodegradation is avoided, since it destroys paintings and other artifacts. It is however partly responsible for remineralization of biomass and is used intentionally in some disinfection technologies. Photodegradation does not apply to how materials may be aged or degraded via infrared light or heat, but does include degradation in all of the ultraviolet light wavebands.\n\nThe protection of food from photodegradation is very important. Some nutrients, like beer for example, are affected by degradation when being exposed to sunlight. In the case of beer, the UV radiation causes a process, which entails the degradation of hop bitter compounds to 3-Methyl-2-buten-1-thiol and therefore changes the taste. As amber glass has the ability to absorb UV radiation, beer bottles are often made from this glass type to avoid this process.\n\nPaints, inks and dyes that are organic are more susceptible to photodegradation than those that are not. Ceramics are almost universally coloured with non-organic origin materials so as to allow the material to retain its colour even with the most relentless photo -degradation.\n\nThe photodegradation of pesticides is of great interest because of the scale of agriculture and the intensive use of chemicals. Pesticides are however selected in part not to photodegrade readily in sunlight in order to allow them to exert their biocidal activity. Thus, additional modalities are implemented to enhance their photodegradation, including the use of photosensitizers, photocatalysts (e.g., titanium dioxide), and the addition of reagents such as hydrogen peroxide that would generate hydroxyl radicals that would attack the pesticides.\n\nThe photodegradation of pharmaceuticals is of interest because they are found in many water supplies. They have deleterious effects on aquatic organisms including toxicity, endocrine disruption, genetic damage. But also in the primary packaging material the photodegradation of pharmaceuticals has to be prevented. For this, amber glasses like Fiolax amber and Corning 51-L are commonly used to protect the pharmaceutical from UV radiations. Iodine (in the form of Lugol's solution) and collodial silver are universally used in packaging that lets through very little UV light so as to avoid degradation.\n\nCommon synthetic polymers that can be attacked include polypropylene and LDPE, where tertiary carbon bonds in their chain structures are the centres of attack. Ultraviolet rays interact with these bonds to form free radicals, which then react further with oxygen in the atmosphere, producing carbonyl groups in the main chain. The exposed surfaces of products may then discolour and crack, and in extreme cases, complete product disintegration can occur. \n\nIn fibre products like rope used in outdoor applications, product life will be low because the outer fibres will be attacked first, and will easily be damaged by abrasion for example. Discolouration of the rope may also occur, thus giving an early warning of the problem. \n\nPolymers which possess UV-absorbing groups such as aromatic rings may also be sensitive to UV degradation. Aramid fibres like Kevlar, for example, are highly UV-sensitive and must be protected from the deleterious effects of sunlight.\n\nMany organic chemicals are thermodynamically unstable in the presence of oxygen, however, their rate of spontaneous oxidation is slow at room temperature. In the language of physical chemistry, such reactions are kinetically limited. This kinetic stability allows the accumulation of complex environmental structures in the environment. Upon the absorption of light, triplet oxygen converts to singlet oxygen, a highly reactive form of the gas, which effects spin-allowed oxidations. In the atmosphere, the organic compounds are degraded by hydroxyl radicals, which are produced from water and ozone.\n\nPhotochemical reactions are initiated by the absorption of a photon, typically in the wavelength range 290-700 nm (at the surface of the Earth). The energy of an absorbed photon is transferred to electrons in the molecule and briefly changes their configuration (i.e., promotes the molecule from a ground state to an excited state). The excited state represents what is essentially a new molecule. Often excited state molecules are not kinetically stable in the presence of O or HO and can spontaneously decompose (oxidize or hydrolyze). Sometimes molecules decompose to produce high energy, unstable fragments that can react with other molecules around them. The two processes are collectively referred to as direct photolysis or indirect photolysis, and both mechanisms contribute to the removal of pollutants.\n\nThe United States federal standard for testing plastic for photo-degradation is 40 CFR Ch. I (7–1–03 Edition)PART 238\n\nPhotodegradation of plastics and other materials can be inhibited with additives, which are widely used. These additives include antioxidants, which interrupt degradation processes. Typical antioxidants are derivatives of aniline. Another type of additive are UV-absorbers. These agents capture the photon and convert it to heat. Typical UV-absorbers are hydroxy-substituted benzophenones, related to the chemicals used in sunscreen.\n\n\n", "id": "3224696", "title": "Photodegradation"}
{"url": "https://en.wikipedia.org/wiki?curid=262401", "text": "Translation (biology)\n\nIn molecular biology and genetics, translation is the process in which ribosomes in the cytoplasm or ER synthesize proteins after the process transcription of DNA to RNA in the cell's nucleus. The entire process is called gene expression.\n\nIn translation, messenger RNA (mRNA) is decoded in a ribosome, outside the nucleus, to produce a specific amino acid chain, or polypeptide. The polypeptide later folds into an active protein and performs its functions in the cell. The ribosome facilitates decoding by inducing the binding of complementary tRNA anticodon sequences to mRNA codons. The tRNAs carry specific amino acids that are chained together into a polypeptide as the mRNA passes through and is \"read\" by the ribosome.\n\nTranslation proceeds in three phases: \n\nIn prokaryotes (bacteria), translation occurs in the cytoplasm, where the large and small subunits of the ribosome bind to the mRNA. In eukaryotes, translation occurs in the cytosol or across the membrane of the endoplasmic reticulum in process called co-tanslational translocation. In co-translational translocation, the entire ribosome/mRNA complex binds to the outer membrane of the rough endoplasmic reticulum (ER) and the new protein is synthesized and released into the ER; the newly created polypeptide can be stored inside the ER for future vesicle transport and secretion outside the cell, or immediately secreted.\n\nMany types of transcribed RNA, such as transfer RNA, ribosomal RNA, and small nuclear RNA, do not undergo translation into proteins.\n\nA number of antibiotics act by inhibiting translation. These include anisomycin, cycloheximide, chloramphenicol, tetracycline, streptomycin, erythromycin, and puromycin. Prokaryotic ribosomes have a different structure from that of eukaryotic ribosomes, and thus antibiotics can specifically target bacterial infections without any harm to a eukaryotic host's cells.\n\nThe basic process of protein production is addition of one amino acid at a time to the end of a protein. This operation is performed by a ribosome. A ribosome is made up of two subunits, a small subunit and a large subunit. these subunits come together before translation of mRNA into a protein to provide a location for translation to be carried out and a polypeptide to be produced. The choice of amino acid type to add is determined by an mRNA molecule. Each amino acid added is matched to a three nucleotide subsequence of the mRNA. For each such triplet possible, the corresponding amino acid is accepted. The successive amino acids added to the chain are matched to successive nucleotide triplets in the mRNA. In this way the sequence of nucleotides in the template mRNA chain determines the sequence of amino acids in the generated amino acid chain.\nAddition of an amino acid occurs at the C-terminus of the peptide and thus translation is said to be amino-to-carboxyl directed.\n\nThe mRNA carries genetic information encoded as a ribonucleotide sequence from the chromosomes to the ribosomes. The ribonucleotides are \"read\" by translational machinery in a sequence of nucleotide triplets called codons. Each of those triplets codes for a specific amino acid.\n\nThe ribosome molecules translate this code to a specific sequence of amino acids. The ribosome is a multisubunit structure containing rRNA and proteins. It is the \"factory\" where amino acids are assembled into proteins.\ntRNAs are small noncoding RNA chains (74-93 nucleotides) that transport amino acids to the ribosome. tRNAs have a site for amino acid attachment, and a site called an anticodon. The anticodon is an RNA triplet complementary to the mRNA triplet that codes for their cargo amino acid.\n\nAminoacyl tRNA synthetases (enzymes) catalyze the bonding between specific tRNAs and the amino acids that their anticodon sequences call for. The product of this reaction is an aminoacyl-tRNA. This aminoacyl-tRNA is carried to the ribosome by EF-Tu, where mRNA codons are matched through complementary base pairing to specific tRNA anticodons. Aminoacyl-tRNA synthetases that mispair tRNAs with the wrong amino acids can produce mischarged aminoacyl-tRNAs, which can result in inappropriate amino acids at the respective position in protein. This \"mistranslation\" of the genetic code naturally occurs at low levels in most organisms, but certain cellular environments cause an increase in permissive mRNA decoding, sometimes to the benefit of the cell.\n\nThe ribosome has three sites for tRNA to bind. They are the aminoacyl site (abbreviated A), the peptidyl site (abbreviated P) and the exit site (abbreviated E). With respect to the mRNA, the three sites are oriented 5’ to 3’ E-P-A, because ribosomes move toward the 3' end of mRNA. The A-site binds the incoming tRNA with the complementary codon on the mRNA. The P-site holds the tRNA with the growing polypeptide chain. The E-site holds the tRNA without its amino acid. When an aminoacyl-tRNA initially binds to its corresponding codon on the mRNA, it is in the A site. Then, a peptide bond forms between the amino acid of the tRNA in the A site and the amino acid of the charged tRNA in the P site. The growing polypeptide chain is transferred to the tRNA in the A site. Translocation occurs, moving the tRNA in the P site, now without an amino acid, to the E site; the tRNA that was in the A site, now charged with the polypeptide chain, is moved to the P site. The tRNA in the E site leaves and another aminoacyl-tRNA enters the A site to repeat the process.\n\nAfter the new amino acid is added to the chain, and after the mRNA is released out of the nucleus and into the ribosome's core, the energy provided by the hydrolysis of a GTP bound to the translocase EF-G (in prokaryotes) and eEF-2 (in eukaryotes) moves the ribosome down one codon towards the 3' end. The energy required for translation of proteins is significant. For a protein containing \"n\" amino acids, the number of high-energy phosphate bonds required to translate it is 4\"n\"-1 . The rate of translation varies; it is significantly higher in prokaryotic cells (up to 17-21 amino acid residues per second) than in eukaryotic cells (up to 6-9 amino acid residues per second).\n\nEven though the ribosomes are usually considered accurate and processive machines, the translation process is subject to errors that can lead either to the synthesis of erroneous proteins or to the premature abandonment of translation. The rate of error in synthesizing proteins has been estimated to be between 1/10 and 1/10 misincorporated amino acids, depending on the experimental conditions. The rate of premature translation abandonment, instead, has been estimated to be of the order of magnitude of 10 events per translated codon.\nThe correct amino acid is covalently bonded to the correct transfer RNA (tRNA) by amino acyl transferases. The amino acid is joined by its carboxyl group to the 3' OH of the tRNA by an ester bond. When the tRNA has an amino acid linked to it, the tRNA is termed \"charged\". Initiation involves the small subunit of the ribosome binding to the 5' end of mRNA with the help of initiation factors (IF). Termination of the polypeptide happens when the A site of the ribosome faces a stop codon (UAA, UAG, or UGA) on the mRNA. tRNA usually cannot recognize or bind to stop codons. Instead, the stop codon induces the binding of a release factor protein that prompts the disassembly of the entire ribosome/mRNA complex and the hydrolysis and the release of the polypeptide chain from the ribosome. Drugs or special sequence motifs on the mRNA can change the ribosomal structure so that near-cognate tRNAs are bound to the stop codon instead of the release factors. In such cases of 'translational readthrough', translation continues until the ribosome encounters the next stop codon.\n\nThe process of translation is highly regulated in both eukaryotic and prokaryotic organisms. Regulation of translation can impact the global rate of protein synthesis which is closely coupled to the metabolic and proliferative state of a cell. In addition, recent work has revealed that genetic differences and their subsequent expression as mRNAs can also impact translation rate in an RNA-specific manner.\n\nWhereas other aspects such as the 3D structure, called tertiary structure, of protein can only be predicted using sophisticated algorithms, the amino acid sequence, called primary structure, can be determined solely from the nucleic acid sequence with the aid of a translation table.\n\nThis approach may not give the correct amino acid composition of the protein, in particular if unconventional amino acids such as selenocysteine are incorporated into the protein, which is coded for by a conventional stop codon in combination with a downstream hairpin (SElenoCysteine Insertion Sequence, or SECIS).\n\nThere are many computer programs capable of translating a DNA/RNA sequence into a protein sequence. Normally this is performed using the Standard Genetic Code, however, few programs can handle all the \"special\" cases, such as the use of the alternative initiation codons. For instance, the rare alternative start codon CTG codes for Methionine when used as a start codon, and for Leucine in all other positions.\n\nExample: Condensed translation table for the Standard Genetic Code (from the NCBI Taxonomy webpage).\n\nEven when working with ordinary eukaryotic sequences such as the Yeast genome, it is often desired to be able to use alternative translation tables—namely for translation of the mitochondrial genes. Currently the following translation tables are defined by the NCBI Taxonomy Group for the translation of the sequences in GenBank:\n\n\n\n\n", "id": "262401", "title": "Translation (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=47145502", "text": "Lattice model (biophysics)\n\nLattice models in biophysics represent a class of statistical-mechanical models which consider a biological macromacromolecule (such as DNA, protein, actin, etc.) as a lattice of units, each unit being in different states or conformations.\n\nFor example, DNA in chromatin can be represented as a one-dimensional lattice, whose elementary units are the nucleotide, base pair or nucleosome. Different states of the unit can be realized either by chemical modifications (e.g. DNA methylation or modifications of DNA-bound histones), or due to quantized internal degrees of freedom (e.g. different angles of the bond joining two neighboring units), or due to binding events involving a given unit (e.g. reversible binding of small ligands or proteins to DNA, or binding/unbinding of two complementary nucleotides in the DNA base pair).\n\n", "id": "47145502", "title": "Lattice model (biophysics)"}
{"url": "https://en.wikipedia.org/wiki?curid=47149841", "text": "Plaque hybridization\n\nPlaque hybridization is a technique used in Molecular biology for the identification of recombinant phages.\nThe procedure can also be used for the detection of differentially represented repetitive DNA. \nThe technique (similar to colony hybridization) involves hybridizing isolated phage DNA to a label probe for the gene of study. This is followed by autoradiography to detect the position of the label.\nThe plaque hybridization procedure has some advantages over colony hybridization due to the smaller and well defined area of the filter to which the DNA binds.\n", "id": "47149841", "title": "Plaque hybridization"}
{"url": "https://en.wikipedia.org/wiki?curid=42804273", "text": "Ribosomally synthesized and post-translationally modified peptides\n\nRibosomally synthesized and post-translationally modified peptides (RiPPs), also known as ribosomal natural products, are a diverse class of natural products of ribosomal origin. Consisting of more than 20 sub-classes, RiPPs are produced by a variety of organisms, including prokaryotes, eukaryotes, and archaea, and they possess a wide range of biological functions.\n\nAs a consequence of the falling cost of genome sequencing and the accompanying rise in available genomic data, scientific interest in RiPPs has increased in the last few decades. Because the chemical structures of RiPPs are more closely predictable from genomic data than are other natural products (e.g. alkaloids, terpenoids), their presence in sequenced organisms can, in theory, be identified rapidly. This makes RiPPs an attractive target of modern natural product discovery efforts.\n\nRiPPs consist of any peptides (i.e. molecular weight below 10 kDa) that are ribosomally-produced and undergo some degree of enzymatic post-translational modification (i.e. chemical transformations occurring after translation). This combination of peptide translation and modification is referred to as “post-ribosomal peptide synthesis” (PRPS) in analogy with nonribosomal peptide synthesis (NRPS).\n\nHistorically, the current sub-classes of RiPPs were studied individually, and common practices in nomenclature varied accordingly in the literature. More recently, with the advent of broad genome sequencing, it has been realized that these natural products share a common biosynthetic origin. In 2013, a set of uniform nomenclature guidelines were agreed upon and published by a large group of researchers in the field. Prior to this report, RiPPs were referred to by a variety of designations, including \"post-ribosomal peptides\", \"ribosomal natural products\", and \"ribosomal peptides\".\n\nThe acronym \"RiPP\" stands for \"ribosomally synthesized and post-translationally modified peptide\".\n\nRiPPs constitute one of the major superfamilies of natural products, like alkaloids, terpenoids, and nonribosomal peptides, although they tend to be large, with molecular weights commonly in excess of 1000 Da. The advent of next-generation sequencing methods has made genome mining of RiPPs a common strategy. In part due to their increased discovery and hypothesized ease of engineering, the use of RiPPs as drugs is increasing. Although they are ribosomal peptides in origin, RiPPs are typically categorized as small molecules rather than biologics due to their chemical properties, such as moderate molecular weight and relatively high hydrophobicity.\n\nThe uses and biological activities of RiPPs are diverse.\n\nRiPPs in commercial use include nisin, a food preservative, thiostrepton, a veterinary topical antibiotic, and and duramycin, which are animal feed additives. Phalloidin functionalized with a fluorophore is used in microscopy as a stain due to its high affinity for actin. Anantin is a RiPP used in cell biology as an atrial natriuretic peptide receptor inhibitor.\n\nA derivatized RiPP in clinical trials is LFF571. LFF571, a derivative of the thiopeptide GE2270-A, completed phase II clinical trials for the treatment of \"Clostridium difficile\" infections, with comparable safety and efficacy to vancomycin. Also recently in clinical trials was the NVB302 (a derivative of the lantibiotic actagardine) which is used for the treatment of \"Clostridium difficile\" infection. Duramycin has completed phase II clinical trials for the treatment of cystic fibrosis.\n\nOther bioactive RiPPs include the antibiotics cyclothiazomycin and bottromycin, the ultra-narrow spectrum antibiotic plantazolicin, and the cytotoxin patellamide A. Streptolysin S, the toxic virulence factor of \"Streptococcus pyogenes\", is also a RiPP. Additionally, human thyroid hormone itself is a RiPP due to its biosynthetic origin as thyroglobulin.\n\nAmatoxins and phallotoxins are 8- and 7-membered natural products, respectively, characterized by N-to-C cyclization in addition to a tryptathionine motif derived from the crosslinking of Cys and Trp. Interestingly, the amatoxins and phallotoxins also differ from other RiPPs based on the presence of a C-terminal recognition sequence in addition to the N-terminal leader peptide. α-Amanitin, an amatoxin, has a number of posttranslational modifications in addition to macrocyclization and formation of the tryptathionine bridge: oxidation of the tryptathionine leads to the presence of a sulfoxide, and numerous hydroxylations decorate the natural product. As an amatoxin, α-amanitin is an inhibitor of RNA polymerase II.\n\nBottromycins contain a C-terminal decarboxylated thiazole in addition to a macrocyclic amidine.\n\nThere are currently six known bottromycin compounds, which differ in the extent of side chain methylation, an additional characteristic of the bottromycin class. The total synthesis of bottromycin A2 was required to definitively determine the structure of the first bottromycin.\n\nThus far, gene clusters predicted to produce bottromycins have been identified in the \"Streptomyces\" genus. Interestingly, bottromycins differ from other RiPPs in that there is no N-terminal leader peptide. Rather, the precursor peptide has a C-terminal extension of 35-37 amino acids, hypothesized to act as a recognition sequence for posttranslational machinery.\n\nCyanobactins are diverse metabolites from cyanobacteria with N-to-C macrocylization of a 6-20 amino acid chain. Cyanobactins are natural products isolated from cyanobacteria, and close to 30% of all cyanobacterial strains are thought to contain cyanobacterial gene clusters. However, while thus far all cyanobactins are credited to cyanobacteria, there exists the possibility that other organisms could produce similar natural products.\n\nInterestingly, the precursor peptide of the cyanobactin family is traditionally designated the \"E\" gene, whereas precursor peptides are designated gene \"A\" in most RiPP gene clusters. \"A\" is a serine protease involved in cleavage of the leader peptide and subsequent macrocyclization of the peptide natural product, in combination with an additional serine protease homologue, the encoded by gene \"G.\" Members of the cyanobactin family may bear thiazolines/oxazolines, thiazoles/oxazoles, and methylations depending on additional modification enzymes. For example, perhaps the most famous cyanobactin is patellamide A, which contains two thiazoles, a methyloxazoline, and an oxazoline in its final state, a macrocycle derived from 8 amino acids.\n\n Lanthipeptides are one of the most well-studied families of RiPPs. The family is characterized by the presence of lanthionine (Lan) and 3-methyllanthionine (MeLan) residues in the final natural product. There are four major classes of lanthipeptides, delineated by the enzymes responsible for installation of Lan and MeLan. The dehydratase and cyclase can be two separate proteins or one multifunctional enzyme. Previously, lanthipeptides were known as \"lantipeptides\" before a consensus was reached in the field.\n\nLantibiotics are lanthipeptides that have known antimicrobial activity. The founding member of the lanthipeptide family, nisin, is a lantibiotic that has been used to prevent the growth of food-born pathogens for over 40 years.\n\nLasso peptides are short peptides containing an N-terminal macrolactam macrocycle \"ring\" through which a linear C-terminal \"tail\" is threaded. Because of this threaded-loop topology, these peptides resemble lassos, giving rise to their name. Additionally, lasso peptides are formally rotaxanes.\n\nThe N-terminal \"ring\" can be from 7 to 9 amino acids long and is formed by an isopeptide bond between the N-terminal amine of the first amino acid of the peptide and the carboxylate side chain of an aspartate or glutamate residue. The C-terminal \"tail\" ranges from 7 to 15 amino acids in length.\n\nInterestingly, the first amino acid of lasso peptides is almost invariably glycine or cysteine, with mutations at this site not being tolerated by known enzymes. Thus, bioinformatics-based approaches to lasso peptide discovery have thus used this as a constraint. However, some lasso peptides were recently discovered that also contain serine or alanine as their first residue.\n\nThe threading of the lasso tail is trapped either by disulfide bonds between ring and tail cysteine residues (class I lasso peptides), by steric effects due to bulky residues on the tail (class II lasso peptides), or both (class III lasso peptides). The compact structure makes lasso peptides frequently resistant to proteases or thermal unfolding.\n\nLinear azole(in)e-containing peptides (LAPs) contain thiazoles and oxazoles, or their reduced thiazoline and oxazoline forms. Thiazol(in)es are the result of cyclization of Cys residues in the precursor peptide, while (methyl)oxazol(in)es are formed from Thr and Ser. Azole and azoline formation also modifies the residue in the -1 position, or directly \"C\"-terminal to the Cys, Ser, or Thr. A dehydrogenase in the LAP gene cluster is required for oxidation of azolines to azoles.\n\nPlantazolicin is a LAP with extensive cyclization. Two sets of five heterocycles endow the natural product with structural rigidity and unusually selective antibacterial activity. Streptolysin S (SLS) is perhaps the most well-studied and most famous LAP, in part because the structure is still unknown since the discovery of SLS in 1901. Thus, while the biosynthetic gene cluster suggests SLS is a LAP, structural confirmation is lacking.\n\nMicrocins are all RiPPs produced by Enterobacteriaceae with a molecular weight <10 kDa. Many members of other RiPP families, such as microcin B17 (LAP) and microcin J25 (Lasso peptide) are also considered microcins. Instead of being classified based on posttranslational modifications or modifying enzymes, microcins are instead identified by molecular weight, native producer, and antibacterial activity. Microcins are either plasmid- or chromosome-encoded, but specifically have activity against Enerobacteriaceae. Because these organisms are also often producers of microcins, the gene cluster contains not only a precursor peptide and modification enzymes, but also a self-immunity gene to protect the producing strain, and genes encoding export of the natural product.\n\nMicrocins have bioactivity against Gram-negative bacteria but usually display narrow-spectrum activity due to hijacking of specific receptors involved in the transport of essential nutrients.\n\nMost of the characterized thiopeptides have been isolated from Actinobacteria. General structural features of thiopeptide macrocycles, are dehydrated amino acids and thiazole rings formed from dehydrated serine/threonine and cyclized cysteine residues, respectively\n\nThe thiopeptide macrocycle is closed with a six-membered nitrogen-bearing ring. Oxidation state and substitution pattern of the nitrogenous ring determines the series of the thiopeptide natural product. While the mechanism of macrocyclization is not known, the nitrogenous ring can exist in thiopeptides as a piperidine, dehydropiperidine, or a fully oxidized pyridine. Additionally, some thiopeptides bear a second macrocycle, which bears a quinaldic acid or indolic acid residue derived from tryptophan. Perhaps the most well-characterized thiopeptide, thiostrepton A, contains a dehydropiperidine ring and a second, quinaldic acid-containing macrocycle. Four residues are dehydrated during posttranslational modification, and the final natural product also bears four thiazoles and one azoline.\n\n\"Autoinducing Peptides\" (AIPs) and quorum sensing peptides are used as signaling molecules in the process called quorum sensing. AIPs are characterized by the presence of a cyclic ester or thioester, unlike other regulatory peptides that are linear. In pathogens, exported AIPs bind to extracellular receptors that trigger the production of virulence factors. In \"Staphylococcus aureus\", AIPs are biosynthesized from a precursor peptide composed of a C-terminal leader region, the core region, and negatively charged tail region that is, along with the leader peptide, cleaved before AIP export.\n\n\"Bacterial Head-to-Tail Cyclized Peptides\" refers exclusively to ribosomally synthesized peptides with 35-70 residues and a peptide bond between the N- and C-termini, sometimes referred to as bacteriocins, although this term is used more broadly. The distinctive nature of this class is not only the relatively large size of the natural products but also the modifying enzymes responsible for macrocyclization. Other N-to-C cyclized RiPPs, such as the cyanobactins and orbitides, have specialized biosynthetic machinery for macrocylization of much smaller core peptides. Thus far, these bacteriocins have been identified only in Gram-positive bacteria. Enterocin AS-48 was isolated from \"Enterococcus\" and, like other bacteriocins, is relatively resistant to high temperature, pH changes, and many proteases as a result of macrocyclization. Based on solution structures and sequence alignments, bacteriocins appear to take on similar 3D structures despite little sequence homology, contributing to stability and resistance to degradation.\n\n\"Conopeptides\" and other toxoglossan peptides are the components of the venom of predatory marine snails, such as the cone snails or \"Conus\". Venom peptides from cone snails are generally smaller than those found in other animal venoms (10-30 amino acids vs. 30-90 amino acids) and have more disulfide crosslinks. A single species may have 50-200 conopeptides encoded in its genome, recognizable by a well-conserved signal sequence.\n\n\"Cyclotides\" are RiPPs with a head-to-tail cyclization and three conserved disulfide bonds that form a knotted structure called a cyclic cysteine knot motif. No other posttranslational modifications have been observed on the characterized cyclotides, which are between 28 - 37 amino acids in size. Cyclotides are plant natural products and the different cyclotides appear to be species-specific. While many activities have been reported for cyclotides, it has been hypothesized that all are united by a common mechanism of binding to and disrupting the cell membrane.\n\n\"Glycocins\" are RiPPS that are glycosylated antimicrobial peptides. Only two members have been fully characterized, making this a small RiPP class. Sublancin 168 and glycocin F are both Cys-glycosylated and, in addition, have disulfide bonds between non-glycosylated Cys residues. While both members bear S-glycosyl groups, RiPPs bearing O- or N-linked carbohydrates will also be included in this family as they are discovered.\n\n\"Linaridins\" are characterized by C-terminal aminovinyl cysteine residues. While this posttranslational modification is also seen in the lanthipeptides epidermin and mersacidin, linaridins do not have Lan or MeLan residues. In addition, the linaridin moiety is formed from modification of two Cys residues, whereas lanthipeptide aminovinyl cysteines are formed from Cys and dehydroalanine (Dha). The first linaridin to be characterized was cypemycin.\n\n\"Microviridins\" are cyclic \"N\"-acetylated trideca- and tetradecapeptides with ω-ester and/or ω-amide bonds. Lactone formation through glutamate or aspartate ω-carboxy groups and the lysine ε-amino group forms macrocycles in the final natural product.\n\n\"Orbitides\" are plant-derived N-to-C cyclized peptides with no disulfide bonds. Also referred to as Caryophyllaceae-like homomonocyclopeptides, orbitides are 5-12 amino acids in length and are composed of mainly hydrophobic residues. Similar to the amatoxins and phallotoxins, the gene sequences of orbitides suggest the presence of a C-terminal recognition sequence. In the flaxseed variety \"Linum usitatissimum\", a precursor peptide was found using Blast searching that potentially contains five core peptides separated by putative recognition sequences.\n\n\"Proteusins\", also called polytheonamides, were originally presumed to be nonribosomal natural products due to the presence of many D-amino acids and other non-proteinogenic amino acids. However, a metagenomic study revealed the natural products as the most extensively modified class of RiPPs known to date. Six enzymes are responsible for installing a total of 48 posttranslational modifications onto the polytheonamide A and B precursor peptides, including 18 epimerizations. Proteusins are exceptionally large, as a single molecule is able to span a cell membrane and form an ion channel.\n\n\"Sactipeptides\" contain intramolecular linkages between the sulfur of Cys residues and the α-carbon of another residue in the peptide. A number of nonribosomal peptides bear the same modification. In 2003, the first RiPP with a sulfur-to-α-carbon linkage was reported when the structure of subtilosin A was determined using isotopically enriched media and NMR spectroscopy. In the case of subtilosin A, isolated from Bacillus subtilis 168, the Cα crosslinks between Cys4 and Phe31, Cys7 and Thr28, and Cys13 and Phe22 are not the only posttranslational modifications; the C- and N-termini form an amide bond, resulting in a circular structure that is conformationally restricted by the Cα bonds. Sactipeptides with antimicrobial activity are commonly referred to as sactibiotics (\"s\"ulfur to \"a\"lpha-\"c\"arbon an\"tibiotic\").\n\nRiPPs are characterized by a common biosynthetic strategy wherein genetically-encoded peptides undergo translation and subsequent chemical modification by biosynthetic enzymes.\n\nAll RiPPs are synthesized first at the ribosome as a precursor peptide. This peptide consists of a core peptide segment which is typically preceded (and occasionally followed) by a leader peptide segment and is typically ~20-110 residues long. The leader peptide is usually important for enabling enzymatic processing of the precursor peptide via aiding in recognition of the core peptide by biosynthetic enzymes and for cellular export. Some RiPPs also contain a recognition sequence C-terminal to the core peptide; these are involved in excision and cyclization. Additionally, eukaryotic RiPPs may contain a signal segment of the precursor peptide which helps direct the peptide to cellular compartments.\n\nDuring RiPP biosynthesis, the unmodified precursor peptide (containing an unmodified core peptide, UCP) is recognized and chemically modified sequentially by biosynthetic enzymes (PRPS). Examples of modifications include dehydration (i.e. lanthipeptides, thiopeptides), cyclodehydration (i.e. thiopeptides), prenylation (i.e. cyanobactins), and cyclization (i.e. lasso peptides), among others. The resulting modified precursor peptide (containing a modified core peptide, MCP) then undergoes proteolysis, wherein the non-core regions of the precursor peptide are removed. This results in the mature RiPP.\n\nPapers published prior to a recent community consensus employ differing sets of nomenclature. The precursor peptide has been referred to previously as \"prepeptide\", \"prepropeptide\", or \"structural peptide\". The leader peptide has been referred to as a \"propeptide\", \"pro-region\", or \"intervening region\". Historical alternate terms for core peptide included \"propeptide\", \"structural peptide\", and \"toxin region\" (for conopeptides, specifically).\n\nLanthipeptides are characterized by the presence lanthionine (Lan) and 3-methyllanthionine (MeLan) residues. Lan residues are formed from a thioether bridge between Cys and Ser, while MeLan residues are formed from the linkage of Cys to a Thr residue. The biosynthetic enzymes responsible for Lan and MeLan installation first dehydrate Ser and Thr to dehydroalanine (Dha) and dehydrobutyrine (Dhb), respectively. Subsequent thioether crosslinking occurs through a Michael-type addition by Cys onto Dha or Dhb.\n\nFour classes of lanthipeptide biosynthetic enzymes have been designated. Class I lanthipeptides have dedicated lanthipeptide dehydratases, called LanB enzymes, though more specific designations are used for particular lanthipeptides (e.g. NisB is the nisin dehydratase). A separate cyclase, LanC, is responsible for the second step in Lan and MeLan biosynthesis. However, class II, III, and IV lanthipeptides have bifunctional lanthionine synthetases in their gene clusters, meaning a single enzyme carries out both dehydration and cyclization steps. Class II synthetases, designated LanM synthetases, have N-terminal dehydration domains with no sequence homology to other lanthipeptide biosynthetic enzymes; the cyclase domain has homology to LanC. Class III (LanKC) and IV (LanL) enzymes have similar N-terminal lyase and central kinase domains, but diverge in C-terminal cyclization domains: the LanL cyclase domain is homologous to LanC, but the class III enzymes lack Zn-ligand binding domains.\n\nThe hallmark of linear azol(in)e-containing peptide (LAP) biosynthesis is the formation of azol(in)e heterocycles from the nucleophilic amino acids serine, threonine, or cysteine. This is accomplished by three enzymes referred to as the B, C, and D proteins; the precursor peptide is referred to as the A protein, as in other classes.\n\nThe C protein is mainly involved in leader peptide recognition and binding and is sometimes called a scaffolding protein. The D protein is an ATP-dependent cyclodehydratase that catalyzes the cyclodehydration reaction, resulting in formation of an azoline ring. This occurs by direct activation of the amide backbone carbonyl with ATP, resulting in stoichiometric ATP consumption. The C and D proteins are occasionally present as a single, fused protein, as is the case for trunkamide biosynthesis. The B protein is a flavin mononucleotide (FMN)-dependent dehydrogenase which oxidizes certain azoline rings into azoles.\n\nThe B protein is typically referred to as the dehydrogenase; the C and D proteins together form the cyclodehydratase, although the D protein alone performs the cyclodehydration reaction. Early work on microcin B17 adopted a different nomenclature for these proteins, but a recent consensus has been adopted by the field as described above.\n\nCyanobactin biosynthesis requires proteolytic cleavage of both N-terminal and C-terminal portions of the precursor peptide. The defining proteins are thus an N-terminal protease, referred to as the A protein, and a C-terminal protease, referred to as the G protein. The G protein is also responsible for macrocyclization.\n\nFor cyanobactins, the precursor peptide is referred to as the E peptide. Minimally, the E peptide requires a leader peptide region, a core (structural) region, and both N-terminal and C-terminal protease recognition sequences. In contrast to most RiPPs, for which a single precursor peptide encodes a single natural product via a lone core peptide, cyanobactin E peptides can contain multiple core regions; multiple E peptides can even be present in a single gene cluster.\n\nMany cyanobactins also undergo heterocyclization by a heterocyclase (referred to as the D protein), installing oxazoline or thiazoline moieties from Ser/Thr/Cys residues prior to the action of the A and G proteases. The heterocyclase is an ATP-dependent YcaO homologue that behaves biochemically in the same manner as YcaO-domain cyclodehydratases in thiopeptide and linear azol(in)e-containing peptide (LAP) biosynthesis (described above).\n\nA common modification is prenylation of hydroxyl groups by an F protein prenyltransferase. Oxidation of azoline heterocycles to azoles can also be accomplished by an oxidase domain located on the G protein. Unusual for ribosomal peptides, cyanobactins can include D-amino acids; these can occur adjacent to azole or azoline residues. The functions of some proteins found commonly in cyanobactin biosynthetic gene clusters, the B and C proteins, are unknown.\n\nThiopeptide biosynthesis involves particularly extensive modification of the core peptide scaffold. Indeed, due to the highly complex structures of thiopeptides, it was commonly thought that these natural products were nonribosomal peptides. Recognition of the ribosomal origin of these molecules came in 2009 with the independent discovery of the gene clusters for several thiopeptides.\n\nThe standard nomenclature for thiopeptide biosynthetic proteins follows that of the thiomuracin gene cluster. In addition to the precursor peptide, referred to as the A peptide, thiopeptide biosynthesis requires at least six genes. These include lanthipeptide-like dehydratases, designated the B and C proteins, which install dehydroalanine and dehydrobutyrine moieties by dehydrating Ser/Thr precursor residues. Azole and azoline synthesis is effected by the E protein, the dehydrogenase, and the G protein, the cyclodehydratase. The nitrogen-containing heterocycle is installed by the D protein cyclase via a putative [4+2] cycloaddition of dehydroalanine moieties to form the characteristic macrocycle. The F protein is responsible for binding of the leader peptide.\n\nThiopeptide biosynthesis is biochemically similar to that of cyanobactins, lanthipeptides, and linear azol(in)e-containing peptides (LAPs). As with cyanobactins and LAPs, azole and azoline synthesis occurs via the action of an ATP-dependent YcaO-domain cyclodehydratase. In contrast to LAPs, where cyclodehydration occurs via the action of two distinct proteins responsible for leader peptide binding and cyclodehydrative catalysis, these are fused into a single protein (G protein) in cyanobactin and thiopeptide biosynthesis. However, in thiopeptides, an additional protein, designated the Ocin-ThiF-like protein (F protein) is necessary for leader peptide recognition and potentially recruiting other biosynthetic enzymes.\n\nLasso peptide biosynthesis requires at least three genes, referred to as the A, B, and C proteins. The A gene encodes the precursor peptide, which is modified by the B and C proteins into the mature natural product. The B protein is an adenosine triphosphate-dependent cysteine protease that cleaves the leader region from the precursor peptide. The C protein displays homology to asparagine synthetase and is thought to activate the carboxylic acid side chain of a glutamate or aspartate residue via adenylylation. The N-terminal amine formed by the B protein (protease) then reacts with this activated side chain to form the macrocycle-forming isopeptide bond. The exact steps and reaction intermediates in lasso peptide biosynthesis remain unknown due to experimental difficulties associated with the proteins. Commonly, the B protein is referred to as the lasso protease, and the C protein is referred to as the lasso cyclase.\n\nSome lasso peptide biosynthetic gene clusters also require an additional protein of unknown function for biosynthesis. Additionally, lasso peptide gene clusters usually include an ABC transporter (D protein) or an isopeptidase, although these are not strictly required for lasso peptide biosynthesis and are sometimes absent. No X-ray crystal structure is yet known for any lasso peptide biosynthetic protein.\n\nThe biosynthesis of lasso peptides is particularly interesting due to the inaccessibility of the threaded-lasso topology to chemical peptide synthesis.\n\n", "id": "42804273", "title": "Ribosomally synthesized and post-translationally modified peptides"}
{"url": "https://en.wikipedia.org/wiki?curid=47515694", "text": "Chromosome territories\n\nIn cell biology, chromosome territories are regions of the nucleus preferentially occupied by particular chromosomes.\n\nInterphase chromosomes are long DNA strands that are extensively folded, and are often described as appearing like a bowl of spaghetti. The chromosome territory concept holds that despite this apparent disorder, chromosomes largely occupy defined regions of the nucleus. Most eukaryotes are thought to have chromosome territories, although the budding yeast \"S. cerevisiae\" is an exception to this.\n\nChromosome territories are spheroid with diameters on the order of one to few micrometers.\n\nNuclear compartments devoid of DNA called interchromatin compartments have been reported to tunnel into chromosome territories to facilitate molecular diffusion into the otherwise tightly packed chromosome-occupied regions.\n\nThe concept of chromosome territories was proposed by Carl Rabl in 1885 based on studies of Salamandra maculata.\n\nChromosome territories have gained recognition using fluorescence labeling techniques (fluorescence in situ hybridization).\n\nStudies of genomic proximity using techniques like chromosome conformation capture have supported the chromosome territory concept by showing that DNA-DNA contacts predominantly happen within particular chromosomes.\n", "id": "47515694", "title": "Chromosome territories"}
{"url": "https://en.wikipedia.org/wiki?curid=47689783", "text": "LINE1\n\nLINE1 (also L1 and LINE-1) are transposable elements in the DNA of some organisms and belong to the group of Long interspersed nuclear elements (LINEs). L1 comprise approximately 17% of the human genome.\n\nThe majority of L1 in the human genome are inactive; however, some retained the ability to retrotranspose. Human L1 has been reported to have transferred to the genome of the gonorrhea bacteria.\n\nA typical L1 element is approximately 6,000 base pairs long and consists of two non-overlapping open reading frames (ORF) which are flanked by UTR and target site duplications. In human, ORF2 is thought to be translated by an unconventional termination/reinitiation mechanism, while mouse L1s contain an internal ribosome entry site (IRES) upstream of each ORF. \n\nThe 5' Untranslated region (UTR) of the L1 element contains a strong, internal RNA Polymerase II transcription promoter in sense \n\nThe 5' UTR of mouse L1s contain a variable number of GC-rich tandemly repeated monomers of around 200pb, followed by a short non-monomeric region. \n\nHuman 5’UTRs are ~900pb in length and do not contain repeated motifs. All families of human L1s harbor in their most 5’ extremity a binding motif for the transcription factor YY1. Younger families have also two binding sites for SOX-family transcription factors, and both YY1 and SOX sites were shown to be required for human L1 transcription initiation and activation.\n\nBoth mouse and human 5’UTRs contain as well a week antisense promoter of unknown function\n\nThe first ORF encode a 500 amino acid - 40kDa protein that lacks homology with any protein of known function. In vertebrates, it contains a conserved C-terminus domain and a highly variable coiled-coil N-terminus that mediates the formation of ORF1 trimetric complexes. ORF1 trimmers have RNA-binding and nucleic acid chaperone activity that are necessary for retrotransposition. \n\nThe second ORF of L1 encodes a protein that has endonuclease and reverse transcriptase activity. The encoded protein has a molecular weight of 150 kDA.\n\n", "id": "47689783", "title": "LINE1"}
{"url": "https://en.wikipedia.org/wiki?curid=1910", "text": "Agarose gel electrophoresis\n\nAgarose gel electrophoresis is a method of gel electrophoresis used in biochemistry, molecular biology, genetics, and clinical chemistry to separate a mixed population of macromolecules such as DNA or proteins in a matrix of agarose, one of the two main components of agar. The proteins may be separated by charge and/or size (isoelectric focusing agarose electrophoresis is essentially size independent), and the DNA and RNA fragments by length. Biomolecules are separated by applying an electric field to move the charged molecules through an agarose matrix, and the biomolecules are separated by size in the agarose gel matrix.\n\nAgarose gel is easy to cast, has relatively fewer charged groups, and is particularly suitable for separating DNA of size range most often encountered in laboratories, which accounts for the popularity of its use. The separated DNA may be viewed with stain, most commonly under UV light, and the DNA fragments can be extracted from the gel with relative ease. Most agarose gels used are between 0.7 - 2% dissolved in a suitable electrophoresis buffer.\n\nAgarose gel is a three-dimensional matrix formed of helical agarose molecules in supercoiled bundles that are aggregated into three-dimensional structures with channels and pores through which biomolecules can pass. The 3-D structure is held together with hydrogen bonds and can therefore be disrupted by heating back to a liquid state. The melting temperature is different from the gelling temperature, depending on the sources, agarose gel has a gelling temperature of 35-42 °C and a melting temperature of 85-95 °C. Low-melting and low-gelling agaroses made through chemical modifications are also available.\n\nAgarose gel has large pore size and good gel strength, making it suitable as an anticonvection medium for the electrophoresis of DNA and large protein molecules. The pore size of a 1% gel has been estimated from 100 nm to 200-500 nm, and its gel strength allows gels as dilute as 0.15% to form a slab for gel electrophoresis. Low-concentration gels (0.1 - 0.2%) however are fragile and therefore hard to handle. Agarose gel has lower resolving power than polyacrylamide gel for DNA but has a greater range of separation, and is therefore used for DNA fragments of usually 50-20,000 bp in size. The limit of resolution for standard agarose gel electrophoresis is around 750 kb, but resolution of over 6 Mb is possible with pulsed field gel electrophoresis (PFGE). It can also be used to separate large proteins, and it is the preferred matrix for the gel electrophoresis of particles with effective radii larger than 5-10 nm. A 0.9% agarose gel has pores large enough for the entry of bacteriophage T4.\n\nThe agarose polymer contains charged groups, in particular pyruvate and sulphate. These negatively charged groups create a flow of water in the opposite direction to the movement of DNA in a process called electroendosmosis (EEO), and can therefore retard the movement of DNA and cause blurring of bands. Higher concentration gel would have higher electroosmotic flow. Low EEO agarose is therefore generally preferred for use in agarose gel electrophoresis of nucleic acids, but high EEO agarose may be used for other purposes. The lower sulphate content of low EEO agarose, particularly low-melting point (LMP) agarose, is also beneficial in cases where the DNA extracted from gel is to be used for further manipulation as the presence of contaminating sulphates may affect some subsequent procedures, such as ligation and PCR. Zero EEO agaroses however are undesirable for some applications as they may be made by adding positively charged groups and such groups can affect subsequent enzyme reactions. Electroendosmosis is a reason agarose is used in preference to agar as the agaropectin component in agar contains a significant amount of negatively charged sulphate and carboxyl groups. The removal of agaropectin in agarose substantially reduce the EEO, as well as reducing the non-specific adsorption of biomolecules to the gel matrix. However, for some applications such as the electrophoresis of serum proteins, a high EEO may be desirable, and agaropeptin may be added in the gel used.\n\nA number of factors can affect the migration of nucleic acids: the dimension of the gel pores (gel concentration), size of DNA being electrophoresed, the voltage used, the ionic strength of the buffer, and the concentration of intercalating dye such as ethidium bromide if used during electrophoresis.\n\nSmaller molecules travel faster than larger molecules in gel, and double-stranded DNA moves at a rate that is inversely proportional to the log of the number of base pairs. This relationship however breaks down with very large DNA fragments, and separation of very large DNA fragments requires the use of pulsed field gel electrophoresis (PFGE), which applies alternating current from two different directions and the large DNA fragments are separated as they reorient themselves with the changing current. \n\nFor standard agarose gel electrophoresis, larger molecules are resolved better using a low concentration gel while smaller molecules separate better at high concentration gel. High concentrations gel however requires longer run times (sometimes days).\n\nThe movement of the DNA may be affected by the conformation of the DNA molecule, for example, supercoiled DNA usually moves faster than relaxed DNA because it is tightly coiled and hence more compact. In a normal plasmid DNA preparation, multiple forms of DNA may be present. Gel electrophoresis of the plasmids would normally show the negatively supercoiled form as the main band, while nicked DNA (open circular form) and the relaxed closed circular form appears as minor bands. The rate at which the various forms move however can change using different electrophoresis conditions, and the mobility of larger circular DNA may be more strongly affected than linear DNA by the pore size of the gel.\n\nEthidium bromide which intercalates into circular DNA can change the charge, length, as well as the superhelicity of the DNA molecule, therefore its presence in gel during electrophoresis can affect its movement. For example, the positive charge of ethidium bromide can reduce the DNA movement by 15%. Agarose gel electrophoresis can be used to resolve circular DNA with different supercoiling topology.\n\nDNA damage due to increased cross-linking will also reduce electrophoretic DNA migration in a dose-dependent way.\n\nThe rate of migration of the DNA is proportional to the voltage applied, i.e. the higher the voltage, the faster the DNA moves. The resolution of large DNA fragments however is lower at high voltage. The mobility of DNA may also change in an unsteady field - in a field that is periodically reversed, the mobility of DNA of a particular size may drop significantly at a particular cycling frequency. This phenomenon can result in band inversion in field inversion gel electrophoresis (FIGE), whereby larger DNA fragments move faster than smaller ones.\n\n\nThe negative charge of its phosphate backbone moves the DNA towards the positively charged anode during electrophoresis. However, the migration of DNA molecules in solution, in the absence of a gel matrix, is independent of molecular weight during electrophoresis. The gel matrix is therefore responsible for the separation of DNA by size during electrophoresis, and a number of models exist to explain the mechanism of separation of biomolecules in gel matrix. A widely accepted one is the Ogston model which treats the polymer matrix as a sieve. A globular protein or a random coil DNA moves through the interconnected pores, and the movement of larger molecules is more likely to be impeded and slowed down by collisions with the gel matrix, and the molecules of different sizes can therefore be separated in this sieving process.\n\nThe Ogston model however breaks down for large molecules whereby the pores are significantly smaller than size of the molecule. For DNA molecules of size greater than 1 kb, a reptation model (or its variants) is most commonly used. This model assumes that the DNA can crawl in a \"snake-like\" fashion (hence \"reptation\") through the pores as an elongated molecule. A biased reptation model applies at higher electric field strength, whereby the leading end of the molecule become strongly biased in the forward direction and pulls the rest of the molecule along. Real-time fluorescence microscopy of stained molecules, however, showed more subtle dynamics during electrophoresis, with the DNA showing considerable elasticity as it alternately stretching in the direction of the applied field and then contracting into a ball, or becoming hooked into a U-shape when it gets caught on the polymer fibres.\n\nThe details of an agarose gel electrophoresis experiment may vary depending on methods, but most follow a general procedure.\nThe gel is prepared by dissolving the agarose powder in an appropriate buffer, such as TAE or TBE, to be used in electrophoresis. The agarose is dispersed in the buffer before heating it to near-boiling point, but avoid boiling. The melted agarose is allowed to cool sufficiently before pouring the solution into a cast as the cast may warp or crack if the agarose solution is too hot. A comb is placed in the cast to create wells for loading sample, and the gel should be completely set before use.\n\nThe concentration of gel affects the resolution of DNA separation. For a standard agarose gel electrophoresis, a 0.8% gives good separation or resolution of large 5–10kb DNA fragments, while 2% gel gives good resolution for small 0.2–1kb fragments. 1% gels is often used for a standard electrophoresis. The concentration is measured in weight of agarose over volume of buffer used (g/ml). High percentage gels are often brittle and may not set evenly, while low percentage gels (0.1-0.2%) are fragile and not easy to handle. Low-melting-point (LMP) agarose gels are also more fragile than normal agarose gel. Low-melting point agarose may be used on its own or simultaneously with standard agarose for the separation and isolation of DNA. PFGE and FIGE are often done with high percentage agarose gels.\n\nOnce the gel has set, the comb is removed, leaving wells where DNA samples can be loaded. Loading buffer is mixed with the DNA sample before the mixture is loaded into the wells. The loading buffer contains a dense compound, which may be glycerol, sucrose, or Ficoll, that raises the density of the sample so that the DNA sample may sink to the bottom of the well. If the DNA sample contains residual ethanol after its preparation, it may float out of the well. The loading buffer also includes colored dyes such as xylene cyanol and bromophenol blue used to monitor the progress of the electrophoresis. The DNA samples are loaded using a pipette.\n\nAgarose gel electrophoresis is most commonly done horizontally in a submarine mode whereby the slab gel is completely submerged in buffer during electrophoresis. It is also possible, but less common, to perform the electrophoresis vertically, as well as horizontally with the gel raised on agarose legs using an appropriate apparatus. The buffer used in the gel is the same as the running buffer in the electrophoresis tank, which is why electrophoresis in the submarine mode is possible with agarose gel.\n\nFor optimal resolution of DNA greater than 2kb in size in standard gel electrophoresis, 5 to 8 V/cm is recommended (the distance in cm refers to the distance between electrodes, therefore this recommended voltage would be 5 to 8 multiplied by the distance between the electrodes in cm). Voltage may also be limited by the fact that it heats the gel and may cause the gel to melt if it is run at high voltage for a prolonged period, especially if the gel used is LMP agarose gel. Too high a voltage may also reduce resolution, as well as causing band streaking for large DNA molecules. Too low a voltage may lead to broadening of band for small DNA fragments due to dispersion and diffusion.\n\nSince DNA is not visible in natural light, the progress of the electrophoresis is monitored using colored dyes. Xylene cyanol (light blue color) comigrates large DNA fragments, while Bromophenol blue (dark blue) comigrates with the smaller fragments. Less commonly used dyes include Cresol Red and Orange G which migrate ahead of bromophenol blue. A DNA marker is also run together for the estimation of the molecular weight of the DNA fragments. Note however that the size of a circular DNA like plasmids cannot be accurately gauged using standard markers unless it has been linearized by restriction digest, alternatively a supercoiled DNA marker may be used.\n\nDNA as well as RNA are normally visualized by staining with ethidium bromide, which intercalates into the major grooves of the DNA and fluoresces under UV light. The intercalation depends on the concentration of DNA and thus, a band with high intensity will indicate a higher amount of DNA compared to a band of less intensity. The ethidium bromide may be added to the agarose solution before it gels, or the DNA gel may be stained later after electrophoresis. Destaining of the gel is not necessary but may produce better images. Other methods of staining are available; examples are SYBR Green, GelRed, methylene blue, brilliant cresyl blue, Nile blue sulphate, and crystal violet. SYBR Green, GelRed and other similar commercial products are sold as safer alternatives to ethidium bromide as it has been shown to be mutagenic in Ames test, although the carcinogenicity of ethidium bromide has not actually been established. SYBR Green requires the use of a blue-light transilluminator. DNA stained with crystal violet can be viewed under natural light without the use of a UV transilluminator which is an advantage, however it may not produce a strong band.\n\nWhen stained with ethidium bromide, the gel is viewed with an ultraviolet (UV) transilluminator. The UV light excites the electrons within the aromatic ring of ethidium bromide, and once they return to the ground state, light is released, making the DNA and ethidium bromide complex fluoresce. Standard transilluminators use wavelengths of 302/312-nm (UV-B), however exposure of DNA to UV radiation for as little as 45 seconds can produce damage to DNA and affect subsequent procedures, for example reducing the efficiency of transformation, \"in vitro\" transcription, and PCR. Exposure of the DNA to UV radiation therefore should be limited. Using a higher wavelength of 365 nm (UV-A range) causes less damage to the DNA but also produces much weaker fluorescence with ethidium bromide. Where multiple wavelengths can be selected in the transillumintor, the shorter wavelength would be used to capture images, while the longer wavelength should be used if it is necessary to work on the gel for any extended period of time.\n\nThe transilluminator apparatus may also contain image capture devices, such as a digital or polaroid camera, that allow an image of the gel to be taken or printed.\n\nFor gel electrophoresis of protein, the bands may be visualised with Coomassie or silver stains.\nThe separated DNA bands are often used for further procedures, and a DNA band may be cut out of the gel as a slice, dissolved and purified. Contaminants however may affect some downstream procedures such as PCR, and low melting point agarose may be preferred in some cases as it contains fewer of the sulphates that can affect some enzymatic reactions. The gels may also be used for blotting techniques.\n\nIn general, the ideal buffer should have good conductivity, produce less heat and have a long life. There are a number of buffers used for agarose electrophoresis; common ones for nucleic acids include Tris/Acetate/EDTA (TAE) and Tris/Borate/EDTA (TBE). The buffers used contain EDTA to inactivate many nucleases which require divalent cation for their function. The borate in TBE buffer can be problematic as borate can polymerize, and/or interact with cis diols such as those found in RNA. TAE has the lowest buffering capacity, but it provides the best resolution for larger DNA. This means a lower voltage and more time, but a better product.\n\nMany other buffers have been proposed, e.g. lithium borate (LB), iso electric histidine, pK matched goods buffers, etc.; in most cases the purported rationale is lower current (less heat) and or matched ion mobilities, which leads to longer buffer life. Tris-phosphate buffer has high buffering capacity but cannot be used if DNA extracted is to be used in phosphate sensitive reaction. LB is relatively new and is ineffective in resolving fragments larger than 5 kbp; However, with its low conductivity, a much higher voltage could be used (up to 35 V/cm), which means a shorter analysis time for routine electrophoresis. As low as one base pair size difference could be resolved in 3% agarose gel with an extremely low conductivity medium (1 mM lithium borate).\n\nOther buffering system may be used in specific applications, for example, barbituric acid-sodium barbiturate or Tris-barbiturate buffers may be used for in agarose gel electrophoresis of proteins, for example in the detection of abnormal distribution of proteins.\n\n\nAgarose gels are easily cast and handled compared to other matrices and nucleic acids are not chemically altered during electrophoresis. Samples are also easily recovered. After the experiment is finished, the resulting gel can be stored in a plastic bag in a refrigerator.\n\nElectrophoresis is performed in buffer solutions to reduce pH changes due to the electric field, which is important because the charge of DNA and RNA depends on pH, but running for too long can exhaust the buffering capacity of the solution. Further, different preparations of genetic material may not migrate consistently with each other, for morphological or other reasons.\n\n\n", "id": "1910", "title": "Agarose gel electrophoresis"}
{"url": "https://en.wikipedia.org/wiki?curid=47887859", "text": "Cycle of quantification/qualification\n\nCycle of quantification/qualification (C) is a parameter used in real-time polymerase chain reaction techniques, indicating the cycle number where a PCR amplification curve meets a predefined mathematical criterion. A C may be used for quantification of the target sequence or to determine whether the target sequence is present or not.\n\nTwo criteria to determine the C are used by different thermocyclers:\nCycle of Threshold (CT) is the number of cycles required for the fluorescent signal to cross a given value threshold. Usually, the threshold is set above the baseline, about 10 times the standard deviation of the noise of the baseline, to avoid random effects on the CT. However, the threshold shouln't be set much higher than that to avoid reduced reproducibility due to uncontrolled factors. \nCrossing point (Cp) and Take off point (TOP) are the cycle value of the maximum second derivative of the amplification curve.\n", "id": "47887859", "title": "Cycle of quantification/qualification"}
{"url": "https://en.wikipedia.org/wiki?curid=2492332", "text": "Restriction site\n\nRestriction sites, or restriction recognition sites, are locations on a DNA molecule containing specific (4-8 base pairs in length) sequences of nucleotides, which are recognized by restriction enzymes. These are generally palindromic sequences (because restriction enzymes usually bind as homodimers), and a particular restriction enzyme may cut the sequence between two nucleotides within its recognition site, or somewhere nearby. For example, the common restriction enzyme EcoRI recognizes the palindromic sequence GAATTC and cuts between the G and the A on both the top and bottom strands, leaving an overhang (an end-portion of a DNA strand with no attached complement) known as a sticky end on each end of AATT. This overhang can then be used to ligate in (see DNA ligase) a piece of DNA with a complementary overhang (another EcoRI-cut piece, for example). Some restriction enzymes cut DNA at a restriction site in a manner which leaves no overhang, called a blunt end. Blunt ends are much less likely to be ligated by a DNA ligase because the blunt end doesn’t have the overhanging base pair that the enzyme can recognize and match with a complementary pair. Sticky ends of DNA however are more likely to successfully bind with the help of a DNA ligase because of the exposed and unpaired nucleotides. For example, a sticky end trailing with AATTG is more likely to bind with a ligase than a blunt end where both the 5' and 3' DNA strands are paired. In the case of the example the AATTG would have a complimentary pair of TTAAC which would reduce the functionality of the DNA ligase enzyme.\n\nRestriction sites can be used for multiple applications in molecular biology such as identifying restriction fragment length polymorphisms (RFLPs).\n", "id": "2492332", "title": "Restriction site"}
{"url": "https://en.wikipedia.org/wiki?curid=47973429", "text": "ViroCap\n\nViroCap is a test announced in 2015 by researchers at Washington University in St. Louis which can detect most of the infectious viruses which affect humans and animals. It was demonstrated to be as sensitive as the various Polymerase chain reaction assays for the viruses. It will not be available for clinical use until validation studies are done, which may take years. The test examines two million sequences of genetic data from viruses. The research was published in September 2015 in the online journal Genome Research.\n\n", "id": "47973429", "title": "ViroCap"}
{"url": "https://en.wikipedia.org/wiki?curid=160125", "text": "Mitochondrial disease\n\nMitochondrial diseases are a group of disorders caused by dysfunctional mitochondria, the organelles that generate energy for the cell. Mitochondria are found in every cell of the human body except red blood cells, and convert the energy of food molecules into the ATP that powers most cell functions.\n\nMitochondrial diseases are sometimes (about 15% of the time) caused by mutations in the mitochondrial DNA that affect mitochondrial function. Other mitochondrial diseases are caused by mutations in genes of the nuclear DNA, whose gene products are imported into the mitochondria (mitochondrial proteins) as well as acquired mitochondrial conditions. Mitochondrial diseases take on unique characteristics both because of the way the diseases are often inherited and because mitochondria are so critical to cell function. The subclass of these diseases that have neuromuscular disease symptoms are often called a mitochondrial myopathy.\n\nSymptoms include poor growth, loss of muscle coordination, muscle weakness, visual problems, hearing problems, learning disabilities, heart disease, liver disease, kidney disease, gastrointestinal disorders, respiratory disorders, neurological problems, autonomic dysfunction and dementia. Acquired conditions in which mitochondrial dysfunction has been involved are: diabetes, Huntington's disease, cancer, Alzheimer's disease, Parkinson's disease, bipolar disorder, schizophrenia, aging and senescence, anxiety disorders, cardiovascular disease, sarcopenia, chronic fatigue syndrome. \n\nThe body, and each mutation, is modulated by other genome variants; the mutation that in one individual may cause liver disease might in another person cause a brain disorder. The severity of the specific defect may also be great or small. Some minor defects cause only \"exercise intolerance\", with no serious illness or disability. Defects often affect the operation of the mitochondria and multiple tissues more severely, leading to multi-system diseases.\n\nAs a rule, mitochondrial diseases are worse when the defective mitochondria are present in the muscles, cerebrum, or nerves, because these cells use more energy than most other cells in the body.\n\nAlthough mitochondrial diseases vary greatly in presentation from person to person, several major clinical categories of these conditions have been defined, based on the most common phenotypic features, symptoms, and signs associated with the particular mutations that tend to cause them.\n\nAn outstanding question and area of research is whether ATP depletion or reactive oxygen species are in fact responsible for the observed phenotypic consequences.\n\nCerebellar atrophy or hypoplasia has sometimes been reported to be associated.\n\nMitochondrial disorders may be caused by mutations (acquired or inherited), in mitochondrial DNA (mtDNA), or in nuclear genes that code for mitochondrial components. They may also be the result of acquired mitochondrial dysfunction due to adverse effects of drugs, infections, or other environmental causes (see MeSH).\n\nNuclear DNA has two copies per cell (except for sperm and egg cells), one copy being inherited from the father and the other from the mother. Mitochondrial DNA, however, is strictly inherited from the mother and each mitochondrial organelle typically contains between 2 and 10 mtDNA copies. During cell division the mitochondria segregate randomly between the two new cells. Those mitochondria make more copies, normally reaching 500 mitochondria per cell. As mtDNA is copied when mitochondria proliferate, they can accumulate random mutations, a phenomenon called heteroplasmy. If only a few of the mtDNA copies inherited from the mother are defective, mitochondrial division may cause most of the defective copies to end up in just one of the new mitochondria (for more detailed inheritance patterns, see human mitochondrial genetics). Mitochondrial disease may become clinically apparent once the number of affected mitochondria reaches a certain level; this phenomenon is called \"threshold expression\".\n\nMitochondrial DNA mutations occur frequently, due to the lack of the error checking capability that nuclear DNA has (see Mutation rate). This means that mitochondrial DNA disorders may occur spontaneously and relatively often. Defects in enzymes that control mitochondrial DNA replication (all of which are encoded for by genes in the nuclear DNA) may also cause mitochondrial DNA mutations.\n\nMost mitochondrial function and biogenesis is controlled by nuclear DNA. Human mitochondrial DNA encodes 13 proteins of the respiratory chain, while most of the estimated 1,500 proteins and components targeted to mitochondria are nuclear-encoded. Defects in nuclear-encoded mitochondrial genes are associated with hundreds of clinical disease phenotypes including anemia, dementia, hypertension, lymphoma, retinopathy, seizures, and neurodevelopmental disorders.\n\nA study by Yale University researchers (published in the February 12, 2004 issue of the \"New England Journal of Medicine\") explored the role of mitochondria in insulin resistance among the offspring of patients with type 2 diabetes. Other studies have shown that the mechanism may involve the interruption of the mitochondrial signaling process in body cells (intramyocellular lipids). A study conducted at the Pennington Biomedical Research Center in Baton Rouge, Louisiana showed that this, in turn, partially disables the genes that produce mitochondria.\n\nExamples of mitochondrial diseases include:\n\n\"Conditions such as Friedreich's ataxia can affect the mitochondria but are not associated with mitochondrial proteins.\"\n\nThe effective overall energy unit for the available body energy is referred to as the daily glycogen generation capacity, and is used to compare the mitochondrial output of healthy individuals to that of afflicted or chronically glycogen-depleted individuals. This value is slow to change in a given individual, as it takes between 18 and 24 months to complete a full cycle.\n\nThe glycogen generation capacity is entirely dependent on, and determined by, the operating levels of the mitochondria in all of the cells of the human body; however, the relation between the energy generated by the mitochondria and the glycogen capacity is very loose and is mediated by many biochemical pathways. The energy output of full healthy mitochondrial function can be predicted exactly by a complicated theoretical argument, but this argument is not straightforward, as most energy is consumed by the brain and is not easily measurable.\n\nMitochondrial diseases are usually detected by analysing muscle samples, where the presence of these organelles is higher. The most common tests for the detection of these diseases are:\n\nAlthough research is ongoing, treatment options are currently limited; vitamins are frequently prescribed, though the evidence for their effectiveness is limited. \nPyruvate has been proposed in 2007 as a treatment option. N-acetyl cysteine reverses many models of mitochondrial dysfunction.. In the case of mood disorders, specifically bipolar disorder, it is hypothesized that N-acetyl-cysteine (NAC), acetyl-L-carnitine (ALCAR), S-adenosylmethionine (SAMe), coenzyme Q10 (CoQ10), alpha-lipoic acid (ALA), creatine monohydrate (CM), and melatonin could be potential treatment options. \n\nSpindle transfer, where the nuclear DNA is transferred to another healthy egg cell leaving the defective mitochondrial DNA behind, is a potential treatment procedure that has been successfully carried out on monkeys. Using a similar pronuclear transfer technique, researchers at Newcastle University led by Douglass Turnbull successfully transplanted healthy DNA in human eggs from women with mitochondrial disease into the eggs of women donors who were unaffected. In such cases, ethical questions have been raised regarding biological motherhood, since the child receives genes and gene regulatory molecules from two different women. Using genetic engineering in attempts to produce babies free of mitochondrial disease is controversial in some circles and raises important ethical issues. A male baby was born in Mexico in 2016 from a mother with Leigh syndrome using spindle transfer.\n\nIn September 2012 a public consultation was launched in the UK to explore the ethical issues involved. Human genetic engineering was used on a small scale to allow infertile women with genetic defects in their mitochondria to have children. \nIn June 2013, the United Kingdom government agreed to develop legislation that would legalize the 'three-person IVF' procedure as a treatment to fix or eliminate mitochondrial diseases that are passed on from mother to child. The procedure could be offered from 29 October 2015 once regulations had been established.\nEmbryonic mitochondrial transplant and protofection have been proposed as a possible treatment for inherited mitochondrial disease, and allotopic expression of mitochondrial proteins as a radical treatment for mtDNA mutation load.\n\nCurrently, human clinical trials are underway at GenSight Biologics (ClinicalTrials.gov # NCT02064569) and the University of Miami (ClinicalTrials.gov # NCT02161380) to examine the safety and efficacy of mitochondrial gene therapy in Leber's hereditary optic neuropathy.\n\nAbout 1 in 4,000 children in the United States will develop mitochondrial disease by the age of 10 years. Up to 4,000 children per year in the US are born with a type of mitochondrial disease. Because mitochondrial disorders contain many variations and subsets, some particular mitochondrial disorders are very rare.\n\nThe average number of births per year among women at risk for transmitting mtDNA disease is estimated to approximately 150 in the United Kingdom and 800 in the United States.\n\nThe first pathogenic mutation in mitochondrial DNA was identified in 1988; from that time to 2016, around 275 other disease-causing mutations were identified.\n\nNotable people who suffered from mitochondrial disease include:\n\n\n", "id": "160125", "title": "Mitochondrial disease"}
{"url": "https://en.wikipedia.org/wiki?curid=46335535", "text": "Hox genes in amphibians and reptiles\n\nHox genes play a massive role in some amphibians and reptiles in their ability to regenerate lost limbs, especially HoxA and HoxD genes.\n\nIf the processes involved in forming new tissue can be reverse-engineered into humans, it may be possible to heal injuries of the spinal cord or brain, repair damaged organs and reduce scarring and fibrosis after surgery. Despite the large conservation of the Hox genes through evolution, mammals and humans specifically cannot regenerate any of their limbs. This raises a question as to why humans which also possess an analog to these genes cannot regrow and regenerate limbs. Beside the lack of specific growth factor, studies have shown that something as small as base pair differences between amphibian and human Hox analogs play a crucial role in human inability to reproduce limbs. Undifferentiated stem cells and the ability to have polarity in tissues is vital to this process.\n\nSome amphibians and reptiles have the ability to regenerate limbs, eyes, spinal cords, hearts, intestines, and upper and lower jaws. The Japanese fire belly newt can regenerate its eye lens 18 times over a period of 16 years and retain its structural and functional properties. The cells at the site of the injury have the ability to undifferentiate, reproduce rapidly, and differentiate again to create a new limb or organ.\n\nHox genes are a group of related genes that control the body plan of an embryo along the head-tail axis. They are responsible for body segment differentiation and express the arrangement of numerous body components during initial embryonic development. Primarily, these sets of genes are utilized during the development of body plans by coding for the transcription factors that trigger production of body segment specific structures. Additionally in most animals, these genes are laid out along the chromosome similar to the order in which they are expressed along the anterior – posterior axis.\n\nVariants of the Hox genes are found almost in every phylum with the exception of the sponge which use a different type of developmental genes. The homology of these genes is of important interest to scientists as they may hold more answers to the evolution of many species. In fact, these genes demonstrate such a high degree of homology that a human Hox gene variant – HOXB4 – could mimic the function of its homolog in the fruit fly (\"Drosophila\"). Studies suggest that the regulation and other target genes in different species are actually what causes such a great difference in phenotypic difference between species.\n\nHox genes contain a DNA sequence known as the homeobox that are involved in the regulation of patterns of anatomical development. They contain a specific DNA sequence with the aim of providing instructions for making a string of 60 protein building blocks - amino acids- which are referred to as the homeodomain. Most homeodomain-containing proteins function as transcription factors and fundamentally bind and regulate the activity of different genes. The homeodomain is the segment of the protein that binds to precise regulatory regions of the target genes. Genes within the homeobox family are implicated in a wide variety of significant activities during growth. These activities include directing the development of limbs and organs along the anterior-posterior axis and regulating the process by which cells mature to carry out specific functions, a process known as cellular differentiation. Certain homeobox genes can act tumor suppressors, which means they help prevent cells from growing and dividing too rapidly or in an uncontrolled way.\n\nDue to the fact that homeobox genes have so many important functions, mutations in these genes are accountable for a wide array of developmental disorders. Changes in certain homeobox genes often result in eye disorders, cause abnormal head, face, and tooth development. Additionally, increased or decreased activity of certain homeobox genes has been associated with several forms of cancer later in life.\n\nEssentially, Hox genes contribute to the specification of three main components of limb development, including the stylopod, zeugopod and autopod. Certain mutations in Hox genes can potentially lead to the proximal and/or distal losses along with different abnormalities. Three different models have been created for outlining the patterning of these regions. The Zone of polarizing activity (ZPA) in the limb bud has pattern-organizing activity through the utilization of a morphogen gradient of a protein called Sonic hedgehog (Shh). Sonic hedgehog is turned on in the posterior region via the early expression of HoxD genes, along with the expression of Hoxb8. Shh is maintained in the posterior through a feedback loop between the ZPA and the AER. Shh cleaves the Ci/Gli3 transcriptional repressor complex to convert the transcription factor Gli3 to an activator, which activates the transcription of HoxD genes along the anterior/posterior axis. It is evident that different Hox genes are critical for proper limb development in different amphibians.\n\nResearchers conducted a study targeting the Hox-9 to Hox-13 genes in different species of frogs and other amphibians. Similar to an ancient tetrapod group with assorted limb types, it is important to note that amphibians are required for the understanding of the origin and diversification of limbs in different land vertebrates. A PCR (Polymerase Chain Reaction) study was conducted in two species of each amphibian order to identify Hox-9 to Hox-13. Fifteen distinct posterior Hox genes and one retro-pseudogene were identified, and the former confirm the existence of four Hox clusters in each amphibian order. Interestingly, certain genes expected to occur in all tetrapods, based on the posterior Hox complement of mammals, fishes and coelacanth, were not recovered. HoxD-12 is absent in frogs and possibly other amphibians. By definition, the autopodium is distal segment of a limb, comprising the hand or foot. Considering Hox-12’s function in autopodium development, the loss of this gene may be related to the absence of the fifth finger in frogs and salamanders.\n\nAs previously mentioned, Hox genes encode transcription factors that regulate embryonic and post-embryonic developmental processes. The expression of Hox genes is regulated in part by the tight, spatial arrangement of conserved coding and non-coding DNA regions. The potential for evolutionary alterations in Hox cluster composition is viewed to be small among vertebrates. On the other hand, recent studies of a small number of non-mammalian taxa propose greater dissimilarity than initially considered. Next, generation sequencing of considerable genomic fragments greater than 100 kilobases from the eastern newt (\"Notophthalmus viridescens\") was analyzed. Subsequently, it was found that the composition of Hox cluster genes were conserved relative to orthologous regions from other vertebrates. Furthermore, it was found that the length of introns and intergenic regions varied. In particular, the distance between HoxD13 and HoxD11 is longer in newt than orthologous regions from vertebrate species with expanded Hox clusters and is predicted to exceed the length of the entire HoxD clusters (HoxD13–HoxD4) of humans, mice, and frogs. Many recurring DNA sequences were recognized for newt Hox clusters, counting an enrichment of DNA transposon-like sequences similar to non-coding genomic fragments. Researchers found the results to suggest that Hox cluster expansion and transposon accumulation are common features of non-mammalian tetrapod vertebrates.\nAfter the loss of a limb, cells draw together to form a clump known as a blastema. This superficially appears undifferentiated, but cells that originated in the skin later develop into new skin, muscle cells into new muscle and cartilage cells into new cartilage. It is only the cells from just beneath the surface of the skin that are pluripotent and able to develop into any type of cell. Salamander Hox genomic regions show elements of conservation and variety in comparison to other vertebrate species. Whereas the structure and organization of Hox coding genes is conserved, newt Hox clusters show variation in the lengths of introns and intergenic regions, and the HoxD13–11 region exceeds the lengths of orthologous segments even among vertebrate species with expanded Hox clusters. Researchers have suggested that the HoxD13–11 expansion predated a basal salamander genome size amplification that occurred approximately 191 million years ago, because it preserved in all three extant amphibian groups. Supplementary verification supports the proposal that Hox clusters are acquiescent to structural evolution and variation is present in the lengths of introns and intergenic regions, relatively high numbers of repetitive sequences, and non-random accumulations of DNA transposons in newts and lizards. Researchers found that the non-random accretion of DNA-like transposons could possibly change developmental encoding by generating sequence motifs for transcriptional control.\n\nIn conclusion, the available data from several non-mammalian tetrapods suggest that Hox structural flexibility is the rule, not the exception. It is thought that this elasticity may allow for developmental variation across non-mammalian taxa. This is of course true for both embryogenesis and during the redeployment of Hox genes during post-embryonic developmental processes, such as metamorphosis and regeneration.\n\nAnother interesting phenomena that exists in animal models is the presence of gradient fields in early development. More specifically, this has been shown in the aquatic amphibian: the newt. These \"gradient fields\" as they are known in developmental biology, have the ability to form the appropriate tissues that they are designed to form when cells from other parts of the embryo are introduced or transplanted into specific fields. The first reporting of this was in 1934. Originally, the specific mechanism behind this rather bizarre phenomenon was not known, however Hox genes have been shown to be prevalent behind this process. More specifically, a concept now known as polarity has been implemented as one - but not the only one - of the mechanisms that are driving this development.\n\nStudies done by Oliver \"et al\" in 1988, showed that different concentrations of XIHbox 1 antigen was present along the anterior-posterior mesoderm of various developing animal models. One conclusion that this varied concentration of protein expression is actually causing differentiation amongst various tissues and could be one of the culprits behind these so-called \"gradient fields\". While the protein products of Hox genes are strongly involved in these fields and differentiation in amphibians and reptiles, there are other causality factors involved. For example, retinoic acid and other growth factors have been shown to play a role in these gradient fields.\n", "id": "46335535", "title": "Hox genes in amphibians and reptiles"}
{"url": "https://en.wikipedia.org/wiki?curid=3109661", "text": "Avecia\n\nAvecia- a part of Nitto Denko Inc.- is a leading private biotechnology company focused on the development and manufacture of innovative biotechnology-based medicines. Avecia operates as a contract manufacturing organization (CMO), specializing in oligonucleotide production.\n\nAvecia has two US FDA-audited facilities, one near Boston, MA, and the second in Cincinnati, OH. The combined capacity of these two sites allows Avecia to handle oligonucleotide programs from the pre-clinical through commercial stages.\n\n", "id": "3109661", "title": "Avecia"}
{"url": "https://en.wikipedia.org/wiki?curid=2547545", "text": "DNA bank\n\nDNA banking is the secure, long term storage of an individual’s genetic material. DNA is most commonly extracted from blood, but can also be obtained from saliva and other tissues. DNA banks allow for conservation of genetic material and comparative analysis of an individual's genetic information. Analyzing an individual's DNA can allow scientists to predict genetic disorders, as used in preventative genetics or gene therapy, and prove that person's identity, as used in the criminal justice system. There are multiple methods for testing and analyzing genetic information including restriction fragment length polymorphism (RFLP) and polymerase chain reactions (PCR).\n\nDNA banking is used to conserve genetic material, especially that of organisms that face extinction. This is a more prominent issue today due to deforestation and climate change, which serve as a threat to biodiversity. The genetic information can be stored within lambda phage and plasma vectors. The National Institute of Agrobiological Sciences (NIAS) DNA Bank, for example, collects the DNA of agricultural organisms, such as rice and fish, for scientific research. Most DNA provided by DNA banks is used for studies to attempt to develop more productive or more environmentally friendly agricultural species. Some DNA banks also store the DNA of rare or endangered species to ensure their survival.\n\nThe DNA bank can be used to compare and analyze DNA samples. Comparison of DNA samples allowed scientists to work on the Human Genome Project, which maps out many of the genes on human DNA. It has also led to the development of preventative genetics. Samples from the DNA bank have been used to identify patterns and determine which genes lead to specific disorders. Once people know which genes lead to disorders, people can take steps to lessen the effects of that disorder. This can occur through adjustments in lifestyle, as demonstrated in preventive healthcare, or even through gene therapy. DNA can be banked at any time during a person's life.\n\nDNA banks were introduced to the criminal justice system in the 1980s. This system makes it possible to rule out or confirm the verdict of a suspect based on their personal genetic code. Once an individual’s DNA is stored, it remains in the system permanently; allowing law enforcement to identify and track criminals more easily. There is some controversy about this topic as some individuals believe the storage of citizen's DNA is an invasion of privacy.\n\nDNA banking capsules are also starting to be used for retaining the DNA of the deceased, a service offered by some funeral homes.\n\nScientists are capable of retrieving genetic information from hair, skin, blood, sperm, tissue, and saliva as long as the sample contains intact DNA. Nucleotide sequences between humans differ by only 0.1%. Even so, this 0.1% includes approximately three million bases. DNA can be analyzed through restriction fragment length polymorphism (RFLP) and Polymerase chain reactions (PCR). The RFLP process was introduced in 1988. Restriction enzymes digest portions of the DNA, leaving short fragments. These fragments are sorted through gel electrophoresis. The gel demonstrates the length of the fragments allowing specialists to determine whether the fragments came from the same person. PCR is more commonly used today because it more efficient and requires smaller samples of genetic samples. \n\nThere are various organizations founded for the purpose of storing and analyzing DNA sample. For example, the UK Biobank contains DNA samples of 500,000 individuals aged between 40 and 69 when their samples were taken in the years 2006-2010 .\n\n\n", "id": "2547545", "title": "DNA bank"}
{"url": "https://en.wikipedia.org/wiki?curid=48698195", "text": "Viability PCR\n\nViability PCR, also named v-PCR or vPCR, is an evolution of PCR. Through the use of a simple pre-treatment of the sample by the means of specific intercalating photo-reactive reagents it's possible to neutralize the DNA of dead cells. As a result, only DNA from live cells will be detected by PCR. This approach expands a lot the analytical scope of PCR procedures. The capability to detect only living cells become very important, because in key applications is more important to know the amount of live cells, than the total cell level. Examples of this are: food and water quality control, infectious diseases diagnostic, veterinary applications, ecological dynamics...\n\nThe first referenced work about this analytical approach was in 2003, Norwegian researchers suggest the use of Ethidium Monoazide, an azide form of Ethidium Bromide, which was used in other analytical fields as Flow Cytometry as a candidate for viability PCR. However, the main important advances were done by Nocker and colleagues, which demonstrated in successive works the potential of this technology and also suggested Propidium monoazide as a better reagent for vPCR.\n\nThis field still is on development, from 2003 up to 2015, the scientific evidences about the applicability of vPCR are stacking, nowadays main efforts are focused in procedure optimization. Since a simple reagent mix with the sample, photo-activation and subsequent PCR not always shows expected results, each procedure needs some optimization. Up to now the main improvements has been :\n\n- Improving the efficiency of photo activation: early procedures were based on high power halogen lamps which overheated the samples and don't ensured constant light dose, these home made solutions have been replaced by led based instruments.\n\n- The use of long PCR aplicons as targets.\n\n- The increase of temperature during dark incubation.\n\nThrough combining different optimizations strategies and controlling the analytical bias, nowadays the vPCR becomes a powerful analytical tool.\n", "id": "48698195", "title": "Viability PCR"}
{"url": "https://en.wikipedia.org/wiki?curid=47886489", "text": "Conservative transposition\n\nTransposition is the process by which a specific genetic sequence, known as a transposon, is moved from one location of the genome to another. Simple, or conservative transposition, is a non-replicative mode of transposition. That is, in conservative transposition the transposon is completely removed from the genome and reintegrated into a new, non-homologous locus, the same genetic sequence is conserved throughout the entire process. The site in which the transposon is reintegrated into the genome is called the target site. A target site can be in the same chromosome as the transposon or within a different chromosome. Conservative transposition uses the \"cut-and-paste\" mechanism driven by the catalytic activity of the enzyme transposase. Transposase acts like DNA scissors; it is an enzyme that cuts through double-stranded DNA to remove the transposon, then transfers and pastes it into a target site. \n\nA simple, or conservative, transposon refers to the specific genetic sequence that is moved via conservative transposition. These specific genetic sequences range in size, they can be hundreds to thousands of nucleotide base-pairs long. A transposon contains genetic sequences that encode for proteins that mediate its own movement, but can also carry genes for additional proteins. Transposase is encoded within the transposon DNA and used to facilitate its own movement, making this process self-sufficient within organisms. All simple transposons contain a transposase encoding region flanked by terminal inverted repeats, but the additional genes within the transposon DNA can vary. Viruses, for example, encode the essential viral transposase needed for conservative transposition as well as protective coat proteins that allow them to survive outside of cells, thus promoting the spread of mobile genetic elements.\n\nThe mechanism by which conservative transposition occurs is called the \"cut-and-paste\" method, which involves five main steps:\nBoth the excision and insertion of the transposon leaves single or double stranded gaps in the DNA, which are repaired by host enzymes such as DNA polymerase.\n\nCurrent researchers have developed gene transfer systems on the basis of conservative transposition which can integrate new DNA in both invertebrates and vertebrate genomes. Scientists alter the genetic sequence of a transposon in a laboratory setting, then insert this sequence into a vector which is then inserted into a target cell. The transposase coding region of these transposons is replaced by a gene of interest intended to be integrated into the genome. Conservative transposition is induced by the expression of transposase from another source within the cell, since the transposon no longer contains the transposase coding region to be self sufficient. Generally a second vector is prepared and inserted into the cell for expression of transposase. This technique is used in transgenesis and insertional mutagenesis research fields. The Sleeping Beauty transposon system is an example of gene transfer system developed for use in vertebrates. Further development in integration site preferences of transposable elements is expected to advance the technologies of human gene therapy.\n", "id": "47886489", "title": "Conservative transposition"}
