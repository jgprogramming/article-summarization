{"url": "https://en.wikipedia.org/wiki?curid=5376", "text": "Cladistics\n\nCladistics (from Greek , \"klados\", i.e., \"branch\") is an approach to biological classification in which organisms are categorized based on shared derived characteristics that can be traced to a group's most recent common ancestor and are not present in more distant ancestors. Therefore, members of a group are assumed to share a common history and are considered to be closely related.\n\nThe techniques and nomenclature of cladistics have been applied to other disciplines. (See phylogenetic nomenclature.)\n\nThe original methods used in cladistic analysis and the school of taxonomy derived from the work of the German entomologist Willi Hennig, who referred to it as phylogenetic systematics (also the title of his 1966 book); the terms \"cladistics\" and \"clade\" were popularized by other researchers. Cladistics in the original sense refers to a particular set of methods used in phylogenetic analysis, although it is now sometimes used to refer to the whole field.\n\nWhat is now called the cladistic method appeared as early as 1901 with a work by Peter Chalmers Mitchell for birds and subsequently by Robert John Tillyard (for insects) in 1921, and W. Zimmermann (for plants) in 1943.\nThe term \"clade\" was introduced in 1958 by Julian Huxley after having been coined by Lucien Cuénot in 1940, \"cladogenesis\" in 1958, \"cladistic\" by Cain and Harrison in 1960, \"cladist\" (for an adherent of Hennig's school) by Mayr in 1965, and \"cladistics\" in 1966. Hennig referred to his own approach as \"phylogenetic systematics\". From the time of his original formulation until the end of the 1970s, cladistics competed as an analytical and philosophical approach to phylogenetic inference with phenetics and so-called evolutionary taxonomy. Phenetics was championed at this time by the numerical taxonomists Peter Sneath and Robert Sokal and the evolutionary taxonomist Ernst Mayr.\n\nOriginally conceived, if only in essence, by Willi Hennig in a book published in 1950, cladistics did not flourish until its translation into English in 1966 (Lewin 1997). Today, cladistics is the most popular method for constructing phylogenies from morphological and molecular data. Unlike phenetics, cladistics is specifically aimed at reconstructing evolutionary histories.\n\nIn the 1990s, the development of effective polymerase chain reaction techniques allowed the application of cladistic methods to biochemical and molecular genetic traits of organisms, as well as to anatomical ones, vastly expanding the amount of data available for phylogenetics. At the same time, cladistics rapidly became the dominant set of methods of phylogenetics in evolutionary biology, because computers made it possible to process large quantities of data about organisms and their characteristics.\n\nThe way for computational phylogenetics was paved by phenetics, a set of methods commonly used from the 1950s to 1980s and to some degree later. Phenetics did not try to reconstruct phylogenetic trees; rather, it tried to build dendrograms from similarity data; its algorithms required less computer power than phylogenetic ones.\n\nThe cladistic method interprets each character state transformation implied by the distribution of shared character states among taxa (or other terminals) as a potential piece of evidence for grouping. The outcome of a cladistic analysis is a cladogram – a tree-shaped diagram (dendrogram) that is interpreted to represent the best hypothesis of phylogenetic relationships. Although traditionally such cladograms were generated largely on the basis of morphological characters and originally calculated by hand, genetic sequencing data and computational phylogenetics are now commonly used in phylogenetic analyses, and the parsimony criterion has been abandoned by many phylogeneticists in favor of more \"sophisticated\" but less parsimonious evolutionary models of character state transformation. Cladists contend that these models are unjustified.\n\nEvery cladogram is based on a particular dataset analyzed with a particular method. Datasets are tables consisting of molecular, morphological, ethological and/or other characters and a list of operational taxonomic units (OTUs), which may be genes, individuals, populations, species, or larger taxa that are presumed to be monophyletic and therefore to form, all together, one large clade; phylogenetic analysis infers the branching pattern within that clade. Different datasets and different methods, not to mention violations of the mentioned assumptions, often result in different cladograms. Only scientific investigation can show which is more likely to be correct.\n\nUntil recently, for example, cladograms like the following have generally been accepted as accurate representations of the ancestral relations among turtles, lizards, crocodilians, and birds:\n\nIf this phylogenetic hypothesis is correct, then the last common ancestor of turtles and birds, at the branch near the lived earlier than the last common ancestor of lizards and birds, near the . Most molecular evidence, however, produces cladograms more like this:\n\nIf this is accurate, then the last common ancestor of turtles and birds lived later than the last common ancestor of lizards and birds. Since the cladograms provide competing accounts of real events, at most one of them is correct.\n\nThe cladogram to the right represents the current universally accepted hypothesis that all primates, including strepsirrhines like the lemurs and lorises, had a common ancestor all of whose descendants were primates, and so form a clade; the name Primates is therefore recognized for this clade. Within the primates, all anthropoids (monkeys, apes and humans) are hypothesized to have had a common ancestor all of whose descendants were anthropoids, so they form the clade called Anthropoidea. The \"prosimians\", on the other hand, form a paraphyletic taxon. The name Prosimii is not used in phylogenetic nomenclature, which names only clades; the \"prosimians\" are instead divided between the clades Strepsirhini and Haplorhini, where the latter contains Tarsiiformes and Anthropoidea.\n\nThe following terms, coined by Hennig, are used to identify shared or distinct character states among groups:\n\nThe terms plesiomorphy and apomorphy are relative; their application depends on the position of a group within a tree. For example, when trying to decide whether the tetrapods form a clade, an important question is whether having four limbs is a synapomorphy of the earliest taxa to be included within Tetrapoda: did all the earliest members of the Tetrapoda inherit four limbs from a common ancestor, whereas all other vertebrates did not, or at least not homologously? By contrast, for a group within the tetrapods, such as birds, having four limbs is a plesiomorphy. Using these two terms allows a greater precision in the discussion of homology, in particular allowing clear expression of the hierarchical relationships among different homologous features.\n\nIt can be difficult to decide whether a character state is in fact the same and thus can be classified as a synapomorphy, which may identify a monophyletic group, or whether it only appears to be the same and is thus a homoplasy, which cannot identify such a group. There is a danger of circular reasoning: assumptions about the shape of a phylogenetic tree are used to justify decisions about character states, which are then used as evidence for the shape of the tree. Phylogenetics uses various forms of parsimony to decide such questions; the conclusions reached often depend on the dataset and the methods. Such is the nature of empirical science, and for this reason, most cladists refer to their cladograms as hypotheses of relationship. Cladograms that are supported by a large number and variety of different kinds of characters are viewed as more robust than those based on more limited evidence.\n\nMono-, para- and polyphyletic taxa can be understood based on the shape of the tree (as done above), as well as based on their character states. These are compared in the table below.\n\nCladistics, either generally or in specific applications, has been criticized from its beginnings. Decisions as to whether particular character states are homologous, a precondition of their being synapomorphies, have been challenged as involving circular reasoning and subjective judgements. Transformed cladistics arose in the late 1970s in an attempt to resolve some of these problems by removing phylogeny from cladistic analysis, but it has remained unpopular.\n\nHowever, homology is usually determined from analysis of the results that are evaluated with homology measures, mainly the CI (consistency index) and RI (retention index), which, it has been claimed, makes the process objective. Also, homology can be equated to synapomorphy, which is what Patterson has done.\n\nThe comparisons used to acquire data on which cladograms can be based are not limited to the field of biology. Any group of individuals or classes that are hypothesized to have a common ancestor, and to which a set of common characteristics may or may not apply, can be compared pairwise. Cladograms can be used to depict the hypothetical descent relationships within groups of items in many different academic realms. The only requirement is that the items have characteristics that can be identified and measured.\n\nAnthropology and archaeology: Cladistic methods have been used to reconstruct the development of cultures or artifacts using groups of cultural traits or artifact features.\n\nComparative mythology and folktale use cladistic methods to reconstruct the protoversion of many myths. Mythological phylogenies constructed with mythemes clearly support low horizontal transmissions (borrowings), historical (sometimes Palaeolithic) diffusions and punctuated evolution. They also are a powerful way to test hypotheses about cross-cultural relationships among folktales.\n\nLiterature: Cladistic methods have been used in the classification of the surviving manuscripts of the \"Canterbury Tales\", and the manuscripts of the Sanskrit \"Charaka Samhita\".\n\nHistorical linguistics: Cladistic methods have been used to reconstruct the phylogeny of languages using linguistic features. This is similar to the traditional comparative method of historical linguistics, but is more explicit in its use of parsimony and allows much faster analysis of large datasets (computational phylogenetics).\n\nTextual criticism or stemmatics: Cladistic methods have been used to reconstruct the phylogeny of manuscripts of the same work (and reconstruct the lost original) using distinctive copying errors as apomorphies. This differs from traditional historical-comparative linguistics in enabling the editor to evaluate and place in genetic relationship large groups of manuscripts with large numbers of variants that would be impossible to handle manually. It also enables parsimony analysis of contaminated traditions of transmission that would be impossible to evaluate manually in a reasonable period of time.\n\nAstrophysics infers the history of relationships between galaxies to create branching diagram hypotheses of galaxy diversification.\n\n\n", "id": "5376", "title": "Cladistics"}
{"url": "https://en.wikipedia.org/wiki?curid=29794000", "text": "Evolution of eusociality\n\nThe evolution of eusociality occurred repeatedly in different orders of animals, particularly the hymenoptera (the wasps, bees, and ants). This 'true sociality' in animals, in which sterile individuals work to further the reproductive success of others, is found in termites, ambrosia beetles, gall-dwelling aphids, thrips, marine sponge-dwelling shrimp (\"Synalpheus regalis\"), naked mole-rats (\"Heterocephalus glaber\"), and the insect order Hymenoptera (which includes bees, wasps, and ants). The fact that this behavior has evolved so many times in the hymenoptera (between 8 and 11 times ), but remains rare throughout the rest of the animal kingdom, has made the evolution of eusociality a topic of debate among evolutionary biologists. Eusocial organisms at first appear to behave in stark contrast with simple interpretations of Darwinian evolution: passing on one’s genes to the next generation, or fitness, is a central idea in evolutionary biology.\n\nCurrent theories propose that the evolution of eusociality occurred either due to kin selection, proposed by W.D. Hamilton, or by the competing theory of multilevel selection as proposed by E.O. Wilson and colleagues. No single trait or model is sufficient to explain the evolution of eusociality, and most likely the pathway to eusociality involved a combination of pre-conditions, ecological factors, and genetic influences.\n\nEusociality can be characterized by four main criteria: overlapping generations, cooperative brood care, philopatry, and reproductive altruism. Overlapping generations means that multiple generations live together, and that older offspring may help the parents raise their siblings. Cooperative brood care is when individuals other than the parents assist in raising the offspring through means such as food gathering and protection. Philopatry is when individuals remain living in their birthplace.\n\nThe final category, reproductive altruism, is the most divergent from other social orders. Altruism occurs when an individual performs a behavior that benefits a recipient in some way, but at the individual’s own expense. Reproductive altruism is one of the most extreme forms of altruism. This is when most members of the group give up their own breeding opportunities in order to participate in the reproductive success of other individuals. The individuals giving up their own reproductive success form a sterile caste of workers within the group. Each species that practices reproductive altruism is ruled by a queen, the only breeding female who is larger than the rest. The remainder of the society is composed of a few breeding males, sterile male and female workers, and the young.\n\nImagining the conditions in which eusociality could have evolved was a problem for Charles Darwin. In \"The Origin of Species\", he described the existence of sterile worker castes in the social insects as \"the one special difficulty, which at first appeared to me insuperable and actually fatal to my whole theory\". In the next paragraph of his book, Darwin describes a solution. If the trait of sterility can be carried by some individuals without expression, and those individuals that do express sterility help reproductive relatives, the sterile trait can persist and evolve.\n\nDarwin was on the right track, except sterility is not a characteristic shared among all eusocial animals. Sterile workers of many eusocial species are not actually physiologically sterile. Male workers can still produce sperm, and female workers sometimes lay eggs, and in some species, become the new queen if the old one dies (observed in Hymenoptera, termites, and shrimp).\n\nThis insight led to inclusive fitness and kin selection becoming important theories during the 20th century to help explain eusociality. Inclusive fitness is described as a combination of one’s own reproductive success and the reproductive success of others that share similar genes. Animals may increase their inclusive fitness through kin selection. Kin selection is when individuals help close relatives with their reproduction process, seemingly because relatives will propagate some of the individual’s own genes. Kin selection follows Hamilton's Rule, which suggests that if the benefit of a behavior to a recipient, taking into account the genetic relatedness of the recipient to the altruist, outweighs the costs of the behavior to the altruist, then it is in the altruist's genetic advantage to perform the altruistic behavior.\n\nWilliam D. Hamilton proposed that eusociality arose in social Hymenoptera by kin selection because of their interesting genetic sex determination trait of haplodiploidy. Because males are produced by parthenogenesis (they come from unfertilized eggs and thus only have one set of chromosomes), and females are produced from fertilized eggs, sisters from a singly-mated mother share 75% of their genes, whereas mothers share only 50% of their genes with their offspring. Thus, sisters will propagate their own genes more by helping their mothers to raise more sisters, than to leave the nest and raise their own daughters.\n\nThough Hamilton's argument appears to work nicely for Hymenoptera, it excludes diploid eusocial organisms (inter-sibling relatedness ≤ parent-offspring relatedness = 0.5). Even in haplodiploid systems, the average relatedness between sisters falls off rapidly when a queen mates with multiple males (r=0.5 for 2 mates, and even lower for more). Moreover, males share only 25% of their sisters' genes, and, in cases of equal sex ratios, females are related to their siblings on average by 0.5 which is no better than raising their own offspring. However, despite the shortcomings of the haplodiploidy hypothesis, it is still considered to have some importance. For example, many bees have female-biased sex ratios and/or invest less in or kill males. Analysis has shown that in Hymenoptera, the ancestral female was monogamous in each of the eight independent cases where eusociality evolved. This indicates that the high relatedness between sisters favored the evolution of eusociality during the initial stages on several occasions. This helps explain the abundance of eusocial genera within the order Hymenoptera, including three separate origins within halcitid bees alone.\n\nThe monogamy hypothesis, formulated by Jacobus Boomsma in 2007, is currently the leading hypothesis concerning the initial evolution of eusociality in the Hymenoptera. It uses Hamilton's kin selection approach in a way that applies to both haploid and diploid organisms. If a queen is lifetime-strictly monogamous - in other words, she mates with only one individual during her entire life - her progeny will be equally related to their siblings and to their own offspring (r=0.5 in both cases - this is an average of sisters [0.75] and brothers [0.25]). Thus, natural selection will favor cooperation in any situation where it's more efficient to raise siblings than offspring, and this could start paving a path towards eusociality. This higher efficiency becomes especially pronounced after group living evolves.\n\nIt's important to note that in many monogamous animals, an individual's death prompts its partner to look for a new mate, which would affect relatedness and hinder the evolution of eusociality (workers would be much more related to their offspring than their siblings). However, many ants, bees, and wasps have an interesting form of lifetime monogamy in which the queen mates with a single male, who then dies before colony founding. This extreme sort of lifetime monogamy seems to be the ancestral state in all Hymenopteran lineages that have evolved eusociality. Most termites also have a unique mating system in which a reproductive female (the queen) commits to a single male for life (the king), and this pattern seems to be ancestral in termites. Lastly, researchers now claim that strict monogamy facilitated eusociality in the sponge-dwelling shrimp.\n\nIn species where philopatry predominates, and there are few emigrants to the nest, intense inbreeding can occur, as is the case in eusocial species. Inbreeding can mimic and even surpass the effects of haplodiploidy. Siblings may actually share greater than 75% of their genes. Like in haplodiploidy kin selection, the individuals can propagate their own genes more through the promotion of more siblings, rather than their own offspring.\n\nIn termites, two additional hypotheses have been proposed. The first is called the Chromosomal Linkage Hypothesis, where a large part of the termite genome is sex-linked. This makes sisters related somewhat above 50%, and brothers somewhat above 50%, but brother-sister relatedness less than 50%. Termite workers might then bias their cooperative brood care towards their own sex. This hypothesis also mimics the effects of haplodiploidy, but proposes that males would help raise only the queen's male offspring, while females would only care for the queen's female offspring.\n\nThe symbiont hypothesis in termites is quite different from the others. With each molt, termites lose the lining of their hindgut and the subsequent bacteria and protozoa that colonize their guts for cellulose digestion. They depend on interactions with other termites for their gut to be recolonized, thus forcing them to become social. This could be a precursor, or pre-condition for why eusociality evolved in termites.\n\nAlthough the symbiont hypothesis serves as a pre-condition for termites to evolve into eusocial societies, scientists have found two crucial pre-conditions for the evolution of eusociality across all species. These include: 1. Altricial offspring (require large amounts of parental care to reach maturity); 2. Low reproductive success rates of solitary pairs that attempt to reproduce. These pre-conditions led to the two lifestyle characteristics that are observed in all eusocial species: nest building and extensive parental care.\n\nEcological factors were also probably a precursor to eusociality. For example, the sponge-dwelling shrimp depend upon the sponge's feeding current for food, termites depend upon dead, decaying wood, and naked mole rats depend upon tubers in the ground. Each of these resources has patchy distributions throughout the environments of these animals. This means there is a high cost to dispersing (individual may not find another source before it starves), and these resources must be defended for the group to survive. These requirements make it a necessity to have high social order for the survival of the group.\n\nGenetic constraints may have influenced the evolution of eusociality. The genome structure of the order Hymenoptera has been found to have the highest recombination rates of any other groups in Animalia. The eusocial genus \"Apis\", the honeybees, have the highest recombination rate in higher eukaryotes. Genes determining worker behavior and division of labor have been found in regions of the \"Apis\" genome with the highest rates of recombination and molecular evolution. These mechanisms are likely important to the evolution of eusociality because high recombination rates are associated with the creation of novel genes, upon which natural selection can act. This could have been important in other eusocial genera. Biased gene conversion rates are also higher in eusocial species. This could increase genotypic diversity, which could allow workers to meet the demands of a changing social structure more easily. Another hypothesis is that the lower overall genetic diversity as eusociality levels increase throughout the family \"Apidae\"is due to a decreased exposure to parasites and pathogens.\n\nEusociality appears to be maintained through manipulation of the sterile workers by the queen. The mechanisms for this include hormonal control through pheromones, restricting food to young in order to control their size, consumption of any eggs laid by females other than the queen, and behavioral dominance. In naked mole rats, this behavioral dominance occurs in the form of the queen facing the worker head-to-head, and shoving it throughout the tunnels of the naked mole rats' burrow for quite a distance.\n\nNowak, et al. (2010) outlines a path by which eusociality could evolve by means of multi-level (group) selection in five steps:\n\nNowak's paper, however, received major criticisms for erroneously separating inclusive fitness theory from \"standard natural selection\". Over 150 authors replied arguing that Nowak, et al. misrepresent 40 years of empirical literature.\n", "id": "29794000", "title": "Evolution of eusociality"}
{"url": "https://en.wikipedia.org/wiki?curid=607464", "text": "Primordial soup\n\nPrimordial soup, or prebiotic soup, is a hypothetical condition of the Earth's atmosphere before the emergence of life. It is a chemical environment in which the first biological molecules (organic compounds) were formed under natural forces. According to the theory, simple organic compounds were created from non-living inorganic molecules (abiogenesis) through physical and chemical reactions on the Earth's surface. The so formed organic molecules accumulate into a rich organic ocean, or a \"soup\". In this soup, simple organic molecules reacted with each other (polymerise) to form more complex molecules, including nucleic acids and proteins, which are the central structural and functional components of all organisms. These molecules then aggregate to become the first forms of life.\n\nA British naturalist Charles Darwin had vaguely imagined the primordial soup as a \"warm little pond\" in 1871. A coherent scientific argument was introduced by a Soviet biochemist Alexander Oparin in 1924. According to Oparin, in the primitive Earth's surface, carbon, hydrogen, water vapour, and ammonia reacted to form the first organic compounds. Unbeknown to Oparin, whose writing was circulated only in Russian, an English scientist John Burdon Sanderson Haldane independently arrived at similar conclusion in 1929. It was Haldane who gave the name \"soup\" to the theory.\n\nThe theory is variously known as \"primordial soup theory\", \"prebiotic soup theory\", and \"Oparin-Haldane hypothesis\". Biochemist Robert Shapiro has summarized the theory in its \"mature form\" as follows:\n\n\nThe notion that living beings originated from inanimate materials originated among the Ancient Greeks—the theory known as spontaneous generation. Aristotle in the 4th century BCE gave a proper explantion, writing:\n\nHe also states that it is not only that animals originate from other similar animals, but also that living things do arise and always have arisen from lifeless matter. His theory remained the dominant idea on origin of life from the ancient philosophers to the Renaissance thinkers in various forms. But with the birth of modern science, experimental refutations emerged. An Italian physician Francesco Redi demonstrated in 1668 that maggots developed from rotten meat only in a jar where flies could enter, but not in close-lid jar. He concluded that: \"omne vivum ex vivo\" (All life comes from life).\n\nThe experiment of a French chemist Louis Pasteur in 1859 is regarded as the death blow to spontaneous generation. He experimentally showed that organisms (microbes) can not grow in a sterilised water, unless it is exposed to air. The experiment won him the Alhumbert Prize in 1862 from the French Academy of Sciences, and he concluded: Never will the doctrine of spontaneous generation recover from the mortal blow of this simple experiment.\n\nBut evolutionary biologists believed that a kind of spontaneous generation, but different from the simple Aristotelian doctrine, must have worked for the emergence of life. A French biologist Jean-Baptiste de Lamarck had speculated that the first life form started from non-living materials. \"Nature, by means of heat, light, electricity and moisture, he wrote in 1809 in \"Philosophie Zoologique\" (\"The Philosophy of Zoology\"), \"forms direct or spontaneous generation at that extremity of each kingdom of living bodies, where the simplest of these bodies are found.\" When an English naturalist Charles Darwin introduced his theory of natural selection in his book \"On the Origin of Species\" in 1859, and even in his subsequent books, his supporters, such as a German zoologist Ernst Haeckel, criticised him for not using his theory to explain the origin of life. Haeckel wrote in 1862: \"The chief defect of the Darwinian theory is that it throws no light on the origin of the primitive organism—probably a simple cell—from which all the others have descended. When Darwin assumes a special creative act for this first species, he is not consistent, and, I think, not quite sincere.\"\n\nBut Darwin wrote (in a personal letter to Joseph Dalton Hooker) in 1871, expressing his idea on the origin of life as:\nAlexander Oparin first postulated his theory in Russian in 1924 in a small pamphlet titled \"Proiskhozhdenie Zhizny \" (\"The Origin of Life\"). According to Oparin, the primitive Earth's surface had a thick red-hot liquid, composed of heavy elements such as carbon (in the form of iron carbide). This nucleus was surrounded by lightest elements, i.e. gases, such as hydrogen. In the presence of water vapour, carbides reacted with hydrogen to form hydrocarbons. Such hydrocarbons were the first organic molecules. These further combined with oxygen and ammonia to produce hydroxy- and amino-derivatives, such as carbohydrates and proteins. These molecules accumulated on the ocean's surface, becoming gel-like substances and growing in size. They gave rise to primitive organisms (cells). In his original theory, Oparin considered oxygen as one of the primordial gases; thus the primordial atmosphere was an oxidising one. However, when he elaborated his theory in 1936 (in a book by the same title, and translated into English in 1938), he modified the chemical composition of primordial environment as strictly reducing, consisting of methane, ammonia, free hydrogen and water vapour—excluding oxygen.\n\nJ.B.S. Haldane independently postulated his primordial soup theory in 1929 in an eight-page article \"The origin of life\" in \"The Rationalist Annual\". According to Haldane the prmitive Earth's atmosphere was essentially reducing, with little or no oxygen. Ultraviolet ray from the Sun induced reaction on a mixture of water, carbon dioxide, and ammonia. Organic substances such as sugars and protein components (amino acids) were synthesised. These molecules \"accumulated till the primitive oceans reached the consistency of hot dilute soup.\" The first reproducing things were created from this soup.\n\nAs to the priority over the theory, Haldane accepted that Oparin came first, saying, \"I have very little doubt that Professor Oparin has the priority over me.\"\n\nOne of the most important pieces of experimental support for the \"soup\" theory came in 1953. A graduate student, Stanley Miller, and his professor, Harold Urey, performed an experiment that demonstrated how organic molecules could have spontaneously formed from inorganic precursors, under conditions like those posited by the Oparin-Haldane Hypothesis. The now-famous \"Miller–Urey experiment\" used a highly reduced mixture of gases—methane, ammonia and hydrogen—to form basic organic monomers, such as amino acids. This provided direct experimental support for the second point of the \"soup\" theory, and it is around the remaining two points of the theory that much of the debate now center\n\nApart from the Miller–Urey experiment, the next most important step in research on prebiotic organic synthesis was the demonstration by Joan Oró that the nucleic acid purine base, adenine, was formed by heating aqueous ammonium cyanide solutions. In support of abiogenesis in eutectic ice, more recent work demonstrated the formation of s-triazines (alternative nucleobases), pyrimidines (including cytosine and uracil), and adenine from urea solutions subjected to freeze-thaw cycles under a reductive atmosphere (with spark discharges as an energy source).\n\nThe spontaneous formation of complex polymers from abiotically generated monomers under the conditions posited by the \"soup\" theory is not at all a straightforward process. Besides the necessary basic organic monomers, compounds that would have prohibited the formation of polymers were formed in high concentration during the Miller–Urey and Oró experiments. The Miller experiment, for example, produces many substances that would undergo cross-reactions with the amino acids or terminate the peptide chain.\n\n", "id": "607464", "title": "Primordial soup"}
{"url": "https://en.wikipedia.org/wiki?curid=15587739", "text": "Haplotype convergence\n\nHaplotype convergence is the unrelated appearance of identical haplotypes in separate populations, through either convergent evolution or random chance. \n\nHaplotype convergence is rare, due to the sheer odds involved of two unrelated individuals independently evolving the exact same genetic sequence in the site of interest. Thus, haplotypes are shared mainly between very closely related individuals, as the genetic information in two related individuals will be much more similar than between unrelated individuals. Substitution bias further increases the likelihood of haplotype convergence, as this increases the probability of mutations occurring at the same site. Sequences may also diverge from the same original sequence and then revert, converging in this manner. Convergence through convergent evolution in two unrelated groups is much less common, as derived traits may arise through dramatically different pathways.\n\nErroneously determining two individuals do be identical due to haplotype convergence becomes much less likely when more genetic markers are tested, since that would require a larger amount of extremely rare coincidences. With modern high-throughput sequencing approaches, sequencing a large set of markers, or even the entire genome, is much more feasible and greatly minimizes these issues. \n\nIn some regions, due to low diversity in the Y-STR gene (often used to study surname origin), haplotype convergence may confuse analyses, concluding unrelated individuals to be very closely related.\n\nSimilarly, a study of New World mitochondrial DNA haplogroups observed that similarities in haplotypes between Native Americans and Asians were a result of the hypervariability of the HVSI region in mitochondrial DNA, rather than common ancestry.\n\nAs an example of haplotype convergence due to convergent evolution in more distantly related groups, threespine stickleback in blackwater environments similar to that of the ancient bluefin killifish and black bream independently evolved the same haplotype in the SWS2 gene, which promotes better eyesight in those conditions.\n", "id": "15587739", "title": "Haplotype convergence"}
{"url": "https://en.wikipedia.org/wiki?curid=1318175", "text": "Signalling theory\n\nWithin evolutionary biology, signalling theory is a body of theoretical work examining communication between individuals, both within species and across species. The central question is when organisms with conflicting interests, such as in sexual selection, should be expected to provide honest signals (no presumption being made of conscious intention) rather than cheating. Mathematical models describe how signalling can contribute to an evolutionarily stable strategy.\n\nSignals are given in contexts such as mate selection by females, which subjects the advertising males' signals to selective pressure. Signals thus evolve because they modify the behaviour of the receiver to benefit the signaller. Signals may be honest, conveying information which usefully increases the fitness of the receiver, or dishonest. An individual can cheat by giving a dishonest signal, which might briefly benefit that signaller, at the risk of undermining the signalling system for the whole population.\n\nThe question of whether selection of signals works at the level of the individual organism or gene, or at the level of the group, has been debated by biologists such as Richard Dawkins, arguing that individuals evolve to signal and to receive signals better, including resisting manipulation. Amotz Zahavi suggested that cheating could be controlled by the handicap principle, where the best horse in a handicap race is the one carrying the largest handicap weight. According to Zahavi's theory, signallers such as male peacocks have 'tails' that are genuinely handicaps, being costly to produce. The system is evolutionarily stable as the large showy tails are honest signals. Biologists have attempted to verify the handicap principle, but with inconsistent results. The mathematical biologist Ronald Fisher analysed the contribution that having two copies of each gene (diploidy) would make to honest signalling, demonstrating that a runaway effect could occur in sexual selection. The evolutionary equilibrium depends sensitively on the balance of costs and benefits.\n\nThe same mechanisms can be expected in humans, where researchers have studied behaviours including risk taking by young men, hunting of large game animals, and costly religious rituals, finding that these appear to qualify as costly honest signals.\n\nWhen animals choose mates, traits such as signalling are subject to evolutionary pressure. For example, the male gray tree frog, \"Hyla versicolor\", produces a call to attract females. Once a female chooses a mate, this selects for a specific style of male calling, thus propagating a specific signalling ability. The signal can be the call itself, the intensity of a call, its variation style, its repetition rate, and so on. Various hypotheses seek to explain why females would select for one call over the other. The sensory exploitation hypothesis proposes that pre-existing preferences in female receivers can drive the evolution of signal innovation in male senders, in a similar way to the hidden preference hypothesis which proposes that successful calls are better able to match some 'hidden preference' in the female. Signallers have sometimes evolved multiple sexual ornaments, and receivers have sometimes evolved multiple trait preferences.\n\nIn biology, signals are traits, including structures and behaviours, that have evolved specifically because they change the behaviour of receivers in ways that benefit the signaller. Traits or actions that benefit the receiver exclusively are called cues. When an alert bird deliberately gives a warning call to a stalking predator and the predator gives up the hunt, the sound is a signal. When a foraging bird inadvertently makes a rustling sound in the leaves that attracts predators and increases the risk of predation, the sound is a 'cue'.\n\nSignalling systems are shaped by mutual interests between signallers and receivers. An alert bird such as a Eurasian jay warning off a stalking predator is communicating something useful to the predator: that it has been detected by the prey; it might as well quit wasting its time stalking this alerted prey, which it is unlikely to catch. When the predator gives up, the signaller can get back to other tasks such as feeding. Once the stalking predator is detected, the signalling prey and receiving predator thus have a mutual interest in terminating the hunt.\n\nWithin species, mutual interests increase with kinship. Kinship is central to models of signalling between relatives, for instance when broods of nestling birds beg and compete for food from their parents.\n\nThe term honesty in animal communication is controversial because in non-technical usage it implies intent, to discriminate deception from honesty in human interactions. However, biologists use the phrase \"honest signals\" in a direct, statistical sense. Biological signals, like warning calls or resplendent tail feathers, are honest if they truly convey useful information to the receiver. That is, the signal trait conveys to the receiver the presence of an otherwise unobservable factor. Honest biological signals do not need to be perfectly informative, reducing uncertainty to zero; all they need to be useful is to be correct \"on average\", so that certain behavioural responses to the signal are advantageous, statistically, compared to the behaviour that would occur in absence of the signal. Ultimately the value of the signalled information depends on the extent to which it allows the receiver to increase its fitness. Hence, \"honest\" signals are evolutionarily stable.\n\nOne class of honest signal is the aposematic warning signal, generally visual, given by poisonous or dangerous animals such as wasps, poison dart frogs, and pufferfish. Warning signals are honest indications of noxious prey, because conspicuousness evolves in tandem with noxiousness. Thus, the brighter and more conspicuous the organism, the more toxic it usually is. The most common and effective colours are red, yellow, black and white.\n\nBecause there are both mutual and conflicting interests in most animal signalling systems, a central problem in signalling theory is dishonesty or cheating. For example, if foraging birds are safer when they give a warning call, cheats could give false alarms at random, just in case a predator is nearby. But too much cheating could cause the signalling system to collapse. Every dishonest signal weakens the integrity of the signalling system, and so reduces the fitness of the group. An example of dishonest signalling comes from Fiddler crabs such as \"Uca lactea mjoebergi\", which have been shown to bluff (no conscious intention being implied) about their fighting ability. When a claw is lost, a crab occasionally regrows a weaker claw that nevertheless intimidates crabs with smaller but stronger claws. The proportion of dishonest signals is low enough for it not to be worthwhile for crabs to test the honesty of every signal through combat.\n\nRichard Dawkins and John Krebs in 1978 considered whether individuals of the same species would act as if attempting to deceive each other. They applied a \"selfish gene\" view of evolution to animals' threat displays to see if it would be in their genes' interests to give dishonest signals. They criticised previous ethologists, such as Nikolaas Tinbergen and Desmond Morris for suggesting that such displays were \"for the good of the species\". They argued that such communication ought to be viewed as an evolutionary arms race in which signallers evolve to become better at manipulating receivers, while receivers evolve to become more resistant to manipulation. The game theoretical model of the war of attrition similarly suggests that threat displays ought not to convey any reliable information about intentions.\n\nIn 1975, Amotz Zahavi proposed a verbal model for how signal costs could constrain cheating and stabilize an \"honest\" correlation between observed signals and unobservable qualities, based on an analogy to sports handicapping systems. He called this idea the handicap principle. The purpose of a sports handicapping system is to reduce disparities in performance, making the contest more competitive. In a handicap race, intrinsically faster horses are given heavier weights to carry under their saddles. Similarly, in amateur golf, better golfers have fewer strokes subtracted from their raw scores. This creates correlations between the handicap and unhandicapped performance, and if the handicaps work as they are supposed to, between the handicap and handicapped performance. If you knew nothing about two race horses or two amateur golfers except their handicaps, you could infer which is most likely to win: the horse with the bigger weight handicap, and the golfer with the smaller stroke handicap. By analogy, if peacock 'tails' (large tail covert feathers) act as a handicapping system, and a peahen knew nothing about two peacocks but the sizes of their tails, she could \"infer\" that the peacock with the bigger tail has greater unobservable intrinsic quality. Display costs can include extrinsic social costs, in the form of testing and punishment by rivals, as well as intrinsic production costs. Another example given in textbooks is the extinct Irish elk, \"Megaloceros giganteus\". The male Irish elk's enormous antlers could perhaps have evolved as displays of ability to overcome handicap, though biologists point out that if the handicap is inherited, its genes ought to be selected against.\nThe essential idea here is intuitive and probably qualifies as folk wisdom. It was articulated by Kurt Vonnegut in his 1961 short story \"Harrison Bergeron\". In Vonnegut’s futuristic dystopia, the Handicapper General uses a variety of handicapping mechanisms to reduce inequalities in performance. A spectator at a ballet comments: \"it was easy to see that she was the strongest and most graceful of all dancers, for her handicap bags were as big as those worn by two hundred pound men.\" Zahavi interpreted this analogy to mean that higher quality peacocks with bigger tails are signalling their ability to \"waste\" more of some resource by trading it off for a bigger tail. This resonates with Thorstein Veblen's idea that conspicuous consumption and extravagant status symbols can signal wealth.\nZahavi’s conclusions rest on his verbal interpretation of a metaphor, and initially the handicap principle was not well received by evolutionary biologists. However, in 1984, Nur and Hasson used life history theory to show how differences in signalling costs, in the form of survival-reproduction tradeoffs, could stabilize a signalling system roughly as Zahavi imagined. Genetic models also suggested this was possible. In 1990 Alan Grafen showed that a handicap-like signalling system was evolutionarily stable if higher quality signallers paid lower marginal survival costs for their signals.\n\nIn 1982, W.D. Hamilton proposed a specific but widely applicable handicap mechanism, parasite-mediated sexual selection. He argued that in the never-ending co-evolutionary race between hosts and their parasites, sexually selected signals indicate health. This idea was tested in 1994 in barn swallows, a species where males have long tail streamers. Møller found that the males with longer tails, and their offspring, did have fewer bloodsucking mites, whereas fostered young did not. The effect was therefore genetic, confirming Hamilton's theory.\n\nAnother example is Lozano's hypothesis that carotenoids have dual but mutually incompatible roles in immune function and signalling. Given that animals cannot synthesize carotenoids \"de novo\", these must be obtained from food. The hypothesis states that animals with carotenoid-depended sexual signals are demonstrating their ability to \"waste\" carotenoids on sexual signals at the expense of their immune system.\n\nThe handicap principle has proven hard to test empirically, partly because of inconsistent interpretations of Zahavi’s metaphor and Grafen’s marginal fitness model, and partly because of conflicting empirical results: in some studies individuals with bigger signals seem to pay higher costs, in other studies they seem to be paying lower costs. A possible explanation for the inconsistent empirical results is given in a series of papers by Getty, who shows that Grafen’s proof of the handicap principle is based on the critical simplifying assumption that signallers trade off costs for benefits in an additive fashion, the way humans invest money to increase income in the same currency. But the assumption that costs and benefits trade off in an additive fashion is true only on a logarithmic scale; for the survival cost – reproduction benefit tradeoff is assumed to mediate the evolution of sexually selected signals. Fitness depends on producing offspring, which is a multiplicative function of reproductive success given an individual is still alive times the probability of still being alive, given investment in signals.\n\nThe effort to discover how costs can constrain an \"honest\" correlation between observable signals and unobservable qualities within signallers is built on strategic models of signalling games, with many simplifying assumptions. These models are most often applied to sexually selected signalling in diploid animals, but they rarely incorporate a fact about diploid sexual reproduction noted by the mathematical biologist Ronald Fisher in the early 20th century: if there are \"preference genes\" correlated with choosiness in females as well as \"signal genes\" correlated with display traits in males, choosier females should tend to mate with showier males. Over generations, showier sons should also carry genes associated with choosier daughters, and choosier daughters should also carry genes associated with showier sons. This can cause the evolutionary dynamic known as Fisherian runaway, in which males become ever showier. Russell Lande explored this with a quantitative genetic model, showing that Fisherian diploid dynamics are sensitive to signalling and search costs. Other models incorporate both costly signalling and Fisherian runaway. These models show that if fitness depends on both survival and reproduction, having sexy sons and choosy daughters (in the stereotypical model) can be adaptive, increasing fitness just as much as having healthy sons and daughters.\n\nSam Brown and W. D. Hamilton and Marco Archetti proposed that autumn leaf colour is a signal from trees to aphids and other pest species that migrate in autumn to the trees. In their theory, bright autumn coloration with pinks and yellows is costly to trees because pigments require energy to synthesize, but the investment may help them to reduce their parasite load.\n\nStotting, for example in Thomson's Gazelle, is cited as an example of signalling: the gazelles jump close to a predator instead of escaping, in what could be a signal of strength.\n\nHuman behaviour may also provide examples of costly signals. In general, these signals provide information about a person’s phenotypic quality or cooperative tendencies. Evidence for costly signalling has been found in many areas of human interaction including risk taking, hunting, and religion.\n\nLarge game hunting has been studied extensively as a signal of men’s willingness to take physical risks, as well as showcase strength and coordination. Costly signalling theory is a useful tool for understanding food sharing among hunter gatherers because it can be applied to situations in which delayed reciprocity is not a viable explanation. Instances that are particularly inconsistent with the delayed reciprocity hypothesis are those in which a hunter shares his kill indiscriminately with all members of a large group. In these situations, the individuals sharing meat have no control over whether or not their generosity will be reciprocated, and free riding becomes an attractive strategy for those receiving meat. Free riders are people who reap the benefits of group-living without contributing to its maintenance. Fortunately, costly signalling theory can fill some of the gaps left by the delayed reciprocity hypothesis. Hawkes has suggested that men target large game and publicly share meat to draw social attention or to show off. Such display and the resulting favorable attention can improve a hunter’s reputation by providing information about his phenotypic quality. High quality signallers are more successful in acquiring mates and allies. Thus, costly signalling theory can explain apparently wasteful and altruistic behaviour.\n\nIn order to be effective, costly signals must fulfill specific criteria. Firstly, signallers must incur different levels of cost and benefit for signalling behaviour. Secondly, costs and benefits must reflect the signallers’ phenotypic quality. Thirdly, the information provided by a signal should be directed at and accessible to an audience. A receiver can be anyone who stands to benefit from information the signaller is sending, such as potential mates, allies, or competitors. Honesty is guaranteed when only individuals of high quality can pay the (high) costs of signalling. Hence, costly signals make it impossible for low-quality individuals to fake a signal and fool a receiver.\nBliege Bird et al. observed turtle hunting and spear fishing patterns in a Meriam community in the Torres Strait of Australia, publishing their findings in 2001. Here, only some Meriam men were able to accumulate high caloric gains for the amount of time spent turtle hunting or spear fishing (reaching a threshold measured in kcal/h). Since a daily catch of fish is carried home by hand and turtles are frequently served at large feasts, members of the community know which men most reliably brought them turtle meat and fish. Thus, turtle hunting qualifies as a costly signal. Furthermore, turtle hunting and spear fishing are actually less productive (in kcal/h) than foraging for shellfish, where success depends only on the amount of time dedicated to searching, so shellfish foraging is a poor signal of skill or strength. This suggests that energetic gains are not the primary reason men take part in turtle hunting and spear fishing. A follow-up study found that successful Meriam hunters do experience greater social benefits and reproductive success than less skilled hunters.\n\nThe Hadza people of Tanzania also share food, possibly to gain in reputation. Hunters cannot be sharing meat mainly to provision their families or to gain reciprocal benefits, as teenage boys often give away their meat even though they do not yet have wives or children, so costly signalling of their qualities is the likely explanation. These qualities include good eyesight, coordination, strength, knowledge, endurance, or bravery. Hadza hunters more often pair with highly fertile, hard-working wives than non-hunters. A woman benefits from mating with a man who possesses such qualities as her children will most likely inherit qualities that increase fitness and survivorship. She may also benefit from her husband’s high social status. Thus, hunting is an honest and costly signal of phenotypic quality.\n\nAmong the men of Ifaluk atoll, costly signalling theory can also explain why men torch fish. Torch fishing is a ritualized method of fishing on Ifaluk whereby men use torches made from dried coconut fronds to catch large dog-toothed tuna. Preparation for torch fishing requires significant time investments and involves a great deal of organization. Due to the time and energetic costs of preparation, torch fishing results in net caloric losses for fishers. Therefore, torch fishing is a handicap that serves to signal men’s productivity. Torch fishing is the most advertised fishing occupation on Ifaluk. Women and others usually spend time observing the canoes as they sail beyond the reef. Also, local rituals help to broadcast information about which fishers are successful and enhance fishers’ reputations during the torch fishing season. Several ritual behavioural and dietary constraints clearly distinguish torch fishers from other men. First, males are only permitted to torch fish if they participated on the first day of the fishing season. The community is well informed as to who participates on this day, and can easily identify the torch fishers. Second, torch fishers receive all of their meals at the canoe house and are prohibited from eating certain foods. People frequently discuss the qualities of torch fishermen. On Ifaluk, women claim that they are looking for hard-working mates. With the distinct sexual division of labor on Ifaluk, industriousness is a highly valued characteristic in males. Torch fishing thus provides women with reliable information on the work ethic of prospective mates, which makes it an honest costly signal.\n\nIn many human cases, a strong reputation built through costly signalling enhances a man’s social status over the statuses of men who signal less successfully. Among northern Kalahari foraging groups, traditional hunters usually capture a maximum of two or three antelopes per year. It was said of a particularly successful hunter:\nAlthough this hunter was sharing meat, he was not doing so in the framework of reciprocity. The general model of costly signalling is not reciprocal; rather, individuals who share acquire more mates and allies. Costly signalling applies to situations in Kalahari foraging groups where giving often goes to recipients who have little to offer in return. A young hunter is motivated to impress community members with daughters so that he can obtain his first wife. Older hunters may wish to attract women interested in an extramarital relationship, or to be a co-wife. In these northern Kalahari groups, the killing of a large animal indicates a man who has mastered the art of hunting and can support a family. Generally, many women seek a man who is a good hunter, has an agreeable character, is generous, and has advantageous social ties. Since hunting ability is a prerequisite for marriage, men who are good hunters enter the marriage market earliest. Costly signalling theory explains seemingly wasteful foraging displays.\n\nCostly signalling can be applied to situations involving physical strain and risk of physical injury or death. Research on physical risk taking is important because information regarding why people, especially young men, take part in high risk activities can help in the development of prevention programs. Reckless driving is a lethal problem among young men in western societies. A male who takes a physical risk is sending the message that he has enough strength and skill to survive extremely dangerous activities. This signal is directed at peers and potential mates.\n\nIn a study of risk taking, some types of risk, such as physical or heroic risk for others' benefit, are viewed more favorably than other types of risk, such as taking drugs. Males and females valued different degrees of heroic risk for mates and same-sex friends. Males valued heroic risk taking by male friends, but preferred less of it in female mates. Females valued heroic risk taking in male mates and less of it in female friends. Females may be attracted to males inclined to physically defend them and their children. Males may prefer heroic risk taking by male friends as they could be good allies.\n\nIn western societies, voluntary blood donation is a common, yet less extreme, form of risk taking. Costs associated with these donations include pain and risk of infection. If blood donation is an opportunity to send costly signals, then donors will be perceived by others as generous and physically healthy. In a survey, both donors and non-donors expressed perceptions of the health, generosity, and ability of blood donors to operate in stressful situations.\n\nCostly religious rituals such as male circumcision, food and water deprivation, and snake handling look paradoxical in evolutionary terms. Devout religious beliefs wherein such traditions are practiced therefore appear maladaptive. Religion may have arisen to increase and maintain intragroup cooperation. Cooperation leads to altruistic behaviour, and costly signalling could explain this. All religions may involve costly and elaborate rituals, performed publicly, to demonstrate loyalty to the religious group. In this way, group members increase their allegiance to the group by signalling their investment in group interests. However, as group size increases among humans, the threat of free riders grows. Costly signalling theory accounts for this by proposing that these religious rituals are costly enough to deter free riders.\n\nIrons proposed that costly signalling theory could explain costly religious behaviour. He argued that hard-to-fake religious displays enhanced trust and solidarity in a community, producing emotional and economic benefits. He showed that display signals among the Yomut Turkmen of northern Iran helped to secure trade agreements. These \"ostentatious\" displays signalled commitment to Islam to strangers and group members. Sosis demonstrated that people in religious communities are four times more likely to live longer than their secular counterparts, and that these longer lifespans were positively correlated with the number of costly requirements demanded from religious community members. However, confounding variables may not have been excluded. Wood found that religion offers a subjective feeling of well-being within a community, where costly signalling protects against free riders and helps to build self-control among committed members. Iannaccone studied the effects of costly signals on religious communities. In a self-reported survey, as the strictness of a church increased, the attendance and contributions to that church increased proportionally. In effect, people were more willing to participate in a church that has more stringent demands on its members. Despite this observation, costly donations and acts conducted in a religious context does not itself establish that membership in these clubs is actually worth the entry costs imposed.\n\nDespite the experimental support for this hypothesis, it remains controversial. A common critique is that devoutness is easy to fake, such as simply by attending a religious service. However, the hypothesis predicts that people are more likely to join and contribute to a religious group when its rituals are costly. Another critique specifically asks: why religion? There is no evolutionary advantage to evolving religion over other signals of commitment such as nationality, as Irons admits. However, the reinforcement of religious rites as well as the intrinsic reward and punishment system found in religion makes it an ideal candidate for increasing intragroup cooperation. Finally, there is insufficient evidence for increase in fitness as a result of religious cooperation. However, Sosis argues for benefits from religion itself, such as increased longevity, improved health, assistance during crises, and greater psychological well being though both the supposed benefits from religion and the costly-signaling mechanism have been contested.\n\n\n", "id": "1318175", "title": "Signalling theory"}
{"url": "https://en.wikipedia.org/wiki?curid=36356112", "text": "Split networks\n\nFor a given set of taxa like X, and a set of splits S on X, usually together with a non-negative weighting, which may represent character changes distance, or may also have a more abstract interpretation, if the set of splits S is compatible, then it can be represented by an unrooted phylogenetic tree and each edge in the tree corresponds to exactly one of the splits. More generally, S can always be represented by a split network, which is an unrooted phylogenetic network with the property that every split s in S is represented by an array of parallel edges in network.\n\nA split network N can be obtained from a number of different types of data:\n\n", "id": "36356112", "title": "Split networks"}
{"url": "https://en.wikipedia.org/wiki?curid=187849", "text": "Fitness (biology)\n\nFitness (often denoted formula_1 or ω in population genetics models) is the quantitative representation of natural and sexual selection within evolutionary biology. It can be defined either with respect to a genotype or to a phenotype in a given environment. In either case, it describes individual reproductive success and is equal to the average contribution to the gene pool of the next generation that is made by individuals of the specified genotype or phenotype. The fitness of a genotype is manifested through its phenotype, which is also affected by the developmental environment. The fitness of a given phenotype can also be different in different selective environments.\n\nWith asexual reproduction, it is sufficient to assign fitnesses to genotypes. With sexual reproduction, genotypes are scrambled every generation. In this case, fitness values can be assigned to alleles by averaging over possible genetic backgrounds. Natural selection tends to make alleles with higher fitness more common over time, resulting in Darwinian evolution.\n\nThe term \"Darwinian fitness\" can be used to make clear the distinction with physical fitness. Fitness does not include a measure of survival or life-span; Herbert Spencer's well-known phrase \"survival of the fittest\" should be interpreted as: \"Survival of the form (phenotypic or genotypic) that will leave the most copies of itself in successive generations.\"\n\nInclusive fitness differs from individual fitness by including the ability of an allele in one individual to promote the survival and/or reproduction of other individuals that share that allele, in preference to individuals with a different allele. One mechanism of inclusive fitness is kin selection.\n\nFitness is often defined as a propensity or probability, rather than the actual number of offspring. For example, according to Maynard Smith, \"Fitness is a property, not of an individual, but of a class of individuals — for example homozygous for allele A at a particular locus. Thus the phrase ’expected number of offspring’ means the average number, not the number produced by some one individual. If the first human infant with a gene for levitation were struck by lightning in its pram, this would not prove the new genotype to have low fitness, but only that the particular child was unlucky.\" \n\nAlternatively, \"the fitness of the individual - having an array x of phenotypes — is the probability, s(x), that the individual will be included among the group selected as parents of the next generation.\"\n\nTo avoid the complications of sex and recombination, we initially restrict our attention to an asexual population without genetic recombination. Then fitnesses can be assigned directly to genotypes rather than having to worry about individual alleles. There are two commonly used measures of fitness; absolute fitness and relative fitness.\n\nThe absolute fitness (formula_2) of a genotype is defined as the proportional change in the abundance of that genotype over one generation attributable to selection. For example, if formula_3 is the abundance of a genotype in generation formula_4 in an infinitely large population (so that there is no genetic drift), and neglecting the change in genotype abundances due to mutations, then \n\nAn absolute fitness larger than 1 indicates growth in that genotype's abundance; an absolute fitness smaller than 1 indicates decline.\n\nWhereas absolute fitness determines changes in genotype abundance, relative fitness (formula_1) determines changes in genotype frequency. If formula_7 is the total population size in generation formula_4, and the relevant genotype's frequency is formula_9, then \n\nwhere formula_11 is the mean relative fitness in the population (again setting aside changes in frequency due to drift and mutation). Relative fitnesses only indicate the change in prevalence of different genotypes relative to each other, and so only their values relative to each other are important; relative fitnesses can be any nonnegative number, including 0. It is often convenient to choose one genotype as a reference and set its relative fitness to 1. Relative fitness is used in the standard Wright-Fisher and Moran models of population genetics.\n\nAbsolute fitnesses can be used to calculate relative fitness, since formula_12 (we have used the fact that formula_13, where formula_14 is the mean absolute fitness in the population). This implies that formula_15, or in other words, relative fitness is proportional to formula_16. It is not possible to calculate absolute fitnesses from relative fitnesses alone, since relative fitnesses contain no information about changes in overall population abundance formula_7.\n\nThe change in genotype frequencies due to selection follows immediately from the definition of relative fitness,\n\nThus, a genotype's frequency will decline or increase depending on whether its fitness is lower or greater than the mean fitness, respectively.\n\nIn the particular case that there are only two genotypes of interest (e.g. representing the invasion of a new mutant allele), the change in genotype frequencies is often written in a different form. Suppose that two genotypes formula_19 and formula_20 have fitnesses formula_21 and formula_22, and frequencies formula_23 and formula_24, respectively. Then formula_25, and so\n\nThus, the change in genotype formula_27's frequency depends crucially on the difference between its fitness and the fitness of genotype formula_28. Supposing that formula_27 is more fit than formula_28, and defining the selection coefficient formula_31 by formula_32, we obtain\n\nwhere the last approximation hold for formula_34. In other words, the fitter genotype's frequency grows approximately logistically.\n\nThe British sociologist Herbert Spencer coined the phrase \"survival of the fittest\" in his 1864 work \"Principles of Biology\" to characterise what Charles Darwin had called natural selection.\n\nThe British biologist J.B.S. Haldane was the first to quantify fitness, in terms of the modern evolutionary synthesis of Darwinism and Mendelian genetics starting with his 1924 paper \"A Mathematical Theory of Natural and Artificial Selection\". The next further advance was the introduction of the concept of inclusive fitness by the British biologist W.D. Hamilton in 1964 in his paper on \"The Genetical Evolution of Social Behaviour\".\n\nGenetic load measures the average fitness of a population of individuals, relative either to a theoretical genotype of optimal fitness, or relative to the most fit genotype actually present in the population. Consider n genotypes formula_35, which have the fitnesses formula_36 and the genotype frequencies formula_37 respectively. Ignoring frequency-dependent selection, then genetic load (formula_38) may be calculated as:\n", "id": "187849", "title": "Fitness (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=29000", "text": "Speciation\n\nSpeciation is the evolutionary process by which populations evolve to become distinct species. The biologist Orator F. Cook coined the term in 1906 for cladogenesis, the splitting of lineages, as opposed to anagenesis, phyletic evolution within lineages. Charles Darwin was the first to describe the role of natural selection in speciation in his 1859 book \"The Origin of Species\". He also identified sexual selection as a likely mechanism, but found it problematic.\n\nThere are four geographic modes of speciation in nature, based on the extent to which speciating populations are isolated from one another: allopatric, peripatric, parapatric, and sympatric. Speciation may also be induced artificially, through animal husbandry, agriculture, or laboratory experiments. Whether genetic drift is a minor or major contributor to speciation is the subject matter of much ongoing discussion.\n\nIn addressing the question of the origin of species, there are two key issues: (1) what are the evolutionary mechanisms of speciation, and (2) what accounts for the separateness and individuality of species in the biota? Since Charles Darwin's time, efforts to understand the nature of species have primarily focused on the first aspect, and it is now widely agreed that the critical factor behind the origin of new species is reproductive isolation. Next we focus on the second aspect of the origin of species.\n\nIn \"On the Origin of Species\" (1859), Darwin interpreted biological evolution in terms of natural selection, but was perplexed by the clustering of organisms into species. Chapter 6 of Darwin's book is entitled \"Difficulties of the Theory.\" In discussing these \"difficulties\" he noted \"Firstly, why, if species have descended from other species by insensibly fine gradations, do we not everywhere see innumerable transitional forms? Why is not all nature in confusion instead of the species being, as we see them, well defined?\" This dilemma can be referred to as the absence or rarity of transitional varieties in habitat space.\n\nAnother dilemma, related to the first one, is the absence or rarity of transitional varieties in time. Darwin pointed out that by the theory of natural selection \"innumerable transitional forms must have existed,\" and wondered \"why do we not find them embedded in countless numbers in the crust of the earth.\" That clearly defined species actually do exist in nature in both space and time implies that some fundamental feature of natural selection operates to generate and maintain species.\n\nIt has been argued that the resolution of Darwin's first dilemma lies in the fact that out-crossing sexual reproduction has an intrinsic cost of rarity. The cost of rarity arises as follows. If, on a resource gradient, a large number of separate species evolve, each exquisitely adapted to a very narrow band on that gradient, each species will, of necessity, consist of very few members. Finding a mate under these circumstances may present difficulties when many of the individuals in the neighborhood belong to other species. Under these circumstances, if any species’ population size happens, by chance, to increase (at the expense of one or other of its neighboring species, if the environment is saturated), this will immediately make it easier for its members to find sexual partners. The members of the neighboring species, whose population sizes have decreased, experience greater difficulty in finding mates, and therefore form pairs less frequently than the larger species. This has a snowball effect, with large species growing at the expense of the smaller, rarer species, eventually driving them to extinction. Eventually, only a few species remain, each distinctly different from the other. The cost of rarity not only involves the costs of failure to find a mate, but also indirect costs such as the cost of communication in seeking out a partner at low population densities.\n\nRarity brings with it other costs. Rare and unusual features are very seldom advantageous. In most instances, they indicate a (non-silent) mutation, which is almost certain to be deleterious. It therefore behooves sexual creatures to avoid mates sporting rare or unusual features. Sexual populations therefore rapidly shed rare or peripheral phenotypic features, thus canalizing the entire external appearance, as illustrated in the accompanying illustration of the African pygmy kingfisher, \"Ispidina picta\". This remarkable uniformity of all the adult members of a sexual species has stimulated the proliferation of field guides on birds, mammals, reptiles, insects, and many other taxa, in which a species can be described with a single illustration (or two, in the case of sexual dimorphism). Once a population has become as homogeneous in appearance as is typical of most species (and is illustrated in the photograph of the African pygmy kingfisher), its members will avoid mating with members of other populations that look different from themselves. Thus, the avoidance of mates displaying rare and unusual phenotypic features inevitably leads to reproductive isolation, one of the hallmarks of speciation.\n\nIn the contrasting case of organisms that reproduce asexually, there is no cost of rarity; consequently, there are only benefits to fine-scale adaptation. Thus, asexual organisms very frequently show the continuous variation in form (often in many different directions) that Darwin expected evolution to produce, making their classification into \"species\" (more correctly, morphospecies) very difficult.\n\nAll forms of natural speciation have taken place over the course of evolution; however, debate persists as to the relative importance of each mechanism in driving biodiversity.\n\nOne example of natural speciation is the diversity of the three-spined stickleback, a marine fish that, after the last glacial period, has undergone speciation into new freshwater colonies in isolated lakes and streams. Over an estimated 10,000 generations, the sticklebacks show structural differences that are greater than those seen between different genera of fish including variations in fins, changes in the number or size of their bony plates, variable jaw structure, and color differences.\n\nDuring allopatric (from the ancient Greek \"allos\", \"other\" + Greek \"patrā\", \"fatherland\") speciation, a population splits into two geographically isolated populations (for example, by habitat fragmentation due to geographical change such as mountain formation). The isolated populations then undergo genotypic or phenotypic divergence as: (a) they become subjected to dissimilar selective pressures; (b) they independently undergo genetic drift; (c) different mutations arise in the two populations. When the populations come back into contact, they have evolved such that they are reproductively isolated and are no longer capable of exchanging genes. Island genetics is the term associated with the tendency of small, isolated genetic pools to produce unusual traits. Examples include insular dwarfism and the radical changes among certain famous island chains, for example on Komodo. The Galápagos Islands are particularly famous for their influence on Charles Darwin. During his five weeks there he heard that Galápagos tortoises could be identified by island, and noticed that finches differed from one island to another, but it was only nine months later that he reflected that such facts could show that species were changeable. When he returned to England, his speculation on evolution deepened after experts informed him that these were separate species, not just varieties, and famously that other differing Galápagos birds were all species of finches. Though the finches were less important for Darwin, more recent research has shown the birds now known as Darwin's finches to be a classic case of adaptive evolutionary radiation.\n\nIn peripatric speciation, a subform of allopatric speciation, new species are formed in isolated, smaller peripheral populations that are prevented from exchanging genes with the main population. It is related to the concept of a founder effect, since small populations often undergo bottlenecks. Genetic drift is often proposed to play a significant role in peripatric speciation.\n\nCase Studies:\n\nIn parapatric speciation, there is only partial separation of the zones of two diverging populations afforded by geography; individuals of each species may come in contact or cross habitats from time to time, but reduced fitness of the heterozygote leads to selection for behaviours or mechanisms that prevent their interbreeding. Parapatric speciation is modelled on continuous variation within a \"single,\" connected habitat acting as a source of natural selection rather than the effects of isolation of habitats produced in peripatric and allopatric speciation.\n\nParapatric speciation may be associated with differential landscape-dependent selection. Even if there is a gene flow between two populations, strong differential selection may impede assimilation and different species may eventually develop. Habitat differences may be more important in the development of reproductive isolation than the isolation time. Caucasian rock lizards \"Darevskia rudis\", \"D. valentini\" and \"D. portschinskii\" all hybridize with each other in their hybrid zone; however, hybridization is stronger between \"D. portschinskii\" and \"D. rudis\", which separated earlier but live in similar habitats than between \"D. valentini\" and two other species, which separated later but live in climatically different habitats.\n\nEcologists refer to parapatric and peripatric speciation in terms of ecological niches. A niche must be available in order for a new species to be successful. Ring species such as \"Larus\" gulls have been claimed to illustrate speciation in progress, though the situation may be more complex. The grass \"Anthoxanthum odoratum\" may be starting parapatric speciation in areas of mine contamination.\n\nSympatric speciation is the formation of two or more descendant species from a single ancestral species all occupying the same geographic location.\n\nOften-cited examples of sympatric speciation are found in insects that become dependent on different host plants in the same area. However, the existence of sympatric speciation as a mechanism of speciation remains highly debated.\n\nThe best illustrated example of sympatric speciation is that of the cichlids of East Africa inhabiting the Rift Valley lakes, particularly Lake Victoria, Lake Malawi and Lake Tanganyika. There are over 800 described species, and according to estimates, there could be well over 1,600 species in the region. Their evolution is cited as an example of both natural and sexual selection. A 2008 study suggests that sympatric speciation has occurred in Tennessee cave salamanders. Sympatric speciation driven by ecological factors may also account for the extraordinary diversity of crustaceans living in the depths of Siberia's Lake Baikal.\n\nBudding speciation has been proposed as a particular form of sympatric speciation, whereby small groups of individuals become progressively more isolated from the ancestral stock by breeding preferentially with one another. This type of speciation would be driven by the conjunction of various advantages of inbreeding such as the expression of advantageous recessive phenotypes, reducing the recombination load, and reducing the cost of sex \nThe hawthorn fly (\"Rhagoletis pomonella\"), also known as the apple maggot fly, appears to be undergoing sympatric speciation. Different populations of hawthorn fly feed on different fruits. A distinct population emerged in North America in the 19th century some time after apples, a non-native species, were introduced. This apple-feeding population normally feeds only on apples and not on the historically preferred fruit of hawthorns. The current hawthorn feeding population does not normally feed on apples. Some evidence, such as that six out of thirteen allozyme loci are different, that hawthorn flies mature later in the season and take longer to mature than apple flies; and that there is little evidence of interbreeding (researchers have documented a 4-6% hybridization rate) suggests that sympatric speciation is occurring.\n\nReinforcement, sometimes referred to as the Wallace effect, is the process by which natural selection increases reproductive isolation. It may occur after two populations of the same species are separated and then come back into contact. If their reproductive isolation was complete, then they will have already developed into two separate incompatible species. If their reproductive isolation is incomplete, then further mating between the populations will produce hybrids, which may or may not be fertile. If the hybrids are infertile, or fertile but less fit than their ancestors, then there will be further reproductive isolation and speciation has essentially occurred (e.g., as in horses and donkeys.)\n\nThe reasoning behind this is that if the parents of the hybrid offspring each have naturally selected traits for their own certain environments, the hybrid offspring will bear traits from both, therefore would not fit either ecological niche as well as either parent. The low fitness of the hybrids would cause selection to favor assortative mating, which would control hybridization. This is sometimes called the Wallace effect after the evolutionary biologist Alfred Russel Wallace who suggested in the late 19th century that it might be an important factor in speciation. \nConversely, if the hybrid offspring are more fit than their ancestors, then the populations will merge back into the same species within the area they are in contact.\n\nReinforcement favoring reproductive isolation is required for both parapatric and sympatric speciation. Without reinforcement, the geographic area of contact between different forms of the same species, called their \"hybrid zone,\" will not develop into a boundary between the different species. Hybrid zones are regions where diverged populations meet and interbreed. Hybrid offspring are very common in these regions, which are usually created by diverged species coming into secondary contact. Without reinforcement, the two species would have uncontrollable inbreeding. Reinforcement may be induced in artificial selection experiments as described below.\n\nEcological selection is \"the interaction of individuals with their environment during resource acquisition\". Natural selection is inherently involved in the process of speciation, whereby, \"under ecological speciation, populations in different environments, or populations exploiting different resources, experience contrasting natural selection pressures on the traits that directly or indirectly bring about the evolution of reproductive isolation\". Evidence for the role ecology plays in the process of speciation exists. Studies of stickleback populations support ecologically-linked speciation arising as a by-product, alongside numerous studies of parallel speciation, where isolation evolves between independent populations of species adapting to contrasting environments than between independent populations adapting to similar environments. Ecological speciation occurs with much of the evidence, \"...accumulated from top-down studies of adaptation and reproductive isolation\".\n\nIt is widely appreciated that sexual selection could drive speciation in many clades, independently of natural selection. However the term “speciation”, in this context, tends to be used in two different, but not mutually exclusive senses. The first and most commonly used sense refers to the “birth” of new species. That is, the splitting of an existing species into two separate species, or the budding off of a new species from a parent species, both driven by a biological \"fashion fad\" (a preference for a feature, or features, in one or both sexes, that do not necessarily have any adaptive qualities). In the second sense, \"speciation\" refers to the wide-spread tendency of sexual creatures to be grouped into clearly defined species, rather than forming a continuum of phenotypes both in time and space - which would be the more obvious or logical consequence of natural selection. This was indeed recognized by Darwin as problematic, and included in his \"On the Origin of Species\" (1859), under the heading \"Difficulties with the Theory\". There are several suggestions as to how mate choice might play a significant role in resolving .\n\nNew species have been created by animal husbandry, but the dates and methods of the initiation of such species are not clear. Often, the domestic counterpart of the wild ancestor can still interbreed and produce fertile offspring as in the case of domestic cattle, that can be considered the same species as several varieties of wild ox, gaur, yak, etc., or domestic sheep that can interbreed with the mouflon.\n\nThe best-documented creations of new species in the laboratory were performed in the late 1980s. William R. Rice and George W. Salt bred \"Drosophila melanogaster\" fruit flies using a maze with three different choices of habitat such as light/dark and wet/dry. Each generation was placed into the maze, and the groups of flies that came out of two of the eight exits were set apart to breed with each other in their respective groups. After thirty-five generations, the two groups and their offspring were isolated reproductively because of their strong habitat preferences: they mated only within the areas they preferred, and so did not mate with flies that preferred the other areas. The history of such attempts is described by Rice and Elen E. Hostert (1993).\nDiane Dodd used a laboratory experiment to show how reproductive isolation can evolve in \"Drosophila pseudoobscura\" fruit flies after several generations by placing them in different media, starch- and maltose-based media.\n\nDodd's experiment has been easy for many others to replicate, including with other kinds of fruit flies and foods. Research in 2005 has shown that this rapid evolution of reproductive isolation may in fact be a relic of infection by \"Wolbachia\" bacteria.\n\nAlternatively, these observations are consistent with the notion that sexual creatures are inherently reluctant to mate with individuals whose appearance or behavior is different from the norm. The risk that such deviations are due to heritable maladaptations is very high. Thus, if a sexual creature, unable to predict natural selection's future direction, is conditioned to produce the fittest offspring possible, it will avoid mates with unusual habits or features. Sexual creatures will then inevitably tend to group themselves into reproductively isolated species.\n\nFew speciation genes have been found. They usually involve the reinforcement process of late stages of speciation. In 2008, a speciation gene causing reproductive isolation was reported. It causes hybrid sterility between related subspecies. The order of speciation of three groups from a common ancestor may be unclear or unknown; a collection of three such species is referred to as a \"trichotomy.\"\n\nPolyploidy is a mechanism that has caused many rapid speciation events in sympatry because offspring of, for example, tetraploid x diploid matings often result in triploid sterile progeny. However, not all polyploids are reproductively isolated from their parental plants, and gene flow may still occur for example through triploid hybrid x diploid matings that produce tetraploids, or matings between meiotically unreduced gametes from diploids and gametes from tetraploids (see also hybrid speciation).\n\nIt has been suggested that many of the existing plant and most animal species have undergone an event of polyploidization in their evolutionary history. Reproduction of successful polyploid species is sometimes asexual, by parthenogenesis or apomixis, as for unknown reasons many asexual organisms are polyploid. Rare instances of polyploid mammals are known, but most often result in prenatal death.\n\nHybridization between two different species sometimes leads to a distinct phenotype. This phenotype can also be fitter than the parental lineage and as such natural selection may then favor these individuals. Eventually, if reproductive isolation is achieved, it may lead to a separate species. However, reproductive isolation between hybrids and their parents is particularly difficult to achieve and thus hybrid speciation is considered an extremely rare event. The Mariana mallard is thought to have arisen from hybrid speciation.\n\nHybridization is an important means of speciation in plants, since polyploidy (having more than two copies of each chromosome) is tolerated in plants more readily than in animals. Polyploidy is important in hybrids as it allows reproduction, with the two different sets of chromosomes each being able to pair with an identical partner during meiosis. Polyploids also have more genetic diversity, which allows them to avoid inbreeding depression in small populations.\n\nHybridization without change in chromosome number is called homoploid hybrid speciation. It is considered very rare but has been shown in \"Heliconius\" butterflies and sunflowers. Polyploid speciation, which involves changes in chromosome number, is a more common phenomenon, especially in plant species.\n\nTheodosius Dobzhansky, who studied fruit flies in the early days of genetic research in 1930s, speculated that parts of chromosomes that switch from one location to another might cause a species to split into two different species. He mapped out how it might be possible for sections of chromosomes to relocate themselves in a genome. Those mobile sections can cause sterility in inter-species hybrids, which can act as a speciation pressure. In theory, his idea was sound, but scientists long debated whether it actually happened in nature. Eventually a competing theory involving the gradual accumulation of mutations was shown to occur in nature so often that geneticists largely dismissed the moving gene hypothesis.\n\nHowever, 2006 research shows that jumping of a gene from one chromosome to another can contribute to the birth of new species. This validates the reproductive isolation mechanism, a key component of speciation.\n\nThere is debate as to the rate at which speciation events occur over geologic time. While some evolutionary biologists claim that speciation events have remained relatively constant and gradual over time (known as \"Phyletic gradualism\" - see diagram), some palaeontologists such as Niles Eldredge and Stephen Jay Gould have argued that species usually remain unchanged over long stretches of time, and that speciation occurs only over relatively brief intervals, a view known as \"punctuated equilibrium\". (See diagram, and .)\n\nEvolution can be extremely rapid, as shown in the creation of domesticated animals and plants in a very short geological space of time, spanning only a few tens of thousands of years. Maize (\"Zea mays\"), for instance, was created in Mexico in only a few thousand years, starting about 7,000 to 12,000 years ago. This raises the question of why the long term rate of evolution is far slower than is theoretically possible.\n\nEvolution is imposed on species or groups. It is not planned or striven for in some Lamarckist way. The mutations on which the process depends are random events, and, except for the \"silent mutations\" which do not affect the functionality or appearance of the carrier, are thus usually disadvantageous, and their chance of proving to be useful in the future is vanishingly small. Therefore, while a species or group might benefit from being able to adapt to a new environment by accumulating a wide range of genetic variation, this is to the detriment of the \"individuals\" who have to carry these mutations until a small, unpredictable minority of them ultimately contributes to such an adaptation. Thus, the \"capability\" to evolve would require group selection, a concept discredited by (for example) George C. Williams, John Maynard Smith and Richard Dawkins as selectively disadvantageous to the individual.\n\nThe resolution to Darwin's second dilemma might thus come about as follows:\n\nIf sexual individuals are disadvantaged by passing mutations on to their offspring, they will avoid mutant mates with strange or unusual characteristics. Mutations that affect the external appearance of their carriers will then rarely be passed on to the next and subsequent generations. They would therefore seldom be tested by natural selection. Evolution is, therefore, effectively halted or slowed down considerably. The only mutations that can accumulate in a population, on this punctuated equilibrium view, are ones that have no noticeable effect on the outward appearance and functionality of their bearers (i.e., they are \"silent\" or \"neutral mutations,\" which can be, and are, used to trace the relatedness and age of populations and species.)\nThis argument implies that evolution can only occur if mutant mates cannot be avoided, as a result of a severe scarcity of potential mates. This is most likely to occur in small, isolated communities. These occur most commonly on small islands, in remote valleys, lakes, river systems, or caves, or during the aftermath of a mass extinction. Under these circumstances, not only is the choice of mates severely restricted but population bottlenecks, founder effects, genetic drift and inbreeding cause rapid, random changes in the isolated population's genetic composition. Furthermore, hybridization with a related species trapped in the same isolate might introduce additional genetic changes. If an isolated population such as this survives its genetic upheavals, and subsequently expands into an unoccupied niche, or into a niche in which it has an advantage over its competitors, a new species, or subspecies, will have come in being. In geological terms this will be an abrupt event. A resumption of avoiding mutant mates will thereafter result, once again, in evolutionary stagnation.\n\nIn apparent confirmation of this punctuated equilibrium view of evolution, the fossil record of an evolutionary progression typically consists of species that suddenly appear, and ultimately disappear, hundreds of thousands or millions of years later, without any change in external appearance. Graphically, these fossil species are represented by lines parallel with the time axis, whose lengths depict how long each of them existed. The fact that the lines remain parallel with the time axis illustrates the unchanging appearance of each of the fossil species depicted on the graph. During each species' existence new species appear at random intervals, each also lasting many hundreds of thousands of years before disappearing without a change in appearance. The exact relatedness of these concurrent species is generally impossible to determine. This is illustrated in the diagram depicting the distribution of hominin species through time since the hominins separated from the line that led to the evolution of our closest living primate relatives, the chimpanzees.\n\nFor similar evolutionary time lines see, for instance, the paleontological list of African dinosaurs, Asian dinosaurs, the Lampriformes and Amiiformes.\n\n", "id": "29000", "title": "Speciation"}
{"url": "https://en.wikipedia.org/wiki?curid=38783360", "text": "Key innovation\n\nIn evolutionary biology, a key Innovation, also known as an adaptive breakthrough or key adaptation, is a novel phenotypic trait that allows subsequent radiation and success of a taxonomic group. Typically they bring new abilities that allows the taxa to rapidly diversify and invade niches that were not previously available. The phenomenon helps to explain how some taxa are much more diverse and have many more species than their sister taxa.\nThe term was first used in 1949 by Alden H. Miller who defined it as \"key adjustments in the morphological and physiological mechanism which are essential to the origin of new major groups\", although a broader, contemporary definition holds that \"a key innovation is an evolutionary change in individual traits that is causally linked to an increased diversification rate in the resulting clade\".\n\nThe theory of key innovations has come under attack because it is hard to test in a scientific manner, but there is evidence to support the idea.\n\nThe mechanism by which a key innovation leads to taxonomic diversity is not certain but several hypotheses have been suggested:\n\nA key innovation may, by increasing the fitness of individuals of the species, result in extinction becoming less likely and allow the taxa to expand and speciate.\n\nLatex and resin canals in plants are used to deter predators by releasing a sticky secretion when punctured which can immobilise insects and some contain toxic or foul tasting substances. They have evolved independently approximately 40 times and are considered a key innovation. By increasing the plant's resistance to predation the canals increase the species fitness and allow them to escape being eaten, at least until the predator evolves an ability to overcome the defence. During the period of resistance the plants are less likely to become extinct and can diversify and speciate, and as such taxa with latex and resin canals are more diverse than their canal lacking sister taxa.\n\nA key innovation may allow a species to invade a new region or niche and thus be freed from competition, allowing subsequent speciation and radiation.\n\nA classic example of this is the fourth cusp of mammalian molars, the hypocone, which allowed early mammalian ancestors to effectively digest their generalised diet. The precursors to this, the triconodont teeth of reptiles, were adapted for gripping and slicing rather than chewing.\nThe evolution of the hypocone and flat molars later allowed animals to adapt to a herbivorous diet as they could be used to break down tough plant matter through grinding.\nThe evolution of this ability led to mammals being able to adapt to utilise a huge variety of food sources, and allowed early mammals to invade novel niches through the evolution of specialised herbivores, which experienced relative success during the middle eocene. Specialising for a plant based diet offered early herbivores sufficient resources to radiate as energy was not lost to higher trophic levels and few competitors existed at the time.\n\nA key innovation may result in reproductive isolation, whereby those individuals with the innovation no longer breed with those without. This can lead to rapid speciation as the two populations separate and accumulate mutations.\n\nThe nectar spurs in \"Aquilegia\", a diverse genus of flowering plant, are considered a key innovation because of this. Nectar spurs aid in pollination by making the nectar further from the stamen, ensuring that insect or bird pollinators pick up pollen as they access it. These led rapid speciation within the genus as plants and their pollinators can become specialised to each other i.e. a species of pollinator exclusively feeds from a species of plant, and thus plant populations could easily become reproductively isolated from one another. In addition the shape and size of the nectar spur can evolve in response to pollinator adaptations, developing a co-evolutionary relationship. The \"Aquilegia\" genus has over 50 species.\n\nAs an evolutionary theory, key innovations has come under critical scrutiny due to the fact that it is hard to test. Identification depends on finding correlation between the innovation and increased diversity by comparing sister taxa, but this does not prove causality or isolate other causes of diversity such as stochasticity or habitat, and it is possible to 'cherry pick' examples that fit the hypothesis.\nIn addition the retrospective identification of key innovations offers little in terms of understanding the processes and pressures that resulted in the adaptation, and may identify a very complex evolutionary process as a single event.\nAn example of this is the evolution of avian flight, which was identified as a key innovation in 1963 by Ernst Mayr. However, separate evolutionary changes had to occur throughout the physiology of the avian ancestor, including the enlargement of the cerebellum and the enlargement and ossification of the sternum. These adaptations arose separately, and millions of years apart, not in one step.\n\n", "id": "38783360", "title": "Key innovation"}
{"url": "https://en.wikipedia.org/wiki?curid=39127332", "text": "High-altitude adaptation in humans\n\nHigh-altitude adaptation in humans is an instance of evolutionary modification in certain human populations, including those of Tibet in Asia, the Andes of the Americas, and Ethiopia in Africa, who have acquired the ability to survive at extremely high altitudes. This adaptation means irreversible, long-term physiological responses to high-altitude environments, associated with heritable behavioural and genetic changes.\n\nWhile the rest of the human population would suffer serious health consequences, the indigenous inhabitants of these regions thrive well in the highest parts of the world. These people have undergone extensive physiological and genetic changes, particularly in the regulatory systems of oxygen respiration and blood circulation, when compared to the general lowland population.\n\nThis special adaptation is now recognised as an example of natural selection in action. The adaptation account of the Tibetans has become the fastest case of human evolution in the scientific record, as it is estimated to have occurred in less than 3,000 years.\n\nModern humans dispersed from Africa less than 100,000 years ago, mixing with previously present populations, and eventually colonising the rest of the world, including the harshest environments of extreme cold and high mountains. The abundance of oxygen in the atmosphere is inversely related to altitude from the sea level; hence, the highest mountain ranges of the world are considered unsuitable for human habitation.\n\nNevertheless, around 140 million people, just under 2% of the world's human population, live permanently at high altitudes, that is, at heights above in South America, East Africa, and South Asia. These populations have done so for millennia without apparent complications. The overwhelming majority, over 98% of humans from other parts of the world, normally suffer symptoms of altitude sickness in these regions, often resulting in life-threatening trauma and even death.\n\nStudies on the detail biological mechanism have revealed that adaptation of the Tibetans, Andeans and Ethiopians is indeed an observable instance of the process of natural selection in acting on favourable characters such as enhanced respiratory mechanisms in humans.\n\nHumans are naturally adapted to lowland environment where oxygen is abundant. When people from the general lowlands go to altitudes above , they experience mountain sickness, which is a type of hypoxia, a clinical syndrome of severe lack of oxygen. Complications include fatigue, dizziness, breathlessness, headaches, insomnia, malaise, nausea, vomiting, body pain, loss of appetite, ear-ringing, blistering and purpling of the hands and feet, and dilated veins.\n\nThe sickness is compounded by related symptoms such as cerebral oedema (swelling of brain) and pulmonary oedema (fluid accumulation in lungs). For several days, they breathe excessively and burn extra energy even when the body is relaxed. The heart rate then gradually decreases. Hypoxia, in fact, is one of the principal causes of death among mountaineers. In women, pregnancy can be severely affected, such as development of high blood pressure, called preeclampsia, which causes premature labour, low birth weight of babies, and often complicated with profuse bleeding, seizures, and death of the mother.\n\nMore than 140 million people worldwide are estimated to live at an elevation higher than above sea level, of which 13 million are in Ethiopia, 1.7 million in Tibet (total of 78 million in Asia), 35 million in the South American Andes, and 0.3 million in Colorado Rocky Mountains. Certain natives of Tibet, Ethiopia, and the Andes have been living at these high altitudes for generations and are protected from hypoxia as a consequence of genetic adaptation. It is estimated that at , every lungful of air only has 60% of the oxygen molecules that people at sea level have. At elevations above , lack of oxygen becomes seriously lethal. That is, these highlanders are constantly exposed to an intolerably low oxygen environment, yet they live without any debilitating problems. Basically, the shared adaptation is the ability to maintain relatively low levels of haemoglobin, which is the chemical complex for transporting oxygen in the blood. One of the best documented effects of high altitude is a progressive reduction in birth weight. It has been known that women of long-resident high-altitude population are not affected. These women are known to give birth to heavier-weight infants than women of lowland inhabitants. This is particularly true among Tibetan babies, whose average birth weight is 294-650 (~470) g heavier than the surrounding Chinese population; and their blood-oxygen level is considerably higher.\n\nThe first scientific investigations of high-altitude adaptation was done by A. Roberto Frisancho of the University of Michigan in the late 1960s among the Quechua people of Peru. However, the best scientific studies were started among the Tibetans in the early 1980s by an anthropologist Cynthia Beall of the Case Western Reserve University.\n\nScientists started to notice the extraordinary physical performance of Tibetans since the beginning of Himalayan climbing era in the early 20th century. The hypothesis of a possible evolutionary genetic adaptation makes sense. The Tibetan plateau has an average elevation of above sea level, and covering more than 2.5 million km, it is the highest and largest plateau in the world. In 1990, it was estimated that 4,594,188 Tibetans live on the plateau, with 53% living at an altitude over . Fairly large numbers (about 600,000) live at an altitude exceeding in the Chantong-Qingnan area. Where the Tibetan highlanders live, the oxygen level is only about 60% of that at sea level. The Tibetans, who have been living in this region for 3,000 years, do not exhibit the elevated haemoglobin concentrations to cope with oxygen deficiency as observed in other populations who have moved temporarily or permanently at high altitudes. Instead, the Tibetans inhale more air with each breath and breathe more rapidly than either sea-level populations or Andeans. Tibetans have better oxygenation at birth, enlarged lung volumes throughout life, and a higher capacity for exercise. They show a sustained increase in cerebral blood flow, lower haemoglobin concentration, and less susceptibility to chronic mountain sickness than other populations, due to their longer history of high-altitude habitation.\n\nGeneral people can develop short-term tolerance with careful physical preparation and systematic monitoring of movements, but the biological changes are quite temporary and reversible when they return to lowlands. Moreover, unlike lowland people who only experience increased breathing for a few days after entering high altitudes, Tibetans retain this rapid breathing and elevated lung-capacity throughout their lifetime. This enables them to inhale larger amounts of air per unit of time to compensate for low oxygen levels. In addition, they have high levels (mostly double) of nitric oxide in their blood, when compared to lowlanders, and this probably helps their blood vessels dilate for enhanced blood circulation. Further, their haemoglobin level is not significantly different (average 15.6 g/dl in males and 14.2 g/dl in females), from those of people living at low altitude. (Normally, mountaineers experience >2 g/dl increase in Hb level at Mt. Everest base camp in two weeks.) In this way they are able to evade both the effects of hypoxia and mountain sickness throughout life. Even when they climbed the highest summits like Mt. Everest, they showed regular oxygen uptake, greater ventilation, more brisk hypoxic ventilatory responses, larger lung volumes, greater diffusing capacities, constant body weight and a better quality of sleep, compared to people from the lowland.\n\nIn contrast to the Tibetans, the Andean highlanders, who have been living at high-altitudes for no more than 11,000 years, show different pattern of haemoglobin adaptation. Their haemoglobin concentration is higher compared to those of lowlander population, which also happens to lowlanders moving to high altitude. When they spend some weeks in the lowland their haemoglobin drops to average of other people. This shows only temporary and reversible acclimatisation. However, in contrast to lowland people, they do have increased oxygen level in their haemoglobin, that is, more oxygen per blood volume than other people. This confers an ability to carry more oxygen in each red blood cell, making a more effective transport of oxygen in their body, while their breathing is essentially at the same rate. This enables them to overcome hypoxia and normally reproduce without risk of death for the mother or baby. The Andean highlanders are known from the 16th-century missionaries that their reproduction had always been normal, without any effect in the giving birth or the risk for early pregnancy loss, which are common to hypoxic stress. They have developmentally acquired enlarged residual lung volume and its associated increase in alveolar area, which are supplemented with increased tissue thickness and moderate increase in red blood cells. Though the physical growth in body size is delayed, growth in lung volumes is accelerated. An incomplete adaptation such as elevated haemoglobin levels still leaves them at risk for mountain sickness with old age. \n\nAmong the Quechua people of the Altiplano, there is a significant variation in \"NOS3\" (the gene encoding endothelial nitric oxide synthase, eNOS), which is associated with higher levels of nitric oxide in high altitude. Nuñoa children of Quechua ancestry exhibit higher blood-oxygen content (91.3) and lower heart rate (84.8) than their counterpart school children of different ethnicity, who have an average of 89.9 blood-oxygen and 88-91 heart rate. High-altitude born and bred females of Quechua origins have comparatively enlarged lung volume for increased respiration. \n\nBlood profile comparisons show that among the Andeans, Aymaran highlanders are better adapted to highlands than the Quechuas. Among the Bolivian Aymara people, the resting ventilation and hypoxic ventilatory response were quite low (roughly 1.5 times lower), in contrast to those of the Tibetans. The intrapopulation genetic variation was relatively less among the Aymara people. Moreover, unlike the Tibetans, the blood haemoglobin level is quite normal among Aymarans, with an average of 19.2 g/dl for males and 17.8 g/dl for females. Among the different native highlander populations, the underlying physiological responses to adaptation are quite different. For example, among four quantitative features, such as are resting ventilation, hypoxic ventilatory response, oxygen saturation, and haemoglobin concentration, the levels of variations are significantly different between the Tibetans and the Aymaras. The Andeans, in general are the most poorly adapted, as particularly shown by their frequent mountain sickness and loss of adaptative characters when they move to lowlands.\n\nThe peoples of the Ethiopian highlands also live at extremely high altitudes, around to . Highland Ethiopians exhibit elevated haemoglobin levels, like Andeans and lowlander peoples at high altitudes, but do not exhibit the Andean’s increased in oxygen-content of haemoglobin. Among healthy individuals, the average haemoglobin concentrations are 15.9 and 15.0 g/dl for males and females respectively (which is lower than normal, almost similar to the Tibetans), and an average oxygen saturation of haemoglobin is 95.3% (which is higher than average, like the Andeans). Additionally, Ethiopian highlanders do not exhibit any significant change in blood circulation of the brain, which has been observed among the Peruvian highlanders (and attributed to their frequent altitude-related illnesses). Yet, similar to the Andeans and Tibetans, the Ethiopian highlanders are immune to the extreme dangers posed by high-altitude environment, and their pattern of adaptation is definitely unique from that of other highland peoples.\n\nThe underlying molecular evolution of high-altitude adaptation has been explored and understood fairly recently. Depending on the geographical and environmental pressures, high-altitude adaptation involves different genetic patterns, some of which\nhave evolved quite recently. For example, Tibetan adaptations became prevalent in the past 3,000 years, a rapid example of recent human evolution. At the turn of the 21st century, it was reported that the genetic make-up of the respiratory components of the Tibetan and the Ethiopian populations are significantly different.\n\nSubstantial evidence in Tibetan highlanders suggests that variation in haemoglobin and blood-oxygen levels are adaptive as Darwinian fitness. It has been documented that Tibetan women with a high likelihood of possessing one to two alleles for high blood-oxygen content (which is odd for normal women) had more surviving children; the higher the oxygen capacity, the lower the infant mortality. In 2010, for the first time, the genes responsible for the unique adaptive traits were identified following genome sequences of 50 Tibetans and 40 Han Chinese from Beijing. Initially, the strongest signal of natural selection detected was a transcription factor involved in response to hypoxia, called endothelial Per-Arnt-Sim (PAS) domain protein 1 (\"EPAS1\"). It was found that one single-nucleotide polymorphism (SNP) at \"EPAS1\" shows a 78% frequency difference between Tibetan and mainland Chinese samples, representing the fastest genetic change observed in any human gene to date. Hence, Tibetan adaptation to high altitude becomes the fastest process of phenotypically observable evolution in humans, which is estimated to occur in less than 3,000 years ago, when the Tibetans split up from the mainland Chinese population. Mutations in \"EPAS1\", at higher frequency in Tibetans than their Han neighbours, correlate with decreased haemoglobin concentrations among the Tibetans, which is the hallmark of their adaptation to hypoxia. Simultaneously, two genes, egl nine homolog 1 (\"EGLN1\") (which inhibits haemoglobin production under high oxygen concentration) and peroxisome proliferator-activated receptor alpha (\"PPARA\"), were also identified to be positively selected in relation to decreased haemoglobin nature in the Tibetans.\n\nSimilarly, the Sherpas, known for their Himalayan hardiness, exhibit similar patterns in the \"EPAS1\" gene, which further fortifies that the gene is under selection for adaptation to the high-altitude life of Tibetans. A study in 2014 indicates that the mutant \"EPAS1\" gene could have been inherited from archaic hominins, the Denisovans. \"EPAS1\" and \"EGLN1\" are definitely the major genes for unique adaptive traits when compared with those of the Chinese and Japanese. Comparative genome analysis in 2014 revealed that the Tibetans inherited an equal mixture of genomes from the Nepalese-Sherpas and Hans, and they acquired the adaptive genes from the sherpa-lineage. Further, the population split was estimated to occur around 20,000 to 40,000 years ago, a range of which support archaeological, mitochondria DNA and Y chromosome evidence for an initial colonisation of the Tibetan plateau around 30,000 years ago.\n\nThe genes (\"EPAS1\", \"EGLN1\", and \"PPARA\") function in concert with another gene named hypoxia inducible factors (\"HIF\"), which in turn is a principal regulator of red blood cell production (erythropoiesis) in response to oxygen metabolism. The genes are associated not only with decreased haemoglobin levels, but also in regulating energy metabolism. \"EPAS1\" is significantly associated with increased lactate concentration (the product of anaerobic glycolysis), and \"PPARA\" is correlated with decrease in the activity of fatty acid oxidation. \"EGLN1\" codes for an enzyme, prolyl hydroxylase 2 (PHD2), involved in erythropoiesis. Among the Tibetans, mutation in \"EGLN1\" (specifically at position 12, where cytosine is replaced with guanine; and at 380, where G is replaced with C) results in mutant PHD2 (aspartic acid at position 4 becomes glutamine, and cysteine at 127 becomes serine) and this mutation inhibits erythropoiesis. The mutation is estimated to occur about 8,000 years ago. Further, the Tibetans are enriched for genes in the disease class of human reproduction (such as genes from the \"DAZ\", \"BPY2\", \"CDY\", and \"HLA-DQ\" and \"HLA-DR\" gene clusters) and biological process categories of response to DNA damage stimulus and DNA repair (such as \"RAD51\", \"RAD52\", and \"MRE11A\"), which are related to the adaptive traits of high infant birth weight and darker skin tone and are most likely due to recent local adaptation.\n\nThe patterns of genetic adaptation among the Andeans are largely distinct from those of the Tibetan, with both populations showing evidence of positive natural selection in different genes or gene regions. However, \"EGLN1\" appears to be the principal signature of evolution, as it shows evidence of positive selection in both Tibetans and Andeans. Even then, the pattern of variation for this gene differs between the two populations. Among the Andeans, there are no significant associations between \"EPAS1\" or \"EGLN1\" SNP genotypes and haemoglobin concentration, which has been the characteristic of the Tibetans. The whole genome sequences of 20 Andeans (half of them having chronic mountain sickness) revealed that two genes, SENP1 (an erythropoiesis regulator) and ANP32D (an oncogene) play vital roles in their weak adaptation to hypoxia.\n\nThe adaptive mechanism of Ethiopian highlanders is quite different. This is probably because their migration to the highland was relatively early; for example, the Amhara have inhabited altitudes above for at least 5,000 years and altitudes around to for more than 70,000 years. Genomic analysis of two ethnic groups, Amhara and Oromo, revealed that gene variations associated with haemoglobin difference among Tibetans or other variants at the same gene location do not influence the adaptation in Ethiopians. Identification of specific genes further reveals that several candidate genes are involved in Ethiopians, including \"CBARA1\", \"VAV3\", \"ARNT2\" and \"THRB\". Two of these genes (\"THRB\" and \"ARNT2\") are known to play a role in the HIF-1 pathway, a pathway implicated in previous work reported in Tibetan and Andean studies. This supports the concept that adaptation to high altitude arose independently among different highlanders as a result of convergent evolution.\n\n\n", "id": "39127332", "title": "High-altitude adaptation in humans"}
{"url": "https://en.wikipedia.org/wiki?curid=1741453", "text": "Parent–offspring conflict\n\nParent–offspring conflict (POC) is an expression coined in 1974 by Robert Trivers. It is used to describe the evolutionary conflict arising from differences in optimal parental investment (PI) in an offspring from the standpoint of the parent and the offspring. PI is any investment by the parent in an individual offspring that decreases the parent's ability to invest in other offspring, while the selected offspring's chance of surviving increases.\n\nPOC occurs in sexually reproducing species and is based on a genetic conflict: Parents are equally related to each of their offspring and are therefore expected to equalize their investment among them. Offspring are only half or less related to their siblings (and fully related to themselves), so they try to get more PI than the parents intended to provide even at their siblings' disadvantage. \nHowever, POC is limited by the close genetic relationship between parent and offspring: If an offspring obtains additional PI at the expense of its siblings, it decreases the number of its surviving siblings. Therefore, any gene in an offspring that leads to additional PI decreases (to some extent) the number of surviving copies of itself that may be located in siblings. Thus, if the costs in siblings are too high, such a gene might be selected against despite the benefit to the offspring. \nThe problem of specifying how an individual is expected to weigh a relative against itself has been examined by W. D. Hamilton in 1964 in the context of kin selection. Hamilton's rule says that altruistic behavior will be positively selected if the benefit to the recipient multiplied by the genetic relatedness of the recipient to the performer is greater than the cost to the performer of a social act. Conversely, selfish behavior can only be favoured when Hamilton's inequality is not satisfied. This leads to the prediction that, other things being equal, POC will be stronger under half siblings (e.g., unrelated males father a female's successive offspring) than under full siblings.\n\nIn plants, POC over the allocation of resources to the brood members may affect both brood size (number of seeds matured within a single fruit) and seed size. Concerning brood size, the most economic use of maternal resources is achieved by packing as many seeds as possible in one fruit, i.e., minimizing the cost of packing per seed. In contrast, offspring benefeits from low numbers of seeds per fruit, which reduces sibling competition before and after dispersal. Conflict over seed size arises because there usually exists an inverse exponential relationship between seed size and fitness, that is, the fitness of a seed increases at a diminishing rate with resource investment but the fitness of the maternal parent has an optimum, as demonstrated by Smith and Fretwell (see also marginal value theorem). However, the optimum resource investment from the offspring's point of view would be the maximum that it can possibly get from the maternal parent.\n\nThis conflict about resource allocation is most obviously manifested in the reduction of brood size (i.e. a decrease in the proportion of ovules matured into seeds). Such reduction can be assumed to be caused by the offspring: If the maternal parent's interest were to produce as few seeds as observed, selection would not favour the production of extra ovules that do not mature into seeds. (Although other explanations for this phenomenon exist, such as genetic load, resource depletion or maternal regulation of offspring quality, they could not be supported by experiments.)\n\nThere are several possibilities how the offspring can affect paternal resource allocation to brood members. Evidence exists for siblicide by dominant embryos: Embryos formed early kill the remaining embryos through an aborting chemical. In oaks, early fertilized ovules prevent the fertilization of other ovules by inhibiting the pollen tube entry into the embryo sac. In some species, the maternal parent has evolved postfertilization abortion of few seeded pods. Nevertheless, cheating by the offspring is also possible here, namely by late siblicide, when the postfertilization abortion has ceased.\n\nAccording to the general POC model, reduction of brood size – if caused by POC – should depend on genetic relatedness between offspring in a fruit. Indeed, abortion of embryos is more common in out-crossing than in self-pollinating plants (seeds in cross-pollinating plants are less related than in self-pollinating plants). Moreover, the level of solicitation of resources by the offspring is also increased in cross-pollinating plants: There are several reports that the average weight of crossed seeds is greater than of seeds produced by self-fertilization.\n\nSome of the earliest examples of parent-offspring conflict were seen in bird broods and especially in raptor species. While parent birds often lay two eggs and attempt to raise two or more young, the strongest fledgling takes a greater share of the food brought by parents and will often kill the weaker sibling (siblicide). Such conflicts have been suggested as a driving force in the evolution of optimal clutch size in birds.\n\nIn the blue-footed booby, parent-offspring conflict results in times of food scarcity. When there is less food available in a given year, the older, dominant chick will often kill the younger chick by either attacking directly, or by driving it from the nest. Parents try to prevent siblicide by building nests with steeper sides and by laying heavier second eggs.\n\nEven before POC theory arose, debates took place over whether infants wean themselves or mothers actively wean their infants. Furthermore, it was discussed whether maternal rejections increase infant independence. It turned out that both mother and infant contribute to infant independence. Maternal rejections can be followed by a short-term increase in infant contact but they eventually result in a long-term decrease of contact as has been shown for several primates: In wild baboons infants that are rejected early and frequently spend less time in contact whereas those that are not rejected stay much longer in the proximity of their mother and suckle or ride even in advanced ages. In wild chimpanzees an abrupt increase in maternal rejections and a decrease in mother-offspring contact is found when mothers resume estrus and consort with males. In rhesus macaques a high probability of conception in the following mating season is associated with a high rate of maternal rejection. Rejection and behavioral conflicts can occur during the first months of an infant's life and when the mother resumes estrus. These findings suggest that the reproduction of the mother is influenced by the interaction with their offspring. So there is a potential for conflicts over PI.\nIt was also observed in rhesus macaques that the number of contacts made by offspring is significantly higher than the number of contacts made by mother during a mating season, whereas the opposite holds for the number of broken contacts. This fact suggests that the mother resists offspring's demands for contact, whereas offspring is apparently more interested in spending time in contact. At three months of infant age a shift from mother to infant in responsibility for maintaining contact takes place. So when the infant becomes more independent, its effort to maintain proximity to its mother increases. This might sound paradoxical but becomes clear when one takes into account that POC increases during the period of PI. In summary, all these findings are consistent with POC-theory.\n\nOne might object that time in contact is not a reasonable measure for PI and that, for example, time for milk transfer (lactation) would be a better one. Here one can argue that mother and infant have different thermoregulatory needs due to the fact that they have different surface-to-volume ratios resulting in more rapid loss of heat in infants compared to adults. So infants may be more sensitive to low temperatures than their mothers. An infant might try to compensate by increased contact time with their mother, which could initiate a behavioral conflict over time. Consistency of this hypothesis has been shown for Japanese macaques where decreasing temperatures result in higher maternal rejections and increased number of contacts made by infants.\n\nIn eusocial species, the parent-offspring conflict takes on a unique role because of haplodiploidy and the prevalence of sterile workers. Sisters are more related to each other (0.75) than to their mothers (0.5) or brothers (0.25). In most cases, this drives female workers to try and obtain a sex ratio of 3:1 (females to males) in the colony. However, queens are equally related to both sons and daughters, so they prefer a sex ratio of 1:1. The conflict in social insects is about the level investment the queen should provide for each sex for current and future offspring. It is generally thought that workers will win this conflict and the sex ratio will be closer to 3:1, however there are examples, like in \"Bombus terrestris\", where the queen has considerable control in forcing a 1:1 ratio.\n\nAn important illustration of such conflict is provided by David Haig’s (1993) work on genetic conflicts in pregnancy. Haig has argued that fetal genes would be selected to draw more resources from the mother than it would be optimal for the mother to give, a hypothesis that has received empirical support. The placenta, for example, secretes allocrine hormones that decrease the sensitivity of the mother to insulin and thus make a larger supply of blood sugar available to the fetus. The mother responds by increasing the level of insulin in her bloodstream, and to counteract this effect the placenta has insulin receptors that stimulate the production of insulin-degrading enzymes.\n\nAbout 30 percent of human conceptions do not progress to full term (22 percent before becoming clinical pregnancies) and this creates a second arena for conflict between the mother and the fetus, because the fetus will have a lower quality cut off point for spontaneous abortion than the mother. The mother's quality cut-off point should also decline as she nears the end of her reproductive life and it may be significant that the offspring of older mothers have a higher incidence of genetic defects. Initially, the maintenance of pregnancy is controlled by the maternal hormone progesterone, but in later stages it is controlled by the fetal human chorionic gonadotrophin released into the maternal bloodstream, which causes the release of maternal progesterone. There is also conflict over blood supply to the placenta, with the fetus being prepared to demand a larger blood supply than is optimal for the mother (or even for itself, since high birth weight is a risk factor). This results in hypertension and, significantly, high birth weight is positively correlated with maternal blood pressure.\n\nAfter birth the young infant may demand more nutritional resources than the lactating mother is prepared to provide, and the presence of benzodiazepine-like compounds in breast milk may function to keep this in check, although its primary function is to calm the infant.Here is an article that supports this claim: Dencker, S. J., Johansson, G., & Milsom, I. (1992). Quantification of naturally occurring benzodiazepine-like substances in human breast milk. Psychopharmacology, 107, 69-72.\n\nDuring pregnancy, there is a two-way traffic of immunologically active cell lines through the placenta. Fetal lymphocyte lines may survive in women even decades after giving birth. These cells may serve the adaptive interest of the mother, however, they may also serve conflicting interests of the fetus or those of the father. This mixture of shared or contradicting interests has been hypothesized as giving rise to diseases like autoimmune diseases, infertility, and habitual abortion in humans.\n\n\n", "id": "1741453", "title": "Parent–offspring conflict"}
{"url": "https://en.wikipedia.org/wiki?curid=2909690", "text": "Bak–Sneppen model\n\nThe Bak–Sneppen model is a simple model of co-evolution between interacting species. It was developed to show how self-organized criticality may explain key features of the fossil record, such as the distribution of sizes of extinction events and the phenomenon of punctuated equilibrium. It is named after Per Bak and Kim Sneppen.\n\nThe model dynamics repeatedly eliminates the least adapted species and mutates it and its neighbors to recreate the interaction between species. A comprehensive study of the details of this model can be found in \"Phys. Rev. E\" 53, 414–443 (1996). A solvable version of the model has been proposed in \"Phys. Rev. Lett.\" 76, 348–351 (1996), which shows that the dynamics evolves sub-diffusively, driven by a long-range memory.\n\nAn evolutionary local search heuristic based on the Bak–Sneppen model, called extremal optimization, has been introduced in \"Artificial Intelligence\" 119, 275–286 (2000). The Bak–Sneppen model has been applied to the theory of scientific progress.\n\nWe consider \"N\" species, which are associated with a fitness factor \"f\"(\"i\"). They are indexed by integers \"i\" around a ring. The algorithm consists in choosing the least fit species, and then replacing it and its two closest neighbors (previous and next integer) by new species, with a new random fitness. After a long run there will be a minimum required fitness, below which species don't survive. These \"long-run\" events are referred to as avalanches, and the model proceeds through these avalanches until it reaches a state of relative stability where all species' fitness are above a certain threshold.\n\n\n", "id": "2909690", "title": "Bak–Sneppen model"}
{"url": "https://en.wikipedia.org/wiki?curid=5066368", "text": "Trivers–Willard hypothesis\n\nIn evolutionary biology and evolutionary psychology, the Trivers–Willard hypothesis, formally proposed by Robert Trivers and Dan Willard, suggests that female mammals are able to adjust offspring sex ratio in response to their maternal condition. For example, it may predict greater parental investment in males by parents in \"good conditions\" and greater investment in females by parents in \"poor conditions\" (relative to parents in good condition). The reasoning for this prediction is as follows: Assume that parents have information on the sex of their offspring and can influence their survival differentially. While pressures exist to maintain sex ratios at 50%, evolution will favor local deviations from this if one sex has a likely greater reproductive payoff than is usual.\n\nTrivers and Willard also identified a circumstance in which reproducing individuals might experience deviations from expected offspring reproductive value—namely, varying maternal condition. In polygynous species males may mate with multiple females and low-condition males will achieve fewer or no matings. Parents in relatively good condition would then be under selection for mutations causing production and investment in sons (rather than daughters), because of the increased chance of mating experienced by these good-condition sons. Mating with multiple females conveys a large reproductive benefit, whereas daughters could translate their condition into only smaller benefits. An opposite prediction holds for poor-condition parents—selection will favor production and investment in daughters, so long as daughters are likely to be mated, while sons in poor condition are likely to be out-competed by other males and end up with zero mates (i.e., those sons will be a reproductive dead end).\n\nThe hypothesis was used to explain why, for example, Red Deer mothers would produce more sons when they are in good condition, and more daughters when in poor condition. In polyandrous species where some females mate with multiple males (and others get no matings) and males mate with one/few females (i.e., \"sex-role reversed\" species), these predictions from the Trivers–Willard hypothesis are reversed: parents in good condition will invest in daughters in order to have a daughter that can out-compete other females to attract multiple males, whereas parents in poor condition will avoid investing in daughters who are likely to get out-competed and will instead invest in sons in order to gain at least some grandchildren.\n\n\"Condition\" can be assessed in multiple ways, including body size, parasite loads, or dominance, which has also been shown in macaques (\"Macaca sylvanus\") to affect the sex of offspring, with dominant females giving birth to more sons and non-dominant females giving birth to more daughters. Consequently, high-ranking females give birth to a higher proportion of males than those who are low-ranking.\n\nThe Trivers–Willard hypothesis rests on certain assumptions:\n\nEvolutionary biologists predict a Trivers–Willard effect where these conditions hold, and no effect when these conditions do not hold. In polygynous species where some males have multiple mates and others have none (i.e., greater variance in mating success among males than females), being in good condition affects males more than females. This is reversed in polyandrous species, and possibly in species where condition is based on social status and males disperse.\n\nIn their original paper, Trivers and Willard were not yet aware of the biochemical mechanism for the occurrence of biased sex ratios. Eventually, however, Melissa Larson et al. (2001) proposed that a high level of circulating glucose in the mother's bloodstream may favor the survival of male blastocysts. This conclusion is based on the observed male-skewed survival rates (to expanded blastocyst stages) when bovine blastocysts were exposed to heightened levels of glucose. As blood glucose levels are highly correlated with access to high-quality food, blood glucose level may serve as a proxy for \"maternal condition\". Thus, heightened glucose functions as one possible biochemical mechanism for observed Trivers–Willard effects.\n\nWild and West published a paper describing a mathematical model built on the Trivers–Willard hypothesis that allows precise predictions of alterations in sex-ratio under different circumstances.\n\nThe Trivers–Willard hypothesis has been applied to resource differences among \"individuals\" in a society as well as to resource differences among \"societies\". Empirical evidence is mixed with higher support in better studies according to Cronk in a 2007 review. One example, in a 1997 study, of a group with a preference for females was Romani in Hungary, a low-status group. They \"had a female-biased sex ratio at birth, were more likely to abort a child after having had one or more daughters, nursed their daughters longer, and sent their daughters to school for longer\".\n\n", "id": "5066368", "title": "Trivers–Willard hypothesis"}
{"url": "https://en.wikipedia.org/wiki?curid=35060755", "text": "Hjernevask\n\nHjernevask (\"Brainwash\") is a Norwegian documentary miniseries about science that aired on NRK1 in 2010. The series, consisting of seven episodes, was created and presented by the comedian and sociologist Harald Eia. Each episode featured Eia interviewing Norwegian social scientists about their theories of gender and social constructionism, and then confronting them with contrary data and testimony he had obtained from experts in other fields, such as biology and evolutionary psychology. Experts interviewed for the series included Simon Baron-Cohen, Steven Pinker, Simon LeVay, David Buss, Glenn Wilson, and Anne Campbell. The documentary caused embarrassment for the Norwegian social scientists and generated much public debate in Norway. The entire series has since been released online.\n\nEia and coproducer Ole Martin Ihle have named Steven Pinker's book \"The Blank Slate\" as an inspiration for the documentary series. The series was a huge success, and Eia was awarded the Fritt Ord Award for \"having precipitated one of the most heated debates on research in recent times\". Shortly afterward, the Nordic Council of Ministers closed down the Nordic Gender Institute.\n\nThe producers have made the series available online. Episodes linked here have English subtitles:\n", "id": "35060755", "title": "Hjernevask"}
{"url": "https://en.wikipedia.org/wiki?curid=39331354", "text": "Sexual coercion\n\nSexual coercion in animals is the use of violence, threats, harassment, and other tactics to help them forcefully copulate. Such behavior has been compared to sexual assault, including rape, among humans.\n\nIn nature, males and females usually differ in reproductive fitness optima. Males generally prefer to maximize their number of offspring, and therefore their number of mates; females, on the other hand, tend to care more for their offspring and have fewer mates. Because of this, there are generally more males available to mate at a given time, making females a limited resource. This leads males to evolve aggressive mating behaviors which can help them acquire mates.\n\nSexual coercion has been observed in many species, including mammals, birds, insects, and fish. While sexual coercion does help increase male fitness, it is very often costly to females. However, in spite of these costs, a possible benefit to the females is a chance to test the stamina of the males, so that only those with \"good genes\" will father their offspring. Sexual coercion has been observed to have consequences, such as intersexual coevolution, speciation, and sexual dimorphism.\n\nHarassment is a technique used by males of many species to force females to submit to mating. It has been observed in numerous species, including mammals, birds, insects and fish. Aggression and harassment have been documented in the males of guppies (\"Poecilia reticulata\"), bottlenose dolphins (\"Tursiops aduncus\"), botos (\"Inia geoffrensis\"), dusky dolphins (\"Lagenorhynchus obscurus\"), Hector's dolphins (\"Cephalorhynchus hectori\"), grizzly bears, polar bears, and ungulates. It is also seen in Chinook salmon (\"Oncorhynchus tshawytscha\"), red-spotted newts (\"Notophthalmus viridescens\"), and seed-eating true bugs (\"Neacoryphus\" spp.). Furthermore, it is prevalent in spider monkeys, wild Barbary macaques (\"Macaca sylvanus\") and many other primates.\n\nIn basically all major primate taxa, aggression is used by the dominant males when herding females and keeping them away from other males. In hamadryas baboons, the males often bite the females’ necks and threaten them. Wild chimpanzees can charge at females, shake branches, hit, slap, kick, pound, drag, and bite them. Orangutans are among the most forceful of mammals. Bornean orangutans (\"Pongo pygmaeus\") exhibited aggression in almost 90 percent of their copulations, including when the females were not resisting. A possible explanation for aggressive behaviors in primates is that it is a way for males to train females to be afraid of them and be more likely to surrender to future sexual advances.\n\nMales may also use more indirect techniques to mate with females, such as intimidation. While most female water striders (Gerridae) have their genitalia exposed, females of the water strider species \"Gerris gracilicornis\" have evolved a shield over their genitals. As a result, males cannot physically coerce females because mating is difficult unless the female exposes her genitalia. Therefore, males intimidate females into mating by attracting predators; they tap on the water’s surface and create ripples that catch the attention of predatory fish. From there, it is in the best interest of the female to mate, and as quickly as possible, to avoid being eaten by predators. Typical mating positions of water striders have the females on the bottom, closer to predators, so the risk of predation is much higher for them. Females succumb to copulation to get males to cease signaling to predators.\n\nAnother indirect form of sexual coercion occurs in red-sided garter snakes, \"Thamnophis sirtalis parietalis\". When males \"court\" females, they line their bodies up to the females' and produce caudocephalic waves, which are a series of muscle contractions that travel through their bodies from tail to head. The exact reason for this behavior is unknown, but some studies show that it relates to stress. Females have nonrespiratory air sacs containing anoxic air, and the waving pushes this air into her lungs. The resulting stress causes her cloaca to open, and aids the male in inserting his hemipenis. The stronger and more frequent the caudocephalic waves and the closer the male’s cloaca to the female’s, the more likely the male is to mate successfully.\n\nMales of certain species have evolved mating behaviors in which they forcefully attempt to mate with and inseminate females, often employing grasping techniques. These male grasping devices exist to increase the duration of copulation and restrict females from mating with other males. They are in some ways a form of mate guarding. While some males have evolved different types of modifications to aid in grasping, others just grab females and attempt to force copulation.\n\nOne type of grasping modification is spiny male genitalia. In seed beetles (Coleoptera: Bruchidae), males possess sclerotized spines on their genitalia. These spines are used during copulation to help overcome female resistance and penetrate into their copulatory duct. In addition to aiding penetration, these spines promote the passage of seminal fluids, and act as an anchor to keep the female from fleeing. Furthermore, spiny genitals can injure the females and make them less likely to remate. Sepsidae fly males have modifications on their forelegs to help them grasp onto female wing bases. These modifications include cuticular outgrowths, indentations, and bristles, and males use them to secure themselves onto females after jumping on them. Once the males grab on, a struggle ensues akin to a rodeo, where males try to hold on while females violently shake them off.\n\nAnother type of modification is found in male diving beetles (of the family Dytiscidae), who are equipped with suction cup structures on their front legs. They use these to grab passing females and attach to their dorsal surfaces. To get the females to submit, males shake the females violently and keep them submerged underwater (diving beetles cannot go long without atmospheric oxygen). Unable to get air, female diving beetles submit to the male’s advances in order to avoid drowning (and they lose the energy to resist). Once the males attach, copulation can occur.\n\nMale waterfowl have developed another modification; while most male birds have no external genitalia, male waterfowl (Aves: Anatidae) have a phallus (length ). Most birds mate with the males balancing on top of the females and touching cloacas in a “cloacal kiss”; this makes forceful insemination very difficult. The phallus that male waterfowl have evolved everts out of their bodies (in a clockwise coil) and aids in inseminating females without their cooperation.\n\nAnother such technique is having a \"lock-like\" mechanism, found in \"Drosophila montana\", dogs, wolves, and pigs. Towards the end of copulation, females struggle to try to dislodge the males, whose genital organs take much longer to deflate than females do; the locking (most commonly known in canids as a \"tie\") allows the males to copulate for as long as they need to until they are finished. In dogs, the male has a knot in his penis that gets engorged with blood and ties the female, locking them together during copulation, until the act is complete. Male dogs have evolved this mechanism during mating in order to prevent other males from penetration whilst they are and the use of the tie enables them to be more likely to inseminate the female and produce a healthy litter of pups. Breaking this \"tie\" can be physically harmful to both females and males.\n\nMales of many species simply grab the females and force a mating. Coercive mating is very common in water striders (Gerridae) because in most of the species, the female genitalia are often exposed and easily accessible to males. Without any courtship behavior, males initiate by forcefully trying to mount the females. Carrying the males on their backs is energetically costly to females, so they try to resist and throw off the males. The males fight back even harder and use their forelegs to tightly grasp the female’s thorax and keep them from escaping. The males then forcefully insert their genitalia into the female vulvar opening. In the newt species \"Notophthalmus viridescens\", males carry out a courtship behavior called amplexus. It consists of males capturing females that do not want to mate with them and using their hind limbs to grasp the females by their pectoral regions.\n\nMale guppies (\"Poecilia reticulata\") have been observed to forcefully copulate with females by trying to insert their gonopodium (male sex organ) into female’s genital pores, whether or not they are accepting. Sometimes, male guppies also try to forcefully mate with \"Skiffia bilineata\" (goodeid) females, which resemble guppy females and tend to share the same habitat, even when guppy females are available. A possible explanation for this is the deeper genital cavity of \"S. bilineata\", which stimulates the males more than when mating with guppy females.\n\nMales of some species are able to immobilize females and force copulation. In pigs and boars, males grab females and maneuver the pelvis to lift the vaginal opening and facilitate copulation. The stimulation following intromission causes the female to be immobilized. The male can then freely continue copulation without worrying about the female escaping. Immobilization of the female also occurs in muscovy ducks.\n\nGrasping and/or grappling mating situations have also been documented in \"Calopteryx haemorrhoidalis haemorrhoidalis\" (Odonata), fallow deer (\"Dama dama\"), wild orangutans (Smuts 1993), wild chimpanzees, water voles (semi-aquatic rats) \"Arvicola amphibius\", feral fowl, mallard (\"Anas platyrhynchos\"), hamadryas baboons and many other primates, coho salmon (\"Oncorhynchus kisutch\"), and others.\n\nIn some mammal species, mostly nonhuman primates, it is common for males to commit infanticide to mate with females. This happens often in species that live in groups, such as Old and New World monkeys, apes, prosimians, and hamadryas baboons. There is usually a single breeding male in a group, and when an outside male aggressively takes over, he kills off all of the young offspring. The males kill infants that are not their own to assert their strength and position, and mate with the females. Sometimes, multiple males will invade a troop and gang up on females, killing their offspring and subsequently mating with them. This occurs in spider monkeys, red-backed squirrel monkeys, chimpanzees, and red howlers.\n\nIn the newt species \"Notophthalmus viridescens\", the males rub off hormonal secretions onto the skin of the females they are courting. These hormones have been shown to make the female more receptive to mating with the male. When the male deposits the secretions, he detaches from the female and releases a spermatophore (containing spermatozoa). It is then the female’s decision to either accept it and pick it up or reject it by running away; these hormones make her more likely to accept it.\n\nAnother form of coercion is male mate guarding, used to keep females from mating with other males, and often involves aggression. Guarding allows the males to assure their paternity. A classic example occurs in diving beetles, family Dytiscidae. After copulation, males continue to guard females for up to six hours. They hold them underwater, occasionally tilting them up for air. Guarding also occurs in water striders where, once males complete their sperm transfer, they often remain on top of the females. This guarding duration varies, lasting from several minutes to several weeks. The purpose of such long guarding periods is for the males to see the females lay their eggs and be assured that the offspring are theirs. This behavior also occurs in hamadryas baboons (\"Papio hamadryas\"), where the leader males practice intensive mate guarding. In \"Drosophila\" Montana, studies have shown that following mate guarding, the chances of a female mating with or being inseminated by another male were greatly diminished. This shows that the mate guarding tactic can be very effective.\n\nMales of some species use bodily fluids, such as seminal fluid from their ejaculate, to aid in the coercion of females. Seminal fluid in males of Drosophila melanogaster may contain chemicals that increase the amount of time it takes for females to remate, decrease the length of successive matings, or keep her from remating at all. The less a female mates with other males after copulation with a male, the more likely it is for him to ensure his paternity. These chemicals may also serve to increase the female’s reproductive success, but at the cost of decreased longevity and immune response.\n\nIn many species, seminal fluid can be used as a sort of mating plug. Males of these species transfer their sperm at the beginning of copulation and use the rest of copulation to transfer substances that help build up the mating plugs. These plugs are effective in ensuring that the female does not mate with any other males and that the male’s paternity is secured.\n\nA major direct cost of sexual coercion is physical injury. Male seed beetles (Coleoptera: Bruchidae) have sclerotized spine on their genitalia, which penetrate the female and leave melanized scars. Females can be physically injured from just one mating, and the more a female mates, the more scarring forms in the copulatory duct. In guppies, the male’s gonopodium can cause damage when forcefully inserted, causing cloacal damage to the females. In fowl, females can be physically injured during forceful copulations. Also, semen transferred from the males can contain pathogens and fecal matter, which can lead to disease and decrease female fitness. In elephant seals, physical injury happens very often. In fact, mating leads to 1 in every 1,000 female elephant seals getting killed. Other species in which the females (and/or their offspring) are injured or even killed include lions, rodents, farm cats, crabeater seals, elephant seals, sea lions, wild Trinidadian guppies (\"Poecilia reticulata\"), red-sided garter snakes (\"Thamnophis sirtalis parietalis\"), and newts (\"N. viridescens\").\n\nAnother cost is the excess energy and time expenditure that comes with mating. For example, female water striders, Gerridae, and marine snails of the genus \"Littorina\" 24 have to carry the males on their backs while they mate. First of all, this is a great loss of energy. Second, both the male and the female are at a much greater risk of predation in this position. Furthermore, the time spent mating interferes with the time that could have been spent foraging and feeding.\n\nIn addition, sexual coercion can lower body condition and immunity in ways other than physical damage. Harassment can lead to stress, which can result in weight loss, decreased immune function and energy stores, and less feeding, which has been seen in red-spotted newts. Furthermore, when females are constantly moving around to avoid violent males, they are not able to form female social ties (for example, Grévy's zebra/Equus grevyi). This also happens in species where herding males sometimes do not permit females to join their family in different groups, like in hamadryas baboons.\n\nIndirect costs are those that affect females in the future. One such cost happens because sexual coercion does not allow females to choose the males they want to mate with, which are usually males that are higher quality, compatible, and/or have good genes that will increase their offspring’s survival and fitness. Coercion decreases this choice and can lead to their offspring having lower genetic quality. Studies of the rose bitterling (Rhodeus ocellatus), have shown that offspring of females with mate choice had higher survival rates than offspring of females that did not. Another ultimate cost comes from when males commit infanticide to obtain mating access. This loss of offspring leads to a decrease in fitness of females.\n\nAs a response to sexual coercion and the costs that females face, one of their counter-adaptations is the evolution of anatomical protection. Females of some species, such as the water striders, developed morphological shields to protect their genitalia from males that want to forcefully copulate. Some Gerridae females have also evolved abdominal spines and altered the shapes of their abdomens to make them less accessible to males.\n\nWaterfowl males of the family Aves: Anatidae have evolved a phallus to aid in coercion. This phallus everts out of the male body (when it’s time to mate) in a counter-clockwise coil. As a response, females have developed vaginal structures called dead end sacs and clockwise coils to protect themselves from forceful intromission. Waterfowl females have evolved these “convoluted vaginal morphologies” to make it harder for males to insert themselves without the female’s consent.\n\nAnother female tactic to counter coercion is to try to avoid males that may cause them harm. To do this, females often change their habitats to get away from aggressive males, as is seen in wild Trinidadian guppies (\"Poecilia reticulata\"). Female bottlenose dolphins behave in similar ways by moving into shallow waters where there are not too many males. Other species that practice mate avoidance are \"Calopteryx haemorrhoidalis\", a species of damselfly, who often try to hide from large groups of males to avoid harassment.\n\nFemales of the marine intertidal periwinkle species (genus \"Littorina\") have another way to avoid males. Males usually recognize female snails by cues in their mucous trails. However, females try to mask their gender by altering these cues. In damselflies, females also try to mask their gender by mimicking male colors, which make them less attractive to males.\n\nAn effective female strategy is the employment of protection and alliances. Some females, such as wild Trinidadian guppies (Poecilia reticulate) associate themselves with protective males who come to their rescue. This also occurs in hamadryas, savanna, and olive baboons, where males and females form friendships where the female gets male protection. In northern elephant seals, the females give loud cries when mounted by undesirable or subordinate males, which attract dominant males to help. A similar phenomenon occurs in elephants, bighorn sheep, and fallow deer, where the females stay close to dominant males for protection.\n\nFemales can also form alliances with other females for protection against aggressive males. In African vervet monkeys, related females often form groups and “gang up” on males. Females of high rank create networks of female alliances; together, they fight away persistent suitors.\n\nResisting males and fighting back are important tactics some species use to counter male coercion. Many females try to vigorously shake off males to dislodge them and flee; this is seen in females sepsid flies and diving beetles. Sepsids also try to bend their abdomen in such a way that males cannot copulate forcefully. Females are especially likely to fight back when they are protecting their offspring. This is seen in mountain gorillas, red howlers, and grey langur females, where males are often infanticidal.\n\nFemale resistance has rarely been found to be effective. Male mammals and birds are usually larger than females, and the sheer size and strength difference makes this very difficult. However, it has been observed in some species, such as squirrel monkeys, patas monkeys, vervets, and captive chimpanzees, that females can “gang up” on males when they are being aggressive. They will even try to protect a female in distress. Females have even been observed to kill immigrant males in wild red colobus monkeys.\n\nSometimes, females choose not to struggle and simply acquiesce to forceful matings. This can happen when they decide that the cost of resisting would be greater than the cost of mating. They use submission to avoid further harassment or aggression, which could end in death or injury. This is often seen in primate species, such as chimpanzees and hamadryas baboons.\n\nSome possible benefits of sexual coercion for females have been hypothesized.\n\nA possible proximate benefit for females is that sometimes after a male mates with a female, he becomes her mate. Then, he would defend and protect her. This is seen in many primate species.\n\nA possible benefit of sexual coercion that would come out in the long run is the “good genes” hypothesis. If males can overcome a female’s resistance, then they must possess good genes that would increase the survival, mating success, and ultimately the fitness of her offspring. The hypothesis is that females can use the sexual coercion process to assess the quality of a male.\n\nSexual coercion often leads to an intersexual coevolutionary arms race. This consists of females evolving adaptations to male advances and males evolving counter-adaptations as a response. Males persist in violent behavior, which favors the evolution of female resistance to defend themselves. In organisms where males have genitalia harmful to females, such as in certain insects, females tend to evolve thicker, less sensitive copulatory tracts. Also, they may evolve shield over their genital openings to prevent intromission. Females of some species of water striders have evolved protection from forceful insemination, such as abdominal spines and downward-bent abdomens to make it harder for males to mate. In response, however, males have counter evolved, also changing the shape of their abdomens to those that would facilitate forceful mating.\n\nThe male waterfowl (Aves: Anatidae) evolution of a phallus to forcefully copulate with females has led to counteradaptations in females in the form of vaginal structures called dead end sacs and clockwise coils. These structures make it harder for males to achieve intromission. The clockwise coils are significant because the male phallus everts out of their body in a counter-clockwise spiral; therefore, a clockwise vaginal structure would impede forceful copulation. Studies have shown that the longer a male’s phallus is, the more elaborate the vaginal structures were.\n\nSpeciation has been observed to be a possible consequence of sexual coercion. In diving beetle species family Dytiscidae, an intersexual arms race occurs between males and females. Males have evolved suction cup structures on their forelegs to help grasp females; females have counter-evolved setose dorsal furrows to impede forceful copulation. This continuous evolution (in both the forward and reverse directions) has led to the recent speciation of \"A. japonicus\" and \"A. kishii\", where females of \"A. kishii\" have lost their dorsal furrows while those of \"A. japonicus\" have not.\n\nSexual coercion can lead to sexual dimorphisms, in which males and females have significant morphological differences. For example, in some species, larger males are more successful in forcefully mating/insemination, leading to a higher fitness. In red-sided garter snakes, \"Thamnophis sirtalis parietalis\", it has been shown that heavier-bodied males were better courters and their size gave them an advantage over smaller bodied snakes. This helps lead to an evolution of sexual dimorphism, with males larger than females. In other species, males that are smaller than females have higher fitness. Basically, many sex-specific morphological adaptations (for example, in Dytiscidae diving beetles, females have setose dorsal furrows that males do not and males have suction cups on their forelegs that females do not) are sexual dimorphisms caused by sexual coercion.\n", "id": "39331354", "title": "Sexual coercion"}
{"url": "https://en.wikipedia.org/wiki?curid=39418458", "text": "Serial homology\n\nSerial homology is a special type of homology, defined by Owen as \"representative or repetitive relation in the segments of the same organism.\" Ernst Haeckel preferred the term \"homotypy\" for the same phenomenon.\n\nClassical examples of serial homologies are the development of forelimbs and hind limbs of tetrapods and the iterative structure of the vertebrae.\n\n", "id": "39418458", "title": "Serial homology"}
{"url": "https://en.wikipedia.org/wiki?curid=5259", "text": "Common descent\n\nCommon descent describes how, in evolutionary biology, a group of organisms share a most recent common ancestor. There is \"massive\" evidence of common descent of all life on Earth from the last universal common ancestor (LUCA). In July 2016, scientists reported identifying a set of 355 genes from the LUCA, by comparing the genomes of the three domains of life, archaea, bacteria, and eukaryotes.\n\nCommon ancestry between organisms of different species arises during speciation, in which new species are established from a single ancestral population. Organisms which share a more recent common ancestor are more closely related. The most recent common ancestor of all currently living organisms is the last universal ancestor, which lived about 3.9 billion years ago. The two earliest evidences for life on Earth are graphite found to be biogenic in 3.7 billion-year-old metasedimentary rocks discovered in western Greenland and microbial mat fossils found in 3.48 billion-year-old sandstone discovered in Western Australia. All currently living organisms on Earth share a common genetic heritage, though the suggestion of substantial horizontal gene transfer during early evolution has led to questions about the monophyly (single ancestry) of life.\n\nUniversal common descent through an evolutionary process was first proposed by the English naturalist Charles Darwin in the concluding sentence of his 1859 book \"On the Origin of Species\":\n\nIn the 1740s, the French mathematician Pierre Louis Maupertuis made the first known suggestion that all organisms had a common ancestor, and had diverged through random variation and natural selection. In \"Essai de cosmologie\" (1750), Maupertuis noted:\n\nMay we not say that, in the fortuitous combination of the productions of Nature, since only those creatures \"could\" survive in whose organizations a certain degree of adaptation was present, there is nothing extraordinary in the fact that such adaptation is actually found in all these species which now exist? Chance, one might say, turned out a vast number of individuals; a small proportion of these were organized in such a manner that the animals' organs could satisfy their needs. A much greater number showed neither adaptation nor order; these last have all perished... Thus the species which we see today are but a small part of all those that a blind destiny has produced.\n\nIn 1790, the philosopher Immanuel Kant wrote in \"Kritik der Urteilskraft\" (\"Critique of Judgement\") that the similarity of animal forms implies a common original type, and thus a common parent.\n\nIn 1794, Charles Darwin's grandfather, Erasmus Darwin, asked:\n[W]ould it be too bold to imagine, that in the great length of time, since the earth began to exist, perhaps millions of ages before the commencement of the history of mankind, would it be too bold to imagine, that all warm-blooded animals have arisen from one living filament, which endued with animality, with the power of acquiring new parts attended with new propensities, directed by irritations, sensations, volitions, and associations; and thus possessing the faculty of continuing to improve by its own inherent activity, and of delivering down those improvements by generation to its posterity, world without end?\n\nCharles Darwin's views about common descent, as expressed in \"On the Origin of Species\", were that it was probable that there was only one progenitor for all life forms:\n\nTherefore I should infer from analogy that probably all the organic beings which have ever lived on this earth have descended from some one primordial form, into which life was first breathed.\n\nAll known forms of life are based on the same fundamental biochemical organization: genetic information encoded in DNA, transcribed into RNA, through the effect of protein- and RNA-enzymes, then translated into proteins by (highly similar) ribosomes, with ATP, NADPH and others as energy sources. Analysis of small sequence differences in widely-shared substances such as cytochrome c further supports universal common descent. Some 23 proteins are found in all organisms, serving as enzymes carrying out core functions like DNA replication. The fact that only one such set of enzymes exists is convincing evidence of a single ancestry.\n\nThe genetic code (the \"translation table\" according to which DNA information is translated into amino acids, and hence proteins) is nearly identical for all known lifeforms, from bacteria and archaea to animals and plants. The universality of this code is generally regarded by biologists as definitive evidence in favor of universal common descent. \n\nThe way that codons (DNA triplets) are mapped to amino acids seems to be strongly optimised. Richard Egel argues that in particular the hydrophobic (non-polar) side-chains are well organised, suggesting that these enabled the earliest organisms to create peptides with water-repelling regions able to support the essential electron exchange (redox) reactions for energy transfer.\n\nSimilarities which have no adaptive relevance cannot be explained by convergent evolution, and therefore they provide compelling support for universal common descent. Such evidence has come from two areas: amino acid sequences and DNA sequences. Proteins with the same three-dimensional structure need not have identical amino acid sequences; any irrelevant similarity between the sequences is evidence for common descent. In certain cases, there are several codons (DNA triplets) that code redundantly for the same amino acid. Since many species use the same codon at the same place to specify an amino acid that can be represented by more than one codon, that is evidence for their sharing a recent common ancestor. Had the amino acid sequences come from different ancestors, they would have been coded for by any of the redundant codons, and since the correct amino acids would already have been in place, natural selection would not have driven any change in the codons, however much time was available. Genetic drift could change the codons, but it would be extremely unlikely to make all the redundant codons in a whole sequence match exactly across multiple lineages. Similarly, shared nucleotide sequences, especially where these are apparently neutral such as the positioning of introns and pseudogenes, provide strong evidence of common ancestry.\n\nThe universality of many aspects of cellular life is often pointed to as supportive evidence to the more compelling evidence listed above. These similarities include the energy carrier adenosine triphosphate (ATP), and the fact that all amino acids found in proteins are left-handed. It is, however, possible that these similarities resulted because of the laws of physics and chemistry, rather than universal common descent and therefore resulted in convergent evolution. In contrast, there is evidence for homology of the central subunits of Transmembrane ATPases throughout all living organisms, especially how the rotating elements are bound to the membrane. This supports the assumption of a LUCA as a cellular organism, although primordial membranes may have been semipermeable and evolved later to the membranes of modern bacteria, and on a second path to those of modern archaea also.\n\nAnother important piece of evidence is from detailed phylogenetic trees (i.e., \"genealogic trees\" of species) mapping out the proposed divisions and common ancestors of all living species. In 2010, Douglas L. Theobald published a statistical analysis of available genetic data, mapping them to phylogenetic trees, that gave \"strong quantitative support, by a formal test, for the unity of life.\" \n\nTraditionally, these trees have been built using morphological methods, such as appearance, embryology, etc. Recently, it has been possible to construct these trees using molecular data, based on similarities and differences between genetic and protein sequences. All these methods produce essentially similar results, even though most genetic variation has no influence over external morphology. That phylogenetic trees based on different types of information agree with each other is strong evidence of a real underlying common descent.\n\nTheobald noted that substantial horizontal gene transfer could have occurred during early evolution. Bacteria today remain capable of gene exchange between distantly-related lineages. This weakens the basic assumption of phylogenetic analysis, that similarity of genomes implies common ancestry, because sufficient gene exchange would allow lineages to share much of their genome whether or not they shared an ancestor (monophyly). This has led to questions about the single ancestry of life. However, biologists consider it very unlikely that completely unrelated proto-organisms could have exchanged genes, as their different coding mechanisms would have resulted only in garble rather than functioning systems. Later, however, many organisms all derived from a single ancestor could readily have shared genes that all worked in the same way, and it appears that they have.\n\nIf early organisms had been driven by the same environmental conditions to evolve similar biochemistry convergently, they might independently have acquired similar genetic sequences. Theobald's \"formal test\" was accordingly criticised by Takahiro Yonezawa and colleagues for not including consideration of convergence. They argued that Theobald's test was insufficient to distinguish between the competing hypotheses. Theobald has defended his method against this claim, arguing that his tests distinguish between phylogenetic structure and mere sequence similarity. Therefore, Theobald argued, his results show that \"real universally conserved proteins are homologous.\"\n\n\n\n", "id": "5259", "title": "Common descent"}
{"url": "https://en.wikipedia.org/wiki?curid=697416", "text": "Talk.origins\n\ntalk.origins (often capitalised to Talk.Origins or abbreviated as t.o.) is a moderated Usenet discussion forum concerning the origins of life, and evolution. It remains a major venue for debate in the creation-evolution controversy, and its official purpose is to draw such debates out of the science newsgroups, such as sci.bio.evolution and sci.bio.paleontology.\n\nThe first post to talk.origins was a starter post by Mark Horton, dated 5 September 1986.\n\nIn the early 1990s, a number of FAQs on various topics were being periodically posted to the newsgroup. In 1994, Brett J. Vickers established an anonymous FTP site to host the collected FAQs of the newsgroup. In 1995, Vickers started the TalkOrigins Archive web site as another means of hosting the talk.origins FAQs. It maintains an extensive FAQ on topics in evolutionary biology, geology and astronomy, with the aim of representing the views of mainstream science. It has spawned other websites, notably TalkDesign \"a response to the intelligent design movement\", Evowiki, and the Panda's Thumb weblog.\n\nThe group was originally created as the unmoderated newsgroup net.origins as a 'dumping ground' for all the various flame threads 'polluting' other newsgroups, then renamed to talk.origins as part of the Great Renaming. Subsequently, after discussion on the newsgroup, the group was voted to be moderated in 1997 by the normal USENET RFD/CFV process, in which only spam and excessive crossposting are censored. The moderator for the newsgroup is David Iain Greig (and technically Jim Lippard as alternate/backup).\n\nThe group is characterized by a long list of in-crowd jokes like the fictitious University of Ediacara, the equally fictitious \"Evil Atheist Conspiracy\" which allegedly hides all the evidence supporting Creationism, a monthly election of the \"Chez Watt\"-award for \"statements that make you go 'say what', or some such.\", pun cascades, a strong predisposition to quoting Monty Python and a habit of calling penguins \"the best birds\".\n\nApart from the humor, the group includes rebuttals to creationist claims. There is an expectation that any claim is to be backed up by actual evidence, preferably in the form of a peer-reviewed publication in a reputable journal. The group as a whole votes for a PoTM-award (Post of The Month), which makes it into the annals of TalkOrigins Archive.\n\n\n", "id": "697416", "title": "Talk.origins"}
{"url": "https://en.wikipedia.org/wiki?curid=35350800", "text": "Evolutionary anachronism\n\nEvolutionary anachronism is a concept in evolutionary biology, named by Connie C. Barlow in her book \"The Ghosts of Evolution\" (2000), to refer to attributes of living species that are best explained as a result of having been favorably selected in the past due to coevolution with other biological species that have since become extinct. When this context is removed, said attributes appear as unexplained amounts of energy investments on the part of the living organism, with no apparent benefit extracted from them, and can even be perjudicial to the continued reproduction of the surviving species. The general theory was formulated by Costa Rican-based American botanist Daniel Janzen and University of Arizona-based geologist Paul S. Martin (a prominent defender of the overkill hypothesis to explain the Quaternary extinction event) in a \"Science\" article published in 1982, titled \"Neotropical Anachronisms: The fruit the gomphotheres ate.\" Previously in 1977, Stanley Temple had proposed a similar idea to explain the decline of the Mauritius endemic tree tambalacoque following the extinction of the iconic dodo.\n\nJanzen, Martin and Barlow mainly discussed evolutionary anachronisms in the context of seed dispersal and passive defense strategies exhibited by plants that had evolved alongside disappeared megaherbivores. However, some examples have also been described in animal species. John Byers used the name relict behavior for animal behavior examples. Evolutionary anachronisms, as properly understood, should not be confused with examples of vestigiality. Though both concepts refer ultimately to organs that evolved to deal with pressures that are no longer present today, in the anachronisms case, the original function of the organ and the capacity of the organism to use it are still retained intact (e.g. the absence of gomphotheres to eat avocados doesn't make the avocado's pulp in any way vestigial, rudimentary or intrinsically incapable of playing its original function of helping disperse the avocado's seeds through zoochory, were a new suitable ecological partner to appear; while a true vestigial organ like the python's pelvic spurs cannot in any way be used to walk again).\n\nDispersal syndromes are complexes of fruit traits that enable plants to disperse seeds. The kind of fruits that birds are attracted to are usually small, with only a thin protective skin, and the colors are red or dark shades of blue or purple. Fruits categorized as mammal syndrome are bigger than bird fruits. They possess a tough rind or husk, emit a strong odor when ripe but retain a dull coloration of brown, burnished yellow, orange or remain green, because most mammals have a powerful sense of smell but poor color vision in general, primates being the most notable exception. The megafauna dispersal syndrome refers to those attributes of fruits that evolved in order to attract megafauna (animals that weigh or weighed more than 44 kilograms) as primary dispersal agents. Since the Holocene extinction, large herbivores have become extinct outside Africa and to a lesser extent Asia, leaving these fruits without a suitable dispersal mechanism in the absence of agriculture.\n\n\n\n", "id": "35350800", "title": "Evolutionary anachronism"}
{"url": "https://en.wikipedia.org/wiki?curid=39830246", "text": "Evolving digital ecological networks\n\nEvolving digital ecological networks are webs of interacting, self-replicating, and evolving computer programs (i.e., digital organisms) that experience the same major ecological interactions as biological organisms (e.g., competition, predation, parasitism, and mutualism). Despite being computational, these programs evolve quickly in an open-ended way, and starting from only one or two ancestral organisms, the formation of ecological networks can be observed in real-time by tracking interactions between the constantly evolving organism phenotypes. These phenotypes may be defined by combinations of logical computations (hereafter tasks) that digital organisms perform and by expressed behaviors that have evolved. The types and outcomes of interactions between phenotypes are determined by task overlap for logic-defined phenotypes and by responses to encounters in the case of behavioral phenotypes. Biologists use these evolving networks to study active and fundamental topics within evolutionary ecology (e.g., the extent to which the architecture of multispecies networks shape coevolutionary outcomes, and the processes involved).\n\nIn nature, species do not evolve in isolation but in large networks of interacting species. One of the main goals in evolutionary ecology is to disentangle the evolutionary mechanisms that shape and are shaped by patterns of interaction between species. A particularly important question concerns how coevolution, the reciprocal evolutionary change in local populations of interacting species driven by natural selection, is shaped by the architecture of food webs, plant-animal mutualistic networks, and host-parasite communities. The concept of diffuse coevolution, where adaptation is in response to a suite of biotic interactions, was the first step towards a framework unifying relevant theories in community ecology and coevolution. Understanding how individual interactions within networks influence coevolution, and conversely how coevolution influences the overall structure of networks, requires an appreciation for how pair-wise interactions change due to their broader community contexts as well as how this community context shapes selective pressures. Accordingly, research is now focusing on how reciprocal selection influences and is embedded within the structure of multispecies interactive webs, not only on particular species in isolation.\n\nCoevolution in a community context can be addressed theoretically via mathematical modeling and simulation, by looking at ancient footprints of evolutionary history via ecological patterns that persist and are observable today, and by performing laboratory experiments with microorganisms. In spite of the long time scales involved and the substantial effort that is necessary to isolate and quantify samples, the latter approach of testing biological evolution in the lab has been successful over the last two decades. However, studying the evolution of interspecific interactions, which involves dealing with more complex webs of multiple interacting species, has proven to be a much more difficult challenge. A meta-analysis of host-phage interaction networks, carried out by Weitz and his team, found a striking statistical structure to the patterns of infection and resistance across a wide variety of environments and methods from which the hosts and phage were obtained. However, the ecological mechanisms and evolutionary processes responsible have yet to be unraveled.\n\nDigital ecological networks enable the direct, comprehensive, and real time observation of evolving ecological interactions between antagonistic and/or mutualistic digital organisms that are difficult to study in nature. Research using self-replicating computer programs can help us understand how coevolution shapes the emergence and diversification of coevolving species interaction networks and, in turn, how changes in the overall structure of the web (e.g., through extinction of taxa or the introduction of invasive species) affect the evolution of a given species. Studying the evolution of species interaction networks in these artificial evolving systems also contributes to the development of the field, while overcoming limitations evolutionary biologists may face. For example, laboratory studies have shown that historical contingency can enable or impede the outcome of the interactions between bacteria and phage, depending on the order in which mutations occur: the phage often, but not always, evolve the ability to infect a novel host. Therefore, in order to obtain statistical power for predicting such outcomes of the coevolutionary process, experiments require a high level of replication. This stochastic nature of the evolutionary process was exemplified by Stephen Jay Gould's inquiry (\"\"What would happen if the tape of the history of life were rewound and replayed?\"\") Because of their ease in scalability and replication, evolving digital ecological networks open the door to experiments that incorporate this approach of \"replaying the tape of life\". Such experiments allow researchers to quantify the role of historical contingency and repeatability in network evolution, enabling predictions about the architecture and dynamics of large networks of interacting species.\n\nThe inclusion of ecological interactions in digital systems enables new research avenues: investigations using self-replicating computer programs complement laboratory efforts by broadening the breadth of viable experiments focused on the emergence and diversification of coevolving interactions in complex communities. This cross-disciplinary research program provides fertile grounds for new collaborations between computer scientists and evolutionary biologists.\n\nThe field of digital life was inspired by the rampant computer viruses of the 1980s. These viruses were self-replicating computer programs that spread from one computer to another, but they did not evolve. Steen Rasmussen was the first to include the possibility of mutation in self-replicating computer programs by extending the once-popular Core War game, where programs competed in a digital battle ground for the computer's resources. Although Rasmussen observed some interesting evolution, mutations in this early genetic programming language produced many unstable organisms, thus prohibiting scientific experiments. Just one year later, Thomas S. Ray developed an alternative system, Tierra, and performed the first successful experiments with evolving populations of self-replicating computer programs.\n\nThomas S. Ray created a genetic language similar to earlier digital systems, but added several key features that made it more suitable for evolution in his artificial life system, Tierra. Primarily, he prevented instructions from writing beyond the \"privately\" allocated memory space, thus limiting the potential for organisms writing over others. The only selective pressure in Tierra was for rapid self-replication. Over the course of evolution, this pressure led to shorter and shorter genomes, reducing the time spent copying instructions during replication. Some individuals even started executing the replication code in other organisms, allowing those \"cheaters\", which were originally referred to as parasites in Ray's work, to further shrink their genetic programs. This form of cheating was the first evolved ecological interaction between organisms in artificial life software. Ray's cheaters pre-dated the formal study of evolving ecological interactions using Tierra-like digital evolution platforms by 20 years.\n\nIn 1993, Christoph Adami, Charles Ofria, and C. Titus Brown created the artificial life platform Avida at the California Institute of Technology. They added the ability for digital organisms to obtain bonus CPU cycles for performing computational tasks, like adding two numbers together. In Avida, researchers can define the available tasks and set the consequences for organisms upon successful calculation. When organisms are rewarded with additional CPU cycles, their replication rate increases. Since Avida was designed specifically as a scientific tool, it allows users to collect a comprehensive suite of data about evolving populations. Due to its flexibility and data tracking abilities, Avida has become the most widely used digital system for studying evolution. The Devolab at the BEACON Center currently continues development of Avida.\n\nDigital organisms in Avida are self-replicating computer programs with a genome composed of assembly-like instructions. The genetic programming language in Avida contains instructions for manipulating values in registers and stacks as well as for control flow and mathematical operations. Each digital organism contains virtual hardware on which its genome is executed. To reproduce, digital organisms must copy their genome instruction by instruction into a new region of memory through a potentially noisy channel that may lead to errors (i.e., mutations). While most mutations are detrimental, mutants will occasionally have higher fitness than their parents, thereby providing the basis for natural selection with all of the necessary components for Darwinian evolution. Digital organisms can acquire random binary numbers from the environment and are able to manipulate them using their genetic instructions, including the logic instruction \"NAND\". With only this instruction, digital organisms can compute any other task by stringing together various operations because NAND is a universal logic function. If the output of processing random numbers from the environment corresponds to the result of a particular logic task, then that task is incorporated into the set of tasks the organism performs, which in turn, defines part of its phenotype.\n\nInteractions between digital organisms occur through phenotypic matching, which, in the case of task-based phenotypes, results from the performance of overlapping logic functions. Different mechanisms for mapping phenotypic matching to interactions can be implemented, depending on the antagonistic or mutualistic nature of the interaction.\n\nIn host-parasite interactions, the parasite organisms benefits at the expense of the host organisms. Parasites in Avida are implemented just like other self-replicating digital organisms, but they live inside hosts and execute parasitic threads using CPU cycles stolen from their hosts. Because parasites impose a cost (lost CPU cycles) on hosts, there is selection for resistance, and when resistance starts to spread in a population, there is selective pressure for parasites to infect those new resistant hosts. Infection occurs when both the parasite and host perform at least one overlapping task. Thus a host is resistant to a particular parasite if they do not share any tasks. This mechanism of infection mimics the inverse-gene-for-gene model, in which infection only occurs if a host susceptibility gene (the presence of a logic task) is matched by a parasite virulence gene (a parasite performing the same task). Additional infection mechanisms, such as the matching allele and gene-for-gene models, can also be implemented.\n\nIn traditional infection genetic models, host resistance and pathogen infectivity have associated costs. These costs are an important part of theory about why defense genes do not always fix rapidly within populations. Costs are also present in digital host-parasite interactions: performing more or more complex tasks implies larger genomes and hence slower reproduction. Tasks may also allow organisms access to resources present in the abiotic environment, and the environment can be carefully manipulated to control the relative costs or benefits of resistance.\n\nBy keeping track of task-based phenotypes as well as tracking information about successful infections in the community, researchers are able to perfectly reconstruct the interaction networks of digital coevolving hosts and parasites. The structure of these networks is a result of the interplay between ecological processes, mainly host abundance, and coevolutionary dynamics, which lead to changes in host specificity.\n\nInteractions in which both species obtain mutual benefit, such as those between flowering plants and pollinators, and birds and fleshy fruits, can be implemented in evolving digital experiments by following the same task matching approach used for host-parasite interactions, but using free-living organisms instead of parasitic threads. For example, one way to set up a plant-pollinator type of interaction is to use an environment containing two mutually exclusive resources: one designated for \"plant\" organisms and one for \"pollinator\" organisms. Similar to parasites attempting infection, if tasks overlap between a pollinator and a plant it visits, pollination is successful and both organisms obtain extra CPU cycles. Thus, these digital organisms obtain mutual benefit when they perform at least one common task, and more common tasks lead to larger mutual benefits. While this is one specific way to enable mutualistic interactions, many others are possible in Avida. Interactions that begin as parasitic may even evolve to be mutualistic under the right conditions. In most cases, coevolution will result in concurrent interactions between multiple phenotypes. Thus, observed networks of mutualistic interactions can inform our understanding about the outcomes and processes of coevolution in complex communities.\n\nWhile host-parasite and mutualistic interactions are determined by task-based phenotypes, predator-prey interactions are determined by behavior. Predators are digital organisms that have evolved from ancestral prey phenotypes to locate, attack, and consume organisms. When a predator executes an attack instruction (acquired through mutation), it kills a neighboring organism. When predators kill prey, they gain resources required for reproduction (e.g., CPU cycles) proportional to the level accumulated by the consumed prey. Selection favors behavioral strategies in prey that enable them to avoid being eaten. At the same time, selection favors predators with behavioral strategies that improve their food finding and prey attacking abilities. The resulting diversity in the continuously evolving behavioral phenotypes creates dynamic predator-prey interaction networks in which selective forces are constantly changing as a consequence of the emergence of new, and loss of old, behaviors. Because predators and prey move around in and use information about their environment, these experiments are typically carried out using spatially structured populations. On the other hand, host-parasite and mutualistic coevolution are often done in well-mixed environments, though the choice of the environment is at the discretion of the experimenter.\n\nUnderstanding how biodiversity is organized in natural ecosystems requires going beyond the study of pairs of interacting species. Using digital organisms, one can find generalities about the evolutionary and ecological processes shaping the web of interactions among species, as well as the coevolutionary processes embedded within these networks. By tracing the evolution of digital communities and their ecological networks, researchers obtain perfect \"fossil records\" of how the number and patterns of links among interacting phenotypes evolved.\n\nThe stability-diversity debate is a long-standing debate about whether more diverse ecological networks are also more stable. Until recently, this debate has focused on one component of biodiversity: species diversity. However, newer research has begun dealing with another component of biodiversity: diversity in species interactions. Mathematical models show that a mixture of antagonistic and mutualistic interactions can stabilize population dynamics and that the loss of one interaction type may critically destabilize ecosystems. Studies with digital organisms can shed light on this debate from an empirical perspective because the types of interactions included can be manipulated and the stability of the resulting evolving digital ecological network can be measured.\n\nEqually addressable using evolving digital ecological networks are many of the open questions concerning the coevolution of ecological interactions in multispecies communities. For example, do coevolutionary dynamics change as communities become richer? Is there any limit to their richness? Is the evolution of interactions between multispecies networks historically contingent? Why do some ecological scenarios lead to predictable network structures and others do not? Do genetic constraints play a large role in the evolution of ecological networks? These are only a few of many open questions concerning the coevolution of ecological interactions in multispecies communities.\n\nThese and many related questions require researchers to look across the evolutionary history of ecological network formation. For natural systems, those data are very difficult to collect. With digital organisms, watching both the coevolutionary process and ecological network formation is possible in real time. Data on the abundance of interacting phenotypes are recorded without error, hence the evolutionary implications of ecological processes can be explored in-depth.\n\nThe study of self-replicating and evolving computer programs offers a tantalizing glimpse into the evolution of interactions among organisms that do not share any ancestry with biochemical life of Earth. This comes with potential caveats in translating predictions of evolving digital networks to biological ones because mechanistic details differ substantially between interacting digital organisms and interacting biological organisms. Nevertheless, these digital networks contain the necessary components for ongoing coevolutionary dynamics in large webs of interacting organisms. In spite of the differences between biological and digital evolution, the study of evolving digital ecological networks can lead to a more predictive understanding of natural dynamics. Because the general operational processes (e.g., Darwinian evolution, mutualism, parasitism) do not differ, studies utilizing digital networks can uncover rules operating on and within ecological networks. Together with microbial experiments, they create opportunities for furthering the understanding of the interplay between ecological and evolutionary processes among interacting species.\n", "id": "39830246", "title": "Evolving digital ecological networks"}
{"url": "https://en.wikipedia.org/wiki?curid=39991948", "text": "Evolutionary fauna\n\nThe concept of the three great evolutionary faunas of marine animals from the Cambrian to the present (that is, the entire Phanerozoic) was introduced by Jack Sepkoski in 1981 using factor analysis of the fossil record. An evolutionary fauna typically displays an increase in biodiversity following a logistic curve followed by extinctions (although the Modern Fauna has not yet exhibited the diminishing part of the curve).\n\nFauna I, known as \"Cambrian\", described as a \"Trilobite-rich assemblage\", encompasses the bulk of the fossils which first appeared in the Cambrian explosion, and largely became extinct in the Ordovician-Silurian extinction event. This fauna comprises trilobites, small shelly fossils (grouped by Sepkoski into \"Polychaeta\", but including cribricyathids, coleolids, and volborthellids), Monoplacophora, inarticulate brachiopods and hyoliths.\n\nFauna II, known as \"Paleozoic\", described as a \"Brachiopod-rich assemblage\", accounts for most of the fossils appearing in the Great Ordovician Biodiversification Event, and largely became extinct in the Permian-Triassic extinction event. This fauna is marked by fossils of the following classes: Articulata, Crinoidea, Ostracoda, Cephalopoda, Anthozoa, Stenolaemata, Stelleroidea.\n\nFauna III, known as \"Modern\", described as a Mollusc-rich assemblage\", arose largely in the Mesozoic-Cenozoic Radiation, still in progress.The following classes are included: Gastropoda, Bivalvia, Osteichthyes, Malacostraca, Echinoidea, Gymnolaemata, Demospongiae, Chondrichthyes.\n\nIn the mid-19th century, John Phillips suggested three great systems: Palaeozoic, Mesozoic and Caenozoic. Writing after Sepkoski, Brenchley and Harper suggested that there were two early evolutionary faunas before the three of Sepkoski: Ediacaran and Tomottian. They also point out similarities with four \"evolutionary terrestrial plant floras\": Early Vascular, Pteridophytes, Gymnospores, Angiospores; and three \"evolutionary terrestrial tetrapod faunas\": \"Megadynasty I (Carboniferous-early Permian)\" \"primitive amphibians and reptiles, most notably ... \"Dimetrodon\"\", \"Megadynasty II (early Permian-mid-Triassic)\" \"mammal-like therapsids\", and \"Megadynasty III (late Triassic-Cretaceous)\" \"included the age of the dinosaurs\".\n\n", "id": "39991948", "title": "Evolutionary fauna"}
{"url": "https://en.wikipedia.org/wiki?curid=40041053", "text": "Genetic purging\n\nGenetic purging is the reduction of the frequency of a deleterious allele occurred by an increased efficiency of natural selection that is prompted by inbreeding.\n\nPurging occurs because many deleterious alleles only express all their harmful effect in homozygosis. During inbreeding, as related individuals mate, they produce offspring that are more likely to be homozygous, so that these deleterious alleles appear more often in homozygous individuals that are less fit and that pass fewer copies of their genes to future generations. This allows natural selection to purge the deleterious alleles.\n\nPurging reduces both the overall number of recessive deleterious alleles and the decline of mean fitness caused by inbreeding (the inbreeding depression for fitness).\n\nThe term \"purge\" is sometimes used for selection against deleterious alleles in a general way. It would avoid ambiguity to use \"purifying selection\" in that general context, and to reserve purging to its more strict meaning defined above.\n\nDeleterious alleles segregating in populations of diploid organisms have a remarkable trend to be, at least, partially recessive. This means that, when they occur in homozygosis (double copies), they reduce fitness by more than twice than when they occur in heterozygosis (single copy). In other words, part of their potential deleterious effect is hidden in heterozygosis but expressed in homozygosis, so that selection is more efficient against them when they occur in homozygosis. Since inbreeding increases the probability of being homozygous, it increases the fraction of the potential deleterious effect that is expressed and, therefore, exposed to selection. This causes some increase in the selective pressure against (partially) recessive deleterious alleles, which is known as purging. Of course, it also causes some reduction in fitness, which is known as inbreeding depression.\n\nPurging can reduce the average frequency of deleterious alleles across the genome below the value expected in a non-inbred population. Although this reduction usually does not compensate for all the negative effects of inbreeding, it has several beneficial consequences for fitness. A consequence is the reduction of the so-called inbreeding load. This means that, after purging, further inbreeding is expected to be less harmful. But the most immediate consequence is the reduction of the actual inbreeding depression of fitness: due to purging, mean fitness declines less than would be expected just from inbreeding and, after some initial decline, it can even rebound up to almost its value before inbreeding.\n\nAccounting for purging when predicting inbreeding depression is important in evolutionary genetics, because the fitness decline caused by inbreeding can be determinant in the evolution of diploidy, sexual reproduction and other main biological features. It is also important in animal breeding and, of course, in conservation genetics, because inbreeding depression may be a relevant factor determining the extinction risk of endangered populations, and because conservation programs can allow some breeding handling in order to control inbreeding.\n\nIn brief, Due to purging, inbreeding depression is not proportional to the standard measure of inbreeding (Wright's inbreeding coefficient ), since this measure only applies to neutral alleles. Instead, fitness decline is proportional to \"purged inbreeding\" , which gives the probability of being homozygous for deleterious alleles due to inbreeding, taking into account how they are being purged.\n\nPurging reduces inbreeding depression in two ways: first, it slows its progress; second, it reduces the overall inbreeding depression expected in the long term. The slower the progress of inbreeding, the more efficient is purging.\n\nIn the absence of natural selection, mean fitness would be expected to decline exponentially as inbreeding increases, where inbreeding is measured using Wright's inbreeding coefficient (the reason why decline is exponential on instead of linear is just that fitness is usually considered a multiplicative trait). The rate at which fitness declines as increases (the inbreeding depression rate \"δ\") depends on the frequencies and deleterious effects of the alleles present in the population before inbreeding.\n\nThe above coefficient is the standard measure of inbreeding, and gives the probability that, at any given neutral locus, an individual has inherited two copies of a same gene of a common ancestor (i.e. the probability of being homozygous \"by descent\"). In simple conditions, can be easily computed in terms of population size or of genealogical information. is often denoted using lowercase (), but should not be confused with the coancestry coefficient.\n\nHowever, the above prediction for the fitness decline rarely applies, since it was derived assuming no selection, and fitness is precisely the target trait of natural selection. Thus, Wright's inbreeding coefficient for neutral loci does not apply to deleterious alleles, unless inbreeding increases so fast that the change in gene frequency is governed just by random sampling (i.e., by genetic drift).\n\nTherefore, the decline of fitness should be predicted using, instead of the standard inbreeding coefficient , a \"purged inbreeding coefficient\" () that gives the probability of being homozygous by descent for (partially) recessive deleterious alleles, taking into account how their frequency is reduced by purging. Due to purging, fitness declines at the same rate than in the absence of selection, but as a function of instead of .\nThis purged inbreeding coefficient can also be computed, to a good approximation, using simple expressions in terms of the population size or of the genealogy of individuals (see BOX 1). However this requires some information on the magnitude of the deleterious effects that are hidden in the heterozygous condition but become expressed in homozygosis. The larger this magnitude, denoted purging coefficient \"d\", the more efficient is purging.\n\nAn interesting property of purging is that, during inbreeding, while increases approaching a final value , can approach a much smaller final value. Hence, it is not just that purging slows the fitness decline, but also that it reduces the overall fitness loss produced by inbreeding in the long term. This is illustrated in BOX 2 for the extreme case of inbreeding depression caused by recessive lethals, which are alleles that cause death before reproduction but only when they occur in homozygosis. Purging is less effective against mildly deleterious alleles than against lethal ones but, in general, the slower is the increase of inbreeding , the smaller becomes the final value of the purged inbreeding coefficient and, therefore, the final reduction in fitness. This implies that, if inbreeding progresses slowly enough, no relevant inbreeding depression is expected in the long term. This results in the fitness of a small population, that has been a small population for a long time, can be the same as a large population with more genetic diversity. In conservation genetics, it would be very useful to ascertain the maximum rate of increase of inbreeding that allows for such efficient purging.\n\nConsider a large non-inbred population with mean fitness . Then, the size of the population reduces to a new smaller value (in fact, the effective population size should be used here), leading to a progressive increase of inbreeding.\n\nThen inbreeding depression occurs at a rate , due to (partially) recessive deleterious alleles that were present at low frequencies at different loci. This means that, in the absence of selection, the expected value for mean fitness after generations of inbreeding, would be:\n\nformula_1\n\nwhere formula_2 is the population mean for Wright's inbreeding coefficient after generations of inbreeding.\n\nHowever, since selection operates upon fitness, mean fitness should be predicted taking into account both inbreeding and purging, as\n\nformula_3\n\nIn the above equation, formula_4 is the average \"purged inbreeding coefficient\" after generations of inbreeding. It depends upon the \"purging coefficient\" , which represents the deleterious effects that are hidden in heterozygosis but exposed in homozygosis.\n\nThe average \"purged inbreeding coefficient\" can be approximated using the recurrent expression\n\nformula_5\n\nThere are also predictive equations to be used with genealogical information.\n\nAs an example of genetic purging, consider a large population where there are recessive lethal alleles segregating at very low frequency in many loci, so that each individual carries on the average one of these alleles. Although about 63% of the individuals carry at least one of these lethal alleles, almost none carry two copies of the same lethal. Therefore, since lethals are considered completely recessive (i.e., they are harmless in heterozygosis), they cause almost no deaths. Now assume that population size reduces to a small value (say \"N\"=10), and remains that small for many generations. As inbreeding increases, the probability of being homozygous for one (or more) of these lethal alleles also increases, causing fitness to decline. However, as those lethals begin to occur in homozygosis, natural selection begins purging them. The figure to the right gives the expected decline of fitness against the number of generations, taking into account just the increase in inbreeding (red line), or both inbreeding and purging (blue line, computed using the purged inbreeding coefficient ). This example shows that purging can be very efficient preventing inbreeding depression. However, for non-lethal deleterious alleles, the efficiency of purging would be smaller, and it can require larger populations to overcome genetic drift.\n\nThe yeast Saccharomyces cerevisiae and Saccharomyces paradoxus have a life cycle that alternates between long periods of asexual reproduction as a diploid, ending in meiosis that is usually immediately followed selfing, with only rare outcrossing. Recessive deleterious mutations accumulate during the diploid expansion phase, and are purged during selfing: this purging has been termed \"genome renewal\".\n\nWhen a previously stable population undergoes inbreeding, if nothing else changes, natural selection should consist mainly of purging. The joint consequences of inbreeding and purging on fitness vary depending on many factors: the previous history of the population, the rate of increase of inbreeding, the harshness of the environment or of the competitive conditions, etc. The effects of purging were first noted by Darwin in plants, and have been detected in laboratory experiments and in vertebrate populations undergoing inbreeding in zoos or in the wild, as well as in humans. The detection of purging is often obscured by many factors, but there is consistent evidence that, in agreement with the predictions explained above, slow inbreeding results in more efficient purging, so that a given inbreeding F leads to less threat to population viability if it has been produced more slowly.\n\nNevertheless, in practical situations, the genetic change in fitness also depends on many other factors, besides inbreeding and purging. For example, adaptation to changing environmental conditions often causes relevant genetic changes during inbreeding. Furthermore, if inbreeding is due to a reduction in population size, selection against new deleterious mutations can become less efficient, and this can induce additional fitness decline in the medium-long term.\n\nIn addition, part of the inbreeding depression could not be due to deleterious alleles, but to an intrinsic advantage of being heterozygous compared to being homozygous for any available allele, which is known as overdominance. Inbreeding depression caused by overdominance cannot be purged, but seems to be a minor cause of overall inbreeding depression, although its actual importance is still a matter of debate.\n\nTherefore, predicting the actual evolution of fitness during inbreeding is highly elusive. However, the component of fitness decline expected from inbreeding and purging on deleterious alleles could be predicted using .\n", "id": "40041053", "title": "Genetic purging"}
{"url": "https://en.wikipedia.org/wiki?curid=39936262", "text": "Homology (psychology)\n\nHomology in psychology, as in biology, refers to a relationship between characteristics that reflects the characteristics' origins in either evolution or development. Homologous behaviors can theoretically be of at least two different varieties. As with homologous anatomical characteristics, behaviors present in different species can be considered homologous if they are likely present in those species because the behaviors were present in a common ancestor of the two species. Alternatively, in much the same way as reproductive structures (e.g., the penis and the clitoris) are considered homologous because they share a common origin in embryonic tissues, behaviors—or the neural substrates associated with those behaviors—can also be considered homologous if they share common origins in development.\n\nBehavioral homologies have been considered since at least 1958, when Konrad Lorenz studied the evolution of behavior. More recently, the question of behavioral homologies has been addressed by philosophers of science such as Marc Ereshefsky, psychologists such as Drew Rendall, and neuroscientists such as Georg Striedter and Glenn Northcutt. It is debatable whether the concept of homology is useful in developmental psychology.\n\nFor example, D. W. Rajecki and Randall C. Flanery, using data on humans and on nonhuman primates, argue that patterns of behaviour in dominance hierarchies are homologous across the primates.\n", "id": "39936262", "title": "Homology (psychology)"}
{"url": "https://en.wikipedia.org/wiki?curid=20693354", "text": "Deployment cost–benefit selection in physiology\n\nDeployment cost–benefit selection in physiology concerns the costs and benefits of physiological process that can be deployed and selected in regard to whether they will increase or not an animal’s survival and biological fitness. Variably deployable physiological processes relate mostly to processes that defend or clear infections as these are optional while also having high costs and circumstance linked benefits. They include immune system responses, fever, antioxidants and the plasma level of iron. Notable determining factors are life history stage, and resource availability.\n\nActivating the immune system has the present and future benefit of clearing infections, but it is also both expensive in regard to present high metabolic energy consumption, and in the risk of resulting in a future immune related disorder. Therefore, an adaptive advantage exists if an animal can control its deployment in regard to actuary-like evaluations of future benefits and costs as to its biological fitness. In many circumstances, such trade-off calculations explain why immune responses are suppressed and infections are tolerated. Circumstances where immunity is not activated due to lack of an actuarial benefit include:\n\nCost benefit trade-off actuary issues apply to the antibacterial and antiviral effects of fever (increased body temperature). Fever has the future benefit of clearing infections since it reduces the replication of bacteria and viruses. But it also has great present metabolic (BMR) cost, and the risk of hyperpyrexia. Where it is achieved internally, each degree raise in blood temperature, raises BMR by 10–15%. 90% of the total cost of fighting pneumonia, goes, for example, on energy devoted to raising body temperature. During sepsis, the resulting fever can raise BMR by 55%—and cause a 15% to 30% loss of body mass. Circumstances in which fever deployment is not selected or is reduced include:\n\nAntioxidants such as carotenoids, vitamin C, Vitamin E, and enzymes such as superoxide dismutase (SOD) and glutathione peroxidase (GPx) can protect against reactive oxygen species that damage DNA, proteins and lipids, and result in cell senescence and death. A cost exists in creating or obtaining these antioxidants. This creates a conflict between the biological fitness benefits of future survival compared with the use of these antioxidants to advantage present reproductive success. In some birds, antioxidants are diverted from maintaining the body to reproduction for this reason with the result that they have accelerated senescence Related to this, birds can show their biological capacity to afford the cost of diverting antioxidants (such as carotenoids) in the form of pigments into plumage as a costly signal.\n\nIron is vital to biological processes, not only of a host, but also to bacteria infecting the host. A biological fitness advantage can exist for hosts to reduce the availability of iron within itself to such bacteria (hypoferremia), even though this happens at a cost of the host impairing itself with anemia. The potential benefits of such self impairment is illustrated by the paradoxical effect that providing iron supplements to those with iron deficiency (which interferes with its antibacterial action) can result in an individual being cured of anemia but having increased bacterial illness.\n\n", "id": "20693354", "title": "Deployment cost–benefit selection in physiology"}
{"url": "https://en.wikipedia.org/wiki?curid=27828644", "text": "Host–parasite coevolution\n\nHost–parasite coevolution is a special case of coevolution, the reciprocal adaptive genetic change of a host and a parasite through reciprocal selective pressures.\n\nIt is characterized by reciprocal genetic change and thus changes in allele frequencies within populations. These are determined by three main types of selection dynamics: negative frequency-dependent selection when a rare allele has a selective advantage; overdominance caused by heterozygote advantage; and directional selective sweeps near an advantageous mutation.\n\nTheories of host–parasite coevolution include the geographic mosaic theory, which assumes a selection mosaic, coevolutionary hotspots, and geographic mixing; the Red Queen hypothesis, which proposes that parasitism favours sexual reproduction in the host; and an evolutionary trade-off between transmission and virulence, since if the parasite kills its host too quickly, the parasite will not be able to reproduce.\n\nModel systems include the nematode \"Caenorhabditis elegans\" with the bacterium \"Bacillus thuringiensis\"; the crustacean \"Daphnia\" and its numerous parasites; and \"Escherichia coli\" and the mammals (including humans) whose intestines it inhabits.\n\nHosts and parasites exert reciprocal selective pressures on each other, which may lead to rapid reciprocal adaptation. For organisms with short generation times, host–parasite coevolution can be observed in comparatively small time periods, making it possible to study evolutionary change in real-time under both field and laboratory conditions. These interactions may thus serve as a counter-example to the common notion that evolution can only be detected across extended time.\n\nThe dynamics of these interactions are summarized in the Red Queen hypothesis, namely that both host and parasite have to change continuously to keep up with each other's adaptations.\n\nHost–parasite coevolution is ubiquitous and of potential importance to all living organisms, including humans, domesticated animals and crops. Major diseases such as malaria, AIDS and influenza are caused by coevolving parasites. Better understanding of coevolutionary adaptations between parasite attack strategies and host immune systems may assist in the development of novel medications and vaccines.\n\nHost–parasite coevolution is characterized by reciprocal genetic change and thus changes in allele frequencies within populations. These changes may be determined by three main types of selection dynamics.\n\nAn allele is subject to negative frequency dependent selection if a rare allelic variant has a selective advantage. For example, the parasite should adapt to the most common host genotype, because it can then infect a large number of hosts. In turn, a rare host genotype may then be favored by selection, its frequency will increase and eventually it becomes common. Subsequently, the parasite should adapt to the former infrequent genotype.\n\nCoevolution determined by negative frequency dependent selection is rapid, potentially occurring across few generations. It maintains high genetic diversity by favoring uncommon alleles. This selection mode is expected for multicellular hosts, because adaptations can occur without the need for novel advantageous mutations, which are less likely to be frequent in these hosts because of relatively small population sizes and relatively long generation times.\n\nOverdominance occurs if the heterozygote phenotype has a fitness advantage over both homozygotes (heterozygote advantage, causing heterosis). One example is sickle cell anemia. It is due to a mutation in the hemoglobin gene leading to sickle shape formation of red blood cells, causing clotting in blood vessels, restricted blood flow, and reduced oxygen transport. At the same time, the mutation confers resistance to malaria, caused by \"Plasmodium\" parasites, which are passed off in red blood cells after transmission to humans by mosquitoes. Hence, homozygote and heterozygote genotypes for the sickle-cell disease allele show malaria resistance, while the homozygote suffers from severe disease phenotype. The alternative homozygote, which does not carry the sickle cell disease allele, is susceptible to infection by \"Plasmodium\". As a consequence, the heterozygote genotype is selectively favored in areas with a high incidence of malaria.\n\nIf an allele provides a fitness benefit, its frequency will increase within a population – selection is directional or positive. Selective sweeps are one form of directional selection, where the increase in frequency will eventually lead to the fixation of the advantageous allele. The process is considered to be slower in comparison to negative frequency dependent selection. It may produce an \"arms race\", consisting of the repeated origin and fixation of new parasite virulence and host defence traits.\n\nThis mode of selection is likely to occur in interactions between unicellular organisms and viruses due to large population sizes, short generation times, often haploid genomes and horizontal gene transfer, which increase the probability of beneficial mutations arising and spreading through populations.\n\nJohn N. Thompson's geographic mosaic theory of coevolution hypothesizes spatially divergent coevolutionary selection, producing genetic differentiation across populations. The model assumes three elements that jointly fuel coevolution:\n\n1) a selection mosaic among populations\n2) coevolutionary hotspots\n3) geographic mixing of traits\n\nAmong plants, \"Plantago lanceolata\" and its parasite the powdery mildew \"Podosphaera plantaginis\" have been intensively studied on the Aland islands in south-western Finland. \"P. plantaginis\" obtains nutrients from its host, a perennial herb, by sending feeding roots into the plant. There are more than 3000 host populations in this region, where both populations can evolve freely, in absence of human-imposed selection, in a heterogeneous landscape. Both partners can reproduce asexually or sexually. The system has spatially divergent coevolutionary dynamics across two metapopulations as predicted by the mosaic theory.\n\nThe New Zealand freshwater snail \"Potamopyrgus antipodarum\" and its different trematode parasites represent a rather special model system. Populations of \"P. antipodarum\" consist of asexual clones and sexual individuals and therefore can be used to study the evolution and advantages of sexual reproduction. There is a high correlation between the presence of parasites and the frequency of sexual individuals within the different populations. This result is consistent with the Red Queen hypothesis that sexual reproduction is favoured during host–parasite coevolution. At the same time, the persistence of sex may also rely on other factors, for example Muller's ratchet and/or the avoidance of the accumulation of deleterious mutations.\n\n\"Tribolium castaneum\", the red flour beetle, is a host for the microsporidian \"Nosema whitei\". This parasitoid kills its host for transmission, so the host's lifespan is important for the parasite's success. In turn, parasite fitness most likely depends on a trade-off between transmission (spore load) and virulence. A higher virulence would increase the potential for the production of more offspring, but a higher spore load would affect the host's lifespan and therefore the transmission rate. This trade-off is supported by coevolutionary experiments, which revealed the decrease of virulence, a constant transmission potential and an increase in the host's lifespan over a period of time.\nFurther experiments demonstrated a higher recombination rate in the host during coevolutionary interactions, which may be selectively advantageous because it should increase diversity of host genotypes.\n\nResources are generally limited. Therefore, investment in one trait (e.g. virulence or immunity) limits investment in other life-history traits (e.g. reproductive rate). Moreover, genes are often pleiotropic, having multiple effects. Thus, a change in a pleiotropic immunity or virulence gene can automatically affect other traits. There is thus a trade-off between benefits and costs of the adaptive changes that may prevent the host population from becoming fully resistant or the parasite population from being highly pathogenic. The costs of gene pleiotropy have been investigated in coevolving \"Escherichia coli\" and bacteriophages. To inject their genetic material, phages need to bind to a specific bacterial cell surface receptor. The bacterium may prevent injection by altering the relevant binding site, e.g. in response to point mutations or deletion of the receptor. However, these receptors have important functions in bacterial metabolism. Their loss would thus decrease fitness (i.e. population growth rate). As a consequence, there is a trade-off between the advantages and disadvantages of a mutated receptor, leading to polymorphism at this locus.\n\n \nThe nematode \"Caenorhabditis elegans\" and the bacterium \"Bacillus thuringiensis\" were only recently established as a model system for studying host–parasite coevolution. Laboratory evolution experiments provided evidence for many of the basic predictions of these coevolutionary interactions, including reciprocal genetic change, and increases in the rate of evolution and genetic diversity.\n\nThe crustacean \"Daphnia\" and its numerous parasites have become one of the main model systems for studying coevolution. \nThe host can be asexual as well as sexual (induced by changes in the external environment), so sexual reproduction can be stimulated in the laboratory. Decades of coevolution between \"Daphnia magna\" and the bacterium \"Pasteuria ramosa\" have been reconstructed, reanimating resting stages of both species from laminated pond sediments and exposing hosts from each layer to parasites from the past, the same and the future layers. The study demonstrated that parasites were on average most infective with their contemporary hosts, consistent with negative frequency dependent selection.\n\n\"Escherichia coli\", a Gram-negative proteobacterium, is a common model in biological research, for which comprehensive data on various aspects of its life-history is available. It has been used extensively for evolution experiments, including those related to coevolution with phages. These studies revealed – among others – that coevolutionary adaptation may be influenced by pleiotropic effects of the involved genes. In particular, binding of the bacteriophage to \"E. coli\" surface receptor is the crucial step in the virus infection cycle. A mutation in the receptor's binding site may cause resistance. Such mutations often show pleiotropic effects and may cause a cost of resistance. In the presence of phages, such pleiotropy may lead to polymorphisms in the bacterial population and thus enhance biodiversity in the community.\n\nAnother model system consists of the plant- and animal-colonizing bacterium \"Pseudomonas\" and its bacteriophages. This system provided new insights into the dynamics of coevolutionary change. It demonstrated that coevolution may proceed via recurrent selective sweeps, favouring generalists for both antagonists. Furthermore, coevolution with phages may promote allopatric diversity, potentially enhancing biodiversity and possibly speciation. Host-parasite coevolution may also affect the underlying genetics, for example by favouring increased mutation rates in the host.\n", "id": "27828644", "title": "Host–parasite coevolution"}
{"url": "https://en.wikipedia.org/wiki?curid=2918703", "text": "Sea Monsters (TV series)\n\nSea Monsters is a 2003 BBC television trilogy which used computer-generated imagery to show past life in Earth's seas. In the U.S. it was known as Chased by Sea Monsters. It was made by Impossible Pictures, the creators of \"Walking with Dinosaurs\", \"Walking with Beasts\" and \"Walking with Monsters\". In the series, the British wildlife presenter Nigel Marven is shown travelling to seven past seas in the history of the Earth and scuba diving there, in order of dangerousness with the most dangerous last. He travels in a white sailboat or motorboat roughly 24 m (80 ft) long named 'The Ancient Mariner'. His time travelling device is not mentioned or shown, and the closest thing to it is his time map, showing the timeline of the seven deadliest seas and the creatures that lived at the time. He uses a scuba set with a fullface mask so he can talk underwater to produce the commentary. He performs some dives using a strong shark cage, which is spherical to make it harder for large sea creatures to bite it.\n\n\"Sea Monsters\" has never been released on DVD in the UK, but featured on the American \"Chased by Dinosaurs\" DVD and a similar Region 2 Dutch DVD. It is also available on Netflix in the UK and the United States, where it is referred to as Episode 2 of \"Chased By Dinosaurs: Three Walking with Dinosaurs Adventures.\"\n\nAs originally broadcast, the first episode had three segments and the second and third two each; a format later inherited by \"Walking with Monsters\"\n", "id": "2918703", "title": "Sea Monsters (TV series)"}
{"url": "https://en.wikipedia.org/wiki?curid=40341221", "text": "Jeewanu\n\nJeewanu (Sanskrit for \"particles of life\") are synthetic chemical particles that possess cell-like structure and seem to have some functional properties; that is, they are a model of primitive cells, or protocells. It was first synthesised by Krishna Bahadur (20 January 1926 — 5 August 1994), an Indian chemist and his team in 1963. Using photochemical reaction, they produced coacervates, microscopic cell-like spheres from a mixture of simple organic and inorganic compounds. Bahadur named these particles 'Jeewanu' because they exhibited some of the basic properties of a cell, such as the presence of semipermeable membrane, amino acids, phospholipids and carbohydrates. Further, like living cells, they had several catalytic activities. Jeewanu are cited as models of protocells for the origin of life, and as artificial cells.\n\nJeewanu is derived from Sanskrit \"jeewa\", meaning \"life\", and \"anu\", meaning the \"smallest part of something\", or the \"indivisible\". In contemporary Hindi, \"jeewanu\" also means unicellular organisms such as bacteria. Bahadur specifically used the term to represent the Indian philosophical tradition not only through the use of Sanskrit but also by inferring ideas on the origin of life from the Vedas. Bahadur, while employing the traditional Hindu philosophy, attempted to incorporate the advances in cell biology to the concept of abiogenesis.\n\nIn 1954 and 1958 Krishna Bahadur and co-workers published the successful synthesis of amino acids from a mixture of paraformaldehyde, colloidal molybdenum oxide or potassium nitrate and ferric chloride under sunlight. It appears that this experimental approach was seminal for the assays to produce Jeewanu, which he first reported in 1963 in an obscure Indian journal, \"Vijnana Parishad Anusandhan Patrika\". His detailed syntheses were published in Germany in 1964 in a series of articles.\n\nTheir initial experiment consisted of a sterilised apparatus in which inorganic nitrogenous compounds (such as ammonium phosphate and ammonium molybdate) and organic compounds such as citric acid (CHO), paraformaldehyde (OH(CHO)H) and formaldehyde (CHO) for carbon sources — were mixed with minerals commonly found in living cells. Inorganic substances such as colloidal ferric chloride or molybdenum compounds supposedly acted as cofactors and catalysts.\n\nWhen the apparatus was exposed to sunlight for several days and constantly shaken, microscopic spherical particles were formed. The interesting features of these particles were that they were enclosed in a semipermeable membrane, like the typical cell membrane. Like living cells, they were reported to contain amino acids, phospholipid membrane and carbohydrates. In addition, they were claimed to have reproductive capability by budding, much like unicellular organisms, but did not grow on any bacterial culture medium. Bahadur reported that the Jeewanu exhibited various catalytic properties and produced their own peptides by metabolic reactions. Bahadur's later work on the Jeewanu also detected the presence of amino acids in peptide form and sugars in the form of ribose, deoxyribose, fructose and glucose, as well as nucleic acid bases (DNA and RNA building blocks) including adenine, guanine, cytosine, thymine and uracil. Bahadur also reported having detected ATPase-like and peroxidase-like activity. Bahadur stated that by using molybdenum as a cofactor, the Jeewanu showed capability of reversible photochemical electron transfer, and released a gas mixture of oxygen and hydrogen at a 1:2 ratio.\n\nBahadur's publications were ambivalently received, and the overall attention of the scientific community seemed limited since Krishna Bahadur and his co-workers reported that the Jeewanus are alive (a striking statement), the team changed the protocols frequently and documented them somewhat idiosyncratically. Bahadur defined \"living units\" as \"'\"[...] those which grow, multiply, and are metabolically active in a systematic, harmonious, and synchronized manner\".\" Then, NASA's Exobiology Division tasked two biologists in 1967 to review and evaluate the literature so far published by Krishna Bahadur (not to replicate the experiments) on the synthesis and characteristics of the Jeewanu. The two NASA biologists did not debate whether these three criteria are an adequate definition of life, but whether the Jeewanu satisfy these criteria. The NASA report concluded that \"the evidence presented on these three points is on the whole unconvincing\". The report also stated that the postulated existence of these living units has not been proved and \"the nature and properties of the Jeewanu remains to be clarified.\"\n\nIn the 1980s, the Hungarian chemist Tibor Gánti discussed the Jeewanu at length in his 'Chemoton theory' —an abstract model of autocatalytic chemical reactions— published first in Hungarian and translated into English in 2003. In the context of self-organizing structures, Gánti considered the Jeewanu a promising model system to understand the origin and fundamentals of life, and one that had never received due attention. In 2011, a German scientist stated that the Jeewanu story pertains to concepts of life, its beginnings, as well as possible artificially created cells.\n\nExperimental duplication work published in 2013 by Gupta and Rai reported that their size varies from 0.5 μ to 3.5 μ in diameter, growth from within, metabolic activities, and \"the presence of RNA-like material.\" The authors stated that the RNA-like material detected in the Jeewanu protocells support the RNA world hypothesis.\"\n\n\n", "id": "40341221", "title": "Jeewanu"}
{"url": "https://en.wikipedia.org/wiki?curid=40364158", "text": "Antibiotic use in livestock\n\nAntibiotic use in livestock is the use of antibiotics for any purpose in the husbandry of livestock, which includes treatment when ill (therapeutic), treatment of a batch of animals when at least one is diagnosed as ill (metaphylaxis, similar to the way bacterial meningitis is treated in children), and preventative treatment (prophylaxis) against disease. The use of subtherapeutic doses in animal feed and/or water to promote growth and improve feed efficiency was eliminated effective January 1, 2017, as a result of new FDA Veterinary Feed Directive. This practice has been banned in Europe since 2006. This article looks at antibiotic use for growth promotion and the situation in the United States and does not cover therapy, prophylaxis or metaphylaxis in Europe.\n\nAntimicrobials (including antibiotics and antifungals) and other drugs can only be used by veterinarians and livestock owners in the U.S. for treatment, control, or prevention of diseases. Some other countries outside Europe can use antimicrobials to increase the growth rates of livestock, poultry, and other farmed animals, although these pharmaceuticals do not always have to be administered by a veterinarian.\n\nThere are also global concerns over the use of antibiotics for growth promotion or therapy purposes because of the potential for some drugs to enter the human food chain despite rigorous withdrawal measures and testing to prevent antibiotic residues in food, increasing antibiotic resistance in animals, a potential although largely unproven link to antibiotic-resistant infections in humans, and what some consider antibiotic misuse. Other drugs may be used only under strict limits, and some organizations and authorities seek to further restrict the use of some or all drugs in animals. Other authorities, such as the World Organization for Animal Health, say that \"Without antibiotics there would be supply problems of animal protein for the human population\".\n\nHowever, in 2013 the CDC finalized and released a report detailing antibiotic resistance and classified the top 18 resistant bacterium as either being urgent, serious or concerning threats (CDC). Of those organisms, three (CDIFF, CRE and Neisseria gonorrhoeae) have been classified as urgent threats and require more monitoring and prevention (CDC). In the US alone, more than 2 million people are diagnosed with antibiotic resistant infections and over 23,000 die per year due to resistant infections (CDC).\n\nGiven the concerns about antibiotic use for feed conversion, research into alternatives is ongoing.\n\nIn 1910 in the United States, a meat shortage resulted in protests and boycotts. After this and other shortages, the public demanded government research into stabilization of food supplies.\nSince the 1900s, livestock production on United States farms has had to rear larger quantities of animals over a short period of time to meet new consumer demands. Factory farming or the use of high intensity feedlots originated in the late 19th century when advances in technology and science allowed for mass production of livestock. Global agriculture production doubled four times within 1820 and 1975, feeding one billion in 1800 and up to 6.5 billion in 2002. Along with the new large animal densities came the threat of disease, therefore requiring a greater disease control of these animals.\nIn 1950, a group of United States scientists found that adding antibiotics to animal feed increases the growth rate of livestock. American Cyanamid published research establishing the practice.\n\nBy 2001 this practice had grown so much that a report by the Union of Concerned Scientists found that nearly 90% of the total use of antimicrobials in the United States was for non-therapeutic purposes in agricultural production.\n\nAntibiotics have an appropriate place in the humane care of illness in livestock, when they reduce the suffering of a sick animal or control the spread of the illness to nearby animals. Thus, ideas that they should \"never\" be used in livestock husbandry are misguided. Instead, the goal is to prevent the allowance of preventive use from being distorted into routine use, constituting overuse.\n\nCertain antibiotics, when given in low, sub-therapeutic doses, are known to improve feed conversion efficiency (more output, such as muscle or milk, for a given amount of feed) and/or may promote greater growth, most likely by affecting gut flora. However, any antibiotics deemed medically important to humans by the CDC are illegal to use as growth promoters in the U.S. Only drugs that have no association with human medicine – and therefore no risk to humans – are allowed to be used for this purpose. It is also important to note that some drugs listed below are ionophores, which are not antibiotics and do not pose any potential risk to human health.\n\nThe use of antibiotics to increase the growth of pigs is most studied of all livestock. This use for growth rather than disease prevention is referred to as subtherapeutic antibiotic use. Studies have shown that administering low doses of antibiotics in livestock feed improves growth rate, reduces mortality and morbidity, and improves reproductive performance. It is estimated that over one-half of the antibiotics produced and sold in the United States is used as a feed additive. Although it is still not completely understood why and how antibiotics increase the growth rate of pigs, possibilities include metabolic effects, disease control effects, and nutritional effects. While subtherapeutic use has many benefits for raising swine, there is growing concern that this practice leads to increased antibiotic resistance in bacteria. Antibiotic resistance occurs when bacteria are resistant to one or more microbial agents that are usually used to treat infection. There are three stages in the possible emergence and continuation of antibiotic resistance: genetic change, antibiotic selection, and spread of antibiotic resistance.\n\nThe use of drugs in food animals is regulated in nearly all countries. Historically, this has been to prevent alteration or contamination of meat, milk, eggs and other products with toxins that are harmful to humans. Treating a sick animal with drugs may lead to some of those drugs remaining in the animal when it is slaughtered or milked. Scientific experiments provide data that shows how long a drug is present in the body of an animal and what the animal's body does to the drug. Of particular concern are drugs that may be passed into milk or eggs. By the use of 'drug withdrawal periods' before slaughter or the use of milk or eggs from treated animals, veterinarians and animal owners ensure that the meat, milk and eggs is free of contamination.\n\nThese restrictions include not only poisons or drugs (such as penicillin) which may result in allergic reactions but also contaminants which may cause cancer. It is illegal in the US to administer drugs or feed substances to animals if they have been shown to cause cancer.\n\nOne of the main restrictions is the amount that is administered to animals in the industry. These drugs should be administered to healthy livestock at a low concentration of 200 g per ton of feed. The amount distributed is also altered throughout the lifespan of livestock in order to meet specific growth needs.\n\nLegality of the use of specific drugs in animal medicine varies according to location.\n\nJust as in human medicine, some drugs are available over the counter and others are restricted to use only on the prescription of a veterinary physician. In the US, the Food and Drug Administration (FDA) requires specific labels on all drugs, giving directions on the use of the drug. For animals, this includes the species, dose, reason for giving the drug (indication) and the required withdrawal period, if any. Federal law requires laypersons to use drugs only in the manner listed. Veterinarians who have examined an animal or a herd of animals may issue a replacement label, giving new directions, based on their medical knowledge, unless it is a feed-grade antibiotic (is administered through the feed or water) in which case the veterinarian cannot issue directions different than the label. It is illegal in the US for any layperson to administer any drug to a food animal in a way not specific to the drug label. Over-the-counter drugs which may be used by laypersons include anti-parasite drugs (including fly sprays) and antimicrobials. These drugs can be applied as sprays, creams, injections, oral pills or fluids, or as a feed additive, depending on the drug and the label.\n\nIn December 2013, the FDA updated its regulations to try to begin reducing use of antibiotics for growth enhancement. Significant lobbying comes from all directions, from those against tighter regulation to those who complain it doesn't go far enough.\n\nCurrently few policies, regulations and laws exist that promote limitation of antibiotic use on factory farms. In addition, few policies are being created that call for this decrease in antibiotic use. However, numerous state senators and members of congress showed support for the Preventing Antibiotic Resistance Act of 2015 (PARA) and the Preservation of Antibiotics for Medical Treatment Act of 2013 (PAMTA). These acts proposed amendments be made to the Federal Food, Drug and Cosmetic Act which would limit and preserve the use of antibiotics for medically necessary situations. Both of these bills died in Congress in 2015.\n\nIn 2015, the FDA approved a new Veterinary Feed Directive (VFD), which is an updated rule that give instructions to pharmaceutical companies, veterinarians, and producers about how to administer necessary drugs through the animal's feed and water. This new rule followed through on the FDA's commitment to phase out the use of antibiotics for growth promotion and increased feed conversion. It is now illegal to use any antimicrobial that is medically important to humans for anything other than treatment, control, and prevention of disease. Furthermore, even then, producers now have to have a licensed veterinarian sign a legal form - much like a prescription - for producers to purchase, store, or administer these products. This document gives specific instructions about the animals to be fed, the dates when they will be fed, and the concentration. Violating a VFD is now a violation of federal law. This is a major step toward reducing antibiotic resistance in both animals and humans. The new VFD took effect on January 1, 2017.\n\nDrugs can be administered to animals in a variety of means, just as with humans. Among these are topical (on the skin), by injection (including intravenous, subcutaneous, subcutaneous implants, intramuscular and intraperitoneal), and orally. Oral drugs can be in pill or liquid form, or can be given by mixing with feed or drinking water. The appropriate route for treatment depends on the specific case and can vary by: illness, severity of illness, selected drug, age or condition of the animal, species of the animal, type of housing and other factors. For animals that are not regularly fed a concentrated feed or which can be handled repeatedly, a slow-release injection might be the most appropriate. Some drugs are not available or appropriate in this form and should be delivered orally. For animals that are fed regularly (rather than grazing freely) or that can not be easily handled, the most appropriate means of administering the drug may be to include the drug in feed or water. This eliminates the stress of daily (or more frequent) handling of animals, which can make the animals more ill. Poultry are most commonly medicated in this fashion, as they are easily stressed to the point of dying. Administering the drug by feed also prevents injection wounds in animals.\n\nThe timely administration of drugs is key to preventing animal suffering and economic loss to the farmer. Animals which are ill can infect other animals, and may become so ill that they can not be sold. A variety of techniques are used to monitor animals for illness so that they can be treated appropriately. Stress reduction, adequate nutrition, shelter, and quarantine of incoming stock are all important factors to promote growth and reduce illness and the need for active treatment. The age and status of an animal is also important in determining correct treatment – a young animal or pregnant animal is at greater risk and are treated more aggressively than an older animal. Specifically in calves, the period in which they begin to separate from their mothers generates stress and makes them more susceptible to catching an infection like pneumonia. Antibiotics are commonly administered in the calves' feed during this time to fight the possibility of stress-induced infections. Feed antibiotics are also used to prevent illnesses in calves caused by liver abscesses that develop during their last stages of growth.\n\nThe European Union (EU) in 1999 implemented an antibiotic resistance monitoring program and phase out plan for all antibiotic use by 2006. Although the European Union banned the use of antibiotics as growth agents from 2006, its use has not changed much until recently. In Germany, 1,734 tons of antimicrobial agents were used for animals in 2011 compared with 800 tons for humans. On the other hand, Sweden banned their use in 1986 and Denmark started cutting down drastically in 1994, so that its use is now 60% less. In the Netherlands, the use of antibiotics to treat diseases increased after the ban on its use for growth purposes in 2006. In 2011, the EU voted to ban the prophylactic use of antibiotics, alarmed at signs that the overuse of antibiotics is blunting their use for humans.\n\nIn 2011, a total of 13.6 million kilograms of antimicrobials were sold for use in food-producing animals in the United States, which represents 80% of all antibiotics sold or distributed in the United States. Of the antibiotics given to animals from 2009 through 2013, just above 60% distributed for food animal use are \"medically-important\" drugs, that are also used in humans. The rest are drug classes like ionophores which are not used in human medicine. Due to concerns about the overuse of antibiotics in food-producing animals, the U.S. Food & Drug Administration has implemented new industry guidelines that will restrict the use of medically-important drugs to uses \"that are considered necessary for assuring animal health\" and will require veterinary oversight. The food animal and veterinary pharmaceutical industries will need to phase out medically important antimicrobial use by January 1, 2017.\n\nEighty percent of antibiotics sold in the United States are used on livestock. The majority of these antibiotics are given to animals that are otherwise healthy. Rather, it is normal practice to mix antibiotics with fodder to promote healthier living conditions and to encourage animal growth. The use of antibiotics in animals is to a large degree involved in the emergence of antibiotic-resistant microorganisms. Antibiotics are used in food with the intention of not only preventing, controlling, and treating diseases, but also to promote growth. Antibiotic use in animals can be classified into therapeutic, prophylactic, metaphylactic, and growth promotion uses of antibiotics. All four patterns select for bacterial resistance, since antibiotic resistance is a natural evolutionary process, but the non-therapeutic uses expose larger number of animals, and therefore of bacteria, for more extended periods, and at lower doses. They therefore greatly increase the cross-section for the evolution of resistance.\n\nIn 2001, the Union of Concerned Scientists estimated that greater than 70% of the antibiotics used in the U.S. are given to food animals (for example, chickens, pigs, and cattle), in the absence of disease. The amounts given are termed \"sub-therapeutic\", i.e., insufficient to combat disease. Despite no diagnosis of disease, the administration of these drugs (most of which are not significant to human medicine) results in decreased mortality and morbidity and increased growth in the animals so treated. It is theorized that sub-therapeutic dosages kills some, but not all, of the bacterial organisms in the animal – likely leaving those that are naturally antibiotic-resistant. Studies have shown, however, that, in essence, the overall population levels of bacteria are unchanged; only the mix of bacteria is affected. The actual mechanism by which sub-therapeutic antibiotic feed additives serve as growth promoters is thus unclear. Some people have speculated that animals and fowl may have sub-clinical infections, which would be cured by low levels of antibiotics in feed, thereby allowing the creatures to thrive. No convincing evidence has been advanced for this theory, and the bacterial load in an animal is essentially unchanged by use of antibiotic feed additives. The mechanism of growth promotion is therefore probably something other than \"killing off the bad bugs\".\n\nAntibiotics are used in U.S. animal feed to promote animal productivity. In particular, poultry feed and drinking water is a common route of administration of drugs, because of higher overall costs when drugs are administered by handling animals individually.\n\nIn research studies, occasional animal-to-human spread of antibiotic-resistant organisms has been demonstrated. Resistant bacteria can be transmitted from animals to humans in three ways: by consuming animal products (milk, meat, eggs, etc.), from close or direct contact with animals or other humans, or through the environment. In the first pathway, food preservation methods can help eliminate, decrease, or prevent the growth of bacteria in some food classes. Evidence for the transfer of macrolide-resistant microorganisms from animals to humans has been scant, and most evidence shows that pathogens of concern in human populations originated in humans and are maintained there, with rare cases of transference to humans.\n\nChina produces and consumes the most antibiotics of all countries.\n\nAntibiotic use has been measured by checking the water near factory farms in China. Measurements have also been taken from animal dung.\n\nIt was calculated that 38.5 million kg (or 84.9 million lbs) of antibiotics were used in China's swine and poultry production in 2012.\n\nIn 2012 India manufactured about a third of the total amount of antibiotics in the world.\n\nBrazil is the world's largest exporter of beef and the government regulates antibiotic use in the cattle production industry.\n\nMore recently, there has been increased concern about the use of anti-microbials in animals (including pets, livestock, and companion animals) contributing to the rise in antibiotic resistant infections in humans. The use of antimicrobials has been linked to the rise of resistance in every drug and species where it has been studied, including humans and livestock. However, the role of antibiotic use in food animals – in contrast to the use of antibiotics in humans – in the rise of resistant infections in humans is in dispute. The use of antimicrobials in various forms is widespread throughout animal industry, and is presented as key to preventing animal suffering and economic loss. It is linked by some activist groups to animal welfare concern, large scale commercial agriculture, international food trade, agricultural protectionist laws, environmental protection (including climate change) and other topics, which make the aims of some groups on both sides of the debate difficult to untangle.\n\nAround 70% of all antibiotics administered are used for livestock. Most of the drugs that are given to livestock are misused and incorporated into their diets daily for the purpose of weight gain or to treat illnesses. The overuse of the antibiotic in livestock is harmful to humans because it creates an antibiotic resistant bacteria that can be transferred through several different ways such as: raw meats, consumption of meats, or it can also be airborne. Waste from food-producing animals can also contain antibiotic-resistant bacteria and is sometimes stored in lagoons. This waste is often sprayed as fertilizer and can thus contaminate crops and water with the antibiotic-resistant bacteria. Antibiotic resistance is harmful to humans because it makes them resistant to certain types of drugs for different diseases, and makes it harder for them to fight off infections.\n\nThe World Health Organization has published a list of Critically Important Antimicrobials for Human Medicine with the intent that it be used \"as a reference to help formulate and prioritize risk assessment and risk management strategies for containing antimicrobial resistance due to human and non-human antimicrobial use.\"\n\nThe practice of using antibiotics for growth stimulation is problematic for these reasons:\n\nDonald Kennedy, former director of the United States Food and Drug Administration, has said \"There's no question that routinely administering non-therapeutic doses of antibiotics to food animals contributes to antibiotic resistance.\" David Aaron Kessler, another former director of the FDA, said that \"We have more than enough scientific evidence to justify curbing the rampant use of antibiotics for livestock, yet the food and drug industries are not only fighting proposed legislation to reduce these practices, they also oppose collecting the data.\"\n\nIn 2013 the United States Centers for Disease Control and Prevention (CDC) published a white paper discussing antibiotic resistance threats in the US and calling for \"improved use of antibiotics\" among other measures to contain the threat to human health. The CDC asked leaders in agriculture, healthcare, and other disciplines to work together to combat the issue of increasing antibiotic resistance.\n\nSome scientists have said that \"all therapeutic antimicrobial agents should be available only by prescription for human and veterinary use.\"\n\nThe Pew Charitable Trusts have stated that \"hundreds of scientific studies conducted over four decades demonstrate that feeding low doses of antibiotics to livestock breeds antibiotic-resistant superbugs that can infect people. The FDA, the U.S. Department of Agriculture and the Centers for Disease Control and Prevention all testified before Congress that there is a definitive link between the routine, non-therapeutic use of antibiotics in food animal production and the challenge of antibiotic resistance in humans.\"\n\nThe World Organisation for Animal Health has acknowledged the need to protect antibiotics but argued against a total ban on antibiotic use in animal production.\n\nIn 2011 the National Pork Producers Council, an American trade association, has said \"Not only is there no scientific study linking antibiotic use in food animals to antibiotic resistance in humans, as the U.S. pork industry has continually pointed out, but there isn't even adequate data to conduct a study.\" The statement contradicts scientific consensus, and was issued in response to a United States Government Accountability Office report that asserts \"antibiotic use in food animals contributes to the emergence of resistant bacteria that may affect humans\".\n\nThe National Pork Board, a Government-owned corporation of the United States, has said that \"the vast majority of producers use (antibiotics) appropriately.\"\n\nWhen government regulation restricts use of antibiotics the negative economic impact is not often considered.\n\nRegulation of antibiotics in livestock production would affect the business models of corporations including Tyson Foods, Cargill, and Hormel.\n\nIt is difficult to set up a comprehensive surveillance system for measuring rates of change in antibiotic resistance. The US Government Accountability Office published a report in 2011 stating that government and commercial agencies had not been collecting sufficient data to make a decision about best practices.\n\nCurrently there is no regulatory agency in the United States that systematically collects detailed data on antibiotic use in humans and animals. It is not clear which antibiotics are prescribed for which purpose and at what time. Furthermore, the world has no surveillance infrastructure to monitor emerging antibiotic resistance threats. Because of these issues, it is difficult to quantify antibiotic resistance, to regulate antibiotic prescribing practices, and to detect and respond to rising threats.\n\nThere have been many studies that document antibiotic resistant bacteria in livestock, though the impact of the different bacteria in humans is still undergoing research. At this time, the most well-documented impact on humans is foodborne gastrointestinal illness. In most cases, these illnesses are mild and do not require antibiotics; though if the infectious bacteria is drug-resistant, research has shown that these bacteria have increased virulence (ability to cause disease), leading to prolonged illness. Furthermore, in approximately 10% of cases, the disease becomes severe, requiring more advanced treatments. These treatments can take the form of intravenous antibiotics, supportive care for blood infections, and hospital stays, leading to higher costs and greater morbidity with a trend toward higher mortality. Severe disease with this outcome is more common with drug-resistant bacteria. Though all people are susceptible, populations shown to be at higher risk for severe disease include children, the elderly, and those with chronic disease.\n\nOver the past 20 years, the most common drug-resistant foodborne bacteria in industrialized countries have been non-typhoidal salmonella and campylobacter. Research has consistently shown the main contributing factors are bacteria sourced in livestock. One example of this was a 1998 outbreak of multidrug-resistant salmonella in Denmark linked back to two Danish swine herds. Coupled with the discovery of this link, there have been improved monitoring systems that have helped to quantify the impact. In the United States, it is estimated that there are approximately 400,000 cases and over 35,000 hospitalizations per year attributable to increasing resistant strains of salmonella and campylobacter. In terms of financial impact in the US, the treatment of non-typhoidal salmonella infections alone is now estimated to cost $365 million per year. In light of this, in its inaugural 2013 report on antibiotic resistance threats in the United States, the CDC identified resistant non-typhoidal salmonella and campylobacter as \"serious threats\" and called for improved surveillance and intervention in food production moving forward.\n\nThere are other bacteria as well, where research is evolving and revealing that bacterial resistance acquired through use in livestock may be contributing to disease in humans. Examples of these include Enterococcus, E. coli 0157 and Staphylococcus Aureus. In the case of foodborne illness from E.coli, though it is still not typically treated with antibiotics because of associated risk of renal failure, increasing rates of antibiotic resistant infections have been correlated with increasing virulence of the bacteria. In the case of enterococcus and staphylococcus aureus, resistant forms of both of these bacteria have resulted in greatly increasing morbidity and mortality in the US. At this point, there have been studies, though a limited number, that definitively link antibiotic use in food production to these resistance patterns in humans and further research will help to further characterize this relationship.\n\nHumans can be exposed to antibiotic-resistant bacteria by ingesting them through the food supply. Dairy products, ground beef and poultry are the most common foods harboring these pathogens. There is evidence that a large proportion of resistant \"E. coli\" isolates causing blood stream infections in people are from livestock produced as food.\n\nWhen manure from antibiotic-fed swine is used as fertilizer elsewhere, the manure may be contaminated with bacteria which can infect humans.\n\nStudies have also shown that direct contact with livestock can lead to the spread of antibiotic-resistant bacteria from animals to humans.,\n\nLegislation and activism worldwide have aimed at restricting antibiotic use in livestock.\n\nOn 1 January 2006 the European Union banned the non-medicinal use of antibiotics in livestock production.\n\nDuring 2007, two federal bills (S. 549 and H.R. 962) aimed at phasing out \"nontherapeutic\" antibiotics in U.S. food animal production. The Senate bill, introduced by Sen. Edward \"Ted\" Kennedy, died. The House bill, introduced by Rep. Louise Slaughter, died after being referred to Committee.\n\nIn March 2012, the United States District Court for the Southern District of New York, ruling in an action brought by the Natural Resources Defense Council and others, ordered the FDA to revoke approvals for the use of antibiotics in livestock that violated FDA regulations. On April 11, 2012 the FDA announced a voluntary program to phase out unsupervised use of drugs as feed additives and convert approved over-the-counter uses for antibiotics to prescription use only, requiring veterinarian supervision of their use and a prescription. In December 2013, the FDA announced the commencement of these steps to phase out the use of antibiotics for the purposes of promoting livestock growth.\nSome grocery stores have policies about voluntarily not selling meat produced by using antibiotics to stimulate growth. In 2012 in the United States advocacy organization Consumers Union organized a petition asking the store Trader Joe's to discontinue the sale of meat produced with antibiotics.\n\nThe U.S. Animal Drug User Fee Act was passed by Congress in 2008 and requires that drug manufacturers report all sales of antibiotics into the food animal production industry.\n\nSome proposed legislation in the US has failed to be adopted. The Animal Drug and Animal Generic Drug User Fee Reauthorization Act of 2013 proposes other regulation.\n\nIn the United States the danger of emergence of antibiotic-resistant bacterial strains due to wide use of antibiotics to promote weight gain in livestock was determined by the United States Food and Drug Administration in 1977, but nothing effective was done to prevent the practice. In March, 2012 the United States District Court for the Southern District of New York, ruling in an action brought by the Natural Resources Defense Council and others, ordered the FDA to revoke approvals for the use of antibiotics in livestock which violated FDA regulations. On 11 April 2012 the FDA announced a program to phase out unsupervised use of drugs as feed additives and, on a voluntary basis, convert approved uses for antibiotics to therapeutic use only, requiring veterinarian supervision of their use and a prescription.\n\nIn response to consumer concerns about the use of antibiotics in poultry, in 2007, Perdue removed all human antibiotics from its feed and launched the Harvestland brand, under which it sold products that met the requirements for an \"antibiotic-free\" label. By 2014, Perdue had also phased out ionophores (antibiotics used in animals to lower production costs by promoting growth, and preventing disease) from its hatchery and began using the \"antibiotic free\" labels on its Harvestland, Simply Smart and Perfect Portions products. By 2015, 52% of the company's chickens were raised without the use of any type of antibiotics.\n\nIn 1970 the FDA started recommending that antibiotic use in livestock be limited but set no actual regulations governing this recommendation. Further, in 2004 the Government Accountability Office (GAO) heavily critiqued the FDA for not collecting enough information and data on antibiotic use in factory farms. From this the GAO concluded that the FDA does not have enough information to create effective policy changes regarding antibiotic use. In response to this the FDA insisted that more research was being conducted and voluntary efforts within the industry would solve the problem of antibiotic resistance.\n\nGrowing U.S. consumer concern about using antibiotics in animal feed has led to greater availability of \"antibiotic-free\" animal products. For example, chicken producer Perdue removed all human antibiotics from its feed and launched products labeled “antibiotic free” under the Harvestland brand in 2007. Consumer response was positive, and in 2014 Perdue also phased out ionophores from its hatchery and began using the “antibiotic free” labels on its Harvestland, Simply Smart, and Perfect Portions products.\n\nIn 2012, U.S. News & World Report described the Chinese government's regulation of antibiotics in livestock production as \"weak\".\n\nIn 2011 the Indian government proposed a \"National policy for containment of antimicrobial resistance\". Other policies set schedules for requiring that food producing animals not be given antibiotics for a certain amount of time before their food goes to market. A study released by Centre for Science and Environment (CSE) on 30 July 2014 found antibiotic residues in chicken. This study claims that Indians are developing resistance to antibiotics – and hence falling prey to a host of otherwise curable ailments. Some of this resistance might be due to large-scale unregulated use of antibiotics in the poultry industry. CSE finds that India has not set any limits for antibiotic residues in chicken and says that India will have to implement a comprehensive set of regulations including banning of antibiotic use as growth promoters in the poultry industry. Not doing this will put lives of people at risk.\n\nAntibiotic resistant bacteria have been found in Brazilian cattle.\n\nIn 1998 some researchers reported use in livestock production was a factor in the high prevalence of antibiotic resistant bacteria in Korea. In 2007 \"The Korea Times\" noted that Korea has relatively high usage of antibiotics in livestock production. In 2011 the Korean government banned the use of antibiotics as growth promoters in livestock.\n\nIn 1999 the New Zealand government issued a statement that they would not then ban the use of antibiotics in livestock production. In 2007 ABC Online reported on antibiotic use in chicken production in New Zealand.\n\nIncreasing concern due to the emergence of antibiotic resistant bacteria has led researchers to look for alternatives to using antibiotics in livestock.\n\nProbiotics, cultures of a single bacteria strain or mixture of different strains, are being studied in livestock as a production enhancer.\n\nPrebiotics are non-digestible carbohydrates. The carbohydrates are mainly made up of oligosaccharides which are short chains of monosaccharides. The two most commonly studied prebiotics are fructooligosaccharides (FOS) and mannanoligosaccharides (MOS). FOS has been studied for use in chicken feed. MOS works as a competitive binding site, as bacteria bind to it rather than the intestine and are carried out.\n\nBacteriophages are able to infect most bacteria and are easily found in most environments colonized by bacteria, and have been studied as well.\n\nIn another study it was found that using probiotics, competitive exclusion, enzymes, immunomodulators and organic acids prevents the spread of bacteria and can all be used in place of antibiotics. Another research team was able to use bacteriocins, antimicrobial peptides and bacteriophages in the control of bacterial infections. While further research is needed in this field, alternative methods have been identified in effectively controlling bacterial infections in animals. All of the alternative methods listed pose no known threat to human health and all can lead the elimination of antibiotics in factory farms. With further research it is highly likely that a cost effective and health effective alternative could and will be found.\n\n", "id": "40364158", "title": "Antibiotic use in livestock"}
{"url": "https://en.wikipedia.org/wiki?curid=24970625", "text": "RNA-based evolution\n\nRNA-based evolution is a theory that posits that RNA is not merely an intermediate between DNA and proteins, but rather a far more dynamic and independent role-player in determining phenotype. By regulating the transcription of DNA sequences, the stability of RNA, and the capability of messenger RNA to be translated, RNA processing events allow for a diverse array of proteins to be synthesized from a single gene. Since RNA processing is heritable, it is subject to natural selection and contributes to the evolution and diversity of most eukaryotic organisms.\n\nIn accordance with the central dogma of molecular biology, RNA passes information between the DNA of a genome and the proteins expressed within an organism. Therefore, from an evolutionary standpoint, a mutation within the DNA bases results in an alteration of the RNA transcripts, which in turn leads to a direct difference in phenotype.\nRNA is also believed to have been the genetic material of the first life on Earth. The role of RNA in the origin of life is best supported by the ease of forming RNA from basic chemical building blocks (such as amino acids, sugars, and hydroxyl acids) that were likely present 4 billion years ago. Molecules of RNA have also been shown to effectively self-replicate, catalyze basic reactions, and store heritable information. As life progressed and evolved over time only DNA, which is much more chemically stable than RNA, could support large genomes and eventually took over the role as the major carrier of genetic information.\n\nResearch within the past decade has shown that strands of RNA are not merely transcribed from regions of DNA and translated into proteins. Rather RNA has retained some of its former independence from DNA, and is subject to a network of processing events that alter the protein expression from that bounded by just the genomic DNA. Processing of RNA influences protein expression by managing the transcription of DNA sequences, the stability of RNA, and the translation of messenger RNA.\n\nSplicing is the process by which non-coding regions of RNA are removed. The number and combination of splicing events varies greatly based on differences in transcript sequence and environmental factors. Variation in phenotype caused by alternative splicing is best seen in the sex determination of \"D. melanogaster\". The \"Tra\" gene, determinant of sex, in male flies becomes truncated as splicing events fail to remove a stop codon that controls the length of the RNA molecule. In others the stop signal is retained within the final RNA molecule and a functional Tra protein is produced resulting in the female phenotype. Thus, alternative RNA splicing events allow differential phenotypes, regardless of the identity of the coding DNA sequence.\n\nPhenotype may also be determined by the number of RNA molecules, as more RNA transcripts lead to a greater expression of protein. Short tails of repetitive nucleic acids are often added to the ends of RNA molecules in order to prevent degradation, effectively increasing the number of RNA strands able to be translated into protein. During mammalian liver regeneration RNA molecules of growth factors increase in number due to the addition of signaling tails. With more transcripts present the growth factors are produced at a higher rate, aiding the rebuilding process of the organ.\n\nSilencing of RNA occurs when double stranded RNA molecules are processed by a series of enzymatic reactions, resulting in RNA fragments that degrade complementary RNA sequences. By degrading transcripts, a lower amount of protein products are translated and the phenotype is altered by yet another RNA processing event.\n\nMost RNA processing events work in concert with one another and produce networks of regulating processes that allow a greater variety of proteins to be expressed than those strictly directed by the genome. These RNA processing events can also be passed on from generation to generation via reverse transcription into the genome. Over time, RNA networks that produce the fittest phenotypes will be more likely to be maintained in a population, contributing to evolution. Studies have shown that RNA processing events have especially been critical with the fast phenotypic evolution of vertebrates—large jumps in phenotype explained by changes in RNA processing events. Human genome searches have also revealed RNA processing events that have provided significant “sequence space for more variability”. On the whole, RNA processing expands the possible phenotypes of a given genotype and contributes to the evolution and diversity of life.\n", "id": "24970625", "title": "RNA-based evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=9334591", "text": "Koinophilia\n\nKoinophilia is an evolutionary hypothesis proposing that during sexual selection, animals preferentially seek mates with a minimum of unusual or mutant features, including functionality, appearance and behavior. Koinophilia intends to explain the clustering of sexual organisms into species and other issues described by . The term derives from the Greek, \"koinos\", \"common\", \"that which is shared\", and \"philia\", \"fondness\".\n\nNatural selection causes beneficial inherited features to become more common at the expense of their disadvantageous counterparts. The koinophilia hypothesis proposes that a sexually-reproducing animal would therefore be expected to avoid individuals with rare or unusual features, and to prefer to mate with individuals displaying a predominance of common or average features. Mutants with strange, odd or peculiar features, would be avoided because most mutations that manifest themselves as changes in appearance, functionality or behavior, are disadvantageous. Because it is impossible to judge whether a new mutation is beneficial (or might be advantageous in the unforeseeable future) or not, koinophilic animals avoid them all, at the cost of avoiding the very occasional potentially beneficial mutation. Thus, koinophilia, although not infallible in its ability to distinguish fit from unfit mates, is a good strategy when choosing a mate. A koinophilic choice ensures that offspring are likely to inherit a suite of features and attributes that have served all the members of the species well in the past.\n\nKoinophilia differs from the \"like prefers like\" mating pattern of assortative mating. If like preferred like, leucistic animals (such as white peacocks) would be sexually attracted to one another, and a leucistic subspecies would come into being. Koinophilia predicts that this is unlikely because leucistic animals are attracted to the average in the same way as are all the other members of its species. Since non-leucistic animals are not attracted by leucism, few leucistic individuals find mates, and leucistic lineages will rarely form.\n\nKoinophilia provides simple explanations for the almost universal canalization of sexual creatures into species, the rarity of transitional forms between species (between both extant and fossil species), evolutionary stasis, punctuated equilibria, and the evolution of cooperation. Koinophilia might also contribute to the \"maintenance\" of sexual reproduction, preventing its reversion to the much simpler and inherently more advantageous asexual form of reproduction.\n\nThe koinophilia hypothesis is supported by the findings of Judith Langlois and her co-workers. They found that the average of two human faces was more attractive than either of the faces from which that average was derived. The more faces (of the same gender and age) that were used in the averaging process the more attractive and appealing the average face became. This work into averageness supports koinophilia as an explanation of what constitutes a beautiful face.\n\nBiologists from Darwin onwards have puzzled over how whose adult members look extraordinarily alike, and distinctively different from the members of other species. Lions and leopards are, for instance, both large carnivores that inhabit the same general environment, and hunt much the same prey, but look quite different. The question is why intermediates do not exist.\n\nThis is the \"horizontal\" dimension of a two-dimensional problem, referring to the almost complete absence of transitional or intermediate forms between present-day species (e.g. between lions, leopards, and cheetahs).\n\nThe \"vertical\" dimension concerns the fossil record. Fossil species are frequently remarkably stable over extremely long periods of geological time, despite continental drift, major climate changes, and mass extinctions. When a change in form occurs, it tends to be abrupt in geological terms, again producing phenotypic gaps (i.e. an absence of intermediate forms), but now between successive species, which then often co-exist for long periods of time. Thus the fossil record suggests that evolution occurs in bursts, interspersed by long periods of evolutionary stagnation in so-called punctuated equilibria. Why this is so has been an evolutionary enigma ever since .\n\nKoinophilia could explain both the horizontal and vertical manifestations of speciation, and why it, as a general rule, involves the entire external appearance of the animals concerned. Since koinophilia affects the \"entire\" external appearance, the members of an interbreeding group are driven to look alike in every detail. Each interbreeding group will rapidly develop its own characteristic appearance. An individual from one group which wanders into another group will consequently be recognized as different, and will be discriminated against during the mating season. Reproductive isolation induced by koinophilia might thus be the first crucial step in the development of, ultimately, physiological, anatomical and behavioral barriers to hybridization, and thus, ultimately, full specieshood. Koinophilia will thereafter defend that species' appearance and behavior against invasion by unusual or unfamiliar forms (which might arise by immigration or mutation), and thus be a paradigm of punctuated equilibria (or the \"vertical\" aspect of the speciation problem).\n\nEvolution can be extremely rapid, as shown by the creation of domesticated animals and plants in a very short period of geological time, spanning only a few tens of thousands of years, by humans with little or no knowledge of genetics. Maize, \"Zea mays\", for instance, was created in Mexico in only a few thousand years, starting about 7 000 to 12 000 years ago. This raises the question of why the long term rate of evolution is far slower than is theoretically possible.\n\nEvolution is imposed on species or groups. It is not planned or striven for in some Lamarckist way. The mutations on which the process depends are random events, and, except for the \"silent mutations\" which do not affect the functionality or appearance of the carrier, are thus usually disadvantageous, and their chance of proving to be useful in the future is vanishingly small. Therefore, while a species or group might benefit by being able to adapt to a new environment through the accumulation of a wide range of genetic variation, this is to the detriment of the \"individuals\" who have to carry these mutations until a small, unpredictable minority of them ultimately contributes to such an adaptation. Thus, the \"capability\" to evolve is a group adaptation, which has been discredited by, among others, George C. Williams, John Maynard Smith and Richard Dawkins. because it is not to the benefit of the individual.\n\nConsequently, sexual \"individuals\" would be expected to avoid transmitting mutations to their progeny by avoiding mates with strange or unusual characteristics. Mutations that therefore affect the external appearance and habits of their carriers will seldom be passed on to the next and subsequent generations. They will therefore seldom be tested by natural selection. Evolutionary change in a large population with a wide choice of mates, will, therefore, come to a virtual standstill. The only mutations that can accumulate in a population are ones that have no noticeable effect on the outward appearance and functionality of their bearers (they are thus termed \"silent\" or \"neutral mutations\").\n\nThe restraint koinophilia exerts on phenotypic change suggests that evolution can only occur if mutant mates cannot be avoided as a result of a severe scarcity of potential mates. This is most likely to occur in small restricted communities, such as on small islands, in remote valleys, lakes, river systems, caves, or during periods of glaciation, or following mass extinctions, when sudden bursts of evolution can be expected. Under these circumstances, not only is the choice of mates severely restricted, but population bottlenecks, founder effects, genetic drift and inbreeding cause rapid, random changes in the isolated population's genetic composition. Furthermore, hybridization with a related species trapped in the same isolate might introduce additional genetic changes. If an isolated population such as this survives its genetic upheavals, and subsequently expands into an unoccupied niche, or into a niche in which it has an advantage over its competitors, a new species, or subspecies, will have come in being. In geological terms this will be an abrupt event. A resumption of avoiding mutant mates will, thereafter, result, once again, in evolutionary stagnation.\n\nThus the fossil record of an evolutionary progression typically consists of species that suddenly appear, and ultimately disappear hundreds of thousands or millions of years later, without any change in external appearance. Graphically, these fossil species are represented by horizontal lines, whose lengths depict how long each of them existed. The horizontality of the lines illustrates the unchanging appearance of each of the fossil species depicted on the graph. During each species' existence new species appear at random intervals, each also lasting many hundreds of thousands of years before disappearing without a change in appearance. The degree of relatedness and the lines of descent of these concurrent species is generally impossible to determine. This is illustrated in the following diagram depicting the evolution of modern humans from the time that the hominins separated from the line that led to the evolution of our closest living primate relatives, the chimpanzees.\n\nThis proposal, that population bottlenecks are possibly the primary generators of the variation that fuels evolution, predicts that evolution will usually occur in intermittent, relatively large scale morphological steps, interspersed with prolonged periods of evolutionary stagnation, instead of in a continuous series of finely graded changes. However, it makes a further prediction. Darwin emphasized that the shared biologically useless oddities and incongruities that characterize a species are signs of an evolutionary history – something that would not be expected if a bird’s wing, for instance, was engineered \"de novo\", as argued by his detractors. The present model predicts that, in addition to vestiges which reflect an organism’s evolutionary heritage, all the members of a given species will also bear the stamp of their isolationary past – arbitrary, random features, accumulated through founder effects, genetic drift and the other genetic consequences of sexual reproduction in small, isolated communities. Thus all lions, African and Asian, have a highly characteristic black tuft of fur at the end of their tails, which is difficult to explain in terms of an adaptation, or as a vestige from an early feline, or more ancient ancestor. The unique, often color- and pattern-rich plumage of each of today’s wide variety of bird species presents a similar evolutionary enigma. This richly varied array of phenotypes is more easily explained as the products of isolates, subsequently defended by koinophilia, than as assemblies of very diverse evolutionary relics, or as sets of uniquely evolved adaptations.\n\nCo-operation is any group behavior that benefits the individuals more than if they were to act as independent agents. \nHowever selfish individuals can exploit the co-operativeness of others by not taking part in the group activity, but still enjoying its benefits. For instance, a selfish individual which does not join the hunting pack and share in its risks, but nevertheless shares in the spoils, has a fitness advantage over the other members of the pack. Thus, although a group of co-operative individuals is fitter than an equivalent group of selfish individuals, selfish individuals interspersed among a community of co-operators are always fitter than their hosts. They will raise, on average, more offspring than their hosts, and will ultimately replace them.\n\nIf, however, the selfish individuals are ostracized, and rejected as mates, because of their deviant and unusual behavior, then their evolutionary advantage becomes an evolutionary liability. Co-operation then becomes evolutionarily stable.\n\nThe best-documented creations of new species in the laboratory were performed in the late 1980s. William Rice and G.W. Salt bred fruit flies, \"Drosophila melanogaster\", using a maze with three different choices of habitat, such as light/dark and wet/dry. Each generation was placed into the maze, and the groups of flies that came out of two of the eight exits were set apart to breed with each other in their respective groups. After thirty-five generations, the two groups and their offspring were isolated reproductively because of their strong habitat preferences: they mated only within the areas they preferred, and so did not mate with flies that preferred the other areas. The history of such attempts is described in Rice and Hostert (1993).\n\nDiane Dodd used a laboratory experiment to show how reproductive isolation can evolve in \"Drosophila pseudoobscura\" fruit flies after several generations by placing them in different media, starch- or maltose-based media.\n\nDodd's experiment has been easy for many others to replicate, including with other kinds of fruit flies and foods.\n\nWilliam B. Miller, in an extensive recent (2013) review of koinophilia theory, notes that while it provides precise explanations for the grouping of sexual animals into species, their unchanging persistence in the fossil record over long periods of time, and the phenotypic gaps between species, both fossil and extant, it represents a major departure from the widely accepted view that beneficial mutations spread, ultimately, to the whole, or some portion of the population (causing it to evolve gene by gene). Darwin recognized that this process had no inherent, or inevitable propensity to produce species. Instead populations would be in a . They would, at any given moment, consist of individuals with varying numbers of beneficial characteristics that may or may not have reached them from their various points of origin in the population, and neutral features will have a scattering determined by random mechanisms such as genetic drift.\n\nHe also notes that koinophilia provides no explanation as to how the physiological, anatomical and genetic causes of reproductive isolation come about. It is only the behavioral reproductive isolation that is addressed by koinophilia. It is furthermore difficult to see how koinophilia might apply to plants, and certain marine creatures that discharge their gametes into the environment to meet up and fuse, it seems, entirely randomly (within conspecific confines). However, when pollen from several compatible donors is used to pollinate stigmata, the donors typically do not sire equal numbers of seeds. Marshall and Diggle state that the existence of some kind of non-random seed paternity is, in fact, not in question in flowering plants. It is how this occurs that remains unknown. Pollen choice is one of the possibilities, taking into account that 50% of the pollen grain’s haploid genome is expressed during its tube’s growth towards the ovule.\n\nThe apparent preference of the females of certain, particularly bird, species for exaggerated male ornaments, such as the peacock’s tail, is not easily reconciled with the concept of koinophilia.\n", "id": "9334591", "title": "Koinophilia"}
{"url": "https://en.wikipedia.org/wiki?curid=40143821", "text": "Mesozoic–Cenozoic radiation\n\nThe Mesozoic–Cenozoic Radiation is the third major extended increase in biodiversity in the Phanerozoic, after the Cambrian Explosion and the Great Ordovician Biodiversification Event. The Mesozoic–Cenozoic Radiation began in the mid-Mesozoic and extends through the Cenozoic, with no indication of having yet ended. It differs from the earlier radiations in that the diversity appears largely in the variety of species and other taxa below the level of Order (biology). \"The spectacular radiation of the angiosperms, mammals and certain reptile groups (such as the snakes) on land is matched by that of the planktonic foraminifera, neogastropods, heteroconch bivalves, cheilostome bryozoans, decapod crustaceans and teleost fish in shallow seas.\" The marine diversification is largely that represented by the Modern Evolutionary Fauna, and there are also the terrestrial diversifications of the birds and among the insects.\n", "id": "40143821", "title": "Mesozoic–Cenozoic radiation"}
{"url": "https://en.wikipedia.org/wiki?curid=5848903", "text": "Evolutionary landscape\n\nAn evolutionary landscape is a metaphor, a construct used to think about and visualize the processes of evolution (e.g. natural selection and genetic drift) acting on a biological entity (e.g., a gene, protein, population, species). This entity can be viewed as searching or moving through a search space. For example, the search space of a gene would be all possible nucleotide sequences. The search space is only part of an evolutionary landscape. The final component is the \"y-axis,\" which is usually fitness. Each value along the search space can result in a high or low fitness for the entity. If small movements through search space cause changes in fitness that are relatively small, then the landscape is considered smooth. Smooth landscapes happen when most fixed mutations have little to no effect on fitness, which is what one would expect with the neutral theory of molecular evolution. In contrast, if small movements result in large changes in fitness, then the landscape is said to be rugged. In either case, movement tends to be toward areas of higher fitness, though usually not the global optima.\n\nWhat exactly constitutes an \"evolutionary landscape\" is confused in the literature. The term 'evolutionary landscape' is often used interchangeably with 'adaptive landscape' and 'fitness landscape', although some authors distinguish between these terms. As discussed below, different authors have different definitions of adaptive and fitness landscapes. Additionally, there is a large disagreement whether evolutionary landscape should be used as a visual metaphor disconnected from the underlying math, a tool for evaluating models of evolution, or a model in and of itself used to generate hypotheses and predictions. \n\nAccording to McCoy (1979), the first evolutionary landscape was presented by Armand Janet of Toulon, France in 1895. In Janet's evolutionary landscape, a species is represented as a point or an area on a polydimensional surface of phenotypes, which is reduced to two dimensions for simplicity. The size of the population is proportional to the amount of variation within the population. Natural selection (the influence of the exterior features) is represented by a vector. Unlike the evolutionary landscapes of those who would follow, in Janet's concept, natural selection pulls species toward the minima instead of the maxima. This is because the y-axis doesn't represent fitness but stability. One important aspect of Janet's evolutionary landscape (versus Wright's) is that the landscape changes as the environment changes.\n\nCredit for the first evolutionary landscape typically goes to Sewall Wright, and his idea has arguably had a much larger audience and greater influence on the field of evolutionary biology. In his 1932 paper, Wright presents the concept of an evolutionary landscape composed of a polydimensional array of gene (allele) or genotype frequencies and an axis of fitness, which served as a visual metaphor to explain his shifting balance theory. Similarly to Janet, Wright felt the landscape could be reduced to two dimensions for simplicity. (This is one of the greatest criticisms, which are discussed below.) Populations are represented by areas, with the size of the area corresponding to the amount of genetic diversity within the population. Natural selection drives populations toward maxima, while drift represents wandering and could potentially cause a peak shift. Movement across the landscape represented changes in gene frequencies. This landscape was represented as a series of contour lines, much like a topological map; while selection kept or moved a biological entity to a peak, genetic drift allowed different peaks to be explored.\n\nIn 1944, Simpson expanded Wright's landscape to include phenotypes. in Simpson's model, the landscape was a means of visualizing the \"relationship between selection, structure, and adaptation.\" Unlike Wright, Simpson used the landscape to represent both natural selection and genetic drift. Uphill movements are due to positive selection, and downhill movements are due to negative selection. The size and shape of a peak indicated the relative specificity of selection; i.e. a sharp and high peak indicates highly specific selection. Another difference between Simpson's and Wright's landscapes is the level at which evolution is acting. For Wright, a population geneticist, only populations of a species were shown. In Simpson's figures, the circles drawn represent all of Equidae. The most important difference is that in Simpson's model, the landscape could vary through time, while in Wright's model, the landscape was static. It is interesting to note that Wright reviewed Simpson's work (\"Tempo and Mode in Evolution\") and did not object to Simpson's use of evolutionary landscapes. In later writings, Simpson referred to peaks as adaptive zones. In a series of papers Russell Lande developed a mathematical model for Simpson's phenotypic landscape. Lande reconciled Wright's population level view with Simpson's use of higher taxonomic levels. Lande considers fitness peaks to be determined by the environment and thus represent ecological niches or adaptive zones for a population. Clusters of peaks inhabited by phenotypically similar populations can be viewed as higher taxonomic levels.\n\nThe concept of evolutionary landscapes changed once more as we entered the era of molecular evolution. It is claimed Maynard Smith (1970) is the first to visualize protein evolution as a network of proteins one mutational step away from others. However, for this to be true, there must be pathways between functional proteins. Acknowledging the work of Kimura, King, and Jukes (the neutral theory of molecular evolution), Maynard Smith realized the proteins along such pathways could have equal functionality or be neutral. In other words, not all moves in evolution are \"uphill.\" In 1984, Gillespie adapted the concept of evolutionary landscapes to nucleotide sequences and so visualized the \"mutational landscape\" whereby all nucleotide sequences are one mutational step away from another, which is remarkably similar and yet fundamentally different from Wright's original concept. This conceptual shift, along with the development of vast computational power, has allowed evolutionary landscapes to move from being a simple visual metaphor to a working model of evolution. As one might expect, this has drawn heavy criticism and generated much research.\n\nOne of the first criticisms (or at least difficulty) with evolutionary landscapes is their dimensionality. Wright recognized that true landscapes can have thousands of dimensions, but he also felt reducing those dimensions to two was acceptable since his point in doing so was to simply convey a complex idea. As a visual metaphor, this might be a valid reduction; however, the work of Gavrilets has shown that taking the high dimensionality of evolutionary landscapes into consideration may matter. In a high-dimensional framework, the peaks and valleys disappear and are replaced with hypervolume areas of high fitness and low fitness, which can be visualized as curved surfaces and holes in a 3-D landscape. While this does not affect visualization of the landscape per se (i.e. holes are equivalent to valleys), it does affect the underlying mathematical model and the predicted outcomes.\n\nThe work of Gavrilets, along with other issues, has prompted Kaplan (2008) to propose abandoning the metaphor of evolutionary landscapes (which he calls adaptive or fitness landscapes). Kaplan (2008) has 6 main criticisms of the metaphor: (1) it has no explanatory power, (2) its lacks a relevant mathematical model, (3) it has no heuristic role, (4) it is imprecise, (5) it confuses more than it explains, (6) and there is no longer a reason to keep thinking in 2D or 3D when we have the computational power to consider higher dimensionality. Others feel Kaplan's criticisms are not warranted because he (and others) want evolutionary landscapes to meet the standards of a mathematical model; however, the landscape metaphor is just that, a metaphor. It has heuristic value as a metaphorical tool allowing one to visualize and evaluate the common core of assumptions in an evolutionary model. While Kaplan (2008) wishes to discard the idea of landscapes all together, Massimo Pigliucci is less drastic. He acknowledges four categories of landscapes: fitness landscapes, adaptive landscapes, fitness surfaces, and morphospaces. Fitness landscapes are those similar to what Wright (1932) proposed (called an adaptive and fitness landscapes below). Adaptive landscapes are the phenotypic landscapes proposed by Simpson (1944), and fitness surfaces are the phenotypic landscapes with Lande's mathematical models applied to them. Morphospaces, pioneered by Raup (1966), are phenotypic landscapes developed \"a priori\" using mathematical models onto which observed measurements are mapped. They lack a fitness axis, and are used to show the occupied areas within the potential phenotypic space. Pigliucci suggests we abandon Wrightian fitness landscapes. Adaptive landscapes and fitness surfaces can be used with caution, i.e. with the understanding they are not phenotypic versions of Wright's original concept and that they are fraught with potentially misleading assumptions. Finally, Pigliucci calls for further research into morphospaces due to their heuristic value but also their ability to generate understandable and testable hypotheses.\n\nAdaptive landscapes represent populations (of biological entities) as a single point, and the axes correspond to frequencies of alleles or genotypes and the mean population fitness.\n\nFitness landscapes represent populations (of biological entities) as clusters of points with each point representing a unique genotype. The axes correspond to the loci of those genotypes and the resulting mean population fitness.\n\nPhenotypic landscapes represent populations of species as clusters of points with each point representing a phenotype. The axes correspond to frequencies of phenotypes and the mean population fitness. See the visualizations below for examples of phenotypic landscapes.\n\nSelection-weighted attraction graphing (SWAG) uses force-directed network graphing to visualize fitness landscapes. In this visualization, genotypes are represented by nodes which are attracted to each other in proportion to the relative change in fitness between them (nodes will tend to be closer if there is stronger selection strength between them on average). Additionally, fitness values may be assigned to the z-axis to create an empirical three-dimensional model of the landscape and depict fitness peaks and valleys. Clusters in this depiction may represent local fitness peaks. \nPhenotypic plasticity landscapes depart from the other landscapes in that it does not use the mean population fitness. Instead, that axis represents characters (phenotypic traits) and other axes represent the underlying factors affecting the character.\n\nEpigenetic landscapes are \"used to describe modal developmental tendency and major deviations\" with a \"space of abstract variables.\" \n\nMorphospaces also lack a dimension of fitness. Instead, their axes are mathematical models of phenotypic traits developed \"a priori\" to observational measurements. Observational measurements are then mapped onto the resulting surface to indicate areas of possible phenotypic space occupied by the species under consideration. .\n\nThe increase of computational power and ease of sequencing has allowed the concept of evolutionary landscapes to be taken from a purely conceptual metaphor to something that can be modeled and explored. This is especially true of the field of genomics. One good example is the research article \"The Evolutionary Landscape of Cytosolic Microbial Sensors in Humans.\" In their study, Vasseur et al. were interested in the evolution of the innate immune system; specifically, they wished to map the genetic diversity—the occupied evolutionary landscape—and patterns of selection and diversification—the movements made and being made along that landscape—of the nod-like receptor (NLRs) family of the pattern-recognition receptors (PRRs) that drive the innate immune response. These genes are responsible for detecting patterns/chemicals (e.g. chitin, oxidative stress) associated with invading microbes, tissue damage, and stress. To achieve this, they sequenced 21 genes from 185 humans and used several statistical methods to examine patterns of selection and diversification. The NLR family can be divided into two sub-families—the NALP subfamily and the NOD/IPAF sub-family. The authors found that the NALP family was under strong purifying selection and exhibited low genetic and functional diversity. They hypothesize this is because these genes have vital, non-redundant roles. Evidence in favor of this hypothesis comes from the independent discovery of rare alleles with mutations in two of the genes leading to a severe inflammatory disease and pregnancy complications. The NOD/IPAF family seems to have evolved under relaxed selection and exhibits a fair amount of genetic and functional diversity. The authors also found evidence of positive selection. The gene with the strongest positive selection was NLRP1, which has two haplotypes undergoing selective sweeps. The first haplotype is seven amino acids in strong linkage disequilibrium. This haplotype is global and seems to be moving toward fixation, which started in Asia and is still occurring in Europe and Africa. The second haplotype is restricted to Europe and is not in linkage disequilibrium with the global haplotype. This European haplotype carries with it a mutation associated with autoimmune diseases. The authors hypothesize another mutation in the haplotype is what selection is acting on and this deleterious mutation is simply hitchhiking along with it.\n\nThe second example comes from a paper titled \"Synonymous Genes Explore Different Evolutionary Landscapes.\" (This article is publicly available; see the citation below.) The authors of this paper are broadly interested in the ability of a protein to evolve. They specifically wanted to know how synonymous substitutions affected the evolutionary landscape of a protein. To do this, they used a program called the Evolutionary Landscape Printer to design a synonymous version of the antibiotic resistance gene \"aac(6')-IB\". A synonymous protein has the same amino acid sequence but different nucleotide sequences. Thus, a synonymous protein has the same function and fitness value but a different surrounding evolutionary landscape. Basically, this is a way to jump peaks without actually moving. The landscape of the original protein and the synonymous copy were explored computationally with Monte Carlo simulations and error-prone PCR. The PCR products were then inserted into competent E. coli cells and screened for novel antibiotic resistance. They found that the two proteins gave rise to very different novel phenotypes that, theoretically, are unreachable by the other. From their results, the authors concluded that synonymous codons allow for a wider exploration of the local evolutionary landscape, and that the method they used increases the odds of finding an advantageous mutation, which is useful for predicting how a population might change and for designing better organisms for industry.\n\nExamples of Visualized Evolutionary Landscapes\n\nFurther Reading\n", "id": "5848903", "title": "Evolutionary landscape"}
{"url": "https://en.wikipedia.org/wiki?curid=9813", "text": "Extinction event\n\nAn extinction event (also known as a mass extinction or biotic crisis) is a widespread and rapid decrease in the biodiversity on Earth. Such an event is identified by a sharp change in the diversity and abundance of multicellular organisms. It occurs when the rate of extinction increases with respect to the rate of speciation. Because most diversity and biomass on Earth is microbial, and thus difficult to measure, recorded extinction events affect the easily observed, biologically complex component of the biosphere rather than the total diversity and abundance of life.\n\nExtinction occurs at an uneven rate. Based on the fossil record, the background rate of extinctions on Earth is about two to five taxonomic families of marine animals every million years. Marine fossils are mostly used to measure extinction rates because of their superior fossil record and stratigraphic range compared to land animals.\n\nThe Great Oxygenation Event was probably the first major extinction event. Since the Cambrian explosion five further major mass extinctions have significantly exceeded the background extinction rate. The most recent and arguably best-known, the Cretaceous–Paleogene extinction event, which occurred approximately million years ago (Ma), was a large-scale mass extinction of animal and plant species in a geologically short period of time. In addition to the five major mass extinctions, there are numerous minor ones as well, and the ongoing mass extinction caused by human activity is sometimes called the sixth extinction. Mass extinctions seem to be a mainly Phanerozoic phenomenon, with extinction rates low before large complex organisms arose.\n\nEstimates of the number of major mass extinctions in the last 540 million years range from as few as five to more than twenty. These differences stem from the threshold chosen for describing an extinction event as \"major\", and the data chosen to measure past diversity.\n\nIn a landmark paper published in 1982, Jack Sepkoski and David M. Raup identified five mass extinctions. They were originally identified as outliers to a general trend of decreasing extinction rates during the Phanerozoic, but as more stringent statistical tests have been applied to the accumulating data, it has been established that multicellular animal life has experienced five major and many minor mass extinctions. The \"Big Five\" cannot be so clearly defined, but rather appear to represent the largest (or some of the largest) of a relatively smooth continuum of extinction events.\n\n\nDespite the popularization of these five events, there is no definite line separating them from other extinction events; using different methods of calculating an extinction's impact can lead to other events featuring in the top five.\n\nThe older the fossil record gets, the more difficult it is to read. This is because:\n\nIt has been suggested that the apparent variations in marine biodiversity may actually be an artifact, with abundance estimates directly related to quantity of rock available for sampling from different time periods. However, statistical analysis shows that this can only account for 50% of the observed pattern, and other evidence (such as fungal spikes) provides reassurance that most widely accepted extinction events are real. A quantification of the rock exposure of Western Europe indicates that many of the minor events for which a biological explanation has been sought are most readily explained by sampling bias.\n\nResearch completed after the seminal 1982 paper has concluded that a sixth mass extinction event is ongoing:\n\nMore recent research has indicated that the End-Capitanian extinction event likely constitutes a separate extinction event from the Permian–Triassic extinction event; if so, it would be larger than many of the \"Big Five\" extinction events.\n\nThis is a list of extinction events:\n\nMass extinctions have sometimes accelerated the evolution of life on Earth. When dominance of particular ecological niches passes from one group of organisms to another, it is rarely because the new dominant group is \"superior\" to the old and usually because an extinction event eliminates the old dominant group and makes way for the new one.\n\nFor example, mammaliformes (\"almost mammals\") and then mammals existed throughout the reign of the dinosaurs, but could not compete for the large terrestrial vertebrate niches which dinosaurs monopolized. The end-Cretaceous mass extinction removed the non-avian dinosaurs and made it possible for mammals to expand into the large terrestrial vertebrate niches. Ironically, the dinosaurs themselves had been beneficiaries of a previous mass extinction, the end-Triassic, which eliminated most of their chief rivals, the crurotarsans.\n\nAnother point of view put forward in the Escalation hypothesis predicts that species in ecological niches with more organism-to-organism conflict will be less likely to survive extinctions. This is because the very traits that keep a species numerous and viable under fairly static conditions become a burden once population levels fall among competing organisms during the dynamics of an extinction event.\n\nFurthermore, many groups which survive mass extinctions do not recover in numbers or diversity, and many of these go into long-term decline, and these are often referred to as \"Dead Clades Walking\".\n\nDarwin was firmly of the opinion that biotic interactions, such as competition for food and space—the ‘struggle for existence’—were of considerably greater importance in promoting evolution and extinction than changes in the physical environment. He expressed this in \"The Origin of Species\": \"Species are produced and exterminated by slowly acting causes…and the most import of all causes of organic change is one which is almost independent of altered…physical conditions, namely the mutual relation of organism to organism-the improvement of one organism entailing the improvement or extermination of others\".\n\nIt has been suggested variously that extinction events occurred periodically, every 26 to 30 million years, or that diversity fluctuates episodically every ~62 million years.\nVarious ideas attempt to explain the supposed pattern, including the presence of a hypothetical companion star to the sun, \noscillations in the galactic plane, or passage through the Milky Way's spiral arms. However, other authors have concluded the data on marine mass extinctions do not fit with the idea that mass extinctions are periodic, or that ecosystems gradually build up to a point at which a mass extinction is inevitable. Many of the proposed correlations have been argued to be spurious.\nOthers have argued that there is strong evidence supporting periodicity in a variety of records,\nand additional evidence in the form of coincident periodic variation in nonbiological geochemical variables.\n\nMass extinctions are thought to result when a long-term stress is compounded by a short term shock. Over the course of the Phanerozoic, individual taxa appear to be less likely to become extinct at any time, which may reflect more robust food webs as well as less extinction-prone species and other factors such as continental distribution.\nHowever, even after accounting for sampling bias, there does appear to be a gradual decrease in extinction and origination rates during the Phanerozoic. This may represent the fact that groups with higher turnover rates are more likely to become extinct by chance; or it may be an artefact of taxonomy: families tend to become more speciose, therefore less prone to extinction, over time; and larger taxonomic groups (by definition) appear earlier in geological time.\n\nIt has also been suggested that the oceans have gradually become more hospitable to life over the last 500 million years, and thus less vulnerable to mass extinctions, but susceptibility to extinction at a taxonomic level does not appear to make mass extinctions more or less probable.\n\nThere is still debate about the causes of all mass extinctions. In general, large extinctions may result when a biosphere under long-term stress undergoes a short-term shock. An underlying mechanism appears to be present in the correlation of extinction and origination rates to diversity. High diversity leads to a persistent increase in extinction rate; low diversity to a persistent increase in origination rate. These presumably ecologically controlled relationships likely amplify smaller perturbations (asteroid impacts, etc.) to produce the global effects observed.\n\nA good theory for a particular mass extinction should: (i) explain all of the losses, not just focus on a few groups (such as dinosaurs); (ii) explain why particular groups of organisms died out and why others survived; (iii) provide mechanisms which are strong enough to cause a mass extinction but not a total extinction; (iv) be based on events or processes that can be shown to have happened, not just inferred from the extinction.\n\nIt may be necessary to consider combinations of causes. For example, the marine aspect of the end-Cretaceous extinction appears to have been caused by several processes which partially overlapped in time and may have had different levels of significance in different parts of the world.\n\nArens and West (2006) proposed a \"press / pulse\" model in which mass extinctions generally require two types of cause: long-term pressure on the eco-system (\"press\") and a sudden catastrophe (\"pulse\") towards the end of the period of pressure.\nTheir statistical analysis of marine extinction rates throughout the Phanerozoic suggested that neither long-term pressure alone nor a catastrophe alone was sufficient to cause a significant increase in the extinction rate.\n\nMacleod (2001) summarized the relationship between mass extinctions and events which are most often cited as causes of mass extinctions, using data from Courtillot \"et al.\" (1996), Hallam (1992) and Grieve \"et al.\" (1996):\n\nThe most commonly suggested causes of mass extinctions are listed below.\n\nThe formation of large igneous provinces by flood basalt events could have:\nFlood basalt events occur as pulses of activity punctuated by dormant periods. As a result, they are likely to cause the climate to oscillate between cooling and warming, but with an overall trend towards warming as the carbon dioxide they emit can stay in the atmosphere for hundreds of years.\n\nIt is speculated that massive volcanism caused or contributed to the End-Permian, End-Triassic and End-Cretaceous extinctions. The correlation between gigantic volcanic events expressed in the large igneous provinces and mass extinctions was shown for the last 260 Myr. Recently such possible correlation was extended for the whole Phanerozoic Eon.\n\nThese are often clearly marked by worldwide sequences of contemporaneous sediments which show all or part of a transition from sea-bed to tidal zone to beach to dry land – and where there is no evidence that the rocks in the relevant areas were raised by geological processes such as orogeny. Sea-level falls could reduce the continental shelf area (the most productive part of the oceans) sufficiently to cause a marine mass extinction, and could disrupt weather patterns enough to cause extinctions on land. But sea-level falls are very probably the result of other events, such as sustained global cooling or the sinking of the mid-ocean ridges.\n\nSea-level falls are associated with most of the mass extinctions, including all of the \"Big Five\"—End-Ordovician, Late Devonian, End-Permian, End-Triassic, and End-Cretaceous.\n\nA study, published in the journal Nature (online June 15, 2008) established a relationship between the speed of mass extinction events and changes in sea level and sediment. The study suggests changes in ocean environments related to sea level exert a driving influence on rates of extinction, and generally determine the composition of life in the oceans.\n\nThe impact of a sufficiently large asteroid or comet could have caused food chains to collapse both on land and at sea by producing dust and particulate aerosols and thus inhibiting photosynthesis. Impacts on sulfur-rich rocks could have emitted sulfur oxides precipitating as poisonous acid rain, contributing further to the collapse of food chains. Such impacts could also have caused megatsunamis and/or global forest fires.\n\nMost paleontologists now agree that an asteroid did hit the Earth about 66 Ma ago, but there is an ongoing dispute whether the impact was the sole cause of the Cretaceous–Paleogene extinction event.\n\nSustained global cooling could kill many polar and temperate species and force others to migrate towards the equator; reduce the area available for tropical species; often make the Earth's climate more arid on average, mainly by locking up more of the planet's water in ice and snow. The glaciation cycles of the current ice age are believed to have had only a very mild impact on biodiversity, so the mere existence of a significant cooling is not sufficient on its own to explain a mass extinction.\n\nIt has been suggested that global cooling caused or contributed to the End-Ordovician, Permian–Triassic, Late Devonian extinctions, and possibly others. Sustained global cooling is distinguished from the temporary climatic effects of flood basalt events or impacts.\n\nThis would have the opposite effects: expand the area available for tropical species; kill temperate species or force them to migrate towards the poles; possibly cause severe extinctions of polar species; often make the Earth's climate wetter on average, mainly by melting ice and snow and thus increasing the volume of the water cycle. It might also cause anoxic events in the oceans (see below).\n\nGlobal warming as a cause of mass extinction is supported by several recent studies.\n\nThe most dramatic example of sustained warming is the Paleocene–Eocene Thermal Maximum, which was associated with one of the smaller mass extinctions. It has also been suggested to have caused the Triassic–Jurassic extinction event, during which 20% of all marine families became extinct. Furthermore, the Permian–Triassic extinction event has been suggested to have been caused by warming.\n\nClathrates are composites in which a lattice of one substance forms a cage around another. Methane clathrates (in which water molecules are the cage) form on continental shelves. These clathrates are likely to break up rapidly and release the methane if the temperature rises quickly or the pressure on them drops quickly—for example in response to sudden global warming or a sudden drop in sea level or even earthquakes. Methane is a much more powerful greenhouse gas than carbon dioxide, so a methane eruption (\"clathrate gun\") could cause rapid global warming or make it much more severe if the eruption was itself caused by global warming.\n\nThe most likely signature of such a methane eruption would be a sudden decrease in the ratio of carbon-13 to carbon-12 in sediments, since methane clathrates are low in carbon-13; but the change would have to be very large, as other events can also reduce the percentage of carbon-13.\n\nIt has been suggested that \"clathrate gun\" methane eruptions were involved in the end-Permian extinction (\"the Great Dying\") and in the Paleocene–Eocene Thermal Maximum, which was associated with one of the smaller mass extinctions.\n\nAnoxic events are situations in which the middle and even the upper layers of the ocean become deficient or totally lacking in oxygen. Their causes are complex and controversial, but all known instances are associated with severe and sustained global warming, mostly caused by sustained massive volcanism.\n\nIt has been suggested that anoxic events caused or contributed to the Ordovician–Silurian, late Devonian, Permian–Triassic and Triassic–Jurassic extinctions, as well as a number of lesser extinctions (such as the Ireviken, Mulde, Lau, Toarcian and Cenomanian–Turonian events). On the other hand, there are widespread black shale beds from the mid-Cretaceous which indicate anoxic events but are not associated with mass extinctions.\n\nThe bio-availability of essential trace elements (in particular selenium) to potentially lethal lows has been shown to coincide with, and likely have contributed to, at least three mass extinction events in the oceans, i.e. at the end of the Ordovician, during the Middle and Late Devonian, and at the end of the Triassic. During periods of low oxygen concentrations very soluble selenate (Se) is converted into much less soluble selenide (Se), elemental Se and organo-selenium complexes. Bio-availability of selenium during these extinction events dropped to about 1% of the current oceanic concentration, a level that has been proven lethal to many extant organisms.\n\nKump, Pavlov and Arthur (2005) have proposed that during the Permian–Triassic extinction event the warming also upset the oceanic balance between photosynthesising plankton and deep-water sulfate-reducing bacteria, causing massive emissions of hydrogen sulfide which poisoned life on both land and sea and severely weakened the ozone layer, exposing much of the life that still remained to fatal levels of UV radiation.\n\nOceanic overturn is a disruption of thermo-haline circulation which lets surface water (which is more saline than deep water because of evaporation) sink straight down, bringing anoxic deep water to the surface and therefore killing most of the oxygen-breathing organisms which inhabit the surface and middle depths. It may occur either at the beginning or the end of a glaciation, although an overturn at the start of a glaciation is more dangerous because the preceding warm period will have created a larger volume of anoxic water.\n\nUnlike other oceanic catastrophes such as regressions (sea-level falls) and anoxic events, overturns do not leave easily identified \"signatures\" in rocks and are theoretical consequences of researchers' conclusions about other climatic and marine events.\n\nIt has been suggested that oceanic overturn caused or contributed to the late Devonian and Permian–Triassic extinctions.\n\nA nearby gamma-ray burst (less than 6000 light-years away) would be powerful enough to destroy the Earth's ozone layer, leaving organisms vulnerable to ultraviolet radiation from the Sun. Gamma ray bursts are fairly rare, occurring only a few times in a given galaxy per million years.\nIt has been suggested that a supernova or gamma ray burst caused the End-Ordovician extinction.\n\nOne theory is that periods of increased geomagnetic reversals will weaken Earth's magnetic field long enough to expose the atmosphere to the solar winds, causing oxygen ions to escape the atmosphere in a rate increased by 3–4 orders, resulting in a disastrous decrease in oxygen.\n\nMovement of the continents into some configurations can cause or contribute to extinctions in several ways: by initiating or ending ice ages; by changing ocean and wind currents and thus altering climate; by opening seaways or land bridges which expose previously isolated species to competition for which they are poorly adapted (for example, the extinction of most of South America's native ungulates and all of its large metatherians after the creation of a land bridge between North and South America). Occasionally continental drift creates a super-continent which includes the vast majority of Earth's land area, which in addition to the effects listed above is likely to reduce the total area of continental shelf (the most species-rich part of the ocean) and produce a vast, arid continental interior which may have extreme seasonal variations.\n\nAnother theory is that the creation of the super-continent Pangaea contributed to the End-Permian mass extinction. Pangaea was almost fully formed at the transition from mid-Permian to late-Permian, and the \"Marine genus diversity\" diagram at the top of this article shows a level of extinction starting at that time which might have qualified for inclusion in the \"Big Five\" if it were not overshadowed by the \"Great Dying\" at the end of the Permian.\n\nMany other hypotheses have been proposed, such as the spread of a new disease, or simple out-competition following an especially successful biological innovation. But all have been rejected, usually for one of the following reasons: they require events or processes for which there is no evidence; they assume mechanisms which are contrary to the available evidence; they are based on other theories which have been rejected or superseded.\n\nScientists have been concerned that human activities could cause more plants and animals to become extinct than any point in the past. Along with human-made changes in climate (see above), some of these extinctions could be caused by overhunting, overfishing, invasive species, or habitat loss. A study published in May 2017 in \"Proceedings of the National Academy of Sciences\" argued that a “biological annihilation” akin to a sixth mass extinction event is underway as a result of anthropogenic causes, such as over-population and over-consumption. The study suggested that as much as 50% of the number of animal individuals that once lived on Earth were already extinct, threatening the basis for human existence too.\n\nThe eventual warming and expanding of the Sun, combined with the eventual decline of atmospheric carbon dioxide could actually cause an even greater mass extinction, having the potential to wipe out even microbes, where rising global temperatures caused by the expanding Sun will gradually increase the rate of weathering, which in turn removes more and more carbon dioxide from the atmosphere. When carbon dioxide levels get too low (perhaps at 50 ppm), all plant life will die out, although simpler plants like grasses and mosses can survive much longer, until levels drop to 10 ppm.\n\nWith all photosynthetic organisms gone, atmospheric oxygen can no longer be replenished, and is eventually removed by chemical reactions in the atmosphere, perhaps from volcanic eruptions. Eventually the loss of oxygen will cause all remaining aerobic life to die out via asphyxiation, leaving behind only simple anaerobic prokaryotes. When the Sun becomes 10% brighter in about a billion years, Earth will suffer a moist greenhouse effect resulting in its oceans boiling away, while the Earth's liquid outer core cools due to the inner core's expansion and causes the Earth's magnetic field to shut down. In the absence of a magnetic field, charged particles from the Sun will deplete the atmosphere and further increase the Earth's temperature to an average of ~420 K (147 °C, 296 °F) in 2.8 billion years, causing the last remaining life on Earth to die out. This is the most extreme instance of a climate-caused extinction event. Since this will only happen late in the Sun's life, such will cause the final mass extinction in Earth's history (albeit a very long extinction event).\n\nThe impact of mass extinction events varied widely. After a major extinction event, usually only weedy species survive due to their ability to live in diverse habitats. Later, species diversify and occupy empty niches. Generally, biodiversity recovers 5 to 10 million years after the extinction event. In the most severe mass extinctions it may take 15 to 30 million years.\n\nThe worst event, the Permian–Triassic extinction event, devastated life on earth and is estimated to have killed off over 90% of species. Life seemed to recover quickly after the P-T extinction, but this was mostly in the form of disaster taxa, such as the hardy \"Lystrosaurus\". The most recent research indicates that the specialized animals that formed complex ecosystems, with high biodiversity, complex food webs and a variety of niches, took much longer to recover. It is thought that this long recovery was due to the successive waves of extinction which inhibited recovery, as well as to prolonged environmental stress to organisms which continued into the Early Triassic. Recent research indicates that recovery did not begin until the start of the mid-Triassic, 4M to 6M years after the extinction; and some writers estimate that the recovery was not complete until 30M years after the P-T extinction, i.e. in the late Triassic. Subsequent to the P-T extinction, there was an increase in provincialization, with species occupying smaller ranges – perhaps removing incumbents from niches and setting the stage for an eventual rediversification.\n\nThe effects of mass extinctions on plants are somewhat harder to quantify, given the biases inherent in the plant fossil record. Some mass extinctions (such as the end-Permian) were equally catastrophic for plants, whereas others, such as the end-Devonian, did not affect the flora.\n\n", "id": "9813", "title": "Extinction event"}
{"url": "https://en.wikipedia.org/wiki?curid=331755", "text": "Transitional fossil\n\nA transitional fossil is any fossilized remains of a life form that exhibits traits common to both an ancestral group and its derived descendant group. This is especially important where the descendant group is sharply differentiated by gross anatomy and mode of living from the ancestral group. These fossils serve as a reminder that taxonomic divisions are human constructs that have been imposed in hindsight on a continuum of variation. Because of the incompleteness of the fossil record, there is usually no way to know exactly how close a transitional fossil is to the point of divergence. Therefore, it cannot be assumed that transitional fossils are direct ancestors of more recent groups, though they are frequently used as models for such ancestors.\n\nIn 1859, when Charles Darwin's \"On the Origin of Species\" was first published, the fossil record was poorly known. Darwin described the perceived lack of transitional fossils as, \"...the most obvious and gravest objection which can be urged against my theory,\" but explained it by relating it to the extreme imperfection of the geological record. He noted the limited collections available at that time, but described the available information as showing patterns that followed from his theory of descent with modification through natural selection. Indeed, \"Archaeopteryx\" was discovered just two years later, in 1861, and represents a classic transitional form between earlier, non-avian dinosaurs and birds. Many more transitional fossils have been discovered since then, and there is now abundant evidence of how all classes of vertebrates are related, including many transitional fossils. Specific examples of class-level transitions are: tetrapods and fish, birds and dinosaurs, and mammals and \"mammal-like reptiles\".\n\nThe term \"missing link\" has been used extensively in popular writings on human evolution to refer to a perceived gap in the hominid evolutionary record. It is most commonly used to refer to any new transitional fossil finds. Scientists, however, do not use the term, as it refers to a pre-evolutionary view of nature.\n\nIn evolutionary taxonomy, the prevailing form of taxonomy during much of the 20th century and still used in non-specialist textbooks, taxa based on morphological similarity are often drawn as \"bubbles\" or \"spindles\" branching off from each other, forming evolutionary trees. Transitional forms are seen as falling between the various groups in terms of anatomy, having a mixture of characteristics from inside and outside the newly branched clade.\n\nWith the establishment of cladistics in the 1990s, relationships commonly came to be expressed in cladograms that illustrate the branching of the evolutionary lineages in stick-like figures. The different so-called \"natural\" or \"monophyletic\" groups form nested units, and only these are given phylogenetic names. While in traditional classification tetrapods and fish are seen as two different groups, phylogenetically tetrapods are considered a branch of fish. Thus, with cladistics there is no longer a transition between established groups, and the term \"transitional fossils\" is a misnomer. Differentiation occurs within groups, represented as branches in the cladogram.\n\nIn a cladistic context, transitional organisms can be seen as representing early examples of a branch, where not all of the traits typical of the previously known descendants on that branch have yet evolved. Such early representatives of a group are usually termed \"basal taxa\" or \"sister taxa,\" depending on whether the fossil organism belongs to the daughter clade or not.\n\nA source of confusion is the notion that a transitional form between two different taxonomic groups must be a direct ancestor of one or both groups. The difficulty is exacerbated by the fact that one of the goals of evolutionary taxonomy is to identify taxa that were ancestors of other taxa. However, it is almost impossible to be sure that any form represented in the fossil record is a direct ancestor of any other. In fact, because evolution is a branching process that produces a complex bush pattern of related species rather than a linear process producing a ladder-like progression, and because of the incompleteness of the fossil record, it is unlikely that any particular form represented in the fossil record is a direct ancestor of any other. Cladistics deemphasizes the concept of one taxonomic group being an ancestor of another, and instead emphasizes the identification of sister taxa that share a more recent common ancestor with one another than they do with other groups. There are a few exceptional cases, such as some marine plankton microfossils, where the fossil record is complete enough to suggest with confidence that certain fossils represent a population that was actually ancestral to a later population of a different species. But, in general, transitional fossils are considered to have features that illustrate the transitional anatomical features of actual common ancestors of different taxa, rather than to \"be\" actual ancestors.\n\n\"Archaeopteryx\" is a genus of theropod dinosaur closely related to the birds. Since the late 19th century, it has been accepted by palaeontologists, and celebrated in lay reference works, as being the oldest known bird, though a study in 2011 has cast doubt on this assessment, suggesting instead that it is a non-avialan dinosaur closely related to the origin of birds.\n\nIt lived in what is now southern Germany in the Late Jurassic period around 150 million years ago, when Europe was an archipelago in a shallow warm tropical sea, much closer to the equator than it is now. Similar in shape to a European magpie, with the largest individuals possibly attaining the size of a raven, \"Archaeopteryx\" could grow to about 0.5 metres (1.6 ft) in length. Despite its small size, broad wings, and inferred ability to fly or glide, \"Archaeopteryx\" has more in common with other small Mesozoic dinosaurs than it does with modern birds. In particular, it shares the following features with the deinonychosaurs (dromaeosaurs and troodontids): jaws with sharp teeth, three fingers with claws, a long bony tail, hyperextensible second toes (\"killing claw\"), feathers (which suggest homeothermy), and various skeletal features. These features make \"Archaeopteryx\" a clear candidate for a transitional fossil between dinosaurs and birds, making it important in the study both of dinosaurs and of the origin of birds.\n\nThe first complete specimen was announced in 1861, and ten more \"Archaeopteryx\" fossils have been found since then. Most of the eleven known fossils include impressions of feathers—among the oldest direct evidence of such structures. Moreover, because these feathers take the advanced form of flight feathers, \"Archaeopteryx\" fossils are evidence that feathers began to evolve before the Late Jurassic.\n\nThe hominid \"Australopithecus afarensis\" represents an evolutionary transition between modern bipedal humans and their quadrupedal ape ancestors. A number of traits of the \"A. afarensis\" skeleton strongly reflect bipedalism, to the extent that some researchers have suggested that bipedality evolved long before \"A. afarensis\". In overall anatomy, the pelvis is far more human-like than ape-like. The iliac blades are short and wide, the sacrum is wide and positioned directly behind the hip joint, and there is clear evidence of a strong attachment for the knee extensors, implying an upright posture.\n\nWhile the pelvis is not entirely like that of a human (being markedly wide, or flared, with laterally orientated iliac blades), these features point to a structure radically remodelled to accommodate a significant degree of bipedalism. The femur angles in toward the knee from the hip. This trait allows the foot to fall closer to the midline of the body, and strongly indicates habitual bipedal locomotion. Present-day humans, orangutans and spider monkeys possess this same feature. The feet feature adducted big toes, making it difficult if not impossible to grasp branches with the hindlimbs. Besides locomotion, \"A. afarensis\" also had a slightly larger brain than a modern chimpanzee (the closest living relative of humans) and had teeth that were more human than ape-like.\n\nThe cetaceans (whales, dolphins and porpoises) are marine mammal descendants of land mammals. The pakicetids are an extinct family of hoofed mammals that are the earliest whales, whose closest sister group is \"Indohyus\" from family Raoellidae. They lived in the Early Eocene, around 53 million years ago. Their fossils were first discovered in North Pakistan in 1979, at a river not far from the shores of the former Tethys Sea. Pakicetids could hear under water, using enhanced bone conduction, rather than depending on tympanic membranes like most land mammals. This arrangement does not give directional hearing under water.\n\n\"Ambulocetus natans\", which lived about 49 million years ago, was discovered in Pakistan in 1994. It was probably amphibious, and looked like a crocodile. In the Eocene, ambulocetids inhabited the bays and estuaries of the Tethys Ocean in northern Pakistan. The fossils of ambulocetids are always found in near-shore shallow marine deposits associated with abundant marine plant fossils and littoral molluscs. Although they are found only in marine deposits, their oxygen isotope values indicate that they consumed water with a range of degrees of salinity, some specimens showing no evidence of sea water consumption and others none of fresh water consumption at the time when their teeth were fossilized. It is clear that ambulocetids tolerated a wide range of salt concentrations. Their diet probably included land animals that approached water for drinking, or freshwater aquatic organisms that lived in the river. Hence, ambulocetids represent the transition phase of cetacean ancestors between freshwater and marine habitat.\n\n\"Tiktaalik\" is a genus of extinct sarcopterygian (lobe-finned fish) from the Late Devonian period, with many features akin to those of tetrapods (four-legged animals). It is one of several lines of ancient sarcopterygians to develop adaptations to the oxygen-poor shallow water habitats of its time—adaptations that led to the evolution of tetrapods. Well-preserved fossils were found in 2004 on Ellesmere Island in Nunavut, Canada.\n\n\"Tiktaalik\" lived approximately 375 million years ago. Paleontologists suggest that it is representative of the transition between non-tetrapod vertebrates such as \"Panderichthys\", known from fossils 380 million years old, and early tetrapods such as \"Acanthostega\" and \"Ichthyostega\", known from fossils about 365 million years old. Its mixture of primitive fish and derived tetrapod characteristics led one of its discoverers, Neil Shubin, to characterize \"Tiktaalik\" as a \"fishapod.\" Unlike many previous, more fish-like transitional fossils, the \"fins\" of \"Tiktaalik\" have basic wrist bones and simple rays reminiscent of fingers. They may have been weight-bearing. Like all modern tetrapods, it had rib bones, a mobile neck with a separate pectoral girdle, and lungs, though it had the gills, scales, and fins of a fish.\n\nTetrapod footprints found in Poland and reported in \"Nature\" in January 2010 were \"securely dated\" at 10 million years older than the oldest known elpistostegids (of which \"Tiktaalik\" is an example), implying that animals like \"Tiktaalik\", possessing features that evolved around 400 million years ago, were \"late-surviving relics rather than direct transitional forms, and they highlight just how little we know of the earliest history of land vertebrates.\"\n\nPleuronectiformes (flatfish) are an order of ray-finned fish. The most obvious characteristic of the modern flatfish is their asymmetry, with both eyes on the same side of the head in the adult fish. In some families the eyes are always on the right side of the body (dextral or right-eyed flatfish) and in others they are always on the left (sinistral or left-eyed flatfish). The primitive spiny turbots include equal numbers of right- and left-eyed individuals, and are generally less asymmetrical than the other families. Other distinguishing features of the order are the presence of protrusible eyes, another adaptation to living on the seabed (benthos), and the extension of the dorsal fin onto the head.\n\n\"Amphistium\" is a 50-million-year-old fossil fish identified as an early relative of the flatfish, and as a transitional fossil In \"Amphistium\", the transition from the typical symmetric head of a vertebrate is incomplete, with one eye placed near the top-center of the head. Paleontologists concluded that \"the change happened gradually, in a way consistent with evolution via natural selection—not suddenly, as researchers once had little choice but to believe.\"\n\n\"Amphistium\" is among the many fossil fish species known from the Monte Bolca \"Lagerstätte\" of Lutetian Italy. \"Heteronectes\" is a related, and very similar fossil from slightly earlier strata of France.\n\nA Middle Devonian precursor to seed plants has been identified from Belgium, predating the earliest seed plants by about 20 million years. \"Runcaria\", small and radially symmetrical, is an integumented megasporangium surrounded by a cupule. The megasporangium bears an unopened distal extension protruding above the multilobed integument. It is suspected that the extension was involved in anemophilous pollination. \"Runcaria\" sheds new light on the sequence of character acquisition leading to the seed, having all the qualities of seed plants except for a solid seed coat and a system to guide the pollen to the seed.\n\nNot every transitional form appears in the fossil record, because the fossil record is not complete. Organisms are only rarely preserved as fossils in the best of circumstances, and only a fraction of such fossils have been discovered. Paleontologist Donald Prothero noted that this is illustrated by the fact that the number of species known through the fossil record was less than 5% of the number of known living species, suggesting that the number of species known through fossils must be far less than 1% of all the species that have ever lived.\n\nBecause of the specialized and rare circumstances required for a biological structure to fossilize, logic dictates that known fossils represent only a small percentage of all life-forms that ever existed—and that each discovery represents only a snapshot of evolution. The transition itself can only be illustrated and corroborated by transitional fossils, which never demonstrate an exact half-way point between clearly divergent forms.\n\nThe fossil record is very uneven and, with few exceptions, is heavily slanted toward organisms with hard parts, leaving most groups of soft-bodied organisms with little to no fossil record. The groups considered to have a good fossil record, including a number of transitional fossils between traditional groups, are the vertebrates, the echinoderms, the brachiopods and some groups of arthropods.\n\nThe idea that animal and plant species were not constant, but changed over time, was suggested as far back as the 18th century. Darwin's \"On the Origin of Species\", published in 1859, gave it a firm scientific basis. A weakness of Darwin's work, however, was the lack of palaeontological evidence, as pointed out by Darwin himself. While it is easy to imagine natural selection producing the variation seen within genera and families, the transmutation between the higher categories was harder to imagine. The dramatic find of the London specimen of \"Archaeopteryx\" in 1861, only two years after the publication of Darwin's work, offered for the first time a link between the class of the highly derived birds, and that of the more primitive reptiles. In a letter to Darwin, the palaeontologist Hugh Falconer wrote:\n\nHad the Solnhofen quarries been commissioned — by august command — to turn out a strange being à la Darwin — it could not have executed the behest more handsomely — than in the \"Archaeopteryx\".\n\nThus, transitional fossils like \"Archaeopteryx\" came to be seen as not only corroborating Darwin's theory, but as icons of evolution in their own right. For example, the Swedish encyclopedic dictionary \"Nordisk familjebok\" of 1904 showed an inaccurate \"Archaeopteryx\" reconstruction (see illustration) of the fossil, \"ett af de betydelsefullaste paleontologiska fynd, som någonsin gjorts\" (\"one of the most significant paleontological discoveries ever made\").\n\nTransitional fossils are not only those of animals. With the increasing mapping of the divisions of plants at the beginning of the 20th century, the search began for the ancestor of the vascular plants. In 1917, Robert Kidston and William Henry Lang found the remains of an extremely primitive plant in the Rhynie chert in Aberdeenshire, Scotland, and named it \"Rhynia\".\n\nThe \"Rhynia\" plant was small and stick-like, with simple dichotomously branching stems without leaves, each tipped by a sporangium. The simple form echoes that of the sporophyte of mosses, and it has been shown that \"Rhynia\" had an alternation of generations, with a corresponding gametophyte in the form of crowded tufts of diminutive stems only a few millimetres in height. \"Rhynia\" thus falls midway between mosses and early vascular plants like ferns and clubmosses. From a carpet of moss-like gametophytes, the larger \"Rhynia\" sporophytes grew much like simple clubmosses, spreading by means of horizontal growing stems growing rhizoids that anchored the plant to the substrate. The unusual mix of moss-like and vascular traits and the extreme structural simplicity of the plant had huge implications for botanical understanding.\n\nThe term \"missing link\" refers back to the originally static pre-evolutionary concept of the great chain of being, a deist idea that all existence is linked, from the lowest dirt, through the living kingdoms to angels and finally to God. The idea of all living things being linked through some sort of transmutation process predates Darwin's theory of evolution. Jean-Baptiste Lamarck envisioned that life is generated in the form of the simplest creatures constantly, and then strive towards complexity and perfection (i.e. humans) through a series of lower forms. In his view, lower animals were simply newcomers on the evolutionary scene.\n\nAfter \"On the Origin of Species\", the idea of \"lower animals\" representing earlier stages in evolution lingered, as demonstrated in Ernst Haeckel's figure of the human pedigree. While the vertebrates were then seen as forming a sort of evolutionary sequence, the various classes were distinct, the undiscovered intermediate forms being called \"missing links.\"\n\nThe term was first used in a scientific context by Charles Lyell in the third edition (1851) of his book \"Elements of Geology\" in relation to missing parts of the geological column, but it was popularized in its present meaning by its appearance on page xi of his book \"Geological Evidences of the Antiquity of Man\" of 1863. By that time it was generally thought that the end of the last glacial period marked the first appearance of humanity, but Lyell drew on new findings in his \"Antiquity of Man\" to put the origin of human beings much further back in the deep geological past. Lyell wrote that it remained a profound mystery how the huge gulf between man and beast could be bridged. Lyell's vivid writing fired the public imagination, inspiring Jules Verne's \"Journey to the Center of the Earth\" (1864) and Louis Figuier's 1867 second edition of \"La Terre avant le déluge\" (\"Earth before the Flood\"), which included dramatic illustrations of savage men and women wearing animal skins and wielding stone axes, in place of the Garden of Eden shown in the 1863 edition.\n\nThe idea of a \"missing link\" between humans and so-called \"lower\" animals remains lodged in the public imagination. The search for a fossil showing transitional traits between apes and humans, however, was fruitless until the young Dutch geologist Eugène Dubois found a skullcap, a molar and a femur on the banks of Solo River, Java in 1891. The find combined a low, ape-like skull roof with a brain estimated at around 1000 cc, midway between that of a chimpanzee and an adult human. The single molar was larger than any modern human tooth, but the femur was long and straight, with a knee angle showing that \"Java Man\" had walked upright. Given the name \"Pithecanthropus erectus\" (\"erect ape-man\"), it became the first in what is now a long list of human evolution fossils. At the time it was hailed by many as the \"missing link,\" helping set the term as primarily used for human fossils, though it is sometimes used for other intermediates, like the dinosaur-bird intermediary \"Archaeopteryx\".\n\"Missing link\" is still a popular term, well recognized by the public and often used in the popular media. It is, however, avoided in the scientific press, as it relates to the concept of the great chain of being and to the notion of simple organisms being primitive versions of complex ones, both of which have been discarded in biology. In any case, the term itself is misleading, as any known transitional fossil, like Java Man, is no longer missing. While each find will give rise to new gaps in the evolutionary story on each side, the discovery of more and more transitional fossils continues to add to our knowledge of evolutionary transitions.\n\nThe theory of punctuated equilibrium developed by Stephen Jay Gould and Niles Eldredge and first presented in 1972 is often mistakenly drawn into the discussion of transitional fossils. This theory, however, pertains only to well-documented transitions within taxa or between closely related taxa over a geologically short period of time. These transitions, usually traceable in the same geological outcrop, often show small jumps in morphology between extended periods of morphological stability. To explain these jumps, Gould and Eldredge envisaged comparatively long periods of genetic stability separated by periods of rapid evolution. Gould made the following observation concerning creationist misuse of his work to deny the existence of transitional fossils:\n\n\n", "id": "331755", "title": "Transitional fossil"}
{"url": "https://en.wikipedia.org/wiki?curid=40852244", "text": "HKA test\n\nThe HKA Test, named after Richard R. Hudson, Martin Kreitman, and Montserrat Aguadé, is a statistical test used in genetics to evaluate the predictions of the Neutral Theory of molecular evolution. By comparing the polymorphism within each species and the divergence observed between two species at two or more loci, the test can determine whether the observed difference is likely due to neutral evolution or rather due to adaptive evolution. Developed in 1989, the HKA test is a precursor to the McDonald-Kreitman test, which was derived in 1991. The HKA test is best used to look for balancing selection, recent selective sweeps or other variation-reducing forces.\n\nNeutral Evolution Theory, first proposed by Kimura in a 1968 paper, and later fully defined and published in 1983, is the basis for many statistical tests that detect selection at the molecular level. Kimura noted that there was much too high of a rate of mutation within the genome (i.e. high polymorphism) to be strictly under directional evolution. Furthermore, functionally less important regions of the genome evolve at a faster rate. Kimura then postulated that most of the modifications to the genome are neutral or nearly neutral, and evolve by random genetic drift. Therefore, under the neutral model, polymorphism within a species and divergence between related species at homologous sites will be highly correlated. The Neutral Evolution theory has become the null model against which tests for selection are based, and divergence from this model can be explained by directional or selective evolution.\n\nThe rate of mutation within a population can be estimated using the Watterson estimator formula: θ=4Νμ, where Ν is the effective population size and μ is the mutation rate (substitutions per site per unit of time). Hudson et al. proposed applying these variables to a chi-squared, goodness-of-fit test.\n\nThe test statistic proposed by Hudson et al., Χ, is:\n\nThis states that, for each locus (\"L\") (for which there must be at least two) the sum of the difference in number of observed polymorphic sites in sample A minus the estimate of expected polymorphism squared, all of which is divided by the variance. Similarly, this formula is then applied to Sample B (from another species) and then can be applied to the divergence between two sample species. The sum of these three variables is the test statistic (X). If the polymorphism within species A, and B, and the divergence between them are all independent, then the test statistic should fall approximately onto a chi-squared distribution.\n\nFor a simple explanation, let D = divergence between species, or the number of fixed differences in locus one. Similarly D = divergence in locus two. Let P and P = the number of polymorphic sites in loci one and two, respectively (a measure of polymorphism within species). If there is no directional evolution, then D/D = P/P.\n\nFor these examples, the distance between two species’ loci is determined by measuring the number of substitutions per site when comparing the two species. We can then calculate the rate of mutation (changes to the DNA sequence pre unit of time) if we know the time since the two species diverged from the common ancestor.\n\nA test that suggests neutral evolution:\nSuppose that you have data from two loci (1 and 2) in two species (A and B). Locus 1 shows high divergence and high polymorphism in both species. Locus 2 shows low divergence and low polymorphism. This can be explained by a neutral difference in the rate of mutations in each loci.\n\nA test that suggests selection:\nAgain suppose you have data as in the last example, only this time locus 2 has equal divergence to locus 1 and yet lower polymorphism in species B. In this case the rate of mutation in each loci is equal, so this can only be explained by a reduction in the effective population size Ne of species B, which is inferred as an act of selection.\n", "id": "40852244", "title": "HKA test"}
{"url": "https://en.wikipedia.org/wiki?curid=41161859", "text": "Ecological speciation\n\nEcological speciation is the process by which ecologically based divergent selection between different environments leads to the creation of reproductive barriers between populations. This is often the result of selection over traits which are genetically correlated to reproductive isolation, thus speciation occurs as a by-product of adaptive divergence.\n\nEcological selection is \"the interaction of individuals with their environment during resource acquisition\". Natural selection is inherently involved in the process of speciation, whereby, \"under ecological speciation, populations in different environments, or populations exploiting different resources, experience contrasting natural selection pressures on the traits that directly or indirectly bring about the evolution of reproductive isolation\". Evidence for the role ecology plays in the process of speciation exists. Studies of stickleback populations support ecologically-linked speciation arising as a by-product, alongside numerous studies of parallel speciation—of which, substantiates speciation's occurrence in nature.\n\nThe key difference between ecological speciation and other kinds of speciation, is that it is triggered by divergent natural selection among different habitats; as opposed to other kinds of speciation processes, like random genetic drift, the fixation of incompatible mutations in populations experiencing similar selective pressures, or various forms of sexual selection not involving selection on ecologically relevant traits. Ecological speciation can occur either in allopatry, sympatry, or parapatry. The only requirement being that speciation occurs as a result of adaptation to different ecological or micro-ecological conditions.\n\nSome debate exists over the framework concerning the delineation of whether a speciation event is ecological or nonecological. \"The pervasive effect of selection suggests that adaptive evolution and speciation are inseparable, casting doubt on whether speciation is ever nonecological\".\n\nParallel speciation is where \"greater reproductive isolation repeatedly evolves between independent populations adapting to contrasting environments than between independent populations adapting to similar environments\". It is established that ecological speciation occurs and with much of the evidence, \"...accumulated from top-down studies of adaptation and reproductive isolation\".\n\nKnown examples of ecological speciation include three-spined stickleback fishes, distinct species of which emerged as the result to adaptation of different conditions along water depth clines in freshwater lakes. Ancestors of the genus \"Ilex\" (holly) became isolated from the remaining \"Ilex\" when the Earth mass broke away into Gondwana and Laurasia about 82 million years ago, resulting in a physical separation of the groups (allopatry) and beginning a process of change to adapt to new conditions; over time survivor species of the holly genus adapted to different ecological niches. The invasive weed species \"Centaurea solstitialis\" is thought to be a case of ecological speciation—in less than 200 years, incipient reproductive isolation appeared as a result to adaptation to different ecological conditions between native and non-native ranges.\n\nParallel speciation occurs for example in mosquito fish in the Bahamas, where \"Gambusia\" fish inhabit \"blue holes\"—carbonate caves and depressions flooded with water throughout the islands. Some of the holes contain the piscivorous predator fish \"Gobiomorus dormitor\", while others have no major predators (excluding birds). The authors of the study tested for ecological speciation by measuring three different data sets: morphological data (to test for divergent natural selection), molecular data (to test for \"replicated trait evolution in independent populations\" with similar phenotypes), and mate-choice trials (to test for reproductive isolation between \"ecologically divergent pairs of populations than ecologically similar ones\": a by-product resulting from divergent traits). The study allowed for a natural experiment to test the effects of predator-mediated natural selection and its by-product: ecological speciation. The results suggested a \"strong confirmation of the ecological speciation hypothesis\" and amply supported parallel speciation taking place within the different blue holes.\n\nAnother example is in skinks, where the \"Plestiodon\" genus (formerly \"Eumeces\") has a complex evolutionary history with ecological speciation and parallel speciation of the three species (within two morphotypes): \"E. skiltonianus\" and \"E. lagunensis\", and \"E. gilberti\". \"E. gilberti\" occupies dry, low-elevation habitats, has a larger body size, and uniform, solid color scales. The other two species inhabit higher-elevation regions, have smaller bodies, and exhibit colored stripes. The members of the group have similar phenotypic stages during early development but differ in their morphology in later stages. A phylogenetic analysis using mtDNA of the entire group, (including all species and subspecies of the \"Eumeces\" group inhabiting the western United States) combined with comparative approaches to morphology, and geographic distribution showed \"instances of parallel morphological evolution...and provide evidence that this system is consistent with a model of ecological speciation\" due to \"the similarity in early ontogenetic trajectories and the close association between differences in body size and color pattern[s]\" of each morphotype.\n\n", "id": "41161859", "title": "Ecological speciation"}
{"url": "https://en.wikipedia.org/wiki?curid=19154280", "text": "Evolutionary trap\n\nThe term evolutionary trap has retained several definitions associated with different biological disciplines.\n\nWithin evolutionary biology, this term has been used sporadically to refer to cases in which an evolved, and presumably adaptive, trait has suddenly become maladaptive, leading to the extinction of the species.\n\nWithin behavioral and ecological sciences, evolutionary traps occur when rapid environmental change triggers organisms to make maladaptive behavioral decisions. While these traps may take place within any type of behavioral context (e.g. mate selection, navigation, nest-site selection), the most empirically and theoretically well-understood type of evolutionary trap is the ecological trap which represents maladaptive habitat selection behavior.\n\nWitherington demonstrates an interesting case of a \"navigational trap\". Over evolutionary time, hatchling sea turtles have evolved the tendency to migrate toward the light of the moon upon emerging from their sand nests. However, in the modern world, this has resulted in them tending to orient towards bright beach-front lighting, which is a more intense light source than the moon. As a result, the hatchlings migrate up the beach and away from the ocean where they exhaust themselves, desiccate and die either as a result of exhaustion, dehydration or predation.\n\nHabitat selection is an extremely important process in the lifespan of most organisms. That choice affects nearly all of an individual’s subsequent choices, so it may not be particularly surprising the type of evolutionary trap with the best empirical support is the ecological trap. Even so, traps may be relatively difficult to detect and so the lack of evidence for other types of evolutionary trap may be a result of the paucity of researchers looking for them coupled with the demanding evidence required to demonstrate their existence.\n\n", "id": "19154280", "title": "Evolutionary trap"}
{"url": "https://en.wikipedia.org/wiki?curid=41123642", "text": "Recurrent evolution\n\nRecurrent evolution is the repeated evolution of a particular character. Most evolution, or changes in allele frequencies from one generation to the next, is the result of drift, or random chance of some alleles getting passed down and others not. Recurrent evolution is when patterns emerge from this stochastic process when looking across populations. These patterns are of particular interest to evolutionary biologists as they can teach people about the underlying forces of evolution.\n\nRecurrent evolution is a broad term, but is usually used to describe recurring regimes of selection within or across lineages. While most commonly used to describe recurring patterns of selection, it can also be used to describe recurring patterns of mutation, for example transitions are more common than transversions. It encompasses both convergent evolution and parallel evolution and can be used to describe the observation of similar repeating changes through directional selection as well as the observation of highly conserved phenotypes or genotypes across lineages through continuous purifying selection over large periods of evolutionary time. The changes can be observed at the phenotype level or the genotype level. At the phenotype level recurrent evolution can be observed across a continuum of levels, which for simplicity can be broken down into molecular phenotype, cellular phenotype, and organismal phenotype. At the genotype level recurrent evolution can only be detected using DNA sequencing data. The same or similar changes in the genomes of different lineages indicates recurrent genomic evolution may have taken place. Recurrent genomic evolution can also occur within a lineage. An example of this would include some types of phase variation that involve highly directed changes at the DNA sequence level. The evolution of different forms of phase variation in separate lineages represent convergent and recurrent evolution toward increased evolvability. In organisms with longer generation times, any potential recurrent genomic evolution within a lineage would be difficult to detect. Recurrent evolution has been studied most extensively at the organismic level but with cheaper and faster sequencing technologies more attention is being paid to recurrent genomic evolution. Recurrent evolution can also be described as recurring or repeated evolution.\n\nThe distinction between convergent and parallel evolution is somewhat unresolved in evolutionary biology. Some authors have claimed it is a false dichotomy while others have argued there are still important distinctions. These debates are important when considering recurrent evolution as their basis is in the degree of phylogenetic relatedness among the organisms being considered and convergent and parallel evolution are the major sources of recurrent evolution. While convergent evolution and parallel evolution are both forms of recurrent evolution they involve multiple lineages whereas recurrent evolution can also take place in a single lineage. As mentioned before, recurrent evolution within a lineage can be difficult to detect in organisms with longer generation times; however paleontological evidence can be used to show recurrent phenotypic evolution within a lineage. The distinction between recurrent evolution across lineages and recurrent evolution within a lineage can be blurred because lineages do not have a set size and convergent or parallel evolution takes place among lineages that are all part of or within the same greater lineage. When speaking of recurrent evolution within a lineage, the simplest example is that given above, of the on-off switch used by bacteria in phase variation, but it can also involve phenotypic swings back and forth over longer periods of evolutionary history. These may be caused by environmental swings, for example the natural fluctuations in the climate or a pathogenic bacteria moving between hosts, and represent the other major source of recurrent evolution. Recurrent evolution caused by convergent and parallel evolution, and recurrent evolution caused by environmental swings, are not necessarily mutually exclusive. If the environmental swings have the same effect on the phenotypes of different species, they could potentially evolve in parallel back and forth together through each swing.\n\nOn the island of Bermuda, the shell size of the land snail poecilozonites has increased during glacial periods and shrunk again during warmer periods. This is due to the increased size of the island during glacial periods resulting in more large vertebrate predators creating selection for larger shell size in the snails.\n\nIn eusocial insects, new colonies are usually formed by a solitary queen; however this is not always the case. Dependent colony formation, when new colonies are formed by more than one individual, has evolved recurrently multiple times in ants, bees, and wasps.\n\nRecurrent evolution of polymorphisms in the colonial invertebrate animal cheilostomata bryozoans has given rise to zooid polymorphs and some skeletal structures several times in evolutionary history.\n\nNeotropical tanagers, \"Diglossa\" and \"Diglossopis\", known as flowerpiercers, have undergone recurrent evolution of divergent bill types.\n\nThere is evidence for at least 133 transitions between dioecy and hermaphroditism in the sexual systems of bryophytes. Additionally the transition rate from hermaphroditism to dioecy was approximately twice the reverse rate suggesting greater diversification among hermphrodites and demonstrating the recurrent evolution of dioecy in mosses.\n\nC4 photosynthesis has evolved over 60 times in different plants. This has occurred through using genes present in the C3 photosynthetic ancestor, altering levels and patterns of gene expression, and adaptive changes in the protein coding region. Recurrent lateral gene transfer has also played a role in optimizing the C4 pathway by providing better adapted C4 genes to the plant.\n\nCertain mutations occur with measurable and consistent frequencies. Deleterious and neutral alleles can increase in frequency if the mutation rate to this phenotype is sufficiently higher than the reverse mutation rate, however this appears to be rare. Beyond creating new variation for selection to act upon mutations plays a primary role in evolution when mutations in one direction are \"weeded out by natural selection\" and mutations in the other direction are neutral. This is purify selection when it acts to maintain functionally important characters but also results in the loss or diminished size of useless organs as the functional constraint is lifted. An example of this is the diminished size of the Y chromosome in mammals and this can be attributed to recurrent mutations and recurrent evolution.\n\nThe existence of mutational hotspots within the genome gives rise to recurrent evolution. Hotspots can arise at certain nucleotide sequences because of interactions between the DNA and DNA repair, replication, and modification enzymes. These sequences can act like fingerprints to locate mutational hotspots.\n\nCis-regulatory elements are frequent targets of evolution resulting in varied morphology. When looking at long term evolution mutations in cis-regulatory regions appear to be even more common. In other words, more interspecific morphological differences are caused by mutations in cis-regulatory regions than intraspecific differences.\n\nAcross drosophila species highly conserved blocks not only in the histone fold domain but also in N-terminal tail of centromeric histone H3 (CenH3) demonstrate recurrent evolution by purifying selection. In fact very similar oligopeptides in the N-terminal tails of CenH3 have also been observed in humans and in mice.\n\nMany divergent eukaryotic lineages have recurrently evolved highly AT-rich genomes. GC-rich genomes are rarer among eukaryotes but when they evolve independently in two different species the recurrent evolution of similar preferential codon usages will usually result.\n\n\"Generally, regulatory genes occupying nodal position in gene regulatory networks, and which function as morphogenetic switches, can be anticipated to be prime targets for evolutionary changes and therefore repeated evolution.\"\n", "id": "41123642", "title": "Recurrent evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=41468418", "text": "Evolution of photosynthesis\n\nThe evolution of photosynthesis refers to the origin and subsequent evolution of photosynthesis, the process by which light energy synthesizes sugars from carbon dioxide, releasing oxygen as a waste product.\n\nThe first photosynthetic organisms probably evolved early in the evolutionary history of life and most likely used reducing agents such as hydrogen or electrons, rather than water. There are three major metabolic pathways by which photosynthesis is carried out: C photosynthesis, C photosynthesis, and CAM photosynthesis. C photosynthesis is the oldest and most common form.\n\nThe biochemical capacity to use water as the source for electrons in photosynthesis evolved in a common ancestor of extant cyanobacteria. The geological record indicates that this transforming event took place early in Earth's history, at least 2450–2320 million years ago (Ma), and, it is speculated, much earlier. Available evidence from geobiological studies of Archean (>2500 Ma) sedimentary rocks indicates that life existed 3500 Ma, but the question of when oxygenic photosynthesis evolved is still unanswered. A clear paleontological window on cyanobacterial evolution opened about 2000 Ma, revealing an already-diverse biota of blue-greens. Cyanobacteria remained principal primary producers throughout the Proterozoic Eon (2500–543 Ma), in part because the redox structure of the oceans favored photoautotrophs capable of nitrogen fixation. Green algae joined blue-greens as major primary producers on continental shelves near the end of the Proterozoic, but only with the Mesozoic (251–65 Ma) radiations of dinoflagellates, coccolithophorids, and diatoms did primary production in marine shelf waters take modern form. Cyanobacteria remain critical to marine ecosystems as primary producers in oceanic gyres, as agents of biological nitrogen fixation, and, in modified form, as the plastids of marine algae.\n\nEarly photosynthetic systems, such as those from green and purple sulfur and green and purple nonsulfur bacteria, are thought to have been anoxygenic, using various molecules as electron donors. Green and purple sulfur bacteria are thought to have used hydrogen and sulfur as an electron donor. Green nonsulfur bacteria used various amino and other organic acids. Purple nonsulfur bacteria used a variety of nonspecific organic molecules.\n\nFossils of what are thought to be filamentous photosynthetic organisms have been dated at 3.4 billion years old.\n\nThe main source of oxygen in the atmosphere is oxygenic photosynthesis, and its first appearance is sometimes referred to as the oxygen catastrophe. Geological evidence suggests that oxygenic photosynthesis, such as that in cyanobacteria, became important during the Paleoproterozoic era around 2 billion years ago. Modern photosynthesis in plants and most photosynthetic prokaryotes is oxygenic. Oxygenic photosynthesis uses water as an electron donor, which is oxidized to molecular oxygen () in the photosynthetic reaction center.\n\nSeveral groups of animals have formed symbiotic relationships with photosynthetic algae. These are most common in corals, sponges and sea anemones. It is presumed that this is due to the particularly simple body plans and large surface areas of these animals compared to their volumes. In addition, a few marine mollusks \"Elysia viridis\" and \"Elysia chlorotica\" also maintain a symbiotic relationship with chloroplasts they capture from the algae in their diet and then store in their bodies. This allows the mollusks to survive solely by photosynthesis for several months at a time. Some of the genes from the plant cell nucleus have even been transferred to the slugs, so that the chloroplasts can be supplied with proteins that they need to survive.\n\nAn even closer form of symbiosis may explain the origin of chloroplasts. Chloroplasts have many similarities with photosynthetic bacteria, including a circular chromosome, prokaryotic-type ribosomes, and similar proteins in the photosynthetic reaction center. The endosymbiotic theory suggests that photosynthetic bacteria were acquired (by endocytosis) by early eukaryotic cells to form the first plant cells. Therefore, chloroplasts may be photosynthetic bacteria that adapted to life inside plant cells. Like mitochondria, chloroplasts still possess their own DNA, separate from the nuclear DNA of their plant host cells and the genes in this chloroplast DNA resemble those in cyanobacteria. DNA in chloroplasts codes for redox proteins such as photosynthetic reaction centers. The CoRR Hypothesis proposes that this Co-location is required for Redox Regulation.\n\nA 2010 study by researchers at Tel Aviv University discovered that the Oriental hornet (\"Vespa orientalis\") converts sunlight into electric power using a pigment called xanthopterin. This is the first scientific evidence of a member of the animal kingdom engaging in photosynthesis.\n\nPhotosynthesis is not quite as simple as adding water to to produce sugars and oxygen. A complex chemical pathway is involved, facilitated along the way by a range of enzymes and co-enzymes. The enzyme RuBisCO is responsible for \"fixing\"  – that is, it attaches it to a carbon-based molecule to form a sugar, which can be used by the plant, releasing an oxygen molecule along the way. However, the enzyme is notoriously inefficient, and just as effectively will also fix oxygen instead of in a process called photorespiration. This is energetically costly as the plant has to use energy to turn the products of photorespiration back into a form that can react with .\n\nThe C metabolic pathway is a valuable recent evolutionary innovation in plants, involving a complex set of adaptive changes to physiology and gene expression patterns. About 7600 species of plants use carbon fixation, which represents about 3% of all terrestrial species of plants. All these 7600 species are angiosperms.\n\nC plants evolved carbon concentrating mechanisms. These work by increasing the concentration of around RuBisCO, thereby facilitating photosynthesis and decreasing photorespiration. The process of concentrating around RuBisCO requires more energy than allowing gases to diffuse, but under certain conditions – i.e. warm temperatures (>25 °C), low concentrations, or high oxygen concentrations – pays off in terms of the decreased loss of sugars through photorespiration.\n\nOne type of C metabolism employs a so-called Kranz anatomy. This transports through an outer mesophyll layer, via a range of organic molecules, to the central bundle sheath cells, where the is released. In this way, is concentrated near the site of RuBisCO operation. Because RuBisCO is operating in an environment with much more than it otherwise would be, it performs more efficiently.\n\nA second mechanism, CAM photosynthesis, is a carbon fixation pathway that evolved in some plants as an adaptation to arid conditions. The most important benefit of CAM to the plant is the ability to leave most leaf stomata closed during the day. This reduces water loss due to evapotranspiration. The stomata open at night to collect , which is stored as the four-carbon acid malate, and then used during photosynthesis during the day. The pre-collected is concentrated around the enzyme RuBisCO, increasing photosynthetic efficiency. More is then harvested from the atmosphere when stomata open, during the cool, moist nights, reducing water loss.\n\nCAM has evolved convergently many times. It occurs in 16,000 species (about 7% of plants), belonging to over 300 genera and around 40 families, but this is thought to be a considerable underestimate. It is found in quillworts (relatives of club mosses), in ferns, and in gymnosperms, but the great majority of plants using CAM are angiosperms (flowering plants).\n\nThese two pathways, with the same effect on RuBisCO, evolved a number of times independently – indeed, C alone arose 62 times in 18 different plant families. A number of 'pre-adaptations' seem to have paved the way for C4, leading to its clustering in certain clades: it has most frequently been innovated in plants that already had features such as extensive vascular bundle sheath tissue. Many potential evolutionary pathways resulting in the phenotype are possible and have been characterised using Bayesian inference, confirming that non-photosynthetic adaptations often provide evolutionary stepping stones for the further evolution of .\n\nThe C construction is most famously used by a subset of grasses, while CAM is employed by many succulents and cacti. The trait appears to have emerged during the Oligocene, around ; however, they did not become ecologically significant until the Miocene, . Remarkably, some charcoalified fossils preserve tissue organised into the Kranz anatomy, with intact bundle sheath cells, allowing the presence C metabolism to be identified without doubt at this time. Isotopic markers are used to deduce their distribution and significance.\n\nC plants preferentially use the lighter of two isotopes of carbon in the atmosphere, C, which is more readily involved in the chemical pathways involved in its fixation. Because C metabolism involves a further chemical step, this effect is accentuated. Plant material can be analysed to deduce the ratio of the heavier C to C. This ratio is denoted . C plants are on average around 14‰ (parts per thousand) lighter than the atmospheric ratio, while C plants are about 28‰ lighter. The of CAM plants depends on the percentage of carbon fixed at night relative to what is fixed in the day, being closer to C plants if they fix most carbon in the day and closer to C plants if they fix all their carbon at night.\n\nIt is troublesome procuring original fossil material in sufficient quantity to analyse the grass itself, but fortunately there is a good proxy: horses. Horses were globally widespread in the period of interest, and browsed almost exclusively on grasses. There's an old phrase in isotope palæontology, \"you are what you eat (plus a little bit)\" – this refers to the fact that organisms reflect the isotopic composition of whatever they eat, plus a small adjustment factor. There is a good record of horse teeth throughout the globe, and their has been measured. The record shows a sharp negative inflection around , during the Messinian, and this is interpreted as the rise of C plants on a global scale.\n\nWhile C enhances the efficiency of RuBisCO, the concentration of carbon is highly energy intensive. This means that C plants only have an advantage over C organisms in certain conditions: namely, high temperatures and low rainfall. C plants also need high levels of sunlight to thrive. Models suggest that, without wildfires removing shade-casting trees and shrubs, there would be no space for C plants. But, wildfires have occurred for 400 million years – why did C take so long to arise, and then appear independently so many times? The Carboniferous period (~) had notoriously high oxygen levels – almost enough to allow spontaneous combustion – and very low , but there is no C isotopic signature to be found. And there doesn't seem to be a sudden trigger for the Miocene rise.\n\nDuring the Miocene, the atmosphere and climate were relatively stable. If anything, increased gradually from before settling down to concentrations similar to the Holocene. This suggests that it did not have a key role in invoking C evolution. Grasses themselves (the group which would give rise to the most occurrences of C) had probably been around for 60 million years or more, so had had plenty of time to evolve C, which, in any case, is present in a diverse range of groups and thus evolved independently. There is a strong signal of climate change in South Asia; increasing aridity – hence increasing fire frequency and intensity – may have led to an increase in the importance of grasslands. However, this is difficult to reconcile with the North American record. It is possible that the signal is entirely biological, forced by the fire- (and elephant?)- driven acceleration of grass evolution – which, both by increasing weathering and incorporating more carbon into sediments, reduced atmospheric levels. Finally, there is evidence that the onset of C from is a biased signal, which only holds true for North America, from where most samples originate; emerging evidence suggests that grasslands evolved to a dominant state at least 15Ma earlier in South America.\n\n", "id": "41468418", "title": "Evolution of photosynthesis"}
{"url": "https://en.wikipedia.org/wiki?curid=41485902", "text": "Evolutionary Synthetic Biology\n\nIn one direction, Evolutionary Synthetic Biology attempts to develop and validate models of DNA and protein evolution. Closing the circle, Evolutionary Synthetic Biology develops novel biomolecules for medicine and industry by exploiting evolutionary platforms.\n\nSee also:\nSynthetic Biology,\nEvolutionary Biology,\nDirected Evolution,\nAncestral sequence reconstruction\n\n", "id": "41485902", "title": "Evolutionary Synthetic Biology"}
{"url": "https://en.wikipedia.org/wiki?curid=41396994", "text": "Illegitimate receiver\n\nAn illegitimate receiver is an organism that intercepts another organism's signal, despite not being the signaler's intended target. In animal communication, a signal is any transfer of information from one organism to another, including visual, olfactory (e.g. pheromones), and auditory signals. If the illegitimate receiver's interception of the signal is a means of finding prey, the interception is typically a fitness detriment (meaning that it reduces survival or reproductive ability) to either the signaler or the organism meant to legitimately receive the signal, but it is a fitness advantage to the illegitimate receiver because it provides energy in the form of food. Illegitimate receivers can have important effects on the evolution of communication behaviors.\n\nIllegitimate receivers can benefit by intercepting signals to locate prey, or, if they are parasites or parasitoids, by intercepting signals to locate host organisms. In addition to locating prey by intercepting signals given by the prey organism, some animals use the signals of other predators to find carcasses that they can scavenge off of. Other organisms benefit by illegitimately receiving the signals of rivals and using this information to improve their own chances of winning in competition for resources, including mates.\n\nIllegitimate receivers can experience fitness costs if they respond to signals given off by illegitimate signalers, which are organisms that utilize deceptive signals to reduce receiver fitness, typically by preying on or parasitizing the organism that responds. Illegitimate receivers may also experience fitness costs if intercepting signals not intended for them reduces their likelihood of receiving signals that are directed at them, such as the mating calls of members of their own species or the warning calls of rivals.\n\nRedeye bass (\"Micropterus coosae\") and midland water snakes (\"Nerodia sipedon pleuralis\") respond to acoustic and visual signals in male tricolor shiners (\"Cyprinella trichroistia\") when detecting prey.\n\nMale Great Bowerbirds sometimes steal nest decorations, which are intended to attract mates, from their rivals and use these decorations in their own nests.\n\nMale túngara frogs (\"Physalaemus pustulosus\") give off mating calls consisting of both \"whines\" and \"chucks,\" with songs that contain chucks favored by females over those containing only whines. However, the fringe-lipped bat (\"Trachops cirrhosus\"), a natural predator of the túngara frog, is an illegitimate receiver of these songs and uses them to locate its prey. These bats are especially attracted to frog songs containing the chuck element, and so túngara frogs rarely incorporate chucks into their calls. In fact, the frogs have been shown to typically only incorporate the chuck element into their songs when they are congregated in large groups, as this reduces the chance of being eaten via the dilution effect.\nOn the island of Kauai, females of a species of parasitoid fly, \"Ormia ochracea\", respond to the stridulation mating calls of male field crickets (\"Teleogryllus oceanicus\") by locating the crickets and then laying their lethal larvae on them. In response to this, male field crickets have evolved via a \"flatwing\" mutation to no longer produce mating songs.\n\nAnother example of evolution in response to illegitimate receivers is that of the Great tits. These European songbirds have evolved to use \"seet\" calls in order to avoid having their signals illegitimately received by hawks or owls. Great tits use two different calls to warn one another of nearby predators: When the predators are flying nearby, the great tits use a \"seet\" alarm call; however, when the predators are perched nearby, the great tits use a mobbing call. The mobbing call is at a much higher frequency than the seet call, allowing for the great tits to recruit nearby individuals of their species when mobbing perched predators in an attempt to chase them out of the area. Meanwhile, the lower frequency of the seet call allows the great tits to warn one another of the danger without attracting the unwanted attention of the mobile hawk or owl. Louder calls are also more frequently exhibited in birds inhabiting more protected habitats, while softer seet calls are more common in unprotected, open areas.\n\nIllegitimate signalers utilize deceptive signals to reduce the receiver's fitness while increasing their own. Examples include the case of the \"Photinus\" and \"Photuris\" fireflies, as well as aggressive mimicry.\n\nHonest signals are signals used by one organism to convey true information to another individual. An example is the begging calls of bird chicks.\n\nAnimal communication includes any transfer of information between individuals, including illegitimate receiving and signaling.\n", "id": "41396994", "title": "Illegitimate receiver"}
{"url": "https://en.wikipedia.org/wiki?curid=822224", "text": "Sperm competition\n\nSperm competition is the competitive process between spermatozoa of two or more different males to fertilize the same egg during sexual reproduction. Competition can occur when females have multiple potential mating partners. Greater choice and variety of mates increases a female's chance to produce more viable offspring. However, multiple mates for a female means an individual male has decreased chances of producing offspring.\nSperm competition is an evolutionary pressure on males, and has led to the development of adaptations to increase males' chance of reproductive success. Sperm competition results in a sexual conflict of interest between males and females. Males have evolved several defensive tactics including: mate-guarding, mating plugs, and releasing toxic seminal substances to reduce female re-mating tendencies to cope with sperm competition. Offensive tactics of sperm competition involve direct interference by one male on the reproductive success of another male, for instance by physically removing another male's sperm prior to mating with a female. For an example, see \"Gryllus bimaculatus\".\n\nSperm competition is often compared to having tickets in a raffle; a male has a better chance of winning (i.e. fathering offspring) the more tickets he has (i.e. the more sperm he inseminates a female with). However, sperm are not free to produce, and as such males are predicted to produce sperm of a size and number that will maximize their success in sperm competition. By making many spermatozoa, males can buy more \"raffle tickets\", and it is thought that selection for numerous sperm has contributed to the evolution of anisogamy with very small sperm (because of the energy trade-off between sperm size and number). Alternatively, a male may evolve faster sperm to enable his sperm to reach and fertilize the female's ovum first. Dozens of adaptations have been documented in males that help them succeed in sperm competition.\n\nMate-guarding is a defensive behavioral trait that occurs in response to sperm competition; males try to prevent other males from approaching the female (and/or vice versa) thus preventing their mate from engaging in further copulations. Precopulatory and postcopulatory mate-guarding occurs in birds, lizards, insects and primates. Mate-guarding also exists in the fish species \"Neolamprologus pulcher\", as some males try to \"sneak\" matings with females in the territory of other males. In these instances the males guard their female by keeping them in close enough proximity so that if an opponent male shows up in his territory he will be able to fight off the rival male which will prevent the female from engaging in extra-pair copulation with the rival male.\n\nOrganisms with polygynous mating systems are controlled by one dominant male. In this type of mating system, the male is able to mate with more than one female in a community. The dominant males will reign over the community until another suitor steps up and overthrows him. The current dominant male will defend his title as the dominant male and he will also be defending the females he mates with and the offspring he sires. The elephant seal falls into this category since he can participate in bloody violent matches in order to protect his community and defend his title as the alpha male. If the alpha male is somehow overthrown by the new comer, his children will most likely be killed and the new alpha male will start over with the females in the group so that his lineage can be passed on.\n\nStrategic mate-guarding occurs when the male only guards the female during her fertile periods. This strategy can be more effective because it may allow the male to engage in both extra-pair paternity and within-pair paternity. This is also because it is energetically efficient for the male to guard his mate at this time. There is a lot of energy that is expended when a male is guarding his mate. For instance, in polygynous mate- guarding systems, the energetic costs of males is defending their title as alpha male of their community. Fighting is very costly in regards to the amount of energy used to guard their mate. These bouts can happen more than once which takes a toll on the physical well- being of the male. Another cost of mate-guarding in this type of mating system is the potential increase of the spread of disease. If one male has an STD he can pass that to the females that he's copulating with potentially resulting in a depletion of the harem. This would be an energetic cost towards both genders for the reason that instead of using the energy for reproduction, they are redirecting it towards ridding themselves of this illness. Some females also benefit from polygyny because extra pair copulations in females increase the genetic diversity with the community of that species. This occurs because the male is not able to watch over all of the females and some will become promiscuous.\nEventually, the male will not have the proper nutrition which makes the male unable to produce sperm. For instance, male amphipods will deplete their reserves of glycogen and triglycerides only to have it replenished after the male is done guarding that mate. Also, if the amount of energy intake does not equal the energy expended, then this could be potentially fatal to the male. Males may even have to travel long distances during the breeding season in order to find a female which absolutely drain their energy supply. Studies were conducted to compare the cost of foraging of fish that migrate and animals that are residential. The studies concluded that fish that were residential had fuller stomachs containing higher quality of prey compared to their migrant counterparts With all of these energy costs that go along with guarding a mate, timing is crucial so that the male can use the minimal amount of energy. This is why it is more efficient for males to choose a mate during their fertile periods. Also, males will be more likely to guard their mate when there is a high density of males in the proximity. Sometimes organisms put in all this time and planning into courting a mate in order to copulate and she may not even be interested. There is a risk of cuckoldry of some sort since a rival male can successfully court the female that the male originally courting her could not do.\n\nHowever, there are benefits that are associated with mate- guarding. In a mating- guarding system, both parties, male and female, are able to directly and indirectly benefit from this. For instance, females can indirectly benefit from being protected by a mate. The females can appreciate a decrease in predation and harassment from other males while being able to observe her male counterpart. This will allow her to recognize particular traits that she finds ideal so that she'll be able to find another male that emulates those qualities. In polygynous relationships, the dominant male of the community benefits because he has the best fertilization success. Communities can include 30 up to 100 females and, compared to the other males, will greatly increase his chances of mating success.\n\nMales who have successfully courted a potential mate will attempt to keep them out of sight of other males before copulation. One way organisms accomplish this is to move the female to a new location. Certain butterflies, after enticing the female, will pick her up and fly her away from the vicinity of potential males. In other insects, the males will release a pheromone in order to make their mate unattractive to other males or the pheromone masks her scent completely. Certain crickets will participate in a loud courtship until the female accepts his gesture and then it suddenly becomes silent. Some insects, prior to mating, will assume tandem positions to their mate or position themselves in a way to prevent other males from attempting to mate with that female. The male checkerspot butterfly has developed a clever method in order to attract and guard a mate. He will situate himself near an area that possesses valuable resources that the female needs. He will then drive away any males that come near and this will greatly increase his chances of copulation with any female that comes to that area.\n\nIn post- copulatory mate- guarding males are trying to prevent other males from mating with the female that they have mated with already. For example, male millipedes in Costa Rica will ride on the back of their mate letting the other males know that she's taken. Japanese beetles will assume a tandem position to the female after copulation. This can last up to several hours allowing him to ward off any rival males giving his sperm a high chance to fertilize that female's egg. These, and other, types of methods have the male playing defense by protecting his mate. Elephant seals are known to engage in bloody battles in order to retain their title has dominant male so that they are able to mate with all the females in their community.\n\nCopulatory plugs are frequently observed in insects, reptiles, some mammals, and spiders. Copulatory plugs are inserted immediately after a male copulates with a female, which reduce the possibility of fertilization by subsequent copulations from another male, by physically blocking the transfer of sperm. The Indian mealmoth (\"Plodia interpunctella\") are restricted in engaging in further mating activities because the spermatacore serves as a copulatory plug immediately after copulation. Bumblebee mating plugs, in addition to providing a physical barrier to further copulations, contain linoleic acid, which reduces re-mating tendencies of females.\n\nSimilarly, \"Drosophila melanogaster\" males release toxic seminal fluids, known as ACPs (accessory gland proteins), from their accessory glands to impede the female from participating in future copulations. These substances act as an anti-aphrodisiac causing a dejection of subsequent copulations, and also stimulate ovulation and oogenesis. Seminal proteins can have a strong influence on reproduction, sufficient to manipulate female behavior and physiology.\n\nAnother strategy, known as sperm partitioning, occurs when males conserve their limited supply of sperm by reducing the quantity of sperm ejected. In \"Drosophila\", ejaculation amount during sequential copulations is reduced; this results in half filled female sperm reserves following a single copulatory event, but allows the male to mate with a larger number of females without exhausting his supply of sperm. To facilitate sperm partitioning, some males have developed complex ways to store and deliver their sperm. In the blue headed wrasse, \"Thalassoma bifasciatum\", the sperm duct is sectioned into several small chambers that are surrounded by a muscle that allows the male to regulate how much sperm is released in one copulatory event.\n\nA strategy common among insects is for males to participate in prolonged copulations. By engaging in prolonged copulations a male has an increased opportunity to place more sperm within the female's reproductive tract and prevent the female from copulating with other males.\n\nIt has been found that some male mollies (\"Poecilia\") have developed deceptive social cues to combat sperm competition. Focal males will direct sexual attention toward typically non-preferred females when an audience of other males is present. This encourages the males that are watching to attempt to mate with the non-preferred female. This is done in an attempt to decrease mating attempts with the female that the focal male prefers, hence decreasing sperm competition.\n\nOffensive adaptation behavior differs from defensive behavior because it involves an attempt to ruin the chances of another male's opportunity in succeeding in copulation by engaging in an act that tries to terminate the fertilization success of the previous male. This offensive behavior is facilitated by the presence of certain traits, which are called armaments. An example of an armament are antlers. Further, the presence of an offensive trait sometimes serves as a status signal. The mere display of an armament can suffice to drive away the competition without engaging in a fight, hence saving energy. \nA male on the offensive side of mate-guarding may terminate the guarding male's chances at a successful insemination by brawling with the guarding male to gain access to the female. In \"Drosophila\", males release seminal fluids that contain additional toxins like pheromones and modified enzymes that are secreted by their accessory glands intended to destroy the sperm that have already made their way into the female's reproductive tract from a recent copulation. Based on the \"last male precedence\" idea, some males can remove sperm from previous males by ejaculating new sperm into the female; hindering successful insemination opportunities of the previous male.\n\nThe \"good sperm hypothesis\" is very common in polyandrous mating systems. The \"good sperm hypothesis\" suggests that a male's genetic makeup will determine the level of his competitiveness in sperm competition. When a male has \"good sperm\" he is able to father more viable offspring than males that do not have the \"good sperm\" genes. Females may select males that have these superior \"good sperm\" genes because it means that their offspring will be more viable and will inherit the \"good sperm\" genes which will increase their fitness levels when their sperm competes.\n\nStudies show that there is more to determining the competitiveness of the sperm in sperm competition in addition to a male's genetic makeup. A male's dietary intake will also affect sperm competition. An adequate diet consisting of increased amounts of diet and sometimes more specific ratio in certain species will optimize sperm number and fertility. Amounts of protein and carbohydrate intake were tested for its effects on sperm production and quality in adult fruit flies (Diptera: Tephritidae). Studies showed these flies need to constantly ingest carbohydrates and water to survive, but protein is also required to attain sexual maturity. In addition, The Mediterranean fruit fly, male diet has been shown to affect male mating success, copula duration, sperm transfer, and male participation in leks. These all require a good diet with nutrients for proper gamete production as well as energy for activities, which includes participation in leks. \n\nIn addition, protein and carbohydrate amounts were shown to have an effect on sperm production and fertility in the speckled cockroach. Holidic diets were used which allowed for specific protein and carbohydrate measurements to be taken, giving it credibility. A direct correlation was seen in sperm number and overall of food intake. More specifically, optimal sperm production was measured at a 1:2 protein to carbohydrate ratio. Sperm fertility was best at a similar protein to carbohydrate ratio of 1:2. This close alignment largely factors in determining male fertility in Nauphoeta cinerea. Surprisingly, sperm viability was not affected by any change in diet or diet ratios. It's hypothesized that sperm viability is more affected by the genetic makeup, like in the \"good sperm hypothesis\". These ratios and results are not consistent with many other species and even conflict with some. It seems there can't be any conclusions on what type of diet is needed to positively influence sperm competition but rather understand that different diets do play a role in determining sperm competition in mate choice.\n\nOne evolutionary response to sperm competition is the variety in penis morphology of many species. For example, the shape of the human penis may have been selectively shaped by sperm competition. The human penis may have been selected to displace seminal fluids implanted in the female reproductive tract by a rival male. Specifically, the shape of the coronal ridge may promote displacement of seminal fluid from a previous mating via thrusting action during sexual intercourse. A 2003 study by Gordon G. Gallup and colleagues concluded that one evolutionary purpose of the thrusting motion characteristic of intense intercourse is for the penis to “upsuck” another man's semen before depositing its own.\n\nEvolution to increase ejaculate volume in the presence of sperm competition has a consequence on testis size. Large testes can produce more sperm required for larger ejaculates, and can be found across the animal kingdom when sperm competition occurs. Males with larger testes have been documented to achieve higher reproductive success rates than males with smaller testes in male yellow pine chipmunks. In chichlid fish, it has been found that increased sperm competition can lead to evolved larger sperm numbers, sperm cell sizes, and sperm swimming speeds.\n\nIn some insects and spiders, for instance \"Nephila fenestrate\", the male copulatory organ breaks off or tears off at the end of copulation and remains within the female to serve as a copulatory plug. This broken genitalia is believed to be an evolutionary response to sperm competition. This damage to the male genitalia means that these males can only mate once.\n\nFemale factors can influence the result of sperm competition through a process known as \"sperm choice\". Proteins present in the female reproductive tract or on the surface of the ovum may influence which sperm succeeds in fertilizing the egg. During sperm choice females are able to discriminate and differentially use the sperm from different males. One instance where this is known to occur is inbreeding; females will preferentially use the sperm from a more distantly related male than a close relative.\n\nInbreeding ordinarily has negative fitness consequences (inbreeding depression), and as a result species have evolved mechanisms to avoid inbreeding. Inbreeding depression is considered to be due largely to the expression of homozygous deleterious recessive mutations. Outcrossing between unrelated individuals ordinarily leads to the masking of deleterious recessive mutations in progeny. \nNumerous inbreeding avoidance mechanisms operating prior to mating have been described. However, inbreeding avoidance mechanisms that operate subsequent to copulation are less well known. In guppies, a post-copulatory mechanism of inbreeding avoidance occurs based on competition between sperm of rival males for achieving fertilization. In competitions between sperm from an unrelated male and from a full sibling male, a significant bias in paternity towards the unrelated male was observed.\n\nIn vitro fertilization experiments in the mouse, provided evidence of sperm selection at the gametic level. When sperm of sibling and non-sibling males were mixed, a fertilization bias towards the sperm of the non-sibling males was observed. The results were interpreted as egg-driven sperm selection against related sperm.\n\nFemale fruit flies (\"Drosophila melanogaster\") were mated with males of four different degrees of genetic relatedness in competition experiments. Sperm competitive ability was negatively correlated with relatedness.\n\nFemale crickets (\"Teleogryllus oceanicus\") appear to use post-copulatory mechanisms to avoid producing inbred offspring. When mated to both a sibling and an unrelated male, females bias paternity towards the unrelated male.\n\nIt has been found that because of female choice (see sexual selection), morphology of sperm in many species occurs in many variations to accommodate or combat (see sexual conflict) the morphology and physiology of the female reproductive tract. However, it is difficult to understand the interplay between female and male reproductive shape and structure that occurs within the female reproductive tract \"after\" mating that allows for the competition of sperm. Polyandrous females mate with many male partners. Females of many species of arthropod, mollusk and other phyla have a specialized sperm-storage organ called the spermatheca in which the sperm of different males sometimes compete for increased reproductive success. Species of crickets, specifically Gryllus bimaculatus, are known to exhibit polyandrous sexual selection. Males will invest more in ejaculation when competitors are in the immediate environment of the female.\n\nEvidence exists that illustrates the ability of genetically similar spermatozoa to cooperate so as to ensure the survival of their counterparts thereby ensuring the implementation of their genotypes towards fertilization. Cooperation confers a competitive advantage by several means, some of these include incapacitation of other competing sperm and aggregation of genetically similar spermatozoa into structures that promote effective navigation of the female reproductive tract and hence improve fertilization ability. Such characteristics lead to morphological adaptations that suit the purposes of cooperative methods during competition. For example, spermatozoa possessed by the wood mouse (\"Apodemus sylvaticus\") possess an apical hook which is used to attach to other spermatozoa to form mobile trains that enhance motility through the female reproductive tract. Spermatozoa that fail to incorporate themselves into mobile trains are less likely to engage in fertilization. Other evidence suggests no link between sperm competition and sperm hook morphology.\n\nSelection to produce more sperm can also select for the evolution of larger testes. Relationships across species between the frequency of multiple mating by females and male testis size are well documented across many groups of animals. For example, among primates, female gorillas are relatively monogamous, so gorillas have smaller testes than humans, which in turn have smaller testes than the highly promiscuous bonobos. Male chimpanzees that live in a structured multi-male, multi-female community, have large testicles to produce more sperm, therefore giving him better odds to fertilize the female. Whereas the community of gorillas consist of one alpha male and two or three females, when the female gorillas are ready to mate, normally only the alpha male is their partner.\n\nRegarding sexual dimorphism among primates, humans falls into an intermediate group with moderate sex differences in body size but relatively large testes. This is a typical pattern of primates where several males and females live together in a group and the male faces an intermediate number of challenges from other males compared to exclusive polygyny and monogamy but frequent sperm competition.\n\nOther means of sperm competition could include improving the sperm itself or its packaging materials (spermatophore).\n\nThe male black-winged damselfly provides a striking example of an adaptation to sperm competition. Female black-winged damselflies are known to mate with several males over the span of only a few hours and therefore possess a receptacle known as a spermatheca which stores the sperm. During the process of mating the male damselfly will pump his abdomen up and down using his specially adapted penis which acts as a scrub brush to remove the sperm of another male. This method proves quite successful and the male damselfly has been known to remove 90-100 percent of the competing sperm.\nA similar strategy has been observed in the dunnock, a small bird. Before mating with the polyandrous female, the male dunnock pecks at the female's cloaca in order to peck out the sperm of the previous male suitor.\n\nA notion emerged in 1996 that in some species, including humans, a significant fraction of sperm specialize in a manner such that they cannot fertilize the egg but instead have the primary effect of stopping the sperm from other males from reaching the egg, e.g. by killing them with enzymes or by blocking their access. This type of sperm specialization became known popularly as \"kamikaze sperm\" or \"killer sperm\", but most follow-up studies to this popularized notion have failed to confirm the initial papers on the matter. While there is also currently little evidence of killer sperm in any non-human animals certain snails have an infertile sperm morph (\"parasperm\") that contains lysozymes, leading to speculation that they might be able to degrade a rivals' sperm.\n\nSperm competition has led to other adaptations such as larger ejaculates, prolonged copulation, deposition of a copulatory plug to prevent the female re-mating, or the application of pheromones that reduce the female's attractiveness.\nThe adaptation of sperm traits, such as length, viability and velocity might be constrained by the influence of cytoplasmic DNA (e.g. mitochondrial DNA); mitochondrial DNA is inherited from the mother only and it is thought that this could represent a constraint in the evolution of sperm.\n\n\n", "id": "822224", "title": "Sperm competition"}
{"url": "https://en.wikipedia.org/wiki?curid=41624922", "text": "Sequence space (evolution)\n\nIn evolutionary biology, sequence space is a way of representing all possible sequences (for a protein, gene or genome). The sequence space has one dimension per amino acid or nucleotide in the sequence leading to highly dimensional spaces.\n\nMost sequences in sequence space have no function, leaving relatively small regions that are populated by naturally occurring genes. Each protein sequence is adjacent to all other sequences that can be reached through a single mutation. It has been estimated that the whole functional protein sequence space has been explored by life on the Earth. Evolution can be visualised as the process of sampling nearby sequences in sequence space and moving to any with improved fitness over the current one. \n\nA sequence space is usually laid out as a grid. For protein sequence spaces, each residue in the protein is represented by a dimension with 20 possible positions along that axis corresponding to the possible amino acids. Hence there are 400 possible dipeptides arranged in a 20x20 space but that expands to 10 for even a small protein of 100 amino acids arranges in a space with 100 dimensions. Although such overwhelming multidimensionality cannot be visualised or represented diagrammatically, it provides a useful abstract model to think about the range of proteins and evolution from one sequence to another.\n\nThese highly multidimensional spaces can be compressed to 2 or 3 dimensions using principal component analysis. A fitness landscape is simply a sequence space with an extra vertical axis of fitness added for each sequence.\n\nDespite the diversity of protein superfamilies, sequence space is extremely sparsely populated by functional proteins. Most random protein sequences have no fold or function. Enzyme superfamilies, therefore, exist as tiny clusters of active proteins in a vast empty space of non-functional sequence.\n\nThe density of functional proteins in sequence space, and the proximity of different functions to one another is a key determinant in understanding evolvability. The degree of interpenetration of two neutral networks of different activities in sequence space will determine how easy it is to evolve from one activity to another. The more overlap between different activities in sequence space, the more cryptic variation for promiscuous activity will be.\n\nProtein sequence space has been compared to the \"Library of Babel\" a theoretical library containing all possible books that are 410 pages long. In the \"Library of Babel\", finding any book that made sense was impossible due to the sheer number and lack of order. The same would be true of protein sequences if it were not for natural selection, which has selected out only protein sequences that make sense. Additionally, each protein sequences is surrounded by a set of neighbours (point mutants) that are likely have at least some function.\n\n", "id": "41624922", "title": "Sequence space (evolution)"}
{"url": "https://en.wikipedia.org/wiki?curid=41625353", "text": "Neutral network (evolution)\n\nA neutral network is a set of genes all related by point mutations that have equivalent function or fitness. Each node represents a gene sequence and each line represents the mutation connecting two sequences. Neutral networks can be thought of as high, flat plateaus in a fitness landscape. During neutral evolution, genes can randomly move through neutral networks and traverse regions of sequence space which may have consequences for robustness and evolvability.\n\nNeutral networks exist in fitness landscapes since proteins are robust to mutations. This leads to extended networks of genes of equivalent function, linked by neutral mutations. Proteins are resistant to mutations because many sequences can fold into highly similar structural folds. A protein adopts a limited ensemble of native conformations because those conformers have lower energy than unfolded and mis-folded states (ΔΔG of folding). This is achieved by a distributed, internal network of cooperative interactions (hydrophobic, polar and covalent). Protein structural robustness results from few single mutations being sufficiently disruptive to compromise function. Proteins have also evolved to avoid aggregation as partially folded proteins can combine to form large, repeating, insoluble protein fibrils and masses. There is evidence that proteins show negative design features to reduce the exposure of aggregation-prone beta-sheet motifs in their structures.\nAdditionally, there is some evidence that the genetic code itself may be optimised such that most point mutations lead to similar amino acids (conservative). Together these factors create a distribution of fitness effects of mutations that contains a high proportion of neutral and nearly-neutral mutations.\n\nNeutral networks are a subset of the sequences in sequence space that have equivalent function, and so form a wide, flat plateau in a fitness landscape. Neutral evolution can therefore be visualised as a population diffusing from one set of sequence nodes, through the neutral network, to another cluster of sequence nodes. Since the majority of evolution is thought to be neutral, a large proportion of gene change is the movement though expansive neutral networks.\n\nThe more neutral neighbours a sequence has, the more robust to mutations it is since mutations are more likely to simply neutrally convert it into an equally functional sequence. Indeed, if there are large differences between the number of neutral neighbours of different sequences within a neutral network, the population is predicted to evolve towards these robust sequences. This is sometimes called circum-neutrality and represents the movement of populations away from cliffs in the fitness landscape.\n\nIn addition to in silico models, these processes are beginning to be confirmed by experimental evolution of cytochrome P450s and B-lactamase.\n\nInterest in the interplay between genetic drift and selection has been around since the 1930s when the shifting-balance theory proposed that in some situations, genetic drift could facilitate later adaptive evolution. Although the specifics of the theory were largely discredited, it drew attention to the possibility that drift could generate cryptic variation that, though neutral to current function, may affect selection for new functions (evolvability).\n\nBy definition, all genes in a neutral network have equivalent function, however some may exhibit promiscuous activities which could serve as starting points for adaptive evolution towards new functions. In terms of sequence space, current theories predict that if the neutral networks for two different activities overlap, a neutrally evolving population may diffuse to regions of the neutral network of the first activity that allow it to access the second. This would only be the case when the distance between activities is smaller than the distance that a neutrally evolving population can cover. The degree of interpenetration of the two networks will determine how common cryptic variation for the promiscuous activity is in sequence space.\n\nThe fact that neutral mutations were probably wide spread was proposed by Freese and Yoshida in 1965. Motoo Kimura later crystallized a theory of neutral evolution in 1968 with King and Jukes independently proposing a similar theory (1969). Kimura computed the rate of nucleotide substitutions in a population (i.e. the average time for one base pair replacement to occur within a genome) and found it to be ~1.8 years. Such a high rate would not be tolerated by any mammalian population according to Haldane's formula. He thus concluded that, in mammals, neutral (or nearly neutral) nucleotide substitution mutations of DNA must dominate. He computed that such mutations were occurring at the rate of roughly 0-5 per year per gamete.\n\nIn later years, a new paradigm emerged, that placed RNA as a precursor molecule to DNA. A primordial molecule principle was put forth as early as 1968 by Crick, and lead to what is now known as The RNA World Hypothesis. DNA is found, predominantly, as fully base paired double helices, while biological RNA is single stranded and often exhibits complex base-pairing interactions. These are due to its increased ability to form hydrogen bonds, a fact which stems from the existence of the extra hydroxyl group in the ribose sugar.\n\nIn the 1970s, Stein and M. Waterman laid the ground work for the combinatorics of RNA secondary structures. Waterman gave the first graph theoretic description of RNA secondary structures and their associated properties, and used them to produce an efficient minimum free energy (MFE) folding algorithm. An RNA secondary structure can be viewed as a diagram over N labeled vertices with its Watson-Crick base pairs represented as non-crossing arcs in the upper half plane. Therefore, a secondary structure is a scaffold having many sequences compatible with its implied base pairing constraints. Later, Smith and Waterman developed an algorithm that performed local sequence alignment. Another prediction algorithm for RNA secondary structure was given by Nussinov Nussinov's algorithm described the folding problem over a two letter alphabet as a planar graph optimization problem, where the quantity to be maximized is the number of matchings in the sequence string.\n\nCome the year 1980, Howell et al. computed a generating function of all foldings of a sequence while D. Sankoff (1985) described algorithms for alignment of finite sequences, the prediction of RNA secondary structures (folding), and the reconstruction of proto-sequences on a phylo-genetic tree. Later, Waterman and Temple (1986) produced a polynomial time dynamic programming (DP) algorithm for predicting general RNA secondary structure. while in the year 1990, John McCaskill presented a polynomial time DP algorithm for computing the full equilibrium partition function of an RNA secondary structure.\n\nM. Zuker, implemented algorithms for computation of MFE RNA secondary structures based on the work of Nussinov et al., Smith and Waterman and Studnicka, et al. Later L. Hofacker (et al., 1994), presented The Vienna RNA package, a software package that integrated MFE folding and the computation of the partition function as well as base pairing probabilities.\n\nPeter Schuster and W. Fontana (1994) shifted the focus towards sequence to structure maps (genotype-phenotype) . They used an inverse folding algorithm, to produce computational evidence that RNA sequences sharing the same structure are distributed randomly in sequence space. They observed that common structures can be reached from a random sequence by just a few mutations. These two facts lead them to conclude that the sequence space seemed to be percolated by neutral networks of nearest neighbor mutants that fold to the same structure.\n\nIn 1997, C. Reidys Stadler and Schuster laid the mathematical foundations for the study and modelling of neutral networks of RNA secondary structures. Using a random graph model they proved the existence of a threshold value for connectivity of random sub-graphs in a configuration space, parametrized by λ, the fraction of neutral neighbors. They showed that the networks are connected and percolate sequence space if the fraction of neutral nearest neighbors exceeds λ*, a threshold value. Below this threshold the networks are partitioned into a largest giant component and several smaller ones. Key results of this analysis where concerned with threshold functions for density and connectivity for neutral networks as well as Schuster's shape space conjecture.\n\n", "id": "41625353", "title": "Neutral network (evolution)"}
{"url": "https://en.wikipedia.org/wiki?curid=12843792", "text": "Genetic pollution\n\nGenetic pollution is a controversial term for uncontrolled gene flow into wild populations. This gene flow is undesirable according to some environmentalists and conservationists, including groups such as Greenpeace, TRAFFIC, and GeneWatch UK.\n\nThe Food and Agriculture Organization of the United Nations defines genetic pollution as:\nSome conservation biologists and conservationists have used genetic pollution for a number of years as a term to describe gene flow from a domestic, feral, non-native or invasive subspecies to a wild indigenous population. The term is of late being associated with the gene flow from a genetically engineered (GE) organism to a non GE organism, frequently by those who consider such gene flow detrimental.\n\nWhether the term 'genetic pollution' and similar phrases such as \"genetic deterioration\", \"genetic swamping\", \"genetic takeover\", and \"genetic aggression\", are an appropriate scientific description of the biology of invasive species is debated. Rhymer and Simberloff argue that these types of terms:\nThey recommend that gene flow from invasive species be termed genetic mixing since:\nEnvironmentalists such as Patrick Moore, an ex-member and cofounder of Greenpeace, questions if the term genetic pollution is more political than scientific. The term is considered to arouse emotional feelings towards the subject matter. In an interview he comments:\n\nGenetic pollution may come from invasive species, genetically modified organisms, deliberately introduced species, and mutated organisms.\n\nConservation biologists and conservationists have, for a number of years, used the term to describe gene flow from domestic, feral, and non-native species into wild indigenous species, which they consider undesirable. For example, TRAFFIC is the international wildlife trade monitoring network that works to limit trade in wild plants and animals so that it is not a threat to conservationist goals. They promote awareness of the effects of introduced invasive species that may \"\"hybridize with native species, causing genetic pollution\"\". Furthermore, the Joint Nature Conservation Committee, the statutory adviser to the UK government, has stated that invasive species \"will alter the genetic pool (a process called \"genetic pollution\"), which is an irreversible change.\" Invasive species can invade both large and small native populations and have a profound effect. Upon invasion, invasive species interbreed with native species to form sterile or more evolutionarily fit hybrids that can outcompete the native populations. Invasive species can cause extinctions of small populations on islands that are particularly vulnerable due to their smaller amounts of genetic diversity. In these populations, local adaptations can be disrupted by the introduction of new genes that may not be as suitable for the small island environments. For example, the \"Cercocarpus traskiae\" of the Catalina Island off the coast of California has faced near extinction with only a single population remaining due to the hybridization of its offspring with \"Cercocarpus betuloides.\"\n\nIn the fields of agriculture, agroforestry and animal husbandry, \"genetic pollution\" is being used to describe gene flows between GE species and wild relatives.\nAn early use of the term \"\"genetic pollution\"\" in this later sense appears in a wide-ranging review of the potential ecological effects of genetic engineering in The Ecologist magazine in July 1989. It was also popularized by environmentalist Jeremy Rifkin in his 1998 book \"The Biotech Century\". While intentional crossbreeding between two genetically distinct varieties is described as hybridization with the subsequent introgression of genes, Rifkin, who had played a leading role in the ethical debate for over a decade before, used genetic pollution to describe what he considered to be problems that might occur due the unintentional process of (modernly) genetically modified organisms (GMOs) dispersing their genes into the natural environment by breeding with wild plants or animals.\n\nSince 2005 there has existed a GM Contamination Register, launched for GeneWatch UK and Greenpeace International that records all incidents of intentional or accidental release of organisms genetically modified using modern techniques.\n\nNot all genetically engineered organisms cause genetic pollution. Genetic engineering has a variety of uses and is specifically defined as a direct manipulation of the genome of an organism. Genetic pollution can occur in response to the introduction of a species that is not native to a particular environment, and genetically engineered organisms are examples of individuals that could cause genetic pollution following introduction. Due to these risks, studies have been done in order to assess the risks of genetic pollution associated with organisms that have been genetically engineered:\n\n\nIntroduced species are species living outside its native range due to the introduction by human beings. The introduction may or may not be intentional. The impact of introduced species is variable and has the potential to cause harm to the native species of its new habitat. \n\nMutations within organisms can be executed through the process of exposing the organism to chemicals or radiation in order to generate mutations. This has been done in plants in order to create mutants that have a desired trait. These mutants can then be bred with other mutants or individuals that are not mutated in order to maintain the mutant trait. However, similar to the risks associated with introducing individuals to a certain environment, the variation created by mutated individuals could have a negative impact on native populations as well.\n\n", "id": "12843792", "title": "Genetic pollution"}
{"url": "https://en.wikipedia.org/wiki?curid=3585292", "text": "Inheritance of acquired characteristics\n\nThe inheritance of acquired characteristics is a hypothesis that physiological changes acquired over the life of an organism (such as the enlargement of a muscle through repeated use) may be transmitted to offspring. The hypothesis is often called \"Lamarckism\", as it has misleadingly been equated with the evolutionary theory of the French naturalist Jean-Baptiste Lamarck (1744-1829).\n\nThe idea was proposed in ancient times by Hippocrates and Aristotle, and was commonly accepted near to Lamarck's time. Erasmus Darwin had described the inheritance of acquired characters in his \"Zoonomia\", 1794. According to historian of science Conway Zirkle:\n\nLamarck was neither the first nor the most distinguished biologist to believe in the inheritance of acquired characters. He merely endorsed a belief which had been generally accepted for at least 2,200 years before his time and used it to explain how evolution could have taken place. The inheritance of acquired characters had been accepted previously by Hippocrates, Aristotle, Galen (?), Roger Bacon, Jerome Cardan, Levinus Lemnius, John Ray, Michael Adanson, Jo. Fried. Blumenbach and Erasmus Darwin among others.\n\nLamarck published his theory in 1809, the year Charles Darwin was born. He noticed several lines of descent by comparing current species with fossil forms. He noticed that the younger the fossils were, the more alike they were to modern species. Two ideas were incorporated in Lamarck’s theory. The first was the theory of use and disuse; the idea that body parts used more often become stronger and larger, while parts not used slowly waste away and disappear. The second idea was the inheritance of acquired characteristics theory, the concept that modifications that occur during an organism's lifetime are passed on to its offspring. His example was the giraffe. \nHe believed that the long neck of the giraffe resulted from the ancestors of giraffes stretching their necks longer and longer while trying to reach the highest branches of the trees. Comte de Buffon, before Lamarck, proposed ideas about evolution involving the concept, and even Charles Darwin, after Lamarck, developed his own theory of inheritance of acquired characters, pangenesis. The basic concept of inheritance of acquired characters was finally widely rejected in the early 20th century.\n\nIn the 1920s, Harvard University researcher William McDougall studied the abilities of rats to correctly solve mazes. His reports claimed that offspring of rats that had learned the maze were able to run it faster. In his data, the first rats would get it wrong 165 times before being able to run it perfectly each time, but after a few generations, it was down to 20. McDougall attributed this to some sort of Lamarckian evolutionary process. However McDougall's results have never been replicated by other experimenters, and have been criticised for having several methodological problems and poor record-keeping.\n\nDuring the rule of Joseph Stalin in the USSR in the 1930s, the theory of inheritance of acquired characteristics was one of the proposals put forth by Trofim Lysenko, president of the Soviet Academy of Agricultural Sciences. Lysenkoism was advanced primarily in service to Soviet agriculture. Lysenko's research contributed to widespread famines and was in the end mistaken about mendelian genetics.\n\nThe idea that germline cells contain information that passes to each generation unaffected by experience and independent of the somatic (body) cells, came to be referred to as the Weismann barrier, and is frequently quoted as putting a final end to theory of inheritance of acquired characteristics. Thus it was conventional wisdom for decades that the only real form of inheritance was hard inheritance, and any idea of soft inheritance was mistaken. \n\nWeismann conducted the experiment of removing the tails of 68 white mice, repeatedly over 5 generations, and reporting that no mice were born in consequence without a tail or even with a shorter tail. He stated that \"901 young were produced by five generations of artificially mutilated parents, and yet there was not a single example of a rudimentary tail or of any other abnormality in this organ.\"\n\nRecently, researchers have reexamined this concept in light of discoveries in epigenetics and transgenerational epigenetics. The study of Heijmans et al. (2008) studied people born during the Dutch famine of 1944. Adults who were conceived during the famine had distinct epigenetic marks that their siblings born before or after the famine did not. These marks reduced the production of insulin-like growth factor 2 (IGF2), affecting the children's growth. While transgenerational epigenetic inheritance could have occurred, the findings could also be explained by in utero modifications due to famine, rather than germline inheritance. Further, environmental stress in experimental mice that caused aggressive behavior in males caused the same behavior in their offspring, who had DNA methylation patterns changes for particular genes.\n\nThe mechanism of transgenerational epigenetic inheritance appears to involve long noncoding RNAs (lncRNAs), which are transcripts generally expressed from regions that are thought not to code for proteins. Some lncRNAs bind to transcripts from protein coding genes. Associated chromatin-remodeling proteins then modify local chromatin and DNA through mechanisms such as DNA methylation, suppressing gene expression. Kevin Morris's 2012 article in \"The Scientist\" discusses heritability of epigenetic changes in depth.\n\n\n\n\n", "id": "3585292", "title": "Inheritance of acquired characteristics"}
{"url": "https://en.wikipedia.org/wiki?curid=8407149", "text": "Mutant\n\nIn biology and especially genetics, a mutant is an organism or a new genetic character arising or resulting from an instance of mutation, which is an alteration of the DNA sequence of a gene or chromosome of an organism. The natural occurrence of genetic mutations is integral to the process of evolution. The study of mutants is an integral part of biology; by understanding the effect that a mutation in a gene has, it is possible to establish the normal function of that gene.\n\nAlthough not all mutations have a noticeable phenotypic effect, the common usage of the word \"mutant\" is generally a pejorative term only used for genetically or phenotypically noticeable mutations. Previously, people used the word \"sport\" (related to spurt) to refer to abnormal specimens. The scientific usage is broader, referring to any organism differing from the wild type.\n\nMutants should not be confused with organisms born with developmental abnormalities, which are caused by errors during morphogenesis. In a developmental abnormality, the DNA of the organism is unchanged and the abnormality cannot be passed on to progeny. Conjoined twins are the result of developmental abnormalities.\n\nChemicals that cause developmental abnormalities are called teratogens; these may also cause mutations, but their effect on development is not related to mutations. Chemicals that induce mutations are called mutagens. Most mutagens are also considered to be carcinogens.\n\n\n", "id": "8407149", "title": "Mutant"}
{"url": "https://en.wikipedia.org/wiki?curid=23084", "text": "Paleontology\n\nPaleontology or palaeontology () is the scientific study of life that existed prior to, and sometimes including, the start of the Holocene Epoch (roughly 11,700 years before present). It includes the study of fossils to determine organisms' evolution and interactions with each other and their environments (their paleoecology). Paleontological observations have been documented as far back as the 5th century BC. The science became established in the 18th century as a result of Georges Cuvier's work on comparative anatomy, and developed rapidly in the 19th century. The term itself originates from Greek παλαιός, \"palaios\", \"old, ancient\", ὄν, \"on\" (gen. \"ontos\"), \"being, creature\" and λόγος, \"logos\", \"speech, thought, study\".\n\nPaleontology lies on the border between biology and geology, but differs from archaeology in that it excludes the study of anatomically modern humans. It now uses techniques drawn from a wide range of sciences, including biochemistry, mathematics, and engineering. Use of all these techniques has enabled paleontologists to discover much of the evolutionary history of life, almost all the way back to when Earth became capable of supporting life, about 3.8 billion years ago. As knowledge has increased, paleontology has developed specialised sub-divisions, some of which focus on different types of fossil organisms while others study ecology and environmental history, such as ancient climates.\n\nBody fossils and trace fossils are the principal types of evidence about ancient life, and geochemical evidence has helped to decipher the evolution of life before there were organisms large enough to leave body fossils. Estimating the dates of these remains is essential but difficult: sometimes adjacent rock layers allow radiometric dating, which provides absolute dates that are accurate to within 0.5%, but more often paleontologists have to rely on relative dating by solving the \"jigsaw puzzles\" of biostratigraphy. Classifying ancient organisms is also difficult, as many do not fit well into the Linnaean taxonomy that is commonly used for classifying living organisms, and paleontologists more often use cladistics to draw up evolutionary \"family trees\". The final quarter of the 20th century saw the development of molecular phylogenetics, which investigates how closely organisms are related by measuring how similar the DNA is in their genomes. Molecular phylogenetics has also been used to estimate the dates when species diverged, but there is controversy about the reliability of the molecular clock on which such estimates depend.\n\nThe simplest definition is \"the study of ancient life\". Paleontology seeks information about several aspects of past organisms: \"their identity and origin, their environment and evolution, and what they can tell us about the Earth's organic and inorganic past\".\n\nPaleontology is one of the historical sciences, along with archaeology, geology, astronomy, cosmology, philology and history itself. This means that it aims to describe phenomena of the past and reconstruct their causes. Hence it has three main elements: description of the phenomena; developing a general theory about the causes of various types of change; and applying those theories to specific facts.\nWhen trying to explain past phenomena, paleontologists and other historical scientists often construct a set of hypotheses about the causes and then look for a \"smoking gun\", a piece of evidence that indicates that one hypothesis is a better explanation than others. Sometimes the smoking gun is discovered by a fortunate accident during other research. For example, the discovery by Luis Alvarez and Walter Alvarez of an iridium-rich layer at the Cretaceous–Tertiary boundary made asteroid impact and volcanism the most favored explanations for the Cretaceous–Paleogene extinction event.\n\nThe other main type of science is experimental science, which is often said to work by conducting experiments to \"disprove\" hypotheses about the workings and causes of natural phenomena – note that this approach cannot confirm a hypothesis is correct, since some later experiment may disprove it. However, when confronted with totally unexpected phenomena, such as the first evidence for invisible radiation, experimental scientists often use the same approach as historical scientists: construct a set of hypotheses about the causes and then look for a \"smoking gun\".\n\nPaleontology lies on the boundary between biology and geology since paleontology focuses on the record of past life but its main source of evidence is fossils, which are found in rocks. For historical reasons paleontology is part of the geology departments of many universities, because in the 19th century and early 20th century geology departments found paleontological evidence important for estimating the ages of rocks while biology departments showed little interest.\n\nPaleontology also has some overlap with archaeology, which primarily works with objects made by humans and with human remains, while paleontologists are interested in the characteristics and evolution of humans as organisms. When dealing with evidence about humans, archaeologists and paleontologists may work together – for example paleontologists might identify animal or plant fossils around an archaeological site, to discover what the people who lived there ate; or they might analyze the climate at the time when the site was inhabited by humans.\n\nIn addition paleontology often uses techniques derived from other sciences, including biology, osteology, ecology, chemistry, physics and mathematics. For example, geochemical signatures from rocks may help to discover when life first arose on Earth, and analyses of carbon isotope ratios may help to identify climate changes and even to explain major transitions such as the Permian–Triassic extinction event. A relatively recent discipline, molecular phylogenetics, often helps by using comparisons of different modern organisms' DNA and RNA to re-construct evolutionary \"family trees\"; it has also been used to estimate the dates of important evolutionary developments, although this approach is controversial because of doubts about the reliability of the \"molecular clock\". Techniques developed in engineering have been used to analyse how ancient organisms might have worked, for example how fast \"Tyrannosaurus\" could move and how powerful its bite was. It is relatively commonplace to study fossils using X-ray microtomography A combination of paleontology, biology, and archaeology, paleoneurobiology is the study of endocranial casts (or endocasts) of species related to humans to learn about the evolution of human brains.\n\nPaleontology even contributes to astrobiology, the investigation of possible life on other planets, by developing models of how life may have arisen and by providing techniques for detecting evidence of life.\n\nAs knowledge has increased, paleontology has developed specialised subdivisions. Vertebrate paleontology concentrates on fossils of vertebrates, from the earliest fish to the immediate ancestors of modern mammals. Invertebrate paleontology deals with fossils of invertebrates such as molluscs, arthropods, annelid worms and echinoderms. Paleobotany focuses on the study of fossil plants, but traditionally includes the study of fossil algae and fungi. Palynology, the study of pollen and spores produced by land plants and protists, straddles the border between paleontology and botany, as it deals with both living and fossil organisms. Micropaleontology deals with all microscopic fossil organisms, regardless of the group to which they belong.\n\nInstead of focusing on individual organisms, paleoecology examines the interactions between different organisms, such as their places in food chains, and the two-way interaction between organisms and their environment.  One example is the development of oxygenic photosynthesis by bacteria, which hugely increased the productivity and diversity of ecosystems. This also caused the oxygenation of the atmosphere. Together, these were a prerequisite for the evolution of the most complex eukaryotic cells, from which all multicellular organisms are built.\n\nPaleoclimatology, although sometimes treated as part of paleoecology, focuses more on the history of Earth's climate and the mechanisms that have changed it – which have sometimes included evolutionary developments, for example the rapid expansion of land plants in the Devonian period removed more carbon dioxide from the atmosphere, reducing the greenhouse effect and thus helping to cause an ice age in the Carboniferous period.\n\nBiostratigraphy, the use of fossils to work out the chronological order in which rocks were formed, is useful to both paleontologists and geologists. Biogeography studies the spatial distribution of organisms, and is also linked to geology, which explains how Earth's geography has changed over time.\n\nFossils of organisms' bodies are usually the most informative type of evidence. The most common types are wood, bones, and shells. Fossilisation is a rare event, and most fossils are destroyed by erosion or metamorphism before they can be observed. Hence the fossil record is very incomplete, increasingly so further back in time. Despite this, it is often adequate to illustrate the broader patterns of life's history. There are also biases in the fossil record: different environments are more favorable to the preservation of different types of organism or parts of organisms. Further, only the parts of organisms that were already mineralised are usually preserved, such as the shells of molluscs. Since most animal species are soft-bodied, they decay before they can become fossilised. As a result, although there are 30-plus phyla of living animals, two-thirds have never been found as fossils.\n\nOccasionally, unusual environments may preserve soft tissues. These lagerstätten allow paleontologists to examine the internal anatomy of animals that in other sediments are represented only by shells, spines, claws, etc. – if they are preserved at all. However, even lagerstätten present an incomplete picture of life at the time. The majority of organisms living at the time are probably not represented because lagerstätten are restricted to a narrow range of environments, e.g. where soft-bodied organisms can be preserved very quickly by events such as mudslides; and the exceptional events that cause quick burial make it difficult to study the normal environments of the animals. The sparseness of the fossil record means that organisms are expected to exist long before and after they are found in the fossil record – this is known as the Signor-Lipps effect.\n\nTrace fossils consist mainly of tracks and burrows, but also include coprolites (fossil feces) and marks left by feeding. Trace fossils are particularly significant because they represent a data source that is not limited to animals with easily fossilised hard parts, and they reflect organisms' behaviours. Also many traces date from significantly earlier than the body fossils of animals that are thought to have been capable of making them. Whilst exact assignment of trace fossils to their makers is generally impossible, traces may for example provide the earliest physical evidence of the appearance of moderately complex animals (comparable to earthworms).\n\nGeochemical observations may help to deduce the global level of biological activity, or the affinity of certain fossils. For example, geochemical features of rocks may reveal when life first arose on Earth, and may provide evidence of the presence of eukaryotic cells, the type from which all multicellular organisms are built. Analyses of carbon isotope ratios may help to explain major transitions such as the Permian–Triassic extinction event.\n\nNaming groups of organisms in a way that is clear and widely agreed is important, as some disputes in paleontology have been based just on misunderstandings over names. Linnaean taxonomy is commonly used for classifying living organisms, but runs into difficulties when dealing with newly discovered organisms that are significantly different from known ones. For example: it is hard to decide at what level to place a new higher-level grouping, e.g. genus or family or order; this is important since the Linnaean rules for naming groups are tied to their levels, and hence if a group is moved to a different level it must be renamed.\n\nPaleontologists generally use approaches based on cladistics, a technique for working out the evolutionary \"family tree\" of a set of organisms. It works by the logic that, if groups B and C have more similarities to each other than either has to group A, then B and C are more closely related to each other than either is to A. Characters that are compared may be anatomical, such as the presence of a notochord, or molecular, by comparing sequences of DNA or proteins. The result of a successful analysis is a hierarchy of clades – groups that share a common ancestor. Ideally the \"family tree\" has only two branches leading from each node (\"junction\"), but sometimes there is too little information to achieve this and paleontologists have to make do with junctions that have several branches. The cladistic technique is sometimes fallible, as some features, such as wings or camera eyes, evolved more than once, convergently – this must be taken into account in analyses.\n\nEvolutionary developmental biology, commonly abbreviated to \"Evo Devo\", also helps paleontologists to produce \"family trees\", and understand fossils. For example, the embryological development of some modern brachiopods suggests that brachiopods may be descendants of the halkieriids, which became extinct in the Cambrian period.\nPaleontology seeks to map out how living things have changed through time. A substantial hurdle to this aim is the difficulty of working out how old fossils are. Beds that preserve fossils typically lack the radioactive elements needed for radiometric dating. This technique is our only means of giving rocks greater than about 50 million years old an absolute age, and can be accurate to within 0.5% or better. Although radiometric dating requires very careful laboratory work, its basic principle is simple: the rates at which various radioactive elements decay are known, and so the ratio of the radioactive element to the element into which it decays shows how long ago the radioactive element was incorporated into the rock. Radioactive elements are common only in rocks with a volcanic origin, and so the only fossil-bearing rocks that can be dated radiometrically are a few volcanic ash layers.\n\nConsequently, paleontologists must usually rely on stratigraphy to date fossils. Stratigraphy is the science of deciphering the \"layer-cake\" that is the sedimentary record, and has been compared to a jigsaw puzzle. Rocks normally form relatively horizontal layers, with each layer younger than the one underneath it. If a fossil is found between two layers whose ages are known, the fossil's age must lie between the two known ages. Because rock sequences are not continuous, but may be broken up by faults or periods of erosion, it is very difficult to match up rock beds that are not directly next to one another. However, fossils of species that survived for a relatively short time can be used to link up isolated rocks: this technique is called \"biostratigraphy\". For instance, the conodont \"Eoplacognathus pseudoplanus\" has a short range in the Middle Ordovician period. If rocks of unknown age are found to have traces of \"E. pseudoplanus\", they must have a mid-Ordovician age. Such index fossils must be distinctive, be globally distributed and have a short time range to be useful. However, misleading results are produced if the index fossils turn out to have longer fossil ranges than first thought. Stratigraphy and biostratigraphy can in general provide only relative dating (\"A\" was before \"B\"), which is often sufficient for studying evolution. However, this is difficult for some time periods, because of the problems involved in matching up rocks of the same age across different continents.\n\nFamily-tree relationships may also help to narrow down the date when lineages first appeared. For instance, if fossils of B or C date to X million years ago and the calculated \"family tree\" says A was an ancestor of B and C, then A must have evolved more than X million years ago.\n\nIt is also possible to estimate how long ago two living clades diverged – i.e. approximately how long ago their last common ancestor must have lived – by assuming that DNA mutations accumulate at a constant rate. These \"molecular clocks\", however, are fallible, and provide only a very approximate timing: for example, they are not sufficiently precise and reliable for estimating when the groups that feature in the Cambrian explosion first evolved, and estimates produced by different techniques may vary by a factor of two.\n\nThe evolutionary history of life stretches back to over , possibly as far as . Earth formed about and, after a collision that formed the Moon about 40 million years later, may have cooled quickly enough to have oceans and an atmosphere about . However, there is evidence on the Moon of a Late Heavy Bombardment from . If, as seems likely, such a bombardment struck Earth at the same time, the first atmosphere and oceans may have been stripped away. The oldest clear evidence of life on Earth dates to , although there have been reports, often disputed, of fossil bacteria from and of geochemical evidence for the presence of life . Some scientists have proposed that life on Earth was \"seeded\" from elsewhere, but most research concentrates on various explanations of how life could have arisen independently on Earth.\n\nFor about 2,000 million years microbial mats, multi-layered colonies of different types of bacteria, were the dominant life on Earth. The evolution of oxygenic photosynthesis enabled them to play the major role in the oxygenation of the atmosphere from about . This change in the atmosphere increased their effectiveness as nurseries of evolution. While eukaryotes, cells with complex internal structures, may have been present earlier, their evolution speeded up when they acquired the ability to transform oxygen from a poison to a powerful source of energy in their metabolism. This innovation may have come from primitive eukaryotes capturing oxygen-powered bacteria as endosymbionts and transforming them into organelles called mitochondria. The earliest evidence of complex eukaryotes with organelles such as mitochondria, dates from .\n\nMulticellular life is composed only of eukaryotic cells, and the earliest evidence for it is the Francevillian Group Fossils from , although specialisation of cells for different functions first appears between (a possible fungus) and (a probable red alga). Sexual reproduction may be a prerequisite for specialisation of cells, as an asexual multicellular organism might be at risk of being taken over by rogue cells that retain the ability to reproduce.\n\nThe earliest known animals are cnidarians from about , but these are so modern-looking that the earliest animals must have appeared before then. Early fossils of animals are rare because they did not develop mineralised hard parts that fossilise easily until about . The earliest modern-looking bilaterian animals appear in the Early Cambrian, along with several \"weird wonders\" that bear little obvious resemblance to any modern animals. There is a long-running debate about whether this Cambrian explosion was truly a very rapid period of evolutionary experimentation; alternative views are that modern-looking animals began evolving earlier but fossils of their precursors have not yet been found, or that the \"weird wonders\" are evolutionary \"aunts\" and \"cousins\" of modern groups. Vertebrates remained an obscure group until the first fish with jaws appeared in the Late Ordovician.\n\nThe spread of life from water to land required organisms to solve several problems, including protection against drying out and supporting themselves against gravity. The earliest evidence of land plants and land invertebrates date back to about and respectively. The lineage that produced land vertebrates evolved later but very rapidly between and ; recent discoveries have overturned earlier ideas about the history and driving forces behind their evolution. Land plants were so successful that they caused an ecological crisis in the Late Devonian, until the evolution and spread of fungi that could digest dead wood.\nDuring the Permian period synapsids, including the ancestors of mammals, may have dominated land environments, but the Permian–Triassic extinction event came very close to wiping out complex life. The extinctions were apparently fairly sudden, at least among vertebrates. During the slow recovery from this catastrophe a previously obscure group, archosaurs, became the most abundant and diverse terrestrial vertebrates. One archosaur group, the dinosaurs, were the dominant land vertebrates for the rest of the Mesozoic, and birds evolved from one group of dinosaurs. During this time mammals' ancestors survived only as small, mainly nocturnal insectivores, but this apparent set-back may have accelerated the development of mammalian traits such as endothermy and hair. After the Cretaceous–Paleogene extinction event killed off the non-avian dinosaurs – birds are the only surviving dinosaurs – mammals increased rapidly in size and diversity, and some took to the air and the sea.\nFossil evidence indicates that flowering plants appeared and rapidly diversified in the Early Cretaceous, between and . Their rapid rise to dominance of terrestrial ecosystems is thought to have been propelled by coevolution with pollinating insects. Social insects appeared around the same time and, although they account for only small parts of the insect \"family tree\", now form over 50% of the total mass of all insects.\n\nHumans evolved from a lineage of upright-walking apes whose earliest fossils date from over . Although early members of this lineage had chimp-sized brains, about 25% as big as modern humans', there are signs of a steady increase in brain size after about . There is a long-running debate about whether \"modern\" humans are descendants of a single small population in Africa, which then migrated all over the world less than 200,000 years ago and replaced previous hominine species, or arose worldwide at the same time as a result of interbreeding.\nLife on earth has suffered occasional mass extinctions at least since . Although they are disasters at the time, mass extinctions have sometimes accelerated the evolution of life on earth. When dominance of particular ecological niches passes from one group of organisms to another, it is rarely because the new dominant group is \"superior\" to the old and usually because an extinction event eliminates the old dominant group and makes way for the new one.\n\nThe fossil record appears to show that the rate of extinction is slowing down, with both the gaps between mass extinctions becoming longer and the average and background rates of extinction decreasing. However, it is not certain whether the actual rate of extinction has altered, since both of these observations could be explained in several ways:\n\nBiodiversity in the fossil record, which is\nshows a different trend: a fairly swift rise from , a slight decline from , in which the devastating Permian–Triassic extinction event is an important factor, and a swift rise from to the present.\n\nAlthough paleontology became established around 1800, earlier thinkers had noticed aspects of the fossil record. The ancient Greek philosopher Xenophanes (570–480 BC) concluded from fossil sea shells that some areas of land were once under water. During the Middle Ages the Persian naturalist Ibn Sina, known as \"Avicenna\" in Europe, discussed fossils and proposed a theory of petrifying fluids on which Albert of Saxony elaborated in the 14th century. The Chinese naturalist Shen Kuo (1031–1095) proposed a theory of climate change based on the presence of petrified bamboo in regions that in his time were too dry for bamboo.\n\nIn early modern Europe, the systematic study of fossils emerged as an integral part of the changes in natural philosophy that occurred during the Age of Reason. In the Italian Renaissance, Leonardo Da Vinci made various significant contributions to the field as well designed numerous fossils. At the end of the 18th century Georges Cuvier's work established comparative anatomy as a scientific discipline and, by proving that some fossil animals resembled no living ones, demonstrated that animals could become extinct, leading to the emergence of paleontology. The expanding knowledge of the fossil record also played an increasing role in the development of geology, particularly stratigraphy.\n\nThe first half of the 19th century saw geological and paleontological activity become increasingly well organised with the growth of geologic societies and museums and an increasing number of professional geologists and fossil specialists. Interest increased for reasons that were not purely scientific, as geology and paleontology helped industrialists to find and exploit natural resources such as coal.\n\nThis contributed to a rapid increase in knowledge about the history of life on Earth and to progress in the definition of the geologic time scale, largely based on fossil evidence. In 1822 Henri Marie Ducrotay de Blanville, editor of \"Journal de Physique\", coined the word \"palaeontology\" to refer to the study of ancient living organisms through fossils. As knowledge of life's history continued to improve, it became increasingly obvious that there had been some kind of successive order to the development of life. This encouraged early evolutionary theories on the transmutation of species.\nAfter Charles Darwin published \"Origin of Species\" in 1859, much of the focus of paleontology shifted to understanding evolutionary paths, including human evolution, and evolutionary theory.\nThe last half of the 19th century saw a tremendous expansion in paleontological activity, especially in North America. The trend continued in the 20th century with additional regions of the Earth being opened to systematic fossil collection. Fossils found in China near the end of the 20th century have been particularly important as they have provided new information about the earliest evolution of animals, early fish, dinosaurs and the evolution of birds. The last few decades of the 20th century saw a renewed interest in mass extinctions and their role in the evolution of life on Earth. There was also a renewed interest in the Cambrian explosion that apparently saw the development of the body plans of most animal phyla. The discovery of fossils of the Ediacaran biota and developments in paleobiology extended knowledge about the history of life back far before the Cambrian.\n\nIncreasing awareness of Gregor Mendel's pioneering work in genetics led first to the development of population genetics and then in the mid-20th century to the modern evolutionary synthesis, which explains evolution as the outcome of events such as mutations and horizontal gene transfer, which provide genetic variation, with genetic drift and natural selection driving changes in this variation over time. Within the next few years the role and operation of DNA in genetic inheritance were discovered, leading to what is now known as the \"Central Dogma\" of molecular biology. In the 1960s molecular phylogenetics, the investigation of evolutionary \"family trees\" by techniques derived from biochemistry, began to make an impact, particularly when it was proposed that the human lineage had diverged from apes much more recently than was generally thought at the time. Although this early study compared proteins from apes and humans, most molecular phylogenetics research is now based on comparisons of RNA and DNA.\n\n\n", "id": "23084", "title": "Paleontology"}
{"url": "https://en.wikipedia.org/wiki?curid=515152", "text": "Fisherian runaway\n\nFisherian runaway or runaway selection is a sexual selection mechanism proposed by the mathematical biologist Ronald Fisher in the early 20th century, to account for the evolution of exaggerated male ornamentation by persistent, directional female choice. An example is the colourful and elaborate peacock plumage compared to the relatively subdued peahen plumage; the costly ornaments, notably the bird's extremely long tail, appear to be incompatible with natural selection. Fisherian runaway can be postulated to include sexually dimorphic phenotypic traits such as behaviour expressed by either sex.\n\nExtreme and apparently maladaptive sexual dimorphism represented a paradox for evolutionary biologists from Charles Darwin's time up to the modern synthesis. Darwin attempted to resolve the paradox by assuming genetic bases for both the preference and the ornament, and supposed an \"aesthetic sense\" in higher animals, leading to powerful selection of both characteristics in subsequent generations. Fisher developed the theory further by assuming genetic correlation between the preference and the ornament, that initially the ornament signalled greater potential fitness (the likelihood of leaving more descendants), so preference for the ornament had a selective advantage. Subsequently, if strong enough, female preference for exaggerated ornamentation in mate selection could be enough to undermine natural selection even when the ornament has become non-adaptive. Over subsequent generations this could lead to runaway selection by positive feedback, and the speed with which the trait and the preference increase could (until counter-selection interferes) increase exponentially (\"geometrically\").\n\nFisherian runaway has been difficult to demonstrate empirically, because it has been difficult to detect both an underlying genetic mechanism and a process by which it is initiated.\n\nCharles Darwin published a book on sexual selection in 1871 called \"The Descent of Man, and Selection in Relation to Sex\", which garnered interest upon its release but by the 1880s the ideas had been deemed too controversial and were largely neglected. Alfred Russel Wallace disagreed with Darwin, particularly after Darwin's death, that sexual selection was a real phenomenon. R.A. Fisher was one of the few other biologists to engage with the question. When Wallace stated that animals show no sexual preference in his 1915 paper, \"The evolution of sexual preference,\" Fisher publicly disagreed:\n\nFisher, in the foundational 1930 book, \"The Genetical Theory of Natural Selection\", first outlined a model by which runaway inter-sexual selection could lead to sexually dimorphic male ornamentation based upon female choice and a preference for \"attractive\" but otherwise non-adaptive traits in male mates. He suggested that selection for traits that increase fitness may be quite common:\n\nA strong female choice for the expression alone, as opposed to the function, of a male ornament can oppose and undermine the forces of natural selection and result in the runaway sexual selection that leads to the further exaggeration of the ornament (as well as the preference) until the costs (incurred by natural selection) of the expression become greater than the benefit (bestowed by sexual selection).\n\nThe plumage dimorphism of the peacock and peahen of the species within the \"Pavo\" genus is a prime example of the ornamentation paradox that has long puzzled evolutionary biologists; Darwin wrote in 1860:The sight of a feather in a peacock’s tail, whenever I gaze at it, makes me sick!The peacock's colorful and elaborate tail requires a great deal of energy to grow and maintain. It also reduces the bird's agility, and may increase the animal's visibility to predators. The tail appears to lower the overall fitness of the individuals who possess it. Yet, it has evolved, indicating that peacocks with longer and more colorfully elaborate tails have some advantage over peacocks who don’t. Fisherian runaway posits that the evolution of the peacock tail is made possible if peahens have a preference to mate with peacocks that possess a longer and more colourful tail. Peahens that select males with these tails in turn have male offspring that are more likely to have long and colourful tails and thus are more likely to be sexually successful themselves. Equally importantly, the female offspring of these peahens are more likely to produce peahen offspring that have a preference for peacocks with longer and more colourful tails. However, though the relative fitness of males with large tails is higher than those without, the absolute fitness levels of all the members of the population (both male and female) is less than it would be if none of the peahens (or only a small number) had a preference for a longer or more colorful tail.\n\nFisher outlined two fundamental conditions that must be fulfilled in order for the Fisherian runaway mechanism to lead to the evolution of extreme ornamentation: \n\nFisher argued in his 1915 paper, \"The evolution of sexual preference\" that the type of female preference necessary for Fisherian runaway could be initiated without any understanding or appreciation for beauty. Fisher suggested that any visible features that indicate fitness, that are not themselves adaptive, that draw attention, and that vary in their appearance amongst the population of males so that the females can easily compare them, would be enough to initiate Fisherian runaway. This suggestion is compatible with his theory, and indicates that the choice of feature is essentially arbitrary, and could be different in different populations. Such arbitrariness is borne out by mathematical modelling, and by observation of isolated populations of sandgrouse, where the males can differ markedly from those in other populations.\n\nFisherian runaway assumes that sexual preference in females and ornamentation in males are both genetically variable (heritable).\n\nFisher argued that the selection for exaggerated male ornamentation is driven by the coupled exaggeration of female sexual preference for the ornament.\n\nOver time a positive feedback mechanism will see more exaggerated sons and choosier daughters being produced with each successive generation; resulting in the runaway selection for the further exaggeration of both the ornament and the preference (until the costs for producing the ornament outweigh the reproductive benefit of possessing it).\n\nSeveral alternative hypotheses use the same genetic runaway (or positive feedback) mechanism but differ in the mechanisms of the initiation. The sexy sons hypothesis (also proposed by Fisher) suggests that females that choose desirably ornamented males will have desirably ornamented (or sexy) sons, and that the effect of that behaviour on spreading the female's genes through subsequent generations may outweigh other factors such as the level of parental investment by the father.\n\nIndicator hypotheses suggest that females choose desirably ornamented males because the cost of producing the desirable ornaments is indicative of good genes by way of the individual's vigour.\n\nOther hypotheses for the evolution of male ornamentation include the sensory bias hypothesis, the compatibility hypothesis and the handicap principle.\n\n", "id": "515152", "title": "Fisherian runaway"}
{"url": "https://en.wikipedia.org/wiki?curid=42138344", "text": "Rosa's rule\n\nRosa's rule, also known as Rosa's law of progressive reduction of variability, is a biological rule that observes the tendency to go from character variation in more primitive representatives of a taxonomic group or clade to a fixed character state in more advanced members. An example of Rosa's rule is that the number of thoracic segments in adults (or holaspids) may vary in Cambrian trilobite species, while from the Ordovician the number of thorax segments is constant in entire genera, families, and even suborders. Thus, a trend of decreasing trait variation between individuals of a taxon as the taxon develops across evolutionary time can be observed. The rule is named for Italian paleontologist Daniele Rosa.\n", "id": "42138344", "title": "Rosa's rule"}
{"url": "https://en.wikipedia.org/wiki?curid=19179678", "text": "Protocell\n\nA protocell (or protobiont) is a self-organized, endogenously ordered, spherical collection of lipids proposed as a stepping-stone to the origin of life. A central question in evolution is how simple protocells first arose and how they could differ in reproductive output, thus enabling the accumulation of novel biological emergences over time, i.e. biological evolution. Although a functional protocell has not yet been achieved in a laboratory setting, the goal to understand the process appears well within reach.\n\nCompartmentalization was important in the origins of life. Membranes create enclosed compartments that are separate from the external environment, thus providing the cell with functionally specialized aqueous spaces. Because lipid bilayer of membranes is impermeable to most hydrophilic molecules (dissolved by water), the cell must have membrane transport systems that are in charge of import of nutritive molecules as well as export of waste. It is very challenging to construct protocells from molecular assemblies. An important step in this challenge is the achievement of vesicle dynamics that are relevant to cellular functions, such as membrane trafficking and self-reproduction, using amphiphilic molecules. On the primitive Earth, numerous chemical reactions of organic compounds produced the ingredients of life. Of these substances, amphiphilic molecules might be the first player in the evolution from molecular assembly to cellular life. A step from vesicle toward protocell might be to develop self-reproducing vesicles coupled with the metabolic system.\n\nSelf-assembled vesicles are essential components of primitive cells. The second law of thermodynamics requires that the universe move in a direction in which disorder (or entropy) increases, yet life is distinguished by its great degree of organization. Therefore, a boundary is needed to separate life processes from non-living matter. The cell membrane is the only cellular structure that is found in all of the cells of all of the organisms on Earth.\n\nResearchers Irene A. Chen and Jack W. Szostak (Nobel Prize in Physiology or Medicine 2009) amongst others, demonstrated that simple physicochemical properties of elementary protocells can give rise to essential cellular behaviors, including primitive forms of Darwinian competition and energy storage. Such cooperative interactions between the membrane and encapsulated contents could greatly simplify the transition from replicating molecules to true cells. Furthermore, competition for membrane molecules would favor stabilized membranes, suggesting a selective advantage for the evolution of cross-linked fatty acids and even the phospholipids of today. This micro-encapsulation allowed for metabolism within the membrane, exchange of small molecules and prevention of passage of large substances across it. The main advantages of encapsulation include increased solubility of the cargo and creating energy in the form of chemical gradient. Energy is thus often said to be stored by cells in the structures of molecules of substances such as carbohydrates (including sugars), lipids, and proteins, which release energy when chemically combined with oxygen during cellular respiration.\n\nA March 2014 study by NASA's Jet Propulsion Laboratory demonstrated a unique way to study the origins of life: fuel cells. Fuel cells are similar to biological cells in that electrons are also transferred to and from molecules. In both cases, this results in electricity and power. The study states that one important factor was that the Earth provides electrical energy at the seafloor. \"This energy could have kick-started life and could have sustained life after it arose. Now, we have a way of testing different materials and environments that could have helped life arise not just on Earth, but possibly on Mars, Europa and other places in the Solar System.\"\n\nWhen phospholipids are placed in water, the molecules spontaneously arrange such that the tails are shielded from the water, resulting in the formation of membrane structures such as bilayers, vesicles, and micelles. In modern cells, vesicles are involved in metabolism, transport, buoyancy control, and enzyme storage. They can also act as natural chemical reaction chambers. A typical vesicle or micelle in aqueous solution forms an aggregate with the hydrophilic \"head\" regions in contact with surrounding solvent, sequestering the hydrophobic single-tail regions in the micelle centre. This phase is caused by the packing behavior of single-tail lipids in a bilayer. Although the protocellular self-assembly process that spontaneously form lipid \"monolayer\" vesicles and micelles in nature resemble the kinds of primordial vesicles or protocells that might have existed at the beginning of evolution, they are not as sophisticated as the \"bilayer\" membranes of today's living organisms.\n\nRather than being made up of phospholipids, however, early membranes may have formed from monolayers or bilayers of fatty acids, which may have formed more readily in a prebiotic environment. Fatty acids have been synthesized in laboratories under a variety of prebiotic conditions and have been found on meteorites, suggesting their natural synthesis in nature.\n\nOleic acid vesicles represent good models of membrane protocells that could have existed in prebiotic times.\n\nElectrostatic interactions induced by short, positively charged, hydrophobic peptides containing 7 amino acids in length or fewer, can attach RNA to a vesicle membrane, the basic cell membrane.\n\nScientists have come to conclude that life began in hydrothermal vents in the deep sea, but a 2012 study suggests that inland pools of condensed and cooled geothermal vapor have the ideal characteristics for the origin of life. The conclusion is based mainly on the chemistry of modern cells, where the cytoplasm is rich in potassium, zinc, manganese, and phosphate ions, which are not widespread in marine environments. Such conditions, the researchers argue, are found only where hot hydrothermal fluid brings the ions to the surface — places such as geysers, mud pots, fumaroles and other geothermal features. Within these fuming and bubbling basins, water laden with zinc and manganese ions could have collected, cooled and condensed in shallow pools.\n\nAnother study in the 1990s showed that montmorillonite clay can help create RNA chains of as many as 50 nucleotides joined together spontaneously into a single RNA molecule. Later, in 2002, it was discovered that by adding montmorillonite to a solution of fatty acid micelles (lipid spheres), the clay sped up the rate of vesicle formation 100-fold.\n\nResearch has shown that some minerals can catalyze the stepwise formation of hydrocarbon tails of fatty acids from hydrogen and carbon monoxide gases - gases that may have been released from hydrothermal vents or geysers. Fatty acids of various lengths are eventually released into the surrounding water, but vesicle formation requires a higher concentration of fatty acids, so it is suggested that protocell formation started at land-bound hydrothermal vents such as geysers, mud pots, fumaroles and other geothermal features where water evaporates and concentrates the solute.\n\nAnother group suggests that primitive cells might have formed inside inorganic clay microcompartments, which can provide an ideal container for the synthesis and compartmentalization of complex organic molecules. Clay-armored \"bubbles\" form naturally when particles of montmorillonite clay collect on the outer surface of air bubbles under water. This creates a semi permeable vesicle from materials that are readily available in the environment. The authors remark that montmorillonite is known to serve as a chemical catalyst, encouraging lipids to form membranes and single nucleotides to join into strands of RNA. Primitive reproduction can be envisioned when the clay bubbles burst, releasing the lipid membrane-bound product into the surrounding medium.\n\nInstead of the more popular phospholipids of modern cells, the membrane of protocells in the RNA world would be composed of fatty acids, and that such membranes have relatively high permeability to ions and small molecules, such as nucleoside monophosphate (NMP), nucleoside diphosphate (NDP), and nucleoside triphosphate (NTP), and may withstand millimolar concentrations of Mg. Osmotic pressure also plays a significant role in protocell membrane transport.\n\nIt has been proposed that electroporation resulting from lightning strikes could be a mechanism of natural horizontal gene transfer. Electroporation is the rapid increase in bilayer permeability induced by the application of a large artificial electric field across the membrane. During electroporation in laboratory procedures, the lipid molecules are not chemically altered but simply shift position, opening up a pore (hole) that acts as the conductive pathway through the bilayer as it is filled with water. The mechanism is the creation of nanometer sized water-filled holes in the membrane. Experimentally, electroporation is used to introduce hydrophilic molecules into cells. It is a particularly useful technique for large highly charged molecules such as DNA and RNA, which would never passively diffuse across the hydrophobic bilayer core. Because of this, electroporation is one of the key methods of transfection as well as bacterial transformation.\n\nSome molecules or particles are too large or too hydrophilic to pass through a lipid bilayer, but can be moved across the cell membrane through fusion or budding of vesicles. This may have eventually led to mechanisms that facilitate movement of molecules to the inside (endocytosis) or to release its contents into the extracellular space (exocytosis).\n\nStarting with a technique commonly used to deposit molecules on a solid surface, Langmuir–Blodgett deposition, scientists are able to assemble phospholipid membranes of arbitrary complexity layer by layer. These artificial phospholipid membranes support functional insertion both of purified and of \"in situ\" expressed membrane proteins. The technique could help astrobiologists understand how the first living cells originated.\n\nJeewanu protocells are synthetic chemical particles that possess cell-like structure and seem to have some functional living properties. First synthesized in 1963 from simple minerals and basic organics while exposed to sunlight, it is still reported to have some metabolic capabilities, the presence of semipermeable membrane, amino acids, phospholipids, carbohydrates and RNA-like molecules. However, the nature and properties of the Jeewanu remains to be clarified.\n\nIn a similar synthesis experiment a frozen mixture of water, methanol, ammonia and carbon monoxide was exposed to ultraviolet (UV) radiation. This combination yielded large amounts of organic material that self-organised to form globules or vesicles when immersed in water. The investigating scientist considered these globules to resemble cell membranes that enclose and concentrate the chemistry of life, separating their interior from the outside world. The globules were between , or about the size of red blood cells. Remarkably, the globules fluoresced, or glowed, when exposed to UV light. Absorbing UV and converting it into visible light in this way was considered one possible way of providing energy to a primitive cell. If such globules played a role in the origin of life, the fluorescence could have been a precursor to primitive photosynthesis. Such fluorescence also provides the benefit of acting as a sunscreen, diffusing any damage that otherwise would be inflicted by UV radiation. Such a protective function would have been vital for life on the early Earth, since the ozone layer, which blocks out the sun's most destructive UV rays, did not form until after photosynthetic life began to produce oxygen.\n\nProtocell research has created controversy and opposing opinions, including critics of the vague definition of \"artificial life\". The creation of a basic unit of life is the most pressing ethical concern, although the most widespread worry about protocells is their potential threat to human health and the environment through uncontrolled replication.\n\n", "id": "19179678", "title": "Protocell"}
{"url": "https://en.wikipedia.org/wiki?curid=24973826", "text": "Human genetic resistance to malaria\n\nHuman genetic resistance to malaria refers to inherited changes in the DNA of humans which increase resistance to the disease and result in increased survival of individuals with the genetic change. Evolutionarily, the existence of these genotypes are likely due to pressure from evolving alongside the parasites that cause malaria (of the genus \"Plasmodium\"). Since malaria infects red blood cells, these genetic changes are most commonly alterations to molecules essential for red blood cell function (and therefore parasite survival), such as hemoglobin or other cellular proteins or enzymes of red blood cells. These alterations generally protect red blood cells from invasion by \"Plasmodium\" parasites or replication of parasites within the red blood cell.\n\nMalaria has placed the strongest known selective pressure on the human genome since the origination of agriculture within the past 10,000 years. \"Plasmodium falciparum\" was probably not able to gain a foothold among African populations until larger sedentary communities emerged in association with the evolution of domestic agriculture in Africa (the agricultural revolution). Several inherited variants in red blood cells have become common in parts of the world where malaria is frequent as a result of selection exerted by this parasite. This selection was historically important as the first documented example of disease as an agent of natural selection in humans. It was also the first example of genetically controlled innate immunity that operates early in the course of infections, preceding adaptive immunity which exerts effects after several days. In malaria, as in other diseases, innate immunity leads into, and stimulates, adaptive immunity.\n\nOne of the key reasons for the high fatality rate in \"P. falciparum malaria\" is the occurrence of so-called cerebral malaria. Patients become confused, disoriented and often lapse into a terminal coma. Clumps of malaria-infested red cells adhere to the endothelium and occlude the microcirculation of the brain with deadly consequences. The \"P. falciparum\" parasite alters the characteristics of the red cell membrane, making them more \"sticky\". Clusters of parasitized red cells exceed the size of the capillary circulation blocking blood flow and producing cerebral hypoxia. Cerebral malaria accounts for 80% of malaria deaths. Thalassemic erythrocytes adhere to parasitized red cells much less readily than do their normal counterparts. This alteration would lessen the chance of developing cerebral malaria.\n\n\"P. vivax\" is clearly a less potent agent of natural selection that is \"P. falciparum\". However, the morbidity of \"P. vivax\" is not negligible. For example, \"P. vivax\" infections induce a greater inflammatory response in the lungs than is observed in \"P. falciparum\" infections, and progressive alveolar capillary dysfunction is observed after the treatment of \"vivax\" malaria. Epidemiological studies in the Amazonian region of Brazil have shown that the number and rate of hospital admissions for \"P. vivax\" infections have recently increased while those of \"P. falciparum\" have decreased.\n\nThese inherited changes to hemoglobin or other characteristic erythrocyte proteins which are critical and rather invariant features of mammalian biochemistry, usually result in some kind of anemia, a disease or defect of red blood cells. These changes are referred to by the names of the diseases resulting from them including sickle cell disease, thalassemia, glucose-6-phosphate dehydrogenase (G6PD) deficiency, and others. These blood disorders cause increased morbidity and mortality in areas of the world where malaria is no longer prevalent.\n\nMicroscopic parasites (like viruses, protozoans that cause malaria, and others) cannot replicate on their own. They replicate by invading the hosts' cells, and usurping the cellular machinery to replicate themselves. Eventually, unchecked replication causes the cells to burst, releasing the infectious organisms into the bloodstream. There they spread and infect other cells. As cells die and toxic products of invasive organism replication accumulate, disease symptoms appear.\n\nThe process of invading the host cell, hijacking the cellular machinery, replication and final release is a complicated set of steps. Very specific proteins coded by the DNA of the infectious organism as well as the host cells allow those steps to happen. Even a very small change in a critical protein might make infection difficult or impossible. Such changes might arise by a process of mutation in the gene that codes for the protein. If the change is in the gamete, that is, the sperm or egg that join to form a zygote that grows into a human being, the protective mutation will be inherited. Since lethal diseases kill many persons who lack protective mutations, in time, many persons in regions where lethal diseases are endemic come to inherit protective mutations.\n\nMutations may have detrimental as well as beneficial effects, and any single mutation may have both. Infectivity of malaria depends on specific proteins present in the cell walls and elsewhere in red blood cells. Protective mutations alter these \nproteins in ways that make them inaccessible to malaria organisms. However, these changes also alter the functioning and form of red blood cells that may have visible effects, either overtly, or by microscopic examination of red blood cells. These changes may impair the function of red blood cells in various ways that have a detrimental effect on the health or longevity of the individual. However, if the net effect of protection against malaria outweighs the other detrimental effects, the protective mutation will tend to be retained and propagated from generation to generation.\n\nThese alterations which protect against malarial infections but impair red blood cells are generally considered blood disorders, since they tend to have overt and detrimental effects. Their protective function has only in recent times, been discovered and acknowledged. Some of these disorders are known by fanciful and cryptic names like sickle-cell anemia, thalassaemia, glucose-6-phosphate dehydrogenase deficiency, ovalocytosis, elliptocytosis and loss of the Gerbich antigen and the Duffy antigen. These names refer to various proteins, enzymes, and the shape or function of red blood cells.\n\nThe potent effect of genetically controlled innate resistance is reflected in the probability of survival of young children in malarious environments. It is necessary to study innate immunity in the susceptible age group, younger than four years; in older children and adults the effects of innate immunity are overshadowed by those of adaptive immunity. It is also necessary to study populations in which random use of antimalarial drugs does not occur.\n\nSome early contributions on innate resistance to infections of vertebrates, including humans, are summarized in Table 1.\n\nTable 1. Innate Resistance to Plasmodia\n\nIt is remarkable that two of the pioneering studies were on malaria. The classical studies on the Toll receptor in \"Drosophila\" fruit fly were rapidly extended to Toll-like receptors in mammals and then to other pattern recognition receptors, which play important roles in innate immunity. However, the early contributions on malaria remain as classical examples of innate resistance, which have stood the test of time.\n\nThe mechanisms by which erythrocytes containing abnormal hemoglobins, or are G6PD deficient, are partially protected against \"P. falciparum\" infections are not fully understood, although there has been no shortage of suggestions. During the peripheral blood stage of replication malaria parasites have a high rate of oxygen consumption and ingest large amounts of hemoglobin. It is likely that HbS in endocytic vesicles is deoxygenated, polymerizes and is poorly digested. In red cells containing abnormal hemoglobins, or which are G6PD deficient, oxygen radicals are produced, and malaria parasites induce additional oxidative stress. This can result in changes in red cell membranes, including translocation of phosphatidylserine to their surface, followed by macrophage recognition and ingestion. The authors suggest that this mechanism is likely to occur earlier in abnormal than in normal red cells, thereby restricting multiplication in the former. In addition, binding of parasitized sickle cells to endothelial cells is significantly decreased because of an altered display of \"P. falciparum\" erythrocyte membrane protein-1 (PfMP-1). This protein is the parasite’s main cytoadherence ligand and virulence factor on the cell surface. During the late stages of parasite replication red cells are adherent to venous endothelium, and inhibiting this attachment could suppress replication.\n\nSickle hemoglobin induces the expression of heme oxygenase-1 in hematopoietic cells. Carbon monoxide, a byproduct of heme catabolism by heme oxygenase-1(HO-1), prevents an accumulation of circulating free heme after \"Plasmodium\" infection, suppressing the pathogenesis of experimental cerebral malaria. Other mechanisms, such as enhanced tolerance to disease mediated by HO-1 and reduced parasitic growth due to translocation of host micro-RNA into the parasite, have been described.\n\nEvidence has accumulated that the first line of defense against malaria is provided by genetically controlled innate resistance, mainly exerted by abnormal hemoglobins and glucose-6-phosphate dehydrogenase deficiency. The three major types of inherited genetic resistance - sickle cell disease, thalassemias, and G6PD deficiency - were present in the Mediterranean world by the time of the Roman Empire.\n\nThis was the first time a genetic disease was linked to a mutation of a specific protein and Pauling introduced his fundamentally important concept of sickle cell anemia as a genetically transmitted molecular disease.\n\nThe molecular basis of sickle cell anemia was finally elucidated in 1959, when Ingram perfected the techniques of tryptic peptide fingerprinting. In the mid-1950s, one of the newest and most reliable ways of separating peptides and amino acids was by means of the enzyme trypsin, which split polypeptide chains by specifically degrading the chemical bonds formed by the carboxyl groups of two amino acids, lysine and arginine. Small differences in hemoglobin A and S will result in small changes in one or more of these peptides . To try to detect these small differences, Ingram combined paper electrophoresis and the paper chromotagraphy methods. By this combination he created a two-dimensional method that enabled him to comparatively \"fingerprint\" the hemoglobin S and A fragments he obtained from the tryspin digest. The fingerprints revealed approximately 30 peptide spots, there was one peptide spot clearly visible in the digest of haernoglobin S which was not obvious in the haemoglobin A \"finger print\". The Hb S gene defect is a mutation of a single nucleotide (A to T) of the β-globin gene replacing the amino acid glutamic acid with the less polar amino acid valine at the sixth position of the β chain.\n\nHbS has a lower negative charge at physiological pH than does normal adult hemoglobin. The consequences of the simple replacement of a charged amino acid with a hydrophobic, neutral amino acid are far ranging, Recent studies in West Africa suggest that the greatest impact of Hb S seems to be to protect against either death or severe disease—that is, profound anemia or cerebral malaria—while having less effect on infection per se. Children who are heterozygous for the sickle cell gene have only one- tenth the risk of death from falciparum as do those who are homozygous for the normal hemoglobin gene. Binding of parasitized sickle erythrocytes to endothelial cells and blood monocytes is significantly reduced due to an altered display of \"Plasmodium falciparum\" erythrocyte membrane protein 1 (PfEMP-1), the parasite’s major cytoadherence ligand and virulence factor on the erythrocyte surface. Protection also derives from the instability of sickle hemoglobin, which clusters the predominant integral red cell membrane protein (called band 3) and triggers accelerated removal by phagocytic cells. Natural antibodies recognize these clusters on senescent erythrocytes. Protection by HbAS involves the enhancement of not only innate but also of acquired immunity to the parasite. Prematurely denatured sickle hemoglobin results in an up regulation of natural antibodies which control erythrocyte adhesion in both malaria and sickle cell disease. Targeting the stimuli that lead to endothelial activation will constitute a promising therapeutic strategy to inhibit sickle red cell adhesion and vasco-occlusion.\nP. Brain also while working in Northern Rhodesia suggested that while homozygotes for the sickle cell gene suffered from several problems heterozygotes might be protected against malaria. \nA scientist named Haldane hypothesized that the sickle cell trait is able to preserve the sickle trait by providing protection against malaria. Eric Elguero and collegues used 3,959 blood samples from 195 villages in central Africa to support the relationship between Malaria and the sickle cell trait. His studies were able to support this correlation by taking into consideration of confounding variables that may have affected other global studies of malaria (Eleguero et al,2015). He realized that many variables,such as the environment,genetic background, life style,and access to healthcare can affect the results of the sickle cell and malaria studies(Eleguero et al,2015). The heterozygote advantage allow the sickle cell gene to be preserved and passed down many generations. The trait is preserved in those that carry the trait and exhibit sickle cell anemia symptoms. Elguero studies proved that the sickle cell trait was resistance to malaria by using blood samples from asymptomatic patients. This supported the hypothesis by confirming that even if the patient wasn't showing symptoms of sickle cell there is still protection against malaria(Eleguero et al,2015). This may be because of the gene provides a mechanism that serves as the protection not only the symptoms that sickle cell carriers have.\n\nReference:\n\nElguero, E., Délicat-Loembet, L., Rougerona, V., Arnathaua, C., Roche, B., Becquarta, P., . . . Prugnollea, F. (2015, March 24). Malaria continues to select for sickle cell trait in Central Africa. PNAS, 112(22), 7051-7054. doi:10.1073/pnas\n\nIt has long been known that a kind of anemia, termed thalassemia, has a high frequency in some Mediterranean populations, including Greeks and southern Italians. The name is derived from the Greek words for sea (\"thalassa\"), meaning the Mediterranean sea, and blood (\"haima\"). Vernon Ingram deserves the credit for explaining the genetic basis of different forms of thalassemia as an imbalance in the synthesis of the two polypeptide chains of hemoglobin.\n\nIn the common Mediterranean variant, mutations decrease production of the β-chain (β-thalassemia). In α-thalassemia, which is relatively frequent in Africa and several other countries, production of the α-chain of hemoglobin is impaired, and there is relative over-production of the β-chain. Individuals homozygous for β-thalassemia have severe anemia and are unlikely to survive and reproduce, so selection against the gene is strong. Those homozygous for α-thalassemia also suffer from anemia and there is some degree of selection against the gene.\n\nThe lower Himalayan foothills and Inner Terai or Doon Valleys of Nepal and India are highly malarial due to a warm climate and marshes sustained during the dry season by groundwater percolating down from the higher hills. Malarial forests were intentionally maintained by the rulers of Nepal as a defensive measure. Humans attempting to live in this zone suffered much higher mortality than at higher elevations or below on the drier Gangetic Plain. However, the Tharu people had lived in this zone long enough to evolve resistance via multiple genes. Medical studies among the Tharu and non-Tharu population of the Terai yielded the evidence that the prevalence of cases of residual malaria is nearly seven times lower among Tharus. The basis for resistance has been established to be homozygosity of α-Thalassemia gene within the local population. Endogamy along caste and ethnic lines appear to have prevented these genes from being more widespread in neighboring populations.\n\nThere is evidence that the persons with α-thalassemia, HbC and HbE have some degree of protection against the parasite.\nHemoglobin C (HbC) is an abnormal hemoglobin with substitution of a lysine residue for glutamic acid residue of the β-globin chain, at exactly the same ß-6 position as the HbS mutation. The \"C\" designation for HbC is from the name of the city where it was discovered—Christchurch, New Zealand. People who have this disease, particularly children, may have episodes of abdominal and joint pain, an enlarged spleen, and mild jaundice, but they do not have severe crises, as occur in sickle cell disease. Haemoglobin C is common in malarious areas of West Africa, especially in Burkina Faso. In a large case–control study performed in Burkina Faso on 4,348 Mossi subjects, that HbC was associated with a 29% reduction in risk of clinical malaria in HbAC heterozygotes and of 93% in HbCC homozygotes. HbC represents a ‘slow but gratis’ genetic adaptation to malaria through a transient polymorphism, compared to the polycentric ‘quick but costly’ adaptation through balanced polymorphism of HbS.\nHbC modifies the quantity and distribution of the variant antigen \"P. falciparum\" erythrocyte membrane protein 1 (PfEMP1) on the infected red blood cell surface and the modified display of malaria surface proteins reduces parasite adhesiveness (thereby avoiding clearance by the spleen) and can reduce the risk of severe disease.\n\nHemoglobin E is due to a single point mutation in the gene for the beta chain with a glutamate-to-lysine substitution at position 26. It is one of the most prevalent hemoglobinopathies with 30 million people affected. Hemoglobin E is very common in parts of Southeast Asia. HbE erythrocytes have an unidentified membrane abnormality that renders the majority of the RBC population relatively resistant to invasion by \"P falciparum\".\n\nMalaria does not occur in the cooler, drier climates of the highlands in the tropical and subtropical regions of the world.\nTens of thousands of individuals have been studied, and high frequencies of abnormal hemoglobins have not been found in any population that was malaria free. The frequencies of abnormal hemoglobins in different populations vary greatly, but some are undoubtedly polymorphic, having frequencies higher than expected by recurrent mutation. There is no longer doubt that malarial selection played a major role in the distribution of all these polymorphisms. All of these are in malarious areas,\n\nThe thalassemias have a high incidence in a broad band extending from the Mediterranean basin and parts of Africa, throughout the Middle East, the Indian subcontinent, Southeast Asia, Melanesia, and into the Pacific Islands.\n\nOther genetic mutations besides hemoglobin abnormalities that confer resistance to \"Plasmodia\" infection involve alterations of the cellular surface antigenic proteins, cell membrane structural proteins, or enzymes involved in glycolysis.\n\nGlucose-6-phosphate dehydrogenase (G6PD) is an important enzyme in red cells, metabolizing glucose through the pentose phosphate pathway, an anabolic alternative to catabolic oxidation (glycolysis), while maintaining a reducing environment. G6PD is present in all human cells but is particularly important to red blood cells. Since mature red blood cells lack nuclei and cytoplasmic RNA, they cannot synthesize new enzyme molecules to replace genetically abnormal or ageing ones. All proteins, including enzymes, have to last for the entire lifetime of the red blood cell, which is normally 120 days.\n\nIn 1956 Alving and colleagues showed that in some African Americans the antimalarial drug primaquine induces hemolytic anemia, and that those individuals have an inherited deficiency of G6PD in erythrocytes. G6PD deficiency is sex-linked, and common in Mediterranean, African and other populations. In Mediterranean countries such individuals can develop a hemolytic diathesis (favism) after consuming fava beans. G6PD deficient persons are also sensitive to several drugs in addition to primaquine.\n\nG6PD deficiency is the most common enzyme deficiency in humans, estimated to affect some 400 million people. There are many mutations at this locus, two of which attain frequencies of 20% or greater in African and Mediterranean populations; these are termed the A- and Med mutations. Mutant varieties of G6PD can be more unstable than the naturally occurring enzyme, so that their activity declines more rapidly as red cells age.\n\nThis question has been studied in isolated populations where antimalarial drugs were not used in Tanzania, East Africa and in the Republic of the Gambia, West Africa, following children during the period when they are most susceptible to \"falciparum\" malaria. In both cases parasite counts were significantly lower in G6PD-deficient persons than in those with normal red cell enzymes. The association has also been studied in individuals, which is possible because the enzyme deficiency is sex-linked and female heterozygotes are mosaics due to lyonization, where random inactivation of an X-chromosome in certain cells creates a population of G6PD deficient red blood cells coexisting with normal red blood cells. Malaria parasites were significantly more often observed in normal red cells than in enzyme-deficient cells. An evolutionary genetic analysis of malarial selection of G6PD deficiency genes has been published by Tishkoff and Verelli. The enzyme deficiency is common in many countries that are, or were formerly, malarious, but not elsewhere.\n\nPyruvate kinase (PK) deficiency, also called erythrocyte pyruvate kinase deficiency, is an inherited metabolic disorder of the enzyme pyruvate kinase. In this condition, a lack of pyruvate kinase slows down the process of glycolysis. This effect is especially devastating in cells that lack mitochondria, because these cells must use anaerobic glycolysis as their sole source of energy because the TCA cycle is not available. One example is red blood cells, which in a state of pyruvate kinase deficiency rapidly become deficient in ATP and can undergo hemolysis. Therefore, pyruvate kinase deficiency can cause hemolytic anemia.\n\nThere is a significant correlation between severity of PK deficiency and extent of protection against malaria.\n\nElliptocytosis a blood disorder in which an abnormally large number of the patient's erythrocytes are elliptical. There is much genetic variability amongst those affected. There are three major forms of hereditary elliptocytosis: common hereditary elliptocytosis, spherocytic elliptocytosis and southeast Asian ovalocytosis.\n\nOvalocytosis is a subtype of elliptocytosis, and is an inherited condition in which erythrocytes have an oval instead of a round shape. In most populations ovalocytosis is rare, but South-East Asian ovalocytosis (SAO) occurs in as many as 15% of the indigenous people of Malaysia and of Papua New Guinea. Several abnormalities of SAO erythrocytes have been reported, including increased red cell rigidity and reduced expression of some red cell antigens.\nSAO is caused by a mutation in the gene encoding the erythrocyte band 3 protein. There is a deletion of codons 400-408 in the gene, leading to a deletion of 9 amino-acids at the boundary between the cytoplasmic and transmembrane domains of band 3 protein. Band 3 serves as the principal binding site for the membrane skeleton, a submembrane protein network composed of ankyrin, spectrin, actin, and band 4.1. Ovalocyte band 3 binds more tightly than normal band 3 to ankyrin, which connects the membrane skeleton to the band 3 anion transporter. These qualitative defects create a red blood cell membrane that is less tolerant of shear stress and more susceptible to permanent deformation.\n\nSAO is associated with protection against cerebral malaria in children because it reduces sequestration of erythrocytes parasitized by \"P. falciparum\" in the brain microvasculature. Adhesion of \"P. falciparum\"-infected red blood cells to CD36 is enhanced by the cerebral malaria-protective SAO trait . Higher efficiency of sequestration via CD36 in SAO individuals could determine a different organ distribution of sequestered infected red blood cells. These provide a possible explanation for the selective advantage conferred by SAO against cerebral malaria.\n\n\"Plasmodium vivax\" has a wide distribution in tropical countries, but is absent or rare in a large region in West and Central Africa, as recently confirmed by PCR species typing. This gap in distribution has been attributed to the lack of expression of the Duffy antigen receptor for chemokines (DARC) on the red cells of many sub-Saharan Africans. Duffy negative individuals are homozygous for a DARC allele, carrying a single nucleotide mutation (DARC 46 T → C), which impairs promoter activity by disrupting a binding site for the hGATA1 erythroid lineage transcription factor. In widely cited \"in vitro\" and \"in vivo\" studies, Miller et al. reported that the Duffy blood group is the receptor for \"P. vivax\" and that the absence of the Duffy blood group on red cells is the resistance factor to \"P. vivax\" in persons of African descent. This has become a well-known example of innate resistance to an infectious agent because of the absence of a receptor for the agent on target cells.\n\nHowever, observations have accumulated showing that the original Miller report needs qualification. In human studies of \"P. vivax\" transmission, there is evidence for the transmission of \"P. vivax\" among Duffy-negative populations in Western Kenya, the Brazilian Amazon region, and Madagascar. The Malagasy people on Madagascar have an admixture of Duffy-positive and Duffy-negative people of diverse ethnic backgrounds. 72% of the island population were found to be Duffy-negative. \"P. vivax\" positivity was found in 8.8% of 476 asymptomatic Duffy-negative people, and clinical \"P. vivax\" malaria was found in 17 such persons. Genotyping indicated that multiple \"P. vivax\" strains were invading the red cells of Duffy-negative people. The authors suggest that among Malagasy populations there are enough Duffy-positive people to maintain mosquito transmission and liver infection. More recently, Duffy negative individuals infected with two different strains of \"P. vivax\" were found in Angola and Equatorial Guinea; further, \"P. vivax\" infections were found both in humans and mosquitoes, which means that active transmission is occurring. The frequency of such transmission is still unknown. Because of these several reports from different parts of the world it is clear that some variants of \"P. vivax\" are being transmitted to humans who are not expressing DARC on their red cells. The same phenomenon has been observed in New World monkeys. However, DARC still appears to be a major receptor for human transmission of \"P. vivax\".\n\nThe distribution of Duffy negativity in Africa does not correlate precisely with that of \"P. vivax\" transmission. Frequencies of Duffy negativity are as high in East Africa (above 80%), where the parasite is transmitted, as they are in West Africa, where it is not. The potency of \"P. vivax\" as an agent of natural selection is unknown, and may vary from location to location. DARC negativity remains a good example of innate resistance to an infection, but it produces a relative and not an absolute resistance to \"P. vivax\" transmission.\n\nThe Gerbich antigen system is an integral membrane protein of the erythrocyte and plays a functionally important role in maintaining erythrocyte shape. It also acts as the receptor for the \"P. falciparum\" erythrocyte binding protein. There are four alleles of the gene which encodes the antigen, Ge-1 to Ge-4. Three types of Ge antigen negativity are known: Ge-1,-2,-3, Ge-2,-3 and Ge-2,+3. persons with the relatively rare phenotype Ge-1,-2,-3, are less susceptible (~60% of the control rate) to invasion by \"P. falciparum\". Such individuals have a subtype of a condition called hereditary elliptocytosis, characterized by oval or elliptical shape erythrocytes.\n\nRare mutations of glycophorin A and B proteins are also known to mediate resistance to \"P. falciparum\".\n\nHuman leucocyte antigen (HLA) polymorphisms common in West Africans but rare in other racial groups, are associated with protection from severe malaria. This group of genes encodes cell-surface antigen-presenting proteins and has many other functions. In West Africa, they account for as great a reduction in disease incidence as the sickle-cell hemoglobin variant. The studies suggest that the unusual polymorphism of major histocompatibility complex genes has evolved primarily through natural selection by infectious pathogens.\n\nPolymorphisms at the HLA loci, which encode proteins that participate in antigen presentation, influence the course of malaria. In West Africa an HLA class I antigen (HLA Bw53) and an HLA class II haplotype (DRB1*13OZ-DQB1*0501) are independently associated with protection against severe malaria. However, HLA correlations vary, depending on the genetic constitution of the polymorphic malaria parasite, which differs in different geographic locations.\n\nEvolutionary biologist J.B.S. Haldane was the first to give a hypothesis on the relationship between malaria and the genetic disease. He first delivered his hypothesis at the Eighth International Congress of Genetics held in 1948 at Stockholm on a topic \"The Rate of Mutation of Human Genes\". He formalised in a technical paper published in 1949 in which he made a prophetic statement: \"The corpuscles of the anaemic heterozygotes are smaller than normal, and more resistant to hypotonic solutions. It is at least conceivable that they are also more resistant to attacks by the sporozoa which cause malaria.\" This became known as 'Haldane's malaria hypothesis', or concisely, the 'malaria hypothesis'.\n\nDetailed study of a cohort of 1022 Kenyan children living near Lake Victoria, published in 2002, confirmed this prediction. Many SS children still died before they attained one year of age. Between 2 and 16 months the mortality in AS children was found to be significantly lower than that in AA children. This well-controlled investigation shows the ongoing action of natural selection through disease in a human population.\n\nAnalysis of genome wide association (GWA) and fine-resolution association mapping is a powerful method for establishing the inheritance of resistance to infections and other diseases. Two independent preliminary analyses of GWA association with severe falciparum malaria in Africans have been carried out, one by the Malariagen Consortium in a Gambian population and the other by Rolf Horstmann (Bernhard Nocht Institute for Tropical Medicine, Hamburg) and his colleagues on a Ghanaian population. In both cases the only signal of association reaching genome-wide significance was with the \"HBB\" locus encoding the \"β\"-chain of hemoglobin, which is abnormal in HbS. This does not imply that HbS is the only gene conferring innate resistance to falciparum malaria; there could be many such genes exerting more modest effects that are challenging to detect by GWA because of the low levels of linkage disequilibrium in African populations. However the same GWA association in two populations is powerful evidence that the single gene conferring strongest innate resistance to \"falciparum\" malaria is that encoding HbS.\n\nThe fitnesses of different genotypes in an African region where there is intense malarial selection were estimated by Anthony Allison in 1954. In the Baamba population living in the Semliki Forest region in Western Uganda the sickle-cell heterozygote (AS) frequency is 40%, which means that the frequency of the sickle-cell gene is 0.255 and 6.5% of children born are SS homozygotes. \nIt is a reasonable assumption that until modern treatment was available three quarters of the SS homozygotes failed to reproduce. To balance this loss of sickle-cell genes, a mutation rate of 1:10.2 per gene per generation would be necessary. This is about 1000 times greater than mutation rates measured in \"Drosophila\" and other organisms and much higher than recorded for the sickle-cell locus in Africans. To balance the polymorphism, Anthony Allison estimated that the fitness of the AS heterozygote would have to be 1.26 times than that of the normal homozygote. Later analyses of survival figures have given similar results, with some differences from site to site. In Gambians, it was estimated that AS heterozygotes have 90% protection against \"P. falciparum\"-associated severe anemia and cerebral malaria, whereas in the Luo population of Kenya it was estimated that AS heterozygotes have 60% protection against severe malarial anemia. These differences reflect the intensity of transmission of \"P. falciparum\" malaria from locality to locality and season to season, so fitness calculations will also vary. In many African populations the AS frequency is about 20%, and a fitness superiority over those with normal hemoglobin of the order of 10% is sufficient to produce a stable polymorphism.\n\n\nactin, ankrin, spectrin - proteins that are the major components of the cytoskeleton scaffolding within a cell's cytoplasm\n\naerobic - uses oxygen for the production of energy (contrast anaerobic)\n\nallele - one of two or more alternative forms of a gene that arise by mutation\n\nα-chain / β-chain (hemoglobin) - subcomponents of the hemoglobin molecule; two α-chains and two β-chains make up normal hemoglobin (HbA)\n\nalveolar - pertaining to the alveoli, the tiny air sacs in the lungs\n\namino acid - any of twenty organic compounds that are subunits of protein in the human body\n\nanabolic - of or relating to the synthesis of complex molecules in living organisms from simpler ones\ntogether with the storage of energy; constructive metabolism (contrast catabolic)\n\nanaerobic - refers to a process or reaction which does not require oxygen, but produces energy by other means (contrast aerobic)\n\nanion transporter (organic) - molecules that play an essential role in the distribution and excretion of numerous endogenous metabolic products and exogenous organic anions\n\nantigen - any substance (as an immunogen or a hapten) foreign to the body that evokes an immune response either alone or after forming a complex with a larger molecule (as a protein) and that is capable of binding with a component (as an antibody or T cell) of the immune system\n\nATP - (Adenosine triphosphate) - an organic molecule containing high energy phosphate bonds used to transport energy within a cell\n\ncatabolic - of or relatig to the breakdown of complex molecules in living organisms to form simpler ones, together with the release of energy; destructive metabolism (contrast anabolic)\n\nchemokine - are a family of small cytokines, or signaling proteins secreted by cells\n\ncodon - a sequence of three nucleotides which specify which amino acid will be added next during protein synthesis\n\ncorpuscle - obsolete name for red blood cell\n\ncytoadherance - infected red blood cells may adhere to blood vellel walls and uninfected red blood cells\n\ncytoplasm - clear jelly-like substance, mostly water, inside a cell\n\ndiathesis - a tendency to suffer from a particular medical condition\n\nDNA - deoxyribonucleic acid, the hereditary material of the genome\n\nDrosophila - a kind of fruit fly used for genetic experimentation because of ease of reproduction and manipulation of its genome\n\nendocytic - the transport of solid matter or liquid into a cell by means of a coated vacuole or vesicle\n\nendogamy - the custom of marrying only within the limits of a local community, clan, or tribe\n\nendothelial - of or referring to the thin inner surface of blood vessels\n\nenzyme - a protein that promotes a cellular process, much like a catalyst in an ordinary chemical reaction\n\nepidemiology - the study of the spread of disease within a population\n\nerythrocyte - red blood cell, which with the leucocytes make up the cellular content of the blood (contrast leucocyte)\n\nerythroid - of or referring to erythrocytes, red blood cells\n\nfitness (genetic) - loosely, reproductive success that tends to propagate a trait or traits (see natural selection)\n\ngenome - (abstractly) all the inheritable traits of an organism; represented by its chromosomes\n\ngenotype - the genetic makeup of a cell, an organism, or an individual usually with reference to a specific trait\n\nglycolysis - the breakdown of glucose by enzymes, releasing energy\n\nglycophorin - transmembrane proteins of red blood cells\n\nhaplotype - a set of DNA variations, or polymorphisms, that tend to be inherited together.\n\nHb (HbC, HbE, HbS, etc.) hemoglobin (hemoglobin polymorphisms: hemoglobin type C, hemoglobin type E, \nhemoglobin type S)\n\nhematopoietic (stem cell) - the blood stem cells that give rise to all other blood cells\n\nheme oxygenase-1 (HO-1) - an enzyme that breaks down heme, the iron-containing non-protein part of hemoglobin\n\nhemoglobin - iron based organic molecule in red blood cells that transports oxygen and gives blood its red color\n\nhemolysis - the rupturing of red blood cells and the release of their contents (cytoplasm) into surrounding fluid (e.g., blood plasma)\n\nheterozygous - possessing only one copy of a gene for a particular trait\n\nhomozygous - possessing two identical copies of a gene for a particular trait, one from each parent\n\nhypotonic - denotes a solution of lower osmotic pressure than another solution with which it is in contact, so that certain molecules will migrate from the region of higher osmotic pressure to the region of lower osmotic pressure, until the pressures are equalized\n\nin vitro - in a test tube or other laboratory vessel; usually used in regard to a testing protocol\n\nin vivo - in a live human (or animal); usually used in regard to a testing protocol\n\nleucocyte - white blood cell, part of the immune system, which together with red blood cells, comprise the cellular component of the blood (contrast erythrocyte)\n\nligand - an extracellular signal molecule, which when it binds to a cellular receptor, causes a response by the cell\n\nlocus (gene or chromosome) - the specific location of a gene or DNA sequence or position on a chromosome\n\nmacrophage - a large white blood cell, part of the immune system that ingests foreign particles and infectious microorganisms\n\nmajor histocompatibility complex (MHC) - proteins found on the surfaces of cells that help the immune system recognize foreign substances; also called the human leucocyte antigen (HLA) system\n\nmicro-RNA - a cellular RNA fragment that prevents the production of a particular protein by binding to and destroying the messenger RNA that would have produced the protein.\n\nmicrovasculature - very small blood vessels\n\nmitochondria - energy producing organelles of a cell\n\nmutation - a spontaneous change to a gene, arising from an error in replication of DNA; usually mutations are referred to in the context of inherited mutations, i.e. changes to the gametes\n\nnatural selection - the gradual process by which biological traits become either more or less common in a population as a function of the effect of inherited traits on the differential reproductive success of organisms interacting with their environment (closely related to fitness)\n\nnucleotide - organic molecules that are subunits, of nucleic acids like DNA and RNA\n\nnucleic acid - a complex organic molecule present in living cells, esp. DNA or RNA, which consist of many nucleotides linked in a long chain.\n\noxygen radical - a highly reactive ion containing oxygen, capable of damaging microorganisms and normal tissues.\n\npathogenesis - the manner of development of a disease\n\nPCR - Polymerase Chain Reaction, an enzymatic reaction by which DNA is replicated in a test tube for subsequent testing or analysis\n\nphenotype - the composite of an organism's observable characteristics or traits, such as its morphology\n\nPlasmodium - the general type (genus) of the protozoan microorganisms that cause malaria, though only a few of them do\n\npolymerize - to combine replicated subunits into a longer molecule (usually referring to synthetic materials, but also organic molecules)\n\npolymorphism - the occurrence of something in several different forms, as for example hemoglobin (HbA, HbC, etc.)\n\npolypeptide - a chain of amino acids forming part of a protein molecule\n\nreceptor (cellular surface) - specialized integral membrane proteins that take part in communication between the cell and the outside world; receptors are responsive to specific ligands that attach to them.\n\nreducing environment (cellular) - reducing environment is one where oxidation is prevented by removal of oxygen and other oxidising gases or vapours, and which may contain actively reducing gases such as hydrogen, carbon monoxide and gases that would oxidize in the presence of oxygen, such as hydrogen sulfide.\n\nRNA - ribonucleic acid, a nucleic acid present in all living cells. Its principal role is to act as a messenger carrying instructions from DNA for controlling the synthesis of proteins\n\nsequestration (biology) - process by which an organism accumulates a compound or tissue (as red blood cells) from the environment\n\nsex-linked - a trait associated with a gene that is carried only by the male or female parent (contrast with autosomal)\n\nSporozoa - a large class of strictly parasitic nonmotile protozoans, including \"Plasmodia\" which cause malaria\n\nTCA cycle - TriCarboxylic Acid cycle is a series of enzyme-catalyzed chemical reactions that form a key part of aerobic respiration in cells\n\ntranslocation (cellular biology) - movement of molecules from outside to inside (or vice versa) of a cell\n\ntransmembrane - existing or occurring across a cell membrane\n\nvenous - of or referring to the veins\n\nvesicle - a small organelle within a cell, consisting of fluid enclosed by a fatty membrane\n\nvirulence factors - enable an infectious agent to replicate and disseminate within a host in part by subverting or eluding host defenses.\n\n\n", "id": "24973826", "title": "Human genetic resistance to malaria"}
{"url": "https://en.wikipedia.org/wiki?curid=41825123", "text": "Molecular paleontology\n\nMolecular paleontology refers to the recovery and analysis of DNA, proteins, carbohydrates, or lipids, and their diagenetic products from ancient human, animal, and plant remains. The field of molecular paleontology has yielded important insights into evolutionary events, species' diasporas, the discovery and characterization of extinct species. By applying molecular analytical techniques to DNA in fossils, one can quantify the level of relatedness between any two organisms for which DNA has been recovered.\n\nAdvancements in the field of molecular paleontology have allowed scientists to pursue evolutionary questions on a genetic level rather than relying on phenotypic variation alone. Using various biotechnological techniques such as DNA isolation, amplification, and sequencing scientists have been able to gain expanded new insights into the divergence and evolutionary history of countless organisms.\n\nThe study of molecular paleontology is said to have begun with the discovery by Abelson of 360 million year old amino acids preserved in fossil shells. However, Svante Pääbo is often the one considered to be the founder of the field of molecular paleontology.\n\nThe field of molecular paleontology has had several major advances since the 1950s and is a continuously growing field. Below is a timeline showing notable contributions that have been made.\n\nmid-1950s: Abelson found preserved amino acids in fossil shells that were about 360 million years old. Produced idea of comparing fossil amino acid sequences with existing organism so that molecular evolution could be studied.\n\n1970s: Fossil peptides are studied by amino acid analysis. Start to use whole peptides and .\n\nLate 1970s: Palaeobotanists (can also be spelled as Paleobotanists) studied molecules from well-preserved fossil plants.\n\n1984: The first successful DNA sequencing of an extinct species, the quagga, a zebra-like species.\n\n1991: Published article on the successful extraction of proteins from the fossil bone of a dinosaur, specifically the seismosaurus.\n\n2005: Scientists resurrect extinct 1918 influenza virus.\n\n2006: Neanderthals nuclear DNA sequence segments begin to be analyzed and published.\n\n2007: Scientists synthesize entire extinct human endogenous retrovirus (HERV-K) from scratch.\n\n2010: A new species of early hominid, the Denisovans, discovered from mitochondrial and nuclear genomes recovered from bone found in a cave in Siberia. Analysis showed that the Denisovan specimen lived approximately 41,000 years ago, and shared a common ancestor with both modern humans and Neanderthals approximately 1 million years ago in Africa.\n\n2013: The first entire Neanderthal genome is successfully sequenced. More information can be found at the Neanderthal genome project.\n\n2013: A 400,000-year-old specimen with remnant mitochondrial DNA sequenced and is found to be a common ancestor to Neanderthals and Denisovans, later named \"Homo heidelbergensis\".\n\n2015: A 110,000-year-old fossil tooth containing DNA from Denisovans was reported.\n\nThe first successful DNA sequencing of an extinct species was in 1984, from a 150-year-old museum specimen of the quagga, a zebra-like species. Mitochondrial DNA (also known as mtDNA) was sequenced from desiccated muscle of the quagga, and was found to differ by 12 base substitutions from the mitochondrial DNA of a mountain zebra. It was concluded that these two species had a common ancestor 3-4 million years ago, which is consistent with known fossil evidence of the species.\n\nThe Denisovans of Eurasia, a hominid species related to Neanderthals and humans, was discovered as a direct result of DNA sequencing of a 41,000-year-old specimen recovered in 2008. Analysis of the mitochondrial DNA from a retrieved finger bone showed the specimen to be genetically distinct from both humans and Neanderthals. Two teeth and a toe bone were later found to belong to different individuals with the same population. Analysis suggests that both the Neanderthals and Denisovans were already present throughout Eurasia when modern humans arrived. In November 2015, scientists reported finding a fossil tooth containing DNA from Denisovans, and estimated its age at 110,000-years-old.\n\nThe mtDNA from the Denisovan finger bone differs from that of modern humans by 385 bases (nucleotides) in the mtDNA strand out of approximately 16,500, whereas the difference between modern humans and Neanderthals is around 202 bases. In contrast, the difference between chimpanzees and modern humans is approximately 1,462 mtDNA base pairs. This suggested a divergence time around one million years ago. The mtDNA from a tooth bore a high similarity to that of the finger bone, indicating they belonged to the same population. From a second tooth, an mtDNA sequence was recovered that showed an unexpectedly large number of genetic differences compared to that found in the other tooth and the finger, suggesting a high degree of mtDNA diversity. These two individuals from the same cave showed more diversity than seen among sampled Neanderthals from all of Eurasia, and were as different as modern-day humans from different continents.\n\nIsolation and sequencing of nuclear DNA has also been accomplished from the Denisova finger bone. This specimen showed an unusual degree of DNA preservation and low level of contamination. They were able to achieve near-complete genomic sequencing, allowing a detailed comparison with Neanderthal and modern humans. From this analysis, they concluded, in spite of the apparent divergence of their mitochondrial sequence, the Denisova population along with Neanderthal shared a common branch from the lineage leading to modern African humans. The estimated average time of divergence between Denisovan and Neanderthal sequences is 640,000 years ago, and the time between both of these and the sequences of modern Africans is 804,000 years ago. They suggest the divergence of the Denisova mtDNA results either from the persistence of a lineage purged from the other branches of humanity through genetic drift or else an introgression from an older hominin lineage.\n\nHomo heidelbergensis was first discovered in 1907 near Heidelberg, Germany and later also found elsewhere in Europe, Africa, and Asia.\nHowever it was not until 2013 that a specimen with retrievable DNA was found, in a ~400,000 year old femur found in the Sima de los Huesos Cave in Spain. The femur was found to contain both mtDNA and nuclear DNA. Improvements in DNA extraction and library preparation techniques allowed for mtDNA to be successfully isolated and sequenced, however the nuclear DNA was found to be too degraded in the observed specimen, and was also contaminated with DNA from an ancient cave bear (\"Ursus deningeri\") present in the cave. The mtDNA analysis found a surprising link between the specimen and the Denisovans, and this finding raised many questions. Several scenarios were proposed in a January 2014 paper titled \"A mitochondrial genome sequence of a hominin from Sima de los Huesos\", elucidating the lack of convergence in the scientific community on how \"Homo heidelbergensis\" is related to other known hominin groups. One plausible scenario that the authors proposed was that the \"H. heidelbergensis\" was an ancestor to both Denisovans and Neanderthals. Completely sequenced nuclear genomes from both Denisovans and Neanderthals suggest a common ancestor approximately 700,000 years ago, and one leading researcher in the field, Svante Paabo, suggests that perhaps this new hominin group is that early ancestor.\n\nMolecular paleontology techniques applied to fossils have contributed to the discovery and characterization of several new species, including the Denisovans and \"Homo heidelbergensis\". We have been able to better understand the path that humans took as they populated the earth, and what species were present during this diaspora.\n\nIt is now possible to revive extinct species using molecular paleontology techniques. This was first accomplished via cloning in 2003 with the Pyrenean ibex, a type of wild goat that became extinct in 2000. Nuclei from the Pyrenean ibex's cells were injected into goat eggs emptied of their own DNA, and implanted into surrogate goat mothers. The offspring lived only seven minutes after birth, due to defects in its lungs. Other cloned animals have been observed to have similar lung defects.\n\nThere are many species that have gone extinct as a direct result of human activity. Some examples include the dodo, the great auk, the Tasmanian tiger, the Chinese river dolphin, and the passenger pigeon. An extinct species can be revived by using allelic replacement of a closely related species that is still living. By only having to replace a few genes within an organism, instead of having to build the extinct species' genome from scratch, it could be possible to bring back several species in this way, even Neanderthals.\n\nThe ethics surrounding the re-introduction of extinct species are very controversial. Critics of bringing extinct species back to life contend that it would divert limited money and resources from protecting the world's current biodiversity problems. With current extinction rates approximated to be 100 to 1,000 times the background extinction rate, it is feared that a de-extinction program might lessen public concerns over the current mass extinction crisis, if it is believed that these species can simply be brought back to life. As the editors of a Scientific American article on de-extinction pose: Should we bring back the wooly mammoth only to let elephants become extinct in the meantime? The main driving factor for the extinction of most species in this era (post 10,000 BC) is the loss of habitat, and temporarily bringing back an extinct species will not recreate the environment they once inhabited.\n\nProponents of de-extinction, such as George Church, speak of many potential benefits. Reintroducing an extinct keystone species, such as the wooly mammoth, could help re-balance the ecosystems that once depended on them. Some extinct species could create broad benefits for the environments they once inhabited, if returned. For example, wooly mammoths may be able to slow the melting of the Russian and Arctic tundra in several ways such as eating dead grass so that new grass can grow and take root, and periodically breaking up the snow, subjecting the ground below to the arctic air. These techniques could also be used to reintroduce genetic diversity in a threatened species, or even introduce new genes and traits to allow the animals to compete better in a changing environment.\n\nWhen a new potential specimen is found, scientists normally first analyze for cell and tissue preservation using histological techniques, and test the conditions for the survivability of DNA. They will then attempt to isolate a DNA sample using the technique described below, and conduct a PCR amplification of the DNA to increase the amount of DNA available for testing. This amplified DNA is then sequenced. Care is taken to verify that the sequence matches the phylogenetic traits of the organism. When an organism dies, a technique called amino acid dating can be used to age the organism. It inspects the degree of racemization of aspartic acid, leucine, and alanine within the tissue. As time passes, the D/L ratio (where \"D\" and \"L\" are mirror images of each other) increase from 0 to 1. In samples where the D/L ratio of aspartic acid is greater than 0.08, ancient DNA sequences can not be retrieved (as of 1996).\n\nMitochondrial DNA (mtDNA) is separate from one's nuclear DNA. It is present in organelles called mitochondria in each cell. Unlike nuclear DNA, which is inherited from both parents and rearranged every generation, an exact copy of mitochondrial DNA gets passed down from mother to her sons and daughters. The benefits of performing DNA analysis with Mitochondrial DNA is that it has a far smaller mutation rate than nuclear DNA, making tracking lineages on the scale of tens of thousands of years much easier. Knowing the base mutation rate for mtDNA, (in humans this rate is also known as the Human mitochondrial molecular clock) one can determine the amount of time any two lineages have been separated. Another advantage of mtDNA is that thousands of copies of it exist in every cell, whereas only two copies of nuclear DNA exist in each cell. All eukaryotes, a group which includes all plants, animals, and fungi, have mtDNA. A disadvantage of mtDNA is that only the maternal line is represented. For example, a child will inherit 1/8 of its DNA from each of its eight great-grandparents, however it will inherit an exact clone of its maternal great-grandmother's mtDNA. This is analogous to a child inheriting only his paternal great-grandfather's last name, and not a mix of all of the eight surnames.\nThere are many things to consider when isolating a substance. First, depending upon what it is and where it is located, there are protocols that must be carried out in order to avoid contamination and further degradation of the sample. Then, handling of the materials is usually done in a physically isolated work area and under specific conditions (i.e. specific Temperature, moisture, etc...) also to avoid contamination and further loss of sample.\n\nOnce the material has been obtained, depending on what it is, there are different ways to isolate and purify it. DNA extraction from fossils is one of the more popular practices and there are different steps that can be taken to get the desired sample. DNA extracted from amber-entombed fossils can be taken from small samples and mixed with different substances, centrifuged, incubated, and centrifuged again. On the other hand, DNA extraction from insects can be done by grinding the sample, mixing it with buffer, and undergoing purification through glass fiber columns. In the end, regardless of how the sample was isolated for these fossils, the DNA isolated must be able to undergo amplification.\n\nThe field of molecular paleontology benefited greatly from the invention of the polymerase chain reaction(PCR), which allows one to make billions of copies of a DNA fragment from just a single preserved copy of the DNA. One of the biggest challenges up until this point was the extreme scarcity of recovered DNA because of degradation of the DNA over time.\n\nDNA sequencing is done to determine the order of nucleotides and genes. There are many different materials from which DNA can be extracted. In animals, the mitochondrial chromosome can be used for molecular study. Chloroplasts can be studied in plants as a primary source of sequence data.\nIn the end, the sequences generated are used to build evolutionary trees. Methods to match data sets include: maximum probability, minimum evolution (also known as neighbor-joining) which searches for the tree with shortest overall length, and the maximum parsimony method which finds the tree requiring the fewest character-state changes. The groups of species defined within a tree can also be later evaluated by statistical tests, such as the bootstrap method, to see if they are indeed significant.\nIdeal environmental conditions for preserving DNA where the organism was desiccated and uncovered are difficult to come by, as well as maintaining their condition until analysis. Nuclear DNA normally degrades rapidly after death by endogenous hydrolytic processes, by UV radiation, and other environmental stressors.\n\nAlso, interactions with the organic breakdown products of surrounding soil have been found to help preserve biomolecular materials. However, they have also created the additional challenge of being able to separate the various components in order to be able to conduct the proper analysis on them. Some of these breakdowns have also been found to interfere with the action of some of the enzymes used during PCR.\n\nFinally, one of the largest challenge in extracting ancient DNA, particularly in ancient human DNA, is in contamination during PCR. Small amounts of human DNA can contaminate the reagents used for extraction and PCR of ancient DNA. These problems can be overcome by rigorous care in the handling of all solutions as well as the glassware and other tools used in the process. It can also help if only one person performs the extractions, to minimize different types of DNA present.\n", "id": "41825123", "title": "Molecular paleontology"}
{"url": "https://en.wikipedia.org/wiki?curid=42572964", "text": "Natural Theology or Evidences of the Existence and Attributes of the Deity\n\nNatural Theology or Evidences of the Existence and Attributes of the Deity is an 1802 work of Christian apologetics and philosophy of religion by the English clergyman William Paley (July 1743 – 25 May 1805). The book expounds his arguments from natural theology, making a teleological argument for the existence of God, notably beginning with the watchmaker analogy.\n\nThe book was written in the context of the natural theology tradition. In earlier centuries, theologians such as John Ray and William Derham, as well as philosophers of classical times such as Cicero, argued for the existence and goodness of God from the general well-being of living things and the physical world.\n\nPaley's \"Natural Theology\" is an extended argument, constructed around a series of examples including finding a watch; comparing the eye to a telescope; and the existence of finely adapted mechanical structures in animals, such as joints which function like hinges or manmade ball and socket joints. Paley argues that these all lead to an intelligent Creator, and that a system is more than the sum of its parts. The last chapters are more theological in character, arguing that the attributes of God must be sufficient for the extent of his operations, and that God must be good because designs seen in nature are beneficial.\n\nThe book was many times republished and remains in print. It continues to be consulted by creationists. Charles Darwin took its arguments seriously and responded to them; evolutionary biologists like Stephen Jay Gould and Richard Dawkins continue to discuss Paley's book to respond to modern proponents with similar ideas.\n\nThe main thrust of William Paley's argument in \"Natural Theology\" is that God's design of the whole creation can be seen in the general happiness, or well-being, that is evident in the physical and social order of things. This sets the book within the broad tradition of the Enlightenment's natural theology; and this explains why Paley based much of his thought on John Ray (1691), William Derham (1711) and Bernard Nieuwentyt (1750).\n\nPaley's argument is built mainly around anatomy and natural history. \"For my part\", he says, \"I take my stand in human anatomy\"; elsewhere he insists upon \"the necessity, in each particular case, of an intelligent designing mind for the contriving and determining of the forms which organized bodies bear\". In making his argument, Paley employed a wide variety of metaphors and analogies. Perhaps the most famous is his analogy between a watch and the world. Historians, philosophers and theologians often call this the watchmaker analogy. Building on this mechanical analogy, Paley presents examples from planetary astronomy and argues that the regular movements of the solar system resemble the workings of a giant clock. To bolster his views he cites the work of his old friend John Law and the Dublin Astronomer Royal John Brinkley.\n\nThe germ of the idea is to be found in ancient writers who used sundials and Ptolemaic epicycles to illustrate the divine order of the world. These types of examples can be seen in the work of the ancient philosopher Cicero, especially in his \"De Natura Deorum\", ii. 87 and 97. The watch analogy was widely used in the Enlightenment, by deists and Christians alike. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first edition of \"Natural Theology: or, Evidences of the Existence and Attributes of the Deity\" was published in 1802 in London by J. Faulder. In the United States, the book was published and released by E Sargeant and Company of New York on December 15, 1802. A later edition published by E. S. Gorham contained revisions by F. LeGros Clark in order to \"harmonize with modern science\".\n\nThe book was republished in many editions by publishers in cities including London, Oxford, Cambridge, Edinburgh and Philadelphia. The twentieth reprint was made in 1820. Versions appeared in years including 1802, 1807, 1809, 1813, 1818, 1819, 1821, 1823, 1825, 1826, 1829, 1830, 1840, 1854 and many later years. The book remains in print, with more recent editions for example in 2006, 2008, 2009, 2010 and 2014. The book was also republished in editions of Paley's \"Collected Works\". It has been translated into languages including French and Welsh.\n\nThe Scottish philosopher David Hume (who died in 1776, before Paley assembled his arguments into \"Natural Theology\") had criticised arguments from design on several grounds. Firstly, he rejected the making of an analogy between the world and a human artifact such as a watch, since these are so dissimilar that any analogy must be very weak and unreliable. Secondly, Hume argued that even if one accepted the analogy, it would not prove that the creator is infinite, good, or perfectly intelligent, nor that there would be only one creator god. After all, wrote Hume, \"what shadow of an argument… can you produce from your hypothesis to prove the unity of the Deity? A great number of men join in building a house or ship, in rearing a city, in framing a commonwealth; why may not several deities combine in contriving and framing a world?\"\n\nTo counter the first argument, Paley strongly defended the analogy, emphasising complex mechanisms in living organisms seen as machines designed for purpose and contending that, in a sense \"That an animal is a machine is neither correctly true nor wholly false\". In replying to the second argument, Paley made a tactical retreat from traditional attributes of God to a more limited definition, in which unity went \"no further than to a unity of counsel\". It sufficed that God demonstrated plan, intelligence and foresight, had inconceivable power, and showed goodness through perceived design being beneficial in the clear majority of cases.\n\nEarly evolutionary ideas presented a new threat to the analogy between living organisms and designed object, as life differs in reproducing itself. In chapter XXIII Paley explicitly dismissed Buffon's concept of \"organic molecules\", then turned to an unattributed concept: \"Another system, which has lately been brought forward, and with much ingenuity, is that of \"appetencies\"\": the term and his description clearly refer to Erasmus Darwin's concept of transmutation of species, as set out in \"Zoonomia\". Paley objected to it dispensing with \"the necessity, in each particular case, of an intelligent, designing mind\", and to it lacking evidence or observations of the process. More specifically, Darwin had adopted the common idea of inheritance of acquired characteristics, and Paley raised objections including the persistence of unused male nipples, and (discreetly put in Latin) the effect of circumcision not being inherited by generations of Jews.\n\nThroughout the book, Paley presented difficulties in examples or analogies that had been presented to support evolutionary explanations or the doctrine of \"appetencies\". He objected that Erasmus Darwin's concept could only explain adaptation directly relating to activity, and could not explain passive adaptation.\n\nThe \"Edinburgh Review\" of 1802-3 commented that\n\nThe review agreed with Paley that \"No thinking man, we conceive, can doubt that there are marks of design in the universe\" and that either a single example like the eye would be conclusive, or no quantity of examples would be. Paley is praised for relying on \"mechanical phenomena\" rather than arguments about human intelligence.\n\nThe bible commentator William Jenks described the book in 1838 as \"a work highly celebrated for the justness of its reflections, and the benevolence, good sense, and piety which it breathes.\"\n\nCharles Darwin's studies at the University of Cambridge included two other texts by Paley, and in his final exams in January 1831 he did well in questions on these texts. He had to stay on until June, and read Paley's \"Natural Theology\" as well as John Herschel's \"Preliminary Discourse on the Study of Natural Philosophy\" and Alexander von Humboldt's \"Personal Narrative\": these books inspired \"a burning zeal\" to research natural history. After the \"Beagle\" voyage he began development of his theory of natural selection, and in 1838 opened a notebook listing \"books to be read\", including \"Paley's Nat. Theology\". In 1859, on completing \"On the Origin of Species\", he told a friend \"I do not think I hardly ever admired a book more than Paley's \"Natural Theology\": I could almost formerly have said it by heart.\"\n\nHe later stated in his autobiography that he was initially convinced by the argument:\n\nIn 1993 the evolutionary biologist Stephen Jay Gould compared Paley to Voltaire's Doctor Pangloss, the man who could argue any case (however hopeless). Gould is struck that Paley can claim that even the agonising pain of gallstones or gout could indicate the goodness of a loving God, with the justification that it felt so good when the pain stopped. Gould makes it clear he finds Paley's argument incorrect scientifically, but states that he respects it as a coherent and well-defended philosophy. Gould particularly respects Paley's method of identifying alternative possibilities and then systematically refuting them. Gould notes that Paley envisages a Lamarckist kind of evolution and rebuts it with the observation that men have not lost their nipples through disuse. However, Gould writes, Paley did not manage to think of one more alternative, natural selection, which has no purpose at all but just kills off whatever works less well in every generation.\n\nThe evolutionary biologist Richard Dawkins described himself as a \"neo-Paleyan\" in \"The Blind Watchmaker\" (1986), where he argued, following the evolutionary biologist and humanist Julian Huxley, that Paley's watch analogy fails to recognise the difference between the complexity of living organisms and that of inanimate objects. Living organisms can reproduce themselves, so they can change to become more complex from generation to generation. Inanimate objects such as watches are unable to pass on any changes, so they never become more complex unless a watchmaker redesigns them. The comparison breaks down, in Dawkins's view, because of this important distinction.\n\nThe arguments in Paley's book have been rejected by \"virtually all biologists\". In its place, the theory of evolution has been widely accepted by scientists from Darwin onwards, and Darwin persuaded \"most educated people\" that processes such as evolution were governed by natural laws. This has not stopped creationists such as those in the Intelligent Design movement from continuing to use Paley's arguments:\n\n\n", "id": "42572964", "title": "Natural Theology or Evidences of the Existence and Attributes of the Deity"}
{"url": "https://en.wikipedia.org/wiki?curid=18962436", "text": "Ohno's law\n\nOhno's law was proposed by a Japanese-American biologist Susumu Ohno, saying that the gene content of the mammalian species has been conserved over species not only in the DNA content but also in the genes themselves. That is, nearly all mammalian species have conserved the X chromosome from their primordial X chromosome of a common ancestor.\n\nAs a cytological evidence, in first, mammalian X chromosomes in various species, including human and mouse, have nearly the same size, with the content of about 5% of the genome. Second, for individual gene loci, a number of X-linked genes are common through mammalian species. Examples are found in glucose-6-phosphate dehydrogenase (G6PD), a gene for polypeptide of antihemophilic globulin (AHG or Factor VIII) in hemophilia A and B, and plasma thromboplastin component gene (PTC or Factor IX). Moreover, no instances were found where an X-linked gene in one species was located on an autosome in the other species.\n\nThe content of a chromosome would be changed mainly by mutation after duplication of the chromosome and translocation with other chromosomes. However, in mammals, since the chromosomal sex-determination mechanism would have been established in their earlier stages of evolution, polyploidy would have not occurred due to its incompatibility with the sex-determining mechanism. Moreover, X-autosome translocation would have been prohibited because it might have resulted in detrimental effects for survival to the organism. Thus in mammals, the content of X chromosomes has been conserved after typical 2 round duplication events at early ancestral stages of evolution, at the fish or amphibia (2R hypothesis).\n\nGenes on the long arm of the human X are contained in the monotreme X and genes on the short arm of the human X are distributed on the autosomes of marsupials. Ohno commented to the result that monotremes and marsupials were not considered to be ancestors of true mammals, but they have diverged very early from the main line of mammals. Chloride channel gene (\"CLCN4\") was mapped to the human X but on chromosome 7 of C57BL/6 mice, species of \"Mus musculus\", though the gene is located on X of \"Mus spretus\" and rat.\n\n", "id": "18962436", "title": "Ohno's law"}
{"url": "https://en.wikipedia.org/wiki?curid=42812764", "text": "Selection shadow\n\nThe selection shadow is a concept involved with the evolutionary theories of ageing that states that selection pressures on an individual decrease as an individual ages and passes sexual maturity, resulting in a \"shadow\" of time where selective fitness is not considered. Over generations, this results in maladaptive mutations that accumulate later in life due to aging being non-adaptive toward reproductive fitness. The concept was first worked out by J. B. S. Haldane and Peter Medawar in the 1940s, with Medawar creating the first graphical model.\n\nThe model developed by Medawar states that due to the dangerous conditions and pressures from the environment, including predators and diseases, most individuals in the wild die not long after sexual maturity. Therefore, there is a low probability for individuals to survive to an advanced age and suffer the effects related to aging. In conjunction with this, the effects of natural selection decrease as age increases, so that later individual performance is ignored by selection forces. This results in beneficial mutations not being selected for if they only have a positive result later in life, along with later in life deleterious mutations not being selected against. Due to the fitness of an individual not being affected once it is past its reproductive prime, later mutations and effects are considered to be in the \"shadow\" of selection.\n\nThis concept would later be adapted into Medawar's 1952 mutation accumulation hypothesis, which was itself expanded upon by George C. Williams in his 1957 antagonistic pleiotropy hypothesis.\n\nA classical requirement and constraint of the model is that the number of individuals within a population that live to reach senescence must be small in number. If this is not true for a population, then the effects of old age will not be under a selection shadow and instead affect adaptation and evolution of the population as a whole. At the same time, however, this requirement has been challenged by increasing evidence of senescence being more common in wild populations than previously expected, especially among birds and mammals, while the effects of the selection shadow remain present.\n\nSome scientists, however, have criticized the idea of aging being non-adaptive, instead adopting the theory of \"Death by Design\". This theory follows the work of August Weismann, which states that aging specifically evolved as an adaptation, and disagrees with Medawar's model as a perceived oversimplification of the impact older organisms have on evolution. It is also claimed that older organisms have a higher reproductive capacity due to being better fit in order to reach their age, rather than their capacity being equal as in Medawar's calculations.\n", "id": "42812764", "title": "Selection shadow"}
{"url": "https://en.wikipedia.org/wiki?curid=38889813", "text": "Viral phylodynamics\n\nViral phylodynamics is defined as the study of how epidemiological, immunological, and evolutionary processes act and potentially interact to shape viral phylogenies.\nSince the coining of the term in 2004, research on viral phylodynamics has focused on transmission dynamics in an effort to shed light on how these dynamics impact viral genetic variation. Transmission dynamics can be considered at the level of cells within an infected host, individual hosts within a population, or entire populations of hosts.\n\nMany viruses, especially RNA viruses, rapidly accumulate genetic variation because of short generation times and high mutation rates.\nPatterns of viral genetic variation are therefore heavily influenced by how quickly transmission occurs and by which entities transmit to one another.\nPatterns of viral genetic variation will also be affected by selection acting on viral phenotypes.\nAlthough viruses can differ with respect to many phenotypes, phylodynamic studies have to date tended to focus on a limited number of viral phenotypes.\nThese include virulence phenotypes, phenotypes associated with viral transmissibility, cell or tissue tropism phenotypes, and antigenic phenotypes that can facilitate escape from host immunity.\nDue to the impact that transmission dynamics and selection can have on viral genetic variation, viral phylogenies can therefore be used to investigate important epidemiological, immunological, and evolutionary processes, such as epidemic spread, spatio-temporal dynamics including metapopulation dynamics, zoonotic transmission, tissue tropism, and antigenic drift.\nThe quantitative investigation of these processes through the consideration of viral phylogenies is the central aim of viral phylodynamics.\n\nIn coining the term \"phylodynamics\", Grenfell and coauthors postulated that viral phylogenies \"... are determined by a combination of immune selection, changes in viral population size, and spatial dynamics\".\nTheir study showcased three features of viral phylogenies, which may serve as rules of thumb for identifying important epidemiological, immunological, and evolutionary processes influencing patterns of viral genetic variation.\n\n\n\n\nAlthough these three phylogenetic features are useful rules of thumb to identify epidemiological, immunological, and evolutionary processes that might be impacting viral genetic variation, there is growing recognition that the mapping between process and phylogenetic pattern can be many-to-one. For instance, although ladder-like trees such as the one shown in figure 3A could reflect the presence of directional selection, ladder-like trees could also reflect sequential genetic bottlenecks that might occur with rapid spatial spread, as in the case of rabies virus. Because of this many-to-one mapping between process and phylogenetic pattern, research in the field of viral phylodynamics has sought to develop and apply quantitative methods to effectively infer process from reconstructed viral phylogenies (see Methods). The consideration of other data sources (e.g., incidence patterns) may aid in distinguishing between competing phylodynamic hypotheses. Combining disparate sources of data for phylodynamic analysis remains a major challenge in the field and is an active area of research.\n\nPhylodynamic models may aid in dating epidemic and pandemic origins.\nThe rapid rate of evolution in viruses allows molecular clock models to be estimated from genetic sequences, thus providing a per-year rate of evolution of the virus.\nWith the rate of evolution measured in real units of time, it is possible to infer the date of the most recent common ancestor (MRCA) for a set of viral sequences.\nThe age of the MRCA of these isolates is a lower bound; the common ancestor of the entire virus population must have existed earlier than the MRCA of the virus sample.\nIn April 2009, genetic analysis of 11 sequences of swine-origin H1N1 influenza suggested that the common ancestor existed at or before 12 January 2009.\nThis finding aided in making an early estimate of the basic reproduction number formula_1 of the pandemic. Similarly, genetic analysis of sequences isolated from within an individual can be used to determine the individual's infection time.\n\nPhylodynamic models may provide insight into epidemiological parameters that are difficult to assess through traditional surveillance means.\nFor example, assessment of formula_1 from surveillance data requires careful control of the variation of the reporting rate and the intensity of surveillance.\nInferring the demographic history of the virus population from genetic data may help to avoid these difficulties and can provide a separate avenue for inference of formula_1.\nSuch approaches have been used to estimate formula_1 in hepatitis C virus and HIV.\nAdditionally, differential transmission between groups, be they geographic-, age-, or risk-related, is very difficult to assess from surveillance data alone.\nPhylogeographic models have the possibility of more directly revealing these otherwise hidden transmission patterns.\nPhylodynamic approaches have mapped the geographic movement of the human influenza virus and quantified the epidemic spread of rabies virus in North American raccoons.\nHowever, nonrepresentative sampling may bias inferences of both formula_1 and migration patterns.\nPhylodynamic approaches have also been used to better understand viral transmission dynamics and spread within infected hosts. For example, phylodynamic studies have been used to infer the rate of viral growth within infected hosts and to argue for the occurrence of viral compartmentalization in hepatitis C infection.\n\nPhylodynamic approaches can also be useful in ascertaining the effectiveness of viral control efforts, particularly for diseases with low reporting rates. For example, the genetic diversity of the DNA-based hepatitis B virus declined in the Netherlands in the late 1990s, following the initiation of a vaccination program. This correlation was used to argue that vaccination was effective at reducing the prevalence of infection, although alternative explanations are possible.\n\nViral control efforts can also impact the rate at which virus populations evolve, thereby influencing phylogenetic patterns. Phylodynamic approaches that quantify how evolutionary rates change over time can therefore provide insight into the effectiveness of control strategies. For example, an application to HIV sequences within infected hosts showed that viral substitution rates dropped to effectively zero following the initiation of antiretroviral drug therapy. This decrease in substitution rates was interpreted as an effective cessation of viral replication following the commencement of treatment, and would be expected to lead to lower viral loads. This finding is especially encouraging because lower substitution rates are associated with slower progression to AIDS in treatment-naive patients.\n\nAntiviral treatment also creates selective pressure for the evolution of drug resistance in virus populations, and can thereby affect patterns of genetic diversity. Commonly, there is a fitness trade-off between faster replication of susceptible strains in the absence of antiviral treatment and faster replication of resistant strains in the presence of antivirals. Thus, ascertaining the level of antiviral pressure necessary to shift evolutionary outcomes is of public health importance. Phylodynamic approaches have been used to examine the spread of Oseltamivir resistance in influenza A/H1N1.\n\nMost often, the goal of phylodynamic analyses is to make inferences of epidemiological processes from viral phylogenies.\nThus, most phylodynamic analyses begin with the reconstruction of a phylogenetic tree.\nGenetic sequences are often sampled at multiple time points, which allows the estimation of substitution rates and the time of the MRCA using a molecular clock model.\nFor viruses, Bayesian phylogenetic methods are popular because of the ability to fit complex demographic scenarios while integrating out phylogenetic uncertainty.\n\nTraditional evolutionary approaches directly utilize methods from computational phylogenetics and population genetics to assess hypotheses of selection and population structure without direct regard for epidemiological models.\nFor example,\nHowever, such analyses were not designed with epidemiological inference in mind and it may be difficult to extrapolate from standard statistics to desired epidemiological quantities.\n\nIn an effort to bridge the gap between traditional evolutionary approaches and epidemiological models, several analytical methods have been developed to specifically address problems related to phylodynamics.\nThese methods are based on coalescent theory, birth-death models, and simulation, and are used to more directly relate epidemiological parameters to observed viral sequences.\n\nThe coalescent is a mathematical model that describes the ancestry of a sample of nonrecombining gene copies.\nIn modeling the coalescent process, time is usually considered to flow backwards from the present.\nIn a selectively neutral population of constant size formula_6 and nonoverlapping generations (the Wright Fisher model),\nthe expected time for a sample of two gene copies to \"coalesce\" (i.e., find a common ancestor) is formula_6 generations.\nMore generally, the waiting time for two members of a sample of formula_8 gene copies to share a common ancestor is exponentially distributed, with rate\nThis time interval is labeled formula_10, and at its end there are formula_11 extant lineages remaining (see figure 4).\nThese remaining lineages will coalesce at the rate formula_12 after intervals formula_13.\nThis process can be simulated by drawing exponential random variables with rates formula_14 until there is only a single lineage remaining (the MRCA of the sample).\nIn the absence of selection and population structure, the tree topology may be simulated by picking two lineages uniformly at random after each coalescent interval formula_15.\n\nThe expected waiting time to find the MRCA of the sample is the sum of the expected values of the internode intervals,\n\nTwo corollaries are :\n\nConsequently, the TMRCA estimated from a relatively small sample of viral genetic sequences is an asymptotically unbiased estimate for the time that the viral population was founded in the host population.\n\nFor example, Robbins et al. estimated the TMRCA for 74 HIV-1 subtype-B genetic sequences collected in North America to be 1968.\nAssuming a constant population size, we expect the time back to 1968 to represent formula_19 of the TMRCA of the North American virus population.\n\nIf the population size formula_20 changes over time, the coalescent rate formula_21 will also be a function of time.\nDonnelley and Tavaré derived this rate for a time-varying population size under the assumption of constant birth rates:\nBecause all topologies are equally likely under the neutral coalescent, this model will have the same properties as the constant-size coalescent under a rescaling of the time variable: formula_23.\n\nVery early in an epidemic, the virus population may be growing exponentially at rate formula_24, so that formula_25 units of time in the past, the population will have size formula_26.\nIn this case, the rate of coalescence becomes\nThis rate is small close to when the sample was collected (formula_28), so that external branches (those without descendants) of a gene genealogy will tend to be long relative to those close to the root of the tree.\nThis is why rapidly growing populations yield trees as depicted in figure 1A.\n\nIf the rate of exponential growth is estimated from a gene genealogy, it may be combined with knowledge of the duration of infection or the serial interval formula_29 for a particular pathogen to estimate the basic reproduction number, formula_30.\nThe two may be linked by the following equation:\nFor example, Fraser et al. generated one of the first estimates of formula_1 for pandemic H1N1 influenza in 2009 by using a coalescent-based analysis of 11 hemagglutinin sequences in combination with prior data about the infectious period for influenza.\n\nInfectious disease epidemics are often characterized by highly nonlinear and rapid changes in the number of infected individuals and the effective population size of the virus. In such cases, birth rates are highly variable, which can diminish the correspondence between effective population size and the prevalence of infection. Many mathematical models have been developed in the field of mathematical epidemiology to describe the nonlinear time series of prevalence of infection and the number of susceptible hosts. A well studied example is the Susceptible-Infected-Recovered (SIR) system of differential equations, which describes the fractions of the population formula_33 susceptible, formula_34 infected, and formula_35 recovered as a function of time:\nHere, formula_39 is the per capita rate of transmission to susceptible hosts, and formula_40 is the rate at which infected individuals recover, whereupon they are no longer infectious. In this case, the incidence of new infections per unit time is formula_41, which is analogous to the birth rate in classical population genetics models. Volz et al. proposed that the general formula for the rate of coalescence will be:\nThe ratio formula_43 can be understood as arising from the probability that two lineages selected uniformly at random are both ancestral to the sample. This probability is the ratio of the number of ways to pick two lineages without replacement from the set of lineages and from the set of all infections: formula_44. Coalescent events will occur with this probability at the rate given by the incidence function formula_45.\n\nFor the simple SIR model, this yields\nThis expression is similar to the Kingman coalescent rate, but is damped by the fraction susceptible formula_33.\n\nEarly in an epidemic, formula_48, so for the SIR model\nThis has the same mathematical form as the rate in the Kingman coalescent, substituting formula_50. Consequently, estimates of effective population size based on the Kingman coalescent will be proportional to prevalence of infection during the early period of exponential growth of the epidemic.\n\nWhen a disease is no longer exponentially growing but has become endemic, the rate of lineage coalescence can also be derived for the epidemiological model governing the disease's transmission dynamics. This can be done by extending the Wright Fisher model to allow for unequal offspring distributions. With a Wright Fisher generation taking formula_51 units of time, the rate of coalescence is given by:\nwhere the effective population size formula_53 is the population size formula_6 divided by the variance of the offspring distribution formula_55. The generation time formula_51 for an epidemiological model at equilibrium is given by the duration of infection and the population size formula_6 is closely related to the equilibrium number of infected individuals. To derive the variance in the offspring distribution formula_55 for a given epidemiological model, one can imagine that infected individuals can differ from one another in their infectivities, their contact rates, their durations of infection, or in other characteristics relating to their ability to transmit the virus with which they are infected. These differences can be acknowledged by assuming that the basic reproduction number is a random variable formula_59 that varies across individuals in the population and that formula_59 follows some continuous probability distribution. The mean and variance of these individual basic reproduction numbers, formula_61 and formula_62, respectively, can then be used to compute formula_55. The expression relating these quantities is given by:\n\nFor example, for the SIR model above, modified to include births into the population and deaths out of the population, the population size formula_6 is given by the equilibrium number of infected individuals, formula_66. The mean basic reproduction number, averaged across all infected individuals, is given by formula_67, under the assumption that the background mortality rate is negligible compared to the rate of recovery formula_40. The variance in individuals' basic reproduction rates is given by formula_69, because the duration of time individuals remain infected in the SIR model is exponentially distributed. The variance in the offspring distribution formula_55 is therefore 2. formula_53 therefore becomes formula_72 and the rate of coalescence becomes:\nThis rate, derived for the SIR model at equilibrium, is equivalent to the rate of coalescence given by the more general formula provided by Volz et al. Rates of coalescence can similarly be derived for epidemiological models with superspreaders or other transmission heterogeneities, for models with individuals who are exposed but not yet infectious, and for models with variable infectious periods, among others. Given some epidemiological information (such as the duration of infection) and a specification of a mathematical model, viral phylogenies can therefore be used to estimate epidemiological parameters that might otherwise be difficult to quantify.\n\nAt the most basic level, the presence of geographic population structure can be revealed by comparing the genetic relatedness of viral isolates to geographic relatedness.\nA basic question is whether geographic character labels are more clustered on a phylogeny than expected under a simple nonstructured model (see figure 3).\nThis question can be answered by counting the number of geographic transitions on the phylogeny via parsimony, maximum likelihood or through Bayesian inference.\nIf population structure exists, then there will be fewer geographic transitions on the phylogeny than expected in a panmictic model.\nThis hypothesis can be tested by randomly scrambling the character labels on the tips of the phylogeny and counting the number of geographic transitions present in the scrambled data.\nBy repeatedly scrambling the data and calculating transition counts, a null distribution can be constructed and a \"p\"-value computed by comparing the observed transition counts to this null distribution.\n\nBeyond the presence or absence of population structure, phylodynamic methods can be used to infer the rates of movement of viral lineages between geographic locations and reconstruct the geographic locations of ancestral lineages.\nHere, geographic location is treated as a phylogenetic character state, similar in spirit to 'A', 'T', 'G', 'C', so that geographic location is encoded as a substitution model.\nThe same phylogenetic machinery that is used to infer models of DNA evolution can thus be used to infer geographic transition matrices.\nThe end result is a rate, measured in terms of years or in terms of nucleotide substitutions per site, that a lineage in one region moves to another region over the course of the phylogenetic tree.\nIn a geographic transmission network, some regions may mix more readily and other regions may be more isolated.\nAdditionally, some transmission connections may be asymmetric, so that the rate at which lineages in region 'A' move to region 'B' may differ from the rate at which lineages in 'B' move to 'A'.\nWith geographic location thus encoded, ancestral state reconstruction can be used to infer ancestral geographic locations of particular nodes in the phylogeny. These types of approaches can be extended by substituting other attributes for geographic locations. For example, in an application to rabies virus, Streicker and colleagues estimated rates of cross-species transmission by considering host species as the attribute.\n\nAs discussed above, it is possible to directly infer parameters of simple compartmental epidemiological models, such as SIR models, from sequence data by looking at genealogical patterns.\nAdditionally, general patterns of geographic movement can be inferred from sequence data, but these inferences do not involve an explicit model of transmission dynamics between infected individuals.\nFor more complicated epidemiological models, such as those involving cross-immunity, age structure of host contact rates, seasonality, or multiple host populations with different life history traits, it is often impossible to analytically predict genealogical patterns from epidemiological parameters.\nAs such, the traditional statistical inference machinery will not work with these more complicated models, and in this case, it is common to instead use a forward simulation-based approach.\n\nSimulation-based models require specification of a transmission model for the infection process between infected hosts and susceptible hosts and for the recovery process of infected hosts.\nSimulation-based models may be compartmental, tracking the numbers of hosts infected and recovered to different viral strains, or may be individual-based, tracking the infection state and immune history of every host in the population.\nGenerally, compartmental models offer significant advantages in terms of speed and memory usage, but may be difficult to implement for complex evolutionary or epidemiological scenarios.\nA forward simulation model may account for geographic population structure or age structure by modulating transmission rates between host individuals of different geographic or age classes.\nAdditionally, seasonality may be incorporated by allowing time of year to influence transmission rate in a stepwise or sinusoidal fashion.\n\nTo connect the epidemiological model to viral genealogies requires that multiple viral strains, with different nucleotide or amino acid sequences, exist in the simulation, often denoted formula_74 for different infected classes.\nIn this case, mutation acts to convert a host in one infected class to another infected class.\nOver the course of the simulation, viruses mutate and sequences are produced, from which phylogenies may be constructed and analyzed.\n\nFor antigenically variable viruses, it becomes crucial to model the risk of transmission from an individual infected with virus strain 'A' to an individual who has previously been infected with virus strains 'B', 'C', etc...\nThe level of protection against one strain of virus by a second strain is known as cross-immunity.\nIn addition to risk of infection, cross-immunity may modulate the probability that a host becomes infectious and the duration that a host remains infectious.\nOften, the degree of cross-immunity between virus strains is assumed to be related to their sequence distance.\n\nIn general, in needing to run simulations rather than compute likelihoods, it may be difficult to make fine-scale inferences on epidemiological parameters, and instead, this work usually focuses on broader questions, testing whether overall genealogical patterns are consistent with one epidemiological model or another. Additionally, simulation-based methods are often used to validate inference results, providing test data where the correct answer is known ahead of time. Because computing likelihoods for genealogical data under complex simulation models has proven difficult, an alternative statistical approach called Approximate Bayesian Computation (ABC) is becoming popular in fitting these simulation models to patterns of genetic variation, following successful application of this approach to bacterial diseases. This is because ABC makes use of easily computable summary statistics to approximate likelihoods, rather than the likelihoods themselves.\n\nHuman influenza is an acute respiratory infection primarily caused by viruses influenza A and influenza B.\nInfluenza A viruses can be further classified into subtypes, such as A/H1N1 and A/H3N2.\nHere, subtypes are denoted according to their hemagglutinin (H or HA) and neuraminidase (N or NA) genes, which as surface proteins, act as the primary targets for the humoral immune response.\nInfluenza viruses circulate in other species as well, most notably as swine influenza and avian influenza.\nThrough reassortment, genetic sequences from swine and avian influenza occasionally enter the human population.\nIf a particular hemagglutinin or neuraminidase has been circulating outside the human population, then humans will lack immunity to this protein and an influenza pandemic may follow a host switch event, as seen in 1918, 1957, 1968 and 2009.\nAfter introduction into the human population, a lineage of influenza generally persists through antigenic drift, in which HA and NA continually accumulate mutations allowing viruses to infect hosts immune to earlier forms of the virus.\nThese lineages of influenza show recurrent seasonal epidemics in temperate regions and less periodic transmission in the tropics.\nGenerally, at each pandemic event, the new form of the virus outcompetes existing lineages.\nThe study of viral phylodynamics in influenza primarily focuses on the continual circulation and evolution of epidemic influenza, rather than on pandemic emergence.\nOf central interest to the study of viral phylodynamics is the distinctive phylogenetic tree of epidemic influenza A/H3N2, which shows a single predominant trunk lineage that persists through time and side branches that persist for only 1–5 years before going extinct (see figure 5).\n\nPhylodynamic techniques have provided insight into the relative selective effects of mutations to different sites and different genes across the influenza virus genome.\nThe exposed location of hemagglutinin (HA) suggests that there should exist strong selective pressure for evolution to the specific sites on HA that are recognized by antibodies in the human immune system.\nThese sites are referred to as epitope sites.\nPhylogenetic analysis of H3N2 influenza has shown that putative epitope sites of the HA protein evolve approximately 3.5 times faster on the trunk of the phylogeny than on side branches (see figure 5).\nThis suggests that viruses possessing mutations to these exposed sites benefit from positive selection and are more likely than viruses lacking such mutations to take over the influenza population.\nConversely, putative nonepitope sites of the HA protein evolve approximately twice as fast on side branches than on the trunk of the H3 phylogeny, indicating that mutations to these sites are selected against and viruses possessing such mutations are less likely to take over the influenza population.\nThus, analysis of phylogenetic patterns gives insight into underlying selective forces.\nA similar analysis combining sites across genes shows that while both HA and NA undergo substantial positive selection, internal genes show low rates of amino acid fixation relative to levels of polymorphism, suggesting an absence of positive selection.\n\nFurther analysis of HA has shown it to have a very small effective population size relative to the census size of the virus population, as expected for a gene undergoing strong positive selection.\nHowever, across the influenza genome, there is surprisingly little variation in effective population size; all genes are nearly equally low.\nThis finding suggests that reassortment between segments occurs slowly enough, relative to the actions of positive selection, that genetic hitchhiking causes beneficial mutations in HA and NA to reduce diversity in linked neutral variation in other segments of the genome.\n\nInfluenza A/H1N1 shows a larger effective population size and greater genetic diversity than influenza H3N2, suggesting that H1N1 undergoes less adaptive evolution than H3N2.\nThis hypothesis is supported by empirical patterns of antigenic evolution; there have been nine vaccine updates recommended by the WHO for H1N1 in the interpandemic period between 1978 and 2009, while there have been 20 vaccine updates recommended for H3N2 during this same time period.\nAdditionally, an analysis of patterns of sequence evolution on trunk and side branches suggests that H1N1 undergoes substantially less positive selection than H3N2.\nHowever, the underlying evolutionary or epidemiological cause for this difference between H3N2 and H1N1 remains unclear.\n\nThe extremely rapid turnover of the influenza population means that the rate of geographic spread of influenza lineages must also, to some extent, be rapid.\nSurveillance data show a clear pattern of strong seasonal epidemics in temperate regions and less periodic epidemics in the tropics. The geographic origin of seasonal epidemics in the Northern and Southern Hemispheres had been a major open question in the field.\nHowever, recent work by Rambaut et al. and Russell et al. has shown that temperate epidemics usually emerge from a global reservoir rather than emerging from within the previous season's genetic diversity.\nThis work, and more recent work by Bedford et al. and Bahl et al., has suggested that the global persistence of the influenza population is driven by viruses being passed from epidemic to epidemic, with no individual region in the world showing continual persistence.\nHowever, there is considerable debate regarding the particular configuration of the global network of influenza, with one hypothesis suggesting a metapopulation in East and Southeast Asia that continually seeds influenza in the rest of the world, and another hypothesis advocating a more global metapopulation in which temperate lineages often return to the tropics at the end of a seasonal epidemic.\n\nAll of these phylogeographic studies necessarily suffer from limitations in the worldwide sampling of influenza viruses.\nFor example, the relative importance of tropical Africa and India has yet to be uncovered.\nAdditionally, the phylogeographic methods used in these studies (see section on phylogeographic methods) make inferences of the ancestral locations and migration rates on only the samples at hand, rather than on the population in which these samples are embedded.\nBecause of this, study-specific sampling procedures are a concern in extrapolating to population-level inferences.\nHowever, through joint epidemiological and evolutionary simulations, Bedford et al. show that their estimates of migration rates appear robust to a large degree of undersampling or oversampling of a particular region.\nFurther methodological progress is required to more fully address these issues.\n\nForward simulation-based approaches for addressing how immune selection can shape the phylogeny of influenza A/H3N2's hemagglutinin protein have been actively developed by disease modelers since the early 2000s.\nThese approaches include both compartmental models and agent-based models.\nOne of the first compartmental models for influenza was developed by Gog and Grenfell, who simulated the dynamics of many strains with partial cross-immunity to one another.\nUnder a parameterization of long host lifespan and short infectious period, they found that strains would form self-organized sets that would emerge and replace one another.\nAlthough the authors did not reconstruct a phylogeny from their simulated results, the dynamics they found were consistent with a ladder-like viral phylogeny exhibiting low strain diversity and rapid lineage turnover.\n\nLater work by Ferguson and colleagues adopted an agent-based approach to better identify the immunological and ecological determinants of influenza evolution.\nThe authors modeled influenza's hemagglutinin as four epitopes, each consisting of three amino acids.\nThey showed that under strain-specific immunity alone (with partial cross-immunity between strains based on their amino acid similarity), the phylogeny of influenza A/H3N2's HA was expected to exhibit 'explosive genetic diversity', a pattern that is inconsistent with empirical data.\nThis led the authors to postulate the existence of a temporary strain-transcending immunity: individuals were immune to reinfection with any other influenza strain for approximately six months following an infection.\nWith this assumption, the agent-based model could reproduce the ladder-like phylogeny of influenza A/H3N2's HA protein.\n\nWork by Koelle and colleagues revisited the dynamics of influenza A/H3N2 evolution following the publication of a paper by Smith and colleagues which showed that the antigenic evolution of the virus occurred in a punctuated manner. The phylodynamic model designed by Koelle and coauthors argued that this pattern reflected a many-to-one genotype-to-phenotype mapping, with the possibility of strains from antigenically distinct clusters of influenza sharing a high degree of genetic similarity.\nThrough incorporating this mapping of viral genotype into viral phenotype (or antigenic cluster) into their model, the authors were able to reproduce the ladder-like phylogeny of influenza's HA protein without generalized strain-transcending immunity.\nThe reproduction of the ladder-like phylogeny resulted from the viral population passing through repeated selective sweeps.\nThese sweeps were driven by herd immunity and acted to constrain viral genetic diversity.\n\nInstead of modeling the genotypes of viral strains, a compartmental simulation model by Gökaydin and colleagues considered influenza evolution at the scale of antigenic clusters (or phenotypes).\nThis model showed that antigenic emergence and replacement could result under certain epidemiological conditions.\nThese antigenic dynamics would be consistent with a ladder-like phylogeny of influenza exhibiting low genetic diversity and continual strain turnover.\n\nIn recent work, Bedford and colleagues used an agent-based model to show that evolution in a Euclidean antigenic space can account for the phylogenetic pattern of influenza A/H3N2's HA, as well as the virus's antigenic, epidemiological, and geographic patterns.\nThe model showed the reproduction of influenza's ladder-like phylogeny depended critically on the mutation rate of the virus as well as the immunological distance yielded by each mutation.\n\nAlthough most research on the phylodynamics of influenza has focused on seasonal influenza A/H3N2 in humans, influenza viruses exhibit a wide variety of phylogenetic patterns.\nQualitatively similar to the phylogeny of influenza A/H3N2’s hemagglutinin protein (see figure 5), influenza A/H1N1 exhibits a ladder-like phylogeny with relatively low genetic diversity at any point in time and rapid lineage turnover.\nHowever, the phylogeny of influenza B's hemagglutinin protein has two circulating lineages: the Yamagata and the Victoria lineage.\nIt is unclear how the population dynamics of influenza B contribute to this evolutionary pattern, although one simulation model has been able to reproduce this phylogenetic pattern with longer infectious periods of the host.\n\nGenetic and antigenic variation of influenza is also present across a diverse set of host species.\nThe impact of host population structure can be seen in the evolution of equine influenza A/H3N8: instead of a single trunk with short side-branches, the hemagglutinin of influenza A/H3N8 splits into two geographically distinct lineages, representing American and European viruses.\nThe evolution of these two lineages is thought to have occurred as a consequence of quarantine measures.\nAdditionally, host immune responses are hypothesized to modulate virus evolutionary dynamics.\nSwine influenza A/H3N2 is known to evolve antigenically at a rate that is six times slower than that of the same virus circulating in humans, although these viruses' rates of genetic evolution are similar.\nInfluenza in aquatic birds is hypothesized to exhibit 'evolutionary stasis', although recent phylogenetic work indicates that the rate of evolutionary change in these hosts is similar to those in other hosts, including humans.\nIn these cases, it is thought that short host lifespans prevent the build-up of host immunity necessary to effectively drive antigenic drift.\n\nThe global diversity of HIV-1 group M is shaped by its origins in Central Africa around the turn of the 20th century.\nThe epidemic underwent explosive growth throughout the early 20th century with multiple radiations out of Central Africa.\nWhile traditional epidemiological surveillance data are almost nonexistent for the early period of epidemic expansion, phylodynamic analyses based on modern sequence data can be used to estimate when the epidemic began and to estimate the early growth rate.\nThe rapid early growth of HIV-1 in Central Africa is reflected in the star-like phylogenies of the virus (caricatured in figure 2), with most coalescent events occurring in the distant past. Multiple founder events have given rise to distinct HIV-1 group M subtypes which predominate in different parts of the world.\nSubtype B is most prevalent in North America and Western Europe, while subtypes A and C, which account for more than half of infections worldwide, are common in Africa.\nHIV subtypes differ slightly in their transmissibility, virulence, effectiveness of antiretroviral therapy, and pathogenesis.\n\nThe rate of exponential growth of HIV in Central Africa in the early 20th century preceding the establishment of modern subtypes has been estimated using coalescent approaches. Several estimates based on parametric exponential growth models are shown in table 1, for different time periods, risk groups and subtypes. The early spread of HIV-1 has also been characterized using nonparametric (\"skyline\") estimates of formula_53.\n\nThe early growth of subtype B in North America was quite high,\nhowever, the duration of exponential growth was relatively short, with saturation occurring in the mid- and late-1980s.\nAt the opposite extreme, HIV-1 group O, a relatively rare group that is geographically confined to Cameroon and that is mainly spread by heterosexual sex, has grown at a lower rate than either subtype B or C.\n\nHIV-1 sequences sampled over a span of five decades have been used with relaxed molecular clock phylogenetic methods to estimate the time of cross-species viral spillover into humans around the early 20th century.\nThe estimated TMRCA for HIV-1 coincides with the appearance of the first densely populated large cities in Central Africa.\nSimilar methods have been used to estimate the time that HIV originated in different parts of the world.\nThe origin of subtype B in North America is estimated to be in the 1960s, where it went undetected until the AIDS epidemic in the 1980s.\nThere is evidence that progenitors of modern subtype B originally colonized the Caribbean before undergoing multiple radiations to North and South America.\nSubtype C originated around the same time in Africa.\n\nAt shorter time scales and finer geographical scales, HIV phylogenies may reflect epidemiological dynamics related to risk behavior and sexual networks.\nVery dense sampling of viral sequences within cities over short periods of time has given a detailed picture of HIV transmission patterns in modern epidemics.\nSequencing of virus from newly diagnosed patients is now routine in many countries for surveillance of drug resistance mutations, which has yielded large databases of sequence data in those areas.\nThere is evidence that HIV transmission within heterogeneous sexual networks leaves a trace in HIV phylogenies, in particular making phylogenies more imbalanced and concentrating coalescent events on a minority of lineages.\n\nBy analyzing phylogenies estimated from HIV sequences from men who have sex with men in London, United Kingdom, Lewis et al. found evidence that transmission is highly concentrated in the brief period of primary HIV infection (PHI), which consists of approximately the first 6 months of the infectious period.\nIn a separate analysis, Volz et al. found that simple epidemiological dynamics explain phylogenetic clustering of viruses collected from patients with PHI.\nPatients who were recently infected were more likely to harbor virus that is phylogenetically close to samples from other recently infected patients. Such clustering is consistent with observations in simulated epidemiological dynamics featuring an early period of intensified transmission during PHI. These results therefore provided further support for Lewis et al.'s findings that HIV transmission occurs frequently from individuals early in their infection.\n\nPurifying immune selection dominates evolution of HIV within hosts, but evolution between hosts is largely decoupled from within-host evolution (see figure 6).\nImmune selection has relatively little influence on HIV phylogenies at the population level for three reasons.\nFirst, there is an extreme bottleneck in viral diversity at the time of sexual transmission. Second, transmission tends to occur early in infection before immune selection has had a chance to operate. Finally, the replicative fitness of a viral strain (measured in transmissions per host) is largely extrinsic to virological factors, depending more heavily on behaviors in the host population. These include heterogeneous sexual and drug-use behaviors.\n\nThere is some evidence from comparative phylogenetic analysis and epidemic simulations that HIV adapts at the level of the population to maximize transmission potential between hosts. This adaptation is towards intermediate virulence levels, which balances the productive lifetime of the host (time until AIDS) with the transmission probability per act. A useful proxy for virulence is the set-point viral load (SPVL), which is correlated with the time until AIDS. SPVL is the quasi-equilibrium titer of viral particles in the blood during chronic infection. For adaptation towards intermediate virulence to be possible, SPVL needs to be heritable and a trade-off between viral transmissibility and the lifespan of the host needs to exist. SPVL has been shown to be correlated between HIV donor and recipients in transmission pairs, thereby providing evidence that SPVL is at least partly heritable. The transmission probability of HIV per sexual act is positively correlated with viral load, thereby providing evidence of the trade-off between transmissibility and virulence. It is therefore theoretically possible that HIV evolves to maximize its transmission potential. Epidemiological simulation and comparative phylogenetic studies have shown that adaptation of HIV towards optimum SPVL could be expected over 100–150 years. These results depend on empirical estimates for the transmissibility of HIV and the lifespan of hosts as a function of SPVL.\n\nUp to this point, phylodynamic approaches have focused almost entirely on RNA viruses, which often have mutation rates on the order of 10 to 10 substitutions per site per year.\nThis allows a sample of around 1000 bases to have power to give a fair degree of confidence in estimating the underlying genealogy connecting sampled viruses.\nHowever, other pathogens may have significantly slower rates of evolution.\nDNA viruses, such as herpes simplex virus, evolve orders of magnitude more slowly.\nThese viruses have commensurately larger genomes.\nBacterial pathogens such as pneumococcus and tuberculosis evolve slower still and have even larger genomes.\nIn fact, there exists a very general negative correlation between genome size and mutation rate across observed systems.\nBecause of this, similar amounts of phylogenetic signal are likely to result from sequencing full genomes of RNA viruses, DNA viruses or bacteria.\nAs sequencing technologies continue to improve, it is becoming increasingly feasible to conduct phylodynamic analyses on the full diversity of pathogenic organisms.\n\nAdditionally, improvements in sequencing technologies will allow detailed investigation of within-host evolution, as the full diversity of an infecting quasispecies may be uncovered given enough sequencing effort.\n\n", "id": "38889813", "title": "Viral phylodynamics"}
{"url": "https://en.wikipedia.org/wiki?curid=19467882", "text": "Urbilaterian\n\nThe urbilaterian (from German ur- 'original') is the hypothetical last common ancestor of the bilaterian clade, i.e., all animals having a bilateral symmetry.\n\nIts appearance is a matter of debate, for no representative has been (or is ever likely to be) identified in the fossil record; the reconstructed morphology that it would display largely depends on whether the bilaterian clade is defined as including the acoelomorpha or not. Since all protostomes and deuterostomes share features, such as blood circulation systems and guts, that are useful only in relatively large (macroscopic) organisms, their common ancestor ought also to have been macroscopic. However, such large animals should have left traces in the sediment in which they moved, and evidence of such traces first appear relatively late in the fossil record — long after the urbilaterian would have lived. This leads to suggestions of a small urbilaterian, which is the supposed state of the ancestor of protostomes, deuterostomes and acoelomorphs.\n\nThe first evidence of bilateria in the fossil record comes from trace fossils in sediments towards the end of the Ediacaran period (about ), and the first fully accepted fossil of a bilaterian \"organism\" is \"Kimberella\", dating to . There are earlier, controversial fossils: \"Vernanimalcula\" has been interpreted as a bilaterian, but may simply represent a fortuitously infilled bubble. Fossil embryos are known from around the time of \"Vernanimalcula\" (), but none of these have bilaterian affinities. This may reflect a genuine absence of bilateria, but caution is due — it could be that bilateria didn't lay eggs in sediment, where they would be likely to fossilise.\n\nMolecular techniques can generate expected dates of the divergence between the bilaterian clades, and thus an assessment of when the urbilaterian lived. These dates have huge margins of error, though they are becoming more accurate with time. More recent estimates are compatible with an Ediacaran bilaterian, although it is possible, especially if early bilaterians were small, that the bilateria had a long cryptic history before they left any evidence in the fossil record.\n\nThe urbilaterian (from German ur- 'original') is the hypothetical last common ancestor of the bilaterian clade, i.e., all animals having a bilateral symmetry. \nIts appearance is a matter of debate, for no representative has been (or is ever likely to be) identified in the fossil record; the reconstructed morphology that it would display largely depends on whether the bilaterian clade is defined as including the acoelomorpha or not. Since all protostomes and deuterostomes share features, such as blood circulation systems and guts, that are useful only in relatively large (macroscopic) organisms, their common ancestor ought also to have been macroscopic. However, such large animals should have left traces in the sediment in which they moved, and evidence of such traces first appear relatively late in the fossil record — long after the urbilaterian would have lived. This leads to suggestions of a small urbilaterian, which is the supposed state of the ancestor of protostomes, deuterostomes and acoelomorphs. A 2 cm long urbilaterian sea worm fossil has been recently found in Peniche, Portugal displaying characteristics that strongly suggest it is the ancestor of both molluscs and chordates.\n\nLight detection (photosensitivity) is present in organisms as simple as seaweeds; the definition of a true eye varies, but in general eyes must have directional sensitivity, and thus have screening pigments so only light from the target direction is detected. Thus defined, they need not consist of more than one photoreceptor cell.\n\nThe presence of genetic machinery (the \"Pax6\" and \"Six\" genes) common to eye formation in all bilaterians suggests that this machinery - and hence eyes - was present in the urbilaterian. The most likely candidate eye type is the simple pigment-cup eye, which is the most widespread among the bilateria.\n\nSince two types of opsin, the c-type and r-type, are found in all bilaterians, the urbilaterian must have possessed both types - although they may not have been found in a centralised eye, but used to synchronise the body clock to daily or lunar variations in lighting.\n\nProponents of a complex urbilaterian point to the shared features and genetic machinery common to all bilateria. They argue that (1) since these are similar in so many respects, they could have evolved only once; and (2) since they are common to all bilateria, they must have been present in the ancestral bilaterian animal.\n\nHowever, as biologists' understanding of the major bilaterian lineages increases, it is beginning to appear that some of these features may have evolved independently in each lineage. Further, the bilaterian clade has recently been expanded to include the acoelomorphs — a group of relatively simple flatworms. This lineage lacks key bilaterian features, and if it truly does reside within the bilaterian \"family\", many of the features listed above are no longer common to all bilateria. Instead, some features — such as segmentation and possession of a heart — are restricted to a sub-set of the bilateria, the deuterostomes and protostomes. Their last common ancestor would still have to be large and complex, but the bilaterian ancestor could be much simpler. However, some scientists stop short of including the acoelomorph clade in the bilateria. This shifts the position of the cladistic node which is being discussed; consequently the urbilaterian in this context is farther out the evolutionary tree and is more derived than the common ancestor of deuterostomes, protostomes \"and\" acoelomorphs.\n\nGenetic reconstructions are unfortunately not much help. They work by considering the \"genes\" common to all bilateria, but problems arise because very similar genes can be co-opted for different roles. For instance, the gene Pax6 has a key role in eye development, but is absent in some animals with eyes; some cnidaria have genes which in bilateria control the development of a layer of cells that the cnidaria don't even possess. This means that even if a gene can be identified as present in the urbilaterian, we can't tell what the gene was coding for. Before this was realised, genetic reconstructions implied a surprisingly complex urbilateria.\n\nThe absence of a fossil record gives a starting point for the reconstruction — the urbilaterian must have been small enough not to leave any traces as it moved over or lived in the sediment surface. This means it must have been well below a centimetre in length. As all Cambrian animals are marine, it is reasonable to assume that the urbilaterian was too.\n\nFurthermore, a reconstruction of the urbilateria must rest on identifying morphological similarities between all bilateria. While some bilateria live attached to a substrate, this appears to be a secondary adaptation, and the urbilaterian was probably mobile. Its nervous system was probably dispersed, but with a small central \"brain\". Since acoelomorphs lack a heart, coelom or organs, the urbilaterian probably did too — it would presumably have been small enough for diffusion to do the job of transporting compounds through the body. A small, narrow gut was probably present, which would have had only one opening — a combined mouth and anus. \nFunctional considerations suggest that the surface of the bilaterian was probably covered with cilia, which it could have used for locomotion or feeding.\n\nThere is still no consensus on whether the characteristics of the deuterostomes and protostomes evolved once or many times. Features such as a heart and blood circulation system may therefore not have been present even in the deuterostome-protostome ancestor, which would mean that this too could have been small (hence explaining the lack of fossil record).\n\n", "id": "19467882", "title": "Urbilaterian"}
{"url": "https://en.wikipedia.org/wiki?curid=23251776", "text": "Acquired characteristic\n\nAn acquired characteristic is a non-heritable change in a function or structure of a living biotic material caused after birth by disease, injury, accident, deliberate modification, variation, repeated use, disuse, or misuse, or other environmental influences. Acquired traits, which is synonymous with acquired characteristics, are not passed on to offspring through reproduction alone.\n\nThe changes that constitute acquired characteristics can have many manifestations and degrees of visibility but they all have one thing in common: they change a facet of a living organisms' function or structure after the organism has left the womb.\n\n\nAcquired characteristics can be minor and temporary like bruises, blisters, shaving body hair, and body building. Permanent but inconspicuous or invisible ones are corrective eye surgery and organ transplant or removal.\nSemi-permanent but inconspicuous or invisible traits are vaccinations and laser hair removal. Perms, tattoos, scars, and amputations are semi-permanent and highly visible.\n\nApplying makeup and nailpolish, dying one's hair or applying henna to the skin, and tooth whitening are not examples of acquired traits. They change the appearance of a facet of an organism, but do not change the structure or functionality.\n\nInheritance of acquired characters was historically proposed by renowned theorists such as Hippocrates, Aristotle, and French naturalist Jean-Baptiste Lamarck. Conversely, this hypothesis was denounced by other renowned theorists such as Charles Darwin.\nToday, although Lamarckism is generally discredited, there is still debate on whether \"some\" acquired characteristics in organisms are actually inheritable.\n\nAcquired characteristics, by definition, are characteristics that are gained by an organism after birth as a result of external influences or its own activities that change its structure or function and cannot be inherited. Therefore, every condition an organism is born with must be considered an inherited characteristic.\n\nInherited characteristics, by definition, are characteristics that are gained or predisposed to by an organism as a result of genetic transmission from its parents and will be passed to the organism's offspring. Therefore, every condition an organism does not gain or develop because of inheritance of its parents' genetic information must be considered an acquired characteristic.\n\nIt is fairly common for mammalian eyes to change color in the first years of life. This happens, with human infants and kittens being some well-known examples, because the eyes of the baby, just like the rest of its body, are still developing. This change can be as simple as blue to brown, or can involve multiple color changes in which neither the child's parents nor his/her doctors know when the changes will stop and what the final eye color will be.\n\nChanges in eye color signal changes in the arrangement and concentration of pigment in the iris, which is an example of structural color. Even though this change happens after birth, it is strictly as result of genes. While changes in eye appearance (and function, and structure) that occur because of acquired characteristics like injury, illness, old age, or malnutrition are definitely acquired characteristics, the infantile color change as described above is \"usually\" considered inherited.\nWhen diseases are caused by environmental influences, such as iodine deficiency or lead poisoning, their resultant symptoms are agreed to be acquired characteristics. However, it is debatable whether changes in bodily functions due to disorders that are partly or wholly genetic in origin are actually \"acquired\".\n\nWholly genetic disorders, such as Huntingtons, are ticking time-bombs that are inherited from parents' genes and are present before birth but the symptoms that develop, aka are \"acquired\", after birth are simply delayed manifestations of the inherited trait.\n\nDisorders that are partially genetic, such as ALS and allergies, mean the organism has inherited a predisposition to develop, aka \"acquire\", a certain condition but that inherited increased likelihood can be reduced or further increased depending on acquired characteristics of the organism.\n\nNew mutations, (often somatic, spontaneous and sporadic), not inherited from either parent are called de novo mutations. The consensus on whether certain prenatal spontaneous mutations and genetic disorders that occur as a result of meiotic and chromosome errors or during cell division after conception, like Cystic fibrosis and Down syndrome, are considered to be acquired or inherited is unclear. Mutations and meiotic errors can be considered inherited since the organism is born with them \"in its genes\", but they can also be seen as prenatal acquired characteristics since they are \"not actually inherited\" from its parents.\nWith de novo mutations and division errors, the relationship between the offspring's altered genes and gene inheritance from the parents is technically spurious. These genetic errors can affect the mind as well as the body and can result in schizophrenia, autism, bi-polar disorder\n, and cognitive disabilities.\n\nThe definitions of inherited and acquired characteristics leave a for trauma, pre-existing and gestational maternal conditions that affect the fetus, as well as chemical and pathogen exposures and trauma that happen before and while an organism is born, such as AIDS, syphilis, Hepatitis B, chickenpox, rubella, unregulated gestational diabetes, and fetal alcohol syndrome. Most infections won't affect a fetus if the pregnant mother contracts it, but some can be transmitted to babies via the placenta or during birth, and others cause more severe symptoms in pregnant women or can cause complications to the pregnancy.\n\nThe World Health Organization defines health as a state of complete physical, mental and social well-being, and not merely the absence of disease or infirmity.\nAcquired characteristics don't need to affect the health of an organism, (a scar, suntan, or perm) but examples that do are often the first that come to mind when thinking of acquired characteristics since they are the easiest to observe and the ones that we, ourselves, are most familiar with.\n\nPhysical acquired characteristics can stem from various environmental influences such as disease, modification, injury, and regular or infrequent use of body parts.\n\nMental traits are acquired by learning and adapting native traits to the environment of the individual.\n\nLittle Albert experiment\n\ntrauma trigger\n\ncognitive\n\ncharacter\n\nBehavior\n\nSocialization\n\nParenting style\n\nSociological Illness\n\nSentiments are the result of the compounding of primary emotions, being \"bound up with knowledge and ideas.\" Only through vast experience in the natural world can humans learn to recognize objects in all of the various orientations in which we encounter them on a day-to-day basis. The ability to do something is an acquired characteristic, since a skill comes from one's knowledge, practice, aptitude, etc.\n\nMany acquired characteristics, such as injuries and diseases, directly affect only the mind but indirectly affect the body or directly affect only the body but indirectly affect the mind.\n\nSuppose you develop an articulation disorder after being hit in the head with a fastball from a Pro baseball pitcher. Once your concussion wears off, this is your \"only\" symptom. There's nothing wrong with your body, and nothing wrong with most of your mind but you stutter and slur and mispronounce your words so that people have a very hard time understanding you.\n\nHow would such a condition affect your life if you were a child?\nA child would be teased mercilessly by peers and have trouble communicating with friends and family.\n\nA student would not be able to communicate easily with classmates (collaborative assignments), teachers, tutors, and certain classes (choir, public speaking).\n\nWhat if you had a job where you needed to be able to talk to people? You wouldn't be able to easily talk to customers, other employees, or your boss. You couldn't answer phones or make calls, greet customers, do presentations.\nYou couldn't call your pet's name. You wouldn't even be able to use a fast food drive-through or tell your waitress what you wanted to order! You couldn't say, \"I love you,\" to your child, parent, or significant other.\n\nHow would you cope with the functional disorder? With the changes and the stress those changes caused?\n\nThe example in the above link deals with the loss of a single mental function. Imagine what would happen if a part of your body stopped working. Even something as simple as a broken finger causes many changes in lifestyle to compensate for the difference in function: writing, typing, drawing, driving, hobbies like gardening, fishing, or sports, using video game and TV controls, texting and dialing phone numbers, using cutlery to eat, opening a bag of chips or soda can, preparing food for yourself or your family, playing with your child or pet, unlocking and opening doors, bathing, brushing your teeth and hair, dressing yourself, typing your shoes, and more.\n\nThe person unable to pronounce words clearly would suffer additional stress, anxiety, frustration, and emotional upset from his functional limitation. There would be anxiety over financial hardships like bills from medical exams and tests, and a possible leave of absence or termination from work.\nPhysical effects of your articulation disorder could be paper cuts from your notepad, headaches over the stress, stubbed toes and sprained ankles from rushing to where you accidentally left you notepad, pulling a muscle from hauling a laptop around, etc. Worry could induce ulcers.\nIf you have to cut back on spending, your (and your family's) nutrition could suffer. There would be less money for fine dining, going out with friends, and indulging in toys and hobbies.\n\nThose close to you - family, friends, lovers - would be negatively affected by your limitation and your relationship with them would suffer as well.\n\nThere are four main types of disease: pathogenic disease, deficiency disease, hereditary disease, and physiological disease.\n\nCongenital disorders, known more commonly as birth defects, are DEFINITION.\n\nIt is possible for an unborn child to contract fetal diseases and perinatal infections.\n\nSuppose a pregnant woman is shot in her abdomen and the bullet hits the fetus, causing a non-fatal injury. Depending on the length of time between injury and birth, the baby could be born either with a still-healing wound or with a fully healed scar.\nRegardless of its state of healing, the gunshot wound could be considered an inherited characteristic, since the baby is born with the condition, or could be considered an acquired characteristic, since the condition did not happen as a result of genes inherited from its parents.\n\nThe gray area is that a prenatal gunshot wound obviously isn't an inherited, \"genetic\" condition but it can't be an acquired trait because it didn't happen \"after\" the baby's birth.\n\nHormones are chemicals released by a cell or a gland in one part of the body that affect cells in other parts of the organism.\n\nChemicals are substances with distinct molecular compositions that are produced by or used in a chemical process.\n\nDrugs are any substance that, when absorbed into the body of a living organism, alter normal bodily function. Drugs can be legal and be used as is medically prescribed by a doctor or be illegal to buy, sell, possess, and administer. They can be naturally occurring (such as caffeine and cannabis) or man-made (such as Chlorpromazine). Both legal and illegal drugs can be misused and abused. They have varying levels of dependence and some can cause addiction.\n\nWorth noting is the importance of prenatal nutrition to proper mental and physical development.\npregnancy\nPrenatal nutrition and birth weight\n\nRisk factors in pregnancy\n\nPremature birth\n\nPregnancy#Concomitant diseases\nMaternal health Complications of pregnancy\n\nAIDS\n\nThyroid disease in pregnancy\nGestational diabetes\n\nSmall for gestational age\nLarge for gestational age\nFailure to thrive\n\nSmoking and pregnancy\nHealth effects of tobacco\nComplications related to amniotic fluid\nMaternal physiological changes in pregnancy\n\nPrenatal stress\nPrenatal memory\n\nImmune tolerance in pregnancy\nPassive immunity\nVertical transmission\n\nNutrition and pregnancy\nVitamin B12 deficiency\n\nMirror syndrome\n\nReproductive health\n\nNon-Mendelian inheritance\nMaternal influence on sex determination\n\nThere is also reason to believe that the immune system of a baby will be healthier if, during pregnancy, the mother's immune system was regularly stimulated by exposure to pathogens.\nAcquired disorder\n\nSeason of birth\n\nIt is posited that the absence of exposure to parasites, bacteria, and viruses is playing a significant role in the development of autoimmune diseases in the more sanitized Western industrialized nations. Lack of exposure to naturally occurring pathogens may result in an increased incidence of autoimmune diseases. (See hygiene hypothesis.)\n\nA complete explanation of how environmental factors play a role in autoimmune diseases has still not been proposed. However epidemiological studies, such as the meta analysis by Leonardi-Bee, et al., have helped to establish the link between parasitic infestation and autoimmune disease development, in other words, exposure to parasites reduces incidence of an autoimmune disease developing.\n\"Early life exposure to microbes (i.e., germs) is an important determinant of adulthood sensitivity to allergic and autoimmune diseases such as hay fever, asthma and inflammatory bowel disease.\"\n\n\"Immunological diseases, such as eczema and asthma, are on the increase in westernized society and represent a major challenge for 21st century medicine. ...[G]rowing up on a farm directly affects the regulation of the immune system and causes a reduction in the immunological responses to food proteins,\" which not only means less severe reactions to food allergies, lactose intolerance, gluten sensitivity, etc., but reductions in the likelihood of developing them in the first place.\n\nDevelopmental impact of child neglect in early childhood\n\nSexuoerotic tragedy\n\nDisease is any condition that impairs the normal physical or mental (or both) function of an organism. (Though this definition includes injuries, it will not be discussed here). Diseases can arise from infection, environmental conditions, accidents, and inherited diseases.\n\nIt is not always easy to classify the source of a health problem. For instance, people can develop gout, which is known to cause permanent or near permanent changes to the human body, because of diet, inherited genetic predisposition, as a secondary condition from other diseases, or as an unintended side effect of certain medications.\n\nInfectious diseases can be caused by pathogens and microorganisms such as viruses, prions, bacteria, parasites, and fungi.\n\nFor infectious, environmental, and genetically predisposed conditions,\nlifestyle choices such as exercise, nutrition, stress level, hygiene, home and work environments, use or abuse of legal and illegal drugs, and access to healthcare (including an individual's financial ability and personal willingness to seek medical attention) especially in the early stages of an illness all combine to determine a person's risk factors for developing a disease or condition.\n\nPrecancerous condition\nProgressive disease\nlocalized disease to spread to other area of the body.\n\nOutline of nutrition\n\ncougar with shortened legs at Big Cat Rescue\n\nPoor nutrition and frequent injury and disease can reduce the individual's adult stature, but the best environment cannot cause growth to a greater stature than is determined by heredity.\n\nVitamin K deficiency\nmalnutrition\nNutrition disorder\n\nSleep debt\n\nTrauma is \"a body wound or shock produced by sudden physical injury, as from violence or accident,\" or, more simply put, is \"a physical wound or injury, such as a fracture or blow.\"\n\nAccidental injuries, most of which can be predicted and thus prevented, are the unintentional negative outcomes of unforeseen or unplanned events or circumstances which may have been avoided or prevented if reasonable measures had been taken or if the risks involving the circumstances leading up to the accident been recognized and acted upon (minimized).\n\nBattery is a criminal offense involving the use of force against another that results in harmful or offensive contact. (Assault is fear/belief of impending battery.)\nViolence is defined by the WHO as the intentional use of physical force or power, threatened or actual, against oneself or others that either results in or has a high likelihood of resulting in injury, death, psychological harm, maldevelopment or deprivation.\nConsent\n\nHead trauma in the form of a traumatic brain injury, stroke, drug or alcohol abuse, and infection have been known in some cases to cause changes to a person's mental processes, the most common being amnesia, ability to deal with stress and changes in aggression. There have also been documented cases of a person's personality changing more drastically, the best-known case being Phineas Gage, who in 1848 who survived a 1.1 meter long tamping iron being driven through his skull (though almost all presentations of Gage's subsequent personality changes are grossly exaggerated).\n\nThere is also the rare condition called Foreign Accent Syndrome in which someone who has suffered a brain injury will appear to speak in a new language or dialect. This is typically thought to be due to an injury to the linguistic center of the brain causing speech impairment that just happens to sound like a persons non-native language. This is thought to be the reasoning behind the urban legend where someone wakes from a coma or surgery and suddenly speaks a new language.\n\nBody modification is the deliberate altering of the human body for any non-medical reason, such as aesthetics, sexual enhancement, a rite of passage, religious reasons, to display group membership or affiliation, to create body art, shock value, or self-expression.\n\nThe of occurrence depends on the location, , and number of modifications, and, perhaps most importantly, on the mind of each individual being asked to accept the modifications on another.\n\nThe change can be extreme but still be accepted or at least tolerated by the majority (cosmetic surgery), or be extreme and opposed by most people (neck rings). Ear piercing is a minor aesthetic alteration that is widely accepted (on females' ears). The acceptance of highly diverse tattoos and body piercings depends on location, size, and number and, perhaps most importantly, on the mind of each individual being asked to accept it.\n\nAlso known as maiming, mutilation is any form of physical injury aimed at the bodily appearance or function of a living organism's body. Examples, include amputation, foot binding, and genital cutting.\nThe reasons for these changes can be rite of passage,\n\nLike other acquired characteristics, body modification results in permanent physiological changes that cannot be passed on to offspring genetically alone.\n\nConstant physical exercise such as swimming, running, and weight lifting can cause muscles to develop. However, muscles and other body parts can also waste away through atrophy due to disuse of said body parts. Both of these phenomena occur most of the time due to the living style of the organism and their effects on the physiology of the organism constitute of acquired characteristics such as either stronger muscles or disintegrated tissue.\n\nMany repetitive activities (sports, manual labor, chewing) create wear patterns and leave impressions on the bones and fascia that are highly specialized (sometimes even unique) to the activity. Over a lifetime, even such simple activities as posture and gait leave telltale impressions on our bodies.\nshoes.\nOccupational disease\nhobbies, sports\n\nPregnancy, drug consumption, exposure to chemicals, and other environmental influences can cause acquired characteristics to occur.\n\nComplications of pregnancy\nWilson's disease\n\n", "id": "23251776", "title": "Acquired characteristic"}
{"url": "https://en.wikipedia.org/wiki?curid=418075", "text": "Digital organism\n\nA digital organism is a self-replicating computer program that mutates and evolves. Digital organisms are used as a tool to study the dynamics of Darwinian evolution, and to test or verify specific hypotheses or mathematical models of evolution. The study of digital organisms is closely related to the area of artificial life.\n\nDigital organisms can be traced back to the game Darwin, developed in 1961 at Bell Labs, in which computer programs had to compete with each other by trying to stop others from executing . A similar implementation that followed this was the game Core War. In Core War, it turned out that one of the winning strategies was to replicate as fast as possible, which deprived the opponent of all computational resources. Programs in the Core War game were also able to mutate themselves and each other by overwriting instructions in the simulated \"memory\" in which the game took place. This allowed competing programs to embed damaging instructions in each other that caused errors (terminating the process that read it), \"enslaved processes\" (making an enemy program work for you), or even change strategies mid-game and heal themselves.\n\nSteen Rasmussen at Los Alamos National Laboratory took the idea from Core War one step further in his core world system by introducing a genetic algorithm that automatically wrote programs. However, Rasmussen did not observe the evolution of complex and stable programs. It turned out that the programming language in which core world programs were written was very brittle, and more often than not mutations would completely destroy the functionality of a program.\n\nThe first to solve the issue of program brittleness was Thomas S. Ray with his Tierra system, which was similar to core world. Ray made some key changes to the programming language such that mutations were much less likely to destroy a program. With these modifications, he observed for the first time computer programs that did indeed evolve in a meaningful and complex way.\n\nLater, Chris Adami, Titus Brown, and Charles Ofria started developing their Avida system, which was inspired by Tierra but again had some crucial differences. In Tierra, all programs lived in the same address space and could potentially overwrite or otherwise interfere with each other. In Avida, on the other hand, each program lives in its own address space. Because of this modification, experiments with Avida became much cleaner and easier to interpret than those with Tierra. With Avida, digital organism research has begun to be accepted as a valid contribution to evolutionary biology by a growing number of evolutionary biologists. Evolutionary biologist Richard Lenski of Michigan State University has used Avida extensively in his work. Lenski, Adami, and their colleagues have published in journals such as \"Nature\" and the \"Proceedings of the National Academy of Sciences\" (USA).\n\nIn 1996, Andy Pargellis created a Tierra-like system called \"Amoeba\" that evolved self-replication from a randomly seeded initial condition.\n\n\n\n", "id": "418075", "title": "Digital organism"}
{"url": "https://en.wikipedia.org/wiki?curid=57414", "text": "Evolutionary developmental biology\n\nEvolutionary developmental biology (informally, evo-devo) is a field of biological research that compares the developmental processes of different organisms to infer the ancestral relationships between them and how developmental processes evolved.\n\nThe field grew from 19th century beginnings, where embryology faced a mystery: zoologists did not know how embryonic development was controlled at the molecular level. Charles Darwin noted that having similar embryos implied common ancestry, but little progress was made until the 1970s. Then, recombinant DNA technology at last brought embryology together with molecular genetics. A key early discovery was of homeotic genes that regulate development in a wide range of eukaryotes.\n\nThe field is characterised by some key concepts, which took evolutionary biologists by surprise. One is deep homology, the finding that dissimilar organs such as the eyes of insects, vertebrates and cephalopod molluscs, long thought to have evolved separately, are controlled by similar genes such as \"pax-6\", from the evo-devo gene toolkit. These genes are ancient, being highly conserved among phyla; they generate the patterns in time and space which shape the embryo, and ultimately form the body plan of the organism. Another is that species do not differ much in their structural genes, such as those coding for enzymes; what does differ is the way that gene expression is regulated by the toolkit genes. These genes are reused, unchanged, many times in different parts of the embryo and at different stages of development, forming a complex cascade of control, switching other regulatory genes as well as structural genes on and off in a precise pattern. This multiple pleiotropic reuse explains why these genes are highly conserved, as any change would have many adverse consequences which natural selection would oppose.\n\nNew morphological features and ultimately new species are produced by variations in the toolkit, either when genes are expressed in a new pattern, or when toolkit genes acquire additional functions. Another possibility is the Neo-Lamarckian theory that epigenetic changes are later consolidated at gene level, something that may have been important early in the history of multicellular life.\n\nA recapitulation theory of evolutionary development was proposed by Étienne Serres in 1824–26, echoing the 1808 ideas of Johann Friedrich Meckel. They argued that the embryos of 'higher' animals went through or recapitulated a series of stages, each of which resembled an animal lower down the great chain of being. For example, the brain of a human embryo looked first like that of a fish, then in turn like that of a reptile, bird, and mammal before becoming clearly human. The embryologist Karl Ernst von Baer opposed this, arguing in 1828 that there was no linear sequence as in the great chain of being, based on a single body plan, but a process of epigenesis in which structures differentiate. Von Baer instead recognised four distinct animal body plans: radiate, like starfish; molluscan, like clams; articulate, like lobsters; and vertebrate, like fish. Zoologists then largely abandoned recapitulation, though Ernst Haeckel revived it in 1866.\n\nFrom the early 19th century through most of the 20th century, embryology faced a mystery. Animals were seen to develop into adults of widely differing body plan, often through similar stages, from the egg, but zoologists knew almost nothing about how embryonic development was controlled at the molecular level, and therefore equally little about how developmental processes had evolved. Charles Darwin argued that a shared embryonic structure implied a common ancestor. As an example of this, Darwin cited in his 1859 book \"On the Origin of Species\" the shrimp-like larva of the barnacle, whose sessile adults looked nothing like other arthropods; Linnaeus and Cuvier had classified them as molluscs. Darwin also noted Alexander Kowalevsky's finding that the tunicate, too, was not a mollusc, but in its larval stage had a notochord and pharyngeal slits which developed from the same germ layers as the equivalent structures in vertebrates, and should therefore be grouped with them as chordates. 19th century zoology thus converted embryology into an evolutionary science, connecting phylogeny with homologies between the germ layers of embryos. Zoologists including Fritz Müller proposed the use of embryology to discover phylogenetic relationships between taxa. Müller demonstrated that crustaceans shared the Nauplius larva, identifying several parasitic species that had not been recognised as crustaceans. Müller also recognised that natural selection must act on larvae, just as it does on adults, giving the lie to recapitulation, which would require larval forms to be shielded from natural selection. Two of Haeckel's other ideas about the evolution of development have fared better than recapitulation: he argued in the 1870s that changes in the timing (heterochrony) and changes in the positioning within the body (heterotopy) of aspects of embryonic development would drive evolution by changing the shape of a descendant's body compared to an ancestor's. It took a century before these ideas were shown to be correct. In 1917, D'Arcy Thompson wrote a book on the shapes of animals, showing with simple mathematics how small changes to parameters, such as the angles of a gastropod's spiral shell, can radically alter an animal's form, though he preferred mechanical to evolutionary explanation.\nBut for the next century, without molecular evidence, progress stalled.\n\nIn the so-called modern synthesis of the early 20th century, Ronald Fisher brought together Darwin's theory of evolution, with its insistence on natural selection, heredity, and variation, and Gregor Mendel's laws of genetics into a coherent structure for evolutionary biology. Biologists assumed that an organism was a straightforward reflection of its component genes: the genes coded for proteins, which built the organism's body. Biochemical pathways (and, they supposed, new species) evolved through mutations in these genes. It was a simple, clear and nearly comprehensive picture: but it did not explain embryology.\n\nThe evolutionary embryologist Gavin de Beer anticipated evolutionary developmental biology in his 1930 book \"Embryos and Ancestors\", by showing that evolution could occur by heterochrony, such as in the retention of juvenile features in the adult. This, de Beer argued, could cause apparently sudden changes in the fossil record, since embryos fossilise poorly. As the gaps in the fossil record had been used as an argument against Darwin's gradualist evolution, de Beer's explanation supported the Darwinian position. However, despite de Beer, the modern synthesis largely ignored embryonic development to explain the form of organisms, since population genetics appeared to be an adequate explanation of how forms evolved.\n\nIn 1961, Jacques Monod, Jean-Pierre Changeux and François Jacob discovered the lac operon in the bacterium \"Escherichia coli\". It was a cluster of genes, arranged in a feedback control loop so that its products would only be made when \"switched on\" by an environmental stimulus. One of these products was an enzyme that splits a sugar, lactose; and lactose itself was the stimulus that switched the genes on. This was a revelation, as it showed for the first time that genes, even in an organism as small as a bacterium, were subject to fine-grained control. The implication was that many other genes were also elaborately regulated.\n\nIn 1977, a revolution in thinking about evolution and developmental biology began, with the arrival of recombinant DNA technology in genetics, and the papers \"Ontogeny and Phylogeny\" by Stephen J. Gould and \"Evolution by Tinkering\" by François Jacob. Gould laid to rest Haeckel's interpretation of evolutionary embryology, while Jacob set out an alternative theory. \nThis led to a second synthesis, at last including embryology as well as molecular genetics, phylogeny, and evolutionary biology to form evo-devo. In 1978, Edward B. Lewis discovered homeotic genes that regulate embryonic development in \"Drosophila\" fruit flies, which like all insects are arthropods, one of the major phyla of invertebrate animals. \nBill McGinnis quickly discovered homeotic gene sequences, homeoboxes, in animals in other phyla, in vertebrates such as frogs, birds, and mammals; they were later also found in fungi such as yeasts, and in plants. There were evidently strong similarities in the genes that controlled development across all the eukaryotes.\nIn 1980, Christiane Nüsslein-Volhard and Eric Wieschaus described gap genes which help to create the segmentation pattern in fruit fly embryos; they and Lewis won a Nobel Prize for their work in 1995.\n\nLater, more specific similarities were discovered: for example, the Distal-less gene was found in 1989 to be involved in the development of appendages or limbs in fruit flies, the fins of fish, the wings of chickens, the parapodia of marine annelid worms, the ampullae and siphons of tunicates, and the tube feet of sea urchins. It was evident that the gene must be ancient, dating back to the last common ancestor of bilateral animals (before the Ediacaran Period, which began some 635 million years ago). Evo-devo had started to uncover the ways that all animal bodies were built during development.\n\nRoughly spherical eggs of different animals give rise to extremely different bodies, from jellyfish to lobsters, butterflies to elephants. Many of these organisms share the same structural genes for bodybuilding proteins like collagen and enzymes, but biologists had expected that each group of animals would have its own rules of development. The surprise of evo-devo is that the shaping of bodies is controlled by a rather small percentage of genes, and that these regulatory genes are ancient, shared by all animals. The giraffe does not have a gene for a long neck, any more than the elephant has a gene for a big body. Their bodies are patterned by a system of switching which causes development of different features to begin earlier or later, to occur in this or that part of the embryo, and to continue for more or less time.\n\nThe puzzle of how embryonic development was controlled began to be solved using the fruit fly \"Drosophila melanogaster\" as a model organism. The step-by-step control of its embryogenesis was visualized by attaching fluorescent dyes of different colours to specific types of protein made by genes expressed in the embryo. A dye such as green fluorescent protein, originally from a jellyfish, was typically attached to an antibody specific to a fruit fly protein, forming a precise indicator of where and when that protein appeared in the living embryo.\n\nUsing such a technique, in 1994 Walter Gehring found that the \"pax-6\" gene, vital for forming the eyes of fruit flies, exactly matches an eye-forming gene in mice and humans. The same gene was quickly found in many other groups of animals, such as squid, a cephalopod mollusc. Biologists including Ernst Mayr had believed that eyes had arisen in the animal kingdom at least 40 times, as the anatomy of different types of eye varies widely. For example, the fruit fly's compound eye is made of hundreds of small lensed structures (ommatidia); the human eye has a blind spot where the optic nerve enters the eye, and the nerve fibres run over the surface of the retina, so light has to pass through a layer of nerve fibres before reaching the detector cells in the retina, so the structure is effectively \"upside-down\"; in contrast, the cephalopod eye has the retina, then a layer of nerve fibres, then the wall of the eye \"the right way around\". The evidence of \"pax-6\", however, was that the same genes controlled the development of the eyes of all these animals, suggesting that they all evolved from a common ancestor. Ancient genes had been conserved through millions of years of evolution to create dissimilar structures for similar functions, demonstrating deep homology between structures once thought to be purely analogous. This has caused a radical revision of the meaning of homology in evolutionary biology.\n\nA small fraction of the genes in an organism's genome control the organism's development. These genes are called the developmental-genetic toolkit. They are highly conserved among phyla, meaning that they are ancient and very similar in widely separated groups of animals. Differences in deployment of toolkit genes affect the body plan and the number, identity, and pattern of body parts. Most toolkit genes are parts of signalling pathways: they encode transcription factors, cell adhesion proteins, cell surface receptor proteins and signalling ligands that bind to them, and secreted morphogens that diffuse through the embryo. All of these help to define the fate of undifferentiated cells in the embryo. Together, they generate the patterns in time and space which shape the embryo, and ultimately form the body plan of the organism. Among the most important toolkit genes are the \"Hox\" genes. These transcription factors contain the homeobox protein-binding DNA motif, also found in other toolkit genes, and create the basic pattern of the body along its front-to-back axis.\nHox genes determine where repeating parts, such as the many vertebrae of snakes, will grow in a developing embryo or larva. \"Pax-6\", already mentioned, is a classic toolkit gene. Homeobox genes are also found in plants, implying they are common to all eukaryotes.\n\nThe protein products of the regulatory toolkit are reused not by duplication and modification, but by a complex mosaic of pleiotropy, being applied unchanged in many independent developmental processes, giving pattern to many dissimilar body structures. The loci of these pleiotropic toolkit genes have large, complicated and modular cis-regulatory elements. For example, while a non-pleiotropic rhodopsin gene in the fruit fly has a cis-regulatory element just a few hundred base pairs long, the pleiotropic eyeless cis-regulatory region contains 6 cis-regulatory elements in over 7000 base pairs. The regulatory networks involved are often very large. Each regulatory protein controls \"scores to hundreds\" of cis-regulatory elements. For instance, 67 fruit fly transcription factors controlled on average 124 target genes each. All this complexity enables genes involved in the development of the embryo to be switched on and off at exactly the right times and in exactly the right places. Some of these genes are structural, directly forming enzymes, tissues and organs of the embryo. But many others are themselves regulatory genes, so what is switched on is often a precisely-timed cascade of switching, involving turning on one developmental process after another in the developing embryo.\n\nSuch a cascading regulatory network has been studied in detail in the development of the fruit fly embryo. The young embryo is oval in shape, like a rugby ball. A small number of genes produce messenger RNAs that set up concentration gradients along the long axis of the embryo. In the early embryo, the \"bicoid\" and \"hunchback\" genes are at high concentration near the anterior end, and give pattern to the future head and thorax; the \"caudal\" and \"nanos\" genes are at high concentration near the posterior end, and give pattern to the hindmost abdominal segments. The effects of these genes interact; for instance, the Bicoid protein blocks the translation of \"caudal\"'s messenger RNA, so the Caudal protein concentration becomes low at the anterior end. Caudal later switches on genes which create the fly's hindmost segments, but only at the posterior end where it is most concentrated.\n\nThe Bicoid, Hunchback and Caudal proteins in turn regulate the transcription of gap genes such as \"giant\", \"knirps\", \"Krüppel\", and \"tailless\" in a striped pattern, creating the first level of structures that will become segments. The proteins from these in turn control the pair-rule genes, which in the next stage set up 7 bands across the embryo's long axis. Finally, the segment polarity genes such as \"engrailed\" split each of the 7 bands into two, creating 14 future segments.\n\nThis process explains the accurate conservation of toolkit gene sequences, which has resulted in deep homology and functional equivalence of toolkit proteins in dissimilar animals (seen, for example, when a mouse protein controls fruit fly development). The interactions of transcription factors and cis-regulatory elements, or of signalling proteins and receptors, become locked in through multiple usages, making almost any mutation deleterious and hence eliminated by natural selection.\n\nAmong the more surprising and, perhaps, counterintuitive (from a neo-Darwinian viewpoint) results of recent research in evolutionary developmental biology is that the diversity of body plans and morphology in organisms across many phyla are not necessarily reflected in diversity at the level of the sequences of genes, including those of the developmental genetic toolkit and other genes involved in development. Indeed, as John Gerhart and Marc Kirschner have noted, there is an apparent paradox: \"where we most expect to find variation, we find conservation, a lack of change\". So, if the observed morphological novelty between different clades does not come from changes in gene sequences (such as by mutation), where does it come from? Novelty may arise by mutation-driven changes in gene regulation.\n\nVariations in the toolkit may have produced a large part of the morphological evolution of animals. The toolkit can drive evolution in two ways. A toolkit gene can be expressed in a different pattern, as when the beak of Darwin's large ground-finch was enlarged by the \"BMP\" gene, or when snakes lost their legs as \"distal-less\" became under-expressed or not expressed at all in the places where other reptiles continued to form their limbs. Or, a toolkit gene can acquire a new function, as seen in the many functions of that same gene, \"distal-less\", which controls such diverse structures as the mandible in vertebrates, legs and antennae in the fruit fly, and eyespot pattern in butterfly wings. Given that small changes in toolbox genes can cause significant changes in body structures, they have often enabled the same function convergently or in parallel. \"distal-less\" generates wing patterns in the butterflies \"Heliconius erato\" and \"Heliconius melpomene\", which are Müllerian mimics. In so-called facilitated variation, their wing patterns arose in different evolutionary events, but are controlled by the same genes. Developmental changes can contribute directly to speciation.\n\nEvolutionary innovation may sometimes begin in Lamarckian style with epigenetic alterations of gene regulation or phenotype generation, subsequently consolidated by changes at the gene level. Epigenetic changes include modification of DNA by reversible methylation, as well as nonprogrammed remoulding of the organism by physical and other environmental effects due to the inherent plasticity of developmental mechanisms. The biologists Stuart A. Newman and Gerd B. Müller have suggested that organisms early in the history of multicellular life were more susceptible to this second category of epigenetic determination than are modern organisms, providing a basis for early macroevolutionary changes.\n\nDevelopment in specific lineages can be biased either positively, towards a given trajectory or phenotype, or negatively, away from producing certain types of change; either may be absolute (the change is always or never produced) or relative. Evidence for any such direction in evolution is however hard to acquire. For example, in the gastropods, the snail-type shell is always built as a tube that grows both in length and in diameter; selection has created a wide variety of shell shapes such as flat spirals, cowries and tall turret spirals within these constraints. Among the centipedes, the Lithobiomorpha always have 15 trunk segments as adults, probably the result of a developmental bias towards an odd number of trunk segments. Another centipede order, the Geophilomorpha, the number of segments varies in different species between 27 and 191, but the number is always odd, making this an absolute constraint; almost all the odd numbers in that range are occupied by one or another species.\n\nEcological evolutionary developmental biology (eco-evo-devo) integrates research from developmental biology and ecology to examine their relationship with evolutionary theory. Researchers study concepts and mechanisms such as developmental plasticity, epigenetic inheritance, genetic assimilation, niche construction and symbiosis.\n\n\n", "id": "57414", "title": "Evolutionary developmental biology"}
{"url": "https://en.wikipedia.org/wiki?curid=42684750", "text": "Contribution of epigenetic modifications to evolution\n\nEpigenetics is a broad term that refers to changes in gene expression that occur via mechanisms such as DNA methylation, histone acetylation, and microRNA modification. When these epigenetics changes are heritable, they can influence evolution. Current research indicates that epigenetics has influenced evolution in a number of organisms, including plants and animals.\n\nDNA methylation and histone modification are two mechanisms used to regulate gene expression in plants. DNA methylation can be mitotically and meiotically stable, allowing for methylation states to be passed to other orthologous genes in a genome. DNA methylation can be reversed via the use of enzymes known as DNA de-methylases, while histone modiciations can be reversed via removing histone acetyl groups with deacetylases. Interspecific differences due to environmental factors are shown to be associated with the difference between annual and perennial life cycles. There can be varying adaptive responses based on this.\n\nForms of histone methylation cause repression of certain genes that are stably inherited through mitosis but that can also be erased during meiosis or with the progression of time. The induction of flowering by exposure to low winter temperatures in Arabidopsis thaliana shows this effect. Histone methylation participates in repression of expression of an inhibitor of flowering during cold. In annual, semelparous species such as Arabidopsis thaliana, this histone methylation is stably inherited through mitosis after return from cold to warm temperatures giving the plant the opportunity to flower continuously during spring and summer until it senesces. However, in perennial, iteroparous relatives the histone modification rapidly disappears when temperatures rise, allowing expression of the floral inhibitor to increase and limiting flowering to a short interval. Epigenetic histone modifications control a key adaptive trait in Arabidopsis thaliana, and their pattern changes rapidly during evolution associated with reproductive strategy.\nAnother study tested several epigenetic recombinant inbred lines (epiRILs) of Arabidopsis thaliana - lines with similar genomes but varying levels of DNA methylation - for their drought sensitivity and their sensitivity to nutritional stress. It was found that there was a significant amount of heritable variation in the lines in regards to traits important for survival of drought and nutrient stress. This study proved that variation in DNA methylation can result in heritable variation of ecologically important plant traits, such as root allocation, drought tolerance, and nutrient plasticity. It also hinted that epigenetic variation alone could result in rapid evolution.\n\nScientists found that changes in DNA methylation induced by stress was inherited in asexual dandelions. Genetically similar plants were exposed to different ecological stresses, and their offspring were raised in an unstressed environment. Amplified fragment-length polymorphism markers that were methylation-sensitive were used to test for methylation on a genome-wide scale. It was found that many of the environmental stresses caused induction of pathogen and herbivore defenses, which caused methylation in the genome. These modifications were then genetically transmitted to the offspring dandelions. The transgenerational inheritance of a stress response can contribute to the heritable plasticity of the organism, allowing it to better survive environmental stresses. It also helps add to the genetic variation of specific lineages with little variability, giving a greater chance of reproductive success.\n\nA comparative analysis of CpG methylation patterns between humans and primates found that there were more than 800 genes that varied in their methylation patterns among orangutans, gorillas, chimpanzees, and bonobos. Despite these apes having the same genes, methylation differences are what account for their phenotypic variation. The genes in question are involved in development. It is not the protein sequences that account for the differences in physical characteristics between humans and apes; rather, it is the epigenetic changes to the genes. Since humans and the great apes share 99% of their DNA, it is thought that the differences in methylation patterns account for their distinction. So far, there are known to be 171 genes that are uniquely methylated in humans, 101 genes that are uniquely methylated in chimpanzees and bonobos, 101 genes that are uniquely methylated in gorillas, and 450 genes that are uniquely methylated in orangutans. For example, genes involved in blood pressure regulation and the development of the inner ear’s semicircular canal are highly methylated in humans, but not in the apes. There are also 184 genes that are conserved at the protein level between human and chimpanzees, but have epigenetic differences. Enrichments in multiple independent gene categories show that regulatory changes to these genes have given humans their specific traits. This research shows that epigenetics plays an important role in evolution in primates.\nIt has also been shown that cis-regulatory elements changes affect the transcription start sites (TSS) of genes. 471 DNA sequences are found to be enriched or depleted in regards to histone trimethylation at the H3K4 histone in chimpanzee, human, and macaque prefrontal cortexes. Among these sequences, 33 are selectively methylated in neuronal chromatin from children and adults, but not from non-neuronal chromatin. One locus that was selectively methylated was DPP10, a regulatory sequence that showed evidence of hominid adaptation, such as higher nucleotide substitution rates and certain regulatory sequences that were missing in other primates. Epigenetic regulation of TSS chromatin has been identified as an important development in the evolution of gene expression networks in the human brain. These networks are thought to play a role in cognitive processes and neurological disorders.\nAn analysis of methylation profiles of humans and primate sperm cells reveals epigenetic regulation plays an important role here as well. Since mammalian cells undergo reprogramming of DNA methylation patterns during germ cell development, the methylomes of human and chimp sperm can be compared to methylation in embryonic stem cells (ESCs). There were many hypomethylated regions in both sperms cells and ESCs that showed structural differences. Also, many of the promoters in human and chimp sperm cells had different amounts of methylation. In essence, DNA methylation patterns differ between germ cells and somatic cells as well as between the human and chimpanzee sperm cells. Meaning, differences in promoter methylation could possible account for the phenotypic differences between humans and primates.\n\nRed Junglefowl, ancestor of domestic chickens, show that gene expression and methylation profiles in the thalamus and hypothalamus differed significantly from that of a domesticated egg laying breed. Methylation differences and gene expression were maintained in the offspring, depicting that epigenetic variation is inherited. Some of the inherited methylation differences were specific to certain tissues, and the differential methylation at specific loci were not altered much after intercrossing between Red Junglefowl and domesticated laying hens for eight generations.The results hint that domestication has led to epigenetic changes, as domesticated chickens maintained a higher level of methylation for more than 70% of the genes.\n\nThe role of epigenetics in evolution is clearly linked to the selective pressures that regulate that process. As organisms leave offspring that are best suited to their environment, environmental stresses change DNA gene expression that are further passed down to their offspring, allowing for them also to better thrive in their environment. The classic case study of the rats who experience licking and grooming from their mothers pass this trait to their offspring shows that a mutation in the DNA sequence is not required for a heritable change. Basically, a high degree of maternal nurturing makes the offspring of that mother more likely to nurture their own children with a high degree of care as well. Rats with a lower degree of maternal nurturing are less likely to nurture their own offspring with so much care. Also, rates of epigenetic mutations, such as DNA methylation, are much higher than rates of mutations transmitted genetically and are easily reversed. This provides a way for variation within a species to rapidly increase, in times of stress, providing opportunity for adaptation to newly arising selection pressures.\n\nJean-Baptiste Lamarck proposed that species experience certain obstacles in their lifetimes which they must overcome. They acquire certain characteristics to deal with these challenges, and such accumulations are then passed to their offspring. In modern terms, this transmission from parent to offspring would be considered a method of epigenetic inheritance. Scientists are now questioning the framework of the modern synthesis, as epigenetics has shown to be in direct contrast with the core of Darwinism while being in agreement with Lamarckism. While some evolutionary biologists have dismissed epigenetics' impact on evolution entirely, others have begun to discover that a fusion of both epigenetic and traditional genetic inheritance may contribute to the variations seen in species today.\n\n", "id": "42684750", "title": "Contribution of epigenetic modifications to evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=3297755", "text": "Arboreal theory\n\nThis theory is proposed by Grafton Elliott-Smith (1912) who was a neuroanatomist who was chiefly concerned with the emergence of the primate brain.\n\nThe arboreal theory claims that primates evolved from their ancestors by adapting to arboreal life.\n\nPrimates are thought to have developed several of their traits and habits initially while living in trees. One key component to this argument is that primates relied on sight over smell. They were able to develop a keen sense of depth perception, perhaps because of the constant leaping that was necessary to move about the trees. Primates also developed hands and feet that were capable of grasping. This was also a result of arboreal life, which required a great deal of crawling along branches, and reaching out for fruit and other food. These early primates were likely to have eaten foods found in trees, such as flowers, fruits, berries, gums, leaves, and insects. They are thought to have shifted their diets towards insects in the early Cenozoic era, when insects became more numerous.\n", "id": "3297755", "title": "Arboreal theory"}
{"url": "https://en.wikipedia.org/wiki?curid=60426", "text": "Symbiogenesis\n\nSymbiogenesis, or endosymbiotic theory, is an evolutionary theory of the origin of eukaryotic cells from prokaryotic organisms, first articulated in 1905 and 1910 by the Russian botanist Konstantin Mereschkowski, and advanced and substantiated with microbiological evidence by Lynn Margulis in 1967. It holds that the organelles distinguishing eukaryote cells evolved through symbiosis of individual single-celled prokaryotes (bacteria and archaea).\n\nThe theory holds that mitochondria, plastids such as chloroplasts, and possibly other organelles of eukaryotic cells represent formerly free-living prokaryotes taken one inside the other in endosymbiosis, around 1.5 billion years ago. In more detail, mitochondria appear to be related to Rickettsiales proteobacteria, and chloroplasts to nitrogen-fixing filamentous cyanobacteria.\n\nAmong the many lines of evidence supporting symbiogenesis are that new mitochondria and plastids are formed only through binary fission, and that cells cannot create new ones otherwise; that the transport proteins called porins are found in the outer membranes of mitochondria, chloroplasts and bacterial cell membranes; that cardiolipin is found only in the inner mitochondrial membrane and bacterial cell membranes; and that some mitochondria and plastids contain single circular DNA molecules similar to the DNA of bacteria.\n\nThe theory of symbiogenesis (Greek: σύν \"syn\" \"together\", βίωσις \"biosis\" \"living\", and γένεσις \"genesis\" \"origin or birth\") was first outlined by the Russian botanist Konstantin Mereschkowski in his 1905 work, \"The nature and origins of chromatophores in the plant kingdom\", and then elaborated in his 1910 \"The Theory of Two Plasms as the Basis of Symbiogenesis, a New Study or the Origins of Organisms\". Mereschkowski was familiar with work by botanist Andreas Schimper, who had observed in 1883 that the division of chloroplasts in green plants closely resembled that of free-living cyanobacteria, and who had himself tentatively proposed (in a footnote) that green plants had arisen from a symbiotic union of two organisms. In 1918 the French scientist Paul Portier published \"Les Symbiotes\" in which he claimed that the mitochondria originated from a symbiosis process. Ivan Wallin advocated the idea of an endosymbiotic origin of mitochondria in the 1920s.\n\nThe Russian botanist Boris Kozo-Polyansky was the first to explain the theory in terms of Darwinian evolution. In his 1924 book \"Novyi printzip biologii. Ocherk teorii simbiogeneza\" (\"The new principle of biology. Essay on the theory of symbiogenesis\"; translated to English as \"Symbiogenesis: A New Principle of Evolution\" in 2010), he wrote, \"The theory of symbiogenesis is a theory of selection relying on the phenomenon of symbiosis.\" These theories were initially dismissed or ignored. More detailed electron microscopic comparisons between cyanobacteria and chloroplasts (for example studies by Hans Ris published in 1961), combined with the discovery that plastids and mitochondria contain their own DNA (which by that stage was recognized to be the hereditary material of organisms) led to a resurrection of the idea in the 1960s.\n\nThe theory was advanced and substantiated with microbiological evidence by Lynn Margulis in a 1967 paper, \"On the origin of mitosing cells.\" In her 1981 work \"Symbiosis in Cell Evolution\" she argued that eukaryotic cells originated as communities of interacting entities, including endosymbiotic spirochaetes that developed into eukaryotic flagella and cilia. This last idea has not received much acceptance, because flagella lack DNA and do not show ultrastructural similarities to bacteria or archaea (see also: Evolution of flagella and Prokaryotic cytoskeleton). According to Margulis and Dorion Sagan, \"Life did not take over the globe by combat, but by networking\" (i.e., by cooperation). The possibility that peroxisomes may have an endosymbiotic origin has also been considered, although they lack DNA. Christian de Duve proposed that they may have been the first endosymbionts, allowing cells to withstand growing amounts of free molecular oxygen in the Earth's atmosphere. However, it now appears that peroxisomes may be formed \"de novo\", contradicting the idea that they have a symbiotic origin.\n\nAccording to Keeling and Archibald, the usual way to distinguish organelles from endosymbionts is by their reduced genome sizes. As an endosymbiont evolves into an organelle, most of their genes are transferred to the host cell genome. The host cell and organelle need to develop a transport mechanism that enables the return of the protein products needed by the organelle but now manufactured by the cell. Cyanobacteria and α-proteobacteria are the most closely related free-living organisms to plastids and mitochondria respectively. Both cyanobacteria and α-proteobacteria maintain a large (>6Mb) genome encoding thousands of proteins. Plastids and mitochondria exhibit a dramatic reduction in genome size when compared to their bacterial relatives. Chloroplast genomes in photosynthetic organisms are normally 120-200kb encoding 20-200 proteins and mitochondrial genomes in humans are approximately 16kb and encode 37 genes, 13 of which are proteins. Using the example of the freshwater amoeboid, however, \"Paulinella chromatophora\", which contains chromatophores found to be evolved from cyanobacteria, Keeling and Archibald argue that this is not the only possible criterion; another is that the host cell has assumed control of the regulation of the former endosymbiont's division, thereby synchronizing it with the cell's own division. Nowack and her colleagues performed gene sequencing on the chromatophore (1.02 Mb) and found that only 867 proteins were encoded by these photosynthetic cells. Comparisons with their closest free living cyanobacteria of the genus \"Synechococcus\" (having a genome size 3 Mb, with 3300 genes) revealed that chromatophores underwent a drastic genome shrinkage. Chromatophores contained genes that were accountable for photosynthesis but were deficient in genes that could carry out other biosynthetic functions; this observation suggests that these endosymbiotic cells are highly dependent on their hosts for their survival and growth mechanisms. Thus, these chromatophores were found to be non-functional for organelle-specific purposes when compared to mitochondria and plastids. This distinction could have promoted the early evolution of photosynthetic organelles.\n\nThe loss of genetic autonomy, that is, the loss of many genes from endosymbionts, occurred very early in evolutionary time. Taking into account the entire original endosymbiont genome, there are three main possible fates for genes over evolutionary time. The first fate involves the loss of functionally redundant genes, in which genes that are already represented in the nucleus are eventually lost. The second fate involves the transfer of genes to the nucleus. The loss of autonomy and integration of the endosymbiont with its host can be primarily attributed to nuclear gene transfer. As organelle genomes have been greatly reduced over evolutionary time, nuclear genes have expanded and become more complex. As a result, many plastid and mitochondrial processes are driven by nuclear encoded gene products. In addition, many nuclear genes originating from endosymbionts have acquired novel functions unrelated to their organelles.\n\nThe mechanisms of gene transfer are not fully known; however, multiple hypotheses exist to explain this phenomenon. The cDNA hypothesis involves the use of messenger RNA (mRNAs) to transport genes from organelles to the nucleus where they are converted to cDNA and incorporated into the genome. The cDNA hypothesis is based on studies of the genomes of flowering plants. Protein coding RNAs in mitochondria are spliced and edited using organelle-specific splice and editing sites. Nuclear copies of some mitochondrial genes, however, do not contain organelle-specific splice sites, suggesting a processed mRNA intermediate. The cDNA hypothesis has since been revised as edited mitochondrial cDNAs are unlikely to recombine with the nuclear genome and are more likely to recombine with their native mitochondrial genome. If the edited mitochondrial sequence recombines with the mitochondrial genome, mitochondrial splice sites would no longer exist in the mitochondrial genome. Any subsequent nuclear gene transfer would therefore also lack mitochondrial splice sites.\n\nThe bulk flow hypothesis is the alternative to the cDNA hypothesis, stating that escaped DNA, rather than mRNA, is the mechanism of gene transfer. According to this hypothesis, disturbances to organelles, including autophagy (normal cell destruction), gametogenesis (the formation of gametes), and cell stress, release DNA which is imported into the nucleus and incorporated into the nuclear DNA using non-homologous end joining (repair of double stranded breaks). For example, in the initial stages of endosymbiosis, due to a lack of major gene transfer, the host cell had little to no control over the endosymbiont. The endosymbiont underwent cell division independently of the host cell, resulting in many \"copies\" of the endosymbiont within the host cell. Some of the endosymbionts lysed (burst), and high levels of DNA were incorporated into the nucleus. A similar mechanism is thought to occur in tobacco plants, who show a high rate of gene transfer and whose cells contain multiple chloroplasts. In addition, the bulk flow hypothesis is also supported by the presence of non-random clusters of organelle genes, suggesting the simultaneous movement of multiple genes.\nIn 2015, the biologist Roberto Cazzolla Gatti provided evidence for a variant theory, endogenosymbiosis, in which not only are organelles endosymbiotic, but that pieces of genetic material from symbiotic parasites (\"gene carriers\" such as viruses, retroviruses and bacteriophages), are included in the host's nuclear DNA, changing the host's gene expression and contributing to the process of speciation.\n\nMolecular and biochemical evidence suggests that mitochondria are related to Rickettsiales proteobacteria (in particular, the SAR11 clade, or close relatives), and that chloroplasts are related to nitrogen-fixing filamentous cyanobacteria.\n\nThe third and final possible fate of endosymbiont genes is that they remain in the organelles. Plastids and mitochondria, although they have lost much of their genomes, retain genes encoding rRNAs, tRNAs, proteins involved in redox reactions, and proteins required for transcription, translation, and replication. There are many hypotheses to explain why organelles retain a small portion of their genome; however no one hypothesis will apply to all organisms and the topic is still quite controversial. The hydrophobicity hypothesis states that highly hydrophobic (water hating) proteins (such as the membrane bound proteins involved in redox reactions) are not easily transported through the cytosol and therefore these proteins must be encoded in their respective organelles. The code disparity hypothesis states that the limit on transfer is due to differing genetic codes and RNA editing between the organelle and the nucleus. The redox control hypothesis states that genes encoding redox reaction proteins are retained in order to effectively couple the need for repair and the synthesis of these proteins. For example, if one of the photosystems is lost from the plastid, the intermediate electron carriers may lose or gain too many electrons, signalling the need for repair of a photosystem. The time delay involved in signalling the nucleus and transporting a cytosolic protein to the organelle results in the production of damaging reactive oxygen species. The final hypothesis states that the assembly of membrane proteins, particularly those involved in redox reactions, requires coordinated synthesis and assembly of subunits; however, translation and protein transport coordination is more difficult to control in the cytoplasm.\n\nThe majority of the genes in the mitochondria and plastids are related to the expression (transcription, translation and replication) of genes encoding proteins involved in either photosynthesis (in plastids) or cellular respiration (in mitochondria). One might predict that the loss of photosynthesis or cellular respiration would allow for the complete loss of the plastid genome or the mitochondrial genome respectively. While there are numerous examples of mitochondrial descendants (mitosomes and hydrogenosomes) that have lost their entire organellar genome, non-photosynthetic plastids tend to retain a small genome. There are two main hypotheses to explain this occurrence:\n\nThe essential tRNA hypothesis notes that there have been no documented functional plastid-to-nucleus gene transfers of genes encoding RNA products (tRNAs and rRNAs). As a result, plastids must make their own functional RNAs or import nuclear counterparts. The genes encoding tRNA-Glu and tRNA-fmet, however, appear to be indispensable. The plastid is responsible for haem biosynthesis, which requires plastid encoded tRNA-Glu (from the gene trnE) as a precursor molecule. Like other genes encoding RNAs, trnE cannot be transferred to the nucleus. In addition, it is unlikely trnE could be replaced by a cytosolic tRNA-Glu as trnE is highly conserved; single base changes in trnE have resulted in the loss of haem synthesis. The gene for tRNA-formylmethionine (tRNA-fmet) is also encoded in the plastid genome and is required for translation initiation in both plastids and mitochondria. A plastid is required to continue expressing the gene for tRNA-fmet so long as the mitochondrion is translating proteins.\n\nThe limited window hypothesis offers a more general explanation for the retention of genes in non-photosynthetic plastids. According to the bulk flow hypothesis, genes are transferred to the nucleus following the disturbance of organelles. Disturbance was common in the early stages of endosymbiosis, however, once the host cell gained control of organelle division, eukaryotes could evolve to have only one plastid per cell. Having only one plastid severely limits gene transfer as the lysis of the single plastid would likely result in cell death. Consistent with this hypothesis, organisms with multiple plastids show an 80-fold increase in plastid-to-nucleus gene transfer compared to organisms with single plastids.\n\nThere are many lines of evidence that mitochondria and plastids including chloroplasts arose from bacteria.\n\nPrimary endosymbiosis involves the engulfment of a cell by another free living organism. Secondary endosymbiosis occurs when the product of primary endosymbiosis is itself engulfed and retained by another free living eukaryote. Secondary endosymbiosis has occurred several times and has given rise to extremely diverse groups of algae and other eukaryotes. Some organisms can take opportunistic advantage of a similar process, where they engulf an alga and use the products of its photosynthesis, but once the prey item dies (or is lost) the host returns to a free living state. Obligate secondary endosymbionts become dependent on their organelles and are unable to survive in their absence (for a review see McFadden 2001). RedToL, the Red Algal Tree of Life Initiative funded by the National Science Foundation highlights the role red algae or Rhodophyta played in the evolution of our planet through secondary endosymbiosis.\n\nOne possible secondary endosymbiosis in process has been observed by Okamoto & Inouye (2005). The heterotrophic protist \"Hatena\" behaves like a predator until it ingests a green alga, which loses its flagella and cytoskeleton, while \"Hatena\", now a host, switches to photosynthetic nutrition, gains the ability to move towards light and loses its feeding apparatus.\n\nThe process of secondary endosymbiosis left its evolutionary signature within the unique topography of plastid membranes. Secondary plastids are surrounded by three (in euglenophytes and some dinoflagellates) or four membranes (in haptophytes, heterokonts, cryptophytes, and chlorarachniophytes). The two additional membranes are thought to correspond to the plasma membrane of the engulfed alga and the phagosomal membrane of the host cell. The endosymbiotic acquisition of a eukaryote cell is represented in the cryptophytes; where the remnant nucleus of the red algal symbiont (the nucleomorph) is present between the two inner and two outer plastid membranes.\n\nDespite the diversity of organisms containing plastids, the morphology, biochemistry, genomic organisation, and molecular phylogeny of plastid RNAs and proteins suggest a single origin of all extant plastids – although this theory is still debated.\n\nSome species including \"Pediculus humanus\" (lice) have multiple chromosomes in the mitochondrion. This and the phylogenetics of the genes encoded within the mitochondrion suggest that mitochondria have multiple ancestors, that these were acquired by endosymbiosis on several occasions rather than just once, and that there have been extensive mergers and rearrangements of genes on the several original mitochondrial chromosomes.\n\n\n\n", "id": "60426", "title": "Symbiogenesis"}
{"url": "https://en.wikipedia.org/wiki?curid=44241044", "text": "Megaevolution\n\nMegaevolution describes the most dramatic events in evolution. It is no longer suggested that the evolutionary processes involved are necessarily special, although in some cases they might be. Whereas macroevolution can apply to relatively modest changes that produced diversification of species and genera and are readily compared to microevolution, \"megaevolution\" is used for great changes. Megaevolution has been extensively debated because it has been seen as a possible objection to Charles Darwin's theory of gradual evolution by natural selection.\n\nA list was prepared by John Maynard Smith and Eörs Szathmáry which they called \"The Major Transitions in Evolution\". On the 1999 edition of the list they included:\n\nSome of these topics had been discussed before.\n\nNumbers one to six on the list are events which are of huge importance, but about which we know relatively little. All occurred before (and mostly very much before) the fossil record started, or at least before the Phanerozoic eon.\n\nNumbers seven and eight on the list are of a different kind from the first six, and have generally not been considered by the other authors. Number four is of a type which is not covered by traditional evolutionary theory, The origin of eukaryotic cells is probably due to symbiosis between prokaryotes. This is a kind of evolution which must be a rare event.\n\nThe Cambrian explosion or Cambrian radiation was the relatively rapid appearance of most major animal phyla around 530 million years ago (mya) in the fossil record, some of which are now extinct. It is the classic example of megaevolution. \"The fossil record documents two mutually exclusive macroevolutionary modes separated by the transitional Ediacaran period\".\n\nBefore about 580 mya it seems that most organisms were simple. They were made of individual cells occasionally organized into colonies. Over the following 70 or 80 million years the rate of evolution accelerated by an order of magnitude. Normally rates of evolution are measured by the extinction and origination rate of species, but here we can say that by the end of the Cambrian every phylum, or almost every phylum, existed. \n\nThe diversity of life began to resemble that of today.\n\nThe Cambrian explosion has caused much scientific debate. The seemingly rapid appearance of fossils in the 'primordial strata' was noted as early as the mid 19th century, and Charles Darwin saw it as one of the main objections that could be made against his theory of evolution by natural selection.\n\n", "id": "44241044", "title": "Megaevolution"}
{"url": "https://en.wikipedia.org/wiki?curid=44311175", "text": "Grit, not grass hypothesis\n\nThe Grit, not grass hypothesis is an evolutionary hypothesis that explains the evolution of high-crowned teeth, particularly in New World mammals. The hypothesis is that the ingestion of gritty soil is the primary driver of hypsodont tooth development, not the silica-rich composition of grass, as was previously thought.\n\nSince the morphology of the hypsodont tooth is suited to a more abrasive diet, hypsodonty was thought to have evolved concurrently with the spread of grasslands. During the Cretaceous Period (145-66 million years ago), the Great Plains were covered by a shallow inland sea called the Western Interior Seaway which began to recede during the Late Cretaceous to the Paleocene (65-55 million years ago), leaving behind thick marine deposits and a relatively flat terrain. During the Miocene and Pliocene epochs (25 million years), the continental climate became favorable to the evolution of grasslands. Existing forest biomes declined and grasslands became much more widespread. The grasslands provided a new niche for mammals, including many ungulates that switched from browsing diets to grazing diets. Grass contains silica-rich phytoliths (abrasive granules), which wear away dental tissue more quickly. So the spread of grasslands was linked to the development of high-crowned (hypsodont) teeth in grazers.\n\nIn 2006 Strömberg examined the independent acquisition of high-crowned cheek teeth (hypsodonty) in several ungulate lineages (e.g., camelids, equids, rhinoceroses) from the early to middle Miocene of North America, which had been classically linked to the spread of grasslands She showed habitats dominated by C3 grasses (cool-season grasses) were established in the Central Great Plains by early late Arikareean (≥21.9 Million years ago), at least 4 million years prior to the emergence of hypsodonty in Equidae. In 2008 Mendoza and Palmqvist determined the relative importance of grass consumption and open habitat foraging in the development of hypsodont teeth using a dataset of 134 species of artiodactyls and perissodactyls. The results suggested that high-crowned teeth represent are adapted for a particular feeding environment, not diet preference. \n\nFurther, examination of mammalian teeth suggests that it is the open, gritty habitat and not the grass itself which is linked to diet changes. Analysis of dental microwear patterns of hypsodont notoungulates from the Late Oligocene Salla Beds of Bolivia showed shearing movements are associated with a diet rich in tough plants, not necessarily grasses. That the relationship between high-crowned mammals and the source of tooth wear in the fossil record may not be straightforward, hence the spread of grasslands in South America, traditionally linked with the development of notoungulate hypsodonty, is in question.\n\nObservations of hypsodonty development in Cenozoic mammals is also supported by the evolution of hypsodonty in other groups. For example, hadrosaurs, a group of herbivorous dinosaurs, likely grazed on low-lying vegetation and microwear patterns show that their diet contained an abrasive material, such as grit or silica. Grasses had evolved by the Late Cretaceous, but were not particularly common, so this study concluded that grass probably did not play a major component in the hadrosaur's diet.\n\nMost importantly, evidence has shown, that the development of hypsodonty is out of sync with the flourishing of grasslands both in North America and South America, where grasslands spread 10 millions years earlier.\n\nHypsodonty is observed both in the fossil record and the modern world. It is a characteristic of large clades (equids) as well as subspecies level specialization. For example, the Sumatran rhinoceros and the Javan rhinoceros both have brachydont, lophodont cheek teeth whereas the Indian rhinoceros, has hypsodont dentition. A mammal may have exclusively hypsodont molars or have a mix of dentitions. Hypsodont dentition is characterized by:\n", "id": "44311175", "title": "Grit, not grass hypothesis"}
{"url": "https://en.wikipedia.org/wiki?curid=44285917", "text": "Local adaptation\n\nLocal adaptation is when a population of organisms has evolved to be more well-suited to its environment than other members of the same species. This occurs due to differential pressures of natural selection on populations from different environments. For example, populations of a species that lives within a wide range of temperatures may be locally adapted to the warmer or cooler climate where they live. More formally, a population is said to be locally adapted if organisms in that population have differentially evolved as compared to other populations within their species in response to selective pressures imposed by some aspect of their local environment. Local adaptation is often determined via reciprocal transplant experiments, where organisms from one population are transplanted into another population, and vice versa, and their fitnesses measured. If the transplanted organisms have lower fitness in the novel environment, than the native population can be said to be locally adapted.\n\nPopulations located in different environments may be faced with different biotic and abiotic pressures, consequently natural selection may drive the evolution of these populations in different directions. This divergent natural selection can lead to differences in trait values among populations for those traits that are heritable and impact organism fitness. Local adaptation of a variety of traits has been demonstrated in numerous, phylogenetically disparate organisms.\n\nExamples of local adaptation abound in the natural world. For instance, many plant populations exhibit local adaptation. This has been established by reciprocally transplanting plants from one population into another population, and vice versa. The transplanted plants often do worse in the novel environment than the native plants that are locally adapted. Many examples of local adaptation exist in host-parasite systems as well. For instance, a host may be resistant to a locally-abundant pathogen or parasite, but conspecific hosts from elsewhere where that pathogen is not abundant may have no evolved no such adaptation. \n", "id": "44285917", "title": "Local adaptation"}
{"url": "https://en.wikipedia.org/wiki?curid=44410336", "text": "Evolution of snake venom\n\nVenom in snakes and some lizards is a form of saliva that has been modified into venom over its evolutionary history. In snakes, venom has evolved to kill or subdue prey, as well as to perform other diet-related functions. The evolution of venom is thought to be responsible for the enormous expansion of snakes across the globe.\n\nThe evolutionary history of snake venom is a matter of debate. The common view of this history before 2014 was that venom originated just once among all Toxicofera approximately 170 million years ago, and then diversified into the wide range of venoms seen today. Under this hypothesis, the original toxicoferan venom was a very simple set of proteins that were assembled in a pair of glands. Subsequently, this set of proteins diversified in the various lineages of toxicoferans, including Serpentes, Anguimorpha, and Iguania. Several snake lineages subsequently lost the ability to produce venom, often due to a change in diet.\n\nThe single-origin hypothesis suggests that the mechanism of evolution in most cases has been gene duplication followed by natural selection for adaptive traits. Some of the various adaptations produced by this process include venom more toxic to specific prey in several lineages, proteins that pre-digest prey, and a method to track down prey after a bite. These various adaptations of venom have also led to considerable debate about the definition of venom and venomous snakes.\n\nThe idea that venom had a single evolutionary origin has been called into question by a 2015 study, which found that venom proteins had homologs in many other tissues in the Burmese python. The study therefore suggested that venom had evolved independently in a number of snake lineages.\n\nThe origin of venom is thought to have provided the catalyst for the rapid diversification of snakes in the Cenozoic period, particularly to the Colubridae and their colonization of the Americas. Scholars suggest that the reason for this huge expansion was the shift from a mechanical to a biochemical method of subduing prey. Snake venoms attack biological pathways and processes that are also targeted by venoms of other taxa; for instance, calcium channel blockers have been found in snakes, spiders, and cone snails, thus suggesting that venom exhibits convergent evolution.\n\nUntil the use of gene sequencing to create phylogenetic trees became practical, phylogenies were created on the basis of morphology. Such traditional phylogenies suggested that venom originated in multiple branches among Squamata approximately 100 million years ago. More recent studies using nuclear gene sequences found the presence of similar venom proteins in several lizards within a clade that was named \"Toxicofera\". This led to the theory that venom originated only once within the entire lineage approximately 170 million years ago. This ancestral venom consisted of a very simple set of proteins, assembled in a pair of glands. The venoms of the different lineages then diversified and evolved independently, along with their means of injecting venom into prey. This diversification included the independent evolution of front-fanged venom delivery from the ancestral rear-fanged venom delivery system. The single origin hypothesis also suggests that venom systems subsequently atrophied, or were completely lost, independently in a number of lineages. The American “rat snakes,” such as \"Pantherophis guttatus\", lost their venom following the evolution of constriction as a means of prey capture. The independent evolution of constriction in the fish-eating aquatic genus \"Acrochordus\" also saw the degradation of the venom system. Two independent lineages, one terrestrial and one marine, that shifted to an egg-based diet, also possess the remnants of an atrophied venom system.\n\nThe view that venom evolved just once has recently been called into doubt. A study performed in 2014 found that homologs of 16 venom proteins, which had been used to support the single origin hypothesis, were all expressed at high levels in a number of body tissues. The authors therefore suggested that previous research, which had found venom proteins to be conserved across the supposed Toxicoferan lineage, might have misinterpreted the presence of more generic \"housekeeping\" genes across this lineage, as a result of only sampling certain tissues within the reptiles' bodies. Therefore, the authors suggested that instead of evolving just once in an ancestral reptile, snake venom evolved independently in a number of lineages. A 2015 study found that homologs of the so-called \"toxic\" genes were present in numerous tissues of a non-venomous snake, the Burmese python. Castoe stated that the team had found homologs to the venom genes in many tissues outside the oral glands, where venom genes might be expected. This demonstrated the weaknesses of only analyzing transcriptomes (the total messenger RNA in a cell). The team suggested that pythons represented a period in snake evolution before major venom development. The researchers also found that the expansion of venom gene families occurred mostly in highly venomous caenophidian snakes (also referred to as \"colubroidian snakes\"), thus suggesting that most venom evolution took place after this lineage diverged from other snakes.\n\nThe primary mechanism for the diversification of venom is thought to be the duplication of gene coding for other tissues, followed by their expression in the venom glands. The proteins then evolved into various venom proteins through natural selection. This process, known as the birth-and-death model, is responsible for several of the protein recruitment events in snake venom. These duplications occurred in a variety of tissue types with a number of ancestral functions. Notable examples include 3FTx, ancestrally a neurotransmitter found in the brain, which has adapted into a neurotoxin that binds and blocks acetylcholine receptors. Another example is phospholipase A2 (PLA2) type IIA, ancestrally involved with inflammatory processes in normal tissue, which has evolved into venom capable of triggering lipase activity and tissue destruction. The change in function of PLA2, in particular, has been well documented; there is evidence of several separate gene duplication events, often associated with the origin of new snake species. Non-allelic homologous recombination (or recombination between DNA sequences that are similar, but not alleles) has been proposed as the mechanism of duplication of PLA2 genes in rattlesnakes, as an explanation for its rapid evolution. These venom proteins have also occasionally been recruited back into tissue genes. Protein recruitment events have occurred at different points in the evolutionary history of snakes. For example, the 3FTX protein family is absent in the viperid lineage, suggesting that it was recruited into snake venom after the viperid snakes branched off from the remaining colubroidae.\n\nThere have been debates about whether the original gene duplication events occurred in a salivary gland or in body tissues themselves. The prevailing idea for many years is the birth-and-death model described above, in which genes in other body tissues are duplicated and then recruited to the venom gland before natural selection for toxicity. However, in 2014 a new model was proposed in which salivary protein genes are duplicated and then restricted to the venom gland. This model does away with the recruitment hypothesis and cites the homology between certain venom and body genes as unrelated in the linear fashion described in the traditional birth-and-death model.\n\nGene duplication is not the only way that venom has become more diverse. There have been instances of new venom proteins generated by alternative splicing. The Elapid snake \"Bungarus fasciatus\", for example, possesses a gene that is alternatively spliced to yield both a venom component and a physiological protein. Further diversification may have occurred by gene loss of specific venom components. For instance, the rattlesnake ancestor is believed to have had genes for neurotoxic PLA2 toxins, which are absent in modern non-neurotoxic \"Crotalus\" species. The various recruitment events had resulted in snake venom evolving into a very complex mixture of proteins. The venom of rattlesnakes, for example, includes nearly 40 different proteins from different protein families, and other snake venoms have been found to contain more than 100 distinct proteins. The composition of this mixture has been shown to vary geographically, and to be related to the prey species available in a certain region. Snake venom has generally evolved very quickly, with changes occurring faster in the venom than in the rest of the organism.\n\nSome traditional hypotheses of snake venom evolution have supported the idea that most snakes inject far more venom into their prey than is required to kill them; thus, venom composition would not be subject to natural selection. This is known as the \"overkill\" hypothesis. However, recent studies of the molecular history of snake venom have contradicted this, instead finding evidence of rapid adaptive evolution in many different clades, including the carpet vipers, \"Echis\", the ground rattlesnakes, \"Sistrurus\", and the Malayan pit viper, as well as generally in the diversification of PLA2 proteins. There is phylogenetic evidence of positive selection and rapid rates of gene gain and loss in venom genes of \"Sistrurus\" taxa feeding on different prey. Several studies have found evidence that venom and resistance to venom in prey species have evolved in a coevolutionary arms race. For example, wood rats of the genus \"Neotoma\" have a high degree of resistance to the venom of rattlesnakes, suggesting that the rats have evolved in response to the snake venom, thus renewing selection pressure upon the snakes. The selection pressure on snake venom is thought to be selecting for functional diversity within the proteins in venom, both within a given species, and across species. The genes that code for venom proteins in some snake genera have a proportion of synonymous mutations that is lower than would be expected if venom were evolving through neutral evolutionary processes. In addition, snake venom is metabolically costly for a snake to produce, which scientists have suggested as further evidence that a selection pressure exists on snake venom (in this case, to minimize the volume of venom required). The use of model organisms, rather than snakes' natural prey, to study prey toxicity, has been suggested as a reason why the \"overkill\" hypothesis may have been overemphasized. However, the pitviper genus \"Agkistrodon\" has been found to be an exception to this; the composition of venom in \"Agkistrodon\" has been found to be related to the position of the species within the phylogeny, suggesting that those venoms have evolved mostly through neutral processes (mutation and genetic drift), and that there may be significant variation in the selection pressure upon various snake venoms.\n\nSnakes use their venom to kill or subdue prey, as well as for other diet-related functions, such as digestion. Current scientific theory suggests that snake venom is not used for defense or for competition between members of the same species, unlike in other taxa. Thus adaptive evolution in snake venom has resulted in several adaptations with respect to these diet-related functions that increase the fitness of the snakes that carry them.\n\nVenom that is toxic only to a certain taxon, or strongly toxic only to a certain taxon, has been found in a number of snakes, suggesting that these venoms have evolved via natural selection to subdue preferred prey species. Examples of this phenomenon have been found in the Mangrove snake \"Boiga dendrophila\", which has a venom specifically toxic to birds, as well as in the genera \"Echis\" and \"Sistrurus\", and in sea snakes. However, while several snakes possess venom that is highly toxic to their preferred prey species, the reverse correlation is not necessarily true: the venoms of several snakes are toxic to taxa that they do not consume in high proportions. Most snake venom, for instance, is highly toxic to lizards, although the proportion of lizard prey varies among snake species. This has led researchers to suggest that toxicity to a certain taxon is nearly independent of toxicity to another distantly related taxon.\n\nThe natural diets of snakes in the widespread viper genus \"Echis\" are highly varied, and include arthropods, such as scorpions, as well as vertebrates. Various \"Echis\" species consume different quantities of arthropods in their diet. A study carried out in 2009 injected scorpions with the venom of various \"Echis\" species, and found a high correlation between the proportion of arthropods that the snakes consumed in their natural habitat, and the toxicity of their venom to scorpions. The researchers also found evidence that the evolution of venom more toxic to arthropods was related to an increase in the proportion of arthropods in the snakes' diet, and that diet and venom may have evolved by a process of coevolution. A phylogeny of the genus constructed using mitochondrial DNA showed that one instance of a change in venom composition in the species ancestral to all \"Echis\" snakes was correlated with a shift to an arthropod based diet, whereas another shift in a more recent lineage was correlated with a shift to a diet of vertebrates. Despite the higher toxicity of the venom of arthropod-consuming species, it was not found to incapacitate or kill prey any faster than that of species with fewer arthropods in their diet. Thus, the venom is thought to have evolved to minimize the volume required, as the production of venom carries a significant metabolic cost, thus providing a fitness benefit. This pattern is also found in other lineages. Similar results were obtained by a 2012 study which found that the venom of arthropod-consuming \"Echis\" species was more toxic to locusts than that of vertebrate-consuming species.\n\nA 2009 study of the venom of four \"Sistrurus\" pit viper species found significant variation in the toxicity to mice. This variation was related to the proportion of small mammals in the diet of those species. The idea that \"Sistrurus\" venom had evolved to accommodate a mammal-based diet was supported by phylogenetic analysis. The researchers suggested that the basis for the difference in toxicity was the difference in muscle physiology in the various prey animals. Two lineages of elapid snakes, common sea snakes and \"Laticauda\" sea kraits, have independently colonized marine environments, and shifted to a very simple diet based on teleosts, or ray-finned fish. A 2005 study found that both these lineages have a much simpler set of venom proteins than their terrestrial relatives on the Australian continent, which have a more varied and complex diet. These findings were confirmed by a 2012 study, which compared the venoms of \"Toxicocalamus longissimus\", a terrestrial species, and \"Hydrophis cyanocinctus\", a marine species, both within the subfamily Hydrophiinae (which is also within the Elapid family). Despite being closely related to one another, the marine species had a significantly simpler set of venom proteins. The venoms of the sea snakes are nonetheless among the most toxic venoms known. It has been argued that since sea snakes are typically unable to prevent the escape of bitten prey, their venoms have evolved to act very rapidly.\n\nThe various subspecies of the rattlesnake genus \"Crotalus\", produce venoms that carry out two conflicting functions. The venom immobilizes prey after a bite, and also helps digestion by breaking down tissues before the snake eats its prey. As with other members of the family Viperidae, the venoms of \"Crotalus\" disrupt the homeostatic processes of prey animals. However, there is a wide variety of venom compositions among the species of \"Crotalus\". A 2010 study found a 100-fold difference in the amount of metalloproteinase activity among the various snakes, with \"Crotalus oreganus cerberus\" having the highest activity and \"Crotalus oreganus concolor\" having the lowest. There was also a 15-fold variation in the amount of protease activity, with \"C. o. concolor\" and \"C. o. cerberus\" having the highest and lowest activities, respectively.\n\nMetalloproteinase activity causes hemorrhage and necrosis following a snake bite, a process which aids digestion. The activity of proteases, on the other hand, disrupts platelet and muscle function and damages cell membranes, and thus contributes to a quick death for the prey animal. The study found that the venoms of \"Crotalus\" fell into two categories; those that favored metalloproteinases (Type I) and those that favored proteases (Type II). The study stated that these functions were essentially mutually exclusive; venoms had been selected for based on either their toxicity or their tenderizing potential. The researchers also hypothesized that the reason for this dichotomy was that a venom high in neurotoxicity, such as a type II venom, kills an animal quickly, preventing the relatively slower acting metalloproteinase from digesting tissue.\n\nAnother example of an adaptive function other than prey immobilization is the role of viperid venom in allowing the snake to track a prey animal it has bitten, a process known as \"prey relocalization.\" This important adaptation allowed rattlesnakes to evolve the strike-and-release bite mechanism, which provided a huge benefit to snakes by minimizing contact with potentially dangerous prey animals. However, this adaptation then requires the snake to track down the bitten animal in order to eat it, in an environment full of other animals of the same species. A 2013 study found that western diamondback rattlesnakes (\"Crotalus atrox\") responded more actively to mouse carcasses that had been injected with crude rattlesnake venom. When the various components of the venom were separated out, the snakes responded to mice injected with two kinds of disintegrins. The study concluded that these disintegrin proteins were responsible for allowing the snakes to track their prey, by changing the odor of the bitten animal.\n\nAccording to the hypothesis that snake venom had a single evolutionary origin, venom in a number of lineages of snakes subsequently atrophied. Evidence of such atrophication has been found in several snakes. A 2005 study in the marbled sea snake, \"Aipysurus eydouxii\" found that the gene for a three-fingered protein found in the venom had undergone a deletion of two nucleotide bases which made the venom 50-100 times less toxic than it had been previously. This change was correlated with a change in diet from fish to a diet consisting almost entirely of fish eggs, suggesting that the adaptation to an egg diet had removed the selection pressure needed to maintain a highly toxic venom, allowing the venom genes to accumulate deleterious mutations. A similar venom degradation following a shift to an egg-based diet has been found in the Common egg-eater \"Dasypeltis scabra\", whose diet consists entirely of birds' eggs, meaning that the snake had no use for its venom. This has led biologists to suggest that if venom is not used by a species, it is rapidly lost.\n\nVenom atrophication has also occurred following the evolution of constriction as a method of prey capture that does not require venom. Constriction is hypothesized to have evolved independently in several lineages. North American ‘rat snakes’ such as the Corn snake \"Pantherophis guttatus\" use constriction to trap and kill their rodent prey, and do not possess functional venom. A similar process has occurred in the African colubrid lineage Lamprophiidae, such as \"Pseudaspis cana\", as well as in the genus \"Acrochordus\".\n\n", "id": "44410336", "title": "Evolution of snake venom"}
{"url": "https://en.wikipedia.org/wiki?curid=1914", "text": "Antimicrobial resistance\n\nAntimicrobial resistance (AMR) is the ability of a microbe to resist the effects of medication previously used to treat them. The term includes the more specific \"antibiotic resistance\", which applies only to bacteria becoming resistant to antibiotics. Resistant microbes are more difficult to treat, requiring alternative medications or higher doses, both of which may be more expensive or more toxic. Microbes resistant to multiple antimicrobials are called multidrug resistant (MDR); or sometimes superbugs.\n\nResistance arises through one of three mechanisms: natural resistance in certain types of bacteria, genetic mutation, or by one species acquiring resistance from another. All classes of microbes can develop resistance: fungi develop antifungal resistance, viruses develop antiviral resistance, protozoa develop antiprotozoal resistance, and bacteria develop antibiotic resistance. Resistance can appear spontaneously because of random mutations; or more commonly following gradual buildup over time.\nPreventive measures include only using antibiotics when needed, thereby stopping misuse of antibiotics or antimicrobials. Narrow-spectrum antibiotics are preferred over broad-spectrum antibiotics when possible, as effectively and accurately targeting specific organisms is less likely to cause resistance. For people who take these medications at home, education about proper use is essential. Health care providers can minimize spread of resistant infections by use of proper sanitation and hygiene, including handwashing and disinfecting between patients, and should encourage the same of the patient, visitors, and family members.\nRising drug resistance is caused mainly by use of antimicrobials in humans and other animals, and spread of resistant strains between the two. Antibiotics increase selective pressure in bacterial populations, causing vulnerable bacteria to die; this increases the percentage of resistant bacteria which continue growing. With resistance to antibiotics becoming more common there is greater need for alternative treatments. Calls for new antibiotic therapies have been issued, but new drug development is becoming rarer.\n\nAntimicrobial resistance is on the rise. Estimates are that 700,000 to several million deaths result per year. Each year in the United States, at least 2 million people become infected with bacteria that are resistant to antibiotics and at least 23,000 people die as a result. There are public calls for global collective action to address the threat include proposals for international treaties on antimicrobial resistance. Worldwide antibiotic resistance is not fully mapped, but poorer countries with weak healthcare systems are more affected.\n\nThe WHO defines antimicrobial resistance as a microorganism's resistance to an antimicrobial drug that was once able to treat an infection by that microorganism.\nA person cannot become resistant to antibiotics. Resistance is a property of the microbe, not a person or other organism infected by a microbe.\n\nA World Health Organization (WHO) report released April 2014 stated, \"this serious threat is no longer a prediction for the future, it is happening right now in every region of the world and has the potential to affect anyone, of any age, in any country. Antibiotic resistance—when bacteria change so antibiotics no longer work in people who need them to treat infections—is now a major threat to public health.\"\n\nBacteria with resistance to antibiotics predate medical use of antibiotics by humans. However, widespread antibiotic use has made more bacteria resistant through the process of evolutionary pressure.\n\nReasons for the widespread use of antibiotics in human medicine include:\n\nOther causes include:\n\nIncreasing bacterial resistance is linked with the volume of antibiotic prescribed, as well as missing doses when taking antibiotics. Inappropriate prescribing of antibiotics has been attributed to a number of causes, including people insisting on antibiotics, physicians prescribing them as they feel they do not have time to explain why they are not necessary, and physicians not knowing when to prescribe antibiotics or being overly cautious for medical and/or legal reasons. Lower antibiotic concentration contributes to the increase of AMR by introducing more mutations that support bacterial growth in higher antibiotic concentration.\n\nFor example, sub-inhibitory concentration have induced genetic mutation in bacteria such as \"Pseudomonas aeruginosa\" and \"Bacteroides fragilis\".\n\nUp to half of antibiotics used in humans are unnecessary and inappropriate For example, a third of people believe that antibiotics are effective for the common cold, and the common cold is the most common reason antibiotics are prescribed even though antibiotics are useless against viruses. A single regimen of antibiotics even in compliant individuals leads to a greater risk of resistant organisms to that antibiotic in the person for a month to possibly a year.\n\nAntibiotic resistance increases with duration of treatment; therefore, as long as an effective minimum is kept, shorter courses of antibiotics are likely to decrease rates of resistance, reduce cost, and have better outcomes with fewer complications. Short course regimens exist for community-acquired pneumonia spontaneous bacterial peritonitis, suspected lung infections in intense care wards, so-called acute abdomen, middle ear infections, sinusitis and throat infections, and penetrating gut injuries. In some situations a short course may not cure the infection as well as a long course. A BMJ editorial recommended that antibiotics can often be safely stopped 72 hours after symptoms resolve.\n\nBecause individuals may feel better before the infection is eradicated, doctors must provide instructions to them so they know when it is safe to stop taking a prescription. Some researchers advocate doctors' using a very short course of antibiotics, reevaluating the patient after a few days, and stopping treatment if there are no clinical signs of infection.\n\nCertain antibiotic classes result in resistance more than others. Increased rates of MRSA infections are seen when using glycopeptides, cephalosporins, and quinolone antibiotics. Cephalosporins, and particularly quinolones and clindamycin, are more likely to produce colonisation with \"Clostridium difficile\".\n\nFactors within the intensive care unit setting such as mechanical ventilation and multiple underlying diseases also appear to contribute to bacterial resistance. Poor hand hygiene by hospital staff has been associated with the spread of resistant organisms.\n\nThe World Health Organization concluded that inappropriate use of antibiotics in animal husbandry is an underlying contributor to the emergence and spread of antibiotic-resistant germs, and that the use of antibiotics as growth promoters in animal feeds should be restricted. The World Organisation for Animal Health has added to the Terrestrial Animal Health Code a series of guidelines with recommendations to its members for the creation and harmonization of national antimicrobial resistance surveillance and monitoring programs, monitoring of the quantities of antibiotics used in animal husbandry, and recommendations to ensure the proper and prudent use of antibiotic substances. Another guideline is to implement methodologies that help to establish associated risk factors and assess the risk of antibiotic resistance.\n\nNaturally occurring antibiotic resistance is common. Genes for resistance to antibiotics, like antibiotics themselves, are ancient. The genes that confer resistance are known as the environmental resistome. These genes may be transferred from non-disease-causing bacteria to those that do cause disease, leading to clinically significant antibiotic resistance.\n\nIn 1952 it was shown that penicillin-resistant bacteria existed before penicillin treatment; and also preexistent bacterial resistance to streptomycin. In 1962, the presence of penicillinase was detected in dormant endospores of \"Bacillus licheniformis\", revived from dried soil on the roots of plants, preserved since 1689 in the British Museum. Six strains of \"Clostridium\", found in the bowels of William Braine and John Hartnell (members of the Franklin Expedition) showed resistance to cefoxitin and clindamycin.\n\nPenicillinase may have emerged as a defense mechanism for bacteria in their habitats, such as the case of penicillinase-rich \"Staphylococcus aureus\", living with penicillin-producing \"Trichophyton\"; however, this may be circumstantial. Search for a penicillinase ancestor has focused on the class of proteins that must be \"a priori\" capable of specific combination with penicillin. The resistance to cefoxitin and clindamycin in turn was attributed to Braine's and Hartnell's contact with microorganisms that naturally produce them or random mutation in the chromosomes of \"Clostridium\" strains.\n\nThere is evidence that heavy metals and other pollutants may select for antibiotic-resistant bacteria, generating a constant source of them in small numbers.\n\nAntibiotic resistance is a growing problem among humans and wildlife in terrestrial or aquatic environments. In this respect, the spread and contamination of the environment, especially through \"hot spots\" such as hospital wastewater and untreated urban wastewater, is a growing and serious public health problem. Antibiotics have been polluting the environment since their introduction through human waste (medication, farming), animals, and the pharmaceutical industry. Along with antibiotic waste, resistant bacteria follow, thus introducing antibiotic-resistant bacteria into the environment. Already in 2011, mapping of sewage and water supply samples in New Delhi showed widespread and uncontrolled infection as indicated by the presence of NDM-1-positive enteric bacteria.\n\nWhile 70 to 80 percent of diarrhea is caused by viral pathogens, for which antibiotics are not effective, around 40 percent of these cases are nevertheless attempted to be treated with antibiotics. In some areas even over 80 percent of cases are attempted to be treated with antibiotics.\n\nAs bacteria replicate quickly, the resistant bacteria that enter the environment replicate their resistance genes as they continue to divide. In addition, bacteria carrying resistance genes have the ability to spread those genes to other species via horizontal gene transfer. Therefore, even if the specific antibiotic is no longer introduced into the environment, antibiotic-resistance genes will persist through the bacteria that have since replicated without continuous exposure. Antibiotic resistance is widespread in marine vertebrates, and they may be important reservoirs of antibiotic-resistant bacteria in the marine environment.\n\nThere have been increasing public calls for global collective action to address the threat, including a proposal for international treaty on antimicrobial resistance. Further detail and attention is still needed in order to recognize and measure trends in resistance on the international level; the idea of a global tracking system has been suggested but implementation has yet to occur. A system of this nature would provide insight to areas of high resistance as well as information necessary for evaluation of programs and other changes made to fight or reverse antibiotic resistance.\n\nFive important strategies needed for minimising antibiotic resistance are as follows:\n\nAntibiotic treatment duration should be based on the infection and other health problems a person may have. For many infections once a person has improved there is little evidence that stopping treatment causes more resistance. Some therefore feel that stopping early may be reasonable in some cases. Other infections, however, do require long courses regardless of whether a person feels better.\n\nThere are multiple national and international monitoring programs for drug-resistant threats, including methicillin-resistant \"Staphylococcus aureus\" (MRSA), vancomycin-resistant \"S. aureus\" (VRSA), extended spectrum beta-lactamase (ESBL), vancomycin-resistant \"Enterococcus\" (VRE), multidrug-resistant \"A. baumannii\" (MRAB).\n\nResistanceOpen is an online global map of antimicrobial resistance developed by HealthMap which displays aggregated data on antimicrobial resistance from publicly available and user submitted data. The website can display data for a 25-mile radius from a location. Users may submit data from antibiograms for individual hospitals or laboratories. European data is from the EARS-Net (European Antimicrobial Resistance Surveillance Network), part of the ECDC.\n\nResistanceMap is a website by the Center for Disease Dynamics, Economics & Policy and provides data on antimicrobial resistance on a global level.\n\nAntibiotic stewardship programmes appear useful in reducing rates of antibiotic resistance.\n\nExcessive antibiotic use has become one of the top contributors to the development of antibiotic resistance. Since the beginning of the antibiotic era, antibiotics have been used to treat a wide range of disease. Overuse of antibiotics has become the primary cause of rising levels of antibiotic resistance. The main problem is that doctors are willing to prescribe antibiotics to ill-informed individuals who believe that antibiotics can cure nearly all illnesses, including viral infections like the common cold. In an analysis of drug prescriptions, 36% of individuals with a cold or an upper respiratory infection (both viral in origin) were given prescriptions for antibiotics. These prescriptions accomplished nothing other than increasing the risk of further evolution of antibiotic resistant bacteria.\n\nAntimicrobial stewardship teams in hospitals are encouraging optimal use of antimicrobials. The goals of antimicrobial stewardship are to help practitioners pick the right drug at the right dose and duration of therapy while preventing misuse and minimizing the development of resistance. Stewardship may reduce the length of stay by an average of slightly over 1 day while not increasing the risk of death.\n\nGiven the volume of care provided in primary care (General Practice), recent strategies have focused on reducing unnecessary antibiotic prescribing in this setting. Simple interventions, such as written information explaining the futility of antibiotics for common infections such as upper respiratory tract infections, have been shown to reduce antibiotic prescribing.\n\nThe prescriber should closely adhere to the five rights of drug administration: the right patient, the right drug, the right dose, the right route, and the right time.\n\nCultures should be taken before treatment when indicated and treatment potentially changed based on the susceptibility report.\n\nAbout a third of antibiotic prescriptions written in outpatient settings in the United States were not appropriate in 2010 and 2011. Doctors in the U.S. wrote 506 annual antibiotic scripts for every 1,000 people, with 353 being medically necessary.\n\nHealth workers and pharmacists can help tackle resistance by: enhancing infection prevention and control; only prescribing and dispensing antibiotics when they are truly needed; prescribing and dispensing the right antibiotic(s) to treat the illness.\n\nPeople can help tackle resistance by using antibiotics only when prescribed by a doctor; completing the full prescription, even if they feel better; never sharing antibiotics with others or using leftover prescriptions.\n\n\nInfectious disease control through improved water, sanitation and hygiene (WASH) infrastructure needs to be placed at the center of the antimicrobial resistance (AMR) agenda. The spread of infectious diseases caused by inadequate WASH standards is a major driver of antibiotic demand in developing countries. Growing usage of antibiotics together with persistent infectious disease levels have led to a dangerous cycle in which reliance on antimicrobials increases while the efficacy of drugs diminishes. The proper use of infrastructure for water, sanitation and hygiene (WASH) can result in a 47–72 percent decrease of diarrhea cases treated with antibiotics depending on the type of intervention and its effectiveness. A reduction of the diarrhea disease burden through improved infrastructure would result in large decreases in the number of diarrhea cases treated with antibiotics. This was estimated as ranging from 5 million in Brazil to up to 590 million in India by the year 2030. The strong link between increased consumption and resistance indicates that this will directly mitigate the accelerating spread of AMR. Sanitation and water for all by 2030 is Goal Number 6 of the Sustainable Development Goals.\n\nAn increase in hand washing compliance by hospital staff results in decreased rates of resistant organisms.\n\nIn 1997, European Union health ministers voted to ban avoparcin and four additional antibiotics used to promote animal growth in 1999. In 2006 a ban on the use of antibiotics in European feed, with the exception of two antibiotics in poultry feeds, became effective. In Scandinavia, there is evidence that the ban has led to a lower prevalence of antibiotic resistance in (nonhazardous) animal bacterial populations. As of 2004, several European countries established a decline of antimicrobial resistance in humans through limiting the usage antimicrobials in agriculture and food industries without jeopardizing animal health or economic cost.\n\nThe United States Department of Agriculture (USDA) and the Food and Drug Administration (FDA) collect data on antibiotic use in humans and in a more limited fashion in animals.\nThe FDA first determined in 1977 that there is evidence of emergence of antibiotic-resistant bacterial strains in livestock. The long-established practice of permitting OTC sales of antibiotics (including penicillin and other drugs) to lay animal owners for administration to their own animals nonetheless continued in all states.\nIn 2000, the FDA announced their intention to revoke approval of fluoroquinolone use in poultry production because of substantial evidence linking it to the emergence of fluoroquinolone-resistant \"Campylobacter\" infections in humans. Legal challenges from the food animal and pharmaceutical industries delayed the final decision to do so until 2006. Fluroquinolones have been banned from extra-label use in food animals in the USA since 2007. However, they remain widely used in companion and exotic animals.\n\nA global action plan to tackle the growing problem of resistance to antibiotics and other antimicrobial medicines was endorsed at the Sixty-eighth World Health Assembly in May 2015. One of the key objectives of the plan is to improve awareness and understanding of antimicrobial resistance through effective communication, education and training. React based in Sweden has produced informative material on AMR for the general public.\n\nVideos are being produced for the general public to generate interest and awareness.\n\nThe World Health Organization has promoted the first World Antibiotic Awareness Week running from 16–22 November 2015. The aim of the week is to increase global awareness of antibiotic resistance. It also wants to promote the correct usage of antibiotics across all fields in order to prevent further instances of antibiotic resistance.\n\nWorld Antibiotic Awareness Week has been held every November since 2015. For 2017, the Food and Agriculture Organization of the United Nations (FAO), the World Health Organization (WHO) and the World Organisation for Animal Health (OIE) are together calling for responsible use of antibiotics in humans and animals to reduce the emergence of antibiotic resistance.\n\nThe four main mechanisms by which microorganisms exhibit resistance to antimicrobials are:\nAntibiotic resistance can be a result of horizontal gene transfer, and also of unlinked point mutations in the pathogen genome at a rate of about 1 in 10 per chromosomal replication. Mutations are rare but the fact that bacteria reproduce at such a high rate allows for the effect to be significant. A mutation may produce a change in the binding site of the antibiotic, which may allow the site to continue proper functioning in the presence of the antibiotic or prevent the binding of the antibiotic to the site altogether.\n\nAntibiotic action against a pathogen can be seen as an environmental pressure. Those bacteria with a mutation that allows them to survive will reproduce, pass the trait to their offspring, which leads to the microevolution of a fully resistant colony. Chromosomal mutations providing antibiotic resistance benefit the bacteria but also confer a cost of fitness. For example, a ribosomal mutation may protect a bacterial cell by changing the binding site of an antibiotic but will also slow protein synthesis. manifesting, in slower growth rate.\n\nIn Gram-negative bacteria, plasmid-mediated resistance genes produce proteins that can bind to DNA gyrase, protecting it from the action of quinolones. Finally, mutations at key sites in DNA gyrase or topoisomerase IV can decrease their binding affinity to quinolones, decreasing the drug's effectiveness.\n\nBacteria can develop antibiotic resistance. Recent findings show no necessity of large populations of bacteria for the appearance of antibiotic resistance. Small populations of E.coli in an antibiotic gradient can become resistant. Any heterogeneous environment with respect to nutrient and antibiotic gradients may facilitate antibiotic resistance in small bacterial populations. Researchers hypothesize that the mechanism of resistance development is based on four SNP mutations in the genome of E.coli produced by the gradient of antibiotic. \n\nAntibiotic resistance can be introduced artificially into a microorganism through laboratory protocols, sometimes used as a selectable marker to examine the mechanisms of gene transfer or to identify individuals that absorbed a piece of DNA that included the resistance gene and another gene of interest.\n\nNew Delhi metallo-beta-lactamase 1 (NDM-1) is an enzyme that makes bacteria resistant to a broad range of beta-lactam antibiotics. The most common bacteria that make this enzyme are gram-negative such as \"Escherichia coli\" and \"Klebsiella pneumoniae\", but the gene for NDM-1 can spread from one strain of bacteria to another by horizontal gene transfer.\n\nSpecific antiviral drugs are used to treat some viral infections. These drugs prevent viruses from reproducing by inhibiting essential stages of the virus's replication cycle in infected cells. Antivirals are used to treat HIV, hepatitis B, hepatitis C, influenza, herpes viruses including varicella zoster virus, cytomegalovirus and Epstein-Barr virus. With each virus, some strains have become resistant to the administered drugs.\n\nResistance to HIV antivirals is problematic, and even multi-drug resistant strains have evolved. Resistant strains of the HIV virus emerge rapidly if only one antiviral drug is used. Using three or more drugs together has helped to control this problem, but new drugs are needed because of the continuing emergence of drug-resistant HIV strains.\n\nInfections by fungi are a cause of high morbidity and mortality in immunocompromised persons, such as those with HIV/AIDS, tuberculosis or receiving chemotherapy. The fungi candida, \"Cryptococcus neoformans\" and \"Aspergillus fumigatus\" cause most of these infections and antifungal resistance occurs in all of them. Multidrug resistance in fungi is increasing because of the widespread use of antifungal drugs to treat infections in immunocompromised individuals.\n\nOf particular note, Fluconazole-resistant Candida species have been highlighted as a growing problem by the CDC. More than 20 Candida species of Candida can cause Candidiasis infection, the most common of which is \"Candida albicans\". Candida yeasts normally inhabit the skin and mucous membranes without causing infection. However, overgrowth of Candida can lead to Candidiasis. Some Candida strains are becoming resistant to first-line and second-line antifungal agents such as azoles and echinocandins.\n\nThe protozoan parasites that cause the diseases malaria, trypanosomiasis, toxoplasmosis, cryptosporidiosis and leishmaniasis are important human pathogens.\n\nMalarial parasites that are resistant to the drugs that are currently available to infections are common and this has led to increased efforts to develop new drugs. Resistance to recently developed drugs such as artemisinin has also been reported. The problem of drug resistance in malaria has driven efforts to develop vaccines.\n\nTrypanosomes are parasitic protozoa that cause African trypanosomiasis and Chagas disease (American trypanosomiasis). There are no vaccines to prevent these infections so drugs such as pentamidine and suramin, benznidazole and nifurtimox and used to treat infections. These drugs are effective but infections caused by resistant parasites have been reported.\n\nLeishmaniasis is caused by protozoa and is an important public health problem worldwide, especially in sub-tropical and tropical countries. Drug resistance has \"become a major concern\".\n\nThe phenomenon of antimicrobial resistance caused by overuse of antibiotics was predicted already by Alexander Fleming who said \"The time may come when penicillin can be bought by anyone in the shops. Then there is the danger that the ignorant man may easily under-dose himself and by exposing his microbes to nonlethal quantities of the drug make them resistant.\"\n\nFor the fiscal year 2016 budget, President Obama has suggested to nearly double the amount of federal funding to \"combat and prevent\" antibiotic resistance to more than $1.2 billion. Many international funding agencies like USAID, DFID, SIDA and Bill & Melinda Gates Foundation has pledged money for developing strategies to counter antimicrobial resistance.\n\nSince the mid-1980s pharmaceutical companies have invested in medications for cancer or chronic disease that have greater potential to make money and have \"de-emphasized or dropped development of antibiotics\". On January 20, 2016 at the World Economic Forum in Davos, Switzerland, more than \"80 pharmaceutical and diagnostic companies\" from around the world called for 'transformational commercial models' at a global level to spur research and development on antibiotics and on the \"enhanced use of diagnostic tests that can rapidly identify the infecting organism\".\n\nSome global health scholars have argued that a global, legal framework is needed to prevent and control antimicrobial resistance. For instance, binding global policies could be used to create antimicrobial use standards, regulate antibiotic marketing, and strengthen global surveillance systems. Ensuring compliance of involved parties is a challenge. Global antimicrobial resistance policies could take lessons from the environmental sector by adopting strategies that have made international environmental agreements successful in the past such as: sanctions for non-compliance, assistance for implementation, majority vote decision-making rules, an independent scientific panel, and specific commitments.\n\nOn March 27, 2015, the White House released a comprehensive plan to address the increasing need for agencies to combat the rise of antibiotic-resistant bacteria. The Task Force for Combating Antibiotic-Resistant Bacteria developed \"The National Action Plan for Combating Antibiotic-Resistant Bacteria\" with the intent of providing a roadmap to guide the US in the antibiotic resistance challenge and with hopes of saving many lives. This plan outlines steps taken by the Federal government over the next five years needed in order to prevent and contain outbreaks of antibiotic-resistant infections; maintain the efficacy of antibiotics already on the market; and to help to develop future diagnostics, antibiotics, and vaccines.\n\nThe Action Plan was developed around five goals with focuses on strengthening health care, public health veterinary medicine, agriculture, food safety and research, and manufacturing. These goals, as listed by the White House, are as follows:\nThe following are goals set to meet by 2020:\n\nAccording to WHO policymakers can help tackle resistance by strengthening resistance tracking and laboratory capacity; regulating and promoting appropriate use of medicines. Policymakers and industry can help tackle resistance by: fostering innovation and research and development of new tools; promoting cooperation and information sharing among all stakeholders.\n\nIt is unclear if rapid viral testing affects antibiotic use in children.\n\nMicroorganisms do not develop resistance to vaccines because a vaccine enhances the body's immune system, whereas an antibiotic operates separately from the body's normal defenses. Furthermore, if the use of vaccines increase, there is evidence that antibiotic resistant strains of pathogens will decrease; the need for antibiotics will naturally decrease as vaccines prevent infection before it occurs. However, new strains that escape immunity induced by vaccines may evolve; for example, an updated influenza vaccine is needed each year.\n\nWhile theoretically promising, antistaphylococcal vaccines have shown limited efficacy, because of immunological variation between \"Staphylococcus\" species, and the limited duration of effectiveness of the antibodies produced. Development and testing of more effective vaccines is underway.\n\nAlternating therapy is a proposed method in which two or three antibiotics are taken in a rotation versus taking just one antibiotic such that bacteria resistant to one antibiotic are killed when the next antibiotic is taken. Studies have found that this method reduces the rate at which antibiotic resistant bacteria emerge in vitro relative to a single drug for the entire duration. \n\nStudies have found that bacteria that evolve antibiotic resistance towards one group of antibiotic may become more sensitive others.\n\nSince the discovery of antibiotics, research and development (R&D) efforts have provided new drugs in time to treat bacteria that became resistant to older antibiotics, but in the 2000s there has been concern that development has slowed enough that seriously ill people may run out of treatment options. Another concern is that doctors may become reluctant to perform routine surgeries because of the increased risk of harmful infection. Backup treatments can have serious side-effects; for example, treatment of multi-drug-resistant tuberculosis can cause deafness or psychological disability. The potential crisis at hand is the result of a marked decrease in industry R&D. Poor financial investment in antibiotic research has exacerbated the situation. The pharmaceutical industry has little incentive to invest in antibiotics because of the high risk and because the potential financial returns are less likely to cover the cost of development than for other pharmaceuticals. In 2011, Pfizer, one of the last major pharmaceutical companies developing new antibiotics, shut down its primary research effort, citing poor shareholder returns relative to drugs for chronic illnesses. However, small and medium-sized pharmaceutical companies are still active in antibiotic drug research.\n\nIn the United States, drug companies and the administration of President Barack Obama have been proposing changing the standards by which the FDA approves antibiotics targeted at resistant organisms. On 12 December 2013, the Antibiotic Development to Advance Patient Treatment (ADAPT) Act of 2013 was introduced in the U.S. Congress. The ADAPT Act aims to fast-track the drug development in order to combat the growing public health threat of 'superbugs'. Under this Act, the FDA can approve antibiotics and antifungals needed for life-threatening infections based on data from smaller clinical trials. The Centers for Disease Control and Prevention (CDC) will reinforce the monitoring of the use of antibiotics that treat serious and life-threatening infections and the emerging resistance, and make the data publicly available. The FDA antibiotics labeling process, 'Susceptibility Test Interpretive Criteria for Microbial Organisms' or 'breakpoints' is also streamlined to allow the most up-to-date and cutting-edge data available to healthcare professionals under the new Act.\n\nOn 18 September 2014 Obama signed an executive order to implement the recommendations proposed in a report by the President's Council of Advisors on Science and Technology (PCAST) which outlines strategies to stream-line clinical trials and speed up the R&D of new antibiotics. Among the proposals:\nThe executive order also included a $20 million prize to encourage the development of diagnostic tests to identify highly resistant bacterial infections.\n\nThe U.S. National Institutes of Health plans to fund a new research network on the issue up to $62 million from 2013 to 2019. Using authority created by the Pandemic and All Hazards Preparedness Act of 2006, the Biomedical Advanced Research and Development Authority in the U.S. Department of Health and Human Services announced that it will spend between $40 million and $200 million in funding for R&D on new antibiotic drugs under development by GlaxoSmithKline.\n\nOne major cause of antibiotic resistance is the increased pumping activity of microbial ABC transporters, which diminishes the effective drug concentration inside the microbial cell. ABC transporter inhibitors that can be used in combination with current antimicrobials are being tested in clinical trials and are available for therapeutic regimens.\n\nPhage therapy is the therapeutic use of bacteriophages to treat pathogenic bacterial infections. Phage therapy has many potential applications in human medicine as well as dentistry, veterinary science, and agriculture.\n\nBacteriophages are much more specific than antibiotics. They are typically harmless not only to the host organism, but also to other beneficial bacteria, such as the gut flora, reducing the chances of opportunistic infections.\n\nBacteriophages are used against antibiotic resistant bacteria in Georgia (George Eliava Institute) and in one institute in Wrocław, Poland.\n\n\n\n", "id": "1914", "title": "Antimicrobial resistance"}
{"url": "https://en.wikipedia.org/wiki?curid=44391262", "text": "Evolution of biparental care in tropical frogs\n\nThe Evolution of biparental care in tropical frogs is the evolution of the behaviour of a parental care system in frogs in which both the mother and father raise their offspring.\n\nMany tropical frogs have developed a parental care system where both the mother and father partake in raising their offspring. The evolution of biparental care, which is the joint effort of both parents, is a topic that is still under investigation. Biparentalism arose in some species of tropical frogs as a result of ecological conditions, the differences between the sexes, and their natural tendencies.\n\nMale parental care could have served as the basis for the development of biparental care. Phylogenetic evidence shows that male parental care is the ancestral strategy in \"Dendrobates\". Currently there are \"Dendrobates\" species, such as \"D. ventrimaculatus\" and \"D. fantasticus\", that exhibit biparental care. The trend of using males to guard or brood eggs for biparental care or paternal care can be understood from the perspective of the female. After oviposition, or when the eggs are laid, the females need to replenish their bodies that have been dedicated to nurturing the eggs before they can mate again. Brooding by the females would delay the opportunity to mate by about two to four weeks.Since this outcome would cause many males to compete for a few females that are able to mate, the males are favored for the brooding.\n\nThe environment can have a substantial impact on the uses of parental care. Not all tropical frogs have the ability to lay their eggs plainly on land or plants. Tropical frogs can choose from a variety of water sources, such as lakes, streams, and small puddles. There is greater risk involved with reproducing in bigger bodies of water because of the higher likelihood of fish and other aquatic predators being there. Instead, frogs can choose to place eggs in phytotelmata. However, there is a trade-off that comes with electing a smaller water source. Not much sunlight reaches these locations, so algae and other food sources cannot grow to feed the inhabitants. Tropical frogs must use alternative methods of feeding their tadpole offspring. In the case of using phytotelmata, it is very difficult for one parent to guard and feed his or her offspring in possibly several different places. Roles performed by both parents provide a great advantage to the offspring. As seen in the Amazon Rain Forrest, the different size of the bodies of water chosen for breeding correlates with the amount of biparental care in two very similar species of the \"Ranitomeya\" genus. \"Ranitomeya imitator\" favors smaller pools and uses biparental care. Conversely, \"Ranitomeya variabilis\" utilizes larger bodies of water for breeding and only males take part in parental care. The ecologic aspects of a species habitat can have significant impacts on the type of parental care exhibited.\n\nTrophic egg feeding plays a key role in the ability of frogs to breed in smaller bodies of water that lack food sources. Many species of tropical frogs have an inherent nature of cannibalism, such as \"Dendrobates vanzolinii\", that allow their tadpoles to utilize the eggs for nourishment. With a male guarding the eggs, an intermediate step to developing biparental care may have been using the eggs from a mating with another female to feed existing tadpoles. Males could direct where the eggs should be positioned, and then he could move them into the water that holds his tadpole offspring. This polygynous relationship puts a cost on the female because she loses eggs to benefit offspring that are not her own. It is possible that the female could counter this effect by participating in biparental care with her mate. Also, since there may be a lack of males, females could benefit from attacking other clutches that her mate might have fertilized. Eliminating competition of a female’s offspring might result in higher survival of those she is trying to protect. Females have been seen eating other females’ eggs in captivity by certain species, such as \"Dendrobates auratus\". This intrasexual competition among the females might have been another important driving force for bringing about biparental care. Multiple factors contributed to the evolution of biparental care in some species of tropical frogs.\n", "id": "44391262", "title": "Evolution of biparental care in tropical frogs"}
{"url": "https://en.wikipedia.org/wiki?curid=19179706", "text": "Abiogenesis\n\nAbiogenesis (British English: , ), biopoiesis, or informally the origin of life, is the natural process by which life arises from non-living matter, such as simple organic compounds. On Earth, the transition from non-living to living entities was not a single event but a gradual process of increasing complexity. Abiogenesis is studied through a combination of paleontology, chemistry, and extrapolation from the characteristics of modern organisms, and aims to determine how pre-life chemical reactions gave rise to life on Earth.\n\nThe study of abiogenesis can be geophysical, chemical, or biological, with more recent approaches attempting a synthesis of all three, as life arose under conditions that are strikingly different from those on Earth today. Life functions through the specialized chemistry of carbon and water and is largely based upon four key families of chemicals: lipids (fatty cell walls), carbohydrates (sugars, cellulose), amino acids (protein metabolism), and nucleic acids (self-replicating DNA and RNA). Any successful theory of abiogenesis must explain the origins and interactions of these classes of molecules. Many approaches to abiogenesis investigate how self-replicating molecules, or their components, came into existence. It is generally thought that current life on Earth is descended from an RNA world, although RNA-based life may not have been the first life to have existed.\n\nThe classic Miller–Urey experiment and similar research demonstrated that most amino acids, the basic chemical constituents of the proteins used in all living organisms, can be synthesized from inorganic compounds under conditions intended to replicate those of the early Earth. Various external sources of energy that may have triggered these reactions have been proposed, including lightning and radiation. Other approaches (\"metabolism-first\" hypotheses) focus on understanding how catalysis in chemical systems on the early Earth might have provided the precursor molecules necessary for self-replication. Complex organic molecules have been found in the Solar System and in interstellar space, and these molecules may have provided starting material for the development of life on Earth.\n\nThe biochemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the age of the universe was only 10 to 17 million years old. The panspermia hypothesis alternatively suggests that microscopic life was distributed to the early Earth by space dust, meteoroids, asteroids and other small Solar System bodies and that life may exist throughout the Universe. The panspermia hypothesis proposes that life originated outside the Earth, not how life came to be.\n\nNonetheless, Earth remains the only place in the Universe known to harbour life, and fossil evidence from the Earth informs most studies of abiogenesis. More than 99% of all species of life forms, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. The age of the Earth is about 4.54 billion years old; the earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago, and possibly as early as the Eoarchean Era (between 3.6 and 4.0 billion years ago), after geological crust started to solidify following the molten Hadean Eon. In May 2017, evidence of the earliest known life on land may have been found in 3.48-billion-year-old geyserite and other related mineral deposits (often found around hot springs and geysers) uncovered in the Pilbara Craton of Western Australia. However, there have been a number of discoveries that suggested the earliest appearance of life on Earth was even earlier. Currently, microfossils within hydrothermal vent precipitates dated from 3.77 to 4.28 billion years old found in Quebec, Canada may be the oldest record of life on Earth, suggesting \"an almost instantaneous emergence of life\" after ocean formation 4.4 billion years ago. According to biologist Stephen Blair Hedges, \"If life arose relatively quickly on Earth … then it could be common in the universe.\"\n\nThe Hadean Earth is thought to have had a secondary atmosphere, formed through degassing of the rocks that accumulated from planetesimal impactors. At first, it was thought that the Earth's atmosphere consisted of hydrogen compounds—methane, ammonia and water vapour—and that life began under such reducing conditions, which are conducive to the formation of organic molecules. During its formation, the Earth lost a significant part of its initial mass, with a nucleus of the heavier rocky elements of the protoplanetary disk remaining. According to later models, suggested by study of ancient minerals, the atmosphere in the late Hadean period consisted largely of water vapour, nitrogen and carbon dioxide, with smaller amounts of carbon monoxide, hydrogen, and sulfur compounds. As Earth lacked the gravity to hold any molecular hydrogen, this component of the atmosphere would have been rapidly lost during the Hadean period, along with the bulk of the original inert gases. The solution of carbon dioxide in water is thought to have made the seas slightly acidic, giving it a pH of about 5.5. The atmosphere at the time has been characterized as a \"gigantic, productive outdoor chemical laboratory.\" It may have been similar to the mixture of gases released today by volcanoes, which still support some abiotic chemistry.\n\nOceans may have appeared first in the Hadean Eon, as soon as two hundred million years (200 Ma) after the Earth was formed, in a hot reducing environment, and the pH of about 5.8 rose rapidly towards neutral. This has been supported by the dating of 4.404 Ga-old zircon crystals from metamorphosed quartzite of Mount Narryer in the Western Australia Jack Hills of the Pilbara, which are evidence that oceans and continental crust existed within 150 Ma of Earth's formation. Despite the likely increased volcanism and existence of many smaller tectonic \"platelets,\" it has been suggested that between 4.4 and 4.3 Ga (billion year), the Earth was a water world, with little if any continental crust, an extremely turbulent atmosphere and a hydrosphere subject to intense ultraviolet (UV) light, from a T Tauri stage Sun, cosmic radiation and continued bolide impacts.\n\nThe Hadean environment would have been highly hazardous to modern life. Frequent collisions with large objects, up to in diameter, would have been sufficient to sterilize the planet and vaporize the ocean within a few months of impact, with hot steam mixed with rock vapour becoming high altitude clouds that would completely cover the planet. After a few months, the height of these clouds would have begun to decrease but the cloud base would still have been elevated for about the next thousand years. After that, it would have begun to rain at low altitude. For another two thousand years, rains would slowly have drawn down the height of the clouds, returning the oceans to their original depth only 3,000 years after the impact event.\n\nThe most commonly accepted location of the root of the tree of life is between a monophyletic domain Bacteria and a clade formed by Archaea and Eukaryota of what is referred to as the \"traditional tree of life\" based on several molecular studies starting with C. Woese. A very small minority of studies have concluded differently, namely that the root is in the Domain Bacteria, either in the phylum Firmicutes or that the phylum Chloroflexi is basal to a clade with Archaea+Eukaryotes and the rest of Bacteria as proposed by Thomas Cavalier-Smith. More recently Peter Ward has established an alternative view which is rooted in abiotic RNA synthesis which becomes enclosed within a capsule and then creates RNA ribozyme replicates. It is proposed that this then bifurcates between Dominion Ribosa (hypothetical Domain Ribosa or RNA life), and after the loss of ribozymes RNA viruses as Domain Viorea, and Dominion Terroa, which after creating a large cell within a lipid wall, creating DNA the 20 based amino acids and the triplet code, is established as the last universal common ancestor or LUCA, of earlier phylogenic trees.\n\nThe earliest life on Earth existed more than 3.5 billion years ago, during the Eoarchean Era when sufficient crust had solidified following the molten Hadean Eon. The earliest physical evidence so far found consists of microfossils in the Nuvvuagittuq Greenstone Belt of Northern Quebec, in \"banded iron formation\" rocks at least 3.77 billion and possibly 4.28 billion years old. This finding suggested that there was almost instant development of life after oceans were formed. The structure of the microbes was noted to be similar to bacteria found near hydrothermal vents in the modern era, and provided support for the hypothesis that abiogenesis began near hydrothermal vents.\n\nAlso noteworthy is biogenic graphite in 3.7 billion-year-old metasedimentary rocks from southwestern Greenland and microbial mat fossils found in 3.48 billion-year-old sandstone from Western Australia. Evidence of early life in rocks from Akilia Island, near the Isua supracrustal belt in southwestern Greenland, dating to 3.7 billion years ago have shown biogenic carbon isotopes. In other parts of the Isua supracrustal belt, graphite inclusions trapped within garnet crystals are connected to the other elements of life: oxygen, nitrogen, and possibly phosphorus in the form of phosphate, providing further evidence for life 3.7 billion years ago. At Strelley Pool, in the Pilbara region of Western Australia, compelling evidence of early life was found in pyrite-bearing sandstone in a fossilized beach, that showed rounded tubular cells that oxidized sulfur by photosynthesis in the absence of oxygen. Further research on zircons from Western Australia in 2015 suggested that life likely existed on Earth at least 4.1 billion years ago.\n\nTraditionally it was thought that during the period between 4.28 and 3.8 Ga, changes in the orbits of the giant planets may have caused a heavy bombardment by asteroids and comets that pockmarked the Moon and the other inner planets (Mercury, Mars, and presumably Earth and Venus). This would likely have repeatedly sterilized the planet, had life appeared before that time. Geologically, the Hadean Earth would have been far more active than at any other time in its history. Studies of meteorites suggests that radioactive isotopes such as aluminium-26 with a half-life of 7.17×10 (717 thousand) years, and potassium-40 with a half-life of 1.250×10 (1.25 billion) years, isotopes mainly produced in supernovae, were much more common. Internal heating as a result of gravitational sorting between the core and the mantle would have caused a great deal of mantle convection, with the probable result of many more smaller and more active tectonic plates than now exist.\n\nThe time periods between such devastating environmental events give time windows for the possible origin of life in the early environments. If the deep marine hydrothermal setting was the site for the origin of life, then abiogenesis could have happened as early as 4.0 to 4.2 Ga. If the site was at the surface of the Earth, abiogenesis could only have occurred between 3.7 and 4.0 Ga.\n\nIn 2016, a set of 355 genes likely present in the Last Universal Common Ancestor (LUCA) of all organisms living on Earth was identified. A total of 6.1 million prokaryotic protein coding genes from various phylogenic trees were sequenced, identifying 355 protein clusters from amongst 286,514 protein clusters that were probably common to LUCA. The results \"depict LUCA as anaerobic, CO-fixing, H-dependent with a Wood–Ljungdahl pathway, N-fixing and thermophilic. LUCA’s biochemistry was replete with FeS clusters and radical reaction mechanisms. Its cofactors reveal dependence upon transition metals, flavins, S-adenosyl methionine, coenzyme A, ferredoxin, molybdopterin, corrins and selenium. Its genetic code required nucleoside modifications and S-adenosylmethionine-dependent methylations.\" The results depict methanogenic clostridia as a basal clade in the 355 phylogenies examined, and suggest that LUCA inhabited an anaerobic hydrothermal vent setting in a geochemically active environment rich in H, CO and iron. M.D. Brazier has shown that the tiny fossils discovered came from a hot poisonous world of the toxic gases methane, ammonia, carbon dioxide and hydrogen sulphide. An analysis of the conventional threefold tree of life shows thermophilic and hyperthermophilic bacteria and archaea are closest to the root, suggesting that life may have evolved in a hot environment.\n\nBelief in spontaneous generation of certain forms of life from non-living matter goes back to Aristotle and ancient Greek philosophy and continued to have support in Western scholarship until the 19th century. This belief was paired with a belief in heterogenesis, i.e., that one form of life derived from a different form (e.g., bees from flowers). Classical notions of spontaneous generation held that certain complex, living organisms are generated by decaying organic substances. According to Aristotle, it was a readily observable truth that aphids arise from the dew that falls on plants, flies from putrid matter, mice from dirty hay, crocodiles from rotting logs at the bottom of bodies of water, and so on. In the 17th century, people began to question such assumptions. In 1646, Sir Thomas Browne published his \"Pseudodoxia Epidemica\" (subtitled \"Enquiries into Very many Received Tenets, and commonly Presumed Truths\"), which was an attack on false beliefs and \"vulgar errors.\" His contemporary, Alexander Ross, erroneously refuted him, stating: \"To question this [spontaneous generation], is to question Reason, Sense, and Experience: If he doubts of this, let him go to \"Ægypt\", and there he will finde the fields swarming with mice begot of the mud of \"Nylus\", to the great calamity of the Inhabitants.\"\n\nIn 1665, Robert Hooke published the first drawings of a microorganism. Hooke was followed in 1676 by Antonie van Leeuwenhoek, who drew and described microorganisms that are now thought to have been protozoa and bacteria. Many felt the existence of microorganisms was evidence in support of spontaneous generation, since microorganisms seemed too simplistic for sexual reproduction, and asexual reproduction through cell division had not yet been observed. Van Leeuwenhoek took issue with the ideas common at the time that fleas and lice could spontaneously result from putrefaction, and that frogs could likewise arise from slime. Using a broad range of experiments ranging from sealed and open meat incubation and the close study of insect reproduction he became, by the 1680s, convinced that spontaneous generation was incorrect.\n\nThe first experimental evidence against spontaneous generation came in 1668 when Francesco Redi showed that no maggots appeared in meat when flies were prevented from laying eggs. It was gradually shown that, at least in the case of all the higher and readily visible organisms, the previous sentiment regarding spontaneous generation was false. The alternative seemed to be biogenesis: that every living thing came from a pre-existing living thing (\"omne vivum ex ovo\", Latin for \"every living thing from an egg\").\n\nIn 1768, Lazzaro Spallanzani demonstrated that microbes were present in the air, and could be killed by boiling. In 1861, Louis Pasteur performed a series of experiments that demonstrated that organisms such as bacteria and fungi do not spontaneously appear in sterile, nutrient-rich media, but could only appear by invasion from without.\n\nThe belief that self-ordering by spontaneous generation was impossible begged for an alternative. By the middle of the 19th century, the theory of biogenesis had accumulated so much evidential support, due to the work of Pasteur and others, that the alternative theory of spontaneous generation had been effectively disproven. John Desmond Bernal, a pioneer in X-ray crystallography, suggested that earlier theories such as spontaneous generation were based upon an explanation that life was continuously created as a result of chance events.\n\nThe term \"biogenesis\" is usually credited to either Henry Charlton Bastian or to Thomas Henry Huxley. Bastian used the term around 1869 in an unpublished exchange with John Tyndall to mean \"life-origination or commencement\". In 1870, Huxley, as new president of the British Association for the Advancement of Science, delivered an address entitled \"Biogenesis and Abiogenesis\". In it he introduced the term \"biogenesis\" (with an opposite meaning to Bastian's) as well as \"abiogenesis\":\n\nSubsequently, in the preface to Bastian's 1871 book, \"The Modes of Origin of Lowest Organisms\", Bastian referred to the possible confusion with Huxley's usage and explicitly renounced his own meaning:\n\nLouis Pasteur remarked, about a finding of his in 1864 which he considered definitive, \"Never will the doctrine of spontaneous generation recover from the mortal blow struck by this simple experiment.\" One alternative was that life's origins on Earth had come from somewhere else in the Universe. Periodically resurrected (see Panspermia, above) Bernal said that this approach \"is equivalent in the last resort to asserting the operation of metaphysical, spiritual entities... it turns on the argument of creation by design by a creator or demiurge.\" Such a theory, Bernal said, was unscientific. A theory popular around the same time was that life was the result of an inner \"life force\", which in the late 19th century was championed by Henri Bergson.\n\nThe idea of evolution by natural selection proposed by Charles Darwin put an end to these metaphysical theologies. In a letter to Joseph Dalton Hooker on 1 February 1871, Darwin discussed the suggestion that the original spark of life may have begun in a \"warm little pond, with all sorts of ammonia and phosphoric salts, light, heat, electricity, , present, that a compound was chemically formed ready to undergo still more complex changes.\" He went on to explain that \"at the present day such matter would be instantly devoured or absorbed, which would not have been the case before living creatures were formed.\" He had written to Hooker in 1863 stating that, \"It is mere rubbish, thinking at present of the origin of life; one might as well think of the origin of matter.\" In \"On the Origin of Species\", he had referred to life having been \"created\", by which he \"really meant 'appeared' by some wholly unknown process\", but had soon regretted using the Old Testament term \"creation\".\n\nNo new notable research or theory on the subject appeared until 1924, when Alexander Oparin reasoned that atmospheric oxygen prevents the synthesis of certain organic compounds that are necessary building blocks for the evolution of life. In his book \"The Origin of Life\", Oparin proposed that the \"spontaneous generation of life\" that had been attacked by Louis Pasteur did in fact occur once, but was now impossible because the conditions found on the early Earth had changed, and preexisting organisms would immediately consume any spontaneously generated organism. Oparin argued that a \"primeval soup\" of organic molecules could be created in an oxygenless atmosphere through the action of sunlight. These would combine in ever more complex ways until they formed coacervate droplets. These droplets would \"grow\" by fusion with other droplets, and \"reproduce\" through fission into daughter droplets, and so have a primitive metabolism in which factors that promote \"cell integrity\" survive, and those that do not become extinct. Many modern theories of the origin of life still take Oparin's ideas as a starting point.\n\nRobert Shapiro has summarized the \"primordial soup\" theory of Oparin and J. B. S. Haldane in its \"mature form\" as follows:\n\nAbout this time, Haldane suggested that the Earth's prebiotic oceans (quite different from their modern counterparts) would have formed a \"hot dilute soup\" in which organic compounds could have formed. Bernal called this idea \"biopoiesis\" or \"biopoesis\", the process of living matter evolving from self-replicating but non-living molecules, and proposed that biopoiesis passes through a number of intermediate stages.\n\nOne of the most important pieces of experimental support for the \"soup\" theory came in 1952. Stanley L. Miller and Harold C. Urey performed an experiment that demonstrated how organic molecules could have spontaneously formed from inorganic precursors under conditions like those posited by the Oparin-Haldane hypothesis. The now-famous Miller–Urey experiment used a highly reducing mixture of gases – methane, ammonia, and hydrogen, as well as water vapour – to form basic organic monomers such as amino acids. The mixture of gases was cycled through an apparatus that delivered electrical sparks to the mixture. After one week, it was found that about 10% to 15% of the carbon in the system was then in the form of a racemic mixture of organic compounds, including amino acids, which are the building blocks of proteins. This provided direct experimental support for the second point of the \"soup\" theory, and it is around the remaining two points of the theory that much of the debate now centres.\n\nBernal showed that based upon this and subsequent work there is no difficulty in principle in forming most of the molecules we recognize as the basic molecules of life from their inorganic precursors. The underlying hypothesis held by Oparin, Haldane, Bernal, Miller and Urey, for instance, was that multiple conditions on the primeval Earth favoured chemical reactions that synthesized the same set of complex organic compounds from such simple precursors. A 2011 reanalysis of the saved vials containing the original extracts that resulted from the Miller and Urey experiments, using current and more advanced analytical equipment and technology, has uncovered more biochemicals than originally discovered in the 1950s. One of the more important findings was 23 amino acids, far more than the five originally found. However, Bernal said that \"it is not enough to explain the formation of such molecules, what is necessary, is a physical-chemical explanation of the origins of these molecules that suggests the presence of suitable sources and sinks for free energy.\"\n\nMore recent studies, in October 2017, support the notion that life may have begun right after the Earth was formed as RNA molecules emerging from \"warm little ponds\".\n\nIn trying to uncover the intermediate stages of abiogenesis mentioned by Bernal, Sidney W. Fox in the 1950s and 1960s studied the spontaneous formation of peptide structures (small chains of amino acids) under conditions that might plausibly have existed early in Earth's history. In one of his experiments, he allowed amino acids to dry out as if puddled in a warm, dry spot in prebiotic conditions. He found that, as they dried, the amino acids formed long, often cross-linked, thread-like, submicroscopic polypeptide molecules now named \"proteinoid microspheres.\"\n\nIn another experiment to set suitable conditions for life to form, Fox collected volcanic material from a cinder cone in Hawaii. He discovered that the temperature was over just beneath the surface of the cinder cone, and suggested that this might have been the environment in which life was created — molecules could have formed and then been washed through the loose volcanic ash into the sea. He placed lumps of lava over amino acids derived from methane, ammonia and water, sterilized all materials, and baked the lava over the amino acids for a few hours in a glass oven. A brown, sticky substance formed over the surface, and when the lava was drenched in sterilized water, a thick, brown liquid leached out. The amino acids had combined to form proteinoids, and the proteinoids had combined to form small globules that Fox called \"microspheres.\" His proteinoids were not cells, although they formed clumps and chains reminiscent of cyanobacteria, but they contained no functional nucleic acids or any encoded information. Based upon such experiments, Colin S. Pittendrigh stated in December 1967 that \"laboratories will be creating a living cell within ten years,\" a remark that reflected the typical contemporary naivety about the complexity of cell structures.\n\nThere is no single, generally accepted model for the origin of life. Scientists have proposed several plausible theories, which share some common elements. While differing in the details, these theories are based on the framework laid out by Alexander Oparin (in 1924) and by J. B. S. Haldane (in 1925), who postulated the molecular or chemical evolution theory of life. According to them, the first molecules constituting the earliest cells \"were synthesized under natural conditions by a slow process of molecular evolution, and these molecules then organized into the first molecular system with properties with biological order\". Oparin and Haldane suggested that the atmosphere of the early Earth may have been chemically reducing in nature, composed primarily of methane (CH), ammonia (NH), water (HO), hydrogen sulfide (HS), carbon dioxide (CO) or carbon monoxide (CO), and phosphate (PO), with molecular oxygen (O) and ozone (O) either rare or absent. According to later models, the atmosphere in the late Hadean period consisted largely of nitrogen (N) and carbon dioxide, with smaller amounts of carbon monoxide, hydrogen (H), and sulfur compounds; while it did lack molecular oxygen and ozone, it was not as chemically reducing as Oparin and Haldane supposed. In the atmosphere proposed by Oparin and Haldane, electrical activity can produce certain basic small molecules (monomers) of life, such as amino acids. The Miller–Urey experiment reported in 1953 demonstrated this.\n\nBernal coined the term \"biopoiesis\" in 1949 to refer to the origin of life. In 1967, he suggested that it occurred in three \"stages\":\n\n\nBernal suggested that evolution commenced between stages 1 and 2. Bernal regarded the third stage – discovering methods by which biological reactions were incorporated behind a cell's boundary – as the most difficult. Modern work on the way that cell membranes self-assemble, and the work on micropores in various substrates may be a halfway house towards the development of independent free-living cells.\n\nThe chemical processes that took place on the early Earth are called \"chemical evolution\". Both Manfred Eigen and Sol Spiegelman demonstrated that evolution, including replication, variation, and natural selection, can occur in populations of molecules as well as in organisms. Spiegelman took advantage of natural selection to synthesize the Spiegelman Monster, which had a genome with just 218 nucleotide bases, having deconstructively evolved from a 4500-base bacterial RNA. Eigen built on Spiegelman's work and produced a similar system further degraded to just 48 or 54 nucleotides – the minimum required for the binding of the replication enzyme.\n\nFollowing on from chemical evolution came the initiation of biological evolution, which led to the first cells. No one has yet synthesized a \"protocell\" using basic components with the necessary properties of life (the so-called \"bottom-up-approach\"). Without such a proof-of-principle, explanations have tended to focus on chemosynthesis. However, some researchers work in this field, notably Steen Rasmussen and Jack W. Szostak. Others have argued that a \"top-down approach\" is more feasible. One such approach, successfully attempted by Craig Venter and others at J. Craig Venter Institute, involves engineering existing prokaryotic cells with progressively fewer genes, attempting to discern at which point the most minimal requirements for life are reached.\n\nThe NASA strategy on abiogenesis states that it is necessary to identify interactions, intermediary structures and functions, energy sources, and environmental factors that contributed to the diversity, selection, and replication of evolvable macromolecular systems. Emphasis must continue to map the chemical landscape of potential primordial informational polymers. The advent of polymers that could replicate, store genetic information, and exhibit properties subject to selection likely was a critical step in the emergence of prebiotic chemical evolution.\n\nThe elements, except for hydrogen and helium, ultimately derive from stellar nucleosynthesis. On 12 October 2016, astronomers reported that the very basic chemical ingredients of life — the carbon-hydrogen molecule (CH, or methylidyne radical), the carbon-hydrogen positive ion (CH+) and the carbon ion (C+) — are largely the result of ultraviolet light from stars, rather than other forms of radiation from supernovae and young stars, as thought earlier. Complex molecules, including organic molecules, form naturally both in space and on planets. There are two possible sources of organic molecules on the early Earth:\n\nBased on recent computer model studies, the complex organic molecules necessary for life may have formed in the protoplanetary disk of dust grains surrounding the Sun before the formation of the Earth. According to the computer studies, this same process may also occur around other stars that acquire planets. (Also see Extraterrestrial organic molecules).\n\nEstimates of the production of organics from these sources suggest that the Late Heavy Bombardment before 3.5 Ga within the early atmosphere made available quantities of organics comparable to those produced by terrestrial sources.\n\nIt has been estimated that the Late Heavy Bombardment may also have effectively sterilized the Earth's surface to a depth of tens of metres. If life evolved deeper than this, it would have also been shielded from the early high levels of ultraviolet radiation from the T Tauri stage of the Sun's evolution. Simulations of geothermically heated oceanic crust yield far more organics than those found in the Miller-Urey experiments (see below). In the deep hydrothermal vents, Everett Shock has found \"there is an enormous thermodynamic drive to form organic compounds, as seawater and hydrothermal fluids, which are far from equilibrium, mix and move towards a more stable state.\" Shock has found that the available energy is maximized at around 100 – 150 degrees Celsius, precisely the temperatures at which the hyperthermophilic bacteria and thermoacidophilic archaea have been found, at the base of the phylogenetic tree of life closest to the Last Universal Common Ancestor (LUCA).\n\nThe accumulation and concentration of organic molecules on a planetary surface is also considered an essential early step for the origin of life. Identifying and understanding the mechanisms that led to the production of prebiotic\nmolecules in various environments is critical for establishing the inventory of ingredients from which life originated on Earth, assuming that the abiotic production of molecules ultimately influenced the selection of molecules from which life emerged.\n\nWhile features of self-organization and self-replication are often considered the hallmark of living systems, there are many instances of abiotic molecules exhibiting such characteristics under proper conditions. Stan Palasek suggested based on a theoretical model that self-assembly of ribonucleic acid (RNA) molecules can occur spontaneously due to physical factors in hydrothermal vents. Virus self-assembly within host cells has implications for the study of the origin of life, as it lends further credence to the hypothesis that life could have started as self-assembling organic molecules.\n\nMultiple sources of energy were available for chemical reactions on the early Earth. For example, heat (such as from geothermal processes) is a standard energy source for chemistry. Other examples include sunlight and electrical discharges (lightning), among others. Unfavourable reactions can also be driven by highly favourable ones, as in the case of iron-sulfur chemistry. For example, this was probably important for carbon fixation (the conversion of carbon from its inorganic form to an organic one). Carbon fixation via iron-sulfur chemistry is highly favourable, and occurs at neutral pH and . Iron-sulfur surfaces, which are abundant near hydrothermal vents, are also capable of producing small amounts of amino acids and other biological metabolites.\n\nFormamide produces all four ribonucleotides and other biological molecules when warmed in the presence of various terrestrial minerals. Formamide is ubiquitous in the Universe, produced by the reaction of water and hydrogen cyanide (HCN). It has several advantages as a biotic precursor, including the ability to easily become concentrated through the evaporation of water. Although HCN is poisonous, it only affects aerobic organisms (eukaryotes and aerobic bacteria), which did not yet exist. It can play roles in other chemical processes as well, such as the synthesis of the amino acid glycine.\n\nIn 1961, it was shown that the nucleic acid purine base adenine can be formed by heating aqueous ammonium cyanide solutions. Other pathways for synthesizing bases from inorganic materials were also reported. Leslie E. Orgel and colleagues have shown that freezing temperatures are advantageous for the synthesis of purines, due to the concentrating effect for key precursors such as hydrogen cyanide. Research by Stanley L. Miller and colleagues suggested that while adenine and guanine require freezing conditions for synthesis, cytosine and uracil may require boiling temperatures. Research by the Miller group notes the formation of seven different amino acids and 11 types of nucleobases in ice when ammonia and cyanide were left in a freezer from 1972 to 1997. Other work demonstrated the formation of s-triazines (alternative nucleobases), pyrimidines (including cytosine and uracil), and adenine from urea solutions subjected to freeze-thaw cycles under a reductive atmosphere (with spark discharges as an energy source). The explanation given for the unusual speed of these reactions at such a low temperature is eutectic freezing. As an ice crystal forms, it stays pure: only molecules of water join the growing crystal, while impurities like salt or cyanide are excluded. These impurities become crowded in microscopic pockets of liquid within the ice, and this crowding causes the molecules to collide more often. Mechanistic exploration using quantum chemical methods provide a more detailed understanding of some of the chemical processes involved in chemical evolution, and a partial answer to the fundamental question of molecular biogenesis.\n\nAt the time of the Miller–Urey experiment, scientific consensus was that the early Earth had a reducing atmosphere with compounds relatively rich in hydrogen and poor in oxygen (e.g., CH and NH as opposed to CO and nitrogen dioxide (NO)). However, current scientific consensus describes the primitive atmosphere as either weakly reducing or neutral (see also Oxygen Catastrophe). Such an atmosphere would diminish both the amount and variety of amino acids that could be produced, although studies that include iron and carbonate minerals (thought present in early oceans) in the experimental conditions have again produced a diverse array of amino acids. Other scientific research has focused on two other potential reducing environments: outer space and deep-sea thermal vents.\n\nThe spontaneous formation of complex polymers from abiotically generated monomers under the conditions posited by the \"soup\" theory is not at all a straightforward process. Besides the necessary basic organic monomers, compounds that would have prohibited the formation of polymers were also formed in high concentration during the Miller–Urey and Joan Oró experiments. The Miller–Urey experiment, for example, produces many substances that would react with the amino acids or terminate their coupling into peptide chains.\n\nA research project completed in March 2015 by John D. Sutherland and others found that a network of reactions beginning with hydrogen cyanide and hydrogen sulfide, in streams of water irradiated by UV light, could produce the chemical components of proteins and lipids, as well as those of RNA, while not producing a wide range of other compounds. The researchers used the term \"cyanosulfidic\" to describe this network of reactions.\n\nAutocatalysts are substances that catalyze the production of themselves and therefore are \"molecular replicators.\" The simplest self-replicating chemical systems are autocatalytic, and typically contain three components: a product molecule and two precursor molecules. The product molecule joins together the precursor molecules, which in turn produce more product molecules from more precursor molecules. The product molecule catalyzes the reaction by providing a complementary template that binds to the precursors, thus bringing them together. Such systems have been demonstrated both in biological macromolecules and in small organic molecules. Systems that do not proceed by template mechanisms, such as the self-reproduction of micelles and vesicles, have also been observed.\n\nIt has been proposed that life initially arose as autocatalytic chemical networks. British ethologist Richard Dawkins wrote about autocatalysis as a potential explanation for the origin of life in his 2004 book \"The Ancestor's Tale\". In his book, Dawkins cites experiments performed by Julius Rebek Jr. and his colleagues in which they combined amino adenosine and pentafluorophenyl esters with the autocatalyst amino adenosine triacid ester (AATE). One product was a variant of AATE, which catalyzed the synthesis of themselves. This experiment demonstrated the possibility that autocatalysts could exhibit competition within a population of entities with heredity, which could be interpreted as a rudimentary form of natural selection.\n\nIn the early 1970s, Manfred Eigen and Peter Schuster examined the transient stages between the molecular chaos and a self-replicating hypercycle in a prebiotic soup. In a hypercycle, the information storing system (possibly RNA) produces an enzyme, which catalyzes the formation of another information system, in sequence until the product of the last aids in the formation of the first information system. Mathematically treated, hypercycles could create quasispecies, which through natural selection entered into a form of Darwinian evolution. A boost to hypercycle theory was the discovery of ribozymes capable of catalyzing their own chemical reactions. The hypercycle theory requires the existence of complex biochemicals, such as nucleotides, which do not form under the conditions proposed by the Miller–Urey experiment.\n\nGeoffrey W. Hoffmann has shown that an early error-prone translation machinery can be stable against an error catastrophe of the type that had been envisaged as problematical for the origin of life, and was known as \"Orgel's paradox\".\n\nHoffmann has furthermore argued that a complex nucleation event as the origin of life involving both polypeptides and nucleic acid is compatible with the time and space available in the primitive oceans of Earth Hoffmann suggests that volcanic ash may provide the many random shapes needed in the postulated complex nucleation event. This aspect of the theory can be tested experimentally.\n\nHomochirality refers to the geometric property of some materials that are composed of chiral units. Chiral refers to nonsuperimposable 3D forms that are mirror images of one another, as are left and right hands. Living organisms use molecules that have the same chirality (\"handedness\"): with almost no exceptions, amino acids are left-handed while nucleotides and sugars are right-handed. Chiral molecules can be synthesized, but in the absence of a chiral source or a chiral catalyst, they are formed in a 50/50 mixture of both enantiomers (called a racemic mixture). Known mechanisms for the production of non-racemic mixtures from racemic starting materials include: asymmetric physical laws, such as the electroweak interaction; asymmetric environments, such as those caused by circularly polarized light, quartz crystals, or the Earth's rotation, statistical fluctuations during racemic synthesis, and spontaneous symmetry breaking.\n\nOnce established, chirality would be selected for. A small bias (enantiomeric excess) in the population can be amplified into a large one by asymmetric autocatalysis, such as in the Soai reaction. In asymmetric autocatalysis, the catalyst is a chiral molecule, which means that a chiral molecule is catalyzing its own production. An initial enantiomeric excess, such as can be produced by polarized light, then allows the more abundant enantiomer to outcompete the other.\n\nClark has suggested that homochirality may have started in outer space, as the studies of the amino acids on the Murchison meteorite showed that L-alanine is more than twice as frequent as its D form, and L-glutamic acid was more than three times prevalent than its D counterpart. Various chiral crystal surfaces can also act as sites for possible concentration and assembly of chiral monomer units into macromolecules. Compounds found on meteorites suggest that the chirality of life derives from abiogenic synthesis, since amino acids from meteorites show a left-handed bias, whereas sugars show a predominantly right-handed bias, the same as found in living organisms.\n\nA protocell is a self-organized, self-ordered, spherical collection of lipids proposed as a stepping-stone to the origin of life. A central question in evolution is how simple protocells first arose and differed in reproductive contribution to the following generation driving the evolution of life. Although a functional protocell has not yet been achieved in a laboratory setting, there are scientists who think the goal is well within reach.\n\nSelf-assembled vesicles are essential components of primitive cells. The second law of thermodynamics requires that the Universe move in a direction in which entropy increases, yet life is distinguished by its great degree of organization. Therefore, a boundary is needed to separate life processes from non-living matter. Researchers Irene A. Chen and Jack W. Szostak amongst others, suggest that simple physicochemical properties of elementary protocells can give rise to essential cellular behaviours, including primitive forms of differential reproduction competition and energy storage. Such cooperative interactions between the membrane and its encapsulated contents could greatly simplify the transition from simple replicating molecules to true cells. Furthermore, competition for membrane molecules would favour stabilized membranes, suggesting a selective advantage for the evolution of cross-linked fatty acids and even the phospholipids of today. Such micro-encapsulation would allow for metabolism within the membrane, the exchange of small molecules but the prevention of passage of large substances across it. The main advantages of encapsulation include the increased solubility of the contained cargo within the capsule and the storage of energy in the form of a electrochemical gradient.\n\nA 2012 study led by Armen Y. Mulkidjanian of Germany's University of Osnabrück, suggests that inland pools of condensed and cooled geothermal vapour have the ideal characteristics for the origin of life. Scientists confirmed in 2002 that by adding a montmorillonite clay to a solution of fatty acid micelles (lipid spheres), the clay sped up the rate of vesicles formation 100-fold.\n\nAnother protocell model is the Jeewanu. First synthesized in 1963 from simple minerals and basic organics while exposed to sunlight, it is still reported to have some metabolic capabilities, the presence of semipermeable membrane, amino acids, phospholipids, carbohydrates and RNA-like molecules. However, the nature and properties of the Jeewanu remains to be clarified.\n\nElectrostatic interactions induced by short, positively charged, hydrophobic peptides containing 7 amino acids in length or fewer, can attach RNA to a vesicle membrane, the basic cell membrane.\n\nThe RNA world hypothesis describes an early Earth with self-replicating and catalytic RNA but no DNA or proteins. It is generally accepted that current life on Earth descends from an RNA world, although RNA-based life may not have been the first life to exist. This conclusion is drawn from many independent lines of evidence, such as the observations that RNA is central to the translation process and that small RNAs can catalyze all of the chemical groups and information transfers required for life. The structure of the ribosome has been called the \"smoking gun,\" as it showed that the ribosome is a ribozyme, with a central core of RNA and no amino acid side chains within 18 angstroms of the active site where peptide bond formation is catalyzed. The concept of the RNA world was first proposed in 1962 by Alexander Rich, and the term was coined by Walter Gilbert in 1986.\n\nPossible precursors for the evolution of protein synthesis include a mechanism to synthesize short peptide cofactors or form a mechanism for the duplication of RNA. It is likely that the ancestral ribosome was composed entirely of RNA, although some roles have since been taken over by proteins. Major remaining questions on this topic include identifying the selective force for the evolution of the ribosome and determining how the genetic code arose.\n\nEugene Koonin said, \"Despite considerable experimental and theoretical effort, no compelling scenarios currently exist for the origin of replication and translation, the key processes that together comprise the core of biological systems and the apparent pre-requisite of biological evolution. The RNA World concept might offer the best chance for the resolution of this conundrum but so far cannot adequately account for the emergence of an efficient RNA replicase or the translation system. The MWO [Ed.: \"many worlds in one\"] version of the cosmological model of eternal inflation could suggest a way out of this conundrum because, in an infinite multiverse with a finite number of distinct macroscopic histories (each repeated an infinite number of times), emergence of even highly complex systems by chance is not just possible but inevitable.\"\n\nRecent evidence for a \"virus first\" hypothesis, which may support theories of the RNA world, has been suggested. One of the difficulties for the study of the origins of viruses is their high rate of mutation; this is particularly the case in RNA retroviruses like HIV. A 2015 study compared protein fold structures across different branches of the tree of life, where researchers can reconstruct the evolutionary histories of the folds and of the organisms whose genomes code for those folds. They argue that protein folds are better markers of ancient events as their three-dimensional structures can be maintained even as the sequences that code for those begin to change. Thus, the viral protein repertoire retain traces of ancient evolutionary history that can be recovered using advanced bioinformatics approaches. Those researchers think that \"the prolonged pressure of genome and particle size reduction eventually reduced virocells into modern viruses (identified by the complete loss of cellular makeup), meanwhile other coexisting cellular lineages diversified into modern cells. The data suggest that viruses originated from ancient cells that co-existed with the ancestors of modern cells. These ancient cells likely contained segmented RNA genomes.\n\nA number of hypotheses of formation of RNA have been put forward. , there were difficulties in the explanation of the abiotic synthesis of the nucleotides cytosine and uracil. Subsequent research has shown possible routes of synthesis; for example, formamide produces all four ribonucleotides and other biological molecules when warmed in the presence of various terrestrial minerals. Early cell membranes could have formed spontaneously from proteinoids, which are protein-like molecules produced when amino acid solutions are heated while in the correct concentration of aqueous solution. These are seen to form micro-spheres which are observed to behave similarly to membrane-enclosed compartments. Other possible means of producing more complicated organic molecules include chemical reactions that take place on clay substrates or on the surface of the mineral pyrite.\n\nFactors supporting an important role for RNA in early life include its ability to act both to store information and to catalyze chemical reactions (as a ribozyme); its many important roles as an intermediate in the expression of and maintenance of the genetic information (in the form of DNA) in modern organisms; and the ease of chemical synthesis of at least the components of the RNA molecule under the conditions that approximated the early Earth. Relatively short RNA molecules have been synthesized, capable of replication. Such replicase RNA, which functions as both code and catalyst provides its own template upon which copying can occur. Jack W. Szostak has shown that certain catalytic RNAs can join smaller RNA sequences together, creating the potential for self-replication. If these conditions were present, Darwinian natural selection would favour the proliferation of such autocatalytic sets, to which further functionalities could be added. Such autocatalytic systems of RNA capable of self-sustained replication have been identified. The RNA replication systems, which include two ribozymes that catalyze each other's synthesis, showed a doubling time of the product of about one hour, and were subject to natural selection under the conditions that existed in the experiment. In evolutionary competition experiments, this led to the emergence of new systems which replicated more efficiently. This was the first demonstration of evolutionary adaptation occurring in a molecular genetic system.\n\nDepending on the definition, life started when RNA chains began to self-replicate, initiating the three mechanisms of Darwinian selection: heritability, variation of type, and differential reproductive output. The fitness of an RNA replicator (its per capita rate of increase) would likely be a function of its intrinsic adaptive capacities, determined by its nucleotide sequence, and the availability of resources. The three primary adaptive capacities may have been: (1) replication with moderate fidelity, giving rise to both heritability while allowing variation of type, (2) resistance to decay, and (3) acquisition of process resources. These capacities would have functioned by means of the folded configurations of the RNA replicators resulting from their nucleotide sequences.\n\nCarl Zimmer has speculated that the chemical conditions, including the presence of boron, molybdenum and oxygen needed for the initial production of RNA, may have been better on early Mars than on early Earth. If so, life-suitable molecules originating on Mars may have later migrated to Earth via meteor ejections.\n\nIt is possible that a different type of nucleic acid, such as PNA, TNA or GNA, was the first to emerge as a self-reproducing molecule, only later replaced by RNA. Larralde \"et al.\", say that \"the generally accepted prebiotic synthesis of ribose, the formose reaction, yields numerous sugars without any selectivity.\" and they conclude that their \"results suggest that the backbone of the first genetic material could not have contained ribose or other sugars because of their instability.\" The ester linkage of ribose and phosphoric acid in RNA is known to be prone to hydrolysis.\n\nPyrimidine ribonucleosides and their respective nucleotides have been prebiotically synthesized by a sequence of reactions which by-pass the free sugars, and are assembled in a stepwise fashion by using nitrogenous or oxygenous chemistries. Sutherland has demonstrated high yielding routes to cytidine and uridine ribonucleotides built from small 2 and 3 carbon fragments such as glycolaldehyde, glyceraldehyde or glyceraldehyde-3-phosphate, cyanamide and cyanoacetylene. One of the steps in this sequence allows the isolation of enantiopure ribose aminooxazoline if the enantiomeric excess of glyceraldehyde is 60% or greater. This can be viewed as a prebiotic purification step, where the said compound spontaneously crystallized out from a mixture of the other pentose aminooxazolines. Ribose aminooxazoline can then react with cyanoacetylene in a mild and highly efficient manner to give the alpha cytidine ribonucleotide. Photoanomerization with UV light allows for inversion about the 1' anomeric centre to give the correct beta stereochemistry. In 2009 they showed that the same simple building blocks allow access, via phosphate controlled nucleobase elaboration, to 2',3'-cyclic pyrimidine nucleotides directly, which are known to be able to polymerize into RNA. This paper also highlights the possibility for the photo-sanitization of the pyrimidine-2',3'-cyclic phosphates.\n\nMetabolism-like reactions could have occurred naturally in early oceans, before the first organisms evolved. Metabolism may predate the origin of life, which may have evolved from the chemical conditions in the earliest oceans. Reconstructions in laboratories show that some of these reactions can produce RNA, and some others resemble two essential reaction cascades of metabolism: glycolysis and the pentose phosphate pathway, that provide essential precursors for nucleic acids, amino acids and lipids. A study at the University of Düsseldorf created phylogenic trees based upon 6 million genes from bacteria and archaea, and identified 355 protein families that were probably present in the LUCA. They were based upon an anaerobic metabolism fixing carbon dioxide and nitrogen. It suggests that the LUCA evolved in an environment rich in hydrogen, carbon dioxide and iron. Following are some observed discoveries and related hypotheses.\n\nIn the 1980s, Günter Wächtershäuser, encouraged and supported by Karl R. Popper, postulated in his iron–sulfur world, a theory of the evolution of pre-biotic chemical pathways as the starting point in the evolution of life. It systematically traces today's biochemistry to primordial reactions which provide alternative pathways to the synthesis of organic building blocks from simple gaseous compounds.\n\nIn contrast to the classical Miller experiments, which depend on external sources of energy (simulated lightning, ultraviolet irradiation), \"Wächtershäuser systems\" come with a built-in source of energy: sulfides of iron (iron pyrite) and other minerals. The energy released from redox reactions of these metal sulfides is available for the synthesis of organic molecules, and such systems may have evolved into autocatalytic sets constituting self-replicating, metabolically active entities predating the life forms known today. Experiments with such sulfides in an aqueous environment at 100 °C produced a relatively small yield of dipeptides (0.4% to 12.4%) and a smaller yield of tripeptides (0.10%) although under the same conditions, dipeptides were quickly broken down.\n\nSeveral models reject the self-replication of a \"naked-gene\", postulating instead the emergence of a primitive metabolism providing a safe environment for the later emergence of RNA replication. The centrality of the Krebs cycle (citric acid cycle) to energy production in aerobic organisms, and in drawing in carbon dioxide and hydrogen ions in biosynthesis of complex organic chemicals, suggests that it was one of the first parts of the metabolism to evolve. Concordantly, geochemist Michael Russell has proposed that \"the purpose of life is to hydrogenate carbon dioxide\" (as part of a \"metabolism-first,\" rather than a \"genetics-first,\" scenario). Physicist Jeremy England of MIT has proposed that life was inevitable from general thermodynamic considerations: \"... when a group of atoms is driven by an external source of energy (like the sun or chemical fuel) and surrounded by a heat bath (like the ocean or atmosphere), it will often gradually restructure itself in order to dissipate increasingly more energy. This could mean that under certain conditions, matter inexorably acquires the key physical attribute associated with life.\"\n\nOne of the earliest incarnations of this idea was put forward in 1924 with Oparin's notion of primitive self-replicating vesicles which predated the discovery of the structure of DNA. Variants in the 1980s and 1990s include Wächtershäuser's iron–sulfur world theory and models introduced by Christian de Duve based on the chemistry of thioesters. More abstract and theoretical arguments for the plausibility of the emergence of metabolism without the presence of genes include a mathematical model introduced by Freeman Dyson in the early 1980s and Stuart Kauffman's notion of collectively autocatalytic sets, discussed later that decade.\n\nOrgel summarized his analysis by stating, \"There is at present no reason to expect that multistep cycles such as the reductive citric acid cycle will self-organize on the surface of FeS/FeS or some other mineral.\" It is possible that another type of metabolic pathway was used at the beginning of life. For example, instead of the reductive citric acid cycle, the \"open\" acetyl-CoA pathway (another one of the five recognized ways of carbon dioxide fixation in nature today) would be compatible with the idea of self-organization on a metal sulfide surface. The key enzyme of this pathway, carbon monoxide dehydrogenase/acetyl-CoA synthase, harbours mixed nickel-iron-sulfur clusters in its reaction centres and catalyzes the formation of acetyl-CoA (similar to acetyl-thiol) in a single step. There are increasing concerns, however, that prebiotic thiolated and thioester compounds are thermodynamically and kinetically unfavourable to accumulate in presumed prebiotic conditions (i.e. hydrothermal vents).\n\nThe Zn-world (zinc world) theory of Armen Y. Mulkidjanian is an extension of Wächtershäuser's pyrite hypothesis. Wächtershäuser based his theory of the initial chemical processes leading to informational molecules (RNA, peptides) on a regular mesh of electric charges at the surface of pyrite that may have facilitated the primeval polymerization by attracting reactants and arranging them appropriately relative to each other. The Zn-world theory specifies and differentiates further. Hydrothermal fluids rich in HS interacting with cold primordial ocean (or Darwin's \"warm little pond\") water leads to the precipitation of metal sulfide particles. Oceanic vent systems and other hydrothermal systems have a zonal structure reflected in ancient volcanogenic massive sulfide deposits (VMS) of hydrothermal origin. They reach many kilometres in diameter and date back to the Archean Eon. Most abundant are pyrite (FeS), chalcopyrite (CuFeS), and sphalerite (ZnS), with additions of galena (PbS) and alabandite (MnS). ZnS and MnS have a unique ability to store radiation energy, e.g. from UV light. During the relevant time window of the origins of replicating molecules, the primordial atmospheric pressure was high enough (>100 bar, about 100 atmospheres) to precipitate near the Earth's surface, and UV irradiation was 10 to 100 times more intense than now; hence the unique photosynthetic properties mediated by ZnS provided just the right energy conditions to energize the synthesis of informational and metabolic molecules and the selection of photostable nucleobases.\n\nThe Zn-world theory has been further filled out with experimental and theoretical evidence for the ionic constitution of the interior of the first proto-cells before archaea, bacteria and proto-eukaryotes evolved. Archibald Macallum noted the resemblance of body fluids such as blood and lymph to seawater; however, the inorganic composition of all cells differ from that of modern seawater, which led Mulkidjanian and colleagues to reconstruct the \"hatcheries\" of the first cells combining geochemical analysis with phylogenomic scrutiny of the inorganic ion requirements of universal components of modern cells. The authors conclude that ubiquitous, and by inference primordial, proteins and functional systems show affinity to and functional requirement for K, Zn, Mn, and phosphate. Geochemical reconstruction shows that the ionic composition conducive to the origin of cells could not have existed in what we today call marine settings but is compatible with emissions of vapour-dominated zones of what we today call inland geothermal systems. Under the oxygen depleted, CO-dominated primordial atmosphere, the chemistry of water condensates and exhalations near geothermal fields would resemble the internal milieu of modern cells. Therefore, the precellular stages of evolution may have taken place in shallow \"Darwin ponds\" lined with porous silicate minerals mixed with metal sulfides and enriched in K, Zn, and phosphorus compounds.\n\nThe deep sea vent, or alkaline hydrothermal vent, theory posits that life may have begun at submarine hydrothermal vents, William Martin and Michael Russell have suggested \"that life evolved in structured iron monosulphide precipitates in a seepage site hydrothermal mound at a redox, pH, and temperature gradient between sulphide-rich hydrothermal fluid and iron(II)-containing waters of the Hadean ocean floor. The naturally arising, three-dimensional compartmentation observed within fossilized seepage-site metal sulphide precipitates indicates that these inorganic compartments were the precursors of cell walls and membranes found in free-living prokaryotes. The known capability of FeS and NiS to catalyze the synthesis of the acetyl-methylsulphide from carbon monoxide and methylsulphide, constituents of hydrothermal fluid, indicates that pre-biotic syntheses occurred at the inner surfaces of these metal-sulphide-walled compartments...\" These form where hydrogen-rich fluids emerge from below the sea floor, as a result of serpentinization of ultra-mafic olivine with seawater and a pH interface with carbon dioxide-rich ocean water. The vents form a sustained chemical energy source derived from redox reactions, in which electron donors (molecular hydrogen) react with electron acceptors (carbon dioxide); see Iron–sulfur world theory. These are highly exothermic reactions.\n\nMichael Russell demonstrated that alkaline vents created an abiogenic proton motive force (PMF) chemiosmotic gradient, in which conditions are ideal for an abiogenic hatchery for life. Their microscopic compartments \"provide a natural means of concentrating organic molecules,\" composed of iron-sulfur minerals such as mackinawite, endowed these mineral cells with the catalytic properties envisaged by Wächtershäuser. This movement of ions across the membrane depends on a combination of two factors:\nThese two gradients taken together can be expressed as an electrochemical gradient, providing energy for abiogenic synthesis. The proton motive force can be described as the measure of the potential energy stored as a combination of proton and voltage gradients across a membrane (differences in proton concentration and electrical potential).\nJack W. Szostak suggested that geothermal activity provides greater opportunities for the origination of life in open lakes where there is a buildup of minerals. In 2010, based on spectral analysis of sea and hot mineral water, Ignat Ignatov and Oleg Mosin demonstrated that life may have predominantly originated in hot mineral water. The hot mineral water that contains bicarbonate and calcium ions has the most optimal range. This case is similar to the origin of life in hydrothermal vents, but with bicarbonate and calcium ions in hot water. This water has a pH of 9–11 and is possible to have the reactions in seawater. According to Melvin Calvin, certain reactions of condensation-dehydration of amino acids and nucleotides in individual blocks of peptides and nucleic acids can take place in the primary hydrosphere with pH 9-11 at a later evolutionary stage. Some of these compounds like hydrocyanic acid (HCN) have been proven in the experiments of Miller. This is the environment in which the stromatolites have been created. David Ward of Montana State University described the formation of stromatolites in hot mineral water at the Yellowstone National Park. Stromatolites survive in hot mineral water and in proximity to areas with volcanic activity. Processes have evolved in the sea near geysers of hot mineral water. In 2011, Tadashi Sugawara from the University of Tokyo created a protocell in hot water.\n\nExperimental research and computer modelling suggest that the surfaces of mineral particles inside hydrothermal vents have catalytic properties similar to those of enzymes and are able to create simple organic molecules, such as methanol (CHOH) and formic, acetic and pyruvic acid out of the dissolved CO in the water.\n\nThe research reported above by William F. Martin in July 2016 supports the thesis that life arose at hydrothermal vents, that spontaneous chemistry in the Earth’s crust driven by rock–water interactions at disequilibrium thermodynamically underpinned life’s origin and that the founding lineages of the archaea and bacteria were H2-dependent autotrophs that used CO2 as their terminal acceptor in energy metabolism. Martin suggests, based upon this evidence that LUCA \"may have depended heavily on the geothermal energy of the vent to survive\".\n\nToday's bioenergetic process of fermentation is carried out by either the aforementioned citric acid cycle or the Acetyl-CoA pathway, both of which have been connected to the primordial Iron–sulfur world. In a different approach, the thermosynthesis hypothesis considers the bioenergetic process of chemiosmosis, which plays an essential role in cellular respiration and photosynthesis, more basal than fermentation: the ATP synthase enzyme, which sustains chemiosmosis, is proposed as the currently extant enzyme most closely related to the first metabolic process.\n\nFirst, life needed an energy source to bring about the condensation reaction that yielded the peptide bonds of proteins and the phosphodiester bonds of RNA. In a generalization and thermal variation of the binding change mechanism of today's ATP synthase, the \"first protein\" would have bound substrates (peptides, phosphate, nucleosides, RNA 'monomers') and condensed them to a reaction product that remained bound until after a temperature change it was released by thermal unfolding.\n\nThe energy source under the thermosynthesis hypothesis was thermal cycling, the result of suspension of protocells in a convection current, as is plausible in a volcanic hot spring; the convection accounts for the self-organization and dissipative structure required in any origin of life model. The still ubiquitous role of thermal cycling in germination and cell division is considered a relic of primordial thermosynthesis.\n\nBy phosphorylating cell membrane lipids, this \"first protein\" gave a selective advantage to the lipid protocell that contained the protein. This protein also synthesized a library of many proteins, of which only a minute fraction had thermosynthesis capabilities. As proposed by Dyson, it propagated functionally: it made daughters with similar capabilities, but it did not copy itself. Functioning daughters consisted of different amino acid sequences.\n\nWhereas the Iron–sulfur world identifies a circular pathway as the most simple, the thermosynthesis hypothesis does not even invoke a pathway: ATP synthase's binding change mechanism resembles a physical adsorption process that yields free energy, rather than a regular enzyme's mechanism, which decreases the free energy. It has been claimed that the emergence of cyclic systems of protein catalysts is implausible.\n\nMontmorillonite, an abundant clay, is a catalyst for the polymerization of RNA and for the formation of membranes from lipids. A model for the origin of life using clay was forwarded by Alexander Graham Cairns-Smith in 1985 and explored as a plausible mechanism by several scientists. The clay hypothesis postulates that complex organic molecules arose gradually on pre-existing, non-organic replication surfaces of silicate crystals in solution.\n\nAt the Rensselaer Polytechnic Institute, James P. Ferris' studies have also confirmed that clay minerals of montmorillonite catalyze the formation of RNA in aqueous solution, by joining nucleotides to form longer chains.\n\nIn 2007, Bart Kahr from the University of Washington and colleagues reported their experiments that tested the idea that crystals can act as a source of transferable information, using crystals of potassium hydrogen phthalate. \"Mother\" crystals with imperfections were cleaved and used as seeds to grow \"daughter\" crystals from solution. They then examined the distribution of imperfections in the new crystals and found that the imperfections in the mother crystals were reproduced in the daughters, but the daughter crystals also had many additional imperfections. For gene-like behaviour to be observed, the quantity of inheritance of these imperfections should have exceeded that of the mutations in the successive generations, but it did not. Thus Kahr concluded that the crystals \"were not faithful enough to store and transfer information from one generation to the next.\"\n\nIn the 1970s, Thomas Gold proposed the theory that life first developed not on the surface of the Earth, but several kilometres below the surface. It is claimed that discovery of microbial life below the surface of another body in our Solar System would lend significant credence to this theory. Thomas Gold also asserted that a trickle of food from a deep, unreachable, source is needed for survival because life arising in a puddle of organic material is likely to consume all of its food and become extinct. Gold's theory is that the flow of such food is due to out-gassing of primordial methane from the Earth's mantle; more conventional explanations of the food supply of deep microbes (away from sedimentary carbon compounds) is that the organisms subsist on hydrogen released by an interaction between water and (reduced) iron compounds in rocks.\n\nPanspermia is the hypothesis that life exists throughout the Universe, distributed by meteoroids, asteroids, comets, planetoids, and, also, by spacecraft in the form of unintended contamination by microorganisms.\n\nPanspermia hypothesis does not attempt to explain how life first originated, but merely shifts it to another planet or a comet. The advantage of an extraterrestrial origin of primitive life is that life is not required to have formed on each planet it occurs on, but rather in a single location, and then spread about the galaxy to other star systems via cometary and/or meteorite impact. Evidence to support the hypothesis is scant, but it finds support in studies of Martian meteorites found in Antarctica and in studies of extremophile microbes' survival in outer space tests. (See also: List of microorganisms tested in outer space.)\n\nAn organic compound is any member of a large class of gaseous, liquid, or solid chemicals whose molecules contain carbon. Carbon is the fourth most abundant element in the Universe by mass after hydrogen, helium, and oxygen. Carbon is abundant in the Sun, stars, comets, and in the atmospheres of most planets. Organic compounds are relatively common in space, formed by \"factories of complex molecular synthesis\" which occur in molecular clouds and circumstellar envelopes, and chemically evolve after reactions are initiated mostly by ionizing radiation. Based on computer model studies, the complex organic molecules necessary for life may have formed on dust grains in the protoplanetary disk surrounding the Sun before the formation of the Earth. According to the computer studies, this same process may also occur around other stars that acquire planets.\n\nObservations suggest that the majority of organic compounds introduced on Earth by interstellar dust particles are considered principal agents in the formation of complex molecules, thanks to their peculiar surface-catalytic activities. Studies reported in 2008, based on C/C isotopic ratios of organic compounds found in the Murchison meteorite, suggested that the RNA component uracil and related molecules, including xanthine, were formed extraterrestrially. On 8 August 2011, a report based on NASA studies of meteorites found on Earth was published suggesting DNA components (adenine, guanine and related organic molecules) were made in outer space. Scientists also found that the cosmic dust permeating the Universe contains complex organics (\"amorphous organic solids with a mixed aromatic–aliphatic structure\") that could be created naturally, and rapidly, by stars. Sun Kwok of The University of Hong Kong suggested that these compounds may have been related to the development of life on Earth said that \"If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life.\"\nGlycolaldehyde, the first example of an interstellar sugar molecule, was detected in the star-forming region near the centre of our galaxy. It was discovered in 2000 by Jes Jørgensen and Jan M. Hollis. In 2012, Jørgensen's team reported the detection of glycolaldehyde in a distant star system. The molecule was found around the protostellar binary IRAS 16293-2422 400 light years from Earth. Glycolaldehyde is needed to form RNA, which is similar in function to DNA. These findings suggest that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation. Because sugars are associated with both metabolism and the genetic code, two of the most basic aspects of life, it is thought the discovery of extraterrestrial sugar increases the likelihood that life may exist elsewhere in our galaxy.\n\nNASA announced in 2009 that scientists had identified another fundamental chemical building block of life in a comet for the first time, glycine, an amino acid, which was detected in material ejected from comet Wild 2 in 2004 and grabbed by NASA's \"Stardust\" probe. Glycine has been detected in meteorites before. Carl Pilcher, who leads the NASA Astrobiology Institute commented that \"The discovery of glycine in a comet supports the idea that the fundamental building blocks of life are prevalent in space, and strengthens the argument that life in the Universe may be common rather than rare.\" Comets are encrusted with outer layers of dark material, thought to be a tar-like substance composed of complex organic material formed from simple carbon compounds after reactions initiated mostly by ionizing radiation. It is possible that a rain of material from comets could have brought significant quantities of such complex organic molecules to Earth. Amino acids which were formed extraterrestrially may also have arrived on Earth via comets. It is estimated that during the Late Heavy Bombardment, meteorites may have delivered up to five million tons of organic prebiotic elements to Earth per year.\nPolycyclic aromatic hydrocarbons (PAH) are the most common and abundant of the known polyatomic molecules in the observable universe, and are considered a likely constituent of the primordial sea. In 2010, PAHs, along with fullerenes (or \"buckyballs\"), have been detected in nebulae.\n\nIn March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like PAHs, the most carbon-rich chemical found in the Universe, may have been formed in red giant stars or in interstellar dust and gas clouds. A group of Czech scientists reported that all four RNA-bases may be synthesized from formamide in the course of high-energy density events like extraterrestrial impacts.\n\nThe lipid world theory postulates that the first self-replicating object was lipid-like. It is known that phospholipids form lipid bilayers in water while under agitation—the same structure as in cell membranes. These molecules were not present on early Earth, but other amphiphilic long-chain molecules also form membranes. Furthermore, these bodies may expand (by insertion of additional lipids), and under excessive expansion may undergo spontaneous splitting which preserves the same size and composition of lipids in the two progenies. The main idea in this theory is that the molecular composition of the lipid bodies is the preliminary way for information storage, and evolution led to the appearance of polymer entities such as RNA or DNA that may store information favourably. Studies on vesicles from potentially prebiotic amphiphiles have so far been limited to systems containing one or two types of amphiphiles. This in contrast to the output of simulated prebiotic chemical reactions, which typically produce very heterogeneous mixtures of compounds.\nWithin the hypothesis of a lipid bilayer membrane composed of a mixture of various distinct amphiphilic compounds there is the opportunity of a huge number of theoretically possible combinations in the arrangements of these amphiphiles in the membrane. Among all these potential combinations, a specific local arrangement of the membrane would have favoured the constitution of a hypercycle, actually a positive feedback composed of two mutual catalysts represented by a membrane site and a specific compound trapped in the vesicle. Such site/compound pairs are transmissible to the daughter vesicles leading to the emergence of distinct lineages of vesicles which would have allowed Darwinian natural selection.\n\nA problem in most scenarios of abiogenesis is that the thermodynamic equilibrium of amino acid versus peptides is in the direction of separate amino acids. What has been missing is some force that drives polymerization. The resolution of this problem may well be in the properties of polyphosphates. Polyphosphates are formed by polymerization of ordinary monophosphate ions PO. Several mechanisms of organic molecule synthesis have been investigated. Polyphosphates cause polymerization of amino acids into peptides. They are also logical precursors in the synthesis of such key biochemical compounds as adenosine triphosphate (ATP). A key issue seems to be that calcium reacts with soluble phosphate to form insoluble calcium phosphate (apatite), so some plausible mechanism must be found to keep calcium ions from causing precipitation of phosphate. There has been much work on this topic over the years, but an interesting new idea is that meteorites may have introduced reactive phosphorus species on the early Earth.\n\nPolycyclic aromatic hydrocarbons (PAH) are known to be abundant in the Universe, including in the interstellar medium, in comets, and in meteorites, and are some of the most complex molecules so far found in space.\n\nOther sources of complex molecules have been postulated, including extraterrestrial stellar or interstellar origin. For example, from spectral analyses, organic molecules are known to be present in comets and meteorites. In 2004, a team detected traces of PAHs in a nebula. In 2010, another team also detected PAHs, along with fullerenes, in nebulae. The use of PAHs has also been proposed as a precursor to the RNA world in the PAH world hypothesis. The Spitzer Space Telescope has detected a star, HH 46-IR, which is forming by a process similar to that by which the Sun formed. In the disk of material surrounding the star, there is a very large range of molecules, including cyanide compounds, hydrocarbons, and carbon monoxide. In September 2012, NASA scientists reported that PAHs, subjected to interstellar medium conditions, are transformed, through hydrogenation, oxygenation and hydroxylation, to more complex organics—\"a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively.\" Further, as a result of these transformations, the PAHs lose their spectroscopic signature which could be one of the reasons \"for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks.\"\n\nNASA maintains a database for tracking PAHs in the Universe. More than 20% of the carbon in the Universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the Universe, and are associated with new stars and exoplanets.\n\nZachary Adam claims that tidal processes that occurred during a time when the Moon was much closer may have concentrated grains of uranium and other radioactive elements at the high-water mark on primordial beaches, where they may have been responsible for generating life's building blocks. According to computer models, a deposit of such radioactive materials could show the same self-sustaining nuclear reaction as that found in the Oklo uranium ore seam in Gabon. Such radioactive beach sand might have provided sufficient energy to generate organic molecules, such as amino acids and sugars from acetonitrile in water. Radioactive monazite material also has released soluble phosphate into the regions between sand-grains, making it biologically \"accessible.\" Thus amino acids, sugars, and soluble phosphates might have been produced simultaneously, according to Adam. Radioactive actinides, left behind in some concentration by the reaction, might have formed part of organometallic complexes. These complexes could have been important early catalysts to living processes.\n\nJohn Parnell has suggested that such a process could provide part of the \"crucible of life\" in the early stages of any early wet rocky planet, so long as the planet is large enough to have generated a system of plate tectonics which brings radioactive minerals to the surface. As the early Earth is thought to have had many smaller plates, it might have provided a suitable environment for such processes.\n\nThe 19th-century Austrian physicist Ludwig Boltzmann first recognized that the struggle for existence of living organisms was neither over raw material nor energy, but instead had to do with entropy production derived from the conversion of the solar spectrum into heat by these systems. Boltzmann thus realized that living systems, like all irreversible processes, were dependent on the dissipation of a generalized chemical potential for their existence. In his book “What is Life”, the 20th-century Austrian physicist Erwin Schrödinger emphasized the importance of Boltzmann’s deep insight into the irreversible thermodynamic nature of living systems, suggesting that this was the physics and chemistry behind the origin and evolution of life. However, irreversible processes, and much less living systems, could not be conveniently analyzed under this perspective until Lars Onsager, and later Ilya Prigogine, developed an elegant mathematical formalism for treating the “self-organization” of material under a generalized chemical potential. This formalism became known as Classical Irreversible Thermodynamics and Prigogine was awarded the Nobel Prize in Chemistry in 1977 \"for his contributions to non-equilibrium thermodynamics, particularly the theory of dissipative structures\". The analysis of Prigogine showed that if a system were left to evolve under an imposed external potential, material could spontaneously organize (lower its entropy) forming what he called “dissipative structures” which would increase the dissipation of the externally imposed potential (augment the global entropy production). Non-equilibrium thermodynamics has since been successfully applied to the analysis of living systems, from the biochemical production of ATP to optimizing bacterial metabolic pathways to complete ecosystems.\n\nIn his “Thermodynamic Dissipation Theory of the Origin and Evolution of Life”, Karo Michaelian has taken the insight of Boltzmann and the work of Prigogine to its ultimate consequences regarding the origin of life. This theory postulates that the hallmark of the origin and evolution of life is the microscopic dissipative structuring of organic pigments and their proliferation over the entire Earth surface. Present day life augments the entropy production of Earth in its solar environment by dissipating ultraviolet and visible photons into heat through organic pigments in water. This heat then catalyzes a host of secondary dissipative processes such as the water cycle, ocean and wind currents, hurricanes, etc. Michaelian argues that if the thermodynamic function of life today is to produce entropy through photon dissipation in organic pigments, then this probably was its function at its very beginnings. It turns out that both RNA and DNA when in water solution are very strong absorbers and extremely rapid dissipaters of ultraviolet light within the 230–290 nm wavelength (UV-C) region, which is a part of the Sun's spectrum that could have penetrated the prebiotic atmosphere. In fact, not only RNA and DNA, but many fundamental molecules of life (those common to all three domains of life) are also pigments that absorb in the UV-C, and many of these also have a chemical affinity to RNA and DNA. Nucleic acids may thus have acted as acceptor molecules to the UV-C photon excited antenna pigment donor molecules by providing an ultrafast channel for dissipation. Michaelian has shown using the formalism of non-linear irreversible thermodynamics that there would have existed during the Archean a thermodynamic imperative to the abiogenic UV-C photochemical synthesis and proliferation of these pigments over the entire Earth surface if they acted as catalysts to augment the dissipation of the solar photons. By the end of the Archean, with life-induced ozone dissipating UV-C light in the Earth’s upper atmosphere, it would have become ever more improbable for a completely new life to emerge that didn’t rely on the complex metabolic pathways already existing since now the free energy in the photons arriving at Earth’s surface would have been insufficient for direct breaking and remaking of covalent bonds. It has been suggested, however, that such changes in the surface flux of ultraviolet radiation due to geophysical events affecting the atmosphere could have been what promoted the development of complexity in life based on existing metabolic pathways, for example during the Cambrian explosion \n\nMany salient characteristics of the fundamental molecules of life (those found in all three domains) all point directly to the involvement of UV-C light in the dissipative structuring of incipient life. Some of the most difficult problems concerning the origin of life, such as enzyme-less replication of RNA and DNA, homochirality of the fundamental molecules, and the origin of information encoding in RNA and DNA, also find an explanation within the same dissipative thermodynamic framework by considering the probable existence of a relation between primordial replication and UV-C photon dissipation. Michaelian suggests that it is erroneous to expect to describe the emergence, proliferation, or even evolution, of life without overwhelming reference to entropy production through the dissipation of a generalized chemical potential, in particular, the prevailing solar photon flux.\n\nDifferent forms of life with variable origin processes may have appeared quasi-simultaneously in the early history of Earth. The other forms may be extinct (having left distinctive fossils through their different biochemistry—e.g., hypothetical types of biochemistry). It has been proposed that:\n\nThe first organisms were self-replicating iron-rich clays which fixed carbon dioxide into oxalic and other dicarboxylic acids. This system of replicating clays and their metabolic phenotype then evolved into the sulfide rich region of the hotspring acquiring the ability to fix nitrogen. Finally phosphate was incorporated into the evolving system which allowed the synthesis of nucleotides and phospholipids. If biosynthesis recapitulates biopoiesis, then the synthesis of amino acids preceded the synthesis of the purine and pyrimidine bases. Furthermore the polymerization of the amino acid thioesters into polypeptides preceded the directed polymerization of amino acid esters by polynucleotides.\n\nArmid Mulkidjanian and co-authors think that the marine environments did not provide the ionic balance and composition universally found in cells, as well as of ions required by essential proteins and ribozymes found in virtually all living organisms, especially with respect to K/Na ratio, Mn, Zn and phosphate concentrations. The only known environments that mimic the needed conditions on Earth are found in terrestrial hydrothermal pools fed by steam vents. Additionally, mineral deposits in these environments under an anoxic atmosphere would have suitable pH (as opposed to current pools in an oxygenated atmosphere), contain precipitates of sulfide minerals that block harmful UV radiation, have wetting/drying cycles that concentrate substrate solutions to concentrations amenable to spontaneous formation of polymers of nucleic acids, and a continual supply of abiotically generated organic molecules, both by chemical reactions in the hydrothermal environment, as well as by exposure to UV light during transport from vents to adjacent pools. Their hypothesized pre-biotic environments are similar to the deep-oceanic vent environments most commonly hypothesized, but add additional components that help explain peculiarities found in reconstructions of the Last Universal Common Ancestor (LUCA) of all living organisms.\n\nBruce Damer and David Deamer have come to the conclusion that cell membranes cannot be formed in salty seawater, and must therefore have originated in freshwater. Before the continents formed, the only dry land on Earth would be volcanic islands, where rainwater would form ponds where lipids could form the first stages towards cell membranes. These predecessors of true cells are assumed to have behaved more like a superorganism rather than individual structures, where the porous membranes would house molecules which would leak out and enter other protocells. Only when true cells had evolved would they gradually adapt to saltier environments and enter the ocean.\n\nColín-García \"et al.\" (2016) discuss the advantages and disadvantages of hydrothermal vents as primitive environments. They mention the exergonic reactions in such systems could have been a source of free energy that promoted chemical reactions, additional to their high mineralogical diversity which implies the induction of important chemical gradients, thus favoring the interaction between electron donors and acceptors. Colín-García \"et al.\" (2016) also summarize a set of experiments proposed to test the role of hydrothermal vents in prebiotic synthesis.\n\nA theory that speaks to the origin of life on Earth and other rocky planets posits life as an information system in which information content grows because of selection. Life must start with minimum possible information, or minimum possible departure from thermodynamic equilibrium, and it requires thermodynamically free energy accessible by means of its information content. The most benign circumstances, minimum entropy variations with abundant free energy, suggest the pore space in the first few kilometres of the surface. Free energy is derived from the condensed products of the chemical reactions taking place in the cooling nebula.\n\n\n", "id": "19179706", "title": "Abiogenesis"}
{"url": "https://en.wikipedia.org/wiki?curid=44447884", "text": "Inbreeding avoidance\n\nInbreeding avoidance, or the inbreeding avoidance hypothesis, is a concept in evolutionary biology that refers to the prevention of the deleterious effects of inbreeding. The inbreeding avoidance hypothesis posits that certain mechanisms develop within a species, or within a given population of a species, as a result of natural and sexual selection in order to prevent breeding among related individuals in that species or population. Although inbreeding may impose certain evolutionary costs, inbreeding avoidance, which limits the number of potential mates for a given individual, can inflict opportunity costs. Therefore, a balance exists between inbreeding and inbreeding avoidance. This balance determines whether inbreeding mechanisms develop and the specific nature of said mechanisms.\n\nInbreeding results in inbreeding depression, which is the reduction of fitness of a given population due to inbreeding. Inbreeding depression occurs via one of two mechanisms. The first mechanism involves the appearance of disadvantageous traits via the pairing of deleterious recessive alleles in a mating pair’s progeny. When two related individuals mate, the probability of deleterious recessive alleles pairing in the resulting offspring is higher as compared to when non-related individuals mate. The second mechanism relates to the increased fitness of heterozygotes.\n\nA review of the genetics of inbreeding depression in wild animal and plant populations, as well as in humans, led to the conclusion that inbreeding depression and its opposite, heterosis (hybrid vigor), are predominantly caused by the presence of recessive deleterious alleles in populations. Inbreeding, including self-fertilization in plants and automictic parthenogenesis (thelytoky) in hymenoptera, tends to lead to the harmful expression of deleterious recessive alleles (inbreeding depression). Cross-fertilization between unrelated individuals ordinarily leads to the masking of deleterious recessive alleles in progeny.\n\nMany studies have demonstrated that homozygous individuals are often disadvantaged with respect to heterozygous individuals. For example, a study conducted on a population of South African cheetahs demonstrated that the lack of genetic variability among individuals in the population has resulted in negative consequences for individuals, such as a greater rate of juvenile mortality and spermatozoal abnormalities. When heterozygotes possess a fitness advantage relative to a homozygote, a population with a large number of homozygotes will have a relatively reduced fitness, thus leading to inbreeding depression. Through these described mechanisms, the effects of inbreeding depression are often severe enough to cause the evolution of inbreeding avoidance mechanisms.\n\nInbreeding avoidance mechanisms have evolved in response to selection against inbred offspring. Inbreeding avoidance occurs in nature by at least four mechanisms: kin recognition, dispersal, extra-pair/extra-group copulations, and delayed maturation/reproductive suppression. Of note, these mechanisms are not mutually exclusive and more than one can occur in a population at a given time.\n\n \nKin recognition is the mechanism by which individuals identify and avoid mating with closely related conspecifics. There have been numerous documented examples of instances in which individuals are shown to find closely related conspecifics unattractive. In one set of studies, researchers formed artificial relative and non-relative mate-pairs (artificial meaning they preferentially paired individuals to mate for the purposes of the experiments) and compared the reproductive results of the two groups. In these studies, paired relatives demonstrated reduced reproduction and higher mating reluctance when compared with non-relatives. For example, in a study by Simmons in field crickets, female crickets exhibited greater mating latency for paired siblings and half-siblings than with non-siblings. In another set of studies, researchers allowed individuals to choose their mates from conspecifics that lie on a spectrum of relatedness. In this set, individuals were more likely to choose non-related over related conspecifics. For example, in a study by Krackow et al., male wild house mice were set up in an arena with four separate openings leading to cages with bedding from conspecifics. The conspecifics exhibited a range of relatedness to the test subjects, and the males significantly preferred the bedding of non-siblings to the bedding of related females.\n\nStudies have shown that kin recognition is more developed in species in which dispersal patterns facilitate frequent adult kin encounters.\n\nThere is a significant amount of variation in the mechanisms used for kin recognition. These mechanisms include recognition based on association or familiarity, an individual’s own phenotypic cues, chemical cues, and the MHC genes. In association/familiarity mechanisms, individuals learn the phenotypic profiles of their kin and use this template for kin recognition. Many species accomplish this by becoming \"familiar\" with their siblings, litter mates, or nestmates. These species rely on offspring being reared in close proximity to achieve kin recognition. This is called the Westermarck effect. For example, Holmes and Sherman conducted a comparative study in Arctic ground squirrels and Belding’s ground squirrels. They manipulated the reared groups to include both siblings and cross-fostered nestmates and found that in both species the individuals were equally aggressive toward their nestmates, regardless of kinship. In certain species where social groups are highly stable, relatedness and association between infants and other individuals are usually highly correlated. Therefore, degree of association can be used as a meter for kin recognition.\n\nIndividuals can also use their own characteristics or phenotype as a template in kin recognition. For example, in one study, Mateo and Johnston had golden hamsters reared with only non-kin then later had them differentiate between odors of related and non-related individuals without any postnatal encounters with kin. The hamsters were able to discriminate between the odors, demonstrating the use of their own phenotype for the purpose of kin recognition. This study also provides an example of a species utilizing chemical cues for kin recognition.\n\nThe major histocompatibility complex genes, or MHC genes, have been implicated in kin recognition. One idea is that the MHC genes code for a specific pheromone profile for each individual, which are used to discriminate between kin and non-kin conspecifics. Several studies have demonstrated the involvement of the MHC genes in kin recognition. For example, Manning et al. conducted a study in house mice that looked at the species's behavior of communal nesting, or nursing one's own pups as well as the pups of other individuals. As Manning et al. state, kin selection theory predicts that the house mice will selectively nurse the pups of their relatives in order to maximize inclusive fitness. Manning et al. demonstrate that the house mice utilize the MHC genes in the process of discriminating between kin by preferring individuals who share the same allelic forms the MHC genes.\n\nExperiments using in vitro fertilization in the mouse, provided evidence of sperm selection at the gametic level. When sperm of sibling and non-sibling males were mixed, a fertilization bias towards the sperm of the non-sibling males was observed. The results were interpreted as egg-driven sperm selection against related sperm.\n\nThe possible use of olfaction-biased mechanisms in human kin recognition and inbreeding avoidance was examined in three different types of study. The results indicated that olfaction may help mediate the development during childhood of incest avoidance (the Westermarck effect).\n\nExperiments were performed with the dioecious plant \"Silene latifolia\" to test whether post-pollination selection favors less related pollen donors and reduces inbreeding. The results showed that in \"S. latifolia\", and presumably in other plant systems with inbreeding depression, pollen or embryo selection after multiple-donor pollination may reduce inbreeding.\n\nSome species will adopt dispersal as a way to separate close relatives and prevent inbreeding. The initial dispersal route species may take is known as natal dispersal, whereby individuals move away from the area of birth. Subsequently, species may then resort to breeding dispersal, whereby individuals move from one non-natal group to another. Nelson-Flower et al. (2012) conducted a study on southern pied babblers and found that individuals may travel farther distances from natal groups than from non-natal groups. This may be attributed to the possibility of encountering kin within local ranges when dispersing. The extent to which an individual in a particular species will disperse depends on whether the benefits of dispersing can outweigh both the costs of inbreeding and the costs of dispersal. Long‐distance movements can bear mortality risks and energetic costs.\n\nIn many cases of dispersal, one sex shows a greater tendency to disperse from their natal area than the opposite sex. The extent of bias for a particular sex is dependent on numerous factors which include, but are not limited to: mating system, social organization, inbreeding and dispersal costs, and physiological factors.\n\nBirds tend to adopt monogamous mating systems in which the males remain in their natal groups to defend familiar territories with high resource quality. Females generally have high energy expenditure when producing offspring, therefore inbreeding is costly for the females in terms of offspring survival and reproductive success. Females will then benefit more by dispersing and choosing amongst these territorial males. In addition, according to the Oedipus hypothesis, daughters of female birds can cheat their mothers through brood parasitism, therefore females will evict the females from the nest, forcing their daughters to disperse. Female dispersal is not seen only in birds; males may remain philopatric in mammals when the average adult male residency in a breeding group exceeds the average age for female maturation and conception. For example, in a community of chimpanzees in Gombe National Park, males tend to remain in their natal community for the duration of their lives, while females typically move to other communities as soon as they reach maturity.\n\nMale dispersal is more common in mammals with cooperative breeding and polygynous systems. Australian marsupial juvenile males have a greater tendency to disperse from their natal groups, while the females remain philopatric. This is due to the fact that males die immediately after mating; therefore when they disperse to mate, they often meet with female natal groups with zero males present. Furthermore, the Oedipus hypothesis also states that fathers in polygynous systems will evict sons with the potential to cuckold them. Polygynous mating systems also influence between males, where in cases where males can guard multiple females and exert their dominance, subordinate males are often forced to disperse to other non-natal groups.\n\nWhen species adopt alternative inbreeding avoidance mechanisms, they can indirectly influence whether a species will disperse. For example, kin recognition has been shown to influence female to view immigrants as more attractive. Their choice for non-natal group males then selects for male dispersal.\n\nWhen the costs and benefits of dispersal are symmetric for both males and females, then no sex-biased dispersal is expected to be observed in species.\n\nThe delayed sexual maturation of offspring in the presence of parents is another mechanism by which individuals avoid inbreeding. Delayed maturation scenarios can involve the removal of the original, opposite-sex parent, as is the case in female lions that exhibit estrus earlier following the replacement of their fathers with new males. Another form of delayed maturation involves parental presence that inhibits reproductive activity, such as in mature marmosets offspring that are reproductively suppressed in the presence of opposite sex parents and siblings in their social groups. \nReproductive suppression occurs when sexually mature individuals in a group are prevented from reproducing due to behavioral or chemical stimuli from other group members that suppress breeding behavior. Social cues from the surrounding environment often dictate when reproductive activity is suppressed and involves interactions between same-sex adults. If the current conditions for reproduction are unfavorable, such as when presented with only inbreeding as a means to reproduce, individuals may increase their lifetime reproductive success by timing their reproductive attempts to occur during more favorable conditions. This can be achieved by individuals suppressing their reproductive activity in poor reproduction conditions.\n\nInbreeding avoidance between philopatric offspring and their parents/siblings severely restricts breeding opportunities of subordinates living in their social groups. A study by O'Riain et al. (2000) examined meerkats social groups and factors affecting reproductive suppression in subordinate females. They found that in family groups, the absence of a dominant individual of either sex led to reproductive quiescence. Reproductive activity only resumed upon another sexually mature female obtaining dominance, and immigration of an unrelated male. Reproduction required both the presence of an unrelated opposite-sex partner, which acted as appropriate stimulus on reproductively suppressed subordinates that were quiescent in the presence of the original dominant individual.\n\nIn various species, females benefit by mating with multiple males, thus producing more offspring of higher genetic quality. Females that are pair bonded to a male of poor genetic quality, as is the case in inbreeding, are more likely to engage in extra-pair copulations in order to improve their reproductive success and the survivability of their offspring. This improved quality in offspring is generated from either the intrinsic effects of good genes, or from interactions between compatible genes from the parents. In inbreeding, loss of heterozygosity contributes to the overall decreased reproductive success, but when individuals engage in extra-pair copulations, mating between genetically dissimilar individuals leads to increased heterozygosity.\n\nExtra-pair copulations involve a number of costs and benefits for both male and female animals. For males, extra-pair copulation involves spending more time away from the original pairing in search of other females. This risks the original female being fertilized by other males while the original male is searching for partners, leading to a loss of paternity. The tradeoff for this cost depends entirely on whether the male is able to fertilize the other females’ eggs in the extra-pair copulation. For females, extra-pair copulations ensure egg fertilization, and provide enhanced genetic variety with compatible sperm that avoid expression of damaging recessive genes that come with inbreeding. Through extra-pair mating, females are able to maximize the genetic variability of their offspring, providing protection against environmental changes that may otherwise target more homozygous populations that inbreeding often produces.\n\nWhether a female engages in extra-pair copulations for the sake of inbreeding avoidance depends on whether the costs of extra-pair copulation outweigh the costs of inbreeding. In extra-pair copulations, both inbreeding costs and pair-bond male loss (leading to the loss of paternal care) must be considered with the benefits of reproductive success that extra-pair copulation provides. When paternal care is absent or has little influence on offspring survivability, it is generally favorable for females to engage in extra-pair mating to increase reproductive success and avoid inbreeding.\n\nInbreeding avoidance has been studied by three major methods: (1) observing individual behavior in the presence and absence of close kin, (2) contrasting costs of avoidance with costs of tolerating close inbreeding, (3) comparing observed and random frequencies of close inbreeding. No single one of these methods is perfect and this brings about debate over whether the inbreeding avoidance hypothesis explains these behavioral observations completely and consistently. Although the first option is the preferred and widely used method, there is still debate over whether it can provide evidence for inbreeding avoidance.\n\nA good majority of the literature on inbreeding avoidance has been published at least 15 years ago, which leaves for growth and development of the study through current experimental methods and technology. There is greater access to more advanced molecular techniques, such as DNA fingerprinting that makes measuring relatedness more efficient and accurate. There has also been an increasing interest for studying inbreeding avoidance in carnivores, where explaining their social behaviors is underway.\n", "id": "44447884", "title": "Inbreeding avoidance"}
{"url": "https://en.wikipedia.org/wiki?curid=44497219", "text": "Nocturnal bottleneck\n\nThe nocturnal bottleneck hypothesis is a hypothesis to explain several mammal traits. The hypothesis states that mammals were mainly or even exclusively nocturnal through most of their evolutionary story, starting with their origin 225 million years ago, and only ending with the demise of the dinosaurs 66 million years ago. While some mammal groups have later evolved to fill diurnal niches, the approximately 160 million years spent as nocturnal animals has left a lasting legacy on basal anatomy and physiology, and most mammals are still nocturnal.\n\nMammals evolved from cynodonts, a group of superficially dog-like mammal-like reptiles in the wake of the Permian–Triassic mass extinction. The emerging archosaurian groups that flourished after the extinction, including crocodiles and dinosaurs and their ancestors, drove the remaining larger cynodonts into extinction, leaving only the smaller forms. The surviving cynodonts could only succeed in niches with minimal competition from the diurnal dinosaurs, evolving into the typical small-bodied insectivorous dwellers of the nocturnal undergrowth. While the early mammals continued to develop into several probably quite common groups of animals during the Mesozoic, they all remained nocturnal.\n\nOnly with the massive extinction at the end of the Cretaceous did the dinosaurs leave the stage open for the establishment of a new fauna of mammals. Despite this, mammals continued to be small-bodied for millions of years. While all the largest animals alive today are mammals, the majority of mammals are still small nocturnal animals scurrying around in the undergrowth.\n\nSeveral different features of mammalian physiology appear to be adaptations to a nocturnal lifestyle, mainly related to the sensory organs. These include:\n\n\n\n", "id": "44497219", "title": "Nocturnal bottleneck"}
{"url": "https://en.wikipedia.org/wiki?curid=40726052", "text": "Community genetics\n\nCommunity genetics is a recently emerged field in biology that fuses elements of community ecology, evolutionary biology, and molecular and quantitative genetics. Antonovics first articulated the vision for such a field, and Whitham et al. formalized its definition as “The study of the genetic interactions that occur between species and their abiotic environment in complex communities.” The field aims to bridge the gaps in the study of evolution and ecology, within the multivariate community context that ecological and evolutionary phenomena are embedded within. The documentary movie \"A Thousand Invisible Cords\" provides an introduction to the field and its implications.\nTo date, the primary focus of most community genetics studies has been on the influences of genetic variation in plants on foliar arthropod communities. In a wide variety of ecosystems, different plant genotypes often support different compositions of associated foliar arthropod communities. Such community phenotypes have been observed in natural hybrid complexes, among genotypes and sibling families within a single species and among different plant populations. To understand the broader impacts of differences among plant genotypes on biodiversity as a whole, researchers have begun to examine the response of other organisms, such as foliar endophytes, mycorrhizal fungi, soil microbes, litter dwelling arthropods, herbaceous plants and epiphytes. These effects are frequently examined with foundation species in temperate ecosystems, who structure ecosystems by modulating and stabilizing resources and ecosystem processes. The emphasis on foundation species allows researchers to focus on the likely most important players in a system without becoming overwhelmed by the complexity of all the genetically variable interactions occurring at the same time. However, unique effects of plant genotypes have also been found with non-foundation species, and can occur in tropical, boreal and alpine systems.\nThe vision for the field of community genetics extends beyond documentation of different communities on different genotypes of a focal species. Other aspects of this field include understanding how species interactions within a community are modulated by host genotype, implications of host genotype on the fitness and evolution of community members, and selection on hosts influencing associated communities. Future progress in the field of community genetics is strongly dependent on breakthroughs of modern molecular DNA-based technology, such as genome sequencing. The application of a community genetics approach to understanding how species and communities of interacting organisms are reacting to rapid changes in climate, as well as informing restoration, are two important applied aspects of community genetics.\n", "id": "40726052", "title": "Community genetics"}
{"url": "https://en.wikipedia.org/wiki?curid=4682359", "text": "Directed evolution\n\nDirected evolution (DE, \"gelenkte Evolution\") is a method used in protein engineering that mimics the process of natural selection to evolve proteins or nucleic acids toward a user-defined goal. It consists of subjecting a gene to iterative rounds of mutagenesis (creating a library of variants), selection (expressing the variants and isolating members with the desired function), and amplification (generating a template for the next round). It can be performed \"in vivo \"(in living cells), or \"in vitro\" (free in solution or microdroplet). Directed evolution is used both for protein engineering as an alternative to rationally designing modified proteins, as well as studies of fundamental evolutionary principles in a controlled, laboratory environment.\n\nDirected evolution is a mimic of the natural evolution cycle in a laboratory setting. Evolution requires three things to happen: variation between replicators, that the variation causes fitness differences upon which selection acts, and that this variation is heritable. In DE, a single gene is evolved by iterative rounds of mutagenesis, selection or screening, and amplification. Rounds of these steps are typically repeated, using the best variant from one round as the template for the next to achieve stepwise improvements.\n\nThe likelihood of success in a directed evolution experiment is directly related to the total library size, as evaluating more mutants increases the chances of finding one with the desired properties.\n\nThe first step in performing a cycle of directed evolution is the generation of a library of variant genes. The sequence space for random sequence is vast (10 possible sequences for a 100 amino acid protein) and extremely sparsely populated by functional proteins. Neither experimental, nor natural evolution can ever get close to sampling so many sequences. Of course, natural evolution samples variant sequences close to functional protein sequences and this is imitated in DE by mutagenising an already functional gene.\nSome calculations suggest it is entirely feasible that for all practical (i.e. functional and structural) purposes, protein sequence space has been fully explored during the course of evolution of life on Earth.\n\nThe starting gene can be mutagenised by random point mutations (by chemical mutagens or error prone PCR) and insertions and deletions (by transposons). Gene recombination can be mimicked by DNA shuffling of several sequences (usually of more than 70% homology) to jump into regions of sequence space between the shuffled parent genes. Finally, specific regions of a gene can be systematically randomised for a more focused approach based on structure and function knowledge. Depending on the method, the library generated will vary in the proportion of functional variants it contains. Even if an organism is used to express the gene of interest, by mutagenising only that gene, the rest of the organism’s genome remains the same and can be ignored for the evolution experiment (to the extent of providing a constant genetic environment).\n\nThe majority of mutations are deleterious and so libraries of mutants tend to mostly have variants with reduced activity. Therefore, a high-throughput assay is vital for measuring activity to find the rare variants with beneficial mutations that improve the desired properties. Two main categories of method exist for isolating functional variants. Selection systems directly couple protein function to survival of the gene, whereas screening systems individually assay each variant and allow a quantitative threshold to be set for sorting a variant or population of variants of a desired activity. Both selection and screening can be performed in living cells (\"in vivo\" evolution) or performed directly on the protein or RNA without any cells (\"in vitro\" evolution).\n\nDuring \"in vivo\" evolution, each cell (usually bacteria or yeast) is transformed with a plasmid containing a different member of the variant library. In this way, only the gene of interest differs between the cells, with all other genes being kept the same. The cells express the protein either in their cytoplasm or surface where its function can be tested. This format has the advantage of selecting for properties in a cellular environment, which is useful when the evolved protein or RNA is to be used in living organisms. When performed without cells, DE involves using \"in vitro\" transcription translation to produce proteins or RNA free in solution or compartmentalised in artificial microdroplets. This method has the benefits of being more versatile in the selection conditions (e.g. temperature, solvent), and can express proteins that would be toxic to cells. Furthermore, \"in vitro\" evolution experiments can generate far larger libraries (up to 10) because the library DNA need not be inserted into cells (often a limiting step).\n\nSelection for binding activity is conceptually simple. The target molecule is immobilised on a solid support, a library of variant proteins is flowed over it, poor binders are washed away, and the remaining bound variants recovered to isolate their genes. Binding of an enzyme to immobilised covalent inhibitor has been also used as an attempt to isolate active catalysts. This approach, however, only selects for single catalytic turnover and is not a good model of substrate binding or true substrate reactivity. If an enzyme activity can be made necessary for cell survival, either by synthesizing a vital metabolite, or destroying a toxin, then cell survival is a function of enzyme activity. Such systems are generally only limited in throughput by the transformation efficiency of cells. They are also less expensive and labour-intensive than screening, however they are typically difficult to engineer, prone to artefacts and give no information on the range of activities present in the library.\n\nAn alternative to selection is a screening system. Each variant gene is individually expressed and assayed to quantitatively measure the activity (most often by a colourgenic or fluorogenic product). The variants are then ranked and the experimenter decides which variants to use as templates for the next round of DE. Even the most high throughput assays usually have lower coverage than selection methods but give the advantage of producing detailed information on each one of the screened variants. This disaggregated data can also be used to characterise the distribution of activities in libraries which is not possible in simple selection systems. Screening systems, therefore, have advantages when it comes to experimentally characterising adaptive evolution and fitness landscapes.\n\nWhen functional proteins have been isolated, it is necessary that their genes are too, therefore a genotype-phenotype link is required. This can be covalent, such as mRNA display where the mRNA gene is linked to the protein at the end of translation by puromycin. Alternatively the protein and its gene can be co-localised by compartmentalisation in living cells or emulsion droplets. The gene sequences isolated are then amplified by PCR or by transformed host bacteria. Either the single best sequence, or a pool of sequences can be used as the template for the next round of mutagenesis. The repeated cycles of Diversification-Selection-Amplification generate protein variants adapted to the applied selection pressures.\n\nRational design of a protein relies on an in-depth knowledge of the protein structure, as well as its catalytic mechanism. Specific changes are then made by site-directed mutagenesis in an attempt to change the function of the protein. A drawback of this is that even when the structure and mechanism of action of the protein are well known, the change due to mutation is still difficult to predict. Therefore, an advantage of DE is that there is no need to understand the mechanism of the desired activity or how mutations would affect it.\n\nA restriction of directed evolution is that a high-throughput assay is required in order to measure the effects of a large number of different random mutations. This can require extensive research and development before it can be used for directed evolution. Additionally, such assays are often highly specific to monitoring a particular activity and so are not transferable to new DE experiments.\n\nAdditionally, selecting for improvement in the assayed function simply generates improvements in the assayed function. To understand how these improvements are achieved, the properties of the evolving enzyme have to be measured. Improvement of the assayed activity can be due to improvements in enzyme catalytic activity or enzyme concentration. There is also no guarantee that improvement on one substrate will improve activity on another. This is particularly important when the desired activity cannot be directly screened or selected for and so a ‘proxy’ substrate is used. DE can lead to evolutionary specialisation to the proxy without improving the desired activity. Consequently, choosing appropriate screening or selection conditions is vital for successful DE.\n\nCombined, 'semi-rational' approaches are being investigated to address the limitations of both rational design and directed evolution. Beneficial mutations are rare, so large numbers of random mutants have to be screened to find improved variants. 'Focussed libraries' concentrate on randomising regions thought to be richer in beneficial mutations for the mutagenesis step of DE. A focussed library contains fewer variants than a traditional random mutagenesis library and so does not require such high-throughput screening.\n\nCreating a focussed library requires some knowledge of which residues in the structure to mutate. For example, knowledge of the active site of an enzyme may allow just the residues known to interact with the substrate to be randomised. Alternatively, knowledge of which protein regions are variable in nature can guide mutagenesis in just those regions.\n\nDirected evolution is frequently used for protein engineering as an alternative to rational design, but can also be used to investigate fundamental questions of enzyme evolution.\n\nAs a protein engineering tool, DE has been most successful in three areas:\n\n\nThe study of natural evolution is traditionally based on extant organisms and their genes. However, research is fundamentally limited by the lack of fossils (and particularly the lack of ancient DNA sequences) and incomplete knowledge of ancient environmental conditions. Directed evolution investigates evolution in a controlled system of genes for individual enzymes, ribozymes and replicators (similar to experimental evolution of eukaryotes, prokaryotes and viruses).\n\nDE allows control of selection pressure, mutation rate and environment (both the abiotic environment such as temperature, and the biotic environment, such as other genes in the organism). Additionally, there is a complete record of all evolutionary intermediate genes. This allows for detailed measurements of evolutionary processes, for example epistasis, evolvability, adaptive constraint fitness landscapes, and neutral networks.\n\n\n\n", "id": "4682359", "title": "Directed evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=2528728", "text": "Species complex\n\nIn biology, a species complex is a group of closely related species that are very similar in appearance to the point that the boundaries between them are often unclear. Terms sometimes used synonymously but with more precise meanings are: cryptic species for two or more species hidden under one species name, sibling species for two cryptic species that are each other's closest relative, and species flock for a group of closely related species living in the same habitat. As informal taxonomic ranks, species group, species aggregate, and superspecies are also in use.\n\nTwo or more taxa once considered conspecific (of the same species) may later be subdivided into infraspecific taxa (taxa within a species, such as bacterial strains or plant varieties), but this is not a species complex.\n\nA species complex is in most cases a monophyletic group with a common ancestor, although there are exceptions. It may represent an early stage after speciation, but may also have been separated for a long time period without evolving morphological differences. Hybrid speciation can be a component in the evolution of a species complex.\n\nSpecies complexes exist in all groups of organisms. They are identified by the rigorous study of differences between individual species, making use of minute morphological details, tests of reproductive isolation, or DNA-based methods such as molecular phylogenetics or DNA barcoding. The existence of extremely similar species may cause local and global species diversity to be underestimated. Recognizing similar but distinct species is important for disease and pest control, and in conservation biology, although drawing dividing lines between species can be inherently difficult.\n\nA species complex is typically considered as a group of close, but distinct species. Obviously, the concept is closely tied to the definition of a species. Modern biology understands a species as \"separately evolving metapopulation lineage\" but acknowledges that the criteria to delimit species may depend on the group studied. Thus, many species defined traditionally, based only on morphological similarity, have been found to comprise several distinct species when other criteria, such as genetic differentiation or reproductive isolation were applied.\n\nA more restricted use applies the term to close species between which hybridisation occurred or is occurring, leading to intermediate forms and blurred species boundaries.  The informal classification, superspecies, can be exemplified by the grizzled skipper butterfly, a superspecies that is further divided into three subspecies.\n\nSome authors apply the term also to a species with intraspecific variability, which might be a sign of ongoing or incipient speciation. Examples are ring species or species with subspecies, where it is often unclear if these should be considered separate species.\n\nSeveral terms are used synonymously for a species complex, but some of them may also have slightly different, or more narrow meanings. In the nomenclature codes of zoology and bacteriology, no taxonomic ranks are defined at the level between subgenera and species, while the botanical code defines four ranks below genera (section, subsections, series and subseries). Different informal taxonomic solutions have been used to indicate a species complex.\n\nDistinguishing close species within a complex requires the study of often very small differences. Morphological differences may be minute and only visible using adapted methods, such as microscopy. However, distinct species may sometimes have no morphological differences. In these cases, other characters, e.g. in the species' life history, behavior, physiology, or karyology can be explored. As an example, territorial songs are indicative of species in the treecreepers, a bird genus with little morphological differences. Mating tests are common in some groups such as fungi to confirm the reproductive isolation of two species.\n\nAnalysis of DNA sequences is becoming increasingly standard for species recognition and may in many cases be the only useful method. Different methods are used to analyse such genetic data, for example molecular phylogenetics or DNA barcoding. Such methods have greatly contributed to the discovery of cryptic species, including such emblematic species as the fly agaric or the African elephants.\n\nSpecies forming a complex have typically diverged very recently from each other, allowing in some cases to retrace the process of speciation. Species with differentiated populations such as ring species are sometimes seen as an example of early, ongoing speciation, i.e. a species complex in formation. Nevertheless, similar but distinct species have sometimes been isolated for a long time without evolving differences, a phenomenon called \"morphological stasis\". As an examples, the Amazonian frog \"Pristimantis ockendeni\" is actually at least three different species that diverged over 5 million years ago.\n\nStabilizing selection has been invoked as a force maintaining similarity in species complexes, especially when adaptation to special environments, such as a host in the case of symbionts, or extreme environments, constrains possible directions of evolution: In such cases, strongly divergent selection is not to be expected. Also, asexual reproduction, such as through apomixis in plants, may separate lineages without producing a great degree of morphological differentiation.\n\nA species complex is usually a group that has one common ancestor (a monophyletic group), although closer examination can sometimes disprove this. As an example, the yellow-spotted \"fire salamanders\" in the genus \"Salamandra\", formerly all classified as one species \"S. salamandra\", are not monophyletic: the Corsican fire salamander's closest relative was shown to be the entirely black Alpine salamander. In such cases, similarity has arisen from convergent evolution.\n\nHybrid speciation can lead to unclear species boundaries through a process of reticulate evolution, where species have two parent species as their most recent common ancestors. In such cases, the hybrid species may have intermediate characters, as demonstrated e.g. in \"Heliconius\" butterflies. Hybrid speciation has been observed in various species complexes, such as insects, fungi, and plants. In plants, hybridization often takes place through polyploidization, and hybrid plant species are called nothospecies.\n\nIn regards to whether or not members of a species group share a range, sources differ. A source from Iowa State University Department of Agronomy says that members of a species group usually have partially overlapping ranges but do not interbreed with each other. \"A Dictionary of Zoology\" (Oxford University Press 1999) describes a species group as complex of related species that exist allopatrically and explains that this \"grouping can often be supported by experimental crosses in which only certain pairs of species will produce hybrids.\" The examples given below may support both uses of the term \"species group.\"\n\nOften such complexes only become evident when a new species is introduced into the system, breaking down existing species barriers. An example is the introduction of the Spanish slug in Northern Europe, where interbreeding with the local black slug and red slug, traditionally considered clearly separate species that did not interbreed, shows they may be actually just subspecies of the same species.\n\nWhere closely related species coexist in sympatry, it is often a particular challenge to understand how these similar species persist without outcompeting each other. Niche partitioning is one mechanism invoked to explain this. Studies in some species complexes indeed suggest that species divergence went in par with ecological differentiation, with species now preferring different microhabitats.\nSimilar methods also found that the Amazonian frog \"Eleutherodactylus ockendeni\" is actually at least 3 different species that diverged over 5 million years ago.\nA \"species flock\" may arise when a species penetrates a new geographical area and diversifies to occupy a variety of ecological niches; this process is known as adaptive radiation. The first species flock to be recognized as such was the 13 species of Darwin's finches on the Galápagos Islands described by Charles Darwin.\n\nIt has been suggested that cryptic species complexes are very common in the marine environment. Although this suggestion came before the detailed analysis of many systems using DNA sequence data, it has been proven correct. The increased use of DNA sequence in the investigation of organismal diversity (also called Phylogeography and DNA barcoding) has led to the discovery of a great many cryptic species complexes in all habitats. In the marine bryozoan \"Celleporella hyalina\", detailed morphological analyses and mating compatibility tests between the isolates identified by DNA sequence analysis were used to confirm that these groups consisted of more than 10 ecologically distinct species that had been diverging for many million years.\n\nEvidence from the identification of cryptic species has led some to conclude that current estimates of global species richness are too low.\n\nPests, species causing diseases, and their vectors, have direct importance for humans. When they are found to be cryptic species complexes, the ecology and virulence of each of these species needs to be reevaluated to devise appropriate control strategies. An example are cryptic species in the malaria vector \"Anopheles\", or the fungi causing cryptococcosis.\n\nWhen a species is found to comprise in fact several phylogenetically distinct species, each of these typically have smaller distribution ranges and population sizes than reckoned before. These different species can also differ in their ecology, e.g. having different breeding strategies or habitat requirements, which has to be taken into account for appropriate management. For example, giraffe populations and subspecies differ genetically to such an extent that they may be considered species; while the giraffe as a whole is not considered threatened, considering each cryptic species separately would mean a much higher level of threat.\n\n", "id": "2528728", "title": "Species complex"}
{"url": "https://en.wikipedia.org/wiki?curid=22670884", "text": "Bateson–Dobzhansky–Muller model\n\nThe Bateson-Dobzhansky-Muller Model, also known as Dobzhansky-Muller Model, is a model of the evolution of genetic incompatibility, important in understanding the evolution of reproductive isolation during speciation and the role of natural selection in bringing it about. The theory was first described by William Bateson in 1909, then independently described by Theodosius Dobzhansky in 1934, and later elaborated in different forms by Herman Muller, H. Allen Orr and Sergey Gavrilets.\n\nThe model states that genetic incompatibility is most likely evolved by alternative fixation of two or more loci instead of just one, so that when hybridization occurs, it is the first time for some of the alleles to co-occur in the same individual. For example, imagine two populations that only recently separated geographically. Both sides are starting with the same genotype \"AABB\". One population can then evolve to \"aaBB\", through the transition state \"AaBB\", while the other evolves to \"AAbb\", through the transition state \"AABb\". During these processes, \"a\" allele will co-occur with \"A\" and \"B\" alleles, and \"b\" will co-occur with \"A\" and \"B\" alleles, without fitness consequences. When hybridization occurs, it is the first time \"a\" and \"b\" alleles co-occur, and they are the alleles that are incompatible.\n", "id": "22670884", "title": "Bateson–Dobzhansky–Muller model"}
{"url": "https://en.wikipedia.org/wiki?curid=45302158", "text": "Karyoklepty\n\nKaryoklepty is a strategy for cellular evolution, whereby a predator cell appropriates the nucleus of a cell from another organism to supplement its own biochemical capabilities.\n\nIn the related process of kleptoplasty, the predator sequesters plastids (especially chloroplasts) from dietary algae. The chloroplasts can still photosynthesize, but do not last long after the prey's cells are metabolised. If the predator can also sequester cell nuclei from the prey to encode proteins for the plastids, it can sustain them. \"Karyoklepty\" is this sequestration of nuclei; even after sequestration, the nuclei are still capable of transcription.\n\nJohnson et al. described and named karyoklepty in 2007 after observing it in the ciliate species \"Myrionecta rubra\". \"Karyoklepty\" is a Greek compound of the words \"karydi\" (\"kernel\") and \"kleftis\" (\"thief\").\n\n", "id": "45302158", "title": "Karyoklepty"}
{"url": "https://en.wikipedia.org/wiki?curid=45338775", "text": "Reticulate evolution\n\nThe adjective reticulate stems from the Latin words \"reticulatus\" “having a net-like pattern” and \"reticulum\" “little net”, according to the Etymology Dictionary. Reticulate evolution, or network evolution, describes the origination of a lineage through the partial merging of two ancestor lineages, leading to relationships better described by a phylogenetic network than a bifurcating tree. Reticulate patterns can be found in the phylogenetic reconstructions of biodiversity lineages obtained by comparing the characteristics of organisms. Reticulation processes can potentially be convergent and divergent at the same time. Reticulate evolution indicates the lack of independence between two evolutionary lineages. Reticulation affects survival, fitness and speciation rates of species.  \n\nReticulate evolution can happen between lineages separated only for a short time, for example through hybrid speciation in a species complex. Nevertheless, it also takes place over larger evolutionary distances, as exemplified by the presence of organelles of bacterial origin in eukaryotic cells.\n\nReticulation occurs at various levels: at a chromosomal level, meiotic recombination causes evolution to be reticulate; at a species level, reticulation arises through hybrid speciation and horizontal gene transfer; and at a population level, sexual recombination causes reticulation.\n\nSince the nineteenth century, scientists from different disciplines have studied how reticulate evolution is induced. Researchers succeeded to increasingly identify these mechanisms and processes. Reticulate evolution is found to be driven by symbiosis, symbiogenesis (endosymbiosis), lateral gene transfer, hybridization and infectious heredity.\n\nSymbiosis is a close and long-term biological interaction between two different biological organisms. Often, both of the organisms involved develop new features upon the interaction with the other organism. This may lead to the development of new, distinct organisms. The alterations in genetic material upon symbiosis can occur via germline transmission or lateral transmission. Therefore, the interaction between different organisms can drive evolution of one or both organisms.\n\nSymbiogenesis (endosymbiosis) is a special form of symbiosis whereby an organism lives inside another, different organism. Symbiogenesis is thought to be very important in the origin and evolution of eukaryotes. Eukaryotic organelles, such as mitochondria, have been theorized to have been originated from cell-invaded bacteria living inside another cell.\n\nLateral gene transfer, or horizontal gene transfer, is the movement of genetic material between unicellular and/or multicellular organisms without a parent-offspring relationship. The horizontal transfer of genes results in new genes, which could give new functions to the recipient and thus could drive evolution.\n\nIn the neo-Darwinian paradigm, one of the assumed definition of a species is that of Mayr’s, which defines species based upon sexual compatibility. Mayr’s definition therefore suggests that individuals that can produce fertile offspring must belong to the same species. However, in hybridisation, two organisms produce offspring while being distinct species. During hybridisation the characteristics of these two different species are combined yielding a new organism, called a hybrid, thus driving evolution.\n\nInfectious agents, such as viruses, can infect the cells of host organisms. Viruses infect cells of other organisms in order to enable their own reproduction. Hereto, many viruses can insert copies of their genetic material into the host genome, potentially altering the phenotype of the host cell. When these viruses insert their genetic material in the genome of germ line cells, the modified host genome will be passed onto the offspring, yielding genetically differentiated organisms. Therefore, infectious heredity plays an important role in evolution, for example in the formation of the female placenta.\n\nReticulate evolution has played a key role in the evolution of some organisms such as bacteria and flowering plants. However, most methods for studying cladistics have been based on a model of strictly branching cladogeny, without assessing the importance of reticulate evolution. Reticulation at chromosomal, genomic and species levels fails to be modelled by a bifurcating tree.\n\nAccording to Ford Doolittle, an evolutionary and molecular biologist: “Molecular phylogeneticists will have failed to find the “true tree,” not because their methods are inadequate or because they have chosen the wrong genes, but because the history of life cannot properly be represented as a tree”.\n\nReticulate evolution refers to evolutionary processes which cannot be successfully represented using a classical phylogenetic tree model, as it gives rise to rapid evolutionary change with horizontal crossings and mergings often preceding a pattern of vertical descent with modification. Reconstructing phylogenetic relationships under reticulate evolution requires adapted analytical methods. Reticulate evolution dynamics contradict the neo-Darwininan theory, compiled in the Modern Synthesis, by which the evolution of life occurs through natural selection and is displayed with a bifurcating or ramificating pattern. Frequent hybridisation between species in natural populations challenges the assumption that species have evolved from a common ancestor by simple branching, in which branches are genetically isolated. The study of reticulate evolution is said to have been largely excluded from the modern synthesis. The urgent need for new models which take reticulate evolution into account has been stressed by many evolutionary biologists, such as Nathalie Gontier who has stated \"\"reticulate evolution today is a vernacular concept for evolutionary change induced by mechanisms and processes of symbiosis, symbiogenesis, lateral gene transfer, hybridization, or divergence with gene flow, and infectious heredity\"\". She calls for an extended evolutionary synthesis that integrates these mechanisms and processes of evolution.\n\nReticulate evolution has been extensively applied by humans in plant hybridization in agriculture and gardening. The first commercial hybrids appeared in the early 1920s. Since then, many protoplast fusion experiments have been carried out, some of which were aimed at improvement of crop species. Wild types possessing desirable agronomic traits are selected and fused in order to yield novel, improved species. The newly generated plant will be improved for traits such as better yield, greater uniformity, improved color, and disease resistance.\n\nReticulate evolution is regarded as a process that has shaped the histories of many organisms. There is evidence of reticulation events in flowering plants, as the variation patterns between angiosperm families strongly suggests there has been widespread hybridisation. Grant states that phylogenetic networks, instead of phylogenetic trees, arise in all major groups of higher plants. Stable speciation events due to hybridisation between angiosperm species supports the occurrence of reticulate evolution and highlights the key role of reticulation in the evolution of plants.\n\nGenetic transfer can occur across wide taxonomic levels in microorganisms and become stably integrated into the new microbial populations, as has been observed through protein sequencing. Reticulation in bacteria usually only involves the transfer of only a few genes or parts of these. Reticulate evolution driven by lateral gene transfer has also been observed in marine live. Lateral genetic transfer of photo-response genes between planktonic bacteria and Archaea has been evidenced in some groups, showing an associated increase in environmental adaptability in organisms inhabiting photic zones.\n\nMoreover, in the well-studied Darwin finches signs of reticulate evolution can be observed. Peter and Rosemary Grant, who carried out extensive research on the evolutionary processes of the Geospiza genus, found that hybridization occurs between some species of Darwin finches, yielding hybrid forms. This event could explain the origin of intermediate species. Jonathan Weiner commented on the observations of the Grants, suggesting the existence of reticulate evolution: \"\"To the Grants, the whole tree of life now looks different from a year ago. The set of young twigs and shoots they study seems to be growing together in some seasons, apart in others. The same forces that created these lines are moving them toward fusion and then back toward fission\".\"; and \"\"The Grants are looking at a pattern that was once dismissed as insignificant in the tree of life. The pattern is known as reticulate evolution, from the Latin reticulum, diminutive for net. The finches' lines are not so much lines or branches at all. They are more like twiggy thickets, full of little networks and delicate webbings\".\"\n\n\n", "id": "45338775", "title": "Reticulate evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=45391102", "text": "Viral dynamics\n\nViral dynamics is a field of applied mathematics concerned with describing the progression of viral infections within a host organism. It employs a family of mathematical models that describe changes over time in the populations of cells targeted by the virus and the viral load. These equations may also track competition between different viral strains and the influence of immune responses. The original viral dynamics models were inspired by compartmental epidemic models (e.g. the SI model), with which they continue to share many common mathematical features, such as the concept of the basic reproductive ratio (\"R\"). The major distinction between these fields is in the scale at which the models operate: while epidemiological models track the spread of infection between individuals within a population (i.e. \"between host\"), viral dynamics models track the spread of infection between cells within an individual (i.e. \"within host\"). Analyses employing viral dynamic models have been used extensively to study HIV, hepatitis B virus, and hepatitis C virus, among other infections\n\n", "id": "45391102", "title": "Viral dynamics"}
{"url": "https://en.wikipedia.org/wiki?curid=569092", "text": "Behavioral modernity\n\nBehavioral modernity is a suite of behavioral and cognitive traits that distinguishes current \"Homo sapiens\" from other anatomically modern humans, hominins, and primates. Although often debated, most scholars agree that modern human behavior can be characterized by abstract thinking, planning depth, symbolic behavior (e.g., art, ornamentation, music), exploitation of large game, and blade technology, among others. Underlying these behaviors and technological innovations are cognitive and cultural foundations that have been documented experimentally and ethnographically. Some of these human universal patterns are cumulative cultural adaptation, social norms, language, and extensive help and cooperation beyond close kin. It has been argued that the development of these modern behavioral traits, in combination with the climatic conditions of the Last Glacial Maximum, was largely responsible for the human replacement of Neanderthals and the peopling of the rest of the world.\n\nArising from differences in the archaeological record, a debate continues as to whether anatomically modern humans were behaviorally modern as well. There are many theories on the evolution of behavioral modernity. These generally fall into two camps: gradualist and cognitive approaches. The Later Upper Paleolithic Model refers to the idea that modern human behavior arose through cognitive, genetic changes abruptly around 40,000–50,000 years ago. Other models focus on how modern human behavior may have arisen through gradual steps; the archaeological signatures of such behavior only appearing through demographic or subsistence-based changes.\n\nIn order to classify what traits should be included in modern human behavior, it is necessary to define behaviors that are universal among living human groups. Some examples of these human universals are abstract thought, planning, trade, cooperative labor, body decoration, control and use of fire. Along with these traits, humans possess a heavy reliance on social learning. This cumulative cultural change or cultural \"ratchet\" separates human culture from social learning in animals. As well, a reliance on social learning may be responsible in part for humans' rapid adaptation to many environments outside of Africa.\nSince cultural universals are found in all cultures including some of the most isolated indigenous groups, these traits must have evolved or have been invented in Africa prior to the exodus.\n\nArchaeologically a number of empirical traits have been used as indicators of modern human behavior. While these are often debated a few are generally agreed upon. Archaeological evidence of behavioral modernity are:\n\nSeveral critiques have been placed against the traditional concept of behavioral modernity, both methodologically and philosophically. Shea (2011) outlines a variety of problems with this concept, arguing instead for \"behavioral variability\", which, according to the author, better describes the archaeological record. The use of trait lists, according to Shea (2011), runs the risk of taphonomic bias, where some sites may yield more artifacts than others despite similar populations; as well, trait lists can be ambiguous in how behaviors may be empirically recognized in the archaeological record. Shea (2011) in particular cautions that population pressure, cultural change, or optimality models, like those in human behavioral ecology, might better predict changes in tool types or subsistence strategies than a change from \"archaic\" to \"modern\" behavior. Some researchers argue that a greater emphasis should be placed on identifying only those artifacts which are unquestionably, or purely, symbolic as a metric for modern human behavior.\n\nThe Late Upper Paleolithic Model, or Upper Paleolithic Revolution, refers to the idea that, though anatomically modern humans first appear around 150,000 years ago, they were not cognitively or behaviorally \"modern\" until around 50,000 years ago, leading to their expansion into Europe and Asia. These authors note that traits used as a metric for behavioral modernity do not appear as a package until around 40–50,000 years ago. Klein (1995) specifically describes evidence of fishing, bone shaped as a tool, hearths, significant artifact diversity, and elaborate graves are all absent before this point. Although assemblages before 50,000 years ago show some diversity the only distinctly modern tool assemblages appear in Europe at 48,000. According to these authors, art only becomes common beyond this switching point, signifying a change from archaic to modern humans. Most researchers argue that a neurological or genetic change, perhaps one enabling complex language, such as FOXP2, caused this revolutionary change in our species.\n\nContrasted with this view of a spontaneous leap in cognition among ancient humans, some authors like Alison S. Brooks, primarily working in African archaeology, point to the gradual accumulation of \"modern\" behaviors, starting well before the 50,000 year benchmark of the Upper Paleolithic Revolution models. Howiesons Poort, Blombos, and other South African archaeological sites, for example, show evidence of marine resource acquisition, trade, and abstract ornamentation at least by 80,000 years ago. Given evidence from Africa and the Middle East, a variety of hypotheses have been put forth to describe an earlier, gradual transition from simple to more complex human behavior. Some authors have pushed back the appearance of fully modern behavior to around 80,000 years ago in order to incorporate the South African data.\n\nOthers focus on the slow accumulation of different technologies and behaviors across time. These researchers describe how anatomically modern humans could have been cognitively the same and what we define as behavioral modernity is just the result of thousands of years of cultural adaptation and learning. D'Errico and others have looked at Neanderthal culture rather than early human behavior for clues into behavioral modernity. Noting that Neanderthal assemblages often portray traits similar to those listed for modern human behavior, researchers stress that the foundations for behavioral modernity may in fact lie deeper in our hominin ancestors. If both modern humans and Neanderthals express abstract art and complex tools then \"modern human behavior\" cannot be a derived trait for our species. They argue that the original 'human revolution' theory reflects a profound Eurocentric bias. Recent archaeological evidence, they argue, proves that humans evolving in Africa some 300,000 or even 400,000 years ago were already becoming cognitively and behaviourally 'modern'. These features include blade and microlithic technology, bone tools, increased geographic range, specialized hunting, the use of aquatic resources, long distance trade, systematic processing and use of pigment, and art and decoration. These items do not occur suddenly together as predicted by the ‘‘human revolution’’ model, but at sites that are widely separated in space and time. This suggests a gradual assembling of the package of modern human behaviours in Africa, and its later export to other regions of the Old World.\n\nBetween these extremes is the view – currently supported by archaeologists Chris Henshilwood, Curtis Marean, Ian Watts and others – that there was indeed some kind of 'human revolution' but that it occurred in Africa and spanned tens of thousands of years. The term 'revolution' in this context would mean not a sudden mutation but a historical development along the lines of 'the industrial revolution' or 'the Neolithic revolution'. In other words, it was a relatively accelerated process, too rapid for ordinary Darwinian 'descent with modification' yet too gradual to be attributed to a single genetic or other sudden event. These archaeologists point in particular to the relatively explosive emergence of ochre crayons and shell necklaces apparently used for cosmetic purposes. These archaeologists see symbolic organisation of human social life as the key transition in modern human evolution. Recently discovered at sites such as Blombos Cave and Pinnacle Point, South Africa, pierced shells, pigments and other striking signs of personal ornamentation have been dated within a time-window of 70,000 – 160,000 years ago in the African Middle Stone Age, suggesting that the emergence of \"Homo sapiens\" coincided, after all, with the transition to modern cognition and behaviour. While viewing the emergence of language as a 'revolutionary' development, this school of thought generally attributes it to cumulative social, cognitive and cultural evolutionary processes as opposed to a single genetic mutation.\n\nA further view, taken by archaeologists such as Francesco D'Errico and João Zilhão, is a multi-species perspective arguing that evidence for symbolic culture in the form of utilised pigments and pierced shells are also found in Neanderthal sites, independently of any 'modern' human influence.\n\nCultural evolutionary models may also shed light on why although evidence of behavioral modernity exists before 50,000 years ago it is not expressed consistently until that point. With small population sizes, human groups would have been affected by demographic and cultural evolutionary forces that may not have allowed for complex cultural traits. According to some authors until population density became significantly high, complex traits could not have been maintained effectively. It is worth noting that some genetic evidence supports a dramatic increase in population size before human migration out of Africa. High local extinction rates within a population also can significantly decrease the amount of diversity in neutral cultural traits, regardless of cognitive ability.\n\nHighly speculatively, bicameral mind theory argues for an additional, and cultural rather than genetic, shift from selfless to self-perceiving forms of human cognition and behavior very late in human history, in the Bronze Age. This is based on a literary analysis of Bronze Age texts which claims to show the first appearances of the concept of self around this time, replacing the voices of gods as the primary form of recorded human cognition. This non-mainstream theory is not widely accepted but does receive serious academic interest from time to time.\n\nBefore the Out of Africa theory was generally accepted, there was no consensus on where the human species evolved and, consequently, where modern human behavior arose. Now, however, African archaeology has become extremely important in discovering the origins of humanity. Since human expansion into Europe around 48,000 years ago is generally accepted as already \"modern\", the question becomes whether behavioral modernity appeared in Africa well before 50,000 years ago, as a late Upper Paleolithic \"revolution\" which prompted migration out of Africa, or arose outside Africa and diffused back.\n\nA variety of evidence of abstract imagery, widened subsistence strategies, and other \"modern\" behaviors have been discovered in Africa, especially South Africa. The Blombos Cave site in South Africa, for example, is famous for rectangular slabs of ochre engraved with geometric designs. Using multiple dating techniques, the site was confirmed to be around 77,000 years old. Beads and other personal ornamentation have been found from Morocco which might be as old as 130,000 years old; as well, the Cave of Hearths in South Africa has yielded a number of beads significantly before 50,000 years ago.\n\nExpanding subsistence strategies beyond big-game hunting and the consequential diversity in tool types has been noted as signs of behavioral modernity. A number of South African sites have shown an early reliance on aquatic resources from fish to shellfish. Pinnacle Point, in particular, shows exploitation of marine resources as early as 120,000 years ago, perhaps in response to more arid conditions inland. Establishing a reliance on predictable shellfish deposits, for example, could reduce mobility and facilitate complex social systems and symbolic behavior. Blombos Cave and Site 440 in Sudan both show evidence of fishing as well. Taphonomic change in fish skeletons from Blombos Cave have been interpreted as capture of live fish, clearly an intentional human behavior.\n\nWhile traditionally described as evidence for the later Upper Paleolithic Model, European archaeology has shown that the issue is more complex. A variety of stone tool technologies are present at the time of human expansion into Europe and show evidence of modern behavior. Despite the problems of conflating specific tools with cultural groups, the Aurignacian tool complex, for example, is generally taken as a purely modern human signature. The discovery of \"transitional\" complexes, like \"proto-Aurignacian\", have been taken as evidence of human groups progressing through \"steps of innovation\". If, as this might suggest, human groups were already migrating into eastern Europe around 40,000 years and only afterward show evidence of behavioral modernity, then either the cognitive change must have diffused back into Africa or was already present before migration.\n\nIn light of a growing body of evidence of Neanderthal culture and tool complexes some researchers have put forth a \"multiple species model\" for behavioral modernity. Neanderthals were often cited as being an evolutionary dead-end, apish cousins who were less advanced than their human contemporaries. Personal ornaments were relegated as trinkets or poor imitations compared to the cave art produced by \"H. sapiens\". Despite this, European evidence has shown a variety of personal ornaments and artistic artifacts produced by Neanderthals; for example, the Neanderthal site of Grotte du Renne has produced grooved bear, wolf, and fox incisors, ochre and other symbolic artifacts. Though burials are few and controversial, there has been circumstantial evidence of Neanderthal ritual burials. There are two options to describe this symbolic behavior among Neanderthals: they copied cultural traits from arriving modern humans or they had their own cultural traditions comparative with behavioral modernity. If they just copied cultural traditions, which is debated by several authors, they still possessed the capacity for complex culture described by behavioral modernity. As discussed above, if Neanderthals also were \"behaviorally modern\" then it cannot be a species-specific derived trait.\n\nMost debates surrounding behavioral modernity have been focused on Africa or Europe but an increasing amount of focus has been placed on East Asia. This region offers a unique opportunity to test hypotheses of multi-regionalism, replacement, and demographic effects. Unlike Europe, where initial migration occurred around 50,000 years ago, human remains have been dated in China to around 100,000 years ago. This early evidence of human expansion calls into question behavioral modernity as an impetus for migration.\n\nStone tool technology is particularly of interest in East Asia. Following Homo erectus migrations out of Africa, Acheulean technology never seems to appear beyond present-day India and into China. Analogously, Mode 3, or Levallois technology, is not apparent in China following later hominin dispersals. This lack of more advanced technology has been explained by serial founder effects and low population densities out of Africa. Though tool complexes comparative to Europe are missing or fragmentary, other archaeological evidence shows behavioral modernity. For example, the peopling of the Japanese archipelago offers an opportunity to investigate the early use of watercraft. Though one site, Kanedori in Honshu, does suggest the use of watercraft as early as 84,000 years ago, there is no other evidence of hominins in Japan until 50,000 years ago.\n\nThe Zhoukoudian cave system near Beijing has been excavated since the 1930s and has yielded precious data on early human behavior in East Asia. Though disputed, there is evidence of possible human burials and interred remains in the cave dated to around 34-20,000 years ago. These remains have associated personal ornaments in the form of beads and worked shell, suggesting symbolic behavior. Along with possible burials, numerous other symbolic objects like punctured animal teeth and beads, some dyed in red ochre, have all been found at Zhoukoudian. Though fragmentary, the archaeological record of eastern Asia shows evidence of behavioral modernity before 50,000 years ago but, like the African record, it is not fully apparent until that time.\n\n", "id": "569092", "title": "Behavioral modernity"}
{"url": "https://en.wikipedia.org/wiki?curid=23829531", "text": "World Summit on Evolution\n\nThe World Summit on Evolution is an evolutionary biology meeting hosted at the Galapagos Islands by Universidad San Francisco de Quito (USFQ), an Ecuadorian private liberal arts university. Its focus is on recent research and new advances in our understanding of evolution and the diversity of life.\n\nThe summit hosts more than 150 participants presenting invited and submitted talks, poster sessions and scientific-outreach talks. It has been called \"The Woodstock of Evolution\" bringing together experts and students from widely different areas of evolutionary biology that rarely meet. It has attracted researchers working on evolution from over 15 different countries, including Peter and Rosemary Grant, Niles Eldredge, Antonio Lazcano, Douglas Futuyma, Lynn Margulis, Ada Yonath, William H. Calvin and Daniel Dennett.\n\nObjectives:\n\nThrough a series of presentations and discussions the participants ask the big questions: What is the evidence for the theory of evolution? How has each field and their respective approaches deepened our understating? And where are the future horizons? Bringing together international experts and students for debate helps to answer these questions and hopefully lead to decisions that will shape the direction of evolutionary science in the foreseeable future.\n\nSubjects: \n\nThe World Summit on Evolution takes place at Galapagos Academic Institute for the Arts and Sciences (GAIAS), part of the Universidad San Francisco de Quito. GAIAS was established in 2002 at the capital town of the Galapagos province, Puerto Baquerizo Moreno, on the island of San Cristobal, one of the largest of the Galapagos Islands.\n\nIts 4.5 hectare campus is the only one located on the historically significant Galapagos islands. GAIAS was founded on the principle that would become a first-rate institution for international students and researchers.\n\nThe Galapagos Islands inspired Charles Darwin to define his evolutionary theory, which revolutionized human understanding in relation to the diversity of species, including humans. His ideas were presented in \"On the Origin of Species\".\n\nThe Galapagos Islands, are important for the scientific studies that have been developed over the centuries after his visit.\n\nThe Second World Summit on Evolution was launched to celebrate Charles Darwin's 200th birthday.\n\nThe 2009 summit included the first meeting of the Sociedad Iberoamericana de Biologia Evolutiva (SIBE). SIBE led to the establishment of academic and intellectual bonds between the Spanish- and Portuguese-speaking specialists in evolutionary biology.\n\nThe summit adopted the theme ‘Why Does Evolution Matter’. 200-attendees met, to listen to 12 keynote speakers, 20 oral presentations and 31 posters by faculty, postdocs and graduate and undergraduate students. The Summit encompassed five sessions: evolution and society, pre-cellular evolution and the RNA world, behavior and environment, genome, and microbes and diseases. USFQ and GAIAS launched officially the Lynn Margulis Center for Evolutionary Biology and showcased the Galapagos Science Center, developed in partnership with the University of North Carolina at Chapel Hill.\n\n", "id": "23829531", "title": "World Summit on Evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=46219227", "text": "Evolution of metal ions in biological systems\n\nEvolution of metal ions in biological systems refers to the incorporation of metallic ions into living organisms and how it has changed over time. Metal ions have been associated with biological systems for billions of years, but only in the last century have scientists began to truly appreciate the scale of their influence. Major (iron, manganese, magnesium and zinc) and minor (copper, cobalt, nickel, molybdenum, tungsten) metal ions have become aligned with living organisms through the interplay of biogeochemical weathering and metabolic pathways involving the products of that weathering. The associated complexes have evolved over time.\n\nNatural development of chemicals and elements challenged organisms to adapt or die. Current organisms require redox reactions to induce metabolism and other life processes. Metals have a tendency to lose electrons and are important for redox reactions. “Metals have become so central to cellular function that the collection of metal-binding proteins (referred to as the metallomes) accounts for over 30% of all proteins in the cell. Metals are known to be involved in over 40% of enzymatic reactions, and metal-binding proteins carry out at least one step in almost all biological pathways”.Metals are also toxic so a balance must be acquired to regulate where the metals are in an organism as well as in what quantities. Many organisms have flexible systems in which they can exchange one metal for another if one is scarce. Metals in this discussion are naturally occurring elements that have a tendency to undergo oxidation. Vanadium, molybdenum, cobalt, copper, chromium, iron, manganese, nickel, and zinc are deemed essential because without them biological function is impaired.\n\nThe Earth began as an iron aquatic world with low oxygen. The Great Oxygenation Event occurred approximately 2.4 Ga (billion years ago) as cyanobacteria and photosynthetic life induced the presence of dioxygen in the planet’s atmosphere. Iron became insoluble (as did other metals) and scarce while other metals became soluble. Sulfur was a very important element during this time. Once oxygen was released into the environment, sulfates made metals more soluble and released those metals into the environment; especially into the water. Incorporation of metals perhaps combatted oxidative stress.\n\nThe central chemistry of all these cells has to be reductive in order that the synthesis of the required chemicals, especially biopolymers, is possible. The different anaerobic, autocatalysed, reductive, metabolic pathways seen in the earliest known cells developed in separate energised vesicles, protocells, where they were produced cooperatively with certain bases of the nucleic acids.\n\nHypotheses proposed for how elements became essential is their relative quantity in the environment as life formed. This has produced research on the origin of life; for instance, Orgel and Crick hypothesized that life was extraterrestrial due to the alleged low abundance of molybdenum on early Earth (it is now suspected that there were larger quantities than previously thought). Another example is life forming around thermal vents based on the availability of zinc and sulfur. In conjunction with this theory is that life evolved as chemoautotrophs. Therefore, life occurred around metals and not in response to their presence. Some evidence for this theory is that inorganic matter has self-contained attributes that life adopted as shown by life's compartmentalization.\n\nThe prebiotic chemistry of life had to be reductive in order to obtain, e.g. Carbon monoxide (CO) and Hydrogen cyanide (HCN) from existing CO and N in the atmosphere. CO and HCN were precursor molecules of the essential biomolecules, proteins, lipids, nucleotides and sugars. However, atmospheric oxygen levels increased considerably, and it was then necessary for cells to have control over the reduction and oxidation of such small molecules in order to build and break down cells when necessary, without the inevitable oxidation (breaking down) of everything. Transition metal ions, due to their multiple oxidation states, were the only elements capable of controlling the oxidation states of such molecules, and thus were selected for.\n\nO-donors such as HPO were abundant in the prebiotic atmosphere. Metal ion binding to such O-donors was required to build the biological polymers, since the bond is generally weak, it can catalyze the required reaction and dissociate after (i.e. Mg in DNA synthesis).\n\nAround 4 Ga , the acidic seawater contained high amounts of HS and thus created a reducing environment with a potential of around -0.2 V. So any element that had a large negative value with respect to the reduction potential of the environment was available in its free ionic form and can subsequently be incorporated into cells, i.e. Mg has a reduction potential of -2.372 V, and was available in its ionic form at that time.\n\nAround 2 Ga, an increase in atmospheric oxygen levels took place, causing an oxidation of HS in the surroundings, and an increase in the pH of the sea water. The resulting environment had become more oxidizing and thus allowed the later incorporation of the heavier metals such as copper and zinc.\n\nAnother factor affecting the availability of metal ions was their solubilities with HS. Hydrogen sulfide was abundant in the early sea giving rise to HS in the prebiotic acidic conditions and HS in the neutral (pH = 7.0) conditions. In the series of metal sulfides, insolubility increases at neutral pH following the Irving-Williams series:\n\nMn(II) < Fe(II) < Co(II) ≤ Ni(II) < Cu(II) > Zn(II)\n\nSo in high amounts of HS, which was the prebiotic condition, only Fe was most prominently available in its ionic form due to its low insolubility with sulfides. The increasing oxidation of HS into SO leads to the later release of Co, Ni, Cu, and Zn since all of their sulfates are soluble.\n\nMagnesium is the eighth most abundant element on earth. It is the fourth most abundant element in vertebrates and the most abundant divalent cation within cells. The most available form of magnesium (Mg) for living organisms can be found in the hydrosphere. The concentration of Mg in seawater is around 55 mM. Mg is readily available to cells during early evolution due to its high solubility in water. Other transition metals like calcium precipitate from aqueous solutions at much lower concentrations than the corresponding Mg salts.\n\nSince magnesium was readily available in early evolution, it can be found in every cell type living organism. Magnesium in anaerobic prokaryotes can be found in MgATP. Magnesium also has many functions in prokaryotes such as glycolysis, all kinases, NTP reaction, signalling, DNA/RNA structures and light capture. In aerobic eukaryotes, magnesium can be found in cytoplasm and chloroplasts. The reactions in these cell compartments are glycolysis, photophosphorylation and carbon assimilation.\n\nATP, the main source of energy in almost all living organisms, must bind with metal ions such as Mg or Ca to function. Examination of cells with limited magnesium supply has shown that a lack of magnesium can cause a decrease in ATP. Magnesium in ATP hydrolysis acts as a co-factor to stabilize the high negative charge transition state. MgATP can be found in both prokaryotes and eukaryotes cells. However, most of the ATP in cells is MgATP. Following the Irving–Williams series, magnesium has a higher binding constant than the Ca. Therefore, the dominant ATP in living organisms is MgATP. A greater binding constant also given magnesium the advantage as a better catalyst over other competing transition metals.\n\nEvidence suggests that manganese (Mn) was first incorporated into biological systems roughly 3.2 - 2.8 billion years ago, during the Archean Period. Together with calcium, it formed the manganese-calcium oxide complex (determined by X-ray diffraction) which consisted of a manganese cluster, essentially an inorganic cubane (cubical) structure. The incorporation of a manganese center in photosystem II was highly significant, as it allowed for photosynthetic oxygen evolution of plants. The oxygen-evolving complex (OEC) is a critical component of photosystem II contained in the thylakoid membranes of chloroplasts; it is responsible for terminal photooxidation of water during light reactions.\n\nThe incorporation of Mn in proteins allowed the complexes the ability to reduce reactive oxygen species in Mn-superoxide dismutatse (MnSOD) and catalase, in electron transfer-dependent catalysis (for instance in certain class I ribonucleotide reductases) and in the oxidation of water by photosystem II (PSII), where the production of thiobarbituric acid-reactive substances is decreased. This is due to manganese's ability to reduce superoxide anion and hydroxyl radicals as well as its chain-breaking capacity.\n\nIron (Fe) is the most abundant element in the Earth and the fourth most abundant element in the crust, approximately 5 percent by mass. Due to the abundance of iron and its role in biological systems, the transition and mineralogical stages of iron have played a key role in Earth surface systems. It played a larger role in the geological past in marine geochemistry, as evidenced by the deposits of Precambrian iron-rich sediments. The redox transformation of Fe(II) to Fe(III), or vice versa, is vital to a number of biological and element cycling processes. The reduction of Fe(III) is seen to oxidize sulfur (from HS to SO), which is a central process in marine sediments. Many of the first metalloproteins consisted of iron-sulphur complexes formed during photosynthesis. Iron is the main redox metal in biological systems. In proteins, it is found in a variety of sites and cofactors, including, for instance, haem groups, Fe–O–Fe sites, and iron–sulfur clusters.\n\nThe prevalence of iron is apparently due to the large availability of Fe(II) in the initial evolution of living organisms, before the rise of photosynthesis and an increase in atmospheric oxygen levels which resulted in the precipitation of iron in the environment as Fe(OH). It has flexible redox properties because such properties are sensitive to ligand coordination, including geometry. Iron can be also used in enzymes due to its Lewis acid properties, for example in nitrile hydratase. Iron is frequently found in mononuclear sites in the reduced Fe(II) form, and functions in dioxygen activation; this function is used as a major mechanism adopted by living organisms to avoid the kinetic barrier hindering the transformation of organic compounds by O. Iron can be taken up selectively as ferredoxins, Fe-O-Fe (hemerythrin and ribonucleotide reductase), Fe (many oxidases), apart from iron porphyrin. Variation in the related proteins with any one of these chemical forms of iron has produced a wide range of enzymes. All of these arrangements are modified to function both in the sense of reactivity and the positioning of the protein in the cell. Iron can have various redox and spin states, and it can be held in many stereochemistries.\n\nAround 4-3 Ga, anaerobic prokaryotes began developing metal and organic cofactors for light absorption. They ultimately ended up making chlorophyll from Mg(II), as is found in cyanobacteria and plants, leading to modern photosynthesis. However, chlorophyll synthesis requires numerous steps. The process starts with uroporphyrin, a primitive precursor to the porphyrin ring which may be biotic or abiotic in origin, which is then modified in cells differently to make Mg, Fe, nickel (Ni), and cobalt (Co) complexes. The centers of these rings are not selective, thus allowing the variety of metal ions to be incorporated. Mg porphyrin gives rise to chlorophyll, Fe porphyrin to heme proteins, Ni porphyrin yields factor F-430, and Co porphyrin Coenzyme B12.\n\nBefore the Great Oxygenation Event, copper was not readily available for living organisms. Most early copper was Cu and Cu. This oxidation state of copper is not very soluble in water. One billion years ago, after the great oxidation event the oxygen pressure rose sufficiently to oxidise Cu to Cu, increasing its solubility in water. As a result, the copper became much more available for living organisms.\n\nMost copper-containing proteins and enzymes can be found in eukaryotes. Only a handful of prokaryotes such as aerobic bacteria and cyanobacteria contain copper enzymes or proteins. Copper can be found in both prokaryotes and eukaryotes superoxide dismutase (SOD) enzyme. There are three distinct types of SOD, containing Mn, Fe and Cu respectively. Mn-SOD and Fe-SOD are found in most prokaryotes and mitochondria of the eukaryotic cell. Cu-SOD can be found in the cytoplasmic fraction of the eukaryotic cells. The three elements, copper, iron and manganese, can all catalyze superoxide to ordinary molecular oxygen or hydrogen peroxide. However, Cu-SOD is more efficient than Fe-SOD and Mn-SOD. Most prokaryotes only utilize Fe-SOD or Mn-SOD due to the lack of copper in the environment. Some organisms did not develop Cu-SOD due to the lack of a gene pool for the Cu-SOD adoption.\n\nZinc (Zn) was incorporated into living cells in two waves. Four to three Ga, anaerobic prokaryotes arose, and the atmosphere was full of HS and highly reductive. Thus most zinc was in the form of insoluble ZnS. However, because seawater at the time was slightly acidic, some Zn(II) was available in its ionic form and became part of early anaerobic prokaryotes’ external proteases, external nucleases, internal synthetases and dehydrogenases.\n\nDuring the second wave, once the Great Oxygenation Event occurred, more Zn(II) ions were available in the seawater. This allowed its incorporation in the single-cell eukaryotes as they arose at this time. It is believed that the later addition of ions such as zinc and copper allowed them to displace iron and manganese from the enzyme superoxide dismutase (SOD). Fe and Mn complexes dissociate readily (Irving-Williams series) while Zn and Cu do not. This is why eukaryotic SOD contains Cu or Zn and its prokaryotic counterpart contains Fe or Mn.\n\nZn (II) doesn’t pose an oxidation threat to the cytoplasm. This allowed it to become a major cytoplasmic element in the eukaryotes. It became associated with a new group of transcription proteins, zinc fingers. This could only have occurred due to the long life of eukaryotes, which allowed time for zinc to exchange and hence become an internal messenger coordinating the action of other transcription factors during growth.\n\nMolybdenum (Mo) is the most abundant transition element in solution in the sea (mostly as dianionic molybdate ion) and in living organisms, its abundance in the Earth’s crust is quite low. Therefore, the use of Mo by living organisms seems surprising at first glance. Archaea, bacteria, fungi, plants, and animals, including humans, require molybdenum. It is also found in over 50 different enzymes. Its hydrolysis to water-soluble oxo-anionic species makes Mo readily accessible. Mo is found in the active sites of metalloenzymes that perform key transformations in the metabolism of carbon, nitrogen, arsenic, selenium, sulfur, and chlorine compounds.\nThe mononuclear Mo enzymes are widely distributed in the biosphere; they catalyze many significant reactions in the metabolism of nitrogen and sulfur-containing compounds as well as various carbonyl compounds (e.g., aldehydes, CO, and CO). Nitrate reductases enzymes are important for the nitrogen cycle. They belong to a class of enzymes with a mononuclear Mo center and they catalyze the metabolism reaction of C, N, S, etc., in bacteria, plants, animals, and humans. Due to the oxidation of sulfides, The first considerable development was that of aerobic bacteria which could now utilize Mo. As oxygen began to accumulate in the atmosphere and oceans, the reaction of MoS to MoO also increased. This reaction made the highly soluble molybdate ion available for incorporation into critical metalloenzymes, and may have thus allowed life to thrive. It allowed organisms to occupy new ecological niches. Mo plays an important role in the reduction of dinitrogen to ammonia, which occurs in one type of nitrogenases. These enzymes are used by bacteria that usually live in a symbiotic relationship with plants; their role is nitrogen fixation, which is vital for sustaining life on earth. Mo enzymes also play important roles in sulfur metabolism of organisms ranging from bacteria to humans.\n\nTungsten is one of the oldest metal ions to be incorporated in biological systems, preceding The Great Oxygenation Event. Before the abundance of oxygen in Earth's atmosphere, oceans teemed with sulfur and tungsten, while molybdenum, a metal that is highly similar chemically, was inaccessible in solid form. The abundance of tungsten and lack of free molybdenum likely explains why early marine organisms incorporated the former instead of the latter. However, as cyanobacteria began to fill the atmosphere with oxygen, molybdenum became available (molybdenum becomes soluble when exposed to oxygen) and molybdenum began to replace tungsten in the majority of metabolic processes, which is seen today, as tungsten is only present in the biological complexes of prokaryotes (methanogens, gram-positive bacteria, gram-negative aerobes and anaerobes), and is only obligated in hyperthermophilic archaea such as P. furiosus. Tungesten's extremely high melting point (3,422C), partially explains its necessity in these archaea, found in extremely hot areas.\n\nAlthough research into the specific enzyme complexes in which tungsten is incorporated is relatively recent (1970s), natural tungstoenzymes are abundantly found in a large number of prokaryotic microorganisms. These include formate dehydrogenase, formyl methanufuran dehydrogenase, acetylene hydratase, and a class of phylogenetically related oxidoreductases that catalyze the reversible oxidation of aldehydes. The first crystal structure of a tungsten- or pterin-containing enzyme, that of aldehyde ferredoxin oxidoreductase from P. furiosus, has revealed a catalytic site with one W atom coordinated to two pterin molecules which are themselves bridged by a magnesium ion.\n", "id": "46219227", "title": "Evolution of metal ions in biological systems"}
{"url": "https://en.wikipedia.org/wiki?curid=46336709", "text": "Precambrian body plans\n\nUntil the late 1950s, the Precambrian era was not believed to have hosted multicellular organisms. However, with radiometric dating techniques, it has been found that fossils initially found in the Ediacara Hills in Southern Australia date back to the late Precambrian era. These fossils are body impressions of organisms shaped like disks, fronds and some with ribbon patterns that were most likely tentacles.\n\nThese are the earliest multicellular organisms in Earth’s history, despite the fact that unicellularity had been around for a long time before that. The requirements for multicellularity were embedded in the genes of some of these cells, specifically choanoflagellates. These are thought to be the precursors for all multicellular organisms. They are highly related to sponges (Porifera), which are the simplest multicellular organisms.\n\nIn order to understand the transition to multicellularity during the Precambrian, it is important to look at the requirements for multicellularity—both biological and environmental.\n\nThe Precambrian era dates from the beginning of Earth’s formation (4.6 billion years ago) to the beginning of the Cambrian era, 542 million years ago. The Precambrian consists of the Hadean, Archaean and Proterozoic Eons. Specifically, this article will examine the Ediacaran, when the first multicellular bodies are believed to have arisen, as well as what caused the rise of muticellularity. This time period arose after the Snowball Earth of the mid Neoproterozoic. The \"Snowball Earth\" was a period of worldwide glaciation, which is believed to have served as a population bottleneck for the subsequent evolution of multicellular organisms.\n\nThe earth formed around 4.6 billion years ago, with unicellular life emerging somewhat later after the cessation of the Late Heavy Bombardment, a period of intense asteroid impacts possibly caused by migration of the gas giant planets to their current orbits, however multicellularity and bodies are a relatively recent event in Earth’s history. Bodies first started appearing towards the end of the Precambrian Era, during the Ediacaran period. The fossils of the Ediacaran period were first found in Southern Australia in the Ediacara Hills, hence the name. However these fossils were initially thought to be part of the Cambrian and it wasn’t until the late 1950s when Martin Glaessner identified the fossils as actually being from the Precambrian era. The fossils that were found date to about 600 million years ago and are found in a variety of morphologies.\n\n\"For more information, see Ediacaran biota.\n\nThe fossils found that date back to the Precambrian era lack distinct structures since there were no skeletal forms during this period. Skeletons did not arise until the Cambrian Era when oxygen levels increased. This is because skeletons require collagen, which uses Vitamin C as a cofactor, which requires oxygen. For more information on the rise of oxygen see the section on oxygen. The majority of fossils from this Era come from either Mistaken Point on the East Coast of Canada or the Ediacara Hills in Southern Australia.\nMost of the fossils are found as impressions of soft-bodied organisms in the shape of disks, ribbons or fronds. There are also trace fossils that provide evidence that some of these Precambrian organisms were most-likely worm-like creatures that were locomotive. Most of these fossils lack any recognizable heads, mouths or digestive organs, and are thought to have fed via absorptive mechanisms and symbiotic relationships with chemoautotrophs (Chemotroph), photoautotrophs (Phototroph) or osmoautotrophs. The ribbon-like fossils resemble tentacled organisms, and are thought to have fed by capturing prey. The frondose fossils resemble sea pens and other cnidarians. The trace fossils suggest that there were annelid type creatures, and the disk fossils resemble sponges. Despite these similarities, much of the identification is speculation since the fossils do not show very distinct structures. Other fossils do not resemble any known lineages.\n\nMany of the organisms, such as \"Charnia\", found in Mistaken Point, were not like any organisms seen today. They had distinct bodies, however were lacking a head and digestive regions. Rather their body was organized in a very simple, fractal-like branching pattern. Every element of the body was finely branched and grew by repetitive branching. This allowed the organism to have a large surface area and maximize nutrient absorption without needing a mouth and digestive system. However, there was minimal genetic information and therefore did not have the requirements that would have allowed them to evolve more efficient feeding techniques. This means they were probably outcompeted by other organisms, and thus became extinct.\n\nThe organisms found in the Ediacaran Hills in Southern Australia displayed either radially symmetric body plans or, one organism, \"Spriggina,\" displayed the first bilateral symmetry. The Ediacaran Hills are thought to have once had a shallow reef where more light could penetrate the bottom of the ocean floor. This allowed for more diversity of organisms. The organisms found here resemble relatives of the cnidarians, mollusks or annelids.\n\n\"Charnia\" fossils were originally found in the Charnwood Forest in England, hence named \"Charnia\". These fossils are from marine organisms that lived on the bottom of the ocean floor. The fossils have a fractal body plan and were frond shaped, meaning they resembled broad-leafed plants such as ferns. However they could not have been plants since they resided in the dark depths of the ocean floor. In Charnwood Forest, \"Charnia\" was found as an isolated species, however there were many more fossils found on the East Coast of Canada in Mistaken Point in Newfoundland. \"Charnia\" was attached to the bottom of the ocean floor, and was strongly current aligned. This is seen because there are disk-like shapes at the bottom of the \"Charnia\" fossil, which show where \"Charnia\" was tethered, and all the nearby fossils are facing the same direction. These fossils at Mistaken Point were preserved well under volcanic ash and layers of soft mud. It has been determined via radiometric dating of the fossils that \"Charnia\" must have lived around 565 million years ago.\n\n\"Dickinsonia\" fossils are another notable fossil from the Ediacaran period, found in Southern Australia and Russia. It remains unknown what type of organism \"Dickinsonia\" was, however it has been considered a polychaete, turbellarian/annelid worm, jellyfish, polyp, protest, lichen or mushroom. They were preserved in quartz sandstones, and date back to around 550 million years ago. \"Dickinsonia\" were soft-bodied organisms, that show some evidence of very slow movement. There are faint, circular imprints in the rock which follow a path, and then following the same path there is a more definite circular imprint of the same size. This indicates that the organism probably moved slowly from one feeding area to the next and absorbed nutrients. It is speculated that the organism probably had very small appendages that allowed it to move much like starfish do today \n\n\"Spriggina\" fossils represent the first known organisms with a bilaterally symmetric body plan. They had a head, tail and almost identical halves. They probably had sensory organs in the head and digestive organs in the tail which would have allowed them to find food more efficiently. They were capable of locomotion, which gave them an advantage over other organisms from that era that were either tethered to the bottom of the ocean floor or moved very slowly. \"Spriggina\" was soft bodied, which leave the fossils as faint imprints. It is most likely related to annelids, however there is some speculation that it could be related to arthropods since it somewhat resembles trilobite fossils.\n\nThe Ediacaran fossils of Southern Australia contain trace fossils, which indicate that there were motile benthic organisms. The organisms that produced the traces in the sediments were all worm-like sediment feeders or detritus feeders (Detritivore). There are a few trace fossils, which resemble arthropod trails. Evidence suggests that arthropod-like organisms existed during the Precambrian. This evidence is in the type of trails left behind; specifically one specimen that shows six pairs of symmetrically placed impressions, which resemble trilobite walking trails \n\nFor the majority of Earth’s history life has been unicellular. However, unicellular organisms had the ingredients in them for multicellularity to arise. Despite having the ingredients for multicellularity, organisms were restricted due to the lack of hospitable environmental conditions. The rise of oxygen (The Great Oxygenation Event) led organisms to be able to develop more complex body plans. In order for multicellularity to have occurred, organisms must have been capable of cellular communication, aggregation, and specialized functions. The transition to multicellularity that began the evolution of animals from protozoa is one of the most poorly understood of history’s life events. Understanding choanoflagellates and their relation to sponges is important when positing theories on the origins of multicellularity\n\nChoanoflagellates, also called \"collar-flagellates\" are unicellular protists that exist in both freshwaters and oceans. Choanoflagellates have a spherical (or ovoid) cell body and a flagellum that is surrounded by a collar composed of actin microvilli. The flagellum is used to facilitate movement and food intake. As the flagellum beats, it takes in water through the microvilli attached to the collar, which helps filter out unwanted bacteria and other tiny food particles. Choanoflagellates are composed of approximately 150 species and reproduce by simple division.\n\nThe Choanoflagellate \"Salpingoeca rosetta\" is a rare freshwater eukaryote consisting of a number of cells embedded in a jelly-like matrix. This organism demonstrates a very primitive level of cell differentiation and specialization. This is seen with flagellated cells and their collar structures that move the cell colony through the water, while the amoeboid cells on the inside serve to divide into new cells to assist in colony growth.\n<br>Similar low level cellular differentiation and specification can also be seen in sponges. They also have collar cells (also called choanocytes due to their similarities to choanoflaggellates) and amoeboid cells arranged in a gelatinous matrix. Unlike Choanoflagellate Salpingoeca Rosetta, sponges also have other cell-types that can perform different functions (see sponges). Also, the collar cells of sponges beat within canals in the sponge body, whereas Salpingoeca Rosetta’s collar cells reside on the inside and it lacks internal canals. Despite these minor differences, there is strong evidence that Proterospongia and Metazoa are highly related \n\nThese choanoflagellates are able to attach to one another via the pairing of collar microvilli.\n\nThese choanoflagellates are capable of forming colonies via fine intercellular bridges that allow the individual cells to attach. These bridges resemble ring canals that link developing spermatogonia or oogonia in animals.\n\nSponges are some of Earth’s oldest and most ubiquitous animals. The appearance of sponge spicule fossils date back to the Precambrian Era around 580 million years ago. An assemblage of these fossils were found in the Doushanto formation in Southern China. Some circular impressions from the Ediacaran Hills in Southern Australia are also reported to be sponges. They are one of the only lineages of metazoans from this era that continue to survive, and remain relatively unchanged.\nSponges are such successful organisms due to their simple, yet effective morphology. They do not possess mouths or any digestive, nervous or circulatory systems. Instead they are filter feeders, which means that they obtain food through nutrients in the water. They have pores, called ostia, that water travels through to a chamber called the spongocoel, and exits through a chamber called the osculum. Through this water filtration system, they obtain nutrients that are needed for their survival. Specifically, they intracellularly digest bacteria, micro-algae or colloids \n\nSponge skeletons consist of either spongin or calcareous and siliceous spicules with some collagen molecules interspersed. The collagen holds the sponge cells together. Different lineages of sponges are distinguished based on the composition of their skeletons. The three main classes of sponges are Demospongiae, Hexactinellid, and Calcareous.\n\nDemonsponges are the most well-known type of sponge since they are used by humans. They are distinguished by a siliceous skeleton of two and four rayed spicules and contain the protein spongin.\n\nHexactinellid are also called glass sponges, and are distinguished by a six-rayed glass skeleton. These sponges are also capable of carrying out action potentials.\n\nCalcareous sponges are characterized by a calcium carbonate skeleton and comprise less than 5% of sponges.\n\nSponges have around 6 different types of cells that can perform different functions. Sponges are a good model for studying the origin of multicellularity because the cells are capable of communicating with one another and re-aggregating. In an experiment conducted by Henry Van Peters Wilson in 1910, it was found that cells from dissociated sponges could send out signals and recognize each other to form a new individual. This suggests that the cells that compose sponges are capable of independent living, however once multicellularity was possible then aggregating together to form one organism was a more efficient way of living.\n\nThe most notable cell types of sponges are the goblet-shaped cells called choanocytes, so named for their similarity to choanoflagellates. The similarities between these two cells types makes scientists believe that choanoflagellates are the sister taxa to metazoa. The flagella of these cells are what drive the water movement through the sponge body. The cell body of choanocytes is what is responsible for nutrient absorption. In some species these cells can develop into gametes.\n\nThe Pinacocytes are the cells on the exterior of the sponge that line the cell body. They are tightly packed together and very thin.\n\nThe mesenchyme lines the region between the pinacocytes and the choanocytes. They contain a matrix composed of proteins and spicules.\n\nArchaeocytes are special types of cells, in that they can transform into all of the other cell types. They will do what is needed in the sponge body, such as ingest and digest food, transport nutrients to other cells in the sponge body. These cells are also capable of developing into gametes in some sponge species.\n\nThe sclerocytes are responsible for the secretion of spicules. In species of sponges that use spongin instead of calcaerous and silicaceous spicules, the sclerocytes are replaced by spongocytes, which secrete spongin skeletal fibres.\n\nThe myocytes and porocytes are responsible for contraction of the sponge. These contractions are analogous to muscle contractions in other organisms, since sponges do not have muscles. They are responsible for regulating the water flow through the sponge.\n\nThe formation of multicellularity was a pivotal point in the evolution of life on Earth. Shortly after multicellularity arose, there was an immense increase in the diversity of living organisms at the beginning of the Cambrian Era, called the Cambrian Explosion. Multicellularity is believed to have evolved multiple times on Earth because it was a beneficial life strategy for organisms. For multicellularity to occur, cells need to be capable of self-replication, cell-cell adhesion and cell-cell communication. There also must have been available oxygen and selective pressures in the environment.\n\nWork by Fairclough, Dayel and King suggests that S. Rosetta can exist in either single-cellular form or in colonies of 4-50 cells, which arrange themselves in tight knit packs of spheres. This was established by performing an experiment involving the introduction of prey bacterium Algoriphagus species to a sample of uni-celled S. Rosetta organism and monitored the activity for 12 hours. Results of this study demonstrated that cell colonies were formed through cell-division of the initial solitary S. Rosetta cell rather than by cell aggregation. Further studies to support the theory of cell-proliferation were done by introducing then removing the drug aphidicolin which serves to block cell-division. When the drug was introduced, cell division stopped and colony formation resulted through cell-cell aggregation. When the drug was removed, cell-division dominated once again \n\nBy looking at the genome of the Choanoflagellate, \"Monosiga brevicollis\", scientists have inferred that choanoflagellates play a key role in the development of multicellularity. Nicole King has done work looking at the genome of \"Monisiga brevicollis\", and has found key protein domains that are shared between metazoans and choanoflagellates. These domains play a role in cell signalling and adhesion processes in metazoans. The finding that choanoflagellates also have these genes is an incredible discovery because it was previously thought that only metazoans had genes responsible for cell-cell communication and aggregation. This suggests that these domains play a key role in the origins of multicellularity since it ties a unicellular organism (choanoflagellates) to multicellular organisms (metazoans). It shows that the components required for multicellularity were present in the common ancestor between metazoans and choanoflagellates.\n\nNeither sponges nor the placozoan Trichoplax adhaerens appear to be equipped with neuron synapses, however they both possess several factors related to the same synaptic function. Therefore, it is likely that central features involved in synaptic transmission arose early in metazoan evolution, most likely around the time that much of the life on Earth was transitioning to multicellularity. It was found that the Munc18/syntaxin 1 complex could be an important component for the production of the SNARE protein. The secretion of SNARE protein from synaptic vesicles is believed to be critical for neuronal communication. The Munc18/syntaxin 1 complex found in M. brevicollis is both structurally and functionally similar to the metazoan complex. This suggests that it constitutes an important step in the reaction pathway toward SNARE assembly. It is believed that the common ancestor of choanoflagellates and metazoans used this primordial secretion machinery as a precursor to synaptic communication. This mechanism would eventually be used for cell-cell communication in animals \n\nDespite the fact that prokaryotic cells contained the building blocks required for multicellularity to arise, this transition did not occur for around 1500 million years after the origins of the first eukaryotic cell. Scientists have proposed two major theories for the reason that multicellularity arose so late after the appearance of life on Earth.\n\nThis theory postulates that multicellularity arose as a means for prey to escape predation. Larger prey are less likely to be preyed upon, and larger predators are more likely to catch prey. Therefore it is likely that multicellularity arose when the first predators evolved. By assembling as a larger, multicelled organism, prey could escape the attempts of a predator. Therefore multicellularity was selectively favoured over unicellularity. This can be seen in a simple experiment conducted by Boraas et al. (1998). When a predatory protist, \"Ochromonas valencia\", was introduced to a prey population of \"Chlorella vulgaris\", it was seen that within less than 100 generations of the prey species a multicellular growth form of the alga became dominant. This is interesting because before the predator was introduced, the population of \"Chlorella vulgaris\" retained its unicellular growth form for thousands of generations. It is likely that it would have remained unicellular indefinitely if the selective pressure that was induced by the predators had not been introduced. After multiple generations with the predator, the algal species retained a growth form of 8-10 cells, which was large enough to avoid the predator, but small enough that each cell still had access to nutrients. This predator-prey relationship provides a likely reason for why it was beneficial for organisms to be multicellular.\n\nDespite the fact that organisms had the potential to become multicellular it is likely that it was not actually possible until the late Neoproterozoic. This is because multicellularity requires oxygen, and before the late Neoproterozoic there was very limited oxygen availability. After the melting of the “Snowball Earth” during the mid Neoproterozoic, nutrients that were trapped in the ice flooded the oceans. Surviving bacteria flourished due to the increased nutrient levels. Among these microbes were cyanobacteria and other oxygen producing bacteria, which led to the massive rise in oxygen levels. The increased oxygen availability allowed it to be used by cells in order to manufacture collagen. Collagen is the key component for cell aggregation, It is a rope-like molecule that “ties” cells together. Oxygen is required for collagen synthesis because ascorbic acid (Vitamin C) is essential for this process to occur. A key component in the ascorbic acid molecule is oxygen (chemical formula C6H8O6). Therefore, it is evident that the rise in oxygen is a crucial step to the rise of multicellularity since it is essential for the synthesis of collagen.\n\nCollagen is the most abundant protein in mammals and is an essential molecule in the formation of bones, skin and other connective tissue. Different types of collagen have been found in all multicellular organisms, including sponges.\n\nIt has been found that sponges do have a gene sequence coding for collagen type IV which is a diagnostic feature of the basal lamina\n\nIt has also been found that 29 types of collagen have been found to exist in humans. This vast group can further be divided into several families according to their primary structures and supramolecular organization. Among the many types of collagens, only the fibrillar and the basement membrane (type IV) collagens have been found in the sponges and cnidarians, which are the two earliest branching metazoan lineages. Studies have focused on the origin of fibrillar collagen molecules. In Sponges, there exist three clades of fibrillar molecules, A, B and C. It is proposed that only the B clade fibrillar collagens preserved their characteristic modular structure from sponge to human.\n\nIn mammals, the fibrillar collagens involved in the formation of cross-striated fibrils are types I–III, V, and XI. Type II and type XI collagens compose the fibrils present in cartilage. These can be distinguished from collagens located in non-cartilaginous tissues, which include type I, III, and V collagens \n\nAdditional research on sponge proteins found that of 42 sponge proteins that were analysed, all of them had homologous proteins that are found in humans. An identity score of 53% was given to the similarity among sponge and human proteins, compared to a score of 42% when the same sequence was compared to that of C. elegans.\n", "id": "46336709", "title": "Precambrian body plans"}
{"url": "https://en.wikipedia.org/wiki?curid=46236492", "text": "Fluctuating selection\n\nFluctuating selection is a mode of natural selection characterized by the fluctuation of the direction of selection on a given phenotype over a relatively brief period of evolutionary time. For example, a species of plant may come in two varieties: one which prefers wetter soil and one which prefers dryer soil. During a period of wet years, the wet variety will be more fit and produce more offspring, and thereby increase the frequency of wet-preferring plants. If this wet period is followed by drought, the dry variety will be selected for and its numbers will increase. As periods of dryness and wetness fluctuate, so too does selection on dry-preferring and wet-preferring plants. Fluctuating selection is also manifest at the genic level. Consider two alleles, A and B, which are found at the same locus. Fluctuating selection dynamics are at play when selection favors A at time t, B at t and A again at t.\n\nFluctuating selection has been characterized by several mathematical models.\nSuch models have led to the identification of fluctuating selection dynamics in evolving populations from a broad range of taxa, including bacteria, eukaryotes and viruses.\n\nThe Red Queen hypothesis describes coevolutionary 'arms races' between antagonistic species (predators and prey, parasites and hosts, competitors with overlapping niches), emphasizing competition between species and populations rather than within them. Under Red Queen dynamics, a species must adapt to shifting selection pressures of the ever-changing biota which constitute its environment or face extinction. Experiments in Red Queen environments on real and simulated populations have offered strong support for the maintenance of sexual reproduction despite the two-fold cost of sex.\n\nFluctuating selection may also play an important role in host-parasite coevolutionary relationships, specifically in the maintenance of sex. It has been shown that coevolutionary arms race dynamics between host and parasite give way to fluctuating selection dynamics in a minimal environment. Fluctuating selection in Red Queen environments has been suggested as an explanation for the persistence of sex:\n\nThe essence of sex in our theory is that it stores genes that are currently bad but have promise for reuse. It continually tries them in combination, waiting for the time when the focus of disadvantage has moved elsewhere. When this has happened, the genotypes carrying such genes spread by successful reproduction, becoming simultaneously stores for other bad genes and thus onward in continuous succession.\n\nIn this conception of sex, the population is a storehouse of variation and sex is a mechanism for distributing old, minority variants once they become useful. This theory depends on fluctuating selection, as fluctuating selection dynamics make adaptive previously maladaptive variants due to ecological shifts.\n\n", "id": "46236492", "title": "Fluctuating selection"}
{"url": "https://en.wikipedia.org/wiki?curid=15441422", "text": "Hybrid iguana\n\nThe hybrid iguana is a first generation hybrid, the result of intergeneric breeding between a male marine iguana (genus \"Amblyrhynchus\") and a female land iguana (genus \"Conolophus\") on South Plaza Island in the Galapagos Islands where the territories of the two species overlap.\n\nThey are dark with white speckles or bands of mottling near the head. (Marine iguanas are solid black, while land iguanas are reddish-yellow.)\n\nThe first hybrid iguana was discovered in 1981. In 1997, high ocean temperatures during a severe El Niño season caused failure of the seaweed beds around the Galapagos Islands and about half the marine iguanas starved to death. Others searched inland for plants to eat. They also mated with the land iguanas, which produced an unusual number of hybrid iguanas. As of 2003, twenty had been found. DNA testing by a German researcher revealed that marine iguanas were the fathers and land iguanas the mothers.\n\nMarine iguanas have sharp claws and are able to grip rock under seawater and eat seaweed, whereas land iguanas lack sharp claws making them unable to climb on the cacti which are their staple foods. Due to decreased quantities of seaweed, marine iguanas will often seek food on the land, taking over the food source from the colonies of land iguanas.Hybrid iguanas have sharp claws and can climb on cacti and also eat seaweed underwater. It is held that the hybrid iguana can survive in both sea and land environments. Despite their long separation time and there being two distinct species from different genera, the offspring are viable, although likely sterile. The hybrid iguanas have a laterally compressed tail like that of the marine iguanas, but they have not been seen swimming. They also have sharp claws like their marine fathers, which enable them to climb for food rather than waiting for it to drop from a cactus as the land iguanas do.\n\n", "id": "15441422", "title": "Hybrid iguana"}
{"url": "https://en.wikipedia.org/wiki?curid=344195", "text": "Living fossil\n\nA living fossil is an extant taxon that closely resembles organisms otherwise known only from the fossil record. As a rule, to be considered a living fossil, the fossil species must be old relative to the time of origin of the extant clade. Living fossils commonly are species-poor lineages, but they need not be. The term \"living fossil\" is not formally defined, but in scientific literature the term usually connotes a bradytelic group. \"Bradytely\", however, is rarely used in modern scientific literature but the characteristic of a bradytelic group is that its changes fluctuate on a small scale and do not accumulate over time. In modern literature, the term most often used for that distinctive evolutionary tempo is \"stasis\". Living fossils exhibit stasis over geologically long time scales.\n\nIn popular literature \"living fossil\" commonly embodies radical misunderstandings such as that the organism somehow has undergone no significant evolution since fossil times, with practically no molecular evolution or morphological, but scientific investigations have repeatedly discredited any such claims about molecular evolution. An important misunderstanding underlying the idea that living fossils do not evolve is that stabilizing selection is an evolutionary process, perhaps even the dominant one in morphological evolution.\n\nLiving fossils have two main characteristics, although some have a third. The first two are required for recognition as a living fossil stasis but some authors include the third. They: \n\nSuch criteria are neither well-defined nor clearly quantifiable, but modern methods for analyzing evolutionary dynamics can document the distinctive tempo of stasis. Lineages that exhibit stasis over very short time scales are not considered living fossils; what is poorly-defined is the time scale over which the morphology must persist for that lineage to be recognized as a living fossil.\n\nThe term \"living fossil\" is much misunderstood in popular media in particular, in which it often is used meaninglessly. In professional literature the expression seldom appears and must be used with far more caution and it has been used inconsistently \n\nOne example of a concept that could be confused with \"living fossil\", is that of a \"Lazarus taxon\", but the two are not equivalent; a Lazarus taxon (whether a single species or a group of related species) is one that suddenly reappears, either in the fossil record or in nature, as if the fossil had \"come to life again\". In contrast to Lazarus taxa, a living fossil in most senses is a species or lineage that has undergone exceptionally little change throughout a long fossil record, giving the impression that the extant taxon had remained identical through the entire fossil and modern period.\n\nThe average species turnover time, meaning the time between when a species first is established and when it finally disappears, varies widely among phyla, but averages about 2–3 million years. So a living taxon that had long been thought to be extinct could be called a Lazarus taxon once it was discovered to be still extant. A dramatic example was the order Coelacanthiformes, of which the genus, \"Latimeria\" was found to be extant in 1938. About that there is little debate. However, whether \"Latimeria\" resembles early members of its lineage sufficiently closely to be considered a living fossil as well as a Lazarus taxon, has been denied by some authors in recent years.\n\nCoelacanths disappeared from the fossil record some 80 million years ago (upper Cretaceous) and to the extent that they exhibit low rates of morphological evolution, extant species qualify as living fossils. It must be emphasised that this criterion reflects fossil evidence, and is totally independent of whether the taxons had been subject to selection at all, which all living populations continuously are, whether they remain genetically unchanged or not. This in turn gives rise to a great deal of confusion; for one thing, the fossil record seldom preserves much more than the general morphology of a specimen; to determine much about its physiology is seldom possible. To determine much about its noncoding DNA is hardly ever possible, but even if a species were hypothetically unchanged in its physiology, it is to be expected from the very nature of the reproductive processes, that its non-functional genomic changes would continue at more or less standard rates. It follows that a fossil lineage with apparently constant morphology need not imply equally constant physiology for example, and certainly neither implies any cessation of the basic evolutionary processes such as natural selection, nor reduction in the usual rate of change of the noncoding DNA. In short, not even the most dramatic examples of living fossils can be expected to be without changes, no matter how persistently constant their fossils and their extant specimens might seem.\n\nSome living fossils are taxa that were known from palaeontological fossils before living representatives were discovered. The most famous examples of this are:\n\nAll of these were described from fossils before later found alive (2 species, 10 species, one species, and one species respectively).\n\nOther examples of living fossils are single living species that have no close living relatives, but are survivors of large and widespread groups in the fossil record. Consider:\n\nThe fact that a living fossil is a surviving representative of an archaic lineage does not imply that it must retain all the \"primitive\" features (plesiomorphies) of its ancestral lineage. Although it is common to say that living fossils exhibit \"morphological stasis\", stasis, in the scientific literature, does not mean that any species is strictly identical to its ancestor, much less remote ancestors.\n\nSome living fossils are relicts of formerly diverse and morphologically varied lineages, but not all survivors of ancient lineages necessarily are regarded as living fossils. See for example the uniquely and highly autapomorphic oxpeckers, which appear to be the only survivors of an ancient lineage related to starlings and mockingbirds.\n\nThe term \"living fossil\" is usually reserved for species or larger clades that are exceptional for their lack of morphological diversity and their exceptional conservatism, and several hypotheses could explain morphological stasis on a geologically long time-scale. Early analyses of evolutionary rates emphasized the persistence of a taxon rather than rates of evolutionary change. Contemporary studies instead analyze rates and modes of phenotypic evolution, but most have focused on clades that are thought to be adaptive radiations rather than on those thought to be living fossils. Thus, very little is presently known about the evolutionary mechanisms that produce living fossils or how common they might be. Some recent studies have documented exceptionally low rates of ecological and phenotypic evolution despite rapid speciation. This has been termed a \"non-adaptive radiation\" referring to diversification not accompanied by adaptation into various significantly different niches. Such radiations are explanation for groups that are morphologically conservative. Persistent adaptation within an adaptive zone is a common explanation for morphological stasis. The subject of very low evolutionary rates, however, has received much less attention in the recent literature than that of high rates\n\nLiving fossils are not expected to exhibit exceptionally low rates of molecular evolution, and some studies have shown that they do not. For example, on tadpole shrimp (\"Triops\"), one article notes, \"Our work shows that organisms with conservative body plans are constantly radiating, and presumably, adapting to novel conditions... I would favor retiring the term ‘living fossil’ altogether, as it is generally misleading.\"\n\nThe question posed by several recent studies pointed out that the morphological conservatism of coelacanths is not supported by paleontological data. In addition, it was shown recently that studies concluding that a slow rate of molecular evolution is linked to morphological conservatism in coelacanths are biased by the \"a priori\" hypothesis that these species are ‘living fossils’. Accordingly, the genome stasis hypothesis is challenged by the recent finding that the genome of the two extant coelacanth species \"L. chalumnae\" and \"L. menadoensis\" contain multiple species-specific insertions, indicating transposable element recent activity and contribution to post-speciation genome divergence. Such studies, however, challenge only a genome stasis hypothesis, not the hypothesis of exceptionally low rates of phenotypic evolution.\n\nThe term was coined by Charles Darwin in his \"On the Origin of Species\" from 1859, when discussing \"Ornithorhynchus\" (the platypus) and \"Lepidosiren\" (the South American lungfish):\n\nA living taxon that lived through a large portion of geologic time.\n\nQueensland lungfish (\"Neoceratodus fosteri\") is an example of an organism that meets this criterion. Fossils identical to modern Queensland lungfish have been dated at over 100 million years making this species one of the oldest if not actually the oldest extant vertebrate species.\n\nA living taxon morphologically and/or physiologically resembling a fossil taxon through a large portion of geologic time (morphological stasis).\n\nA living taxon with many characteristics believed to be primitive.\n\nThis is a more neutral definition. However, it does not make it clear whether the taxon is truly old, or it simply has many plesiomorphies. Note that, as mentioned above, the converse may hold for true living fossil taxa; that is, they may possess a great many derived features (autapomorphies), and not be particularly \"primitive\" in appearance.\n\nAny one of the above three definitions, but also with a relict distribution in refuges.\n\nSome paleontologists believe that living fossils with large distributions (such as \"Triops cancriformis\") are not real living fossils. In the case of \"Triops cancriformis\" (living from the Triassic until now), the Triassic specimens lost most of their appendages (mostly only carapaces remain), and they have not been thoroughly examined since 1938.\n\nAny of the first three definitions, but the clade also has a low taxonomic diversity (low diversity lineages).\n\nOxpeckers are morphologically somewhat similar to starlings due to shared plesiomorphies, but are uniquely adapted to feed on parasites and blood of large land mammals, which has always obscured their relationships. This lineage forms part of a radiation that includes Sturnidae and Mimidae, but appears to be the most ancient of these groups. Biogeography strongly suggests that oxpeckers originated in eastern Asia and only later arrived in Africa, where they now have a relict distribution.\n\nThe two living species thus seem to represent an entirely extinct and (as Passerida go) rather ancient lineage, as certainly as this can be said in the absence of actual fossils. The latter is probably due to the fact that the oxpecker lineage never occurred in areas where conditions were good for fossilization of small bird bones, but of course, fossils of ancestral oxpeckers may one day turn up enabling this theory to be tested.\n\n- Proposed in 2017: lineage has a slow rate of evolution and occurs close to the centroid of the lineage's morphospace.\n\nSome of these are informally known as \"living fossils\".\n\n\n\n\n\n \n\n\n\n", "id": "344195", "title": "Living fossil"}
{"url": "https://en.wikipedia.org/wiki?curid=47151085", "text": "Contest competition\n\nIn ecology, contest competition refers to a situation where available resources, such as food and mates, are utilized only by one or a few individuals, thus preventing development or reproduction of other individuals. It refers to a hypothetical situation in which several individuals stage a contest for which one eventually emerges victorious. Contest competition has been demonstrated in controlled laboratory experiments among parasitic wasps. Contest competition is the opposite of scramble competition, a situation in which available resources are shared equally among individuals.\n\nAs contest competition allows the monopolization of resources, offspring will typically always be produced and survive until adulthood independent of the population size. This results in stable population dynamics, in stark contrast to scramble competition which can result in periodic or chaotic population dynamics. The Beverton-Holt model is often used to represent population dynamics arising from contest competition. This model, and a few other well-known population models, can be explicitly derived from individual-level processes assuming contest competition and a random distribution of individuals among resources.\n\n", "id": "47151085", "title": "Contest competition"}
{"url": "https://en.wikipedia.org/wiki?curid=20650", "text": "Macroevolution\n\nMacroevolution is evolution on a scale at or above the level of species, in contrast with microevolution, which refers to smaller evolutionary changes of allele frequencies within a species or population. Macroevolution and microevolution describe fundamentally identical processes on different time scales. \n\nThe process of speciation may fall within the purview of either, depending on the forces thought to drive it. Paleontology, evolutionary developmental biology, comparative genomics and genomic phylostratigraphy contribute most of the evidence for macroevolution's patterns and processes.\n\nRussian entomologist Yuri Filipchenko first coined the terms \"macroevolution\" and \"microevolution\" in 1927 in his German language work, \"Variabilität und Variation\". Since the inception of the two terms, their meanings have been revised several times. The term macroevolution fell into a certain disfavour when it was taken over by writers such as the paleontologist Otto Schindewolf to describe their theories of orthogenesis. This was the vitalist belief that organisms evolve in a definite direction due to an internal \"driving force\".\n\nMacroevolution includes changes occurring on geological time scales, in contrast to microevolution, which occurs on any time scale.\n\nWithin the modern synthesis of the early 20th century, macroevolution is thought of as the compounded effects of microevolution. Thus, the distinction between micro- and macroevolution is not a fundamental one – the only difference between them is of time and scale. As Ernst W. Mayr observes, \"transspecific evolution is nothing but an extrapolation and magnification of the events that take place within populations and species...it is misleading to make a distinction between the causes of micro- and macroevolution\". However, time is not a necessary distinguishing factormacroevolution can happen without gradual compounding of small changes; whole-genome duplication can result in speciation occurring over a single generation – this is especially common in plants.\n\nChanges in the genes regulating development have also been proposed as being important in producing speciation through large and relatively sudden changes in animals' morphology.\n\nThere are many ways to view macroevolution, for example, by observing changes in the genetics, morphology, taxonomy, ecology, and behavior of organisms, though these are interrelated. Sahney et al. stated the connection as \"As taxonomic diversity has increased, there have been incentives for tetrapods to move into new modes of life, where initially resources may seem unlimited, there are few competitors and possible refuge from danger. And as ecological diversity increases, taxa diversify from their ancestors at a much greater rate among faunas with more superior, innovative or more flexible adaptations.\"\n\nMolecular evolution occurs through small changes in the molecular or cellular level. Over a long period of time, this can cause big effects on the genetics of organisms. Taxonomic evolution occurs through small changes between populations and then species. Over a long period of time, this can cause big effects on the taxonomy of organisms, with the growth of whole new clades above the species level. Morphological evolution occurs through small changes in the morphology of an organism. Over a long period of time, this can cause big effects on the morphology of major clades. This can be clearly seen in the Cetacea, where throughout the group's early evolution, hindlimbs were still present. However over millions of years the hindlimbs regressed and became internal.\n\nAbrupt transformations from one biologic system to another, for example the passing of life from water into land or the transition from invertebrates to vertebrates, are rare. Few major biological types have emerged during the evolutionary history of life. When lifeforms take such giant leaps, they meet little to no competition and are able to exploit many available niches, following an adaptive radiation. This can lead to convergent evolution as the empty niches are filled by whichever lifeform encounters them.\n\nSubjects studied within macroevolution include:\n\n\n\n", "id": "20650", "title": "Macroevolution"}
{"url": "https://en.wikipedia.org/wiki?curid=19919849", "text": "Precambrian rabbit\n\n\"Precambrian rabbits\" or \"fossil rabbits in the Precambrian\" are reported to have been among responses given by the biologist J.B.S. Haldane when asked what evidence could destroy his confidence in the theory of evolution and the field of study. The answers became popular imagery in debates about evolution and the scientific field of evolutionary biology in the 1990s. Many of Haldane's statements about his scientific research were popularized in his lifetime.\n\nSome accounts use this response to rebut claims that the theory of evolution is not falsifiable by any empirical evidence. This followed an assertion by philosopher, Karl Popper, who had proposed that \"falsifiability\" is an essential feature of a scientific theory. Popper also expressed doubts about the scientific status of evolutionary theory, although he later concluded that the field of study was genuinely scientific.\n\nRabbits are mammals. From the perspective of the philosophy of science, it is doubtful whether the genuine discovery of mammalian fossils in Precambrian rocks would overthrow the theory of evolution instantly, although, if authentic, such a discovery would indicate serious errors in modern understanding about the evolutionary process. Mammals are a class of animals, whose emergence in the geologic timescale is dated to much later than any found in Precambrian strata. Geological records indicate that although the first true mammals appeared in the Triassic period, modern mammalian orders appeared in the Palaeocene and Eocene epochs of the Palaeogene period. Hundreds of millions of years separate this period from the Precambrian.\n\nSeveral authors have written that J.B.S. Haldane (1892–1964) said that the discovery of a fossil rabbit in Precambrian rocks would be enough to destroy his belief in evolution. However these references date from the 1990s or later. In 1996 Michael J. Benton cited the 1993 edition of Mark Ridley's book \"Evolution\", Richard Dawkins wrote in 2005 that Haldane was responding to a challenge by a \"Popperian zealot\". In 2004 Richa Arora wrote that the story was told by John Maynard Smith (1920–2004) in a television programme. John Maynard Smith attributed the phrase to Haldane in a conversation with Paul Harvey in the early 1970s.\n\nThe philosopher Karl Popper held that any scientific proposition must be \"falsifiable\", in other words it must at least be possible to imagine some reproducible experiment or observation whose outcome would disprove the hypothesis. Initially he thought that Charles Darwin's theory of natural selection (often summarized as \"the survival of the fittest\") was untestable in this sense, and therefore \"almost tautological.\" Popper later changed his view, concluding that the theory of natural selection is falsifiable and that Darwin's own example of the peacock's tail had disproved one extreme variation of it, that \"all\" evolution is driven by natural selection. Although in 1978 Popper wrote that his earlier objection had been specifically to the theory of natural selection, in lectures and articles from 1949 to 1974 he had stated that \"Darwinism\" or \"Darwin's theory of evolution\" was a \"metaphysical research programme\" because it was not falsifiable. In fact he continued to express dissatisfaction with contemporary statements of the theory of evolution which focused on population genetics, the study of the relative frequencies of alleles (different forms of the same gene). Unfortunately some of the adjustments he proposed resembled Lamarckianism or saltationism, evolutionary theories that were and still are considered obsolete, and evolutionary biologists therefore disregarded his criticisms. In 1981 Popper complained that he had been misinterpreted as saying that \"historical sciences\" such as paleontology or the history of evolution of life on Earth were not genuine sciences, when in fact he believed they could make falsifiable predictions.\n\nFurther confusion arose in 1980–1981, when there was a long debate in the pages of \"Nature\" about the scientific status of the theory of evolution. Specifically, the argument was on the factors influencing and nature of the unit of selection in the genome, with one side positing natural selection, and the other, neutral mutation. Neither of the parties seriously doubted that the theory was both scientific and, according to current scientific knowledge, true. Some participants objected to statements that appeared to present the theory of evolution as an absolute dogma, however, rather than as a hypothesis that so far has performed very well, and both sides quoted Popper in support of their positions. Evolution critics such as Phillip E. Johnson took this as an opportunity to declare that the theory of evolution was unscientific.\n\nEvolutionary biologist Richard Dawkins said that the discovery of fossil mammals in Precambrian rocks would \"completely blow evolution out of the water.\" Philosopher Peter Godfrey-Smith doubted that a single set of anachronistic fossils, however, even rabbits in the Precambrian, would disprove the theory of evolution outright. The first question raised by the assertion of such a discovery would be whether the alleged \"Precambrian rabbits\" really were fossilized rabbits. Alternative interpretations might include incorrect identification of the \"fossils\", incorrect dating of the rocks, and a hoax such as the Piltdown Man was shown to be. Even if the \"Precambrian rabbits\" turned out to be genuine, they would not instantly refute the theory of evolution, because that theory is a large package of ideas, including: that life on Earth has evolved over billions of years; that this evolution is driven by certain mechanisms; and that these mechanisms have produced a specific \"family tree\" that defines the relationships among species and the order in which they appeared. Hence, \"Precambrian rabbits\" would prove that there were one or more serious errors somewhere in this package, and the next task would be to identify those errors.\n\nBenton pointed out that, in the short term, scientists often have to accept the existence of competing hypotheses, each of which explains large parts—but not all—of the observed relevant data.\n\nGenuine fossils of earliest rabbits are from the Eocene Epoch, about 56 to 33.9 million years ago. Members of the genus \"Gomphos\" are established to be the phylogenetic root of lagomorph rabbits and hares. To date, the oldest \"Gomphos\" is \"G. elkema\" discovered in 2008 from Gujarat, India. The fossil is dated to 53 million years old.\n\n", "id": "19919849", "title": "Precambrian rabbit"}
{"url": "https://en.wikipedia.org/wiki?curid=37894792", "text": "Weak Selection\n\nWeak selection in evolutionary biology is when individuals with different phenotypes possess similar fitness, i.e. one phenotype is weakly preferred over the other. Weak selection, therefore, is an evolutionary theory to explain the maintenance of multiple phenotypes in a stable population [1].\n\nWeak selection can only be used to explain the maintenance of mutations in a Moran process [1]. A Moran process in one which birth and death are paired events, and therefore population size remains constant. If the population size was increasing, both wild type and mutant phenotypes can proliferate and the weak selection for one phenotype results in no particular selection for either. Hence weak selection requires a finite population to operate. Otherwise there would be no expectation of fixation and hence no selection.\n\nThe result of weak selection is two phenotypes with similar fixation probabilities. Weak selection works to elongate fixation time for two competing alleles. Consequently, weak selection provides a model for describing how evolution can occur in large steps in a population in which multiple alleles are maintained [1].\n\nThere are two basic reasons that two phenotypes could have very similar fitness. One reason could be that the phenotypic differences between wild type and mutant are large but the significance of the mutation is minor. An example could be a change in pigmentation. Another reason could be that the phenotypic differences between wild type and mutant are actually small, such as tail length variation. In either case, the significance of the mutation, which is determined by the environment creating the selective pressure, is low in comparison to other mutations. Hence, almost near neutral mutations result in phenotypes that are weakly selected [1].\n\nWeak selection creates a situation in which the evolutionary dynamics governing the phenotype frequencies in a population are mainly driven by random fluctuations. Hence weak selection increases the impact of stochastic processes on the evolutionary dynamics of the trait being weakly selected. For example, genetic drift could cause a nearly neutral mutation to become the dominant allele in a population by wiping out the wild type. Weak selection is therefore also especially sensitive to the effects of population size. In smaller populations, a weakly selected mutation could proliferate due to stochastic processes such as genetic drift even more easily [2].\n\nEmpirically, nonsynonymous substitutions have been reported to proliferate through weak selection in Drosophila melanogaster and Arabidopsis. These non-neutral mutations are thought to have special significance evolutionarily when they affect gene regulatory elements. This is because differential gene expression is critical development and therefore can potentially affect the morphology of an organism. Furthermore, weak selection operates in codon-usage bias resulting in differential levels of gene expression by altering the rate of transcription in mutants with non-preferred codons. Hence, even so called “silent” mutations can result in slight variations in the fitness of an organism. Additionally, gene duplication offers another way in which an apparently nonfunctional mutation can be maintained through weak selection. Differential expression of duplicate gene copies provides a mechanism through which a protein can evolve new functions. [3]\n\n1. Wild, G., & Traulsen, A. (July 21, 2007). The different limits of weak selection and the evolutionary dynamics of finite populations. Journal of Theoretical Biology, 247, 2, 382-390.\n\n2. Kimura, M. (February 17, 1968). Evolutionary Rate at the Molecular Level.Nature, 217, 5129, 624-626.\n\n3. Ohta, T. (December 10, 2002). Near-Neutrality in Evolution of Genes and Gene Regulation. Proceedings of the National Academy of Sciences of the United States of America, 99, 25, 16134-16137.\n", "id": "37894792", "title": "Weak Selection"}
{"url": "https://en.wikipedia.org/wiki?curid=47344746", "text": "Zero-Force Evolutionary Law\n\nThe Zero-Force Evolutionary Law, or ZFEL, is a theory proposed by Robert Brandon and Dan McShea regarding the evolution of diversity and complexity. Brandon and McShea define diversity and complexity in terms of variation, diversity being variation between organisms and complexity being variation among parts within an organism. McShea had previously defined a part as \"a system that is both integrated internally and isolated from its surround,\" giving the digestive tract, epidermis, and skeleton, as examples. As an analogue to the theory of relativity, the theory has a special and general formulation. The special formulation states \"in an evolutionary system in which there is variation and heredity, in the absence of natural selection, other forces, or constraints on diversity or complexity, diversity and complexity will increase on average\" while the general formulation states that \"in an evolutionary system in which there is variation and heredity, there is a tendency for diversity and complexity to increase, one that is always present but may be opposed or augmented by natural selection, other forces, or constraints acting on diversity or complexity\". The rationale for the claim is that as replicators such as genes replicate, errors will accumulate. If not eliminated by negative selection, these variations will lead to greater diversity and complexity.\n\n", "id": "47344746", "title": "Zero-Force Evolutionary Law"}
{"url": "https://en.wikipedia.org/wiki?curid=15732918", "text": "The eclipse of Darwinism\n\nJulian Huxley used the phrase \"the eclipse of Darwinism\" to describe the state of affairs prior to what he called the modern synthesis, when evolution was widely accepted in scientific circles but relatively few biologists believed that natural selection was its primary mechanism. Historians of science such as Peter J. Bowler have used the same phrase as a label for the period within the history of evolutionary thought from the 1880s to around 1920, when alternatives to natural selection were developed and explored—as many biologists considered natural selection to have been a wrong guess on Charles Darwin's part, or at least as of relatively minor importance. An alternative term, the interphase of Darwinism, has been proposed to avoid the largely incorrect implication that the putative eclipse was preceded by a period of vigorous Darwinian research.\n\nWhile there had been multiple explanations of evolution including vitalism, catastrophism, and structuralism through the 19th century, four major alternatives to natural selection were in play at the turn of the 20th century:\n\n\nTheistic evolution largely disappeared from the scientific literature by the end of the 19th century as direct appeals to supernatural causes came to be seen as unscientific. The other alternatives had significant followings well into the 20th century; mainstream biology largely abandoned them only when developments in genetics made them seem increasingly untenable, and when the development of population genetics and the modern synthesis demonstrated the explanatory power of natural selection. Ernst Mayr wrote that as late as 1930 most textbooks still emphasized such non-Darwinian mechanisms.\n\nEvolution was widely accepted in scientific circles within a few years after the publication of \"On the Origin of Species\", but acceptance of natural selection as its driving mechanism was much less. Six objections were raised to the theory in the 19th century:\n\n\nBoth Darwin and his close supporter Thomas Henry Huxley freely admitted, too, that selection might not be the whole explanation; Darwin was prepared to accept a measure of Lamarckism, while Huxley was comfortable with both sudden (mutational) change and directed (orthogenetic) evolution.\n\nBy the end of the 19th century, criticism of natural selection had reached the point that in 1903 the German botanist, , wrote that \"We are now standing at the death bed of Darwinism\", and in 1907 the Stanford University entomologist Vernon Lyman Kellogg, who supported natural selection, asserted that \"... the fair truth is that the Darwinian selection theory, considered with regard to its claimed capacity to be an independently sufficient mechanical explanation of descent, stands today seriously discredited in the biological world.\" He added, however, that there were problems preventing the widespread acceptance of any of the alternatives, as large mutations seemed too uncommon, and there was no experimental evidence of mechanisms that could support either Lamarckism or orthogenesis. Ernst Mayr wrote that a survey of evolutionary literature and biology textbooks showed that as late as 1930 the belief that natural selection was the most important factor in evolution was a minority viewpoint, with only a few population geneticists being strict selectionists.\n\nA variety of different factors motivated people to propose other evolutionary mechanisms as alternatives to natural selection, some of them dating back before Darwin's \"Origin of Species\". Natural selection, with its emphasis on death and competition, did not appeal to some naturalists because they felt it was immoral, and left little room for teleology or the concept of progress in the development of life. Some of these scientists and philosophers, like St. George Jackson Mivart and Charles Lyell, who came to accept evolution but disliked natural selection, raised religious objections. Others, such as Herbert Spencer, the botanist George Henslow (son of Darwin's mentor John Stevens Henslow also a botanist), and Samuel Butler, felt that evolution was an inherently progressive process that natural selection alone was insufficient to explain. Still others, including the American paleontologists Edward Drinker Cope and Alpheus Hyatt, had an idealist perspective and felt that nature, including the development of life, followed orderly patterns that natural selection could not explain.\n\nAnother factor was the rise of a new faction of biologists at the end of the 19th century, typified by the geneticists Hugo DeVries and Thomas Hunt Morgan, who wanted to recast biology as an experimental laboratory science. They distrusted the work of naturalists like Darwin and Alfred Russel Wallace, dependent on field observations of variation, adaptation, and biogeography, considering these overly anecdotal. Instead they focused on topics like physiology, and genetics that could be easily investigated with controlled experiments in the laboratory, and discounted natural selection and the degree to which organisms were adapted to their environment, which could not easily be tested experimentally.\n\nBritish science developed in the early 19th century on a basis of natural theology which saw the adaptation of fixed species as evidence that they had been specially created to a purposeful divine design. The philosophical concepts of German idealism inspired concepts of an ordered plan of harmonious creation, which Richard Owen reconciled with natural theology as a pattern of homology showing evidence of design. Similarly, Louis Agassiz saw the recapitulation theory as symbolising a pattern of the sequence of creations in which humanity was the goal of a divine plan. In 1844 \"Vestiges\" adapted Agassiz's concept into theistic evolutionism. Its anonymous author Robert Chambers proposed a \"law\" of divinely ordered progressive development, with transmutation of species as an extension of recapitulation theory. This popularised the idea, but it was strongly condemned by the scientific establishment. Agassiz remained forcefully opposed to evolution, and after he moved to America in 1846 his idealist argument from design of orderly development became very influential. In 1858 Owen cautiously proposed that this development could be a real expression of a continuing creative law, but distanced himself from transmutationists. Two years later in his review of Darwin's \"On the Origin of Species\" Owen attacked Darwin while at the same time openly supporting evolution, expressing belief in a pattern of transmutation by law-like means. This idealist argument from design was taken up by other naturalists such as George Jackson Mivart, and the Duke of Argyll who rejected natural selection altogether in favor of laws of development that guided evolution down preordained paths.\n\nMany of Darwin's supporters accepted evolution on the basis that it could be reconciled with design. In particular, Asa Gray considered natural selection to be the main mechanism of evolution and sought to reconcile it with natural theology. He proposed that natural selection could be a mechanism in which the problem of evil of suffering produced the greater good of adaptation, but conceded that this had difficulties and suggested that God might influence the variations on which natural selection acted to guide evolution. For Darwin and Thomas Henry Huxley such pervasive supernatural influence was beyond scientific investigation, and George Frederick Wright, an ordained minister who was Gray's colleague in developing theistic evolution, emphasised the need to look for secondary or known causes rather than invoking supernatural explanations: \"If we cease to observe this rule there is an end to all science and all sound science.\"\n\nA secular version of this methodological naturalism was welcomed by a younger generation of scientists who sought to investigate natural causes of organic change, and rejected theistic evolution in science. By 1872 Darwinism in its broader sense of the fact of evolution was accepted as a starting point. Around 1890 only a few older men held onto the idea of design in science, and it had completely disappeared from mainstream scientific discussions by 1900. There was still unease about the implications of natural selection, and those seeking a purpose or direction in evolution turned to neo-Lamarckism or orthogenesis as providing natural explanations.\n\nJean-Baptiste Lamarck had originally proposed a theory on the transmutation of species that was largely based on a progressive drive toward greater complexity. Lamarck also believed, as did many others at the time, that characteristics acquired during the course of an organism's life could be inherited by the next generation, and he saw this as a secondary evolutionary mechanism that produced adaptation to the environment. Typically, such characteristics included changes caused by the use or disuse of a particular organ. It was this mechanism of evolutionary adaptation through the inheritance of acquired characteristics that much later came to be known as Lamarckism. Although Alfred Russel Wallace completely rejected the concept in favor of natural selection, Charles Darwin always included what he called \"Effects of the increased Use and Disuse of Parts, as controlled by Natural Selection\" in \"On the Origin of Species\", giving examples such as large ground feeding birds getting stronger legs through exercise, and weaker wings from not flying until, like the ostrich, they could not fly at all.\n\nIn the late 19th century the term neo-Lamarckism came to be associated with the position of naturalists who viewed the inheritance of acquired characteristics as the most important evolutionary mechanism. Advocates of this position included the British writer and Darwin critic Samuel Butler, the German biologist Ernst Haeckel, the American paleontologists Edward Drinker Cope and Alpheus Hyatt, and the American entomologist Alpheus Packard. They considered Lamarckism to be more progressive and thus philosophically superior to Darwin's idea of natural selection acting on random variation. Butler and Cope both believed that this allowed organisms to effectively drive their own evolution, since organisms that developed new behaviors would change the patterns of use of their organs and thus kick-start the evolutionary process. In addition, Cope and Haeckel both believed that evolution was a progressive process. The idea of linear progress was an important part of Haeckel's recapitulation theory of evolution, which held that the embryological development of an organism repeats its evolutionary history. Cope and Hyatt looked for, and thought they found, patterns of linear progression in the fossil record. Packard argued that the loss of vision in the blind cave insects he studied was best explained through a Lamarckian process of atrophy through disuse combined with inheritance of acquired characteristics. Packard also wrote a book about Lamarck and his writings.\n\nMany American proponents of neo-Lamarckism were strongly influenced by Louis Agassiz and a number of them, including Hyatt and Packard, were his students. Agassiz had an idealistic view of nature, connected with natural theology, that emphasized the importance of order and pattern. Agassiz never accepted evolution; his followers did, but they continued his program of searching for orderly patterns in nature, which they considered to be consistent with divine providence, and preferred evolutionary mechanisms like neo-Lamarckism and orthogenesis that would be likely to produce them.\n\nIn Britain the botanist George Henslow, the son of Darwin's mentor John Stevens Henslow, was an important advocate of neo-Lamarckism. He studied how environmental stress affected the development of plants, and he wrote that the variations induced by such environmental factors could largely explain evolution. The historian of science Peter J. Bowler writes that, as was typical of many 19th century Lamarckians, Henslow did not appear to understand the need to demonstrate that such environmentally induced variations would be inherited by descendants that developed in the absence of the environmental factors that produced them, but merely assumed that they would be.\n\nCritics of neo-Lamarckism pointed out that no one had ever produced solid evidence for the inheritance of acquired characteristics. The experimental work of the German biologist August Weismann resulted in the germ plasm theory of inheritance. This led him to declare that inheritance of acquired characteristics was impossible, since the Weismann barrier would prevent any changes that occurred to the body after birth from being inherited by the next generation. This effectively polarised the argument between the Darwinians and the neo-Lamarckians, as it forced people to choose whether to agree or disagree with Weismann and hence with evolution by natural selection. Despite Weismann's criticism, neo-Lamarckism remained the most popular alternative to natural selection at the end of the 19th century, and would remain the position of some naturalists well into the 20th century.\n\nAs a consequence of the debate over the viability of neo-Lamarckism, in 1896 James Mark Baldwin, Henry Fairfield Osborne and C. Lloyd Morgan all independently proposed a mechanism where new learned behaviors could cause the evolution of new instincts and physical traits through natural selection without resort to the inheritance of acquired characteristics. They proposed that if individuals in a species benefited from learning a particular new behavior, the ability to learn that behavior could be favored by natural selection, and the end result would be the evolution of new instincts and eventually new physical adaptations. This became known as the Baldwin effect and it has remained a topic of debate and research in evolutionary biology ever since.\n\nOrthogenesis was the theory that life has an innate tendency to change, in a unilinear fashion in a particular direction. The term was popularized by Theodor Eimer, a German zoologist, in his 1898 book \"On Orthogenesis: And the Impotence of Natural Selection in Species Formation\". He had studied the coloration of butterflies, and believed he had discovered non-adaptive features which could not be explained by natural selection. Eimer also believed in Lamarckian inheritance of acquired characteristics, but he felt that internal laws of growth determine which characteristics would be acquired and guided the long term direction of evolution down certain paths.\n\nOrthogenesis had a significant following in the 19th century, its proponents including the Russian biologist Leo S. Berg, and the American paleontologist Henry Fairfield Osborn. Orthogenesis was particularly popular among some paleontologists, who believed that the fossil record showed patterns of gradual and constant unidirectional change. Those who accepted this idea, however, did not necessarily accept that the mechanism driving orthogenesis was teleological (goal-directed). They did believe that orthogenetic trends were non-adaptive; in fact they felt that in some cases they led to developments that were detrimental to the organism, such as the large antlers of the Irish elk that they believed led to the animal's extinction.\n\nSupport for orthogenesis began to decline during the modern synthesis in the 1940s, when it became apparent that orthogenesis could not explain the complex branching patterns of evolution revealed by statistical analysis of the fossil record by paleontologists. A few biologists however hung on to the idea of orthogenesis as late as the 1950s, claiming that the processes of macroevolution, the long term trends in evolution, were distinct from the processes of microevolution.\n\nMutationism was the idea that new forms and species arose in a single step as a result of large mutations. It was seen as a much faster alternative to the Darwinian concept of a gradual process of small random variations being acted on by natural selection. It was popular with early geneticists such as Hugo de Vries, who along with Carl Correns helped rediscover Gregor Mendel's laws of inheritance in 1900, William Bateson a British zoologist who switched to genetics, and early in his career, Thomas Hunt Morgan.\n\nThe 1901 mutation theory of evolution held that species went through periods of rapid mutation, possibly as a result of environmental stress, that could produce multiple mutations, and in some cases completely new species, in a single generation. Its originator was the Dutch botanist Hugo de Vries. De Vries looked for evidence of mutation extensive enough to produce a new species in a single generation and thought he found it with his work breeding the evening primrose of the genus \"Oenothera\", which he started in 1886. The plants that de Vries worked with seemed to be constantly producing new varieties with striking variations in form and color, some of which appeared to be new species because plants of the new generation could only be crossed with one another, not with their parents. DeVries himself allowed a role for natural selection in determining which new species would survive, but some geneticists influenced by his work, including Morgan, felt that natural selection was not necessary at all. De Vries's ideas were influential in the first two decades of the 20th century, as some biologists felt that mutation theory could explain the sudden emergence of new forms in the fossil record; research on \"Oenothera\" spread across the world. However, critics including many field naturalists wondered why no other organism seemed to show the same kind of rapid mutation.\n\nMorgan was a supporter of de Vries's mutation theory and was hoping to gather evidence in favor of it when he started working with the fruit fly \"Drosophila melanogaster\" in his lab in 1907. However, it was a researcher in that lab, Hermann Joseph Muller, who determined in 1918 that the new varieties de Vries had observed while breeding \"Oenothera\" were the result of polyploid hybrids rather than rapid genetic mutation. While they were doubtful of the importance of natural selection, the work of geneticists like Morgan, Bateson, de Vries and others from 1900 to 1915 established Mendelian genetics linked to chromosomal inheritance, which validated August Weismann's criticism of neo-Lamarckian evolution by discounting the inheritance of acquired characteristics. The work in Morgan's lab with \"Drosophila\" also undermined the concept of orthogenesis by demonstrating the random nature of mutation.\n\nDuring the period 1916–1932, the discipline of population genetics developed largely through the work of the geneticists Ronald Fisher, J.B.S. Haldane, and Sewall Wright. Their work recognized that the vast majority of mutations produced small effects that served to increase the genetic variability of a population rather than creating new species in a single step as the mutationists assumed. They were able to produce statistical models of population genetics that included Darwin's concept of natural selection as the driving force of evolution.\n\nDevelopments in genetics persuaded field naturalists such as Bernhard Rensch and Ernst Mayr to abandon neo-Lamarckian ideas about evolution in the early 1930s. By the late 1930s, Mayr and Theodosius Dobzhansky had synthesized the ideas of population genetics with the knowledge of field naturalists about the amount of genetic diversity in wild populations, and the importance of genetically distinct subpopulations (especially when isolated from one another by geographical barriers) to create the early 20th century modern synthesis. In 1944 George Gaylord Simpson integrated paleontology into the synthesis by statistically analyzing the fossil record to show that it was consistent with the branching non-directional form of evolution predicted by the modern synthesis, and in particular that the linear trends cited by earlier paleontologists in support of Lamarckism and orthogenesis did not stand up to careful analysis. Mayr wrote that by the end of the synthesis natural selection together with chance mechanisms like genetic drift had become the universal explanation for evolutionary change.\n\nThe concept of eclipse suggests that Darwinian research paused, implying in turn that there had been a preceding period of vigorously Darwinian activity among biologists. However, historians of science such as Mark Largent have argued that while biologists broadly accepted the extensive evidence for evolution presented in \"The Origin of Species\", there was less enthusiasm for natural selection as a mechanism. Biologists instead looked for alternative explanations more in keeping with their worldviews, which included the beliefs that evolution must be directed and that it constituted a form of progress. Further, the idea of a dark eclipse period was convenient to scientists such as Julian Huxley, who wished to paint the modern synthesis as a bright new achievement, and accordingly to depict the preceding period as dark and confused. Huxley's 1942 book \"\" therefore, argued Largent, suggested that the so-called modern synthesis began after a long period of eclipse lasting until the 1930s, in which Mendelians, neo-Lamarckians, mutationists, and Weismannians, not to mention experimental embryologists and Haeckelian recapitulationists fought running battles with each other. The idea of an eclipse also allowed Huxley to step aside from what was to him the inconvenient association of evolution with aspects such as social Darwinism, eugenics, imperialism, and militarism. Accounts such as Michael Ruse's very large book \"Monad to Man\" ignored, claimed Largent, almost all the early 20th century American evolutionary biologists. Largent has suggested as an alternative to eclipse a biological metaphor, the interphase of Darwinism, interphase being an apparently quiet period in the cycle of cell division and growth.\n\n\n", "id": "15732918", "title": "The eclipse of Darwinism"}
