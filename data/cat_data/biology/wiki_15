{"url": "https://en.wikipedia.org/wiki?curid=9233359", "text": "Vibrational circular dichroism\n\nVibrational circular dichroism (VCD) is a spectroscopic technique which detects differences in attenuation of left and right circularly polarized light passing through a sample. It is the extension of circular dichroism spectroscopy into the infrared and near infrared ranges.\n\nBecause VCD is sensitive to the mutual orientation of distinct groups in a molecule, it provides three-dimensional structural information. Thus, it is a powerful technique as VCD spectra of enantiomers can be simulated using \"ab initio\" calculations, thereby allowing the identification of absolute configurations of small molecules in solution from VCD spectra. Among such quantum computations of VCD spectra resulting from the chiral properties of small organic molecules are those based on density functional theory (DFT) and gauge-invariant atomic orbitals (GIAO). As a simple example of the experimental results that were obtained by VCD are the spectral data obtained within the carbon-hydrogen (C-H) stretching region of 21 amino acids in heavy water solutions. Measurements of vibrational optical activity (VOA) have thus numerous applications, not only for small molecules, but also for large and complex biopolymers such as muscle proteins (myosin, for example) and DNA.\n\nWhile the fundamental quantity associated with the infrared absorption is the dipole strength, the differential absorption is also proportional to the rotational strength, a quantity which depends on both the electric and magnetic dipole transition moments. Sensitivity of the handedness of a molecule toward circularly polarized light results from the form of the rotational strength. A rigorous theoretical development of VCD was developed concurrently by the late Professor P.J. Stephens, FRS, at the University of Southern California, and the group of Professor A.D. Buckingham, FRS, at Cambridge University in the UK, and first implemented analytically in the Cambridge Analytical Derivative Package (CADPAC) by R.D. Amos. Previous developments by D.P. Craig and T. Thirmachandiman at the Australian National University and Larry A. Nafie and Teresa B. Freedman at Syracuse University though theoretically correct, were not able to be straightfowardly implemented, which prevented their use. Only with the development of the Stephens formalism as implemented in CADPAC did a fast efficient and theoretically rigorous theoretical calculation of the VCD spectra of chiral molecules become feasible. This also stimulated the commercialization of VCD instruments by Biotools, Bruker, Jasco and Thermo-Nicolet (now Thermo-Fisher).\n\nExtensive VCD studies have been reported for both polypeptides and several proteins in solution; several recent reviews were also compiled. An extensive but not comprehensive VCD publications list is also provided in the \"References\" section. The published reports over the last 22 years have established VCD as a powerful technique with improved results over those previously obtained by visible/UV circular dichroism (CD) or optical rotatory dispersion (ORD) for proteins and nucleic acids.\n\nThe effects due to solvent on stabilizing the structures (conformers and zwitterionic species) of amino acids and peptides and the corresponding effects seen in the vibrational circular dichroism (VCD) and Raman optical activity spectra (ROA) have been recently documented by a combined theoretical and experimental work on L-alanine and N-acetyl L-alanine N'-methylamide. Similar effects have also been seen in the nuclear magnetic resonance (NMR) spectra by the Weise and Weisshaar NMR groups at the University of Wisconsin-Madison.\n\nVCD spectra of nucleotides, synthetic polynucleotides and several nucleic acids, including DNA, have been reported and assigned in terms of the type and number of helices present in A-, B-, and Z-DNA.\n\nVCD can be regarded as a relatively recent technique. Although Vibrational Optical Activity and in particular Vibrational Circular Dichroism, has been known for a long time, the first VCD instrument was developed in 1973 and commercial instruments were available only since 1997.\n\nFor biopolymers such as proteins and nucleic acids, the difference in absorbance between the levo- and dextro- configurations is five orders of magnitude smaller than the\ncorresponding (unpolarized) absorbance. Therefore, VCD of biopolymers requires the use of very sensitive, specially built instrumentation as well as time-averaging over relatively long intervals of time even with such sensitive VCD spectrometers.\nMost CD instruments produce left- and right- circularly polarized light which is then either sine-wave or square-wave modulated, with subsequent phase-sensitive detection and lock-in amplification of the detected signal. In the case of FT-VCD,\na photo-elastic modulator (PEM) is employed in conjunction with an FTIR interferometer set-up. An example is that of a Bomem model MB-100 FTIR interferometer equipped with additional polarizing optics/ accessories needed for recording VCD spectra.\nA parallel beam emerges through a side port of the interferometer which passes first through a wire grid linear polarizer and then through an octagonal-shaped ZnSe crystal PEM which modulates the polarized beam at a fixed, lower frequency such as 37.5 kHz. A mechanically stressed crystal such as ZnSe exhibits birefringence when stressed by an adjacent piezoelectric transducer. The linear polarizer is positioned close to, and at 45 degrees, with respect to the ZnSe crystal axis. The polarized radiation focused onto the detector is doubly modulated, both by the PEM and by the interferometer setup. A very low noise detector, such as MCT (HgCdTe), is also selected for the VCD signal phase-sensitive detection. The first dedicated VCD spectrometer brought to market was the ChiralIR from Bomem/BioTools, Inc. in 1997. Today, Thermo-Electron, Bruker, Jasco and BioTools offer either VCD accessories or stand-alone instrumentation. To prevent detector saturation an appropriate, long wave pass filter is placed before the very low noise MCT detector, which allows only radiation below 1750 cm to reach the MCT detector; the latter however measures radiation only down to 750 cm. FT-VCD spectra accumulation of the selected sample solution is then carried out, digitized and stored by an in-line computer. Published reviews that compare various VCD methods are also available.\n\nVCD spectra have also been reported in the presence of an applied external magnetic field. This method can enhance the VCD spectral resolution for small molecules.\n\nROA is a technique complementary to VCD especially useful in the 50–1600 cm spectral region; it is considered as the technique of choice for determining optical activity for photon energies less than 600 cm.\n", "id": "9233359", "title": "Vibrational circular dichroism"}
{"url": "https://en.wikipedia.org/wiki?curid=16898089", "text": "R.EcoRII\n\nRestriction endonuclease (REase) EcoRII (pronounced \"eco R two\") is an enzyme of restriction modification system (RM) naturally found in \"Escherichia coli\", a Gram-negative bacteria. Its molecular mass is 45.2 kDa, being composed of 402 amino acids.\n\nEcoRII is a bacterial Type IIE REase that interacts with two or three copies of the pseudopalindromic DNA recognition sequence 5'-CCWGG-3' (W = A or T), one being the actual target of cleavage, the other(s) serving as the allosteric activator(s). EcoRII cut target DNA sequence CCWGG generating sticky ends.\n\nThe apo crystal structure of EcoRII mutant R88A () has been solved at 2.1 Å resolution. The EcoRII monomer has two domains, N-terminal and C-terminal, linked through a hinge loop.\n\nThe N-terminal effector-binding domain has an archetypal DNA-binding pseudobarrel fold () with a prominent cleft. Structural superposition showed it is evolutionarily related to:\n\nThe C-terminal catalytic domain has a typical restriction endonuclease-like fold () and belongs to the large (more than 30 members) restriction endonuclease superfamily ().\n\nStructure-based sequence alignment and site-directed mutagenesis identified the putative PD..D/EXK active sites of the EcoRII catalytic domain dimer that in apo structure are spatially blocked by the N-terminal domains.\n\n", "id": "16898089", "title": "R.EcoRII"}
{"url": "https://en.wikipedia.org/wiki?curid=23046982", "text": "Nicking enzyme amplification reaction\n\nNicking Enzyme Amplification Reaction (NEAR) is a method for \"in vitro\" DNA amplification like the polymerase chain reaction (PCR). NEAR is isothermal, replicating DNA at a constant temperature using a polymerase (and nicking enzyme) to exponentially amplify the DNA at a temperature range of 55 °C to 59 °C.\n\nOne disadvantage of PCR is that it consumes time uncoiling the double-stranded DNA with heat into single strands (a process called denaturation) . This leads to amplification times typically thirty minutes or more for significant production of amplified products.\n\nPotential advantages of NEAR over PCR are increased speed and lower energy requirements, characteristics that are shared with other isothermal amplification schemes. A major disadvantage of NEAR relative to PCR is that production of nonspecific amplification products is a common issue with isothermal amplification reactions.\n\nThe NEAR reaction uses naturally occurring or engineered endonucleases that introduce a strand break on only one strand of a double-stranded DNA cleavage site. The ability of several of these enzymes to catalyze isothermal DNA amplification was disclosed but not claimed in the patents issued for the enzymes themselves.\n\n", "id": "23046982", "title": "Nicking enzyme amplification reaction"}
{"url": "https://en.wikipedia.org/wiki?curid=23048210", "text": "Nicking enzyme\n\nA nicking enzyme (or nicking endonuclease) is an enzyme that cuts one strand of a double-stranded DNA at a specific recognition nucleotide sequences known as a restriction site. Such enzymes hydrolyse (cut) only one strand of the DNA duplex, to produce DNA molecules that are “nicked”, rather than cleaved.\n\nThey can be used for strand-displacement amplification, Nicking Enzyme Amplification Reaction, exonucleotyic degradation, the creation of small gaps, or nick translation. The latter process has been successfully used to incorporate both radioactively labelled nucleotides and fluorescent nucleotides allowing specific regions on a double stranded DNA to be studied. Over 200 nicking enzymes have been studied, and 13 of these are available commercially and are routinely used for research and in commercial products.\n\n", "id": "23048210", "title": "Nicking enzyme"}
{"url": "https://en.wikipedia.org/wiki?curid=18159480", "text": "Conformational epitope\n\nA conformational epitope is a sequence of sub-units (usually amino acids) composing an antigen that come in direct contact with a receptor of the immune system.\n\nAn antigen is any substance that the immune system can recognize as foreign. Antigens are usually proteins that are too large to bind as a whole to any receptor so only specific segments, that form the antigen, bind with a specific receptor. Such segments are called epitopes. Likewise, it is only paratope of the receptor that comes in contact with the epitope.\n\nProteins are composed of repeating nitrogen-containing subunits called amino acids that in nature do not exist as straight chains called primary structure, but as folded whorls with complex loops. The latter is known as the tertiary structure of a protein. So, whenever a receptor interacts with an undigested antigen, the surface amino acids that come in contact may not be continuous with each other if the protein is unwound. Such discontinuous amino acids that come together in three-dimensional conformation and interact with the receptor's paratope are called conformational epitopes. In contrast, if the antigen is digested, small segments called peptides are formed, which bind with major histocompatibility complex molecules, and then later with T cell receptors through amino acids that are continuous in a line. These are known as linear epitopes.\n\n", "id": "18159480", "title": "Conformational epitope"}
{"url": "https://en.wikipedia.org/wiki?curid=7805539", "text": "Ty5 retrotransposon\n\nThe Ty5 is a type of retrotransposon native to the \"Saccharomyces cerevisiae\" organism.\n\nTy5 is one of five endogenous retrotransposons native to the model organism \"Saccharomyces cerevisiae\", all of which target integration to gene poor regions. Endogenous retrotransposons are hypothesized to target gene poor chromosomal targets in order to reduce the chance of inactivating host genes. Ty1-Ty4 integrate upstream of Pol III promoters, while Ty5 targets integration to loci bound in heterochromatin. In the case of Ty5, this likely occurs by means of an interaction between the C-terminus of integrase and a target protein. The tight targeting patterns seen for the Ty elements are thought to be a means to limit damage to its host, which has a very gene dense genome. Ty5 was discovered in the mid 90s in the laboratory of Daniel Voytas at Iowa State University. Ty5 is used as a model system by which to understand the biology of the telomere and heterochromatin. The Ty5 retrotransposon is used as a genetic model to study the architecture and dynamics of the telomeres and heterochromatin.\n\nHeterochromatin in \"S. cerevisiae\" is composed of a wide array of proteins and plays several roles. The first stage of heterochromatin formation requires DNA binding proteins, which interact with specific cis DNA sequences at the telomeres, rDNA and HM loci. These proteins, including Rap1p and the origin recognition complex (ORC), serve as a platform for other proteins to bind, condense the DNA, and modify neighboring histones. Some of these proteins, notably Rap1p, also play other roles, including initiation of transcription. The first known step in the formation of dedicated heterochromatin is the binding of Sir4p to Rap1p (Luo, Vega-Palas et al. 2002). Sir4p is one of four ‘Silent Information Regulator’ proteins that also include Sir1p, Sir2p and Sir3p. Of these, Sir2p, Sir3p and Sir4p form the core of heterochromatin. Sir4p serves as a binding site for Sir2p, which is the next to bind. Sir2p deacetylates adjacent histones, which is thought to further condense the chromatin and prevent the binding of other transcription promoting histone modification enzymes. Sir3p binding follows, further condensing the heterochromatin. Sir1p plays a role in the initiation of silencing at the HM loci. A large number of other proteins act in both a synergistic and antagonistic manner.\n\nTy5 is a relative of the \"Retroviridae\" family of retroviruses, which includes the human pathogen HIV. Ty5 is a tractable system in which to study the biology of retrovirus integration.\n", "id": "7805539", "title": "Ty5 retrotransposon"}
{"url": "https://en.wikipedia.org/wiki?curid=23498666", "text": "Society for Mathematical Biology\n\nThe Society for Mathematical Biology (SMB) is an international association co-founded in 1972 in USA by Drs.George Karreman, Herbert Daniel Landahl and (initially chaired) by Anthony Bartholomay for the furtherance of joint scientific activities between Mathematics and Biology research communities. SMB publishes the \"\"Bulletin of Mathematical Biology\"\" (), as well as the SMB annual newsletter.\n\nThe Society for Mathematical Biology emerged and grew from the earlier school of mathematical biophysics, initiated and supported by the Founder of Mathematical Biology, Nicolas Rashevsky. Thus, the roots of SMB go back to the publication in 1939 of the first international journal of mathematical biology, previously entitled \"The Bulletin of Mathematical Biophysics\"—which was founded by Nicolas Rashevsky, and which is currently published by SMB under the name of \"\"Bulletin of Mathematical Biology\"\". Professor Rashevsky also founded in 1969 the non-profit organization \"\"Mathematical Biology, Incorporated\"\"—the precursor of SMB. Another notable member of the University of Chicago school of mathematical biology was Anatol Rapoport whose major interests were in developing basic concepts in the related area of mathematical sociology, who cofounded the Society for General Systems Research and became a president of the latter society in 1965. Herbert D. Landahl was initially also a member of Rashevsky's school of mathematical biology, and became the second president of SMB in the 1980s; both Herbert Landahl and Robert Rosen from Rashevsky's research group were focused on dynamical systems approaches to complex systems biology, with the latter researcher becoming in 1980 the president of the Society for General Systems Research. \nIn addition to its research and news publications, The Society for Mathematical Biology\nsupports education in: mathematical biology, mathematical biophysics, complex systems biology and theoretical biology through sponsorship of several topic-focused graduate and postdoctoral courses. To encourage and stimulate young researchers in this relatively new and rapidly developing field of mathematical biology \"The Society for Mathematical Biology\" awards several prizes, as well as lists regularly new open international opportunities for researchers and students in this field.\n\n\n\n", "id": "23498666", "title": "Society for Mathematical Biology"}
{"url": "https://en.wikipedia.org/wiki?curid=2203789", "text": "Amplified fragment length polymorphism\n\nAFLP-PCR or just AFLP is a PCR-based tool used in genetics research, DNA fingerprinting, and in the practice of genetic engineering. Developed in the early 1990s by Keygene, AFLP uses restriction enzymes to digest genomic DNA, followed by ligation of adaptors to the sticky ends of the restriction fragments. A subset of the restriction fragments is then selected to be amplified. This selection is achieved by using primers complementary to the adaptor sequence, the restriction site sequence and a few nucleotides inside the restriction site fragments (as described in detail below). The amplified fragments are separated and visualized on denaturing on agarose gel electrophoresis , either through autoradiography or fluorescence methodologies, or via automated capillary sequencing instruments.\n\nAlthough AFLP is commonly referred to as \"Amplified fragment length polymorphism\", the resulting data are not scored as length polymorphisms, but instead as presence-absence polymorphisms.\n\nAFLP-PCR is a highly sensitive method for detecting polymorphisms in DNA. The technique was originally described by Vos and Zabeau in 1993. In detail, the procedure of this technique is divided into three steps:\n\n\nThe AFLP technology has the capability to detect various polymorphisms in different genomic regions simultaneously. It is also highly sensitive and reproducible. As a result, AFLP has become widely used for the identification of genetic variation in strains or closely related species of plants, fungi, animals, and bacteria. The AFLP technology has been used in criminal and paternity tests, also to determine slight differences within populations, and in linkage studies to generate maps for quantitative trait locus (QTL) analysis.\n\nThere are many advantages to AFLP when compared to other marker technologies including randomly amplified polymorphic DNA (RAPD), restriction fragment length polymorphism (RFLP), and microsatellites. AFLP not only has higher reproducibility, resolution, and sensitivity at the whole genome level compared to other techniques, but it also has the capability to amplify between 50 and 100 fragments at one time. In addition, no prior sequence information is needed for amplification (Meudt & Clarke 2007). As a result, AFLP has become extremely beneficial in the study of taxa including bacteria, fungi, and plants, where much is still unknown about the genomic makeup of various organisms.\n\nThe AFLP technology is covered by patents and patent applications of Keygene N.V. AFLP is a registered trademark of Keygene N.V.\n\nSoftware for analyzing AFLP data\n\nFreeware for analyzing AFLP data\n\nOnline programs for simulation of AFLP-PCR\n", "id": "2203789", "title": "Amplified fragment length polymorphism"}
{"url": "https://en.wikipedia.org/wiki?curid=235077", "text": "Reverse transcription polymerase chain reaction\n\nReverse transcription polymerase chain reaction (RT-PCR), a variant of polymerase chain reaction (PCR), is a technique commonly used in molecular biology to detect RNA expression. RT-PCR is often confused with real-time polymerase chain reaction (qPCR) by students and scientists alike, but they are separate and distinct techniques. While RT-PCR is used to qualitatively detect gene expression through creation of complementary DNA (cDNA) transcripts from RNA, qPCR is used to quantitatively measure the amplification of DNA using fluorescent dyes. qPCR is also referred to as quantitative PCR, quantitative real-time PCR, and real-time quantitative PCR.\n\nAlthough RT-PCR and the traditional PCR both produce multiple copies of particular DNA isolates through amplification, the applications of the two techniques are fundamentally different. Traditional PCR is used to exponentially amplify target DNA sequences. RT-PCR is used to clone expressed genes by reverse transcribing the RNA of interest into its DNA complement through the use of reverse transcriptase. Subsequently, the newly synthesized cDNA is amplified using traditional PCR.\n\nIn addition to the qualitative study of gene expression, quantitative PCR can be utilized for quantification of RNA, in both relative and absolute terms, by incorporating qPCR into the technique. The combined technique, described as quantitative RT-PCR or real-time RT-PCR (sometimes even quantitative real-time RT-PCR), is often abbreviated as qRT-PCR, RT-qPCR, or RRT-PCR. Compared to other RNA quantification methods, such as northern blot, qRT-PCR is considered to be the most powerful, sensitive, and quantitative assay for the detection of RNA levels. It is frequently used in the expression analysis of single or multiple genes, and expression patterns for identifying infections and diseases.\n\nIn order to avoid confusion, the following abbreviations will be used consistently throughout this article:\n\nSince its introduction in 1977, northern blot had been used extensively for RNA quantification despite its shortcomings: (a) time-consuming technique, (b) requires a large quantity of RNA for detection, and (c) quantitatively inaccurate in the low abundance of RNA content. However, the discovery of reverse transcriptase during the study of viral replication of genetic material led to the development of RT-PCR, which has since displaced northern blot as the method of choice for RNA detection and quantification.\n\nRT-PCR has risen to become the benchmark technology for the detection and/or comparison of RNA levels for several reasons: (a) it does not require post PCR processing, (b) a wide range (>10-fold) of RNA abundance can be measured, and (c) it provides insight into both qualitative and quantitative data. Due to its simplicity, specificity and sensitivity, RT-PCR is used in a wide range of applications from experiments as simple as quantification of yeast cells in wine to more complex uses as diagnostic tools for detecting infectious agents such as the avian flu virus.\n\nIn RT-PCR, the RNA template is first converted into a complementary DNA (cDNA) using a reverse transcriptase. The cDNA is then used as a template for exponential amplification using PCR. QT-NASBA is currently the most sensitive method of RNA detection available. The use of RT-PCR for the detection of RNA transcript has revolutionalized the study of gene expression in the following important ways:\n\nThe quantification of mRNA using RT-PCR can be achieved as either a one-step or a two-step reaction. The difference between the two approaches lies in the number of tubes used when performing the procedure. In the one-step approach, the entire reaction from cDNA synthesis to PCR amplification occurs in a single tube. On the other hand, the two-step reaction requires that the reverse transcriptase reaction and PCR amplification be performed in separate tubes. The one-step approach is thought to minimize experimental variation by containing all of the enzymatic reactions in a single environment. However, the starting RNA templates are prone to degradation in the one-step approach, and the use of this approach is not recommended when repeated assays from the same sample is required. Additionally, one-step approach is reported to be less accurate compared to the two-step approach. It is also the preferred method of analysis when using DNA binding dyes such as SYBR Green since the elimination of primer-dimers can be achieved through a simple change in the melting temperature. The disadvantage of the two-step approach is susceptibility to contamination due to more frequent sample handling.\n\nQuantification of RT-PCR products can largely be divided into two categories: end-point and real-time. The use of end-point RT-PCR is preferred for measuring gene expression changes in small number of samples, but the real-time RT-PCR has become the gold standard method for validating results obtained from array analyses or gene expression changes on a global scale.\n\nThe measurement approaches of end-point RT-PCR requires the detection of gene expression levels by the use of fluorescent dyes like ethidium bromide, P32 labeling of PCR products using phosphorimager, or by scintillation counting. End-point RT-PCR is commonly achieved using three different methods: relative, competitive and comparative.\n\nThe emergence of novel fluorescent DNA labeling techniques in the past few years have enabled the analysis and detection of PCR products in real-time and has consequently led to the widespread adoption of real-time RT-PCR for the analysis of gene expression. Not only is real-time RT-PCR now the method of choice for quantification of gene expression, it is also the preferred method of obtaining results from array analyses and gene expressions on a global scale. Currently, there are four different fluorescent DNA probes available for the real-time RT-PCR detection of PCR products: SYBR Green, TaqMan, Molecular Beacons, and Scorpions. All of these probes allow the detection of PCR products by generating a fluorescent signal. While the SYBR Green dye emits its fluorescent signal simply by binding to the double-stranded DNA in solution, the TaqMan probes, Molecular Beacons and Scorpions generation of fluorescence depend on Förster Resonance Energy Transfer (FRET) coupling of the dye molecule and a quencher moiety to the oligonucleotide substrates.\n\nTwo strategies are commonly employed to quantify the results obtained by real-time RT-PCR; the standard curve method and the comparative threshold method.\n\nThe exponential amplification via reverse transcription polymerase chain reaction provides for a highly sensitive technique in which a very low copy number of RNA molecules can be detected. RT-PCR is widely used in the diagnosis of genetic diseases and, semiquantitatively, in the determination of the abundance of specific different RNA molecules within a cell or tissue as a measure of gene expression.\n\nRT-PCR is commonly used in research methods to measure gene expression. For example, Lin et al. used qRT-PCR to measure expression of Gal genes in yeast cells. First, Lin et al. engineered a mutation of a protein suspected to participate in the regulation of Gal genes. This mutation was hypothesized to selectively abolish Gal expression. To confirm this, gene expression levels of yeast cells containing this mutation were analyzed using qRT-PCR. The researchers were able to conclusively determine that the mutation of this regulatory protein reduced Gal expression. Northern blot analysis is used to study the RNA's gene expression further.\n\nRT-PCR can also be very useful in the insertion of eukaryotic genes into prokaryotes. Because most eukaryotic genes contain introns, which are present in the genome but not in the mature mRNA, the cDNA generated from a RT-PCR reaction is the exact (without regard to the error-prone nature of reverse transcriptases) DNA sequence that would be directly translated into protein after transcription. When these genes are expressed in prokaryotic cells for the sake of protein production or purification, the RNA produced directly from transcription need not undergo splicing as the transcript contains only exons. (Prokaryotes, such as E. coli, lack the mRNA splicing mechanism of eukaryotes).\n\nRT-PCR can be used to diagnose genetic disease such as Lesch–Nyhan syndrome. This genetic disease is caused by a malfunction in the HPRT1 gene, which clinically leads to the fatal uric acid urinary stone and symptoms similar to gout.[6] Analyzing a pregnant mother and a fetus for mRNA expression levels of HPRT1 will reveal if the mother is a carrier and if the fetus will likely to develop Lesch–Nyhan syndrome.\n\nScientists are working on ways to use RT-PCR in cancer detection to help improve prognosis, and monitor response to therapy. Circulating tumor cells produce unique mRNA transcripts depending on the type of cancer. The goal is to determine which mRNA transcripts serve as the best biomarkers for a particular cancer cell type and then analyze its expression levels with RT-PCR.\n\nRT-PCR is commonly used in studying the genomes of viruses whose genomes are composed of RNA, such as Influenzavirus A and retroviruses like HIV.\n\nDespite its major advantages, RT-PCR is not without drawbacks. The exponential growth of the reverse transcribed complementary DNA (cDNA) during the multiple cycles of PCR produces inaccurate end point quantification due to the difficulty in maintaining linearity. In order to provide accurate detection and quantification of RNA content in a sample, qRT-PCR was developed using fluorescence-based modification to monitor the amplification products during each cycle of PCR. The extreme sensitivity of the technique can be a double edged sword since even the slightest DNA contamination can lead to undesirable results. A simple method for elimination of false positive results is to include anchors, or tags, to the 5' region of a gene specific primer. Additionally, planning and design of quantification studies can be technically challenging due to the existence of numerous sources of variation including template concentration and amplification efficiency.\n\nRT-PCR can be carried out by the one-step RT-PCR protocol or the two-step RT-PCR protocol.\n\nOne-step RT-PCR take mRNA targets (up to 6 kb) and subjects them to reverse transcription and then PCR amplification in a single test tube. Use only intact, high quality RNA for the best results. Be sure to use a sequence-specific primer.\n\nTwo-step RT-PCR, as the name implies, occurs in two steps. First the reverse transcription and then the PCR. This method is more sensitive than the one-step method. Kits are also useful for two-step RT-PCR. Just as for one-step, use only intact, high quality RNA for the best results. The primer for two-step does not have to be sequence specific.\n\n\n\nQuantitative RT-PCR assay is considered to be the gold standard for measuring the number of copies of specific cDNA targets in a sample but it is poorly standardized. As a result, while there are numerous publications utilizing the technique, many provide inadequate experimental detail and use unsuitable data analysis to draw inappropriate conclusions. Due to the inherent variability in the quality of any quantitative PCR data, reviewers not only have a difficult time evaluating these manuscripts, the studies also become impossible to replicate. Recognizing the need for the standardization of the reporting of experimental conditions, the Minimum Information for Publication of Quantitative Real-Time PCR Experiments (MIQE, pronounced mykee) guidelines have been published by the international consortium of academic scientists. The MIQE guidelines describe the minimum information necessary for evaluating quantitative PCR experiments that should be required for publication for encouraging better experimental practice and ensuring the relevance, accuracy, correct interpretation, and repeatability of quantitative PCR data.\n\nBesides reporting guidelines, the MIQE stresses the need to standardize the nomenclature associated with quantitative PCR to avoid confusion; for example, the abbreviation qPCR should be used for quantitative real-time PCR and RT-qPCR should be used for reverse transcription-qPCR, and genes used for normalisation should be referred to as reference genes instead of housekeeping genes. It is also proposes that commercially derived terms like TaqMan® probes should not be used but instead referred to as hydrolysis probes. Additionally, it is proposed that quantification cycle (Cq) be used to describe the PCR cycle used for quantification instead of threshold cycle (Ct), crossing point (Cp), and takeoff point (TOP), which refer to the same value but were coined by different manufacturers of real-time instruments.\n\nThe guideline consists of the following elements: 1) experimental design, 2) sample, 3) nucleic acid extraction, 4) reverse transcription, 5) qPCR target information, 6) oligonucleotides, 7) protocol, 8) validation, and 9) data analysis. Specific items within each element carry a label of either E (essential) or D (desirable). Those labelled E are considered critical and indispensable while those labelled D are considered peripheral yet important for best-practices.\n\n", "id": "235077", "title": "Reverse transcription polymerase chain reaction"}
{"url": "https://en.wikipedia.org/wiki?curid=23402198", "text": "Wetware (biology)\n\nThe term wetware is used to describe the protocols and molecular devices used in molecular biology and synthetic biology.\n\nWhere biological components and systems are treated in a similar manner to software, and similar development models and methodologies are applied, the term 'wetware' can be used to imply an approach to their problems as 'bugs' and their beneficial aspects as 'features'. In this manner, genetic code can be subjected to Version Control Systems such as Git, for the development of improvements and new gene edits, therapeutic components and therapies.\n\nThe National Science Foundation (NSF) funded Wiki project Open Wetware (OWW) provides a resource for reagent, project and laboratory notebook sharing.\n\nA somewhat related NSF consortium Synthetic Biology Engineering Research Center (SynBERC) constructs and distributes wetware.\n", "id": "23402198", "title": "Wetware (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=10663351", "text": "Oligonucleotide synthesis\n\nOligonucleotide synthesis is the chemical synthesis of relatively short fragments of nucleic acids with defined chemical structure (sequence). The technique is extremely useful in current laboratory practice because it provides a rapid and inexpensive access to custom-made oligonucleotides of the desired sequence. Whereas enzymes synthesize DNA and RNA only in a 5' to 3' direction, chemical oligonucleotide synthesis does not have this limitation, although it is, most often, carried out in the opposite, 3' to 5' direction. Currently, the process is implemented as solid-phase synthesis using phosphoramidite method and phosphoramidite building blocks derived from protected 2'-deoxynucleosides (dA, dC, dG, and T), ribonucleosides (A, C, G, and U), or chemically modified nucleosides, e.g. LNA or BNA.\n\nTo obtain the desired oligonucleotide, the building blocks are sequentially coupled to the growing oligonucleotide chain in the order required by the sequence of the product (see Synthetic cycle below). The process has been fully automated since the late 1970s. Upon the completion of the chain assembly, the product is released from the solid phase to solution, deprotected, and collected. The occurrence of side reactions sets practical limits for the length of synthetic oligonucleotides (up to about 200 nucleotide residues) because the number of errors accumulates with the length of the oligonucleotide being synthesized. Products are often isolated by high-performance liquid chromatography (HPLC) to obtain the desired oligonucleotides in high purity. Typically, synthetic oligonucleotides are single-stranded DNA or RNA molecules around 15–25 bases in length.\n\nOligonucleotides find a variety of applications in molecular biology and medicine. They are most commonly used as antisense oligonucleotides, small interfering RNA, primers for DNA sequencing and amplification, probes for detecting complementary DNA or RNA via molecular hybridization, tools for the targeted introduction of mutations and restriction sites, and for the synthesis of artificial genes.\n\nThe evolution of oligonucleotide synthesis saw four major methods of the formation of internucleosidic linkages and has been reviewed in the literature in great detail.\n\nIn the early 1950s, Alexander Todd’s group pioneered H-phosphonate and phosphate triester methods of oligonucleotide synthesis. The reaction of compounds 1 and 2 to form H-phosphonate diester 3 is an H-phosphonate coupling in solution while that of compounds 4 and 5 to give 6 is a phosphotriester coupling (see phosphotriester synthesis below).\n\nThirty years later, this work inspired, independently, two research groups to adopt the H-phosphonate chemistry to the solid-phase synthesis using nucleoside H-phosphonate monoesters 7 as building blocks and pivaloyl chloride, 2,4,6-triisopropylbenzenesulfonyl chloride (TPS-Cl), and other compounds as activators. The practical implementation of H-phosphonate method resulted in a very short and simple synthetic cycle consisting of only two steps, detritylation and coupling (Scheme 2). Oxidation of internucleosidic H-phosphonate diester linkages in 8 to phosphodiester linkages in 9 with a solution of iodine in aqueous pyridine is carried out at the end of the chain assembly rather than as a step in the synthetic cycle. If desired, the oxidation may be carried out under anhydrous conditions. Alternatively, 8 can be converted to phosphorothioate 10 or phosphoroselenoate 11 (X = Se), or oxidized by CCl in the presence of primary or secondary amines to phosphoramidate analogs 12. The method is very convenient in that various types of phosphate modifications (phosphate/phosphorothioate/phosphoramidate) may be introduced to the same oligonucleotide for modulation of its properties.\n\nMost often, H-phosphonate building blocks are protected at the 5'-hydroxy group and at the amino group of nucleic bases A, C, and G in the same manner as phosphoramidite building blocks (see below). However, protection at the amino group is not mandatory.\n\nIn the 1950s, Har Gobind Khorana and co-workers developed a phosphodiester method where 3’-\"O\"-acetylnucleoside-5’-\"O\"-phosphate 2 (Scheme 3) was activated with \"N\",\"N\"-dicyclohexylcarbodiimide (DCC) or 4-toluenesulfonyl chloride (Ts-Cl). The activated species were reacted with a 5’-\"O\"-protected nucleoside 1 to give a protected dinucleoside monophosphate 3. Upon the removal of 3’-\"O\"-acetyl group using base-catalyzed hydrolysis, further chain elongation was carried out. Following this methodology, sets of tri- and tetradeoxyribonucleotides were synthesized and were enzymatically converted to longer oligonucleotides, which allowed elucidation of the genetic code. The major limitation of the phosphodiester method consisted in the formation of pyrophosphate oligomers and oligonucleotides branched at the internucleosidic phosphate. The method seems to be a step back from the more selective chemistry described earlier; however, at that time, most phosphate-protecting groups available now had not yet been introduced. The lack of the convenient protection strategy necessitated taking a retreat to a slower and less selective chemistry to achieve the ultimate goal of the study.\n\nIn the 1960s, groups led by R. Letsinger and C. Reese developed a phosphotriester approach. The defining difference from the phosphodiester approach was the protection of the phosphate moiety in the building block 1 (Scheme 4) and in the product 3 with 2-cyanoethyl group. This precluded the formation of oligonucleotides branched at the internucleosidic phosphate. The higher selectivity of the method allowed the use of more efficient coupling agents and catalysts, which dramatically reduced the length of the synthesis. The method, initially developed for the solution-phase synthesis, was also implemented on low-cross-linked \"popcorn\" polystyrene, and later on controlled pore glass (CPG, see \"Solid support material\" below), which initiated a massive research effort in solid-phase synthesis of oligonucleotides and eventually led to the automation of the oligonucleotide chain assembly.\n\nIn the 1970s, substantially more reactive P(III) derivatives of nucleosides, 3'-\"O\"-chlorophosphites, were successfully used for the formation of internucleosidic linkages. This led to the discovery of the phosphite triester methodology. The group led by M. Caruthers took the advantage of less aggressive and more selective 1\"H\"-tetrazolidophosphites and implemented the method on solid phase. Very shortly after, the workers from the same group further improved the method by using more stable nucleoside phosphoramidites as building blocks. The use of 2-cyanoethyl phosphite-protecting group in place of a less user-friendly methyl group led to the nucleoside phosphoramidites currently used in oligonucleotide synthesis (see Phosphoramidite building blocks below). Many later improvements to the manufacturing of building blocks, oligonucleotide synthesizers, and synthetic protocols made the phosphoramidite chemistry a very reliable and expedient method of choice for the preparation of synthetic oligonucleotides.\n\nAs mentioned above, the naturally occurring nucleotides (nucleoside-3'- or 5'-phosphates) and their phosphodiester analogs are insufficiently reactive to afford an expeditious synthetic preparation of oligonucleotides in high yields. The selectivity and the rate of the formation of internucleosidic linkages is dramatically improved by using 3'-\"O\"-(\"N\",\"N\"-diisopropyl phosphoramidite) derivatives of nucleosides (nucleoside phosphoramidites) that serve as building blocks in phosphite triester methodology. To prevent undesired side reactions, all other functional groups present in nucleosides have to be rendered unreactive (protected) by attaching protecting groups. Upon the completion of the oligonucleotide chain assembly, all the protecting groups are removed to yield the desired oligonucleotides. Below, the protecting groups currently used in commercially available and most common nucleoside phosphoramidite building blocks are briefly reviewed:\n\nThe protection of the exocyclic amino groups has to be orthogonal to that of the 5'-hydroxy group because the latter is removed at the end of each synthetic cycle. The simplest to implement, and hence the most widely used, strategy is to install a base-labile protection group on the exocyclic amino groups. Most often, two protection schemes are used.\n\n\nNon-nucleoside phosphoramidites are the phosphoramidite reagents designed to introduce various functionalities at the termini of synthetic oligonucleotides or between nucleotide residues in the middle of the sequence. In order to be introduced inside the sequence, a non-nucleosidic modifier has to possess at least two hydroxy groups, one of which is often protected with the DMT group while the other bears the reactive phosphoramidite moiety.\n\nNon-nucleosidic phosphoramidites are used to introduce desired groups that are not available in natural nucleosides or that can be introduced more readily using simpler chemical designs. A very short selection of commercial phosphoramidite reagents is shown in Scheme for the demonstration of the available structural and functional diversity. These reagents serve for the attachment of 5'-terminal phosphate (1), NH (2), SH (3), aldehydo (4), and carboxylic groups (5), CC triple bonds (6), non-radioactive labels and quenchers (exemplified by 6-FAM amidite 7 for the attachment of fluorescein and dabcyl amidite 8, respectively), hydrophilic and hydrophobic modifiers (exemplified by hexaethyleneglycol amidite 9 and cholesterol amidite 10, respectively), and biotin amidite 11.\n\nOligonucleotide synthesis is carried out by a stepwise addition of nucleotide residues to the 5'-terminus of the growing chain until the desired sequence is assembled. Each addition is referred to as a synthetic cycle (Scheme 5) and consists of four chemical reactions:\n\nThe DMT group is removed with a solution of an acid, such as 2% trichloroacetic acid (TCA) or 3% dichloroacetic acid (DCA), in an inert solvent (dichloromethane or toluene). The orange-colored DMT cation formed is washed out; the step results in the solid support-bound oligonucleotide precursor bearing a free 5'-terminal hydroxyl group.\nIt is worth remembering that conducting detritylation for an extended time or with stronger than recommended solutions of acids leads to depurination of solid support-bound oligonucleotide and thus reduces the yield of the desired full-length product.\n\nA 0.02–0.2 M solution of nucleoside phosphoramidite (or a mixture of several phosphoramidites) in acetonitrile is activated by a 0.2–0.7 M solution of an acidic azole catalyst, 1\"H\"-tetrazole, 5-ethylthio-1H-tetrazole, 2-benzylthiotetrazole, 4,5-dicyanoimidazole, or a number of similar compounds. A more extensive information on the use of various coupling agents in oligonucleotide synthesis can be found in a recent review. The mixing is usually very brief and occurs in fluid lines of oligonucleotide synthesizers (see below) while the components are being delivered to the reactors containing solid support. The activated phosphoramidite in 1.5 – 20-fold excess over the support-bound material is then brought in contact with the starting solid support (first coupling) or a support-bound oligonucleotide precursor (following couplings) whose 5'-hydroxy group reacts with the activated phosphoramidite moiety of the incoming nucleoside phosphoramidite to form a phosphite triester linkage. The coupling of 2'-deoxynucleoside phosphoramidites is very rapid and requires, on small scale, about 20 s for its completion. In contrast, sterically hindered 2'-\"O\"-protected ribonucleoside phosphoramidites require 5-15 min to be coupled in high yields. The reaction is also highly sensitive to the presence of water, particularly when dilute solutions of phosphoramidites are used, and is commonly carried out in anhydrous acetonitrile. Generally, the larger the scale of the synthesis, the lower the excess and the higher the concentration of the phosphoramidites is used. In contrast, the concentration of the activator is primarily determined by its solubility in acetonitrile and is irrespective of the scale of the synthesis. Upon the completion of the coupling, any unbound reagents and by-products are removed by washing.\n\nThe capping step is performed by treating the solid support-bound material with a mixture of acetic anhydride and 1-methylimidazole or, less often, DMAP as catalysts and, in the phosphoramidite method, serves two purposes.\n\n\nThe newly formed tricoordinated phosphite triester linkage is not natural and is of limited stability under the conditions of oligonucleotide synthesis. The treatment of the support-bound material with iodine and water in the presence of a weak base (pyridine, lutidine, or collidine) oxidizes the phosphite triester into a tetracoordinated phosphate triester, a protected precursor of the naturally occurring phosphate diester internucleosidic linkage. Oxidation may be carried out under anhydrous conditions using tert-Butyl hydroperoxide or, more efficiently, (1S)-(+)-(10-camphorsulfonyl)-oxaziridine (CSO). The step of oxidation is substituted with a sulfurization step to obtain oligonucleotide phosphorothioates (see \"Oligonucleotide phosphorothioates and their synthesis\" below). In the latter case, the sulfurization step is best carried out prior to capping.\n\nIn solid-phase synthesis, an oligonucleotide being assembled is covalently bound, via its 3'-terminal hydroxy group, to a solid support material and remains attached to it over the entire course of the chain assembly. The solid support is contained in columns whose dimensions depend on the scale of synthesis and may vary between 0.05 mL and several liters. The overwhelming majority of oligonucleotides are synthesized on small scale ranging from 10 nmol to 1 μmol. More recently, high-throughput oligonucleotide synthesis where the solid support is contained in the wells of multi-well plates (most often, 96 or 384 wells per plate) became a method of choice for parallel synthesis of oligonucleotides on small scale. At the end of the chain assembly, the oligonucleotide is released from the solid support and is eluted from the column or the well.\n\nIn contrast to organic solid-phase synthesis and peptide synthesis, the synthesis of oligonucleotides proceeds best on non-swellable or low-swellable solid supports. The two most often used solid-phase materials are controlled pore glass (CPG) and macroporous polystyrene (MPPS).\n\n\nTo make the solid support material suitable for oligonucleotide synthesis, non-nucleosidic linkers or nucleoside succinates are covalently attached to the reactive amino groups in aminopropyl CPG, LCAA CPG, or aminomethyl MPPS. The remaining unreacted amino groups are capped with acetic anhydride. Typically, three conceptually different groups of solid supports are used.\n\nOligonucleotide phosphorothioates (OPS) are modified oligonucleotides where one of the oxygen atoms in the phosphate moiety is replaced by sulfur. Only the phosphorothioates having sulfur at a non-bridging position as shown in figure are widely used and are available commercially. The replacement of the non-bridging oxygen with sulfur creates a new center of chirality at phosphorus. In a simple case of a dinucleotide, this results in the formation of a diastereomeric pair of S- and R-dinucleoside monophosphorothioates whose structures are shown in Figure. In an \"n\"-mer oligonucleotide where all (\"n\" – 1) internucleosidic linkages are phosphorothioate linkages, the number of diastereomers \"m\" is calculated as \"m\" = 2. Being non-natural analogs of nucleic acids, OPS are substantially more stable towards hydrolysis by nucleases, the class of enzymes that destroy nucleic acids by breaking the bridging P-O bond of the phosphodiester moiety. This property determines the use of OPS as antisense oligonucleotides in \"in vitro\" and \"in vivo\" applications where the extensive exposure to nucleases is inevitable. Similarly, to improve the stability of siRNA, at least one phosphorothioate linkage is often introduced at the 3'-terminus of both sense and antisense strands. In chirally pure OPS, all-Sp diastereomers are more stable to enzymatic degradation than their all-Rp analogs. However, the preparation of chirally pure OPS remains a synthetic challenge. In laboratory practice, mixtures of diastereomers of OPS are commonly used.\n\nSynthesis of OPS is very similar to that of natural oligonucleotides. The difference is that the oxidation step is replaced by sulfur transfer reaction (sulfurization) and that the capping step is performed after the sulfurization. Of many reported reagents capable of the efficient sulfur transfer, only three are commercially available:\n\n\nIn the past, oligonucleotide synthesis was carried out manually in solution or on solid phase. The solid phase synthesis was implemented using, as containers for the solid phase, miniature glass columns similar in their shape to low-pressure chromatography columns or syringes equipped with porous filters.\nCurrently, solid-phase oligonucleotide synthesis is carried out automatically using computer-controlled instruments (oligonucleotide synthesizers) and is technically implemented in column, multi-well plate, and array formats. The column format is best suited for research and large scale applications where a high-throughput is not required. Multi-well plate format is designed specifically for high-throughput synthesis on small scale to satisfy the growing demand of industry and academia for synthetic oligonucleotides. A number of oligonucleotide synthesizers for small scale synthesis and medium to large scale synthesis are available commercially.\n\nIn March 1982 a practical course was hosted by the Department of Biochemistry, Technische Hochschule Darmstadt, Germany. M.H. Caruthers, M.J. Gait, H.G. Gassen, H.Koster, K. Itakura, and C. Birr among others attended. The program comprised practical work, lectures, and seminars on solid-phase chemical synthesis of oligonucleotides. A select group of 15 students attended and had an unprecedented opportunity to be instructed by the esteemed teaching staff. \nAlong with manual exercises, several prominent automation companies attended the course. Biosearch of Novato, CA, Genetic Design of Watertown, MA, were two of several companies to demonstrate automated synthesizers at the course. Biosearch presented their new SAM I synthesizer. The Genetic Design had developed their synthesizer from the design of its sister companies (Sequemat) solid phase peptide sequencer. The Genetic Design arranged with Dr Christian Birr (Max-Planck-Institute for Medical Research) a week before the event to convert his solid phase sequencer into the semi-automated synthesizer. The team led by Dr Alex Bonner and Rick Neves converted the unit and transported it to Darmstadt for the event and installed into the Biochemistry lab at the Technische Hochschule. As the system was semi-automatic, the user injected the next base to be added to the growing sequence during each cycle. The system worked well and produced a series of test tubes filled with bright red trityl color indicating complete coupling at each step. This system was later fully automated by inclusion of an auto injector and was designated the Model 25A.\n\nLarge scale oligonucleotide synthesizers were often developed by augmenting the capabilities of a preexisting instrument platform. One of the first mid scale synthesizers appeared in the late 1980s, manufactured by the Biosearch company in Novato, CA (The 8800). This platform was originally designed as a peptide synthesizer and made use of a fluidized bed reactor essential for accommodating the swelling characteristics of polystyrene supports used in the Merrifield methodology. Oligonucleotide synthesis involved the use of CPG (controlled pore glass) which is a rigid support and is more suited for column reactors as described above. The scale of the 8800 was limited to the flow rate required to fluidize the support. Some novel reactor designs as well as higher than normal pressures enabled the 8800 to achieve scales that would prepare 1 mmole of oligonucleotide. In the mid 1990s several companies developed platforms that were based on semi-preparative and preparative liquid chromatographs. These systems were well suited for a column reactor approach. In most cases all that was required was to augment the number of fluids that could be delivered to the column. Oligo synthesis requires a minimum of 10 and liquid chromatographs usually accommodate 4. This was an easy design task and some semi-automatic strategies worked without any modifications to the preexisting LC equipment. PerSeptive Biosystems as well as Pharmacia (GE) were two of several companies that developed synthesizers out of liquid chromatographs. Genomic Technologies, Inc. was one of the few companies to develop a large scale oligonucleotide synthesizer that was, from the ground up, an oligonucleotide synthesizer. The initial platform called the VLSS for very large scale synthesizer utilized large Pharmacia liquid chromatograph columns as reactors and could synthesize up to 75 millimoles of material. Many oligonucleotide synthesis factories designed and manufactured their own custom platforms and little is known due to the designs being proprietary. The VLSS design continued to be refined and is continued in the QMaster synthesizer which is a scaled down platform providing milligram to gram amounts of synthetic oligonucleotide.\n\nThe current practices of synthesis of chemically modified oligonucleotides on large scale have been recently reviewed.\n\nOne may visualize an oligonucleotide microarray as a miniature multi-well plate where physical dividers between the wells (plastic walls) are intentionally removed. With respect to the chemistry, synthesis of oligonucleotide microarrays is different from the conventional oligonucleotide synthesis in two respects:\n\n\nAfter the completion of the chain assembly, the solid support-bound oligonucleotide is fully protected:\nTo furnish a functional oligonucleotide, all the protecting groups have to be removed. The N-acyl base protection and the 2-cyanoethyl phosphate protection may be, and is often removed simultaneously by treatment with inorganic bases or amines. However, the applicability of this method is limited by the fact that the cleavage of 2-cyanoethyl phosphate protection gives rise to acrylonitrile as a side product. Under the strong basic conditions required for the removal of N-acyl protection, acrylonitrile is capable of alkylation of nucleic bases, primarily, at the N3-position of thymine and uracil residues to give the respective N3-(2-cyanoethyl) adducts via Michael reaction. The formation of these side products may be avoided by treating the solid support-bound oligonucleotides with solutions of bases in an organic solvent, for instance, with 50% triethylamine in acetonitrile or 10% diethylamine in acetonitrile. This treatment is strongly recommended for medium- and large scale preparations and is optional for syntheses on small scale where the concentration of acrylonitrile generated in the deprotection mixture is low.\n\nRegardless of whether the phosphate protecting groups were removed first, the solid support-bound oligonucleotides are deprotected using one of the two general approaches.\n\nAs with any other organic compound, it is prudent to characterize synthetic oligonucleotides upon their preparation. In more complex cases (research and large scale syntheses) oligonucleotides are characterized after their deprotection and after purification. Although the ultimate approach to the characterization is sequencing, a relatively inexpensive and routine procedure, the considerations of the cost reduction preclude its use in routine manufacturing of oligonucleotides. In day-by-day practice, it is sufficient to obtain the molecular mass of an oligonucleotide by recording its mass spectrum. Two methods are currently widely used for characterization of oligonucleotides: electrospray mass spectrometry (ES MS) and matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (MALDI-TOF). To obtain informative spectra, it is very important to exchange all metal ions that might be present in the sample for ammonium or trialkylammonium [\"e.c.\" triethylammonium, (CH)NH] ions prior to submitting a sample to the analysis by either of the methods.\n\n\n", "id": "10663351", "title": "Oligonucleotide synthesis"}
{"url": "https://en.wikipedia.org/wiki?curid=13752880", "text": "URA3\n\nURA3 is a gene on chromosome V in \"Saccharomyces cerevisiae\" (yeast). Its systematic name is YEL021W. URA3 is often used in yeast research as a \"marker gene\", that is, a gene to label chromosomes or plasmids. URA3 encodes Orotidine 5'-phosphate decarboxylase (ODCase), which is an enzyme that catalyzes one reaction in the synthesis of pyrimidine ribonucleotides (a component of RNA).\n\nLoss of ODCase activity leads to a lack of cell growth unless uracil or uridine is added to the media. The presence of the URA3 gene in yeast restores ODCase activity, facilitating growth on media not supplemented with uracil or uridine, thereby allowing selection for yeast carrying the gene. In contrast, if 5-FOA (5-Fluoroorotic acid) is added to the media, the active ODCase will convert 5-FOA into the toxic compound (a suicide inhibitor) 5-fluorouracil causing cell death, which allows for selection against yeast carrying the gene.\n\nSince URA3 allows for both positive and negative selection, it has been developed as a genetic marker for DNA transformations and other genetic techniques in bacteria and many fungal species. It is one of the most important genetic markers in yeast genetic modification. While URA3 is a powerful selectable marker it has a high background. This background is because cells that pick up mutations in URA3 may also grow on 5-FOA. Colonies should be verified by a second assay such as PCR to confirm the desired strain has been created.\n\nS. cerevisiae database (2005) URA3 gene. http://db.yeastgenome.org/cgi-bin/locus.pl?locus=ura3#summaryParagraph (accessed 16/10/07).\n", "id": "13752880", "title": "URA3"}
{"url": "https://en.wikipedia.org/wiki?curid=23376161", "text": "Ligation-independent cloning\n\nLigation-independent cloning (LIC) is a form of molecular cloning that is able to be performed without the use of restriction endonucleases or DNA ligase. This allows genes that have restriction sites to be cloned without worry of chopping up the insert.\n\n\n", "id": "23376161", "title": "Ligation-independent cloning"}
{"url": "https://en.wikipedia.org/wiki?curid=20295637", "text": "PUC19\n\npUC19 is one of a series of plasmid cloning vectors created by Joachim Messing and co-workers. The designation \"pUC\" is derived from the classical \"p\" prefix (denoting \"plasmid\") and the abbreviation for the University of California, where early work on the plasmid series had been conducted. It is a circular double stranded DNA and has 2686 base pairs. pUC19 is one of the most widely used vector molecules as the recombinants, or the cells into which foreign DNA has been introduced, can be easily distinguished from the non-recombinants based on color differences of colonies on growth media. pUC18 is similar to pUC19, but the MCS region is reversed.\n\nIt has one \"amp\" gene (ampicillin resistance gene), and an N-terminal fragment of β-galactosidase (\"lacZ\") gene of \"E. coli\". The multiple cloning site (MCS) region is split into the \"lacZ\" gene (codons 6–7 of \"lacZ\" are replaced by MCS), where various restriction sites for many restriction endonucleases are present. In addition to β-galactosidase, pUC19 also encodes for an enzyme called β-lactamase, which can degrade ampicillin and reduce its toxicity to the host.\n\nThe \"ori\" site, or origin of replication, is derived from the plasmid pMB1. pUC19 is small but has a high copy number. The high copy number is a result of the lack of the \"rop\" gene and a single point mutation in the ori of pMB1 . The \"lacZ\" gene codes for β-galactosidase. The recognition sites for HindIII, SphI, PstI, SalI, XbaI, BamHI, SmaI, KpnI, SacI and EcoRI restriction enzymes have been derived from the vector M13mp19.\n\nThis plasmid is introduced into a bacterial cell by a process called \"transformation\", where it can multiply and express itself. However, due to the presence of MCS and several restriction sites, a foreign piece of DNA of choice can be introduced into it by inserting it into place in MCS region. The cells which have taken up the plasmid can be differentiated from cells which have not taken up the plasmid by growing it on media with ampicillin. Only the cells with the plasmid containing the ampicillin resistance (\"amp\") gene will survive. Furthermore, the transformed cells containing the plasmid with the gene of our interest can be distinguished from cells with the plasmid but without the gene of interest, just by looking at the color of the colony they make on agar media supplemented with IPTG and X-gal. Recombinants are white, whereas non-recombinants are blue. This is the most notable feature of pUC19.\n\nThe \"lac Z\" fragment, whose synthesis can be induced by IPTG, is capable of intra-allelic complementation with a defective form of β-galactosidase enzyme encoded by host chromosome (mutation lacZDM15 in E. coli JM109, DH5α and XL1-Blue strains). In the presence of IPTG in growth medium, bacteria synthesise both fragments of the enzyme. Both the fragments can together hydrolyse X-gal (5-bromo-4-chloro-3-indolyl- beta-D-galactopyranoside) and form blue colonies when grown on media where it is supplemented.\n\nInsertion of foreign DNA into the MCS located within the \"lac Z\" gene causes insertional inactivation of this gene at the N-terminal fragment of beta-galactosidase and abolishes intra-allelic complementation. Thus bacteria carrying recombinant plasmids in the MCS cannot hydrolyse X-gal, giving rise to white colonies, which can be distinguished on culture media from non-recombinant cells, which are blue.\n\nTherefore, the media used should contain ampicillin, IPTG, and X-gal.\n\nDue to its extensive use as a cloning vector in research and industry, pUC19 is frequently used in research as a model plasmid. For example, biophysical studies on its naturally supercoiled state have determined its radius of gyration to be 65.6 nm and its Stokes radius to be 43.6 nm.\n\n\n", "id": "20295637", "title": "PUC19"}
{"url": "https://en.wikipedia.org/wiki?curid=10388309", "text": "BamHI\n\n\"Bam\"H I (from \"Bacillus amyloliquefaciens\") is a type II restriction endonuclease, having the capacity for recognizing short sequences (6 b.p.) of DNA and specifically cleaving them at a target site. This exhibit focuses on the structure-function relations of \"Bam\"H I as described by Newman, et al. (1995). \"Bam\"H I binds at the recognition sequence 5'-GGATCC-3', and cleaves these sequences just after the 5'-guanine on each strand. This cleavage results in sticky ends which are 4 b.p. long. In its unbound form, \"Bam\"H I displays a central b sheet, which resides in between α-helices. \"Bam\"H I is an extraordinarily unique molecule in that it undergoes a series of unconventional conformational changes upon DNA recognition. This allows the DNA to maintain its normal B-DNA conformation without distorting to facilitate enzyme binding. \"Bam\"H I is a symmetric dimer. DNA is bound in a large cleft that is formed between dimers; the enzyme binds in a \"crossover\" manner. Each \"Bam\"H I subunit makes the majority of its backbone contacts with the phosphates of a DNA half site but base pair contacts are made between each \"Bam\"H I subunit and nitrogenous bases in the major groove of the opposite DNA half site. The protein binds the bases through either direct hydrogen bonds or water-mediated H-bonds between the protein and every H-bond donor/acceptor group in the major groove. Major groove contacts are formed by atoms residing on the amino-terminus of a parallel 4 helix bundle. This bundle marks the \"Bam\"H I dimer interface, and it is thought that the dipole moments of the NH2-terminal atoms on this bundle may contribute to electrostatic stabilization.\n\nThe \"Bam\"H I enzyme is capable of making a large number of contacts with DNA. Water-mediated hydrogen bonding, as well as both main-chain and side-chain interactions aid in binding of the \"Bam\"H I recognition sequence. In the major groove, the majority of enzyme/DNA contacts take place at the amino terminus of the parallel-4-helix bundle, made up of a4 and a6 from each subunit. Although a6 from each subunit does not enter the DNA major groove, its preceding loops interact with the outer ends of the recognition site. Conversely, a4 from each subunit does enter the major groove in the center of the recognition sequence. A total of 18 bonds are formed between the enzyme and DNA across the 6 base pair recognition sequence (12 direct and 6 water mediated bonds). Arg155 and Asp154 located in a spiral ring before a6 are connected with G:C base pairs outside while the middle G:C pairs are connected with Asp154, Arg122, and Asn116 (direct binding). Hydrogen bonding between water and Asn116 results in binding at A:T base pairs inside (water-mediated binding). As discussed above, the L and R subunits bind in a cross over manner, whereby the R-subunit of \"Bam\"H I contacts the left DNA half-site of the recognition sequence. The binding of each \"Bam\"H I subunit is precisely the same as its symmetrical partner. The recognition site for \"Bam\"H I has a palindromic sequence which can be cut in half for ease in showing bonds.\n\nAs of the end of 2010, there were 5 crystal structures of \"Bam\"H I in the Protein Data Bank\n\nBamHI, type II restriction endonucleases, often requires divalent metals as cofactors to catalyze DNA cleavage. Two-metal ion mechanism is one of the possible catalytic mechanisms of BamHI since the BamHI crystal structure has the ability to bind two metal ions at the active site, which is suitable for the classical two-metal ion mechanism to proceed. Two-metal ion mechanism is the use of two metal ions to catalyze the cleavage reaction of restriction enzyme. BamHI has three critical active site residues that are important for metal catalyst. They are known as Asp94, Glu111 and Glu113. These residues are usually acidic. In the presence of a metal ion, the residues are pointed toward the metal ion. In the absence of metal ions, the residues are pointed outward. The two metal ions (A and B) are 4.1 apart from each other in the active site and are in-line with these residues. In general, when the two metal ions (A and B) are bonded to the active site, they help stabilize a cluster distribution of negative charges localized at the active site created by the leaving of an oxygen atom during the transition state. First, a water molecule will be activated by metal ion A at the active site. This water molecule will act as the attacking molecule attacking the BamHI-DNA complex and thus making the complex negative. Later, another water will bound to metal ion B and donate a proton to the leaving group of complex, stabilizing the build-up of negative charge on the leaving oxygen atom.\n\nThe function of Ca2+ in the active site of BamHI is known. It is an inhibitor of DNA cleavage, converting BamHI into the pre-reactive state. This revealed the water molecular is the attacking molecule. It donates a proton to the leaving group that is bounded to Ca2+ forming a 90o O-P-O bond angles. If Glu 113 is replaced by lysine, the cleavage is lost since Glu 113 accepts the proton from the attacking water molecule.\n\nBecause of its ability to recognize specific DNA sequence and cleave by a nuclease, \"Bam\"H I carries various importances in understanding Type II restriction endonuclease, cloning DNA, and possibly treating certain DNA mutation-derived diseases through genetic therapy. NARP and MILS syndromes, for example, are mitochondrial diseases that can be caused by mutations in the mitochondrial DNA. Mitochondria can recover its functions after the excision of the mutant sequence through restriction endonuclease.\n\n\n", "id": "10388309", "title": "BamHI"}
{"url": "https://en.wikipedia.org/wiki?curid=21728811", "text": "Methylated DNA immunoprecipitation\n\nMethylated DNA immunoprecipitation (MeDIP or mDIP) is a large-scale (chromosome- or genome-wide) purification technique in molecular biology that is used to enrich for methylated DNA sequences. It consists of isolating methylated DNA fragments via an antibody raised against 5-methylcytosine (5mC). This technique was first described by Weber M. \"et al.\" in 2005 and has helped pave the way for viable methylome-level assessment efforts, as the purified fraction of methylated DNA can be input to high-throughput DNA detection methods such as high-resolution DNA microarrays (MeDIP-chip) or next-generation sequencing (MeDIP-seq). Nonetheless, understanding of the methylome remains rudimentary; its study is complicated by the fact that, like other epigenetic properties, patterns vary from cell-type to cell-type.\n\nDNA methylation, referring to the reversible methylation of the 5 position of cytosine by methyltransferases, is a major epigenetic modification in multicellular organisms. In mammals, this modification primarily occurs at CpG sites, which in turn tend to cluster in regions called CpG islands. There is a small fraction of CpG islands that can overlap or be in close proximity to promoter regions of transcription start sites. The modification may also occur at other sites, but methylation at either of these sites can repress gene expression by either interfering with the binding of transcription factors or modifying chromatin structure to a repressive state.\n\nDisease condition studies have largely fueled the effort in understanding the role of DNA methylation. Currently, the major research interest lies in investigating disease conditions such as cancer to identify regions of the DNA that has undergone extensive methylation changes. The genes contained in these regions are of functional interest as they may offer a mechanistic explanation to the underlying genetic causes of a disease. For instance, the abnormal methylation pattern of cancer cells was initially shown to be a mechanism through which tumor suppressor-like genes are silenced, although it was later observed that a much broader range of gene types are affected.\n\nThere are two approaches to methylation analysis: typing and profiling technologies. Typing technologies are targeted towards a small number of loci across many samples, and involve the use of techniques such as PCR, restriction enzymes, and mass spectrometry. Profiling technologies such as MeDIP are targeted towards a genome- or methylome-wide level assessment of methylation; this includes restriction landmark genomic scanning (RLGS), and bisulfite conversion-based methods, which rely on the treatment of DNA with bisulfite to convert unmethylated cytosine residues to uracil.\n\nOther methods mapping and profiling the methylome have been effective but are not without their limitations that can affect resolution, level of throughput, or experimental variations. For instance, RLGS is limited by the number of restriction sites in genome that can be targets for the restriction enzyme; typically, a maximum of ~4100 landmarks can be assessed. Bisulfite sequencing-based methods, despite possible single-nucleotide resolution, have a drawback: the conversion of unmethylated cytosine to uracil can be unstable. In addition, when bisulfite conversion is coupled with DNA microarrays to detect bisulfite converted sites, the reduced sequence complexity of DNA is a problem. Microarrays capable of comprehensively profiling the whole-genome become difficult to design as fewer unique probes are available.\n\nThe following sections outline the method of MeDIP coupled with either high-resolution array hybridization or high-throughput sequencing. Each DNA detection method will also briefly describe post-laboratory processing and analysis. Different post-processing of the raw data is required depending on the technology used to identify the methylated sequences. This is analogous to data generated using ChIP-chip and ChIP-seq.\n\nGenomic DNA is extracted (DNA extraction) from the cells and purified. The purified DNA is then subjected to sonication to shear it into random fragments. This sonication process is quick, simple, and avoids restriction enzyme biases. The resulting fragments range from 300 to 1000 base pairs (bp) in length, although they are typically between 400 and 600 bp. The short length of these fragments is important in obtaining adequate resolution, improving the efficiency of the downstream step in immunoprecipitation, and reducing fragment-length effects or biases. Also, the size of the fragment affects the binding of 5-methyl-cytidine (5mC) antibody because the antibody needs more than just a single 5mC for efficient binding. To further improve binding affinity of the antibodies, the DNA fragments are denatured to produce single-stranded DNA. Following denaturation, the DNA is incubated with monoclonal 5mC antibodies. The classical immunoprecipitation technique is then applied: magnetic beads conjugated to anti-mouse-IgG are used to bind the anti-5mC antibodies, and unbound DNA is removed in the supernatant. To purify the DNA, proteinase K is added to digest the antibodies and release the DNA, which can be collected and prepared for DNA detection.\n\nFor more details regarding the experimental steps see.\n\nA fraction of the input DNA obtained after the sonication step above is labeled with cyanine-5 (Cy5; red) deoxy-cytosine-triphosphate while the methylated DNA, enriched after the immunoprecipitation step, is labeled with cyanine-3 (Cy3; green). The labeled DNA samples are cohybridized on a 2-channel, high-density genomic microarray to probe for presence and relative quantities. The purpose of this comparison is to identify sequences that show significant differences in hybridization levels, thereby confirming the sequence of interest is enriched. \nArray-based identification of MeDIP sequences are limited to the array design. As a result, the resolution is restricted to the probes in the array design. There are additional standard steps required in signal processing to correct for hybridization issues such as noise, as is the case with most array technologies.\n\nSee for more details.\n\nThe MeDIP-seq approach, i.e. the coupling of MeDIP with next generation, short-read sequencing technologies such as 454 pyrosequencing or Illumina (Solexa), was first described by Down \"et al.\" in 2008. The high-throughput sequencing of the methylated DNA fragments produces a large number of short reads (36-50bp or 400 bp, depending on the technology). The short reads are aligned to a reference genome using alignment software such as Mapping and Assembly with Quality (Maq), which uses a Bayesian approach, along with base and mapping qualities to model error probabilities for the alignments. The reads can then be extended to represent the ~400 to 700 bp fragments from the sonication step. The coverage of these extended reads can be used to estimate the methylation level of the region. A genome browser such as Ensembl can also be used to visualize the data.\n\nValidation of the approach to assess quality and accuracy of the data can be done with quantitative PCR. This is done by comparing a sequence from the MeDIP sample against an unmethylated control sequence. The samples are then run on a gel and the band intensities are compared. The relative intensity serves as the guide for finding enrichment. The results can also be compared with MeDIP-chip results to help determine coverage needed.\n\nThe DNA methylation level estimations can be confounded by varying densities of methylated CpG sites across the genome when observing data generated by MeDIP. This can be problematic for analyzing CpG-poor (lower density) regions. One reason for this density issue is its effect on the efficiency of immunoprecipitation. In their study, Down \"et al.\" developed a tool to estimate absolute methylation levels from data generated by MeDIP by modeling the density of methylated CpG sites. This tool is called Bayesian tool for methylation analysis (Batman). The study reports the coverage of ~90% of all CpG sites in promoters, gene-coding regions, islands, and regulatory elements where methylation levels can be estimated; this is almost 20 times better coverage than any previous methods.\n\nStudies using MeDIP-seq or MeDIP-chip are both genome-wide approaches that have the common aim of obtaining the functional mapping of the methylome. Once regions of DNA methylation are identified, a number of bioinformatics analyses can be applied to answer certain biological questions. One obvious step is to investigate genes contained in these regions and investigate the functional significance of their repression. For example, silencing of tumour-suppressor genes in cancer can be attributed to DNA methylation. By identifying mutational events leading to hypermethylation and subsequent repression of known tumour-suppressor genes, one can more specifically characterize the contributing factors to the cause of the disease. Alternatively, one can identify genes that are known to be normally methylated but, as a result of some mutation event, is no longer silenced.\n\nAlso, one can try and investigate and identify whether some epigenetic regulator has been affected such as DNA methyltransferase (DNMT); in these cases, enrichment may be more limited.\n\nGene-set analysis (for example using tools like DAVID and GoSeq) has been shown to be severely biased when applied to high-throughput methylation data (e.g. MeDIP-seq and MeDIP-ChIP); it has been suggested that this can be corrected using sample label permutations or using a statistical model to control for differences in the numberes of CpG probes / CpG sites that target each gene.\n\nLimitations to take note when using MeDIP are typical experimental factors. This includes the quality and cross-reactivity of 5mC antibodies used in the procedure. Furthermore, DNA detection methods (i.e. array hybridization and high-throughput sequencing) typically involve well established limitations. Particularly for array-based procedures, as mentioned above, sequences being analyzed are limited to the specific array design used.\n\nMost typical limitations to high-throughput, next generation sequencing apply. The problem of alignment accuracy to repetitive regions in the genome will result in less accurate analysis of methylation in those regions. Also, as was mentioned above, short reads (e.g. 36-50bp from an Illumina Genome Analyzer) represent a part of a sheared fragment when aligned to the genome; therefore, the exact methylation site can fall anywhere within a window that is a function of the fragment size. In this respect, bisulfite sequencing has much higher resolution (down to a single CpG site; single nucleotide level). However, this level of resolution may not be required for most applications, as the methylation status of CpG sites within < 1000 bp has been shown to be significantly correlated.\n\n\n", "id": "21728811", "title": "Methylated DNA immunoprecipitation"}
{"url": "https://en.wikipedia.org/wiki?curid=6737235", "text": "Molecular medicine\n\nMolecular medicine is a broad field, where physical, chemical, biological, bioinformatics and medical techniques are used to describe molecular structures and mechanisms, identify fundamental molecular and genetic errors of disease, and to develop molecular interventions to correct them. The concept of the distribution of medicine to each individual cell just as oxygen would be an example of the practice of molecular medicine (via lung chemical reaction manipulation an idea coined by Saniab Jacob). The molecular medicine perspective emphasizes cellular and molecular phenomena and interventions rather than the previous conceptual and observational focus on patients and their organs.\n\nIn November 1949, with the seminal paper, \"Sickle Cell Anemia, a Molecular Disease\", in \"Science\" magazine, Linus Pauling, Harvey Itano and their collaborators laid the groundwork for establishing the field of molecular medicine. In 1956, Roger J. Williams wrote \"Biochemical Individuality\", a prescient book about genetics, prevention and treatment of disease on a molecular basis, and nutrition which is now variously referred to as individualized medicine and orthomolecular medicine. Another paper in \"Science\" by Pauling in 1968, introduced and defined this view of molecular medicine that focuses on natural and nutritional substances used for treatment and prevention.\n\nPublished research and progress was slow until the 1970s' \"biological revolution\" that introduced many new techniques and commercial applications.\n\nSome researchers separate molecular surgery as a compartment of molecular medicine.\n\nMolecular medicine is a new scientific discipline in European universities. Combining contemporary medical studies with the field of biochemistry, it offers a bridge between the two subjects. At present only a handful of universities offer the course to undergraduates. With a degree in this discipline the graduate is able to pursue a career in medical sciences, scientific research, laboratory work and postgraduate medical degrees.\n\nCore subjects are similar to biochemistry courses and typically include gene expression, research methods, proteins, cancer research, immunology, biotechnology and many more. In some universities molecular medicine is combined with another discipline such as chemistry, functioning as an additional study to enrich the undergraduate program.\n\n\n", "id": "6737235", "title": "Molecular medicine"}
{"url": "https://en.wikipedia.org/wiki?curid=23929075", "text": "Massively parallel signature sequencing\n\nMassive parallel signature sequencing (MPSS) is a procedure that is used to identify and quantify mRNA transcripts, resulting in data similar to serial analysis of gene expression (SAGE), although it employs a series of biochemical and sequencing steps that are substantially different.\n\nMPSS is a method for determining expression levels of mRNA by counting the number of individual mRNA molecules produced by each gene. It is \"open ended\" in the sense that the identity of the RNAs to be measured are not pre-determined as they are with gene expression microarrays.\n\nA sample of mRNA are first converted to complementary DNA (cDNA) using reverse transcriptase, which makes subsequent manipulations easier. These cDNA are fused to a small oligonucleotide \"tag\" which allows the cDNA to be PCR amplified and then coupled to microbeads. After several rounds of sequence determination, using hybridization of fluorescent labeled probes, a sequence signature of ~16-20 bp is determined from each bead. Fluorescent imaging captures the signal from all of the beads, while affixed to a 2-dimensional surface, so DNA sequences are determined from all the beads in parallel. There is some amplification of the starting material so, in the end, approximately 1,000,000 sequence reads are obtained per experiment.\n\nMPSS allows mRNA transcripts to be identified through the generation of a 17-20 bp (base pair) signature sequence adjacent to the 3’-end of the 3’- most site of the designated restriction enzyme (commonly Sau3A or DpnII). Each signature sequence is cloned onto one of a million microbeads. The technique ensures that only one type of DNA sequence is on a microbead. So if there are 50 copies of a specific transcript in the biological sample, these transcripts will be captured onto 50 different microbeads, each bead holding roughly 100,000 amplified copies of the specific signature sequence. The microbeads are then arrayed in a flow cell for sequencing and quantification. The sequence signatures are deciphered by the parallel identification of four bases by hybridization to fluorescently labeled encoders (Figure 5). Each of the encoders has a unique label which is detected after hybridization by taking an image of the microbead array. The next step is to cleave and remove that set of four bases and reveal the next four bases for a new round of hybridization to encoders and image acquisition. The raw output is a list of 17-20 bp signature sequences, that can be annotated to the human genome for gene identification.\n\nThe longer tag sequence confers a higher specificity than the classical SAGE tag of 9-10 bp. The level of unique gene expression is represented by the count of transcripts present per million molecules, similar to SAGE output. A significant advantage is the larger library size compared with SAGE. An MPSS library typically holds 1 million signature tags, which is roughly 20 times the size of a SAGE library. Some of the disadvantages related to SAGE apply to MPSS as well, such as loss of certain transcripts due to lack of restriction enzyme recognition site and ambiguity in tag annotation. The high sensitivity and absolute gene expression certainly favors MPSS. However, the technology is only available through Lynxgen Therapeutics, Inc. (then Solexa Inc till 2006 and then Illumina).\n\n\n", "id": "23929075", "title": "Massively parallel signature sequencing"}
{"url": "https://en.wikipedia.org/wiki?curid=24514868", "text": "Consensus site\n\nA consensus site is a term in molecular biology that refers to a site on a protein that is often modified in a particular way. Modifications may be N- or O- linked glycosylation, phosphorylation, tyrosin sulfation or other.\n", "id": "24514868", "title": "Consensus site"}
{"url": "https://en.wikipedia.org/wiki?curid=10094209", "text": "Photoreceptor protein\n\nPhotoreceptor proteins are light-sensitive proteins involved in the sensing and response to light in a variety of organisms. Some examples are rhodopsin in the photoreceptor cells of the vertebrate retina, phytochrome in plants, and bacteriorhodopsin and bacteriophytochromes in some bacteria. They mediate light responses as varied as visual perception, phototropism and phototaxis, as well as responses to light-dark cycles such as circadian rhythm and other photoperiodisms including control of flowering times in plants and mating seasons in animals.\n\nPhotoreceptor proteins typically consist of a protein moiety and a non-protein photopigment that reacts to light via photoisomerization or photoreduction, thus initiating a change of the receptor protein which triggers a signal transduction cascade. Pigments found in photoreceptors include retinal (retinylidene proteins, for example rhodopsin in animals), flavin (flavoproteins, for example cryptochrome in plants and animals) and bilin (biliproteins, for example phytochrome in plants).\n\n\"(Also see: Photoreceptor cell)\"\n\nIn plant seeds, the photoreceptor phytochrome is responsible for the process termed photomorphogenesis. This occurs when a seed initially situated in an environment of complete darkness is exposed to light. A brief exposure to electromagnetic radiation, particularly that whose wavelength is within the red and far-red lights, results in the activation of the photorecepter phytochrome within the seed. This in turn sends a signal through the signal transduction pathway into the nucleus, and triggers hundreds of genes responsible for growth and development.\n\n\"(Also see: Eyespot apparatus)\"\n\n\n\n", "id": "10094209", "title": "Photoreceptor protein"}
{"url": "https://en.wikipedia.org/wiki?curid=1708412", "text": "Restriction digest\n\nA restriction digest is a procedure used in molecular biology to prepare DNA for analysis or other processing. It is sometimes termed \"DNA fragmentation\" (this term is used for other procedures as well). Hartl and Jones describe it this way: \nThis enzymatic technique can be used for cleaving DNA molecules at specific sites, ensuring that all DNA fragments that contain a particular sequence at a particular location have the same size; furthermore, each fragment that contains the desired sequence has the sequence located at exactly the same position within the fragment. The cleavage method makes use of an important class of DNA-cleaving enzymes isolated primarily from bacteria. These enzymes are called restriction endonucleases or restriction enzymes, and they are able to cleave DNA molecules at the positions at which particular short sequences of bases are present. \n\nThe resulting digested DNA is very often selectively amplified using PCR, making it more suitable for analytical techniques such as agarose gel electrophoresis, and chromatography. It is used in genetic fingerprinting, plasmid subcloning, and RFLP analysis. \n\nA given restriction enzyme cuts DNA segments within a specific nucleotide sequence, at what is called a restriction site. These \"recognition sequences\" are typically four, six, eight, ten, or twelve nucleotides long and generally palindromic (i.e. the same nucleotide sequence in the 5' - 3' direction). Because there are only so many ways to arrange the four nucleotides that compose DNA (Adenine, Thymine, Guanine and Cytosine) into a four- to twelve-nucleotide sequence, recognition sequences tend to occur by chance in any long sequence. Restriction enzymes specific to hundreds of distinct sequences have been identified and synthesized for sale to laboratories, and as a result, several potential \"restriction sites\" appear in almost any gene or locus of interest on any chromosome. Furthermore, almost all artificial plasmids include a (often entirely synthetic) polylinker (also called \"multiple cloning site\") that contains dozens of restriction enzyme recognition sequences within a very short segment of DNA. This allows the insertion of almost any specific fragment of DNA into plasmid vectors, which can be efficiently \"cloned\" by insertion into replicating bacterial cells. \n\nAfter restriction digest, DNA can then be analysed using agarose gel electrophoresis. In gel electrophoresis, a sample of DNA is first \"loaded\" onto a slab of agarose gel (literally pipetted into small wells at one end of the slab). The gel is then subjected to an electric field, which draws the negatively charged DNA across it. The molecules travel at different rates (and therefore end up at different distances) depending on their net charge (more highly charged particles travel further), and size (smaller particles travel further). Since none of the four nucleotide bases carry any charge, net charge becomes insignificant and size is the main factor affecting rate of diffusion through the gel. Net charge in DNA is produced by the sugar-phosphate backbone. This is in contrast to proteins, in which there is no \"backbone\", and net charge is generated by different combinations and numbers of charged amino acids.\n\nRestriction digest is most commonly used as part of the process of the molecular cloning of DNA fragment into a vector (such as a cloning vector or an expression vector). The vector typically contains a multiple cloning site where many restriction site may be found, and a foreign piece of DNA may be inserted into the vector by first cutting the restriction sites in the vector as well the DNA fragment, followed by ligation of the DNA fragment into the vector.\n\nRestriction digests are also necessary for performing any of the following analytical techniques:\n\nThere are numerous types of restriction enzymes, each of which will cut DNA differently. Most commonly used restriction enzymes are Type II restriction endonuclease (See article on Restriction enzymes for examples). There are some that cut a three base pair sequence while others can cut four, six, and even eight. Each enzyme has distinct properties that determine how efficiently it can cut and under what conditions. Most manufacturers that produce such enzymes will often provide a specific buffer solution that contains the unique mix of cations and other components that aid the enzyme in cutting as efficiently as possible. Different restriction enzymes may also have different optimal temperatures under which they function. \n\nNote that for efficient digest of DNA, the restriction site should not be located at the very end of a DNA fragment. The restriction enzymes may require a minimum number of base pairs between the restriction site and the end of the DNA for the enzyme to work efficiently. This number may vary between enzymes, but for most commonly used restriction enzymes around 6-10 base pair is sufficient.\n\n\n", "id": "1708412", "title": "Restriction digest"}
{"url": "https://en.wikipedia.org/wiki?curid=14346042", "text": "Double-stranded RNA viruses\n\nDouble-stranded (ds) RNA viruses are a diverse group of viruses that vary widely in host range (humans, animals, plants, fungi, and bacteria), genome segment number (one to twelve) and virion organization (T-number, capsid layers or turrets). Members of this group include the rotaviruses, known globally as a common cause of gastroenteritis in young children, and bluetongue virus, an economically important pathogen of cattle and sheep.\n\nOf these families, the \"Reoviridae\" is the largest and most diverse in terms of host range.\n\nIn recent years the increasing knowledge of virus particle assembly, virus-cell interactions, and viral pathogenesis allow approaches for the development of novel antiviral strategies or agents.\n\nViruses with dsRNA genomes are currently grouped into a number of families, unassigned genera and species.\n\nThree families infect fungi: \"Totiviridae\", \"Partitiviridae\" and \"Chrysoviridae\". These families have monopartite, bipartite and quadripartite genomes respectively. They are typically isometric particles 25–50 nanometers in diameter. Based on sequence similarity of the RNA dependent RNA polymerase, the partitiviruses are probably derived from a totivirus ancestor. A fourth family — \"Alternaviridae\" — has recently been described also with quadripartite genome.\n\nHypoviruses are mycoviruses (fungal viruses) with unencapsidated dsRNA genomes. They may have common ancestry with plant positive strand RNA viruses in supergroup 1 with potyvirus lineages, respectively\n\nA new clade (as yet unnamed) of six viruses infecting filamentous fungi has been reported.\n\nFamilies\n\n\nUnassigned species\n\n\"Reoviridae\" are currently classified into nine genera. The genomes of these viruses consist of 10 to 12 segments of dsRNA, each generally encoding one protein. The mature virions are non-enveloped. Their capsids, formed by multiple proteins, have icosahedral symmetry and are arranged generally in concentric layers. A distinguishing feature of the dsRNA viruses, irrespective of their family association, is their ability to carry out transcription of the dsRNA segments, under appropriate conditions, within the capsid. In all these viruses, the enzymes required for endogenous transcription are thus part of the virion structure.\n\nThe orthoreoviruses (reoviruses) are the prototypic members of the virus \"Reoviridae\" family and representative of the turreted members, which comprise about half the genera. Like other members of the family, the reoviruses are non-enveloped and characterized by concentric capsid shells that encapsidate a segmented dsRNA genome. In particular, reovirus has eight structural proteins and ten segments of dsRNA. A series of uncoating steps and conformational changes accompany cell entry and replication. High-resolution structures are known for almost all of the proteins of mammalian reovirus (MRV), which is the best-studied genotype. Electron cryo-microscopy (cryoEM) and X-ray crystallography have provided a wealth of structural information about two specific MRV strains, type 1 Lang (T1L) and type 3 Dearing (T3D).\n\nThe cytoplasmic polyhedrosis viruses (CPVs) form the genus Cypovirus of the family \"Reoviridae\". CPVs are classified into 14 species based on the electrophoretic migration profiles of their genome segments. Cypovirus has only a single capsid shell, which is similar to the orthoreovirus inner core. CPV exhibits striking capsid stability and is fully capable of endogenous RNA transcription and processing. The overall folds of CPV proteins are similar to those of other reoviruses. However, CPV proteins have insertional domains and unique structures that contribute to their extensive intermolecular interactions. The CPV turret protein contains two methylase domains with a highly conserved helix-pair/β-sheet/helix-pair sandwich fold but lacks the β-barrel flap present in orthoreovirus λ2. The stacking of turret protein functional domains and the presence of constrictions and A spikes along the mRNA release pathway indicate a mechanism that uses pores and channels to regulate the highly coordinated steps of RNA transcription, processing, and release.\n\nRotavirus is the most common cause of acute gastroenteritis in infants and young children worldwide. This virus contains a dsRNA genome and is a member of the \"Reoviridae\" family. The genome of rotavirus consists of eleven segments of dsRNA. Each genome segment codes for one protein with the exception of segment 11, which codes for two proteins. Among the twelve proteins, six are structural and six are non-structural proteins.\nIt is a double-stranded RNA non-enveloped virus\n\nThe members of genus \"Orbivirus\" within the \"Reoviridae\" family are arthropod borne viruses and are responsible for high morbidity and mortality in ruminants. Bluetongue virus (BTV) which causes disease in livestock (sheep, goat, cattle) has been in the forefront of molecular studies for the last three decades and now represents the best understood orbivirus at the molecular and structural levels. BTV, like other members of the family, is a complex non-enveloped virus with seven structural proteins and a RNA genome consisting of 10 variously sized dsRNA segments.\n\nPhytoreoviruses are non-turreted reoviruses that are major agricultural pathogens, particularly in Asia. One member of this family, Rice Dwarf Virus (RDV), has been extensively studied by electron cryomicroscopy and x-ray crystallography. From these analyses, atomic models of the capsid proteins and a plausible model for capsid assembly have been derived. While the structural proteins of RDV share no sequence similarity to other proteins, their folds and the overall capsid structure are similar to those of other \"Reoviridae\".\n\nThe L-A dsRNA virus of the yeast \"Saccharomyces cerevisiae\" has a single 4.6 kb genomic segment that encodes its major coat protein, Gag (76 kDa) and a Gag-Pol fusion protein (180 kDa) formed by a -1 ribosomal frameshift. L-A can support the replication and encapsidation in separate viral particles of any of several satellite dsRNAs, called M dsRNAs, each of which encodes a secreted protein toxin (the killer toxin) and immunity to that toxin. L-A and M are transmitted from cell to cell by the cytoplasmic mixing that occurs in the process of mating. Neither is naturally released from the cell or enters cells by other mechanisms, but the high frequency of yeast mating in nature results in the wide distribution of these viruses in natural isolates. Moreover, the structural and functional similarities with dsRNA viruses of mammals has made it useful to consider these entities as viruses.\n\nInfectious bursal disease virus (IBDV) is the best-characterized member of the family \"Birnaviridae\". These viruses have bipartite dsRNA genomes enclosed in single layered icosahedral capsids with \"T\" = 13l geometry. IBDV shares functional strategies and structural features with many other icosahedral dsRNA viruses, except that it lacks the \"T\" = 1 (or pseudo \"T\" = 2) core common to the \"Reoviridae\", \"Cystoviridae\", and \"Totiviridae\". The IBDV capsid protein exhibits structural domains that show homology to those of the capsid proteins of some positive-sense single-stranded RNA viruses, such as the nodaviruses and tetraviruses, as well as the \"T\" = 13 capsid shell protein of the \"Reoviridae\". The \"T\" = 13 shell of the IBDV capsid is formed by trimers of VP2, a protein generated by removal of the C-terminal domain from its precursor, pVP2. The trimming of pVP2 is performed on immature particles as part of the maturation process. The other major structural protein, VP3, is a multifunctional component lying under the \"T\" = 13 shell that influences the inherent structural polymorphism of pVP2. The virus-encoded RNA-dependent RNA polymerase, VP1, is incorporated into the capsid through its association with VP3. VP3 also interacts extensively with the viral dsRNA genome.\n\nBacteriophage Φ6, is a member of the \"Cystoviridae\" family. It infects \"Pseudomonas\" bacteria (typically plant-pathogenic \"P. syringae\"). It has a three-part, segmented, double-stranded RNA genome, totalling ~13.5 kb in length. Φ6 and its relatives have a lipid membrane around their nucleocapsid, a rare trait among bacteriophages. It is a lytic phage, though under certain circumstances has been observed to display a delay in lysis which may be described as a \"carrier state\".\n\nSince cells do not produce double-stranded RNA during normal nucleic acid metabolism, natural selection has favored the evolution of enzymes that destroy dsRNA on contact. The best known class of this type of enzymes is Dicer. It is hoped that broad-spectrum anti-virals could be synthesized that take advantage of this vulnerability of double-stranded RNA viruses.\n\n", "id": "14346042", "title": "Double-stranded RNA viruses"}
{"url": "https://en.wikipedia.org/wiki?curid=13958600", "text": "Rotavirus translation\n\nRotavirus translation, the process of translating mRNA into proteins, occurs in a different way in Rotaviruses. Unlike the vast majority of cellular proteins in other organisms, in Rotaviruses the proteins are translated from capped but nonpolyadenylated mRNAs. The viral nonstructural protein NSP3 specifically binds the 3'-end consensus sequence of viral mRNAs and interacts with the eukaryotic translation initiation factor eIF4G. The Rotavirus replication cycle occurs entirely in the cytoplasm. Upon virus entry, the viral transcriptase synthesizes capped but nonpolyadenylated mRNA The viral mRNAs bear 5' and 3' untranslated regions (UTR) of variable length and are flanked by two different sequences common to all genes.\n\nIn the group A rotaviruses, the 3'-end consensus sequence UGACC is highly conserved among the 11 genes. Rotavirus NSP3 presents several similarities to PABP; in rotavirus-infected cells, NSP3 can be cross-linked to the 3' end of rotavirus mRNAs and is coimmunoprecipitated with eIF4G. The binding of NSP3A to eIF4G and its specific interaction with the 3' end of viral mRNA brings the viral mRNA and the translation initiation machinery into contact, thus favoring efficient translation of the viral mRNA. NSP3 interacts with the same region of eIF4G as PABP does. As a consequence, during rotavirus infection PABP is evicted from eIF4G, probably impairing the translation of polyadenylated mRNA and leading to the shutoff of cellular mRNA translation observed during rotavirus infection.\n", "id": "13958600", "title": "Rotavirus translation"}
{"url": "https://en.wikipedia.org/wiki?curid=14555613", "text": "Molecular microbiology\n\nMolecular microbiology is the branch of microbiology devoted to the study of the molecular basis of the physiological processes that occur in microorganisms.\n\nMainly because of their relative simplicity, ease of manipulation and growth in vitro, and importance in medicine, bacteria were instrumental in the development of molecular biology. The complete genome sequence for a large number of bacterial species is now available. A list of sequenced prokaryotic genomes is available. Molecular microbiology techniques are currently being used in the development of new genetically engineered vaccines, in bioremediation, biotechnology, food microbiology, probiotic research, antibacterial development and environmental microbiology.\n\nMany bacteria have become model organisms for molecular studies.\n\nMolecular techniques have had a direct influence on the clinical practice of medical microbiology. In many cases where traditional phenotypic methods of microbial identification and typing are insufficient or time-consuming, molecular techniques can provide rapid and accurate data, potentially improving clinical outcomes. Specific examples include:\n\nBacteria possess diverse proteins and RNA that can sense changes to their intracellular and extracellular environment. The signals received by these macromolecules are transmitted to key genes or proteins, which alter their activities to suit the new conditions.\n\nViruses are important pathogens of humans and animals. Their genomes are relatively small. For these reasons they were among the first organisms to be fully sequenced. The complete DNA sequence of the Epstein-Barr virus was completed in 1984. Bluetongue virus (BTV) has been in the forefront of molecular studies for last three decades and now represents one of the best understood viruses at the molecular and structural levels. Other viruses such as Papillomavirus, Coronavirus, Caliciviruses, Paramyxoviruses and Influenza virus have also been extensively studied at the molecular level.\n\nBacterial viruses, or bacteriophages, are estimated to be the most widely distributed and diverse entities in the biosphere. Bacteriophages, or \"phage\", have been fundamental in the development of the science of molecular biology and became \"model organisms\" for probing the basic chemistry of life. The first DNA-genome project to be completed was the phage Φ-X174 in 1977. Φ29 phage, a phage of Bacillus, is a paradigm for the study of several molecular mechanisms of general biological processes, including DNA replication and regulation of transcription.\n\nSome viruses are used as vectors for gene therapy. Virus vectors have been developed that mediate stable genetic modification of treated cells by chromosomal integration of the transferred vector genomes. Gammaretroviral and lentiviral vectors, for example, can be utilized in clinical gene therapy aimed at the long-term correction of genetic defects, e.g., in stem and progenitor cells. Gammaretroviral and lentiviral vectors have so far been used in more than 300 clinical trials, addressing treatment options for various diseases.\n\nYeasts and molds are eukaryotic microorganisms classified in the kingdom Fungi.\n\nStudies involving genetic recombination have contributed substantially to our understanding of microbial molecular mechanisms. For example, among pathogenic bacteria studies of genetic transformation involving recombination have added to our understanding of molecular aspects of pathogen infectivity. Studies involving genetic recombination in eukaryotic microbial pathogens and pathogenic viruses have also contributed to understanding molecular aspects of infectivity in these organisms.\n\nPolymerase chain reaction (PCR) is used in microbiology to amplify (replicate many times) a single DNA sequence. If required, the sequence can also be altered in predetermined ways. Quantitative PCR is used for the rapid detection of microorganisms and is currently employed in diagnostic clinical microbiology laboratories, environmental analysis, food microbiology, and many other fields. The closely related technique of quantitative PCR permits the quantitative measurement of DNA or RNA molecules and is used to estimate the densities of the reference pathogens in food, water and environmental samples. Quantitative PCR provides both specificity and quantification of target microorganisms.\n\nGel electrophoresis is used routinely in microbiology to separate DNA, RNA, or protein molecules using an electric field by virtue of their size, shape or electric charge.\n\nSouthern blotting, northern blotting, western blotting and Eastern blotting are molecular techniques for detecting the presence of microbial DNA sequences (Southern), RNA sequences (northern), protein molecules (western) or protein modifications (Eastern).\n\nDNA microarrays are used in microbiology as the modern alternative to the \"blotting\" techniques. Microarrays permit the exploration of thousands of sequences at one time. This technique is used in molecular microbiology to detect the presence of pathogens in a sample (air, water, organ tissue, etc.). It is also used to determine the genetic differences between two microbial strains.\n\nDNA sequencing and genomics have been used for many decades in molecular microbiology studies. Due to their relatively small size, viral genomes were the first to be completely analysed by DNA sequencing. A huge range of sequence and genomic data is now available for a number of species and strains of microorganisms.\n\nRNA interference (RNAi) was discovered as a cellular gene regulation mechanism in 1998, but several RNAi-based applications for gene silencing have already made it into clinical trials. RNA interference (RNAi) technology has formed the basis of novel tools for biological research and drug discovery.\n\n\n", "id": "14555613", "title": "Molecular microbiology"}
{"url": "https://en.wikipedia.org/wiki?curid=1430855", "text": "Phage display\n\nPhage display is a laboratory technique for the study of protein–protein, protein–peptide, and protein–DNA interactions that uses bacteriophages (viruses that infect bacteria) to connect proteins with the genetic information that encodes them. In this technique, a gene encoding a protein of interest is inserted into a phage coat protein gene, causing the phage to \"display\" the protein on its outside while containing the gene for the protein on its inside, resulting in a connection between genotype and phenotype. These displaying phages can then be screened against other proteins, peptides or DNA sequences, in order to detect interaction between the displayed protein and those other molecules. In this way, large libraries of proteins can be screened and amplified in a process called \"in vitro\" selection, which is analogous to natural selection.\n\nThe most common bacteriophages used in phage display are M13 and fd filamentous phage, though T4, T7, and λ phage have also been used.\n\nPhage display was first described by George P. Smith in 1985, when he demonstrated the display of peptides on filamentous phage by fusing the peptide of interest onto gene III of filamentous phage. A patent by George Pieczenik claiming priority from 1985 also describes the generation of phage display libraries. This technology was further developed and improved by groups at the Laboratory of Molecular Biology with Greg Winter and John McCafferty, The Scripps Research Institute with Lerner and Barbas and the German Cancer Research Center with Breitling and Dübel for display of proteins such as antibodies for therapeutic protein engineering.\n\nLike the two-hybrid system, phage display is used for the high-throughput screening of protein interactions. In the case of M13 filamentous phage display, the DNA encoding the protein or peptide of interest is ligated into the pIII or pVIII gene, encoding either the minor or major coat protein, respectively. Multiple cloning sites are sometimes used to ensure that the fragments are inserted in all three possible reading frames so that the cDNA fragment is translated in the proper frame. The phage gene and insert DNA hybrid is then inserted (a process known as \"transduction\") into \"Escherichia coli\" (E. coli) bacterial cells such as TG1, SS320, ER2738, or XL1-Blue \"E. coli\". If a \"phagemid\" vector is used (a simplified display construct vector) phage particles will not be released from the \"E. coli\" cells until they are infected with helper phage, which enables packaging of the phage DNA and assembly of the mature virions with the relevant protein fragment as part of their outer coat on either the minor (pIII) or major (pVIII) coat protein. \nBy immobilizing a relevant DNA or protein target(s) to the surface of a microtiter plate well, a phage that displays a protein that binds to one of those targets on its surface will remain while others are removed by washing. Those that remain can be eluted, used to produce more phage (by bacterial infection with helper phage) and so produce a phage mixture that is enriched with relevant (i.e. binding) phage. The repeated cycling of these steps is referred to as 'panning', in reference to the enrichment of a sample of gold by removing undesirable materials.\nPhage eluted in the final step can be used to infect a suitable bacterial host, from which the phagemids can be collected and the relevant DNA sequence excised and sequenced to identify the relevant, interacting proteins or protein fragments.\n\nThe use of a helper phage can be eliminated by using 'bacterial packaging cell line' technology.\n\nElution can be done combining low-pH elution buffer with sonification, which, in addition to loosening the peptide-target interaction, also serves to detach the target molecule from the immobilization surface. This ultrasound-based method enables single-step selection of a high-affinity peptide.\n\nApplications of phage display technology include determination of interaction partners of a protein (which would be used as the immobilised phage \"bait\" with a DNA library consisting of all coding sequences of a cell, tissue or organism) so that the function or the mechanism of the function of that protein may be determined. Phage display is also a widely used method for \"in vitro\" protein evolution (also called protein engineering). As such, phage display is a useful tool in drug discovery. It is used for finding new ligands (enzyme inhibitors, receptor agonists and antagonists) to target proteins. The technique is also used to determine tumour antigens (for use in diagnosis and therapeutic targeting) and in searching for protein-DNA interactions using specially-constructed DNA libraries with randomised segments.\n\nCompeting methods for \"in vitro\" protein evolution include yeast display, bacterial display, ribosome display, and mRNA display.\n\nThe invention of antibody phage display revolutionised antibody drug discovery. Initial work was done by laboratories at the MRC Laboratory of Molecular Biology (Greg Winter and John McCafferty), the Scripps Research Institute (Richard Lerner and Carlos F. Barbas) and the German Cancer Research Centre (Frank Breitling and Stefan Dübel). In 1991, The Scripps group reported the first display and selection of human antibodies on phage. This initial study described the rapid isolation of human antibody Fab that bound tetanus toxin and the method was then extended to rapidly clone human anti-HIV-1 antibodies for vaccine design and therapy.\n\nPhage display of antibody libraries has become a powerful method for both studying the immune response as well as a method to rapidly select and evolve human antibodies for therapy. Antibody phage display was later used by Carlos F. Barbas at The Scripps Research Institute to create synthetic human antibody libraries, a principle first patented in 1990 by Breitling and coworkers (Patent CA 2035384), thereby allowing human antibodies to be created in vitro from synthetic diversity elements.\n\nAntibody libraries displaying millions of different antibodies on phage are often used in the pharmaceutical industry to isolate highly specific therapeutic antibody leads, for development into antibody drugs primarily as anti-cancer or anti-inflammatory therapeutics. One of the most successful was HUMIRA (adalimumab), discovered by Cambridge Antibody Technology as D2E7 and developed and marketed by Abbott Laboratories. HUMIRA, an antibody to TNF alpha, was the world's first fully human antibody, which achieved annual sales exceeding $1bn.\n\nBelow is the sequence of events that are followed in phage display screening to identify polypeptides that bind with high affinity to desired target protein or DNA sequence:\n\npIII is the protein that determines the infectivity of the virion. pIII is composed of three domains (N1, N2 and CT) connected by glycine-rich linkers. The N2 domain binds to the F pilus during virion infection freeing the N1 domain which then interacts with a TolA protein on the surface of the bacterium. Insertions within this protein are usually added in position 249 (within a linker region between CT and N2), position 198 (within the N2 domain) and at the N-terminus (inserted between the N-terminal secretion sequence and the N-terminus of pIII). However, when using the BamHI site located at position 198 one must be careful of the unpaired Cysteine residue (C201) that could cause problems during phage display if one is using a non-truncated version of pIII.\n\nAn advantage of using pIII rather than pVIII is that pIII allows for monovalent display when using a phagemid (Ff-phage derived plasmid) combined with a helper phage. Moreover, pIII allows for the insertion of larger protein sequences (>100 amino acids) and is more tolerant to it than pVIII. However, using pIII as the fusion partner can lead to a decrease in phage infectivity leading to problems such as selection bias caused by difference in phage growth rate or even worse, the phage's inability to infect its host. Loss of phage infectivity can be avoided by using a phagemid plasmid and a helper phage so that the resultant phage contains both wild type and fusion pIII.\n\ncDNA has also been analyzed using pIII via a two complementary leucine zippers system, Direct Interaction Rescue or by adding an 8-10 amino acid linker between the cDNA and pIII at the C-terminus.\n\npVIII is the main coat protein of Ff phages. Peptides are usually fused to the N-terminus of pVIII. Usually peptides that can be fused to pVIII are 6-8 amino acids long. The size restriction seems to have less to do with structural impediment caused by the added section and more to do with the size exclusion caused by pIV during coat protein export. Since there are around 2700 copies of the protein on a typical phages, it is more likely that the protein of interest will be expressed polyvalently even if a phagemid is used. This makes the use of this protein unfavorable for the discovery of high affinity binding partners.\n\nTo overcome the size problem of pVIII, artificial coat proteins have been designed. An example is Weiss and Sidhu's inverted artificial coat protein (ACP) which allows the display of large proteins at the C-terminus. The ACP's could display a protein of 20kDa, however, only at low levels (mostly only monovalently).\n\npVI has been widely used for the display of cDNA libraries. The display of cDNA libraries via phage display is an attractive alternative to the yeast-2-hybrid method for the discovery of interacting proteins and peptides due to its high throughput capability. pVI has been used preferentially to pVIII and pIII for the expression of cDNA libraries because one can add the protein of interest to the C-terminus of pVI without greatly affecting pVI's role in phage assembly. This means that the stop codon in the cDNA is no longer an issue. However, phage display of cDNA is always limited by the inability of most prokaryotes in producing post-translational modifications present in eukaryotic cells or by the misfolding of multi-domain proteins.\n\nWhile pVI has been useful for the analysis of cDNA libraries, pIII and pVIII remain the most utilized coat proteins for phage display.\n\nIn an experiment in 1995, display of Glutathione S-transferase was attempted on both pVII and pIX and failed. However, phage display of this protein was completed successfully after the addition of a periplasmic signal sequence (pelB or ompA) on the N-terminus. In a recent study, it has been shown that AviTag, FLAG and His could be displayed on pVII without the need of a signal sequence. Then the expression of single chain Fv's (scFv), and single chain T cell receptors (scTCR) were expressed both with and without the signal sequence.\n\nPelB (an amino acid signal sequence that targets the protein to the periplasm where a signal peptidase then cleaves off PelB) improved the phage display level when compared to pVII and pIX fusions without the signal sequence. However, this led to the incorporation of more helper phage genomes rather than phagemid genomes. In all cases, phage display levels were lower than using pIII fusion. However, lower display might be more favorable for the selection of binders due to lower display being closer to true monovalent display. In five out of six occasions, pVII and pIX fusions without pelB was more efficient than pIII fusions in affinity selection assays. The paper even goes on to state that pVII and pIX display platforms may outperform pIII in the long run.\n\nThe use of pVII and pIX instead of pIII might also be an advantage because virion rescue may be undertaken without breaking the virion-antigen bond if the pIII used is wild type. Instead, one could cleave in a section between the bead and the antigen to elute. Since the pIII is intact it does not matter whether the antigen remains bound to the phage.\n\nThe issue of using Ff phages for phage display is that they require the protein of interest to be translocated across the bacterial inner membrane before they are assembled into the phage. Some proteins cannot undergo this process and therefore cannot be displayed on the surface of Ff phages. In these cases, T7 phage display is used instead. In T7 phage display, the protein to be displayed is attached to the C-terminus of the gene 10 capsid protein of T7.\nThe disadvantage of using T7 is that the size of the protein that can be expressed on the surface is limited to shorter peptides because large changes to the T7 genome cannot be accommodated like it is in M13 where the phage just makes its coat longer to fit the larger genome within it. However, it can be useful for the production of a large protein library for scFV selection where the scFV is expressed on an M13 phage and the antigens are expressed on the surface of the T7 phage.\n\nDatabases and computational tools for mimotopes have been an important part of phage display study. Databases, programs and web servers have been widely used to exclude target-unrelated peptides, characterize small molecules-protein interactions and map protein-protein interactions. Users can use three dimensional structure of a protein and the peptides selected from phage display experiment to map conformational eptiopes. Some of the fast and efficient computational methods are available online (e.g. EpiSearch http://curie.utmb.edu/episearch.html ).\n\nCompeting techniques:\n\n", "id": "1430855", "title": "Phage display"}
{"url": "https://en.wikipedia.org/wiki?curid=335864", "text": "Restriction modification system\n\nThe restriction modification system (RM system) is found in bacteria and other prokaryotic organisms, and provides a defense against foreign DNA, such as that borne by bacteriophages. \n\nBacteria have restriction enzymes, also called restriction endonucleases, which cleave double stranded DNA at specific points into fragments, which are then degraded further by other endonucleases. This prevents infection by effectively destroying the foreign DNA introduced by an infectious agent (such as a bacteriophage). Approximately one-quarter of known bacteria possess RM systems and of those about one-half have more than one type of system.\nAs the sequences recognized by the restriction enzymes are very short, the bacterium itself will almost certainly contain some within its genome. In order to prevent destruction of its own DNA by the restriction enzymes, methyl groups are added. These modifications must not interfere with the DNA base-pairing, and therefore, usually only a few specific bases are modified on each strand.\n\nEndonucleases cleave internal/non-terminal phosphodiester bonds. Restriction endonucleases cleave internal phosphodiester bonds only after recognising specific sequences in DNA which are usually 4-6 base pairs long, and often palindromic.\n\nThe RM system was first discovered by Salvatore Luria and Mary Human in 1952 and 1953. They found that bacteriophage growing within an infected bacterium could be modified, so that upon their release and re-infection of a related bacterium the bacteriophage’s growth is restricted (inhibited) (also described by Luria in his autobiography on pages 45 and 99 in 1984). In 1953, Jean Weigle and Giuseppe Bertani reported similar examples of host-controlled modification using different bacteriophage system. Later work by Daisy Roulland-Dussoix and Werner Arber in 1962 and many other subsequent workers led to the understanding that restriction was due to attack and breakdown of the modified bacteriophage’s DNA by specific enzymes of the recipient bacteria. As reviewed by Daniel Nathans and Hamilton O. Smith in 1975, this work resulted in the discovery of the class of enzymes now known as restriction enzymes. When these enzymes were isolated in the laboratory they could be used for controlled manipulation of DNA, thus providing the foundation for the development of genetic engineering. Werner Arber, Daniel Nathans, and Hamilton Smith were awarded the Nobel Prize in Physiology or Medicine in 1978 for their work on restriction-modification.\n\nThere are four categories of restriction modification systems: type I, type II, type III and type IV, all with restriction enzyme activity and a methylase activity (except for type IV that has no methylase activity). They were named in the order of discovery, although the type II system is the most common.\n\nType I systems are the most complex, consisting of three polypeptides: R (restriction), M (modification), and S (specificity). The resulting complex can both cleave and methylate DNA. Both reactions require ATP, and cleavage often occurs a considerable distance from the recognition site. The S subunit determines the specificity of both restriction and methylation. Cleavage occurs at variable distances from the recognition sequence, so discrete bands are not easily visualized by gel electrophoresis.\n\nType II systems are the simplest and the most prevalent. Instead of working as a complex, the methyltransferase and endonuclease are encoded as two separate proteins and act independently (there is no specificity protein). Both proteins recognize the same recognition site, and therefore compete for activity. The methyltransferase acts as a monomer, methylating the duplex one strand at a time. The endonuclease acts as a homodimer, which facilitates the cleavage of both strands. Cleavage occurs at a defined position close to or within the recognition sequence, thus producing discrete fragments during gel electrophoresis. For this reason, Type II systems are used in labs for DNA analysis and gene cloning.\n\nType III systems have R (res) and M (mod) proteins that form a complex of modification and cleavage. The M protein, however, can methylate on its own. Methylation also only occurs on one strand of the DNA unlike most other known mechanisms. The heterodimer formed by the R and M proteins competes with itself by modifying and restricting the same reaction. This results in incomplete digestion.\n\nType IV systems are not true RM systems because they only contain a restriction enzyme and not a methylase. Unlike the other types, type IV restriction enzymes recognize and cut only modified DNA.\n\n\"Neisseria meningitides\" has multiple type II restriction endonuclease systems that are employed in natural genetic transformation. Natural genetic transformation is a process by which a recipient bacterial cell can take up DNA from a neighboring donor bacterial cell and integrate this DNA into its genome by recombination. Although early work on restriction modification systems focused on the benefit to bacteria of protecting themselves against invading bacteriophage DNA or other foreign DNA, it is now known that these systems can also be used to restrict DNA introduced by natural transformation from other members of the same, or related species.\n\nIn the pathogenic bacterium \"Neisseria meningitidis\" (meningococci), competence for transformation is a highly evolved and complex process where multiple proteins at the bacterial surface, in the membranes and in the cytoplasm interact with the incoming transforming DNA. Restriction-modification systems are abundant in the genus \"Neisseria\". \"N. meningitidis\" has multiple type II restriction endonuclease systems. The restriction modification systems in \"N. meningitides\" vary in specificity between different clades. This specificity provides an efficient barrier against DNA exchange between clades. Luria, on page 99 of his autobiography, referred to such a restriction behavior as “an extreme instance of unfriendliness.” Restriction-modification appears to be a major driver of sexual isolation and speciation in the meningococci. Caugant and Maiden suggested that restriction-modification systems in meningococci may act to allow genetic exchange among very close relatives while reducing (but not completely preventing) genetic exchange among meningococci belonging to different clonal complexes and related species.\n\nRM systems can also act as selfish genetic elements, forcing their maintenance on the cell through postsegregational cell killing.\n\nSome viruses have evolved ways of subverting the restriction modification system, usually by modifying their own DNA, by adding methyl or glycosyl groups to it, thus blocking the restriction enzymes. Other viruses, such as bacteriophages T3 and T7, encode proteins that inhibit the restriction enzymes.\n\nTo counteract these viruses, some bacteria have evolved restriction systems which only recognize and cleave modified DNA, but do not act upon the host's unmodified DNA. Some prokaryotes have developed multiple types of restriction modification systems.\n\n(a) Cloning: RM systems can be cloned into plasmids and selected because of the resistance provided by the methylation enzyme. Once the plasmid begins to replicate, the methylation enzyme will be produced and methylate the plasmid DNA, protecting it from a specific restriction enzyme.\n\n(b) Restriction Fragment Length Polymorphisms: Restriction enzymes are also used to analyse the composition of DNA in regard to presence or absence of mutations that affect the specificity of the REase cleavage specificity. When wild-type and mutants are analysed by digestion with different REases, the gel-electrophoretic products vary in length, largely because mutant genes will not be cleaved in a similar pattern as wild-type for presence of mutations that render the REases nonb-specific to the mutant sequence.\n\nThe bacteria R-M system has been proposed as a model for devising human anti-viral gene or genomic vaccines and therapies since the RM system serves an innate defense-role in bacteria by restricting tropism of bacteriophages. Research is on REases and ZFN that can cleave the DNA of various human viruses, including HSV-2, high-risk HPVs and HIV-1, with the ultimate goal of inducing target mutagenesis and aberrations of human-infecting viruses. Interestingly, the human genome already contains remnants of retroviral genomes that have been inactivated and harnessed for self-gain. Indeed, the mechanisms for silencing active L1 genomic retroelements by the three prime repair exonuclease 1 (TREX1) and excision repair cross complementing 1(ERCC) appear to mimic the action of RM-systems in bacteria, and the non-homologous end-joining (NHEJ) that follows the use of ZFN without a repair template.\n\nA major advance is the creation of artificial restriction enzymes created by linking the FokI DNA cleavage domain with an array of DNA binding proteins or zinc finger arrays, denoted now as zinc finger nucleases (ZFN).) ZFNs are a powerful tool for host genome editing due to their enhanced sequence specificity. ZFN work in pairs, their dimerization being mediated in-situ through the FoKI domain. Each zinc finger array (ZFA) is capable of recognizing 9-12 base-pairs, making for 18-24 for the pair. A 5-7 bp spacer between the cleavage sites further enhances the specificity of ZFN, making them a safe and more precise tool that can be applied in humans. A recent Phase I clinical trial of ZFN for the targeted abolition of the CCR5 co-receptor for HIV-1 has been undertaken \n\nR-M systems are major players in the co-evolutionary interaction between mobile genetic elements (MGEs) and their hosts. Genes encoding R-M systems have been reported to move between prokaryotic genomes within MGEs such as plasmids, prophages, insertion sequences/transposons, integrative conjugative elements (ICEs) and integrons. However, it was recently found that there are relatively few R-M systems in plasmids, some in prophages, and practically none in phages. On the other hand, all these MGEs encode a large number of solitary R-M genes, notably MTases. In light of this, it is likely that R-M mobility may be less dependent on MGEs and more dependent, for example, on the existence of small genomic integration hotspots. It is also possible that R-M systems frequently exploit other mechanisms such as natural transformation, vesicles, nanotubes, gene transfer agents or generalized transduction in order to move between genomes.\n\n", "id": "335864", "title": "Restriction modification system"}
{"url": "https://en.wikipedia.org/wiki?curid=11200776", "text": "Molecular Phylogenetics and Evolution\n\nMolecular Phylogenetics and Evolution is a peer-reviewed scientific journal of evolutionary biology and phylogenetics. The journal is edited by D.E. Wildman.\n\nThe journal is indexed in:\n", "id": "11200776", "title": "Molecular Phylogenetics and Evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=25119137", "text": "Nested gene\n\nA nested gene is a gene whose entire coding sequence lies within the bounds (between the start codon and the stop codon) of a larger external gene. The coding sequence for a nested gene differs greatly from the coding sequence for its external host gene. Typically, nested genes and their host genes encode functionally unrelated proteins, and have different expression patterns in an organism.\n\nThere are two categories of nested genes: \n\nA nested intronic gene lies within the non-coding intronic region of a larger gene, and occurs relatively frequently, especially in the introns of metazoans and higher eukaryotes. Because only eukaryotic DNA contains with intronic regions, this type of gene does not occur in bacteria or archaea.\n\nThe human genome contains a relatively high proportion of nested intronic genes. It is predicted to contain at least 158 functional intronic nested genes, with an additional 212 pseudogenes and three snoRNA genes nested in intronic regions. These genes seem to be distributed randomly across all chromosomes, and the majority code for proteins that are functionally unrelated to their host genes.\n\nGenes nested opposite the coding sequences of their host genes are very rare, and have been observed in prokaryotes, and more recently, in yeast (\"S. cerevisiae\") and in \"Tetrahymena thermophila\". These non-intronic nested genes remain to be identified in metazoan genomes. As with intronic nested genes, nonintronic nested genes typically do not share functions or expression patterns with their host genes.\n\n", "id": "25119137", "title": "Nested gene"}
{"url": "https://en.wikipedia.org/wiki?curid=25240121", "text": "Multiplex polymerase chain reaction\n\nMultiplex polymerase chain reaction (Multiplex PCR) refers to the use of polymerase chain reaction to amplify several different DNA sequences simultaneously (as if performing many separate PCR reactions all together in one reaction). This process amplifies DNA in samples using multiple primers and a temperature-mediated DNA polymerase in a thermal cycler. The primer design for all primers pairs has to be optimized so that all primer pairs can work at the same annealing temperature during PCR.\n\nMultiplex-PCR was first described in 1988 as a method to detect deletions in the dystrophin gene. It has also been used with the steroid sulfatase gene. In 2008, multiplex-PCR was used for analysis of microsatellites and SNPs.\n\nMultiplex-PCR consists of multiple primer sets within a single PCR mixture to produce amplicons of varying sizes that are specific to different DNA sequences. By targeting multiple sequences at once, additional information may be gained from a single test run that otherwise would require several times the reagents and more time to perform. Annealing temperatures for each of the primer sets must be optimized to work correctly within a single reaction, and amplicon sizes, i.e., their base pair length, should be different enough to form distinct bands when visualized by gel electrophoresis. Alternatively, if amplicon sizes overlap, the different amplicons may be differentiated and visualised using primers that have been dyed with different colour fluorescent dyes. Commercial multiplexing kits for PCR are available and used by many forensic laboratories to amplify degraded DNA samples.\n\nSome of the applications of multiplex PCR include:<br>\n1. Pathogen Identification<br>\n2. High Throughput SNP Genotyping<br>\n3. Mutation Analysis<br>\n4. Gene Deletion Analysis<br>\n5. Template Quantitation<br>\n6. Linkage Analysis<br>\n7. RNA Detection<br>\n8. Forensic Studies<br>\n9. Diet Analysis\n\nVisual OMP - Software for Multiplex PCR Primer design\n\nPrimerPlex - Software for Multiplex PCR Primer design\n\nuMelt Batch 1.5 - Software for Predicting Melting Curves for Multiplex PCR Products (sum of curves)\n\nPrimer Pooler\n", "id": "25240121", "title": "Multiplex polymerase chain reaction"}
{"url": "https://en.wikipedia.org/wiki?curid=9335254", "text": "Polyclonal B cell response\n\nPolyclonal B cell response is a natural mode of immune response exhibited by the adaptive immune system of mammals. It ensures that a single antigen is recognized and attacked through its overlapping parts, called epitopes, by multiple clones of B cell.\n\nIn the course of normal immune response, parts of pathogens (e.g. bacteria) are recognized by the immune system as foreign (non-self), and eliminated or effectively neutralized to reduce their potential damage. Such a recognizable substance is called an antigen. The immune system may respond in multiple ways to an antigen; a key feature of this response is the production of antibodies by B cells (or B lymphocytes) involving an arm of the immune system known as humoral immunity. The antibodies are soluble and do not require direct cell-to-cell contact between the pathogen and the B-cell to function.\n\nAntigens can be large and complex substances, and any single antibody can only bind to a small, specific area on the antigen. Consequently, an effective immune response often involves the production of many different antibodies by many different B cells against the \"same\" antigen. Hence the term \"polyclonal\", which derives from the words \"poly\", meaning \"many\", and \"clones\" (\"Klon\"=Greek for sprout or twig); a clone is a group of cells arising from a common \"mother\" cell. The antibodies thus produced in a polyclonal response are known as polyclonal antibodies. The polyclonal antibodies are distinct from monoclonal antibody molecules, which are identical and react against a single epitope only, i.e., are more specific.\n\nAlthough the polyclonal response confers advantages on the immune system, in particular, greater probability of reacting against pathogens, it also increases chances of developing certain autoimmune diseases resulting from the reaction of the immune system against native molecules produced within the host.\n\nDiseases which can be transmitted from one organism to another are known as infectious diseases, and the causative biological agent involved is known as a pathogen. The process by which the pathogen is introduced into the body is known as inoculation, and the organism it affects is known as a biological host. When the pathogen establishes itself in a step known as colonization, it can result in an infection, consequently harming the host directly or through the harmful substances called toxins it can produce. This results in the various symptoms and signs characteristic of an infectious disease like pneumonia or diphtheria.\n\nCountering the various infectious diseases is very important for the survival of the organism, in particular, and the species, in general. This is achieved by the host by eliminating the pathogen and its toxins or rendering them nonfunctional. The collection of various cells, tissues and organs that specializes in protecting the body against infections is known as the immune system. The immune system accomplishes this through direct contact of certain white blood cells with the invading pathogen involving an arm of the immune system known as the cell-mediated immunity, or by producing substances that move to sites \"distant\" from where they are produced, \"seek\" the disease-causing cells and toxins by specifically binding with them, and neutralize them in the process–known as the humoral arm of the immune system. Such substances are known as soluble antibodies and perform important functions in countering infections.\n\nAntibodies serve various functions in protecting the host against the pathogen. Their soluble forms which carry out these functions are produced by plasma B cells, a type of white blood cell. This production is tightly regulated and requires the activation of B cells by activated T cells (another type of white blood cell), which is a sequential procedure. The major steps involved are:\n\nPathogens synthesize proteins that can serve as \"\"recognizable\"\" antigens; they may express the molecules on their surface or release them into the surroundings (body fluids). What makes these substances recognizable is that they bind very specifically and somewhat strongly to certain host proteins called \"antibodies\". The same antibodies can be anchored to the surface of cells of the immune system, in which case they serve as receptors, or they can be secreted in the blood, known as soluble antibodies. On a molecular scale, the proteins are relatively large, so they cannot be recognized as a whole; instead, their segments, called epitopes, can be recognized. An epitope comes in contact with a very small region (of 15–22 amino acids) of the antibody molecule; this region is known as the paratope. In the immune system, membrane-bound antibodies are the B cell receptor (BCR). Also, while the T cell receptor is not biochemically classified as an antibody, it serves a similar function in that it specifically binds to epitopes complexed with major histocompatibility complex (MHC) molecules. The binding between a paratope and its corresponding antigen is very specific, owing to its structure, and is guided by various noncovalent bonds, not unlike the pairing of other types of ligands (any atom, ion or molecule that binds with any receptor with at least some degree of \"specificity\" and \"strength\"). The specificity of binding does not arise out of a rigid lock and key type of interaction, but rather requires both the paratope and the epitope to undergo slight conformational changes in each other's presence.\n\nIn figure at left, the various segments that form the epitope have been shown to be continuously collinear, meaning that they have been shown as sequential; however, for the situation being discussed here (i.e., the antigen recognition by the B cell), this explanation is too simplistic. Such epitopes are known as \"sequential\" or \"linear epitopes\", as all the amino acids on them are in the same sequence (line). This mode of recognition is possible only when the peptide is small (about six to eight amino acids long), and is employed by the T cells (T lymphocytes).\n\nHowever, the B memory/naive cells recognize intact proteins present on the pathogen surface. In this situation, the protein in its tertiary structure is so greatly folded that some loops of amino acids come to lie in the interior of the protein, and the segments that flank them may lie on the surface. The paratope on the B cell receptor comes in contact only with those amino acids that lie on the \"surface\" of the protein. The surface amino acids may actually be discontinuous in the protein's primary structure, but get owing to the complex protein folding patterns (as in the adjoining figure). Such epitopes are known as \"conformational\" epitopes and tend to be longer (15–22 amino acid residues) than the linear epitopes. Likewise, the antibodies produced by the plasma cells belonging to the same clone would bind to the same conformational epitopes on the pathogen proteins.\n\nThe binding of a specific antigen with corresponding BCR molecules results in increased production of the MHC-II molecules. This assumes significance as the same does not happen when the same antigen would be internalized by a relatively nonspecific process called pinocytosis, in which the antigen with the surrounding fluid is \"drunk\" as a small vesicle by the B cell. Hence, such an antigen is known as a \"nonspecific antigen\" and does not lead to activation of the B cell, or subsequent production of antibodies against it.\n\nMacrophages and related cells employ a different mechanism to recognize the pathogen. Their receptors recognize certain present on the invading pathogen that are very \"unlikely\" to be present on a host cell. Such repeating motifs are recognized by pattern recognition receptors (PRRs) like the Toll-like receptors (TLRs) expressed by the macrophages. Since the same receptor could bind to a given motif present on surfaces of widely disparate microorganisms, this mode of recognition is relatively nonspecific, and constitutes an innate immune response.\n\nAfter recognizing an antigen, an antigen presenting cell such as the macrophage or B lymphocyte engulfs it completely by a process called phagocytosis. The engulfed particle, along with some material surrounding it, forms the endocytic vesicle (the phagosome), which fuses with lysosomes. Within the lysosome, the antigen is broken down into smaller pieces called peptides by proteases (enzymes that degrade larger proteins). The individual peptides are then complexed with major histocompatibility complex class II (MHC class II) molecules located in the lysosome – this method of \"handling\" the antigen is known as the exogenous or endocytic pathway of antigen processing in contrast to the \"endogenous or cytosolic pathway\", which complexes the \"abnormal\" proteins produced within the cell (e.g. under the influence of a viral infection or in a tumor cell) with MHC class I molecules.\n\nAn alternate pathway of endocytic processing had also been demonstrated wherein certain proteins like fibrinogen and myoglobin can bind as a whole to MHC-II molecules after they are denatured and their disulfide bonds are reduced (breaking the bond by adding hydrogen atoms across it). The proteases then degrade the exposed regions of the protein-MHC II-complex.\n\nAfter the processed antigen (peptide) is complexed to the MHC molecule, they both migrate together to the cell membrane, where they are exhibited (elaborated) as a complex that can be recognized by the CD 4+ (T helper cell) – a type of white blood cell. This is known as \"antigen presentation.\" However, the epitopes (conformational epitopes) that are recognized by the B cell prior to their digestion may not be the same as that presented to the T helper cell. Additionally, a B cell may present different peptides complexed to different MHC-II molecules.\n\nThe CD 4+ cells through their T cell receptor-CD3 complex recognize the epitope-bound MHC II molecules on the surface of the antigen presenting cells, and get 'activated'. Upon this activation, these T cells proliferate and differentiate into T2 cells. This makes them produce soluble chemical signals that promote their own survival. However, another important function that they carry out is the stimulation of B cell by establishing \"direct\" physical contact with them.\n\nComplete stimulation of T helper cells requires the B7 molecule present on the antigen presenting cell to bind with CD28 molecule present on the T cell surface (in close proximity with the T cell receptor). Likewise, a second interaction between the CD40 ligand or CD154 (CD40L) present on T cell surface and CD40 present on B cell surface, is also necessary. The same interactions that stimulate the T helper cell also stimulate the B cell, hence the term \"costimulation\". The entire mechanism ensures that an activated T cell only stimulates a B cell that recognizes the antigen containing the \"same\" epitope as recognized by the T cell receptor of the \"costimulating\" T helper cell. The B cell gets stimulated, apart from the direct costimulation, by certain growth factors, viz., interleukins 2, 4, 5, and 6 in a paracrine fashion. These factors are usually produced by the newly activated T helper cell. However, this activation occurs only after the B cell receptor present on a memory or a naive B cell itself would have bound to the corresponding epitope, without which the initiating steps of phagocytosis and antigen processing would not have occurred.\n\nA naive (or \"inexperienced\") B cell is one which belongs to a clone which has never encountered the epitope to which it is specific. In contrast, a memory B cell is one which derives from an activated naive or memory B cell. The activation of a naive or a memory B cell is followed by a manifold proliferation of that particular B cell, most of the progeny of which terminally differentiate into plasma B cells; the rest survive as memory B cells. So, when the naive cells belonging to a particular clone encounter their specific antigen to give rise to the plasma cells, and also leave a few memory cells, this is known as the \"primary immune response\". In the course of proliferation of this clone, the B cell receptor genes can undergo frequent (one in every \"two\" cell divisions) mutations in the genes coding for paratopes of antibodies. These frequent mutations are termed somatic hypermutation. Each such mutation alters the epitope-binding ability of the paratope slightly, creating new clones of B cells in the process. Some of the newly created paratopes bind \"more strongly\" to the same epitope (leading to the selection of the clones possessing them), which is known as \"affinity maturation\". Other paratopes bind better to epitopes that are \"slightly\" different from the original epitope that had stimulated proliferation. Variations in the epitope structure are also usually produced by mutations in the genes of pathogen coding for their antigen. Somatic hypermutation, thus, makes the B cell receptors and the soluble antibodies in subsequent encounters with antigens, more inclusive in their antigen recognition potential of \"altered\" epitopes, apart from bestowing greater specificity for the antigen that induced proliferation in the first place. When the memory cells get stimulated by the antigen to produce plasma cells (just like in the clone's primary response), and leave even more memory cells in the process, this is known as a \"secondary immune response,\" which translates into greater numbers of plasma cells and faster rate of antibody production lasting for longer periods. The memory B cells produced as a part of secondary response recognize the corresponding antigen faster and bind more strongly with it (i.e., greater affinity of binding) owing to affinity maturation. The soluble antibodies produced by the clone show a similar enhancement in antigen binding.\n\nResponses are polyclonal in nature as each clone somewhat specializes in producing antibodies against a given epitope, and because, each antigen contains multiple epitopes, each of which in turn can be recognized by more than one clone of B cells. To be able to react to innumerable antigens, as well as multiple constituent epitopes, the immune system requires the ability to recognize a very great number of epitopes in all, i.e., there should be a great diversity of B cell clones.\n\nMemory and naïve B cells normally exist in relatively small numbers. As the body needs to be able to respond to a large number of potential pathogens, it maintains a pool of B cells with a wide range of specificities. Consequently, while there is almost always at least one B (naive or memory) cell capable of responding to any given epitope (of all that the immune system can react against), there are very few exact duplicates. However, when a single B cell encounters an antigen to which it can bind, it can proliferate very rapidly. Such a group of cells with identical specificity towards the epitope is known as a \"clone\", and is derived from a common \"mother\" cell. All the \"daughter\" B cells match the original \"mother\" cell in their epitope specificity, and they secrete antibodies with identical paratopes. These antibodies are monoclonal antibodies, since they derive from clones of the same parent cell. A polyclonal response is one in which clones of multiple B cells react to the same antigen.\n\nA single antigen can be thought of as a sequence of multiple overlapping epitopes. Many unique B cell clones may be able to bind to the individual epitopes. This imparts even greater multiplicity to the overall response. All of these B cells can become activated and produce large colonies of plasma cell clones, each of which can secrete up to 1000 antibody molecules against each epitope per second. \n\nIn addition to different B cells reacting to \"different\" epitopes on the same antigen, B cells belonging to different clones may also be able to react to the \"same\" epitope. An epitope that can be attacked by many different B cells is said to be highly \"immunogenic\". In these cases, the \"binding affinities\" for respective epitope-paratope pairs vary, with some B cell clones producing antibodies that bind strongly to the epitope, and others producing antibodies that bind weakly.\n\nThe clones that bind to a particular epitope with greater strength are more likely to be \"selected\" for further proliferation in the germinal centers of the follicles in various lymphoid tissues like the lymph nodes. This is not unlike natural selection: clones are selected for their fitness to attack the epitopes (strength of binding) on the encountered pathogen.\nWhat makes the analogy even stronger is that the B lymphocytes have to compete with each other for signals that promote their survival in the germinal centers.\n\nAlthough there are many diverse pathogens, many of which are constantly mutating, it is a surprise that a majority of individuals remain free of infections. Thus, maintenance of health requires the body to recognize all pathogens (antigens they present or produce) likely to exist. This is achieved by maintaining a pool of immensely large (about 10) clones of B cells, each of which reacts against a specific epitope by recognizing and producing antibodies against it. However, at any given time very few clones actually remain receptive to their specific epitope. Thus, approximately 10 different epitopes can be recognized by all the B cell clones combined. Moreover, in a lifetime, an individual usually requires the generation of antibodies against very few antigens in comparison with the number that the body can recognize and respond against.\n\nIf an antigen can be recognized by more than one component of its structure, it is less likely to be \"missed\" by the immune system. Mutation of pathogenic organisms can result in modification of antigen—and, hence, epitope—structure. If the immune system \"remembers\" what the other epitopes look like, the antigen, and the organism, will still be recognized and subjected to the body's immune response. Thus, the polyclonal response widens the range of pathogens that can be recognized.\n\nMany viruses undergo frequent mutations that result in changes in amino acid composition of their important proteins. Epitopes located on the protein may also undergo alterations in the process. Such an altered epitope binds less strongly with the antibodies specific to the unaltered epitope that would have stimulated the immune system. This is unfortunate because somatic hypermutation does give rise to clones capable of producing soluble antibodies that would have bound the altered epitope avidly enough to neutralize it. But these clones would consist of naive cells which are not allowed to proliferate by the weakly binding antibodies produced by the priorly stimulated clone. This doctrine is known as the \"original antigenic sin\". This phenomenon comes into play particularly in immune responses against influenza, dengue and HIV viruses. This limitation, however, is not imposed \"by\" the phenomenon of polyclonal response, but rather, \"against it\" by an immune response that is biased in favor of experienced memory cells against the \"novice\" naive cells.\n\nIn autoimmunity the immune system wrongly recognizes certain native molecules in the body as foreign (\"self-antigen\"), and mounts an immune response against them. Since these native molecules, as normal parts of the body, will naturally always exist in the body, the attacks against them can get stronger over time (akin to secondary immune response). Moreover, many organisms exhibit molecular mimicry, which involves showing those antigens on their surface that are antigenically similar to the host proteins. This has two possible consequences: first, either the organism will be spared as a self antigen; or secondly, that the antibodies produced against it will also bind to the mimicked native proteins. The antibodies will attack the self-antigens and the tissues harboring them by activating various mechanisms like the complement activation and antibody-dependent cell-mediated cytotoxicity. Hence, wider the range of antibody-specificities, greater the chance that one or the other will react against self-antigens (native molecules of the body).\n\nMonoclonal antibodies are structurally identical immunoglobulin molecules with identical epitope-specificity (all of them bind with the same epitope with same affinity) as against their polyclonal counterparts which have varying affinities for the same epitope.\nThey are usually not produced in a natural immune response, but only in diseased states like multiple myeloma, or through specialized laboratory techniques. Because of their specificity, monoclonal antibodies are used in certain applications to quantify or detect the presence of substances (which act as antigen for the monoclonal antibodies), and for targeting individual cells (e.g. cancer cells). Monoclonal antibodies find use in various diagnostic modalities (see: western blot and immunofluorescence) and therapies—particularly of cancer and diseases with autoimmune component. But, since virtually all responses in nature are polyclonal, it makes production of immensely useful monoclonal antibodies less straightforward.\n\nThe first evidence of presence of a neutralizing substance in the blood that could counter infections came when Emil von Behring along with Kitasato Shibasaburō in 1890 developed effective serum against diphtheria. This they did by transferring serum produced from animals immunized against diphtheria to animals suffering from it. Transferring the serum thus could cure the infected animals. Behring was awarded the Nobel Prize for this work in 1901.\n\nAt this time though the chemical nature of what exactly in the blood conferred this protection was not known. In a few decades to follow, it was shown that the protective serum could neutralize and precipitate toxins, and clump bacteria. All these functions were attributed to different substances in the serum, and named accordingly as \"antitoxin\", \"precipitin\" and \"agglutinin\". That all the three substances were one entity (gamma globulins) was demonstrated by Elvin A. Kabat in 1939. In the preceding year Kabat had demonstrated the heterogeneity of antibodies through ultracentrifugation studies of horses' sera.\n\nUntil this time, cell-mediated immunity and humoral immunity were considered to be contending theories to explain effective immune response, but the former lagged behind owing to lack of advanced techniques. Cell-mediated immunity got an impetus in its recognition and study when in 1942, Merrill Chase successfully transferred immunity against tuberculosis between pigs by transferring white blood cells.\nIt was later shown in 1948 by Astrid Fagraeus in her doctoral thesis that the plasma B cells are specifically involved in antibody production. The role of lymphocytes in mediating both cell-mediated and humoral responses was demonstrated by James Gowans in 1959.\n\nIn order to account for the wide range of antigens the immune system can recognize, Paul Ehrlich in 1900 had hypothesized that preexisting \"\"side chain receptors\"\" bind a given pathogen, and that this interaction induces the cell exhibiting the receptor to multiply and produce more copies of the same receptor. This theory, called \"the selective theory\" was not proven for next five decades, and had been challenged by several \"instructional theories\" which were based on the notion that an antibody would assume its effective structure by folding around the antigen. In the late 1950s however, the works of three scientists—Jerne, Talmage and Burnet (who largely modified the theory)—gave rise to the clonal selection theory, which proved all the elements of Ehrlich's hypothesis except that the specific receptors that could neutralize the agent were soluble and not membrane-bound.\n\nThe clonal selection theory was proved correct when Sir Gustav Nossal showed that each B cell always produces only one antibody.\n\nIn 1974, the role of MHC in antigen presentation was demonstrated by Rolf Zinkernagel and Peter C. Doherty.\n\n\n\n", "id": "9335254", "title": "Polyclonal B cell response"}
{"url": "https://en.wikipedia.org/wiki?curid=25271931", "text": "Ribosomal intergenic spacer analysis\n\nRibosomal RNA (rRNA) intergenic spacer analysis (RISA) is a method of microbial community analysis that provides a means of comparing differing environments or treatment impacts without the bias imposed by culture- dependent approaches. This type of analysis is often referred to as community fingerprinting. RISA involves PCR amplification of a region of the rRNA gene operon between the small (16S) and large (23S) subunits called the intergenic spacer region ISR.\n\nBy using oligonucleotide primers targeted to conserved regions in the 16S and 23S genes, RISA fragments can be generated from most of the dominant bacteria in an environmental sample. While the majority of the rRNA operon serves a structural function, portions of the 16S-23S intergenic region can encode tRNAs depending on the bacterial species. However the taxonomic value of the ISR lies in the significant heterogeneity in both length and nucleotide sequence. In RISA, we attempt to exploit the length heterogeneity of the ISR, which has been shown to range between 150 and 1500 bp with the majority of the ISR lengths being between 150 and 500 bp.\n\nThe resulting PCR product will be a mixture of fragments contributed by several dominant community members. This product is electrophoresed in a polyacrylamide gel, and the DNA is visualized following staining. The result is a complex banding pattern that provides a community-specific profile, with each DNA band corresponding to a bacterial population on the original assemblage.\n", "id": "25271931", "title": "Ribosomal intergenic spacer analysis"}
{"url": "https://en.wikipedia.org/wiki?curid=3567878", "text": "Inverse polymerase chain reaction\n\nInverse polymerase chain reaction (Inverse PCR) is a variant of the polymerase chain reaction that is used to amplify DNA with only one known sequence. One limitation of conventional PCR is that it requires primers complementary to both termini of the target DNA, but this method allows PCR to be carried out even if only one sequence is available from which primers may be designed.\n\nInverse PCR is especially useful for the determination of insert locations. For example, various retroviruses and transposons randomly integrate into genomic DNA. To identify the sites where they have entered, the known, \"internal\" viral or transposon sequences can be used to design primers that will amplify a small portion of the flanking, \"external\" genomic DNA. The amplified product can then be sequenced and compared with DNA databases to locate the sequence which has been disrupted.\n\nThe inverse PCR method involves a series of restriction digests and ligation, resulting in a looped fragment that can be primed for PCR from a single section of known sequence. Then, like other polymerase chain reaction processes, the DNA is amplified by the temperature-sensitive DNA polymerase:\n\n\nFinally the sequence is compared with the sequence available in the data base.\n\n", "id": "3567878", "title": "Inverse polymerase chain reaction"}
{"url": "https://en.wikipedia.org/wiki?curid=1128142", "text": "Chromosome jumping\n\nChromosome jumping is a tool of molecular biology that is used in the physical mapping of genomes. It is related to several other tools used for the same purpose, including chromosome walking. \n\nChromosome jumping is used to bypass regions difficult to clone, such as those containing repetitive DNA, that cannot be easily mapped by chromosome walking, and is useful in moving along a chromosome rapidly in search of a particular gene.\n\nChromosome jumping allows more rapid movement through the genome compared to other techniques, such as chromosome walking, and can be used to generate genomic markers with known chromosomal locations.\n\nChromosome jumping enables two ends of a DNA sequence to be cloned without the middle section. Genomic DNA may be partially digested using restriction endonucleases and with the aid of DNA ligase, the fragments are circularized. From a known sequence, a primer is designed to sequence across the circularised junction. This primer is used to jump 100 kb-300 kb intervals: a sequence 100 kb away would have come near the known sequence on circularisation. Thus, sequences not reachable by chromosome walking can be sequenced. Chromosome walking can be used from the new jump position (in either direction) to look for gene-like sequences, or additional jumps can be used to progress further along the chromosome.\n\n", "id": "1128142", "title": "Chromosome jumping"}
{"url": "https://en.wikipedia.org/wiki?curid=630936", "text": "Library (biology)\n\nIn molecular biology, a library is a collection of DNA fragments that is stored and propagated in a population of micro-organisms through the process of molecular cloning. There are different types of DNA libraries, including cDNA libraries (formed from reverse-transcribed RNA), genomic libraries (formed from genomic DNA) and randomized mutant libraries (formed by de novo gene synthesis where alternative nucleotides or codons are incorporated). DNA library technology is a mainstay of current molecular biology, and the applications of these libraries depends on the source of the original DNA fragments. There are differences in the cloning vectors and techniques used in library preparation, but in general each DNA fragment is uniquely inserted into a cloning vector and the pool of recombinant DNA molecules is then transferred into a population of bacteria (a Bacterial Artificial Chromosome or BAC library) or yeast such that each organism contains on average one construct (vector + insert). As the population of organisms is grown in culture, the DNA molecules contained within them are copied and propagated (thus, \"cloned\").\n\nThe term \"library\" can refer to a population of organisms, each of which carries a DNA molecule inserted into a cloning vector, or alternatively to the collection of all of the cloned vector molecules.\n\nA cDNA library represents a sample of the mRNA purified from a particular source (either a collection of cells, a particular tissue, or an entire organism), which has been converted back to a DNA template by the use of the enzyme reverse transcriptase. It thus represents the genes that were being actively transcribed in that particular source under the physiological, developmental, or environmental conditions that existed when the mRNA was purified. cDNA libraries can be generated using techniques that promote \"full-length\" clones or under conditions that generate shorter fragments used for the identification of \"expressed sequence tags\".\n\ncDNA libraries are useful in reverse genetics, but they only represent a very small (less than 1%) portion of the overall genome in a given organism.\n\nApplications of cDNA libraries include:\n\nA genomic library is a set of clones that together represents the entire genome of a given organism. The number of clones that constitute a genomic library depends on (1) the size of the genome in question and (2) the insert size tolerated by the particular cloning vector system. For most practical purposes, the tissue source of the genomic DNA is unimportant because each cell of the body contains virtually identical DNA (with some exceptions).\n\nApplications of genomic libraries include:\n\nIn contrast to the library types described above, a randomized mutant library is created by de novo synthesis of a gene. During synthesis, alternative nucleotides or codons are incorporated into the DNA sequence at specific positions. This results in a mixture of double stranded DNA molecules which represent variants of the original gene. These variants can then be ligated into an expression vector, individual clones can be created, and the encoded protein variants can be expressed.\n\nThe expressed proteins can then be screened for variants which exhibit favourable properties. Typically the properties that are to be improved by screening a randomized mutant library are the binding affinity of antibodies or other protein-protein interactions, the activity of enzymes, or the stability of a protein. Multiple cycles of creating gene variants and screening the expression products are typically involved in directed evolution experiments.\n\nIf creating an mRNA library (ie with cDNA clones), there are several possible protocols for isolating full length mRNA. To extract DNA for genomic DNA (also known as gDNA) libraries, a DNA mini-prep may be useful.\n\ncDNA libraries require care to ensure that full length clones of mRNA are captured as cDNA (which will later be inserted into vectors). Several protocols have been designed to optimise the synthesis of the 1st cDNA strand and the 2nd cDNA strand for this reason, and also to make directional cloning into the vector more likely. \n\ngDNA fragments are generated from the extracted gDNA by using non-specific frequent cutter restriction enzymes.\n\nThe nucleotide sequences of interest are preserved as inserts to a plasmid or the genome of a bacteriophage that has been used to infect bacterial cells.\n\nVectors are propagated most commonly in bacterial cells, but if using a YAC (Yeast Artificial Chromosome) then yeast cells may be used. Vectors could also be propagated in viruses, but this can be time consuming and tedious. However, the high transfection efficiency achieved by using viruses (often phages) makes them useful for packaging the vector (with the ligated insert) and then introducing them into the bacterial (or yeast) cell. \n\nAdditionally, for cDNA libraries, a system using the Lambda Zap II phage, ExAssist, and 2 E. coli species has been developed. A Cre-Lox system using loxP sites and the in vivo expression of the recombinase enzyme can also be used instead. These are examples of in vivo excision systems. In vitro excision involves subcloning often using traditional restriction enzymes and cloning strategies. In vitro excision can be more time-consuming and may require more \"hands-on\" work than in vivo excision systems. In either case, the systems allow the movement of the vector from the phage into a live cell, where the vector can replicate and propagate until the library is to be used.\n\nThis involves \"screening\" for the sequences of interest. There are multiple possible methods to achieve this.\n", "id": "630936", "title": "Library (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=617061", "text": "Class II gene\n\nA class II gene is a type of gene that codes for a protein. Class II genes are transcribed by RNAP II.\n\nClass II genes have a promoter that may contain a TATA box. \n\nBasal transcription of class II genes requires the formation of a preinitiation complex.\n\nThey are transcribed by RNA polymerase II, include both intron and exon, and code for polypeptide.\n", "id": "617061", "title": "Class II gene"}
{"url": "https://en.wikipedia.org/wiki?curid=553745", "text": "Histone fold\n\nA histone fold is a structurally conserved motif found near the C-terminus in every core histone sequence in a histone octamer responsible for the binding of histones into heterodimers.\n\nThe histone fold averages about 70 amino acids and consists of three alpha helices connected by two short, unstructured loops. When not in the presence of DNA, the core histones assemble into head-to-tail intermediates (H3 and H4 first assemble into heterodimers then fuse two heterodimers to form a tetramer, while H2A and H2B form heterodimers) via extensive hydrophobic interactions between each histone fold domain in a \"handshake motif\".\n", "id": "553745", "title": "Histone fold"}
{"url": "https://en.wikipedia.org/wiki?curid=553741", "text": "Histone octamer\n\nA histone octamer is the eight protein complex found at the center of a nucleosome core particle. It consists of two copies of each of the four core histone proteins (H2A, H2B, H3 and H4). The octamer assembles when a tetramer, containing two copies of both H3 and H4, complexes with two H2A/H2B dimers. Each histone has both an N-terminal tail and a C-terminal histone-fold. Both of these key components interact with DNA in their own way through a series of weak interactions, including hydrogen bonds and salt bridges. These interactions keep the DNA and histone octamer loosely associated and ultimately allow the two to re-position or separate entirely.\n\nHistone post-translational modifications were first identified and listed as having a potential regulatory role on the synthesis of RNA in 1964. Since then, over several decades, chromatin theory has evolved. Chromatin subunit models as well as the notion of the nucleosome were established in 1973 and 1974, respectively. Richmond and his research group has been able to elucidate the crystal structure of the histone octamer with DNA wrapped up around it at a resolution of 7 Å in 1984. The structure of the octameric core complex was revisited seven years later and a resolution of 3.1 Å was elucidated for its crystal at a high salt concentration. Though sequence similarity is low between the core histones, each of the four have a repeated element consisting of a helix-loop-helix called the histone fold motif. Furthermore, the details of protein-protein and protein-DNA interactions were fine-tuned by X-ray crystallography studies at 2.8 and 1.9 Å, respectively, in the 2000s.\n\nCore histones are four proteins called H2A, H2B, H3 and H4 and they are all found in equal parts in the cell. All four of the core histone amino acid sequences contain between 20 to 24% of lysine and arginine and the size or the protein ranges between 11400 and 15400 Daltons, making them relatively small, yet highly positively charged proteins. High content of positively charged amino acids allow them to closely associate with negatively charged DNA. Heterodimers, or histone-only intermediates are formed from histone-fold domains. The formation of histone only-intermediates proceeds when core histones are paired into the interlocked crescent shape quasi-symmetric heterodimer. Each histone fold domain is composed of 3 α-helix regions that are separated by disordered loops. The histone fold domain is responsible for formation of head-to-tail heterodimers of two histones: H2A-H2B and H3-H4. However, H3 and H4 histones first form a heterodimer and then in turn the heterodimer dimerizes to form a tetramer H3-H4. The heterodimer formation is based on the interaction of hydrophobic amino acid residue interactions between the two proteins.\n\nQuasi symmetry allows the heterodimer to be superimposed on itself by a 180 degree rotation around this symmetry axis. As a result of the rotation, two ends of histones involved in DNA binding of the crescent shape H3-H4 are equivalent, yet they organize different stretches of DNA. The H2A-H2B dimer also folds similarly. The H3-H4 tetramer is wrapped with DNA around it as a first step of nucleosome formation. Then two H2A-H2B dimers are connected to the DNA- H3-H4 complex to form a nucleosome.\n\nEach of the four core histones, in addition to their histone-fold domains, also contain flexible, unstructured extensions called histone “tails”. Treatment of nucleosomes with protease trypsin indicates that after histone tails are removed, DNA is able to stay tightly bound to the nucleosome. Histone tails are subject to a wide array of modifications which includes phosphorylation, acetylation, and methylation of serine, lysine and arginine residues.\n\nThe nucleosome core particle is the most basic form of DNA compaction in eukaryotes. Nucleosomes consist of a histone octamer surrounded by 147 base pairs of DNA wrapped in a superhelical manner. In addition to compacting the DNA, the histone octamer plays a key role in the transcription of the DNA surrounding it. The histone octamer interacts with the DNA through both its core histone folds and N-terminal tails. The histone fold interacts chemically and physically with the DNA’s minor groove. Studies have found that the histones interact more favorably with A:T enriched regions than G:C enriched regions in the minor grooves. The N-terminal tails do not interact with a specific region of DNA but rather stabilize and guide the DNA wrapped around the octamer. The interactions between the histone octamer and DNA, however, are not permanent. The two can be separated quite easily and often are during replication and transcription. Specific remodeling proteins are constantly altering the chromatin structure by breaking the bonds between the DNA and nucleosome.\n\nHistones are composed of mostly positively charged amino acid residues such as lysine and arginine. The positive charges allow them to closely associate with the negatively charged DNA through electrostatic interactions. Neutralizing the charges in the DNA allows it to become more tightly packed.\n\nThe histone-fold domains’ interaction with the minor groove accounts for the majority of the interactions in the nucleosome. As the DNA wraps around the histone octamer, it exposes its minor groove to the histone octamer at 14 distinct locations. At these sites, the two interact through a series of weak, non-covalent bonds. The main source of bonds comes from hydrogen bonds, both direct and water-mediated. The histone-fold hydrogen bonds with both phosphodiester backbone and the A:T rich bases. In these interactions, the histone fold binds to the oxygen atoms and hydroxyl side chains, respectively. Together these sites have a total of about 40 hydrogen bonds, most of which are from the backbone interactions. Additionially, 10 out of the 14 times that the minor groove faces the histone fold, an arginine side chain from the histone fold is inserted into the minor groove. The other four times, the arginine comes from a tail region of the histone.\n\nAs mentioned above the histone tails have been shown to directly interact with the DNA of the nucleosome. Each histone in the octamer has an N-terminal tail that protrudes from the histone core. The tails play roles both in inter and intra nucleosomal interactions that ultimately influence gene access. Histones are positively charged molecules which allow a tighter bonding to the negatively charged DNA molecule. Reducing the positive charge of histone proteins reduces the strength of binding between the histone and DNA, making it more open to gene transcription (expression). Moreover, these flexible units direct DNA wrapping in a left-handed manner around the histone octamer during nucleosome formation. Once the DNA is bound the tails continue to interact with the DNA. The parts of the tail closest to the DNA hydrogen bond and strengthen the DNA’s association with the octamer; the parts of the tail furthest away from the DNA, however, work in a very different manner. Cellular enzymes modify the amino acids in the distal sections of the tail to influence the accessibility of the DNA. The tails have also been implicated in the stabilization of 30-nm fibers. Research has shown removing certain tails prevents the nucleosomes from forming properly and a general failure to produce chromatin fiber. In all, these associations protect the nucleosomal DNA from the external environment but also lower their accessibility to cellular replication and transcriptional machinery.\n\nIn order to access the nucelosomal DNA, the bonds between it and the histone octamer must be broken. This change takes place periodically in the cell as specific regions are transcribed, and it happens genome-wide during replication. Remodeling proteins work in three distinct ways: they can slide the DNA along the surface of the octamer, replace the one histone dimer with a variant, or remove the histone octamer entirely. No matter the method, in order to modify the nucleosomes, the remodeling complexes require energy from ATP hydrolysis to drive their actions.\n\nOf the three techniques, sliding is the most common and least extreme. The basic premise of the technique is to free up a region of DNA that the histone octamer would normally tightly bind. While the technique is not well defined, the most prominent hypothesis is that the sliding is done in an “inchworm” fashion. In this method, using ATP as an energy source, the translocase domain of the nucleosome-remodeling complex detaches a small region of DNA from the histone octamer. This “wave” of DNA, spontaneously breaking and remaking the hydrogen bonds as it goes, then propagates down the nucleosomal DNA until it reaches the last binding site with the histone octamer. Once the wave reaches the end of the histone octamer the excess that was once at the edge is extended into the region of linker DNA. In total, one round of this method moves the histone octamer several base pairs in a particular direction—away from the direction the “wave” propagated.\n\nNumerous reports show a link between age-related diseases, birth defects, and several types of cancer with disruption of certain histone post translational modifications. Studies have identified that N- and C-terminal tails are main targets for acetylation, methylation, ubiquitination and phosphorylation. New evidence is pointing to several modifications within the histone core. Research is turning towards deciphering the role of these histone core modifications at the histone-DNA interface in the chromatin. p300 and cAMP response element-binding protein (CBP) possess histone acetyltransferase activity. p300 and CBP are the most promiscuous histone acetyltransferase enzymes acetylating all four core histones on multiple residues. Lysine 18 and Lysine 27 on H3 were the only histone acetylation sites reduced upon CBP and p300 depletion in mouse embryonic fibroblasts.<ref name=\"Distinct roles of GCN5/PCAF-mediated H3K9ac and CBP/p300-mediated H3K18/27ac in nuclear receptor transactivation.\"></ref> Also, CBP and p300 knockout mice have an open neural tube defect and therefore die before birth. p300−/− embryos exhibit defective development of the heart. CBP+/− mice display growth retardation, craniofacial abnormalities, hematological malignancies, which are not observed in mice with p300+/−. Mutations of both p300 have been reported in human tumors such as colorectal, gastric, breast, ovarian, lung, and pancreatic carcinomas. Also, activation or localization of two histone acetyltransferases can be oncogenic.\n\n\n", "id": "553741", "title": "Histone octamer"}
{"url": "https://en.wikipedia.org/wiki?curid=1388252", "text": "Hybridization probe\n\nIn molecular biology, a hybridization probe is a fragment of DNA or RNA of variable length (usually 100–1000 bases long) which can be radioactively labeled. It can then be used in DNA or RNA samples to detect the presence of nucleotide sequences (the DNA target) that are complementary to the sequence in the probe. The probe thereby hybridizes to single-stranded nucleic acid (DNA or RNA) whose base sequence allows probe–target base pairing due to complementarity between the probe and target. The labeled probe is first denatured (by heating or under alkaline conditions such as exposure to sodium hydroxide) into single stranded DNA (ssDNA) and then hybridized to the target ssDNA (Southern blotting) or RNA (northern blotting) immobilized on a membrane or in situ.\nTo detect hybridization of the probe to its target sequence, the probe is tagged (or \"labeled\") with a molecular marker of either radioactive or (more recently) fluorescent molecules; commonly used markers are P (a radioactive isotope of phosphorus incorporated into the phosphodiester bond in the probe DNA) or Digoxigenin, which is a non-radioactive, antibody-based marker. DNA sequences or RNA transcripts that have moderate to high sequence similarity to the probe are then detected by visualizing the hybridized probe via autoradiography or other imaging techniques. Normally, either X-ray pictures are taken of the filter, or the filter is placed under UV light. Detection of sequences with moderate or high similarity depends on how stringent the hybridization conditions were applied—high stringency, such as high hybridization temperature and low salt in hybridization buffers, permits only hybridization between nucleic acid sequences that are highly similar, whereas low stringency, such as lower temperature and high salt, allows hybridization when the sequences are less similar. Hybridization probes used in DNA microarrays refer to DNA covalently attached to an inert surface, such as coated glass slides or gene chips, to which a mobile cDNA target is hybridized.\n\nDepending on the method, the probe may be synthesized using the phosphoramidite method, or it can be generated and labeled by PCR amplification or cloning (both are older methods). In order to increase the \"in vivo\" stability of the probe RNA is not used, instead RNA analogues may be used, in particular morpholino- derivatives. Molecular DNA- or RNA-based probes are now routinely used in screening gene libraries, detecting nucleotide sequences with blotting methods, and in other gene technologies, such as nucleic acid and tissue microarrays.\n\n\nWithin the field of microbial ecology, oligonucleotide probes are used in order to determine the presence of microbial species, genera, or microorganisms classified on a more broad level, such as bacteria, archaea, and eukaryotes via fluorescence in situ hybridization (FISH). rRNA probes have enabled scientists to visualize microorganisms, yet to be cultured in laboratory settings, by retrieval of rRNA sequences directly from the environment. Examples of these types of microorganisms include:\n\n\nIn some instances, differentiation between species may be problematic when using 16S rRNA sequences due to similarity. In such instances, 23S rRNA may be a better alternative. The global standard library of rRNA sequences is constantly becoming larger and continuously being updated, and thus the possibility of a random hybridization event between a specifically-designed probe (based on complete and current data from a range of test organisms) and an undesired/unknown target organism cannot be easily dismissed. On the contrary, it is plausible that there exist microorganisms, yet to be identified, which are phylogenetically members of a probe target group, but have partial or near-perfect target sites. usually applies when designing group-specific probes.\n\nProbably the greatest practical limitation to this technique is the lack of available automation.\n\nIn forensic science, hybridization probes are used, for example, for detection of short tandem repeats (microsatellite) regions and in restriction fragment length polymorphism (RFLP) methods, all of which are widely used as part of DNA profiling analysis.\n", "id": "1388252", "title": "Hybridization probe"}
{"url": "https://en.wikipedia.org/wiki?curid=4448546", "text": "Viral transformation\n\nViral transformation is the change in growth, phenotype, or indefinite reproduction of cells caused by the introduction of inheritable material. Through this process, a virus causes harmful transformations of an in vivo cell or cell culture. The term can also be understood as DNA transfection using a viral vector. \n\nViral transformation can occur both naturally and medically. Natural transformations can include viral cancers, such as human papillomavirus (HPV) and T-cell Leukemia virus type I. Hepatitis B and C are also the result of natural viral transformation of the host cells. Viral transformation can also be induced for use in medical treatments.\n\nCells that have been virally transformed can be differentiated from untransformed cells through a variety of growth, surface, and intracellular observations. The growth of transformed cells can be impacted by a loss of growth limitation caused by cell contact, less oriented growth, and high saturation density. Transformed cells can lose their tight junctions, increase their rate of nutrient transfer, and increase their protease secretion. Transformation can also affect the cytoskeleton and change in the quantity of signal molecules.\n\nThere are three types of viral infections that can be considered under the topic of viral transformation. These are cytocidal, persistent, and transforming infections. Cytocidal infections can cause fusion of adjacent cells, disruption of transport pathways including ions and other cell signals, disruption of DNA, RNA and protein synthesis, and nearly always leads to cell death. Persistent infections involve viral material that lays dormant within a cell until activated by some stimulus. This type of infection usually causes few obvious changes within the cell but can lead to long chronic diseases. Transforming infections are also referred to as malignant transformation. This infection causes a host cell to become malignant and can be either cytocidal (usually in the case of RNA viruses) or persistent (usually in the case of DNA viruses). Cells with transforming infections undergo immortalization and inherit the genetic material to produce tumors. Since the term cytocidal, or cytolytic, refers to cell death, these three infections are not mutually exclusive. Many transforming infections by DNA tumor viruses are also cytocidal.\n\nTable 1: Cellular effects of viral infections\nCytocidal infections are often associated with changes in cell morphology, physiology and are thus important for the complete viral replication and transformation. \"Cytopathic Effects\", often include a change in cell's morphology such as fusion with adjacent cells to form polykaryocytes as well as the synthesis of nuclear and cytoplasmic inclusion bodies. \"Physiological changes\" include the insufficient movement of ions, formation of secondary messengers, and activation of cellular cascades to continue cellular activity. \"Biochemically\", many viruses inhibit the synthesis of host DNA, RNA, proteins directly or even interfere with protein-protein, DNA-protein, RNA-protein interactions at the subcellular level. \"Genotoxicity\" involves breaking, fragmenting, or rearranging chromosomes of the host. Lastly, \"biologic effects\" include the viruses' ability to affect the activity of antigens and immunologlobulins in the host cell.\n\nThere are two types of cytocidal infections, productive and abortive. In productive infections, additional infectious viruses are produced. Abortive infections do not produce infectious viruses. One example of a productive cytocidal infection is the herpes virus.\n\nThere are three types of persistent infections, latent, chronic and slow, in which the virus stays inside the host cell for prolonged periods of time. During \"latent infections\" there is minimal to no expression of infected viral genome. The genome remains within the host cell until the virus is ready for replication. \"Chronic infections\" have similar cellular effects as acute cytocidal infections but there is a limited number of progeny and viruses involved in transformation. Lastly, \"slow infections\" have a longer incubation period in which no physiological, morphological or subcellular changes may be involved.\n\nTransformation infections is limited to abortive or restrictive infections. This constitutes the broadest category of infections as it can include both cytocidal and persistent infection. Viral transformation is most commonly understood as transforming infections, so the remainder of the article focuses on detailing transforming infections.\n\n In order for a cell to be transformed by a virus, the viral DNA must be entered into the host cell. The simplest consideration is viral transformation of a bacterial cell. This process is called lysogeny. As shown in Figure 2, a bacteriophage lands on a cell and pins itself to the cell. The phage can then penetrate the cell membrane and inject the viral DNA into the host cell. The viral DNA can then either lay dormant until stimulated by a source such as UV light or it can be immediately taken up by the host's genome. In either case the viral DNA will replicate along with the original host DNA during cell replication causing two cells to now be infected with the virus. The process will continue to propagate more and more infected cells. This process is in contrast to the lytic cycle where a virus only uses the host cell's replication machinery to replicate itself before destroying the host cell.\n\nViral transformation disrupts the normal expression of the host cell's genes in favor of expressing a limited number of viral genes. The virus also can disrupt communication between cells and cause cells to divide at an increased rate.\n\nViral transformation can impose characteristically determinable features upon a cell. Typical phenotypic changes include high saturation density, anchorage-independent growth, loss of contact inhibition, loss of orientated growth, immortalization, disruption of the cell's cytoskeleton.\n\nViral genes are expressed through the use of the host cell's replication machinery; therefore, many viral genes have promoters that support binding of many transcription factors found naturally in the host cells. These transcription factors along with the virus' own proteins can repress or activate genes from both the virus and the host cell's genome. Many viruses can also increase the production of the cell's regulatory proteins.\n\nDepending on the virus, a variety of genetic changes can occur in the host cell. In the case of a lytic cycle virus, the cell will only survive long enough to the replication machinery to be used to create additional viral units. In other cases, the viral DNA will persist within the host cell and replicate as the cell replicates. This viral DNA can either be incorporated into the host cell's genetic material or persist as a separate genetic vector. Either case can lead to damage of the host cell's chromosomes. It is possible that the damage can be repaired; however, the most common result is an instability in the original genetic material or suppression or alteration of the gene expression.\n\nAn assay is an analytic tool often used in a laboratory setting in order to assess or measure some quality of a target entity. In virology, assays can be used to differentiate between transformed and non-transformed cells. Varying the assay used, changes the selective pressure on the cells and therefore can change what properties are selected in the transformed cells.\n\nThree common assays used are the focus forming assay, the Anchorage independent growth assay, and the reduced serum assay.\n\nThe focus forming assay (FFA) is used to grow cells containing a transforming oncogene on a monolayer of non-transformed cells. The transformed cells will form raised, dense spots on the sample as they grow without contact inhibition. This assay is highly sensitive compared to other assays used for viral analysis, such as the yield reduction assay.\n\nAn example of the Anchorage independent growth assay is the soft agar assay. The assay is assessing the cells' ability to grow in a gel or viscous fluid. Transformed cells can grow in this environment and are considered anchorage independent. Cells that can only grow when attached to a solid surface are anchorage dependent untransformed cells. This assay is considered one of the most stringent for detection of malignant transformation \n\nIn a reduced serum assay, cells are assayed by exploiting the changes in cell serum requirements. Non-transformed cells require at least a 5% serum medium in order to grow; however, transformed cells can grow in an environment with significantly less serum.\n\nNatural transformation is the viral transformation of cells without the interference of medical science. This is the most commonly considered form of viral transformation and includes many cancers and diseases, such as HIV, Hepatitis B, and T-cell Leukemia virus type I.\n\nAs many as 20% of human tumors are caused by viruses. Some such viruses that are commonly recognized include HPV, T-cell Leukemia virus type I, and hepatitis B.\n\nViral oncogenesis are most common with DNA and RNA tumor viruses, most frequently the retroviruses. There are two types of oncogenic retroviruses: acute transforming viruses and non-acute transforming viruses. Acute transforming viruses induce a rapid tumor growth since they carry viral oncogenes in their DNA/RNA to induce such growth. An example of an acute transforming virus is the Rous Sarcoma Virus (RSV) that carry the v-src oncogene. v-Src is part of the c-src, which is a cellular proto-oncogene that stimulates rapid cell growth and expansion. A non-acute transforming virus on the other hand induces a slow tumor growth, since it does not carry any viral oncogenes. It induces tumor growth by transcriptionally activating the proto-oncogenes particularly the long terminal repeat (LTR) in the proto-oncogenes.\n\nViral Oncogonesis through transformation can occur via 2 mechanisms:\n\n\nOne or both of these mechanisms can occur in the same host cell.\n\nThe Hepatitis B viral protein X is believed to cause hepatocellular carcinoma through transformation, typically of liver cells. The viral DNA is incorporated into the host cell's genome causing rapid cell replication and tumor growth.\n\nPapillomaviruses typically target epithelial cells and cause everything from warts to cervical cancer. When human papillomavirus (HPV) transforms a cell, it interferes with the function of cellular proteins while degrading other cellular proteins.\n\nThe herpesviruses, Kaposi's sarcoma-associated herpesvirus and Epstein-Barr virus, are believed to cause cancer in humans, such as Kaposi's sarcoma, Burkitt's lymphoma, and nasopharyngeal carcinoma. Although genes have been identified in these viruses that cause transformation, the manner in which the virus transforms and replicates the host cell is not understood.\n\nThe retroviruses include T-cell Leukemia virus type I, HIV, and Rous Sarcoma Virus (RSV). The viral gene tax is expressed when the T-cell Leukemia virus transforms a cell altering the expression of cellular growth control genes and causing the transformed cells to become cancerous. HIV works differently by not directly causing cells to become cancerous but by instead making those infected more susceptible to lymphoma and Kaposi's sarcoma. Many other retroviruses contain the three genes, gag, pol, and env, which do not directly cause transformation or tumor formation.\n\nHuman immunodeficiency virus is a viral infection that targets the lymph nodes. HIV binds to the immune CD4 cell and reverse transcriptase alters the host cell genome to allow integration of the viral DNA via integrase. The virus replicates using the host cell's machinery and then leaves the cell to infect additional cells via budding.\n\nThere are many applications in which viral transformation can be artificially induced in a cell culture in order to treat an illness or other condition. A cell culture is infected with a virus causing the transformation; transformed cells can then be used to either produce treatments or be directly introduced into the body.\n\nType I interferons (IFNs) are used to treat a wide variety of medical conditions including hepatitis C, cancers, viral and inflammatory diseases. IFNs can either be extracted from a natural source, such as cultured human cells or blood leukocytes, or they can be manufactured with recombinant DNA technologies. Most of these IFN treatments have a low response rate.\n\nThe use of viral transformation of the Epstein-Barr virus (EBV) has been recommended to create personalized IFNs. In this process, primary B lymphocytes are transformed with EBV. These cells can then be used to produce IFNs specific for the patient from which the B lymphocytes were extracted. This personalization decreases the likelihood of an antibody response and therefore increases the effectiveness of the treatment.\n\nWhen a virus transforms a cell it often causes cancer by either altering the cells' existing genome or introducing additional genetic material which causes cells to uncontrollably replicate. It is rarely considered that what causes so much harm also has the capability of reversing the process and slowing the cancer growth or even leading to remission. Viruses transform host cells in order to survive and replicate; however, the immune responses of the host cell are typically compromised during transformation making transformed cells more susceptible to other viruses.\n\nThe idea of using viruses to treat cancers was first introduced in 1951 when a 4-year-old boy suddenly went into a temporary remission from leukemia while he had chickenpox. This led to research in the 1990s where scientists worked to create a strain of the herpes simplex virus strong enough to infect and transform tumor cells but weak enough to leave healthy cells unharmed. Treating patients with viral transformation has the possibility of treating patients more safely and more effectively than using traditional methods, such as chemotherapy. Viruses used in the treatment of cancer gain strength and increase their effectiveness as the multiply in the body while causing only minor side effects, such as nausea, fatigue, and aches.\n\n", "id": "4448546", "title": "Viral transformation"}
{"url": "https://en.wikipedia.org/wiki?curid=564380", "text": "Expression vector\n\nAn expression vector, otherwise known as an expression construct, is usually a plasmid or virus designed for gene expression in cells. The vector is used to introduce a specific gene into a target cell, and can commandeer the cell's mechanism for protein synthesis to produce the protein encoded by the gene. Expression vectors are the basic tools in biotechnology for the production of proteins.\n\nThe vector is engineered to contain regulatory sequences that act as enhancer and promoter regions and lead to efficient transcription of the gene carried on the expression vector. The goal of a well-designed expression vector is the efficient production of protein, and this may be achieved by the production of significant amount of stable messenger RNA, which can then be translated into protein. The expression of a protein may be tightly controlled, and the protein is only produced in significant quantity when necessary through the use of an inducer, in some systems however the protein may be expressed constitutively. \"Escherichia coli\" is commonly used as the host for protein production, but other cell types may also be used. An example of the use of expression vector is the production of insulin, which is used for medical treatments of diabetes.\n\nAn expression vector has features that any vector may have, such as an origin of replication, a selectable marker, and a suitable site for the insertion of a gene such as the multiple cloning site. The cloned gene may be transferred from a specialized cloning vector to an expression vector, although it is possible to clone directly into an expression vector. The cloning process is normally performed in \"Escherichia coli\", and vectors used for protein production in organisms other than \"E.coli\" may have, in addition to a suitable origin of replication for its propagation in \"E. coli\", elements that allow them to be maintained in another organism, and these vectors are called shuttle vectors.\n\nAn expression vector must have elements necessary for gene expression. These may include a promoter, the correct translation initiation sequence such as a ribosomal binding site and start codon, a termination codon, and a transcription termination sequence. There are differences in the machinery for protein synthesis between prokaryotes and eukaryotes, therefore the expression vectors must have the elements for expression that is appropriate for the chosen host. For example, prokaryotes expression vectors would have a Shine-Dalgarno sequence at its translation initiation site for the binding of ribosomes, while eukaryotes expression vectors would contain the Kozak consensus sequence.\n\nThe promoter initiates the transcription and is therefore the point of control for the expression of the cloned gene. The promoters used in expression vector are normally inducible, meaning that protein synthesis is only initiated when required by the introduction of an inducer such as IPTG. Gene expression however may also be constitutive (i.e. protein is constantly expressed) in some expression vectors. Low level of constitutive protein synthesis may occur even in expression vectors with tightly controlled promoters.\n\nAfter the expression of the gene product, it is usually necessary to purify the expressed protein; however, separating the protein of interest from the great majority of proteins of the host cell can be a protracted process. To make this purification process easier, a purification tag may be added to the cloned gene. This tag could be histidine (His) tag, other marker peptides, or a fusion partners such as glutathione S-transferase or maltose-binding protein. Some of these fusion partners may also help to increase the solubility of some expressed proteins. Other fusion proteins such as green fluorescent protein may act as a reporter gene for the identification of successful cloned genes.\n\nThe expression vector is transformed or transfected into the host cell for protein synthesis. Some expression vectors may have elements for transformation or the insertion of DNA into the host chromosome, for example the \"vir\" genes for plant transformation, and integrase sites for chromosomal insertion.\n\nSome vectors may include targeting sequence that may target the expressed protein to a specific location such as the periplasmic space of bacteria.\n\nDifferent organisms may be used to express a gene's target protein, and the expression vector used will therefore have elements specific for use in the particular organism. The most commonly used organism for protein production is the bacterium \"Escherichia coli\". However, not all proteins can be successfully expressed in \"E. coli\", or be expressed with the correct form of post-translational modifications such as glycosylations, and other systems may therefore be used.\n\nThe expression host of choice for the expression of many proteins is \"Escherichia coli\" as the production of heterologous protein in \"E. coli\" is relatively simple and convenient, as well as being rapid and cheap. A large number of \"E. coli\" expression plasmids are also available suitable for a wide variety of needs. Other bacteria used for protein production include \"Bacillus subtilis\".\n\nMost heterologous proteins are expressed in the cytoplasm of \"E. coli\". However, not all proteins formed may be soluble in the cytoplasm, and incorrectly folded proteins formed in cytoplasm can form insoluble aggregates called inclusion bodies. Such insoluble proteins will require refolding, which can be an involved process and may not necessarily produce high yield. Proteins which have disulphide bonds are often not able to fold correctly due to the reducing environment in the cytoplasm which prevents such bond formation, and a possible solution is to target the protein to the periplasmic space by the use of an N-terminal signal sequence. Another possibility is to manipulate the redox environment of the cytoplasm. Other more sophisticated systems are also being developed; such systems may allow for the expression of proteins previously thought impossible in \"E. coli\", such as glycosylated proteins.\n\nThe promoters used for these vector are usually based on the promoter of the \"lac\" operon or the T7 promoter, and they are normally regulated by the \"lac\" operator. These promoters may also be hybrids of different promoters, for example, the Tac-Promoter is a hybrid of \"trp\" and \"lac\" promoters. Note that most commonly used \"lac\" or \"lac\"-derived promoters are based on the \"lac\"UV5 mutant which is insensitive to catabolite repression. This mutant allows for expression of protein under the control of the \"lac\" promoter when the growth medium contains glucose since glucose would inhibit gene expression if wild-type \"lac\" promoter is used. Presence of glucose nevertheless may still be used to reduce background expression through residual inhibition in some systems.\n\nExamples of \"E. coli\" expression vectors are the pGEX series of vectors where glutathione S-transferase is used as a fusion partner and gene expression is under the control of the tac promoter, and the pET series of vectors which uses a T7 promoter.\n\nIt is possible to simultaneously express two or more different proteins in \"E. coli\" using different plasmids. However, when 2 or more plasmids are used, each plasmid needs to use a different antibiotic selection as well as a different origin of replication, otherwise the plasmids may not be stably maintained. Many commonly used plasmids are based on the ColE1 replicon and are therefore incompatible with each other; in order for a ColE1-based plasmid to coexist with another in the same cell, the other would need to be of a different replicon, e.g. a p15A replicon-based plasmid such as the pACYC series of plasmids. Another approach would be to use a single two-cistron vector or design the coding sequences in tandem as a bi- or poly-cistronic construct.\n\nA yeast commonly used for protein production is \"Pichia pastoris\". Examples of yeast expression vector in \"Pichia\" are the pPIC series of vectors, and these vectors use the AOX1 promoter which is inducible with methanol. The plasmids may contain elements for insertion of foreign DNA into the yeast genome and signal sequence for the secretion of expressed protein. Proteins with disulphide bonds and glycosylation can be efficiently produced in yeast. Another yeast used for protein production is \"Kluyveromyces lactis\" and the gene is expressed, driven by a variant of the strong lactase LAC4 promoter.\n\n\"Saccharomyces cerevisiae\" is particularly widely used for gene expression studies in yeast, for example in yeast two-hybrid system for the study of protein-protein interaction. The vectors used in yeast two-hybrid system contain fusion partners for two cloned genes that allow the transcription of a reporter gene when there is interaction between the two proteins expressed from the cloned genes.\n\nBaculovirus, a rod-shaped virus which infects insect cells, is used as the expression vector in this system. Insect cell lines derived from Lepidopterans (moths and butterflies), such as \"Spodoptera frugiperda\", are used as host. A cell line derived from the cabbage looper is of particular interest, as it has been developed to grow fast and without the expensive serum normally needed to boost cell growth. The shuttle vector is called bacmid, and gene expression is under the control of a strong promoter pPolh. Baculovirus has also been used with mammalian cell lines in the BacMam system.\n\nBaculovirus is normally used for production of glycoproteins, although the glycosylations may be different from those found in vertebrates. In general, it is safer to use than mammalian virus as it has a limited host range and does not infect vertebrates without modifications.\n\nMany plant expression vectors are based on the Ti plasmid of \"Agrobacterium tumefaciens\". In these expression vectors, DNA to be inserted into plant is cloned into the T-DNA, a stretch of DNA flanked by a 25-bp direct repeat sequence at either end, and which can integrate into the plant genome. The T-DNA also contains the selectable marker. The \"Agrobacterium\" provides a mechanism for transformation, integration of into the plant genome, and the promoters for its \"vir\" genes may also be used for the cloned genes. Concerns over the transfer of bacterial or viral genetic material into the plant however have led to the development of vectors called intragenic vectors whereby functional equivalents of plant genome are used so that there is no transfer of genetic material from an alien species into the plant.\n\nPlant viruses may be used as vectors since the \"Agrobacterium\" method does not work for all plants. Examples of plant virus used are the tobacco mosaic virus (TMV), potato virus X, and cowpea mosaic virus. The protein may be expressed as a fusion to the coat protein of the virus and is displayed on the surface of assembled viral particles, or as an unfused protein that accumulates within the plant. Expression in plant using plant vectors is often constitutive, and a commonly used constitutive promoter in plant expression vectors is the cauliflower mosaic virus (CaMV) 35S promoter.\n\nMammalian expression vectors offer considerable advantages for the expression of mammalian proteins over bacterial expression systems - proper folding, post-translational modifications, and relevant enzymatic activity. It may also be more desirable than other eukaryotic non-mammalian systems whereby the proteins expressed may not contain the correct glycosylations. It is of particular use in producing membrane-associating proteins that require chaperones for proper folding and stability as well as containing numerous post-translational modifications. The downside, however, is the low yield of product in comparison to prokaryotic vectors as well as the costly nature of the techniques involved. Its complicated technology, and potential contamination with animal viruses of mammalian cell expression have also placed a constraint on its use in large-scale industrial production.\n\nCultured mammalian cell lines such as the Chinese hamster ovary (CHO), COS, including human cell lines such as HEK and HeLa may be used to produce protein. Vectors are transfected into the cells and the DNA may be integrated into the genome by homologous recombination in the case of stable transfection, or the cells may be transiently transfected. Examples of mammalian expression vectors include the adenoviral vectors, the pSV and the pCMV series of plasmid vectors, vaccinia and retroviral vectors, as well as baculovirus. The promoters for cytomegalovirus (CMV) and SV40 are commonly used in mammalian expression vectors to drive gene expression. Non-viral promoter, such as the elongation factor (EF)-1 promoter, is also known.\n\n\"E. coli\" cell lysate containing the cellular components required for transcription and translation are used in this \"in vitro\" method of protein production. The advantage of such system is that protein may be produced much faster than those produced \"in vivo\" since it does not require time to culture the cells, but it is also more expensive. Vectors used for \"E. coli\" expression can be used in this system although specifically designed vectors for this system are also available. Eukaryotic cell extracts may also be used in other cell-free systems, for example, the wheat germ cell-free expression systems. Mammalian cell-free systems have also been produced.\n\nExpression vector in an expression host is now the usual method used in laboratories to produce proteins for research. Most proteins are produced in \"E. coli\", but for glycosylated proteins and those with disulphide bonds, yeast, baculovirus and mammalian systems may be used.\n\nMost protein pharmaceuticals are now produced through recombinant DNA technology using expression vectors. These peptide and protein pharmaceuticals may be hormones, vaccines, antibiotics, antibodies, and enzymes. The first human recombinant protein used for disease management, insulin, was introduced in 1982. Biotechnology allows these peptide and protein pharmaceuticals, some of which were previously rare or difficult to obtain, to be produced in large quantity. It also reduces the risks of contaminants such as host viruses, toxins and prions. Examples from the past include prion contamination in growth hormone extracted from pituitary glands harvested from human cadavers, which caused Creutzfeldt–Jakob disease in patients receiving treatment for dwarfism, and viral contaminants in clotting factor VIII isolated from human blood that resulted in the transmission of viral diseases such as hepatitis and AIDS. Such risk is reduced or removed completely when the proteins are produced in non-human host cells.\n\nIn recent years, expression vectors have been used to introduce specific genes into plants and animals to produce transgenic organisms, for example in agriculture it is used to produce transgenic plants. Expression vectors have been used to introduce a vitamin A precursor, beta-carotene, into rice plants. This product is called golden rice. This process has also been used to introduce a gene into plants that produces an insecticide, called Bacillus thuringiensis toxin or Bt toxin which reduces the need for farmers to apply insecticides since it is produced by the modified organism. In addition expression vectors are used to extend the ripeness of tomatoes by altering the plant so that it produces less of the chemical that causes the tomatoes to rot. There have been controversies over using expression vectors to modify crops due to the fact that there might be unknown health risks, possibilities of companies patenting certain genetically modified food crops, and ethical concerns. Nevertheless, this technique is still being used and heavily researched.\n\nTransgenic animals have also been produced to study animal biochemical processes and human diseases, or used to produce pharmaceuticals and other proteins. They may also be engineered to have advantageous or useful traits. Green fluorescent protein is sometimes used as tags which results in animal that can fluoresce, and this have been exploited commercially to produce the fluorescent GloFish.\n\nGene therapy is a promising treatment for a number of diseases where a \"normal\" gene carried by the vector is inserted into the genome, to replace an \"abnormal\" gene or supplement the expression of particular gene. Viral vectors are generally used but other nonviral methods of delivery are being developed. The treatment is still a risky option due to the viral vector used which can cause ill-effects, for example giving rise to insertional mutation that can result in cancer. However, there have been promising results.\n\n\n", "id": "564380", "title": "Expression vector"}
{"url": "https://en.wikipedia.org/wiki?curid=6183028", "text": "Deoxycytidine triphosphate\n\nDeoxycytidine triphosphate (dCTP) is a nucleoside triphosphate that contains the pyrimidine base cytosine. The triphosphate group contains high-energy phosphoanhydride bonds, which liberate energy when hydrolized.\n\nDNA polymerase enzymes use this energy to incorporate deoxycytidine into a newly synthesized strand of DNA. A chemical equation can be written that represents the process:\n\nThat is, dCTP has the PP (pyrophosphate) cleaved off and the dCMP is incorporated into the DNA strand at the 3' end.\nSubsequent hydrolysis of the PP drives the equilibrium of the reaction toward the right side, i.e. incorporation of the nucleotide in the growing DNA chain.\n\nLike other nucleoside triphosphates, manufacturers recommend that dCTP be stored in aqueous solution at −20 °C.\n\n\n", "id": "6183028", "title": "Deoxycytidine triphosphate"}
{"url": "https://en.wikipedia.org/wiki?curid=2520961", "text": "Protein tag\n\nProtein tags are peptide sequences genetically grafted onto a recombinant protein. Often these tags are removable by chemical agents or by enzymatic means, such as proteolysis or intein splicing. Tags are attached to proteins for various purposes.\n\nAffinity tags are appended to proteins so that they can be purified from their crude biological source using an affinity technique. These include chitin binding protein (CBP), maltose binding protein (MBP), Strep-tag and glutathione-S-transferase (GST). The poly(His) tag is a widely used protein tag, which binds to metal matrices.\n\nSolubilization tags are used, especially for recombinant proteins expressed in chaperone-deficient species such as \"E. coli\", to assist in the proper folding in proteins and keep them from precipitating. These include thioredoxin (TRX) and poly(NANP). Some affinity tags have a dual role as a solubilization agent, such as MBP, and GST.\n\nChromatography tags are used to alter chromatographic properties of the protein to afford different resolution across a particular separation technique. Often, these consist of polyanionic amino acids, such as FLAG-tag.\n\nEpitope tags are short peptide sequences which are chosen because high-affinity antibodies can be reliably produced in many different species. These are usually derived from viral genes, which explain their high immunoreactivity. Epitope tags include V5-tag, Myc-tag, HA-tag and NE-tag. These tags are particularly useful for western blotting, immunofluorescence and immunoprecipitation experiments, although they also find use in antibody purification.\n\nFluorescence tags are used to give visual readout on a protein. GFP and its variants are the most commonly used fluorescence tags. More advanced applications of GFP include using it as a folding reporter (fluorescent if folded, colorless if not).\n\nProtein tags may allow specific enzymatic modification (such as biotinylation by biotin ligase) or chemical modification (such as reaction with FlAsH-EDT2 for fluorescence imaging). Often tags are combined, in order to connect proteins to multiple other components. However, with the addition of each tag comes the risk that the native function of the protein may be abolished or compromised by interactions with the tag. Therefore, after purification, tags are sometimes removed by specific proteolysis (e.g. by TEV protease, Thrombin, Factor Xa or Enteropeptidase).\n\n\n\n\n", "id": "2520961", "title": "Protein tag"}
{"url": "https://en.wikipedia.org/wiki?curid=7075607", "text": "Sequence (biology)\n\nA sequence in biology is the one-dimensional ordering of monomers, covalently linked within a biopolymer; it is also referred to as the primary structure of the biological macromolecule.\n\n", "id": "7075607", "title": "Sequence (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=927421", "text": "Molecular lesion\n\nA molecular lesion or point lesion is damage to the structure of a biological molecule such as DNA, enzymes, or proteins that results in reduction or absence of normal function or, in rare cases, the gain of a new function. Lesions in DNA consist of breaks and other changes in the chemical structure of the helix (see \"types of DNA lesions\") while lesions in proteins consist of both broken bonds and improper folding of the amino acid chain.\n", "id": "927421", "title": "Molecular lesion"}
{"url": "https://en.wikipedia.org/wiki?curid=256141", "text": "Fluorescent tag\n\nIn molecular biology and biotechnology, a fluorescent tag, also known as a label or probe, is a molecule that is attached chemically to aid in the labeling and detection of a biomolecule such as a protein, antibody, or amino acid. Generally, fluorescent tagging, or labeling, uses a reactive derivative of a fluorescent molecule known as a fluorophore. The fluorophore selectively binds to a specific region or functional group on the target molecule and can be attached chemically or biologically. Various labeling techniques such as enzymatic labeling, protein labeling, and genetic labeling are widely utilized. Ethidium bromide, fluorescein and green fluorescent protein are common tags. The most commonly labelled molecules are antibodies, proteins, amino acids and peptides which are then used as specific probes for detection of a particular target.\n\nThe development of methods to detect and identify biomolecules has been motivated by the ability to improve the study of molecular structure and interactions. Before the technology of fluorescent labeling, radioisotopes were used to detect and identify molecular compounds. Since then, safer methods have been developed that involve the use of fluorescent dyes or fluorescent proteins as tags or probes as a means to label and identify biomolecules. Although fluorescent tagging in this regard has only been recently utilized, the discovery of fluorescence has been around for a much longer time.\n\nSir George Stokes developed the Stokes Law of Fluorescence in 1852 which states that the wavelength of fluorescence emission is greater than that of the exciting radiation. Richard Meyer then termed fluorophore in 1897 to describe a chemical group associated with fluorescence. Since then, Fluorescein was created as a fluorescent dye by Adolph von Baeyer in 1871 and the method of staining was developed and utilized with the development of fluorescence microscopy in 1911.\n\nWithin the past century, Ethidium bromide and variants were developed in the 1950s, and in 1994, fluorescent proteins or FPs were introduced. Green fluorescent protein or GFP was discovered by Osamu Shimomura in the 1960s and was developed as a tracer molecule by Douglas Prasher in 1987. FPs led to a breakthrough of live cell imaging with the ability to selectively tag genetic protein regions and observe protein functions and mechanisms. For this breakthrough, Shimomura was awarded the Nobel Prize in 2008.\n\nIn recent years, new methods for tracking biomolecules have been developed including the use of colorimetric biosensors, photochromic compounds, biomaterials, and electrochemical sensors. Fluorescent labeling is also a common method in which applications have expanded to enzymatic labeling, chemical labeling, protein labeling, and genetic labeling.\n\nThere are currently several labeling methods for tracking biomolecules. Some of the methods include the following.\n\nCommon species that isotope markers are used for include proteins. In this case, amino acids with stable isotopes of either carbon, nitrogen, or hydrogen are incorporated into polypeptide sequences. These polypeptides are then put through mass spectrometry. Because of the exact defined change that these isotopes incur on the peptides, it is possible to tell through the spectrometry graph which peptides contained the isotopes. By doing so, one can extract the protein of interest from several others in a group. Isotopic compounds play an important role as photochromes, described below.\n\nBiosensors are attached to a substance of interest. Normally, this substance would not be able to absorb light, but with the attached biosensor, light can be absorbed and emitted on a spectrophotometer. Additionally, biosensors that are fluorescent can be viewed with the naked eye. Some fluorescent biosensors also have the ability to change color in changing environments (ex: from blue to red). A researcher would be able to inspect and get data about the surrounding environment based on what color he or she could see visibly from the biosensor-molecule hybrid species.\n\nColorimetric assays are normally used to determine how much concentration of one species there is relative to another.\n\nPhotochromic compounds have the ability to switch between a range or variety of colors. Their ability to display different colors lies in how they absorb light. Different isomeric manifestations of the molecule absorbs different wavelengths of light, so that each isomeric species can display a different color based on its absorption. These include photoswitchable compounds, which are proteins that can switch from a non-fluorescent state to that of a fluorescent one given a certain environment.\n\nThe most common organic molecule to be used as a photochrome is diarylethene. Other examples of photoswitchable proteins include PADRON-C, rs-FastLIME-s and bs-DRONPA-s, which can be used in plant and mammalian cells alike to watch cells move into different environments.\n\nFluorescent biomaterials are a possible way of using external factors to observe a pathway more visibly. The method involves fluorescently labeling peptide molecules that would alter an organism's natural pathway. When this peptide is inserted into the organism's cell, it can induce a different reaction. This method can be used, for example to treat a patient and then visibly see the treatment's outcome.\n\nElectrochemical sensors can used for label-free sensing of biomolecules. They detect changes and measure current between a probed metal electrode and an electrolyte containing the target analyte. A known potential to the electrode is then applied from a feedback current and the resulting current can be measured. For example, one technique using electrochemical sensing includes slowly raising the voltage causing chemical species at the electrode to be oxidized or reduced. Cell current vs voltage is plotted which can ultimately identify the quantity of chemical species consumed or produced at the electrode. Fluorescent tags can be used in conjunction with electrochemical sensors for ease of detection in a biological system.\n\nOf the various methods of labeling biomolecules, fluorescent labels are advantageous in that they are highly sensitive even at low concentration and non-destructive to the target molecule folding and function.\n\nGreen fluorescent protein is a naturally occurring fluorescent protein from the jellyfish \"Aequorea victoria\" that is widely \nused to tag proteins of interest. GFP emits a photon in the green region of the light spectrum when excited by the absorption of light. The chromophore consists of an oxidized tripeptide -Ser^65-Tyr^66-Gly^67 located within a β barrel. GFP catalyzes the oxidation and\nonly requires molecular oxygen. GFP has been modified by changing the wavelength of light absorbed to include other colors of fluorescence. YFP or yellow fluorescent protein, BFP or blue fluorescent protein, and CFP or cyan fluorescent protein are examples of GFP variants. These variants are produced by the genetic engineering of the GFP gene.\n\nSynthetic fluorescent probes can also be used as fluorescent labels. Advantages of these labels include a smaller size with more variety in color. They can be used to tag proteins of interest more selectively by various methods including chemical recognition-based labeling, such as utilizing metal-chelating peptide tags, and biological recognition-based labeling utilizing enzymatic reactions. However, despite their wide array of excitation and emission wavelengths as well as better stability, synthetic probes tend to be toxic to the cell and so are not generally used in cell imaging studies.\n\nFluorescent labels can be hybridized to mRNA to help visualize interaction and activity, such as mRNA localization. An antisense strand labeled with the fluorescent probe is attached to a single mRNA strand, and can then be viewed during cell development to see the movement of mRNA within the cell.\n\nA fluorogen is ligand (fluorogenic ligand) which is not itself fluorescent, but when it is bound by a specific protein or RNA structure becomes fluorescent. For instance, Y-FAST is an variant of Photoactive Yellow Protein which was engineered to bind chemical mimics of the GFP tripeptide chromophore. Likewise, the Spinach aptamer is an engineered RNA sequence which can bind GFP chromophore chemical mimics, thereby conferring conditional and reversible fluorescence on RNA molecules containining the sequence.\n\nFluorescent labeling is known for its non-destructive nature and high sensitivity. This has made it one of the most widely used methods for labeling and tracking biomolecules. Several techniques of fluorescent labeling can be utilized depending on the nature of the target.\n\nIn enzymatic labeling, a DNA construct is first formed, using a gene and the DNA of a fluorescent protein. After transcription, a hybrid RNA + fluorescent is formed. The object of interest is attached to an enzyme that can recognize this hybrid DNA. Usually fluorescein or biotin is used as the fluorophore.\n\nChemical labeling or the use of chemical tags utilizes the interaction between a small molecule and a specific genetic amino acid sequence. Chemical labeling is sometimes used as an alternative for GFP. Synthetic proteins that function as fluorescent probes are smaller than GFP's, and therefore can function as probes in a wider variety of situations. Moreover, they offer a wider range of colors and photochemical properties. With recent advancements in chemical labeling, Chemical tags are preferred over fluorescent proteins due to the architectural and size limitations of the fluorescent protein's characteristic β-barrel. Alterations of fluorescent proteins would lead to loss of fluorescent properties.\n\nProtein labeling uses a short tag to minimize disruption of protein folding and function. Transition metals are used to link specific residues in the tags to site-specific targets such as the N-termini, C-termini, or internal sites within the protein. Examples of tags used for protein labeling include biarsenical tags, Histidine tags, and FLAG tags.\n\nFluorescence in situ hybridization, or FISH, is an example of a genetic labeling technique that utilizes probes that are specific for chromosomal sites along the length of a chromosome, also known as chromosome painting. Multiple fluorescent dyes that each have a distinct excitation and emission wavelength are bound to a probe which is then hybridized to chromosomes. A fluorescence microscope can detect the dyes present and send it to a computer that can reveal the karyotype of a cell. This technique allows abnormalities such as deletions and duplications to be revealed.\n\nIn recent years, chemical tags have been tailored to advanced imaging technologies more so than fluorescent proteins due to the fact that chemical tags can localize photosensitizers closer to the target proteins. Proteins can then be labeled and detected with imaging such as super-resolution microscopy, Ca-imaging, pH sensing, hydrogen peroxide detection, chromophore assisted light inactivation, and multi-photon light microscopy. In vivo imaging studies in live animals have been performed for the first time with the use of a monomeric protein derived from the bacterial haloalkane dehalogenase known as the Halo-tag. The Halo-tag covalently links to its ligand and allows for better expression of soluble proteins.\n\nAlthough fluorescent dyes may not have the same sensitivity that radioactive probes did, they are able to show real-time activity of molecules in action. Moreover, radiation and appropriate handling is no longer a concern.\n\nWith the development of fluorescent tagging, fluorescent microscopy has allowed the visualization of specific proteins in both fixed and live cell images. Localization of specific proteins has led to important concepts in cellular biology such as the functions of distinct groups of proteins in cellular membranes and organelles. In live cell imaging, fluorescent tags enable movements of proteins and their interactions to be monitored.\n\nLatest advances in methods involving fluorescent tags have led to the visualization of mRNA and its localization within various organisms. Live cell imaging of RNA can be achieved by introducing synthesized RNA that is chemically coupled with a fluorescent tag into living cells by microinjection. This technique was used to show how the \"oskar\" mRNA in the \"Drosophila\" embryo localizes to the posterior region of the oocyte.\n\n", "id": "256141", "title": "Fluorescent tag"}
{"url": "https://en.wikipedia.org/wiki?curid=14367590", "text": "P1-derived artificial chromosome\n\nThe P1-derived artificial chromosome are DNA constructs that are derived from the DNA of P1 bacteriophage. They can carry large amounts (about 100–300 kilobases) of other sequences for a variety of bioengineering purposes. It is one type of vector used to clone DNA fragments (100- to 300-kb insert size; average, 150 kb) in \"Escherichia coli\" cells.\n\nP1 was developed as a cloning vector by Nat Sternberg and colleagues in the 1990s.\n\n\n", "id": "14367590", "title": "P1-derived artificial chromosome"}
{"url": "https://en.wikipedia.org/wiki?curid=8730922", "text": "P1 phage\n\nP1 is a temperate bacteriophage that infects \"Escherichia coli\" and some other bacteria. When undergoing a lysogenic cycle the phage genome exists as a plasmid in the bacterium unlike other phages (e.g. the lambda phage) that integrate into the host DNA. P1 has an icosahedral head containing the DNA attached to a contractile tail with six tail fibers.\nThe P1 phage has gained research interest because it can be used to transfer DNA from one bacterial cell to another in a process known as transduction. As it replicates during its lytic cycle it captures fragments of the host chromosome. If the resulting viral particles are used to infect a different host the captured DNA fragments can be integrated into the new host's genome. This method of in vivo genetic engineering was widely used for many years and is still used today, though to a lesser extent. P1 can also be used to create the P1-derived artificial chromosome cloning vector which can carry relatively large fragments of DNA. P1 encodes a site-specific recombinase, Cre, that is widely used to carry out cell-specific or time-specific DNA recombination by flanking the target DNA with \"loxP\" sites (see Cre-Lox recombination).\n\nThe virion is similar in structure to the T4 phage but simpler. It has an icosahedral head containing the genome attached at one vertex to the tail. The tail has a tube surrounded by a contractile sheath. It ends in a base plate with six tail fibres. The tail fibres are involved in attaching to the host and providing specificity.\n\nThe genome of the P1 phage is moderately large, around 93Kbp in length (compared to the genomes of e.g. T4 - 169Kbp, lambda - 48Kbp and Ff - 6.4Kbp). In the viral particle it is in the form of a linear double stranded DNA molecule. Once inserted into the host it circularizes and replicates as a plasmid.\n\nIn the viral particle the DNA molecule is longer (110Kbp) than the actual length of the genome. It is created by cutting an appropriately sized fragment from a concatemeric DNA chain having multiple copies of the genome (see the section below on lysis for how this is made). Due to this the ends of the DNA molecule are identical. This is referred to as being terminally redundant. This is important for the DNA to be circularized in the host. Another consequence of the DNA being cut out of a concatemer is that a given linear molecule can start at any location on the circular genome. This is called a cyclical permutation.\n\nThe genome is especially rich in Chi sequences recognized by the bacterial recombinase RecBCD. The genome contains two origins of replication: oriR which replicates it during the lysogenic cycle and oriL which replicates it during the lytic stage. The genome of P1 encodes three tRNAs which are expressed in the lytic stage.\n\nProteome. The genome of P1 encodes 112 proteins and 5 untranslated genes and is this about twice the size of bacteriophage lambda.\n\nThe phage particle adsorbs onto the surface of the bacterium using the tail fibers for specificity. The tail sheath contracts and the DNA of the phage is injected into the host cell. The host DNA recombination machinery or the cre enzyme translated from the viral DNA recombine the terminally redundant ends and circularize the genome. Depending on various physiological cues, the phage may immediately proceed to the lytic phase or it may enter a lysogenic state.\n\nThe gene that encodes the tail fibers have a set of sequences that can be targeted by a site specific recombinase \"Cin\". This causes the C terminal end of the protein to switch between two alternate forms at a low frequency. The viral tail fibers are responsible for the specificity of binding to the host receptor. The targets of the viral tail fibers are under a constant pressure to evolve and evade binding. This method of recombinational diversity of the tail allows the virus to keep up with the bacterium. This system has close sequence homologies to recombinational systems in the tail fibers of unrelated phages like the mu phage and the lambda phage.\n\nThe genome of the P1 phage is maintained as a low copy number plasmid in the bacterium. The relatively large size of the plasmid requires it to keep a low copy number lest it become too large a metabolic burden while it is a lysogen. As there is usually only one copy of the plasmid per bacterial genome, the plasmid stands a high chance of not being passed to both daughter cells. The P1 plasmid combats this by several methods:\n\nThe P1 plasmid has a separate origin of replication (oriL) that is activated during the lytic cycle. Replication begins by a regular bidirectional theta replication at oriL but later in the lytic phase,it switches to a rolling circle method of replication using the host recombination machinery. This results in numerous copies of the genome being present on a single linear DNA molecule called a concatemer. The end of the concatemer is cut a specific site called the \"pac\" site or packaging site. This is followed by the packing of the DNA into the heads till they are full. The rest of the concatemer that does not fit into one head is separated and the machinery begins packing this into a new head. The location of the cut is not sequence specific. Each head holds around 110kbp of DNA so there is a little more than one complete copy of the genome (~90kbp) in each head, with the ends of the strand in each head being identical. After infecting a new cell this terminal redundancy is used by the host recombination machinery to cyclize the genome if it lacks two copies of the lox locus. If two lox sites are present (one in each terminally redundant end) the cyclization is carried out by the cre recombinase.\n\nOnce the complete virions are assembled, the host cell is lysed, releasing the viral particles.\n\nP1 was discovered in 1951 by Giuseppe Bertani in Salvador Luria's laboratory, but the phage was little studied until Ed Lennox, also in Luria's group, showed in 1954–5 that it could transduce genetic material between host bacteria. This discovery led to the phage being used for genetic exchange and genome mapping in \"E. coli\", and stimulated its further study as a model organism. In the 1960s, Hideo Ikeda and Jun-ichi Tomizawa showed the phage's DNA genome to be linear and double-stranded, with redundancy at the ends. In the 1970s, Nat Sternberg characterised the Cre–\"lox\" site-specific recombination system, which allows the linear genome to circularise to form a plasmid after infection. During the 1980s, Sternberg developed P1 as a vector for cloning large pieces of eukaryotic DNA. A P1 gene map based on a partial DNA sequence was published in 1993 by Michael Yarmolinsky and Małgorzata Łobocka, and the genome was completely sequenced by Łobocka and colleagues in 2004.\n\n", "id": "8730922", "title": "P1 phage"}
{"url": "https://en.wikipedia.org/wiki?curid=30651", "text": "Transposable element\n\nA transposable element (TE or transposon) is a DNA sequence that can change its position within a genome, sometimes creating or reversing mutations and altering the cell's genetic identity and genome size. Transposition often results in duplication of the same genetic material. Barbara McClintock's discovery of these jumping genes earned her a Nobel Prize in 1983.\n\nTransposable elements make up a large fraction of the genome and are responsible for much of the mass of DNA in a eukaryotic cell. It has been shown that TEs are important in genome function and evolution. In \"Oxytricha\", which has a unique genetic system, these elements play a critical role in development. Transposons are also very useful to researchers as a means to alter DNA inside a living organism.\n\nThere are at least two classes of TEs: Class I TEs or retrotransposons generally function via reverse transcription, while Class II TEs or DNA transposons encode the protein transposase, which they require for insertion and excision, and some of these TEs also encode other proteins.\n\nBarbara McClintock discovered the first TEs in maize (\"Zea mays\") at the Cold Spring Harbor Laboratory in New York. McClintock was experimenting with maize plants that had broken chromosomes.\n\nIn the winter of 1944–1945, McClintock planted corn kernels that were self-pollinated, meaning that the silk (style) of the flower received pollen from its own anther. These kernels came from a long line of plants that had been self-pollinated, causing broken arms on the end of their ninth chromosomes. As the maize plants began to grow, McClintock noted unusual color patterns on the leaves. For example, one leaf had two albino patches of almost identical size, located side by side on the leaf. McClintock hypothesized that during cell division certain cells lost genetic material, while others gained what they had lost. However, when comparing the chromosomes of the current generation of plants with the parent generation, she found certain parts of the chromosome had switched position. This refuted the popular genetic theory of the time that genes were fixed in their position on a chromosome. McClintock found that genes could not only move, but they could also be turned on or off due to certain environmental conditions or during different stages of cell development.\n\nMcClintock also showed that gene mutations could be reversed. She presented her report on her findings in 1951, and published an article on her discoveries in \"Genetics\" in November 1953 entitled \"Induction of Instability at Selected Loci in Maize\".\n\nHer work would be largely dismissed and ignored until the late 1960s–1970s when it would be rediscovered after TEs were found in bacteria. She was awarded a Nobel Prize in Physiology or Medicine in 1983 for her discovery of TEs, more than thirty years after her initial research.\n\nApproximately 90% of the maize genome is made up of TEs, as is 44% of the human genome.\n\nTransposable elements represent one of several types of mobile genetic elements. TEs are assigned to one of two classes according to their mechanism of transposition, which can be described as either \"copy and paste\" (Class I TEs) or \"cut and paste\" (Class II TEs).\n\nClass I TEs are copied in two stages: first, they are transcribed from DNA to RNA, and the RNA produced is then reverse transcribed to DNA. This copied DNA is then inserted back into the genome at a new position. The reverse transcription step is catalyzed by a reverse transcriptase, which is often encoded by the TE itself. The characteristics of retrotransposons are similar to retroviruses, such as HIV.\n\nRetrotransposons are commonly grouped into three main orders:\n\n[Note : Retroviruses can also be considered TEs. For example, after conversion of retroviral RNA into DNA inside a host cell, the newly produced retroviral DNA is integrated into the genome of the host cell. These integrated DNAs are termed \"proviruses\". The provirus is a specialized form of eukaryotic retrotransposon, which can produce RNA intermediates that may leave the host cell and infect other cells. The transposition cycle of retroviruses has similarities to that of prokaryotic TEs, suggesting a distant relationship between the two].\nThe cut-and-paste transposition mechanism of class II TEs does not involve an RNA intermediate. The transpositions are catalyzed by several transposase enzymes. Some transposases non-specifically bind to any target site in DNA, whereas others bind to specific target sequences. The transposase makes a staggered cut at the target site producing sticky ends, cuts out the DNA transposon and ligates it into the target site. A DNA polymerase fills in the resulting gaps from the sticky ends and DNA ligase closes the sugar-phosphate backbone. This results in target site duplication and the insertion sites of DNA transposons may be identified by short direct repeats (a staggered cut in the target DNA filled by DNA polymerase) followed by inverted repeats (which are important for the TE excision by transposase).\n\nCut-and-paste TEs may be duplicated if their transposition takes place during S phase of the cell cycle, when a donor site has already been replicated but a target site has not yet been replicated. Such duplications at the target site can result in gene duplication, which plays an important role in genomic evolution.\n\nNot all DNA transposons transpose through the cut-and-paste mechanism. In some cases, a replicative transposition is observed in which a transposon replicates itself to a new target site (e.g. helitron).\n\nClass II TEs comprise less than 2% of the human genome, making the rest Class I.\n\nTransposition can be classified as either \"autonomous\" or \"non-autonomous\" in both Class I and Class II TEs. Autonomous TEs can move by themselves, whereas non-autonomous TEs require the presence of another TE to move. This is often because dependent TEs lack transposase (for Class II) or reverse transcriptase (for Class I).\n\nActivator element (\"Ac\") is an example of an autonomous TE, and dissociation elements (\"Ds\") is an example of a non-autonomous TE. Without \"Ac,\" \"Ds\" is not able to transpose.\n\n\nTEs are mutagens and their movements are often the causes of genetic disease. They can damage the genome of their host cell in different ways:\n\nDiseases often caused by TEs include hemophilia A and B, severe combined immunodeficiency, porphyria, predisposition to cancer, and Duchenne muscular dystrophy. \"LINE1\" (\"L1\") TEs that land on the human Factor VIII have been shown to cause haemophilia and insertion of \"L1\" into the \"APC\" gene causes colon cancer, confirming that TEs play an important role in disease development.\n\nAdditionally, many TEs contain promoters which drive transcription of their own transposase. These promoters can cause aberrant expression of linked genes, causing disease or mutant phenotypes.\n\nOne study estimated the rate of transposition of a particular retrotransposon, the Ty1 element in \"Saccharomyces cerevisiae\". Using several assumptions, the rate of successful transposition event per single Ty1 element came out to be about once every few months to once every few years. Some TEs contain heat-shock like promoters and their rate of transposition increases if the cell is subjected to stress, thus increasing the mutation rate under these conditions, which might be beneficial to the cell.\n\nCells defend against the proliferation of TEs in a number of ways. These include piRNAs and siRNAs, which silence TEs after they have been transcribed.\n\nIf organisms are mostly composed of TEs, one might assume that disease caused by misplaced TEs is very common, but in most cases TEs are silenced through epigenetic mechanisms like DNA methylation, chromatin remodeling and piRNA, such that little to no phenotypic effects nor movements of TEs occur as in some wild-type plant TEs. Certain mutated plants have been found to have defects in methylation-related enzymes (methyl transferase) which cause the transcription of TEs, thus affecting the phenotype.\n\nOne hypothesis suggests that only approximately 100 LINE1 related sequences are active, despite their sequences making up 17% of the human genome. In human cells, silencing of LINE1 sequences is triggered by an RNA interference (RNAi) mechanism. Surprisingly, the RNAi sequences are derived from the 5' untranslated region (UTR) of the LINE1, a long terminal which repeats itself. Supposedly, the 5' LINE1 UTR that codes for the sense promoter for LINE1 transcription also encodes the antisense promoter for the miRNA that becomes the substrate for siRNA production. Inhibition of the RNAi silencing mechanism in this region showed an increase in LINE1 transcription.\n\nTEs are found in most life forms, and the scientific community is still exploring their evolution and their effect on genome evolution. It is unclear whether TEs originated in the last universal common ancestor, arose independently multiple times, or arose once and then spread to other kingdoms by horizontal gene transfer. While some TEs confer benefits on their hosts, most are regarded as selfish DNA parasites. In this way, they are similar to viruses. Various viruses and TEs also share features in their genome structures and biochemical abilities, leading to speculation that they share a common ancestor.\n\nBecause excessive TE activity can damage exons, many organisms have acquired mechanisms to inhibit their activity. Bacteria may undergo high rates of gene deletion as part of a mechanism to remove TEs and viruses from their genomes, while eukaryotic organisms typically use RNA interference to inhibit TE activity. Nevertheless, some TEs generate large families often associated with speciation events. Evolution often deactivates DNA transposons, leaving them as introns (inactive gene sequences). In vertebrate animal cells, nearly all 100,000+ DNA transposons per genome have genes that encode inactive transposase polypeptides. In humans, all Tc1-like transposons are inactive. The first synthetic transposon designed for use in vertebrate cells, the Sleeping Beauty transposon system, is a Tc1/mariner-like transposon. It exists in the human genome as an intron and was activated through reconstruction.\n\nLarge quantities of TEs within genomes may still present evolutionary advantages, however. Interspersed repeats within genomes are created by transposition events accumulating over evolutionary time. Because interspersed repeats block gene conversion, they protect novel gene sequences from being overwritten by similar gene sequences and thereby facilitate the development of new genes. TEs may also have been co-opted by the vertebrate immune system as a means of producing antibody diversity. The V(D)J recombination system operates by a mechanism similar to that of some TEs.\n\nTEs can contain many types of genes, including those conferring antibiotic resistance and ability to transpose to conjugative plasmids. Some TEs also contain integrons, genetic elements that can capture and express genes from other sources. These contain integrase, which can integrate gene cassettes. There are over 40 antibiotic resistance genes identified on cassettes, as well as virulence genes.\n\nTransposons do not always excise their elements precisely, sometimes removing the adjacent base pairs; this phenomenon is called exon shuffling. Shuffling two unrelated exons can create a novel gene product or, more likely, an intron.\n\nThe first TE was discovered in maize (\"Zea mays\") and is named dissociator (\"Ds\"). Likewise, the first TE to be molecularly isolated was from a plant (snapdragon). Appropriately, TEs have been an especially useful tool in plant molecular biology. Researchers use them as a means of mutagenesis. In this context, a TE jumps into a gene and produces a mutation. The presence of such a TE provides a straightforward means of identifying the mutant allele relative to chemical mutagenesis methods.\n\nSometimes the insertion of a TE into a gene can disrupt that gene's function in a reversible manner, in a process called insertional mutagenesis; transposase-mediated excision of the DNA transposon restores gene function. This produces plants in which neighboring cells have different genotypes. This feature allows researchers to distinguish between genes that must be present inside of a cell in order to function (cell-autonomous) and genes that produce observable effects in cells other than those where the gene is expressed.\n\nTEs are also a widely used tool for mutagenesis of most experimentally tractable organisms. The Sleeping Beauty transposon system has been used extensively as an insertional tag for identifying cancer genes.\n\nThe Tc1/mariner-class of TEs Sleeping Beauty transposon system, awarded Molecule of the Year in 2009, is active in mammalian cells and is being investigated for use in human gene therapy.\n\nTEs are used for the reconstruction of phylogenies by the means of presence/absence analyses.<ref name=\"DOI10.1371/journal.pbio.0040091\"></ref>\n\n\"De novo\" repeat identification is an initial scan of sequence data that seeks to find the repetitive regions of the genome, and to classify these repeats. Many computer programs exist to perform \"de novo\" repeat identification, all operating under the same general principles. As short tandem repeats are generally 1–6 base pairs in length and are often consecutive, their identification is relatively simple. Dispersed repetitive elements, on the other hand, are more challenging to identify, due to the fact that they are longer and have often acquired mutations. However, it is important to identify these repeats as they are often found to be transposable elements (TEs).\n\n\"De novo\" identification of transposons involves three steps: 1) find all repeats within the genome, 2) build a consensus of each family of sequences, and 3) classify these repeats. There are three groups of algorithms for the first step. One group is referred to as the k-mer approach, where a k-mer is a sequence of length k. In this approach, the genome is scanned for overrepresented k-mers; that is, k-mers that occur more often than is likely based on probability alone. The length k is determined by the type of transposon being searched for. The k-mer approach also allows mismatches, the number of which is determined by the analyst. Some k-mer approach programs use the k-mer as a base, and extend both ends of each repeated k-mer until there is no more similarity between them, indicating the ends of the repeats. Another group of algorithms employs a method called sequence self-comparison. Sequence self-comparison programs use databases such as AB-BLAST to conduct an initial sequence alignment. As these programs find groups of elements that partially overlap, they are useful for finding highly diverged transposons, or transposons with only a small region copied into other parts of the genome. Another group of algorithms follows the periodicity approach. These algorithms perform a Fourier transformation on the sequence data, identifying periodicities, regions that are repeated periodically, and are able to use peaks in the resultant spectrum to find candidate repetitive elements. This method works best for tandem repeats, but can be used for dispersed repeats as well. However, it is a slow process, making it an unlikely choice for genome scale analysis.\n\nThe second step of \"de novo\" repeat identification involves building a consensus of each family of sequences. A consensus sequence is a sequence that is created based on the repeats that comprise a TE family. A base pair in a consensus is the one that occurred most often in the sequences being compared to make the consensus. For example, in a family of 50 repeats where 42 have a T base pair in the same position, the consensus sequence would have a T at this position as well, as the base pair is representative of the family as a whole at that particular position, and is most likely the base pair found in the family's ancestor at that position. Once a consensus sequence has been made for each family, it is then possible to move on to further analysis, such as TE classification and genome masking in order to quantify the overall TE content of the genome.\n\nTransposable elements have been recognized as good candidates for stimulating gene adaptation, through their ability to regulate the expression levels of nearby genes. Combined with their \"mobility\", transposable elements can be relocated adjacent to their targeted genes, and control the expression levels of the gene, dependent upon the circumstances.\n\nThe study conducted in 2008, \"High Rate of Recent Transposable Element–Induced Adaptation in Drosophila melanogaster\", used \"D. melanogaster\" that had recently migrated from Africa to other parts of the world, as a basis for studying adaptations caused by transposable elements. Although most of the TEs were located on introns, the experiment showed the significant difference on gene expressions between the population in Africa and other parts of the world. The four TEs that caused the selective sweep were more prevalent in \"D. melanogaster\" from temperate climates, leading the researchers to conclude that the selective pressures of the climate prompted genetic adaptation. From this experiment, it has been confirmed that adaptive TEs are prevalent in nature, by enabling organisms to adapt gene expression as a result of new selective pressures.\n\nHowever, not all effects of adaptive TEs are beneficial to the population. In the research conducted in 2009, \"A Recent Adaptive Transposable Element Insertion Near Highly Conserved Developmental Loci in Drosophila melanogaster\", a TE, inserted between Jheh 2 and Jheh 3, revealed a downgrade in the expression level of both of the genes. Down regulation of such genes has caused \"Drosophila\" to exhibit extended developmental time and reduced egg to adult viability. Although this adaptation was observed in high frequency in all non-African populations, it was not fixed in any of them. This is not hard to believe, since it is logical for a population to favor higher egg to adult viability, therefore trying to purge the trait caused by this specific TE adaptation.\n\nAt the same time, there have been several reports showing the advantageous adaptation caused by TEs. In the research done with silkworms, \"An Adaptive Transposable Element insertion in the Regulatory Region of the EO Gene in the Domesticated Silkworm\", a TE insertion was observed in the cis-regulatory region of the EO gene, which regulates molting hormone 20E, and enhanced expression was recorded. While populations without the TE insert are often unable to effectively regulate hormone 20E under starvation conditions, those with the insert had a more stable development, which resulted in higher developmental uniformity.\n\nThese three experiments all demonstrated different ways in which TE insertions can be advantageous or disadvantageous, through means of regulating the expression level of adjacent genes. The field of adaptive TE research is still under development and more findings can be expected in the future.\n\n\n\n", "id": "30651", "title": "Transposable element"}
{"url": "https://en.wikipedia.org/wiki?curid=25389339", "text": "Dda (DNA-dependent ATPase)\n\nDda (short for DNA-dependent ATPase; also known as Dda helicase and Dda DNA helicase) is the 439-amino acid 49,897-atomic mass unit protein coded by the Dda gene of the bacteriophage T4 phage, a virus that infects enterobacteria.\n\nDda is a molecular motor, specifically a helicase that moves in the 5' end to 3' direction along a nucleic acid phosphodiester backbone, separating two annealed nucleic acid strands, using the free energy released by the hydrolysis of adenosine triphosphate. The National Center for Biotechnology Information (NCBI) Reference Sequence accession number is NP_049632.\n\nDda is involved in the initiation of T4 DNA replication and DNA recombination.\n\nThe Dda gene is 31,219 base pair long. The GenBank accession number is AAD4255. The coding strand (see also: sense strand) begins in base number 9,410 and ends in base number 10,729.\n\nDda is toxic to cells at elevated levels.\n\nEnzymes\nMotor proteins\nTranscription (genetics)\n\n", "id": "25389339", "title": "Dda (DNA-dependent ATPase)"}
{"url": "https://en.wikipedia.org/wiki?curid=8142925", "text": "I-CreI\n\nI-\"Cre\"I is a homing endonuclease whose gene was first discovered in the chloroplast genome of \"Chlamydomonas reinhardtii\", a species of unicellular green algae. It is named for the facts that: it resides in an Intron; it was isolated from \"Clamydomonas reinhardtii\"; it was the first (I) such gene isolated from \"C. reinhardtii\". Its gene resides in a group I intron in the 23S ribosomal RNA gene of the \"C. reinhardtii\" chloroplast, and I-\"Cre\"I is only expressed when its mRNA is spliced from the primary transcript of the 23S gene. I-\"Cre\"I enzyme, which functions as a homodimer, recognizes a 22-nucleotide sequence of duplex DNA and cleaves one phosphodiester bond on each strand at specific positions. I-\"Cre\"I is a member of the LAGLIDADG family of homing endonucleases, all of which have a conserved LAGLIDADG amino acid motif that contributes to their associative domains and active sites. When the I-\"Cre\"I-containing intron encounters a 23S gene lacking the intron, I-\"Cre\"I enzyme \"homes\" in on the \"intron-minus\" allele of 23S and effects its parent intron's insertion into the intron-minus allele. Introns with this behavior are called mobile introns. Because I-\"Cre\"I provides for its own propagation while conferring no benefit on its host, it is an example of selfish DNA.\n\nI-\"Cre\"I was first observed as an intervening sequence in the 23S rRNA gene of the \"C. reinhardtii\" chloroplast genome. The 23S gene is an RNA gene, meaning that its transcript is not translated into protein. As RNA, it forms part of the large subunit of the ribosome. An open reading frame coding for a 163-amino acid protein was found in this 23S intron, suggesting that a protein might facilitate the homing behavior of the mobile intron. Furthermore, the predicted protein had a LAGLIDADG motif, a conserved amino acid sequence that is present in other proteins coded for in group I mobile introns. A 1991 study established that the ORF codes for a DNA endonuclease, I-\"Cre\"I, which selectively cuts a site corresponding to where the intron is spliced out of the 23S primary transcript. The study also showed that the intron was able to invade 23S alleles that did not already have it.\n\nI-\"Cre\"I has evolved to cut a 22-nucleotide sequence of DNA that occurs in alleles of the 23S ribosomal RNA gene that lack the I-\"Cre\"I-containing intron. When such an \"intron-minus\" allele is cut, pathways of double-strand break repair are activated in the cell. The cell uses as a template for repair the 23S allele that yielded the responsible I-\"Cre\"I enzyme, thus replicating the I-\"Cre\"I-containing intron. The resulting \"intron-plus\" allele no longer contains an intact homing site for the I-\"Cre\"I enzyme, and is therefore not cleaved. Since this intron provides for its own replication without conferring any benefit on its host, I-\"Cre\"I is a form of selfish DNA.\n\n Because I-\"Cre\"I has evolved to cut such a long sequence of DNA, unlike restriction endonucleases that typically cut four- or six-nucleotide sequences, it is capable of cutting a single site within a very large genome. A four- or six-nucleotide sequence is expected to occur many, many times in a genome of millions or billions of nucleotides simply by chance, whereas a 22-nucleotide sequence might occur only once (10/4 vs. 10/4). This specificity of I-\"Cre\"I cleavage makes I-\"Cre\"I a promising tool for gene targeting. If a person were to have a disease due to a defective allele of some gene, it would be helpful to be able to replace that allele with a functional one. If one could cause I-\"Cre\"I to cut the DNA only in the defective allele while simultaneously providing a normal allele for the cell to use as a repair template, the patient's own homologous recombination machinery could insert the desired allele in place of the dysfunctional one. The specificity of I-\"Cre\"I also allows for the reduction of deleterious effects due to double-strand breaks outside of the gene of interest.\n\nIn order to use I-\"Cre\"I as a tool in this fashion, it is necessary to make it recognize and cleave sequences of DNA different from its native homing site. An \"Escherichia coli\" genetic system for studying the relationship between I-\"Cre\"I structure and its homing site specificity was created in 1997. In 1997, the structure of the I-CreI protein was determined, and in 1998, its crystal structure bound to its native DNA homing site was solved, greatly aiding research in altering the homing site recognition of the protein. Mutant forms of the protein have since been created that exhibit altered homing site specificity. A genetic system in \"Saccharomyces cerevisiae\" has also been created, yielding additional I-\"Cre\"I mutants with modified homing site specificities.\n\nI-\"Cre\"I has already been used successfully to induce homologous recombination in \"Drosophila melanogaster\", an extremely popular eukaryotic model organism. It seems very likely that advances in molecular biological techniques and generation of a library of I-\"Cre\"I-derived novel endonucleases will eventually allow for the targeting of many genes of etiological significance.\n", "id": "8142925", "title": "I-CreI"}
{"url": "https://en.wikipedia.org/wiki?curid=886069", "text": "HindII\n\n\"Hin\"dII is a type II restriction enzyme found in \"Haemophilus influenzae\".\n\nH.O. Smith, K.W. Wilcox, and T.J. Kelley, working at Johns Hopkins University in 1968, isolated and characterized the first restriction nuclease whose functioning depended on a specific DNA nucleotide sequence. Working with Haemophilus influenzae bacteria, this group isolated an enzyme, called HindII, that always cuts DNA molecules at a particular point within a specific sequence of six base pairs. This sequence is:\n\nThey found that the HindII enzyme always cuts directly in the center of this sequence. Wherever this particular sequence of six base pairs occurs unmodified in a DNA molecule, HindII will cleave both sugar-phosphate backbones of the DNA between the 3rd and 4th base pairs of the sequence. Moreover, HindII will only cleave a DNA molecule at this particular site. For this reason, this specific base sequence is known as the \"recognition sequence\" for HindII.\n\n", "id": "886069", "title": "HindII"}
{"url": "https://en.wikipedia.org/wiki?curid=26061258", "text": "Molecular Inversion Probe\n\nMolecular Inversion Probe (MIP) \nbelongs to the class of Capture by Circularization molecular techniques for performing \"genomic partitioning\", a process through which one captures and enriches specific regions of the genome. Probes used in this technique are single stranded DNA molecules and, similar to other genomic partitioning techniques, contain sequences that are complementary to the target in the genome; these probes hybridize to and capture the genomic target. MIP stands unique from other genomic partitioning strategies in that MIP probes share the common design of two genomic target complementary segments separated by a linker region. With this design, when the probe hybridizes to the target, it undergoes an inversion in configuration (as suggested by the name of the technique) and circularizes. Specifically, the two target complementary regions at the 5’ and 3’ ends of the probe become adjacent to one another while the internal linker region forms a free hanging loop. The technology has been used extensively in the HapMap project for large-scale SNP genotyping as well as for studying gene copy alterations\nand characteristics of specific genomic loci to identify biomarkers for different diseases such as cancer. Key strengths of the MIP technology include its high specificity to the target and its scalability for high-throughput, multiplexed analyses where tens of thousands of genomic loci are assayed simultaneously.\n\nThe probes are designed with sequences that are complementary to the genomic target at its <nowiki>5’</nowiki> and <nowiki>3’</nowiki> ends\nThe internal region contains two universal PCR primer sites that are common to all MIPs as well as a probe-release site, which is usually a restriction site. If the identification of the captured genomic target is performed using array-based hybridization approaches, the internal region may optionally contain a probe-specific tag sequence that uniquely identifies the given probe as well as a tag-release site, which, similar to the probe-release site, is also a restriction site.\n\nProbes are added to the genomic DNA sample. After a denaturation followed by an annealing step, the target-complementary ends of the probe are hybridized to the target DNA. The probes then undergo circularization in this process. These probes, however, are designed such that a gap delimited by the hybridized ends of the probes remains over the target region. The size of the gap ranges from a single nucleotide for SNP genotyping \n to several hundred nucleotides for loci capture (e.g. exome capture).\n\nThe gap is filled by DNA polymerase using free nucleotides and the ends of the probe are ligated by ligase, resulting in a fully circularized probe.\n\nSince gap filling is not performed for non-reacted probes, they remain linear. Exonuclease treatment removes these non-reacted probes as well as any remaining linear DNA in the reaction.\n\nIn some versions of the protocol, the probe-release site (commonly a restriction site) is cleaved by restriction enzymes such that the probe becomes linearized. In this linearized probe the universal PCR primer sequences are located at the 5’ and 3’ ends and the captured genomic target becomes part of the internal segment of the probe. Other protocols leave the probe as a circularized molecule.\n\nIf the probe is linearized, traditional PCR amplification is performed to enrich the captured target using the universal primers of the probe. Otherwise, rolling circle amplification is performed for the circular probe.\n\nThe captured target can be identified either via array-based hybridization approaches or by sequencing of the target. If array-based approach is used, the probe may optionally contain a probe-specific tag that uniquely identifies the probe as well as the genomic region targeted by it. The tags from each probe are released by cleaving the tag release site with restriction enzymes. These tags are then hybridized to the sequences that are placed on the array and are complementary to them. The captured target can also be identified by sequencing the probe, now also containing the target. Traditional Sanger sequencing or cheaper, more high-throughput technologies such as SOLiD, Illumina or Roche 454 can be used for this purpose.\n\nMultiplex analysis\nAlthough each probe examines one specific genomic locus, multiple probes can be combined into a single tube for multiplexed assay that simultaneously examines multiple loci. Currently, multiplexed MIP analysis can examine more than 55,000 loci in a single assay.\n\nThe design of the molecular inversion probes (MIP) originated from padlock probes, a molecular biology technique first reported by Nilsson \"et al.\" in 1994 \nSimilar to MIP, padlock probes are single stranded DNA molecules with two 20-nucleotide long segments complementary to the target connected by a 40-nucleotide long linker sequence. When the target complementary regions are hybridized to the DNA target, the padlock probes also become circularized. However, unlike MIP, padlock probes are designed such that the target complementary regions span the entire target region upon hybridization, leaving no gaps. Thus, padlock probes are only useful for detecting DNA molecules with known sequences.\n\nNilsson \"et al.\" demonstrated the use of padlock probes to detect numerous DNA targets, including a synthetic oligonucleotide and a circular genomic clone. Padlock probes have high specificity towards their target and can distinguish target molecules that closely resemble one another. Nilsson \"et al.\" also demonstrated the use of padlock probes to differentiate between a normal and a mutant cystic fibrosis conductance receptor (\"CFCR\") where the CFCR mutant had a 3bp deletion corresponding to one of the ends of the probe. Since ligation requires the ends of the probe to be immediately adjacent to one another when hybridized to the target, the 3bp deletion in the mutant prevented successful ligation. Padlock probes were also successfully used for in situ hybridization to detect alphoid repeats specific to chromosome 12 in a sample of chromosomes in metastasis state. Here, traditional, linear oligonucleotide probes failed to yield results. Thus, padlock probes possess sufficient specificity to detect single copy elements in the genome.\n\nIn order to perform SNP genotyping, Hardenbol \"et al.\" modified padlock probes such that when the probe is hybridized to the genomic target, there is a gap at the SNP position. Gap filling using a nucleotide that is complementary to the nucleotide at the SNP location determines the identity of the polymorphism. This design brings numerous benefits over the more traditional padlock probe technique. Using multiple padlock probes specific to a plausible SNP requires careful balancing of the concentration of these allele specific probes to ensure SNP counts at a given locus are properly normalized. In addition, with this design, bad probes affect all genotypes at a given locus equally. For instance, since MIP probes can assay multiple genotypes at a particular genomic locus, if the probe for a given locus does not work (e.g. fails to properly hybridize to the genomic target), none of the genotypes at this locus will be detected. In contrast, for padlock probes, one needs to design a distinct padlock probe to detect each plausible genotype a given locus (e.g. one padlock probe is needed for detecting \"A\" at a given SNP locus and another padlock probe is needed for detecting \"T\" at the locus). Thus, a bad padlock probe will only affect the detection of the specific genotype that the probe is designed to detect whereas a bad MIP probe will affect all genotypes at the locus. Using MIP, one avoids potential incorrect SNP calling since if the probe designed to assay a given locus does not work, no data is generated for this locus and no SNP calling is performed.\n\nIn their procedure, Hardenbol \"et al.\" assayed more than 1000 SNP loci simultaneously in a single tube where the tube contained more than 1000 probes with distinct designs. The pool of probes was aliquoted into four tubes for four different reactions. In each reaction, a distinct nucleotide (A, T, C or G) was used for gap filling. Only when the nucleotide at the SNP locus was complementary to the applied nucleotide would the gap be closed by ligation and the probe be circularized. Identification of the captured SNPs was performed on genotyping arrays where each spot on the array contained sequences complementary to the locus-specific tags in the probes. \nSince the DNA array costs is a major contributor to the cost of this technique, the performance of four-chip-one-color detection was compared to two-chip-two color detection. The results were found to be similar in terms of SNP call rate and signal-to-noise ratio.\n\nIn a recent report, this group successfully increased the level of multiplexing to simultaneously assay more than 10,000 SNP loci, using 12,000 distinct probes. The study examined SNP polymorphisms in 30 trio samples (each trio consisted of a mother, father and their child). Knowing the genotypes of the parents, the accuracy of the SNP genotypes predicted in the child was determined by examining whether a concordance existed between the expected Mendelian inheritance patterns and the predicted genotypes. Trio concordance rate has been found to be > 99.6%. In addition, a set of MIP-specific performance metrics was developed. This work set the framework for high-throughput SNP genotyping in the HapMap project.\n\nTo capture longer genomic regions than a single nucleotide, Akhras \"et al.\" modified the design of MIP by extending the gap delimited by the hybridized probe ends and named the design Connector Inversion Probe (CIP). The gap corresponds to the genomic region of interest to be captured (e.g. exons). Gap filling reaction is achieved with DNA polymerase, using all four nucleotides. Identification of the captured regions can then be done by sequencing them using locus-specific primers that map to one of the target complementary ends of the probes.\n\nAkhras \"et al.\" also developed the multiplexing multiplex padlocks (MMP) barcode system in order to lower the costs of reagents. A single assay might involve DNA samples from multiple individuals and examine multiple genomic loci in each individual. A DNA barcode system that uniquely identifies each plausible combination of individual and genomic locus is represented as DNA tags that were inserted into the linker region of the probes. Thus, sequences from the captured regions would include the barcode, allowing the non-ambiguous determination of the individual and the genomic locus that the captured region belongs to.\n\nThis group has also developed a software for designing locus-specific CIPs (CIP creator 1.0.1).\n\nMolecular Inversion Probe (MIP) is one of the techniques widely used to capture a small region of the genome for further examination. With the invention of the next generation sequencing technologies, the cost of sequencing whole genomes has decreased dramatically, however the cost is still too high for these sequencing machines to be used in practice in every laboratory. Instead, different genome partitioning techniques can be used to isolate smaller but highly specific regions of the genome for further analysis. MIP, for instance, can be used to capture targets for SNPgenotyping, copy number variation or allelic imbalance studies, to name a few.\n\nIn SNP genotyping, the probes are separated into four reactions and a different type of nucleotide is added to each reaction. If the SNP at the target region is complementary to the added nucleotide, the ligation is successful and the probe becomes fully circularized. Since each probe hybridizes to exactly one SNP target in the genome, successfully circularized probes provide the nucleotide identities of the SNPs. The tag sequences from the four nucleotide-specific reactions are then hybridized to either four genotyping arrays or two, dual-colour arrays (one channel for each reaction). Analyzing which spots on the array are bound by the tags allows the determination of the SNP identities at the genomic loci represented by those tags.\n\nThe SNPs targeted by MIP can then be used in areas of research such as quantitative trait loci (QTL) analysis or genome-wide association studies (GWAS) where the SNPs are used in either indirect linkage disequilibrium studies or directly screened for causative mutations.\n\nMolecular inversion probe technique can also be used for copy number variation (CNV) detection. This dual role in SNP genotyping as well as CNV analysis of MIP is similar to the high-density SNP genotyping arrays which have recently been used for CNV detection and analysis as well. These techniques extract the allele-specific signal intensities from genotyping data and use that to generate CNV results. These techniques have higher precision and resolution than traditional techniques such as G-banded karyotypic analyses, fluorescence in situ hybridization (FISH) or array comparative genomic hybridization (aCGH).\n\nMIP has been used extensively in many areas of research; some of the examples of the use of this technique in recent literature are outlined below:\n\n\nTo optimize the degree of multiplexing and the lengths of the captured regions, a number of factors should be considered when designing probes:\n\n\nA number of experimental conditions can be modified for optimization, these include:\n\nThese factors are critical since in one study, proper optimization strategies increased target capture efficiency from 18 to 91 percent.\n\nTurner \"et al.\" 2009 summarized two metrics that are commonly reported in MIP-based genomic capture experiments that identify the target by sequencing.\n\n\nThese two metrics are directly affected by the quality of the batch of probes.\nTo improve the results for low quality probes, higher levels of sequencing depths can be performed. The amount of sequencing scales needed nearly exponentially with decreases in uniformity and specificity.\n\nHardenbol \"et al.\" 2005 proposed a set of metrics that concern SNP genotyping using MIPs.\n\nAn inherent trade-off exists between probe conversion rate and accuracy. Removing probes that yielded incorrect genotypes \"increases\" the accuracy but \"decreases\" the probe conversion rate. In contrast, using a lenient probe acceptance threshold \"increases\" probe conversion rate but \"decreases\" the accuracy.\n\nTo reduce the costs from sequencing whole genomes, many methods that enrich specific genomic regions of interest have been proposed.\n\nGene selector method: An initial multiplex PCR step is performed to enrich the targets of interest. The PCR products are circularized upon hybridization to target-specific probes with sequences complementary to the two primers used in the PCR step.\n\nCapture by selective circularization method: The genomic DNA is digested into fragments with restriction enzymes. Using selector probes with flanking regions that are complementary to the target of interest, the digested DNA fragments are circularized upon hybridization to the selector probes.\n\nEach method demonstrates trade offs between uniformity, capture specificity, cost, scalability and availability.\n\n\n\n\n\n", "id": "26061258", "title": "Molecular Inversion Probe"}
{"url": "https://en.wikipedia.org/wiki?curid=26224857", "text": "Paired-end tag\n\nPaired-end tags (PET) (sometimes \"Paired-End diTags\", or simply \"ditags\") are the short sequences at the 5’ and 3’ ends of a DNA fragment which are unique enough that they (theoretically) exist together only once in a genome, therefore making the sequence of the DNA in between them available upon search (if full-genome sequence data is available) or upon further sequencing (since tag sites are unique enough to serve as primer annealing sites). Paired-end tags (PET) exist in PET libraries with the intervening DNA absent, that is, a PET \"represents\" a larger fragment of genomic or cDNA by consisting of a short 5' linker sequence, a short 5' sequence tag, a short 3' sequence tag, and a short 3' linker sequence. It was shown conceptually that 13 base pairs are sufficient to map tags uniquely. However, longer sequences are more practical for mapping reads uniquely. The endonucleases (discussed below) used to produce PETs give longer tags (18/20 base pairs and 25/27 base pairs) but sequences of 50–100 base pairs would be optimal for both mapping and cost efficiency. After extracting the PETs from many DNA fragments, they are linked (concatenated) together for efficient sequencing. On average, 20–30 tags could be sequenced with the Sanger method, which has a longer read length. Since the tag sequences are short, individual PETs are well suited for next-generation sequencing that has short read lengths and higher throughput. The main advantages of PET sequencing are its reduced cost by sequencing only short fragments, detection of structural variants in the genome, and increased specificity when aligning back to the genome compared to single tags, which involves only one end of the DNA fragment. \n\nPET libraries are typically prepared in two general methods: cloning based and cloning-free based.\n\nFragmented genomic DNA or complementary DNA (cDNA) of interest is cloned into plasmid vectors. The cloning sites are flanked with adaptor sequences that contain restriction sites for endonucleases (discussed below). Inserts are ligated to the plasmid vectors and individual vectors are then transformed into \"E. coli\" making the PET library. PET sequences are obtained by purifying plasmid and digesting with specific endonuclease leaving two short sequences on the ends of the vectors. Under intramolecular (dilute) conditions, vectors are re-circularized and ligated, leaving only the ditags in the vector. The sequences unique to the clone are now paired together. Depending on the next-generation sequencing technique, PET sequences can be left singular, dimerized, or concatenated into long chains.\n\nInstead of cloning, adaptors containing the endonuclease sequence are ligated to the ends of fragmented genomic DNA or cDNA. The molecules are then self-circularized and digested with endonuclease, releasing the PET. Before sequencing, these PETs are ligated to adaptors to which PCR primers anneal for amplification. \nThe advantage of cloning based construction of the library is that it maintains the fragments or cDNA intact for future use. However, the construction process is much longer than the cloning-free method. Variations on library construction have been produced by next-generation sequencing companies to suit their respective technologies.\n\nUnlike other endonucleases, the MmeI (type IIS) and EcoP15I (type III) restriction endonucleases cut downstream of their target binding sites. MmeI cuts 18/20 base pairs downstream and EcoP15I cuts 25/27 base pairs downstream. As these restriction enzymes bind at their target sequences located in the adaptors, they cut and release vectors that contain short sequences of the fragment or cDNA ligated to them, producing PETs.\n\n", "id": "26224857", "title": "Paired-end tag"}
{"url": "https://en.wikipedia.org/wiki?curid=26415811", "text": "COLD-PCR\n\nCOLD-PCR (co-amplification at lower denaturation temperature-PCR) is a modified Polymerase Chain Reaction (PCR) protocol that enriches variant alleles from a mixture of wildtype and mutation-containing DNA. The ability to preferentially amplify and identify minority alleles and low-level somatic DNA mutations in the presence of excess wildtype alleles is useful for the detection of mutations. Detection of mutations is important in the case of early cancer detection from tissue biopsies and body fluids such as blood plasma or serum, assessment of residual disease after surgery or chemotherapy, disease staging and molecular profiling for prognosis or tailoring therapy to individual patients, and monitoring of therapy outcome and cancer remission or relapse. Common PCR will amplify both the major (wildtype) and minor (mutant) alleles with the same efficiency, occluding the ability to easily detect the presence of low-level mutations. The capacity to detect a mutation in a mixture of variant/wildtype DNA is valuable because this mixture of variant DNAs can occur when provided with a heterogeneous sample – as is often the case with cancer biopsies. Currently, traditional PCR is used in tandem with a number of different downstream assays for genotyping or the detection of somatic mutations. These can include the use of amplified DNA for RFLP analysis, MALDI-TOF (matrix-assisted laser-desorption–time-of-flight) genotyping, or direct sequencing for detection of mutations by Sanger sequencing or pyrosequencing. Replacing traditional PCR with COLD-PCR for these downstream assays will increase the reliability in detecting mutations from mixed samples, including tumors and body fluids.\n\nThe underlying principle of COLD-PCR is that single nucleotide mismatches will slightly alter the melting temperature (Tm) of the double-stranded DNA. Depending on the sequence context and position of the mismatch, Tm changes of 0.2-1.5 °C (0.36-2.7 °F). are common for sequences up to 200bp or higher. Knowing this the authors of the protocol took advantage of two observations:\n\nKeeping these principles in mind the authors developed the following general protocol:\n\nThere are two forms of COLD-PCR that have been developed to date. Full COLD-PCR and Fast COLD-PCR.\nFull COLD-PCR is identical to the protocol outlined above. These five stages are used for each round of amplification.\n\nFast COLD-PCR differs from Full COLD-PCR in that the denaturation and intermediate annealing stages are skipped. This is because, in some cases, the preferential amplification of the mutant DNA is so great that ensuring the formation of the mutant/wildtype heteroduplex DNA is not needed. Thus the denaturation can occur at the Tc, proceed to primer annealing, and then polymerase-mediated extension. Each round of amplification will include these three stages in that order. By utilizing the lower denaturation temperature, the reaction will discriminate towards the products with the lower Tm – i.e. the variant alleles. Fast COLD-PCR produces much faster results due to the shortened protocol. However, it is important to note that Full COLD-PCR is essential for amplification of all possible mutations in the starting mixture of DNA.\n\nTwo-round COLD-PCR is a modified version of Fast COLD-PCR. During the second round of Fast COLD-PCR nested primers are used. This improves the sensitivity of mutation detection compared to one-round Fast COLD-PCR.\n\nCOLD-PCR has been used to improve the reliability of a number of different assays that traditionally use conventional PCR.\nA restriction fragment length polymorphism results in the cleavage (or absence thereof) of DNA for a specific mutation by a selected restriction enzyme that will not cleave the wildtype DNA. In a study using a mixture of wildtype and mutation containing DNA amplified by regular PCR or COLD-PCR, COLD-PCR preceding RFLP analysis was shown to improve the mutation detection by 10-20 fold.\n\nSanger sequencing recently was used to evaluate the enrichment of mutant DNA from a mixture of 1:20 mutant:wildtype DNA. The variant DNA containing a mutation was obtained from a breast cancer cell line known to contain p53 mutations. Comparison of Sanger sequencing chromatograms indicated that the mutant allele was enriched 13 fold when COLD-PCR was used compared to traditional PCR alone. This was determined by the size of the peaks on the chromatogram at the variant allele location.\n\nAs well, COLD-PCR was used to detect p53 mutations from lung-adenocarcinoma samples. The study was able to detect 8 low level (under 20% abundance) mutations that would likely have been missed using conventional methods that don’t enrich for variant sequence DNA .\n\nSimilar to its use in direct Sanger sequencing, with pyrosequencing COLD-PCR was shown to be capable of detecting mutations that had a prevalence 0.5-1% from the samples used. COLD-PCR was used to detect p53 and KRAS mutations by pyrosequencing, and was shown to outperform conventional PCR in both cases.\n\nThe same research group that developed COLD-PCR and used it to compare the sensitivity of regular PCR for genotyping with direct Sanger sequencing, RFLP, and pyrosequencing, also ran a similar study using MALDI-TOF as a downstream application for detecting mutations. Their results indicated that COLD-PCR could enrich mutation sequences from a mixture of DNA by 10-100 fold and that mutations with an initial prevalence of 0.1-0.5% would be detectable. Compared to the 5-10% low-level detection rate expected with traditional PCR.\n\nCOLD-PCR run on a quantitative PCR machine, using TaqMan probes specific for a mutation, was shown to increase the measured difference between mutant and wildtype samples.\n\n\n\nCOLD-PCR was originally described by Li \"et al.\" in a Nature Medicine paper published in 2008 from Dr. Mike Makrigiorgos’s lab group at the Dana Farber Cancer Institute of Harvard Medical School. As summarized above, the technology has been used in a number of proof-of-principle experiments and medical research diagnostic experiments.\n\nRecently, the COLD-PCR technology has been licensed by Transgenomic, Inc. The licensing terms include the exclusive rights to commercialize the technology combined with Sanger sequencing. The plans are to develop commercial applications that will allow for rapid high-sensitivity detection of low-level somatic and mitochondrial DNA mutations.\n\nOther technologies are available for the detection of minority DNA mutations, and these methods can be segregated into their ability to enrich for and detect either known or unknown mutations.\n", "id": "26415811", "title": "COLD-PCR"}
{"url": "https://en.wikipedia.org/wiki?curid=26615202", "text": "Biological pathway\n\nA biological pathway is a series of interactions among molecules in a cell that leads to a certain product or a change in a cell. Such a pathway can trigger the assembly of new molecules, such as a fat or protein. Pathways can also turn genes on and off, or spur a cell to move. Some of the most common biological pathways are involved in metabolism, the regulation of gene expression and the transmission of signals. Pathways play key role in advanced studies of genomics.\n\nMost common types of biological pathways:\n\n\n", "id": "26615202", "title": "Biological pathway"}
{"url": "https://en.wikipedia.org/wiki?curid=8371887", "text": "Interrupted gene\n\nAn interrupted gene (also called a split gene) is a gene that contains sections of DNA called exons, which are expressed as RNA and protein, interrupted by sections of DNA called introns, which are not expressed.\n\nThe DNA sequence in the exon provides instructions for coding proteins. The function of the intron was not understood at first, and they were called noncoding or junk DNA. Split genes were independently discovered by Richard J. Roberts and Phillip A. Sharp in 1977, for which they shared the 1993 Nobel Prize in Physiology or Medicine Their discovery implied the existence of then-unknown machinery for splicing out introns and assembling genes; namely, the spliceosome. It was soon accepted that 94% of human genes were interrupted, and perhaps 50% of hereditary diseases involved errors in splicing introns out of interrupted genes. The best-known example of a disease caused by a splicing error is Beta-thalassemia, in which extra intronic material is erroneously spliced into the gene for making hemoglobin.\n\nLower eukaryotes, including yeast, have many \"uninterrupted\" regions, as they contain long stretches of exons that create the mRNA necessary for the synthesis of proteins. This does not mean, however, that these sections are fully uninterrupted, as tRNA synthesis requires excision of a nucleotide sequence, followed by ligation. Nevertheless, gene interruption is the rule.\n\nMost bacteria have some interruption of some genes. Interrupted genes are universal in eukaryotes; yeasts may display single interruptions of a minority of genes, while in higher organisms most genes are interrupted, some multiple times and with introns that can be longer than exons. Introns are well-conserved across evolutionary history, suggesting their structure has some importance for the organism, and they are longer in advanced organisms (higher plants and animals), whose longer growth and development requires longer sequences of gene activation and down-regulation. Details of the role of introns in the regulation of gene accessibility and transcription have yet to be worked out.\n\nThe architecture of the interrupted gene allows for the process of alternative splicing, where various mRNA products can be produced from a single gene.\n", "id": "8371887", "title": "Interrupted gene"}
{"url": "https://en.wikipedia.org/wiki?curid=26080977", "text": "List of homing endonuclease cutting sites\n\nThe homing endonucleases are a special type of restriction enzymes encoded by introns or inteins. They act on the cellular DNA of the cell that synthesizes them; to be precise, in the opposite allele of the gene that encode them.\n\nThe list includes some of the most studied examples. The following concepts have been detailed:\n\n<nowiki>*</nowiki>: Nicking endonuclease: These enzymes cut only one DNA strand, leaving the other strand untouched.<br>\n<nowiki>**</nowiki>: Unknown cutting site: Researchers have not been able to determine the exact cutting site of these enzymes yet.\n\n\n\"Databases and lists of restriction enzymes:\"\n\"Databases of proteins:\"\n", "id": "26080977", "title": "List of homing endonuclease cutting sites"}
{"url": "https://en.wikipedia.org/wiki?curid=9299409", "text": "Nucleic acid thermodynamics\n\nNucleic acid thermodynamics is the study of how temperature affects the nucleic acid structure of double-stranded DNA (dsDNA). The melting temperature (\"T\") is defined as the temperature at which half of the DNA strands are in the random coil or single-stranded (ssDNA) state. \"T\" depends on the length of the DNA molecule and its specific nucleotide sequence. DNA, when in a state where its two strands are dissociated (i.e., the dsDNA molecule exists as two independent strands), is referred to as having been denatured by the high temperature.\n\nHybridization is the process of establishing a non-covalent, sequence-specific interaction between two or more complementary strands of nucleic acids into a single complex, which in the case of two strands is referred to as a duplex. Oligonucleotides, DNA, or RNA will bind to their complement under normal conditions, so two perfectly complementary strands will bind to each other readily. In order to reduce the diversity and obtain the most energetically preferred complexes, a technique called annealing is used in laboratory practice. However, due to the different molecular geometries of the nucleotides, a single inconsistency between the two strands will make binding between them less energetically favorable. Measuring the effects of base incompatibility by quantifying the temperature at which two strands anneal can provide information as to the similarity in base sequence between the two strands being annealed. The complexes may be dissociated by thermal denaturation, also referred to as melting. In the absence of external negative factors, the processes of hybridization and melting may be repeated in succession indefinitely, which lays the ground for polymerase chain reaction. Most commonly, the pairs of nucleic bases A=T and G≡C are formed, of which the latter is more stable.\n\nDNA denaturation, also called DNA melting, is the process by which double-stranded deoxyribonucleic acid unwinds and separates into single-stranded strands through the breaking of hydrophobic stacking attractions between the bases. See Hydrophobic effect. Both terms are used to refer to the process as it occurs when a mixture is heated, although \"denaturation\" can also refer to the separation of DNA strands induced by chemicals like formamide or urea.\n\nThe process of DNA denaturation can be used to analyze some aspects of DNA. Because cytosine / guanine base-pairing is generally stronger than adenine / thymine base-pairing, the amount of cytosine and guanine in a genome (called the \"GC content\") can be estimated by measuring the temperature at which the genomic DNA melts. Higher temperatures are associated with high GC content.\n\nDNA denaturation can also be used to detect sequence differences between two different DNA sequences. DNA is heated and denatured into single-stranded state, and the mixture is cooled to allow strands to rehybridize. Hybrid molecules are formed between similar sequences and any differences between those sequences will result in a disruption of the base-pairing. On a genomic scale, the method has been used by researchers to estimate the genetic distance between two species, a process known as DNA-DNA hybridization. In the context of a single isolated region of DNA, denaturing gradient gels and temperature gradient gels can be used to detect the presence of small mismatches between two sequences, a process known as temperature gradient gel electrophoresis.\n\nMethods of DNA analysis based on melting temperature have the disadvantage of being proxies for studying the underlying sequence; DNA sequencing is generally considered a more accurate method.\n\nThe process of DNA melting is also used in molecular biology techniques, notably in the polymerase chain reaction. Although the temperature of DNA melting is not diagnostic in the technique, methods for estimating \"T\" are important for determining the appropriate temperatures to use in a protocol. DNA melting temperatures can also be used as a proxy for equalizing the hybridization strengths of a set of molecules, e.g. the oligonucleotide probes of DNA microarrays.\n\nAnnealing, in genetics, means for complementary sequences of single-stranded DNA or RNA to pair by hydrogen bonds to form a double-stranded polynucleotide. Before annealing can occur, one of the strands may need to be phosphorylated by an enzyme such as kinase to allow proper hydrogen bonding to occur. The term annealing is often used to describe the binding of a DNA probe, or the binding of a primer to a DNA strand during a polymerase chain reaction. The term is also often used to describe the reformation (renaturation) of reverse-complementary strands that were separated by heat (thermally denatured). Proteins such as RAD52 can help DNA anneal.\n\nSeveral formulas are used to calculate \"T\" values. Some formulas are more accurate in predicting melting temperatures of DNA duplexes. For DNA oligonucleotides, i.e. short sequences of DNA, the thermodynamics of hybridization can be accurately described as a two-state process. In this approximation one neglects the possibility of intermediate partial binding states in the formation of a double strand state from two single stranded oligonucleotides. Under this assumption one can elegantly describe the thermodynamic parameters for forming double-stranded nucleic acid AB from single-stranded nucleic acids A and B.\n\nThe equilibrium constant for this reaction is formula_1. According to the Van´t Hoff equation, the relation between free energy, Δ\"G\", and \"K\" is Δ\"G°\" = -\"RT\"ln \"K\", where \"R\" is the ideal gas law constant, and \"T\" is the kelvin temperature of the reaction. This gives, for the nucleic acid system,\n\nformula_2.\n\nThe melting temperature, \"T\", occurs when half of the double-stranded nucleic acid has dissociated. If no additional nucleic acids are present, then [A], [B], and [AB] will be equal, and equal to half the initial concentration of double-stranded nucleic acid, [AB]. This gives an expression for the melting point of a nucleic acid duplex of\n\nformula_3.\n\nBecause Δ\"G\"° = Δ\"H\"° -\"T\"Δ\"S\"°, \"T\" is also given by\n\nformula_4.\n\nThe terms Δ\"H\"° and Δ\"S\"° are usually given for the association and not the dissociation reaction (see the nearest-neighbor method for example). This formula then turns into:\n\nformula_5, where [B] < [A].\n\nAs mentioned, this equation is based on the assumption that only two states are involved in melting: the double stranded state and the random-coil state. However, nucleic acids may melt via several intermediate states. To account for such complicated behavior, the methods of statistical mechanics must be used, which is especially relevant for long sequences.\n\nThe previous paragraph shows how melting temperature and thermodynamic parameters (Δ\"G\"° or Δ\"H\"° & Δ\"S\"°) are related to each other. From the observation of melting temperatures one can experimentally determine the thermodynamic parameters. Vice versa, and important for applications, when the thermodynamic parameters of a given nucleic acid sequence are known, the melting temperature can be predicted. It turns out that for oligonucleotides, these parameters can be well approximated by the nearest-neighbor model.\n\nThe interaction between bases on different strands depends somewhat on the neighboring bases. Instead of treating a DNA helix as a string of interactions between base pairs, the nearest-neighbor model treats a DNA helix as a string of interactions between 'neighboring' base pairs. So, for example, the DNA shown below has nearest-neighbor interactions indicated by the arrows.\nThe free energy of forming this DNA from the individual strands, Δ\"G\"°, is represented (at 37 °C) as\n\nΔ\"G\"°(predicted) = Δ\"G\"°(CG initiation) + Δ\"G\"°(CG/GC) + Δ\"G\"°(GT/CA) + Δ\"G\"°(TT/AA) + Δ\"G\"°(TG/AC) + Δ\"G\"°(GA/CT) + Δ\"G\"°(AT initiation)\n\nThe first term represents the free energy of the first base pair, CG, in the absence of a nearest neighbor. The second term includes both the free energy of formation of the second base pair, GC, and stacking interaction between this base pair and the previous base pair. The remaining terms are similarly defined. In general, the free energy of forming a nucleic acid duplex is\n\nformula_6.\n\nEach Δ\"G\"° term has enthalpic, Δ\"H\"°, and entropic, Δ\"S\"°, parameters, so the change in free energy is also given by\n\nformula_7.\n\nValues of Δ\"H\"° and Δ\"S\"° have been determined for the ten possible pairs of interactions. These are given in Table 1, along with the value of Δ\"G\"° calculated at 37 °C. Using these values, the value of Δ\"G\"° for the DNA helix shown above is calculated to be −22.4 kJ/mol. The experimental value is −21.8 kJ/mol.\n\nThe parameters associated with the ten groups of neighbors shown in table 1 are determined from melting points of short oligonucleotide duplexes. Curiously, it works out that only eight of the ten groups are independent.\n\nThe nearest-neighbor model can be extended beyond the Watson-Crick pairs to include parameters for interactions between mismatches and neighboring base pairs. This allows the estimation of the thermodynamic parameters of sequences containing isolated mismatches, like e.g. (arrows indicating mismatch)\n\nThese parameters have been fitted from melting experiments and an extension of Table 1 which includes mismatches can be found in literature.\n\nA more realistic way of modeling the behavior of nucleic acids would seem to be to have parameters that depend on the neighboring groups on both sides of a nucleotide, giving a table with entries like \"TCG/AGC\". However, this would involve around 32 groups for Watson-Crick pairing and even more for sequences containing mismatches; the number of DNA melting experiments needed to get reliable data for so many groups would be inconveniently high. However, other means exist to access thermodynamic parameters of nucleic acids: microarray technology allows hybridization monitoring of tens of thousands sequences in parallel. This data, in combination with molecular adsorption theory allows the determination of many thermodynamic parameters in a single experiment and to go beyond the nearest neighbor model. In general the predictions from the nearest neighbor method agree reasonably well with experimental results, but some unexpected outlying sequences, calling for further insights, do exist. Finally, we should also mention the increased accuracy provided by single molecule unzipping assays which provide a wealth of new insight into the thermodynamics of DNA hybridization and the validity of the nearest-neighbour model as well.\n\n\n", "id": "9299409", "title": "Nucleic acid thermodynamics"}
{"url": "https://en.wikipedia.org/wiki?curid=779847", "text": "Cosmid\n\nA cosmid is a type of hybrid plasmid that contains a Lambda phage cos sequence. Cosmids (cos sites + plasmid = cosmids) DNA sequences are originally from the lambda phage. They are often used as a cloning vector in genetic engineering. Cosmids can be used to build genomic libraries. They were first described by Collins and Hohn in 1978.\nCosmids can contain 37 to 52 (normally 45) kb of DNA, limits based on the normal bacteriophage packaging size. They can replicate as plasmids if they have a suitable origin of replication: for example SV40 ori in mammalian cells, ColE1 ori for double-stranded DNA replication or f1 ori for single-stranded DNA replication in prokaryotes. They frequently also contain a gene for selection such as antibiotic resistance, so that the transformed cells can be identified by plating on a medium containing the antibiotic. Those cells which did not take up the cosmid would be unable to grow.\n\nUnlike plasmids, they can also be packaged in phage capsids, which allows the foreign genes to be transferred into or between cells by transduction. Plasmids become unstable after a certain amount of DNA has been inserted into them, because their increased size is more conducive to recombination. To circumvent this, phage transduction is used instead. This is made possible by the \"cohesive ends\", also known as \"cos\" sites. In this way, they are similar to using the lambda phage as a vector, except \"all\" the lambda genes have been deleted with the exception of the \"cos\" sequence.\n\n\"Cos\" sequences are ~200 base pairs long and essential for packaging. They contain a \"cosN\" site where DNA is nicked at each strand, 12 bp apart, by terminase. This causes linearization of the circular cosmid with two \"cohesive\" or \"sticky ends\" of 12bp. (The DNA must be linear to fit into a phage head.) The \"cosB\" site holds the terminase while it is nicking and separating the strands. The \"cosQ\" site of next cosmid (as rolling circle replication often results in linear concatemers) is held by the terminase after the previous cosmid has been packaged, to prevent degradation by cellular DNases.\n\nCosmids are predominantly plasmids with a bacterial \"oriV\", an antibiotic selection marker and a cloning site, but they carry one, or more recently two, cos sites derived from bacteriophage lambda. Depending on the particular aim of the experiment broad host range cosmids, shuttle cosmids or 'mammalian' cosmids (linked to SV40 oriV and mammalian selection markers) are available. The loading capacity of cosmids varies depending on the size of the vector itself but usually lies around 40–45 kb. The cloning procedure involves the generation of two vector arms which are then joined to the foreign DNA. Selection against wild type cosmid DNA is simply done via size exclusion. Cosmids, however, always form colonies and not plaques. Also the clone density is much lower with around 10 – 10 CFU per µg of ligated DNA.\n\nAfter the construction of recombinant lambda or cosmid libraries the total DNA is transferred into an appropriate \"E. coli\" host via a technique called in vitro packaging. The necessary packaging extracts are derived from \"E. coli\" cI857 lysogens (red- gam- Sam and Dam (head assembly) and Eam (tail assembly) respectively). These extracts will recognize and package the recombinant molecules \"in vitro\", generating either mature phage particles (lambda-based vectors) or recombinant plasmids contained in phage shells (cosmids). These differences are reflected in the different infection frequencies seen in favour of lambda-replacement vectors. This compensates for their slightly lower loading capacity. Phage libraries are also stored and screened more easily than cosmid libraries.\n\nTarget DNA: the genomic DNA to be cloned has to be cut into the appropriate size range of restriction fragments. This is usually done by partial restriction followed by either size fractionation or dephosphorylation (using calf-intestine phosphatase) to avoid chromosome scrambling, i.e. the ligation of physically unlinked fragments.\n\n", "id": "779847", "title": "Cosmid"}
{"url": "https://en.wikipedia.org/wiki?curid=25827435", "text": "Recombination detection program\n\nRecombination detection program (RDP) is a computer program used to analyse nucleotide sequence data and identify evidence of genetic recombination. Besides applying a large number of different recombination detection methods it also implements various phylogenetic tree construction methods and recombination hotspot tests. The latest version in RDP4.\n\n\n", "id": "25827435", "title": "Recombination detection program"}
{"url": "https://en.wikipedia.org/wiki?curid=26418006", "text": "Exome sequencing\n\nExome sequencing, also known as whole exome sequencing (WES), is a genomic technique for sequencing all of the protein-coding genes in a genome (known as the exome). It consists of two steps: the first step is to select only the subset of DNA that encodes proteins. These regions are known as exons – humans have about 180,000 exons, constituting about 1% of the human genome, or approximately 30 million base pairs. The second step is to sequence the exonic DNA using any high-throughput DNA sequencing technology.\n\nThe goal of this approach is to identify genetic variants that alter protein sequences, and to do this at a much lower cost than whole-genome sequencing. Since these variants can be responsible for both Mendelian and common polygenic diseases, such as Alzheimer's disease, whole exome sequencing has been applied both in academic research and as a clinical diagnostic.\n\nExome sequencing is especially effective in the study of rare Mendelian diseases, because it is an efficient way to identify the genetic variants in all of an individual's genes. These diseases are most often caused by very rare genetic variants that are only present in a tiny number of individuals; by contrast, techniques such as SNP arrays can only detect shared genetic variants that are common to many individuals in the wider population. Furthermore, because severe disease-causing variants are much more likely (but by no means exclusively) to be in the protein coding sequence, focusing on this 1% costs far less than whole genome sequencing but still detects a high yield of relevant variants.\n\nIn the past, clinical genetic tests were chosen based on the clinical presentation of the patient (i.e. focused on one gene or a small number known to be associated with a particular syndrome), or surveyed only certain types of variation (e.g. comparative genomic hybridization) but provided definitive genetic diagnoses in fewer than half of all patients. Exome sequencing is now increasingly used to complement these other tests: both to find mutations in genes already known to cause disease as well as to identify novel genes by comparing exomes from patients with similar features.\n\nTarget-enrichment methods allow one to selectively capture genomic regions of interest from a DNA sample prior to sequencing. Several target-enrichment strategies have been developed since the original description of the direct genomic selection (DGS) method in 2005.\n\nThough many techniques have been described for targeted capture, only a few of these have been extended to capture entire exomes. The first target enrichment strategy to be applied to whole exome sequencing was the array-based hybrid capture method in 2007, but in-solution capture has gained popularity in recent years. Twist Bioscience recently introduced Human Core Exome Enrichment Kit that enables researchers to perform more efficient capture of exomes than any other available method resulting in more complete enrichment of target sequences and lower sequencing depth requirements. \n\nMicroarrays contain single-stranded oligonucleotides with sequences from the human genome to tile the region of interest fixed to the surface. Genomic DNA is sheared to form double-stranded fragments. The fragments undergo end-repair to produce blunt ends and adaptors with universal priming sequences are added. These fragments are hybridized to oligos on the microarray.\nUnhybridized fragments are washed away and the desired fragments are eluted. The fragments are then amplified using PCR.\n\nRoche NimbleGen was first to take the original DGS technology and adapt it for next-generation sequencing. They developed the Sequence Capture Human Exome 2.1M Array to capture ~180,000 coding exons. This method is both time-saving and cost-effective compared to PCR based methods. The Agilent Capture Array and the comparative genomic hybridization array are other methods that can be used for hybrid capture of target sequences. Limitations in this technique include the need for expensive hardware as well as a relatively large amount of DNA.\n\nTo capture genomic regions of interest using in-solution capture, a pool of custom oligonucleotides (probes) is synthesized and hybridized in solution to a fragmented genomic DNA sample. The probes (labeled with beads) selectively hybridize to the genomic regions of interest after which the beads (now including the DNA fragments of interest) can be pulled down and washed to clear excess material. The beads are then removed and the genomic fragments can be sequenced allowing for selective DNA sequencing of genomic regions (e.g., exons) of interest.\n\nThis method was developed to improve on the hybridization capture target-enrichment method. In solution capture as opposed to hybrid capture, there is an excess of probes to target regions of interest over the amount of template required. The optimal target size is about 3.5 megabases and yields excellent sequence coverage of the target regions. The preferred method is dependent on several factors including: number of base pairs in the region of interest, demands for reads on target, equipment in house, etc.\n\nThere are many Next Generation Sequencing sequencing platforms available, postdating classical Sanger sequencing methodologies. Other platforms include Roche 454 sequencer and Life Technologies SOLiD systems, the Life Technologies Ion Torrent and Illumina's Illumina Genome Analyzer II (defunct) and subsequent Illumina MiSeq, HiSeq, and NovaSeq series instruments, all of which can be used for massively parallel exome sequencing. These 'short read' NGS systems are particularly well suited to analyse many relatively short stretches of DNA sequence, as found in human exons.\n\nThere are multiple technologies available that identify genetic variants. Each technology has advantages and disadvantages in terms of technical and financial factors. Two such technologies are microarrays and whole-genome sequencing.\n\nMicroarrays use hybridization probes to test the prevalence of known DNA sequences, thus they cannot be used to identify unexpected genetic changes. In contrast, the high-throughput sequencing technologies used in exome sequencing directly provide the nucleotide sequences of DNA at the thousands of exonic loci tested. Hence, WES addresses some of the present limitations of hybridization genotyping arrays.\n\nAlthough exome sequencing is more expensive than hybridization-based technologies on a per-sample basis, its cost has been decreasing due to the falling cost and increased throughput of whole genome sequencing.\n\nExome sequencing is only able to identify those variants found in the coding region of genes which affect protein function. It is not able to identify the structural and non-coding variants associated with the disease, which can be found using other methods such as whole genome sequencing. There remains 99% of the human genome that is not covered using exome sequencing. Presently, whole genome sequencing is rarely practical in the clinical context due to the high costs and time associated with sequencing full genomes. Exome sequencing allows sequencing of portions of the genome over at least 20 times as many samples compared to whole genome sequencing, at the same cost. For translation of identified rare variants into the clinic, sample size and the ability to interpret the results to provide a clinical diagnosis indicates that with the current knowledge in genetics, exome sequencing may be the most valuable.\n\nThe statistical analysis of the large quantity of data generated from sequencing approaches is a challenge. Even by only sequencing the exomes of individuals, a large quantity of data and sequence information is generated which requires a significant amount of data analysis. Challenges associated with the analysis of this data include changes in programs used to align and assemble sequence reads. Various sequence technologies also have different error rates and generate various read-lengths which can pose challenges in comparing results from different sequencing platforms.\n\nFalse positive and false negative findings are associated with genomic resequencing approaches and is a critical issue. A few strategies have been developed to improve the quality of exome data such as:\n\n\nRare recessive disorders would not have single nucleotide polymorphisms (SNPs) in public databases such as dbSNP. More common recessive phenotypes may have disease-causing variants reported in dbSNP. For example, the most common cystic fibrosis variant has an allele frequency of about 3% in most populations. Screening out such variants might erroneously exclude such genes from consideration. Genes for recessive disorders are usually easier to identify than dominant disorders because the genes are less likely to have more than one rare nonsynonymous variant. The system that screens common genetic variants relies on dbSNP which may not have accurate information about the variation of alleles. Using lists of common variation from a study exome or genome-wide sequenced individual would be more reliable. A challenge in this approach is that as the number of exomes sequenced increases, dbSNP will also increase in the number of uncommon variants. It will be necessary to develop thresholds to define the common variants that are unlikely to be associated with a disease phenotype.\n\nGenetic heterogeneity and population ethnicity are also major limitations as they may increase the number of false positive and false negative findings which will make the identification of candidate genes more difficult. Of course, it is possible to reduce the stringency of the thresholds in the presence of heterogeneity and ethnicity, however this will reduce the power to detect variants as well. Using a genotype-first approach to identify candidate genes might also offer a solution to overcome these limitations.\n\nNew technologies in genomics have changed the way researchers approach both basic and translational research. With approaches such as exome sequencing, it is possible to significantly enhance the data generated from individual genomes which has put forth a series of questions on how to deal with the vast amount of information. Should the individuals in these studies be allowed to have access to their sequencing information? Should this information be shared with insurance companies? This data can lead to unexpected findings and complicate clinical utility and patient benefit. This area of genomics still remains a challenge and researchers are looking into how to address these questions.\n\nBy using exome sequencing, fixed-cost studies can sequence samples to much higher depth than could be achieved with whole genome sequencing. This additional depth makes exome sequencing well suited to several applications that need reliable variant calls.\n\nCurrent association studies have focused on common variation across the genome, as these are the easiest to identify with our current assays. However, disease-causing variants of large effect have been found to lie within exomes in candidate gene studies, and because of negative selection, are found in much lower allele frequencies and may remain untyped in current standard genotyping assays. Whole genome sequencing is a potential method to assay novel variant across the genome. However, in complex disorders, a large number of genes are thought to be associated with disease risk. This heterogeneity of underlying risk means that very large sample sizes are required for gene discovery, and thus whole genome sequencing is not particularly cost-effective. In addition, variants in coding regions have been much more extensively studied and their functional implications are much easier to derive, making the practical applications of variants within the targeted exome region more immediately accessible.\n\nExome sequencing in rare variant gene discovery remains a very active and ongoing area of research: to date, few associated genes have been uncovered thus far, but there is growing evidence that a significant burden of risk is observed across sets of genes.\n\nIn Mendelian disorders of large effect, findings thus far suggest one or a very small number of variants within coding genes underlie the entire condition. Because of the severity of these disorders, the few causal variants are presumed to be extremely rare or novel in the population, and would be missed by any standard genotyping assay. Exome sequencing provides high coverage variant calls across coding regions, which are needed to separate true variants from noise. A successful model of Mendelian gene discovery involves the discovery of de novo variants using trio sequencing, where parents and proband are genotyped.\n\nA study published in September 2009 discussed a proof of concept experiment to determine if it was possible to identify causal genetic variants using exome sequencing. They sequenced four individuals with Freeman-Sheldon syndrome (FSS) (OMIM 193700), a rare autosomal dominant disorder known to be caused by a mutation in the gene MYH3. Eight HapMap individuals were also sequenced to remove common variants in order to identify the causal gene for FSS. After exclusion of common variants, the authors were able to identify MYH3, which confirms that exome sequencing can be used to identify causal variants of rare disorders. This was the first reported study that used exome sequencing as an approach to identify an unknown causal gene for a rare mendelian disorder.\n\nSubsequently, another group reported successful clinical diagnosis of a suspected Bartter syndrome patient of Turkish origin. Bartter syndrome is a renal salt-wasting disease. Exome sequencing revealed an unexpected well-conserved recessive mutation in a gene called SLC26A3 which is associated with congenital chloride diarrhea (CLD). This molecular diagnosis of CLD was confirmed by the referring clinician. This example provided proof of concept of the use of whole-exome sequencing as a clinical tool in evaluation of patients with undiagnosed genetic illnesses. This report is regarded as the first application of next generation sequencing technology for molecular diagnosis of a patient.\n\nA second report was conducted on exome sequencing of individuals with a mendelian disorder known as Miller syndrome (MIM#263750), a rare disorder of autosomal recessive inheritance. Two siblings and two unrelated individuals with Miller syndrome were studied. They looked at variants that have the potential to be pathogenic such as non-synonymous mutations, splice acceptor and donor sites and short coding insertions or deletions. Since Miller syndrome is a rare disorder, it is expected that the causal variant has not been previously identified. Previous exome sequencing studies of common single nucleotide polymorphisms (SNPs) in public SNP databases were used to further exclude candidate genes. After exclusion of these genes, the authors found mutations in DHODH that were shared among individuals with Miller syndrome. Each individual with Miller syndrome was a compound heterozygote for the DHODH mutations which were inherited as each parent of an affected individual was found to be a carrier.\n\nThis was the first time exome sequencing was shown to identify a novel gene responsible for a rare mendelian disease. This exciting finding demonstrates that exome sequencing has the potential to locate causative genes in complex diseases, which previously has not been possible due to limitations in traditional methods. Targeted capture and massively parallel sequencing represents a cost-effective, reproducible and robust strategy with high sensitivity and specificity to detect variants causing protein-coding changes in individual human genomes.\n\nExome sequencing can be used to diagnose the genetic cause of disease in a patient. Identification of the underlying disease gene mutation(s) can have major implications for diagnostic and therapeutic approaches, can guide prediction of disease natural history, and makes it possible to test at-risk family members. There are many factors that make exome sequencing superior to single gene analysis including the ability to identify mutations in genes that were not tested due to an atypical clinical presentation or the ability to identify clinical cases where mutations from different genes contribute to the different phenotypes in the same patient.\n\nHaving diagnosed a genetic cause of a disease, this information may guide the selection of appropriate treatment. The first time this strategy was performed successfully in the clinic was in the treatment of an infant with inflammatory bowel disease. A number of conventional diagnostics had previously been used, but the results could not explain the infant's symptoms. Analysis of exome sequencing data identified a mutation in the XIAP gene. Knowledge of this gene's function guided the infant's treatment, leading to a bone marrow transplantation which cured the child of disease.\n\nThe first laboratory to offer a WES diagnostic service for clinicians, including medical interpretation, was Ambry Genetics with the \"Clinical Diagnostic Exome\"\n\nResearchers have used exome sequencing to identify the underlying mutation for a patient with Bartter Syndrome and congenital chloride diarrhea. Bilgular's group also used exome sequencing and identified the underlying mutation for a patient with severe brain malformations, stating \"\"[These findings]highlight the use of whole exome sequencing to identify disease loci in settings in which traditional methods have proved challenging... Our results demonstrate that this technology will be particularly valuable for gene discovery in those conditions in which mapping has been confounded by locus heterogeneity and uncertainty about the boundaries of diagnostic classification, pointing to a bright future for its broad application to medicine\"\".\n\nResearchers at University of Cape Town, South Africa used exome sequencing to discover the genetic mutation of CDH2 as the underlying cause of a genetic disorder known as arrhythmogenic right ventricle cardiomyopathy (ARVC)‚ which increases the risk of heart disease and cardiac arrest. \n\nMultiple companies have offered exome sequencing to consumers.\n\nKnome was the first company to offer exome sequencing services to consumers, at a cost of several thousand dollars. Later, 23andMe ran a pilot WES program that was announced in September 2011 and was discontinued in 2012. Consumers could obtain exome data at a cost of $999. The company provided raw data, and did not offer analysis.\n\nIn November 2012, DNADTC, a division of Gene by Gene started offering exomes at 80X coverage and introductory price of $695. This price per DNADTC web site is currently $895. In October 2013, BGI announced a promotion for personal whole exome sequencing at 50X coverage for $499. \nIn June 2016 Genos was able to achieve an even lower price of $399 with a CLIA-certified 75X consumer exome sequenced from saliva. Helix launched their first offering of \"exome plus\" sequencing in November 2016 partnering with National Geographic which will empower consumers to learn about their genetic information through their application based platform.\n\n\n", "id": "26418006", "title": "Exome sequencing"}
{"url": "https://en.wikipedia.org/wiki?curid=8697", "text": "DNA ligase\n\nDNA ligase is a specific type of enzyme, a ligase, () that facilitates the joining of DNA strands together by catalyzing the formation of a phosphodiester bond. It plays a role in repairing single-strand breaks in duplex DNA in living organisms, but some forms (such as DNA ligase IV) may specifically repair double-strand breaks (i.e. a break in both complementary strands of DNA). Single-strand breaks are repaired by DNA ligase using the complementary strand of the double helix as a template, with DNA ligase creating the final phosphodiester bond to fully repair the DNA.\n\nDNA ligase is used in both DNA repair and DNA replication (see \"Mammalian ligases\"). In addition, DNA ligase has extensive use in molecular biology laboratories for recombinant DNA experiments (see \"Applications in molecular biology research\"). Purified DNA ligase is used in gene cloning to join DNA molecules together to form recombinant DNA.\n\nThe mechanism of DNA ligase is to form two covalent phosphodiester bonds between 3' hydroxyl ends of one nucleotide (\"acceptor\"), with the 5' phosphate end of another (\"donor\"). Two ATP molecules are consumed for each phosphodiester bond formed. AMP is required for the ligase reaction, which proceeds in four steps:\n\n1. Reorganization of activity site such as nicks in DNA segments or Okazaki fragments etc.\n\n2. Adenylation (addition of AMP) of a lysine residue in the active center of the enzyme, pyrophosphate is released;\n\n3. Transfer of the AMP to the 5' phosphate of the so-called donor, formation of a pyrophosphate bond; \n4. Formation of a phosphodiester bond between the 5' phosphate of the donor and the 3' hydroxyl of the acceptor. \n\nLigase will also work with blunt ends, although higher enzyme concentrations and different reaction conditions are required.\n\nThe \"E. coli\" DNA ligase is encoded by the \"lig\" gene. DNA ligase in \"E. coli\", as well as most prokaryotes, uses energy gained by cleaving nicotinamide adenine dinucleotide (NAD) to create the phosphodiester bond. It does not ligate blunt-ended DNA except under conditions of molecular crowding with polyethylene glycol, and cannot join RNA to DNA efficiently.\n\nThe DNA ligase from bacteriophage T4 is the ligase most-commonly used in laboratory research. It can ligate either cohesive or blunt ends of DNA, oligonucleotides, as well as RNA and RNA-DNA hybrids, but not single-stranded nucleic acids. It can also ligate blunt-ended DNA with much greater efficiency than \"E. coli\" DNA ligase. Unlike \"E. coli\" DNA ligase, T4 DNA ligase cannot utilize NAD and it has an absolute requirement for ATP as a cofactor. Some engineering has been done to improve the \"in vitro\" activity of T4 DNA ligase; one successful approach, for example, tested T4 DNA ligase fused to several alternative DNA binding proteins and found that the constructs with either p50 or NF-kB as fusion partners were over 160% more active in blunt-end ligations for cloning purposes than wild type T4 DNA ligase. A typical reaction for inserting a fragment into a plasmid vector would use about 0.01 (sticky ends) to 1 (blunt ends) units of ligase. The optimal incubation temperature for T4 DNA ligase is 16 °C. \n\nIn mammals, there are four specific types of ligase.\n\n\nDNA ligase from eukaryotes and some microbes uses adenosine triphosphate (ATP) rather than NAD.\n\nDerived from a thermophilic bacterium, the enzyme is stable and active at much higher temperatures than conventional DNA ligases. Its half-life is 48 hours at 65°C and greater than 1 hour at 95°C. Ampligase DNA Ligase has been shown to be active for at least 500 thermal cycles (94°C/80°C) or 16 hours of cycling. This exceptional thermostability permits extremely high hybridization stringency and ligation specificity.\n\nThere are at least three different units used to measure the activity of DNA ligase:\n\nDNA ligases have become indispensable tools in modern molecular biology research for generating recombinant DNA sequences. For example, DNA ligases are used with restriction enzymes to insert DNA fragments, often genes, into plasmids.\n\nControlling the optimal temperature is a vital aspect of performing efficient recombination experiments involving the ligation of cohesive-ended fragments. Most experiments use T4 DNA Ligase (isolated from bacteriophage T4), which is most active at 37 °C. However, for optimal ligation efficiency with cohesive-ended fragments (\"sticky ends\"), the optimal enzyme temperature needs to be balanced with the melting temperature T of the sticky ends being ligated, the homologous pairing of the sticky ends will not be stable because the high temperature disrupts hydrogen bonding. A ligation reaction is most efficient when the sticky ends are already stably annealed, and disruption of the annealing ends would therefore result in low ligation efficiency. The shorter the overhang, the lower the T.\n\nSince blunt-ended DNA fragments have no cohesive ends to anneal, the melting temperature is not a factor to consider within the normal temperature range of the ligation reaction. The limiting factor in blunt end ligation is not the activity of the ligase but rather the number of alignments between DNA fragment ends that occur. The most efficient ligation temperature for blunt-ended DNA would therefore be the temperature at which the greatest number of alignments can occur. The majority of blunt-ended ligations are carried out at 14-25 °C overnight. The absence of stably annealed ends also means that the ligation efficiency is lowered, requiring a higher ligase concentration to be used.\n\nA novel use of DNA ligase can be seen in the field of nano chemistry, specifically in DNA origami.  DNA based self-assembly principles have proven useful for organizing nanoscale objects, such as biomolecules, nanomachines, nanoelectronic and photonic component. Assembly of such nano structure requires the creation of an intricate mesh of DNA molecules. Although DNA self-assembly is possible without any outside help using different substrates such as provision of catatonic surface of Aluminium foil, DNA ligase can provide the enzymatic assistance that is required to make DNA lattice structure from DNA over hangs.\n\nThe first DNA ligase was purified and characterized in 1967 by the Gellert, Lehman, Richardson, and Hurwitz laboratories. It was first purified and characterized by Weiss and Richardson using a six-step chromatographic-fractionation process beginning with elimination of cell debris and addition of streptomycin, followed by several Diethylaminoethyl (DEAE)-cellulose column washes and a final phosphocellulose fractionation. The final extract contained 10% of the activity initially recorded in the \"E. coli \"media; along the process it was discovered that ATP and Mg++ were necessary to optimize the reaction. The common commercially available DNA ligases were originally discovered in bacteriophage T4, \"E. coli\" and other bacteria.\n\nGenetic deficiencies in human DNA ligases have been associated with clinical syndromes marked by immunodeficiency, radiation sensitivity, and developmental abnormalities,  LIG4 syndrome (Ligase IV syndrome) is a rare disease associated with mutations in DNA ligase 4 and interferes with dsDNA break-repair mechanisms. Ligase IV syndrome causes immunodeficiency in individuals and is commonly associated with microcephaly and marrow hypoplasia. A list of prevalent diseases caused due to lack of or malfunctioning of DNA ligase are as follows.\n\nXeroderma pigmentosum, which is commonly known as XP, is an inherited condition characterized by an extreme sensitivity to ultraviolet (UV) rays from sunlight. This condition mostly affects the eyes and areas of skin exposed to the sun. Some affected individuals also have problems involving the nervous system.\n\nMutations in the ATM gene cause ataxia-telangiectasia. The ATM gene provides instructions for making a protein that helps control cell division and is involved in DNA repair. This protein plays an important role in the normal development and activity of several body systems, including the nervous system and immune system. The ATM protein assists cells in recognizing damaged or broken DNA strands and coordinates DNA repair by activating enzymes that fix the broken strands. Efficient repair of damaged DNA strands helps maintain the stability of the cell's genetic information. Affected children typically develop difficulty walking, problems with balance and hand coordination, involuntary jerking movements (chorea), muscle twitches (myoclonus), and disturbances in nerve function (neuropathy). The movement problems typically cause people to require wheelchair assistance by adolescence. People with this disorder also have slurred speech and trouble moving their eyes to look side-to-side (oculomotor apraxia).\n\nFanconi anemia (FA) is a rare, inherited blood disorder that leads to bone marrow failure. FA prevents bone marrow from making enough new blood cells for the body to work normally. FA also can cause the bone marrow to make many faulty blood cells. This can lead to serious health problems, such as leukemia.\n\nBloom syndrome results in skin that is sensitive to sun exposure, and usually the development of a butterfly-shaped patch of reddened skin across the nose and cheeks. A skin rash can also appear on other areas that are typically exposed to the sun, such as the back of the hands and the forearms. Small clusters of enlarged blood vessels (telangiectases) often appear in the rash; telangiectases can also occur in the eyes. Other skin features include patches of skin that are lighter or darker than the surrounding areas (hypopigmentation or hyperpigmentation respectively). These patches appear on areas of the skin that are not exposed to the sun, and their development is not related to the rashes.\n\nIn recent studies, human DNA ligase I was used in Computer-aided drug design to identify DNA ligase inhibitors as possible therapeutic agents to treat cancer. Since excessive cell growth is a hallmark of cancer development, targetes chemotherapy that disrupts the functioning of DNA ligase can impede adjuvant cancer forms. Furthermore, it has been shown that DNA ligase can be broadly divided into two categories namely, ATP dependent and NAD+ dependent. Previous research has shown that although NAD-dependent DNA ligases have been discovered in sporadic cellular or viral niches outside the bacterial domain of life, there is no instance in which a NAD-dependent ligase is present in a eukaryal organism. The narrow phylogenetic distribution, unique substrate specificity, and distinctive domain structure of NAD+ depandant compared with ATP-dependent human DNA ligases recommend the NAD ligases as targets for the development of new antibacterial drugs.\n\n\n", "id": "8697", "title": "DNA ligase"}
{"url": "https://en.wikipedia.org/wiki?curid=4144434", "text": "P-bodies\n\nProcessing bodies (P-bodies) are distinct foci within the cytoplasm of the eukaryotic cell consisting of many enzymes involved in mRNA turnover. P-bodies have been observed in somatic cells originating from vertebrates and invertebrates, plants and yeast. To date, P-bodies have been demonstrated to play fundamental roles in general mRNA decay, nonsense-mediated mRNA decay, adenylate-uridylate-rich element mediated mRNA decay, and microRNA induced mRNA silencing. Not all mRNAs which enter P-bodies are degraded, as it has been demonstrated that some mRNAs can exit P-bodies and re-initiate translation. Purification and sequencing of the mRNA from purified processing bodies showed that these mRNAs are largely translationally repressed upstream of translation initiation and are protected from 5' mRNA decay.\n\nThe following activities have been demonstrated to occur in or to be associated with P-bodies:\n\nIn neurons, P-bodies move by motor proteins in response to stimulation. This is likely tied to local translation in dendrites.\n\nP-bodies were first described in the scientific literature by Bashkirov \"et al.\" in 1997, in which they describe \"small granules… discrete, prominent foci\" as the cytoplasmic location of the mouse exoribonuclease mXrn1p. It wasn’t until 2002 that a glimpse into the nature and importance of these cytoplasmic foci was published. In 2002, researchers demonstrated that multiple proteins involved with mRNA degradation localize to the foci. During this time, many descriptive names were used to identify the processing bodies, including \"GW-bodies\" and \"decapping-bodies\"; however \"P-bodies\" was the term chosen and is now widely used and accepted in the scientific literature. Recently evidence has been presented suggesting that GW-bodies and P-bodies may in fact be different cellular components. The evidence being that GW182 and Ago2, both associated with miRNA gene silencing, are found exclusively in multivesicular bodies or GW-bodies and are not localized to P-bodies. Also of note, P-bodies are not equivalent to stress granules and they contain largely non-overlapping proteins. The two structures support overlapping cellular functions but generally occur under different stimuli. Hoyle et al. suggests a novel site termed EGP bodies, or stress granules, may be responsible for mRNA storage as these sites lack the decapping enzyme.\n\nmicroRNA mediated repression occurs in two ways, either by translational repression or stimulating mRNA decay. miRNA recruit the RISC complex to the mRNA to which they are bound. The link to P-bodies comes by the fact that many, if not most, of the proteins necessary for miRNA gene silencing are localized to P-bodies, as reviewed by Kulkarni \"et al.\" (2010). These proteins include, but are not limited to, the scaffold protein GW182, Argonaute (Ago), decapping enzymes and RNA helicases.\nThe current evidence points toward P-bodies as being scaffolding centers of miRNA function, especially due to the evidence that a knock down of GW182 disrupts P-body formation. However, there remain many unanswered questions about P-bodies and their relationship to miRNA activity. Specifically, it is unknown whether there is a context dependent (stress state versus normal) specificity to the P-body's mechanism of action. Based on the evidence that P-bodies sometimes are the site of mRNA decay and sometimes the mRNA can exit the P-bodies and re-initiate translation, the question remains of what controls this switch. Another ambiguous point to be addressed is whether the proteins that localize to P-bodies are actively functioning in the miRNA gene silencing process or whether they are merely on standby.\n\nIn 2017 a new method to purify processing bodies was published. Hubstenberger et al. used fluorescence-activated particle sorting (a method based on the ideas of fluorescence-activated cell sorting) to purify processing bodies from human epithelial cells. From these purified processing bodies they were able to use mass spectrometry and RNA sequencing to determine which proteins and RNAs are found in processing bodies, respectively. This study identified 125 proteins that are significantly associated with processing bodies.\n", "id": "4144434", "title": "P-bodies"}
{"url": "https://en.wikipedia.org/wiki?curid=23634", "text": "Protein\n\nProteins ( or ) are large biomolecules, or macromolecules, consisting of one or more long chains of amino acid residues. Proteins perform a vast array of functions within organisms, including catalysing metabolic reactions, DNA replication, responding to stimuli, and transporting molecules from one location to another. Proteins differ from one another primarily in their sequence of amino acids, which is dictated by the nucleotide sequence of their genes, and which usually results in protein folding into a specific three-dimensional structure that determines its activity.\n\nA linear chain of amino acid residues is called a polypeptide. A protein contains at least one long polypeptide. Short polypeptides, containing less than 20–30 residues, are rarely considered to be proteins and are commonly called peptides, or sometimes oligopeptides. The individual amino acid residues are bonded together by peptide bonds and adjacent amino acid residues. The sequence of amino acid residues in a protein is defined by the sequence of a gene, which is encoded in the genetic code. In general, the genetic code specifies 20 standard amino acids; however, in certain organisms the genetic code can include selenocysteine and—in certain archaea—pyrrolysine. Shortly after or even during synthesis, the residues in a protein are often chemically modified by post-translational modification, which alters the physical and chemical properties, folding, stability, activity, and ultimately, the function of the proteins. Sometimes proteins have non-peptide groups attached, which can be called prosthetic groups or cofactors. Proteins can also work together to achieve a particular function, and they often associate to form stable protein complexes.\n\nOnce formed, proteins only exist for a certain period of time and are then degraded and recycled by the cell's machinery through the process of protein turnover. A protein's lifespan is measured in terms of its half-life and covers a wide range. They can exist for minutes or years with an average lifespan of 1–2 days in mammalian cells. Abnormal or misfolded proteins are degraded more rapidly either due to being targeted for destruction or due to being unstable.\n\nLike other biological macromolecules such as polysaccharides and nucleic acids, proteins are essential parts of organisms and participate in virtually every process within cells. Many proteins are enzymes that catalyse biochemical reactions and are vital to metabolism. Proteins also have structural or mechanical functions, such as actin and myosin in muscle and the proteins in the cytoskeleton, which form a system of scaffolding that maintains cell shape. Other proteins are important in cell signaling, immune responses, cell adhesion, and the cell cycle. In animals, proteins are needed in the diet to provide the essential amino acids that cannot be synthesized. Digestion breaks the proteins down for use in the metabolism.\n\nProteins may be purified from other cellular components using a variety of techniques such as ultracentrifugation, precipitation, electrophoresis, and chromatography; the advent of genetic engineering has made possible a number of methods to facilitate purification. Methods commonly used to study protein structure and function include immunohistochemistry, site-directed mutagenesis, X-ray crystallography, nuclear magnetic resonance and mass spectrometry.\n\nMost proteins consist of linear polymers built from series of up to 20 different -α-amino acids. All proteinogenic amino acids possess common structural features, including an α-carbon to which an amino group, a carboxyl group, and a variable side chain are bonded. Only proline differs from this basic structure as it contains an unusual ring to the N-end amine group, which forces the CO–NH amide moiety into a fixed conformation. The side chains of the standard amino acids, detailed in the list of standard amino acids, have a great variety of chemical structures and properties; it is the combined effect of all of the amino acid side chains in a protein that ultimately determines its three-dimensional structure and its chemical reactivity.\nThe amino acids in a polypeptide chain are linked by peptide bonds. Once linked in the protein chain, an individual amino acid is called a \"residue,\" and the linked series of carbon, nitrogen, and oxygen atoms are known as the \"main chain\" or \"protein backbone.\"\n\nThe peptide bond has two resonance forms that contribute some double-bond character and inhibit rotation around its axis, so that the alpha carbons are roughly coplanar. The other two dihedral angles in the peptide bond determine the local shape assumed by the protein backbone. The end with a free amino group is known as the N-terminus or amino terminus, whereas the end of the protein with a free carboxyl group is known as the C-terminus or carboxy terminus (the sequence of the protein is written from N-terminus to C-terminus, from left to right).\n\nThe words \"protein\", \"polypeptide,\" and \"peptide\" are a little ambiguous and can overlap in meaning. \"Protein\" is generally used to refer to the complete biological molecule in a stable conformation, whereas \"peptide\" is generally reserved for a short amino acid oligomers often lacking a stable three-dimensional structure. However, the boundary between the two is not well defined and usually lies near 20–30 residues. \"Polypeptide\" can refer to any single linear chain of amino acids, usually regardless of length, but often implies an absence of a defined conformation.\n\nIt has been estimated that average-sized bacteria contain about 2 million proteins per cell (e.g. \"E. coli\" and \"Staphylococcus aureus\"). Smaller bacteria, such as \"Mycoplasma\" or \"spirochetes\" contain fewer molecules, on the order of 50,000 to 1 million. By contrast, eukaryotic cells are larger and thus contain much more protein. For instance, yeast cells have been estimated to contain about 50 million proteins and human cells on the order of 1 to 3 billion. The concentration of individual protein copies ranges from a few molecules per cell up to 20 million. Not all genes coding proteins are expressed in most cells and their number depends on, for example, cell type and external stimuli. For instance, of the 20,000 or so proteins encoded by the human genome, only 6,000 are detected in lymphoblastoid cells. Moreover, the number of proteins the genome encodes correlates well with the organism complexity. Eukaryotes, bacteria, archaea and viruses have on average 15145, 3200, 2358 and 42 proteins respectively coded in their genomes.\n\nProteins are assembled from amino acids using information encoded in genes. Each protein has its own unique amino acid sequence that is specified by the nucleotide sequence of the gene encoding this protein. The genetic code is a set of three-nucleotide sets called codons and each three-nucleotide combination designates an amino acid, for example AUG (adenine-uracil-guanine) is the code for methionine. Because DNA contains four nucleotides, the total number of possible codons is 64; hence, there is some redundancy in the genetic code, with some amino acids specified by more than one codon. Genes encoded in DNA are first transcribed into pre-messenger RNA (mRNA) by proteins such as RNA polymerase. Most organisms then process the pre-mRNA (also known as a \"primary transcript\") using various forms of Post-transcriptional modification to form the mature mRNA, which is then used as a template for protein synthesis by the ribosome. In prokaryotes the mRNA may either be used as soon as it is produced, or be bound by a ribosome after having moved away from the nucleoid. In contrast, eukaryotes make mRNA in the cell nucleus and then translocate it across the nuclear membrane into the cytoplasm, where protein synthesis then takes place. The rate of protein synthesis is higher in prokaryotes than eukaryotes and can reach up to 20 amino acids per second.\n\nThe process of synthesizing a protein from an mRNA template is known as translation. The mRNA is loaded onto the ribosome and is read three nucleotides at a time by matching each codon to its base pairing anticodon located on a transfer RNA molecule, which carries the amino acid corresponding to the codon it recognizes. The enzyme aminoacyl tRNA synthetase \"charges\" the tRNA molecules with the correct amino acids. The growing polypeptide is often termed the \"nascent chain\". Proteins are always biosynthesized from N-terminus to C-terminus.\n\nThe size of a synthesized protein can be measured by the number of amino acids it contains and by its total molecular mass, which is normally reported in units of \"daltons\" (synonymous with atomic mass units), or the derivative unit kilodalton (kDa). The average size of a protein increases from Archaea to Bacteria to Eukaryote (283, 311, 438 residues and 31, 34, 49 kDa respecitvely) due to a bigger number of protein domains constituting proteins in higher organisms. For instance, yeast proteins are on average 466 amino acids long and 53 kDa in mass. The largest known proteins are the titins, a component of the muscle sarcomere, with a molecular mass of almost 3,000 kDa and a total length of almost 27,000 amino acids.\n\nShort proteins can also be synthesized chemically by a family of methods known as peptide synthesis, which rely on organic synthesis techniques such as chemical ligation to produce peptides in high yield. Chemical synthesis allows for the introduction of non-natural amino acids into polypeptide chains, such as attachment of fluorescent probes to amino acid side chains. These methods are useful in laboratory biochemistry and cell biology, though generally not for commercial applications. Chemical synthesis is inefficient for polypeptides longer than about 300 amino acids, and the synthesized proteins may not readily assume their native tertiary structure. Most chemical synthesis methods proceed from C-terminus to N-terminus, opposite the biological reaction.\n\nMost proteins fold into unique 3-dimensional structures. The shape into which a protein naturally folds is known as its native conformation. Although many proteins can fold unassisted, simply through the chemical properties of their amino acids, others require the aid of molecular chaperones to fold into their native states. Biochemists often refer to four distinct aspects of a protein's structure:\n\nProteins are not entirely rigid molecules. In addition to these levels of structure, proteins may shift between several related structures while they perform their functions. In the context of these functional rearrangements, these tertiary or quaternary structures are usually referred to as \"conformations\", and transitions between them are called \"conformational changes.\" Such changes are often induced by the binding of a substrate molecule to an enzyme's active site, or the physical region of the protein that participates in chemical catalysis. In solution proteins also undergo variation in structure through thermal vibration and the collision with other molecules.\n\nProteins can be informally divided into three main classes, which correlate with typical tertiary structures: globular proteins, fibrous proteins, and membrane proteins. Almost all globular proteins are soluble and many are enzymes. Fibrous proteins are often structural, such as collagen, the major component of connective tissue, or keratin, the protein component of hair and nails. Membrane proteins often serve as receptors or provide channels for polar or charged molecules to pass through the cell membrane.\n\nA special case of intramolecular hydrogen bonds within proteins, poorly shielded from water attack and hence promoting their own dehydration, are called dehydrons.\n\nMany proteins are composed of several protein domains, i.e. segments of a protein that fold into distinct structural units. Domains usually also have specific functions, such as enzymatic activities (e.g. kinase) or they serve as binding modules (e.g. the SH3 domain binds to proline-rich sequences in other proteins).\n\nShort amino acid sequences within proteins often act as recognition sites for other proteins. For instance, SH3 domains typically bind to short PxxP motifs (i.e. 2 prolines [P], separated by 2 unspecified amino acids [x], although the surrounding amino acids may determine the exact binding specificity). A large number of such motifs has been collected in the Eukaryotic Linear Motif (ELM) database.\n\nProteins are the chief actors within the cell, said to be carrying out the duties specified by the information encoded in genes. With the exception of certain types of RNA, most other biological molecules are relatively inert elements upon which proteins act. Proteins make up half the dry weight of an \"Escherichia coli\" cell, whereas other macromolecules such as DNA and RNA make up only 3% and 20%, respectively. The set of proteins expressed in a particular cell or cell type is known as its proteome.\n\nThe chief characteristic of proteins that also allows their diverse set of functions is their ability to bind other molecules specifically and tightly. The region of the protein responsible for binding another molecule is known as the binding site and is often a depression or \"pocket\" on the molecular surface. This binding ability is mediated by the tertiary structure of the protein, which defines the binding site pocket, and by the chemical properties of the surrounding amino acids' side chains. Protein binding can be extraordinarily tight and specific; for example, the ribonuclease inhibitor protein binds to human angiogenin with a sub-femtomolar dissociation constant (<10 M) but does not bind at all to its amphibian homolog onconase (>1 M). Extremely minor chemical changes such as the addition of a single methyl group to a binding partner can sometimes suffice to nearly eliminate binding; for example, the aminoacyl tRNA synthetase specific to the amino acid valine discriminates against the very similar side chain of the amino acid isoleucine.\n\nProteins can bind to other proteins as well as to small-molecule substrates. When proteins bind specifically to other copies of the same molecule, they can oligomerize to form fibrils; this process occurs often in structural proteins that consist of globular monomers that self-associate to form rigid fibers. Protein–protein interactions also regulate enzymatic activity, control progression through the cell cycle, and allow the assembly of large protein complexes that carry out many closely related reactions with a common biological function. Proteins can also bind to, or even be integrated into, cell membranes. The ability of binding partners to induce conformational changes in proteins allows the construction of enormously complex signaling networks.\nAs interactions between proteins are reversible, and depend heavily on the availability of different groups of partner proteins to form aggregates that are capable to carry out discrete sets of function, study of the interactions between specific proteins is a key to understand important aspects of cellular function, and ultimately the properties that distinguish particular cell types.\n\nThe best-known role of proteins in the cell is as enzymes, which catalyse chemical reactions. Enzymes are usually highly specific and accelerate only one or a few chemical reactions. Enzymes carry out most of the reactions involved in metabolism, as well as manipulating DNA in processes such as DNA replication, DNA repair, and transcription. Some enzymes act on other proteins to add or remove chemical groups in a process known as posttranslational modification. About 4,000 reactions are known to be catalysed by enzymes. The rate acceleration conferred by enzymatic catalysis is often enormous—as much as 10-fold increase in rate over the uncatalysed reaction in the case of orotate decarboxylase (78 million years without the enzyme, 18 milliseconds with the enzyme).\n\nThe molecules bound and acted upon by enzymes are called substrates. Although enzymes can consist of hundreds of amino acids, it is usually only a small fraction of the residues that come in contact with the substrate, and an even smaller fraction—three to four residues on average—that are directly involved in catalysis. The region of the enzyme that binds the substrate and contains the catalytic residues is known as the active site.\n\nDirigent proteins are members of a class of proteins that dictate the stereochemistry of a compound synthesized by other enzymes.\n\nMany proteins are involved in the process of cell signaling and signal transduction. Some proteins, such as insulin, are extracellular proteins that transmit a signal from the cell in which they were synthesized to other cells in distant tissues. Others are membrane proteins that act as receptors whose main function is to bind a signaling molecule and induce a biochemical response in the cell. Many receptors have a binding site exposed on the cell surface and an effector domain within the cell, which may have enzymatic activity or may undergo a conformational change detected by other proteins within the cell.\n\nAntibodies are protein components of an adaptive immune system whose main function is to bind antigens, or foreign substances in the body, and target them for destruction. Antibodies can be secreted into the extracellular environment or anchored in the membranes of specialized B cells known as plasma cells. Whereas enzymes are limited in their binding affinity for their substrates by the necessity of conducting their reaction, antibodies have no such constraints. An antibody's binding affinity to its target is extraordinarily high.\n\nMany ligand transport proteins bind particular small biomolecules and transport them to other locations in the body of a multicellular organism. These proteins must have a high binding affinity when their ligand is present in high concentrations, but must also release the ligand when it is present at low concentrations in the target tissues. The canonical example of a ligand-binding protein is haemoglobin, which transports oxygen from the lungs to other organs and tissues in all vertebrates and has close homologs in every biological kingdom. Lectins are sugar-binding proteins which are highly specific for their sugar moieties. Lectins typically play a role in biological recognition phenomena involving cells and proteins. Receptors and hormones are highly specific binding proteins.\n\nTransmembrane proteins can also serve as ligand transport proteins that alter the permeability of the cell membrane to small molecules and ions. The membrane alone has a hydrophobic core through which polar or charged molecules cannot diffuse. Membrane proteins contain internal channels that allow such molecules to enter and exit the cell. Many ion channel proteins are specialized to select for only a particular ion; for example, potassium and sodium channels often discriminate for only one of the two ions.\n\nStructural proteins confer stiffness and rigidity to otherwise-fluid biological components. Most structural proteins are fibrous proteins; for example, collagen and elastin are critical components of connective tissue such as cartilage, and keratin is found in hard or filamentous structures such as hair, nails, feathers, hooves, and some animal shells. Some globular proteins can also play structural functions, for example, actin and tubulin are globular and soluble as monomers, but polymerize to form long, stiff fibers that make up the cytoskeleton, which allows the cell to maintain its shape and size.\n\nOther proteins that serve structural functions are motor proteins such as myosin, kinesin, and dynein, which are capable of generating mechanical forces. These proteins are crucial for cellular motility of single celled organisms and the sperm of many multicellular organisms which reproduce sexually. They also generate the forces exerted by contracting muscles and play essential roles in intracellular transport.\n\nThe activities and structures of proteins may be examined \"in vitro,\" \"in vivo, and in silico\". In vitro studies of purified proteins in controlled environments are useful for learning how a protein carries out its function: for example, enzyme kinetics studies explore the chemical mechanism of an enzyme's catalytic activity and its relative affinity for various possible substrate molecules. By contrast, in vivo experiments can provide information about the physiological role of a protein in the context of a cell or even a whole organism. In silico studies use computational methods to study proteins.\n\nTo perform \"in vitro\" analysis, a protein must be purified away from other cellular components. This process usually begins with cell lysis, in which a cell's membrane is disrupted and its internal contents released into a solution known as a crude lysate. The resulting mixture can be purified using ultracentrifugation, which fractionates the various cellular components into fractions containing soluble proteins; membrane lipids and proteins; cellular organelles, and nucleic acids. Precipitation by a method known as salting out can concentrate the proteins from this lysate. Various types of chromatography are then used to isolate the protein or proteins of interest based on properties such as molecular weight, net charge and binding affinity. The level of purification can be monitored using various types of gel electrophoresis if the desired protein's molecular weight and isoelectric point are known, by spectroscopy if the protein has distinguishable spectroscopic features, or by enzyme assays if the protein has enzymatic activity. Additionally, proteins can be isolated according their charge using electrofocusing.\n\nFor natural proteins, a series of purification steps may be necessary to obtain protein sufficiently pure for laboratory applications. To simplify this process, genetic engineering is often used to add chemical features to proteins that make them easier to purify without affecting their structure or activity. Here, a \"tag\" consisting of a specific amino acid sequence, often a series of histidine residues (a \"His-tag\"), is attached to one terminus of the protein. As a result, when the lysate is passed over a chromatography column containing nickel, the histidine residues ligate the nickel and attach to the column while the untagged components of the lysate pass unimpeded. A number of different tags have been developed to help researchers purify specific proteins from complex mixtures.\n\nThe study of proteins \"in vivo\" is often concerned with the synthesis and localization of the protein within the cell. Although many intracellular proteins are synthesized in the cytoplasm and membrane-bound or secreted proteins in the endoplasmic reticulum, the specifics of how proteins are targeted to specific organelles or cellular structures is often unclear. A useful technique for assessing cellular localization uses genetic engineering to express in a cell a fusion protein or chimera consisting of the natural protein of interest linked to a \"reporter\" such as green fluorescent protein (GFP). The fused protein's position within the cell can be cleanly and efficiently visualized using microscopy, as shown in the figure opposite.\n\nOther methods for elucidating the cellular location of proteins requires the use of known compartmental markers for regions such as the ER, the Golgi, lysosomes or vacuoles, mitochondria, chloroplasts, plasma membrane, etc. With the use of fluorescently tagged versions of these markers or of antibodies to known markers, it becomes much simpler to identify the localization of a protein of interest. For example, indirect immunofluorescence will allow for fluorescence colocalization and demonstration of location. Fluorescent dyes are used to label cellular compartments for a similar purpose.\n\nOther possibilities exist, as well. For example, immunohistochemistry usually utilizes an antibody to one or more proteins of interest that are conjugated to enzymes yielding either luminescent or chromogenic signals that can be compared between samples, allowing for localization information. Another applicable technique is cofractionation in sucrose (or other material) gradients using isopycnic centrifugation. While this technique does not prove colocalization of a compartment of known density and the protein of interest, it does increase the likelihood, and is more amenable to large-scale studies.\n\nFinally, the gold-standard method of cellular localization is immunoelectron microscopy. This technique also uses an antibody to the protein of interest, along with classical electron microscopy techniques. The sample is prepared for normal electron microscopic examination, and then treated with an antibody to the protein of interest that is conjugated to an extremely electro-dense material, usually gold. This allows for the localization of both ultrastructural details as well as the protein of interest.\n\nThrough another genetic engineering application known as site-directed mutagenesis, researchers can alter the protein sequence and hence its structure, cellular localization, and susceptibility to regulation. This technique even allows the incorporation of unnatural amino acids into proteins, using modified tRNAs, and may allow the rational design of new proteins with novel properties.\n\nThe total complement of proteins present at a time in a cell or cell type is known as its proteome, and the study of such large-scale data sets defines the field of proteomics, named by analogy to the related field of genomics. Key experimental techniques in proteomics include 2D electrophoresis, which allows the separation of a large number of proteins, mass spectrometry, which allows rapid high-throughput identification of proteins and sequencing of peptides (most often after in-gel digestion), protein microarrays, which allow the detection of the relative levels of a large number of proteins present in a cell, and two-hybrid screening, which allows the systematic exploration of protein–protein interactions. The total complement of biologically possible such interactions is known as the interactome. A systematic attempt to determine the structures of proteins representing every possible fold is known as structural genomics.\n\nA vast array of computational methods have been developed to analyze the structure, function, and evolution of proteins.\n\nThe development of such tools has been driven by the large amount of genomic and proteomic data available for a variety of organisms, including the human genome. It is simply impossible to study all proteins experimentally, hence only a few are subjected to laboratory experiments while computational tools are used to extrapolate to similar proteins. Such homologous proteins can be efficiently identified in distantly related organisms by sequence alignment. Genome and gene sequences can be searched by a variety of tools for certain properties. Sequence profiling tools can find restriction enzyme sites, open reading frames in nucleotide sequences, and predict secondary structures. Phylogenetic trees can be constructed and evolutionary hypotheses developed using special software like ClustalW regarding the ancestry of modern organisms and the genes they express. The field of bioinformatics is now indispensable for the analysis of genes and proteins.\n\nDiscovering the tertiary structure of a protein, or the quaternary structure of its complexes, can provide important clues about how the protein performs its function. Common experimental methods of structure determination include X-ray crystallography and NMR spectroscopy, both of which can produce information at atomic resolution. However, NMR experiments are able to provide information from which a subset of distances between pairs of atoms can be estimated, and the final possible conformations for a protein are determined by solving a distance geometry problem. Dual polarisation interferometry is a quantitative analytical method for measuring the overall protein conformation and conformational changes due to interactions or other stimulus. Circular dichroism is another laboratory technique for determining internal β-sheet / α-helical composition of proteins. Cryoelectron microscopy is used to produce lower-resolution structural information about very large protein complexes, including assembled viruses; a variant known as electron crystallography can also produce high-resolution information in some cases, especially for two-dimensional crystals of membrane proteins. Solved structures are usually deposited in the Protein Data Bank (PDB), a freely available resource from which structural data about thousands of proteins can be obtained in the form of Cartesian coordinates for each atom in the protein.\n\nMany more gene sequences are known than protein structures. Further, the set of solved structures is biased toward proteins that can be easily subjected to the conditions required in X-ray crystallography, one of the major structure determination methods. In particular, globular proteins are comparatively easy to crystallize in preparation for X-ray crystallography. Membrane proteins, by contrast, are difficult to crystallize and are underrepresented in the PDB. Structural genomics initiatives have attempted to remedy these deficiencies by systematically solving representative structures of major fold classes. Protein structure prediction methods attempt to provide a means of generating a plausible structure for proteins whose structures have not been experimentally determined.\n\nComplementary to the field of structural genomics, \"protein structure prediction\" develops efficient mathematical models of proteins to computationally predict their structures in theory, instead of detecting structures with laboratory observation. The most successful type of structure prediction, known as homology modeling, relies on the existence of a \"template\" structure with sequence similarity to the protein being modeled; structural genomics' goal is to provide sufficient representation in solved structures to model most of those that remain. Although producing accurate models remains a challenge when only distantly related template structures are available, it has been suggested that sequence alignment is the bottleneck in this process, as quite accurate models can be produced if a \"perfect\" sequence alignment is known. Many structure prediction methods have served to inform the emerging field of protein engineering, in which novel protein folds have already been designed. A more complex computational problem is the prediction of intermolecular interactions, such as in molecular docking and protein–protein interaction prediction.\n\nMathematical models to simulate dynamic processes of protein folding and binding involve molecular mechanics, in particular, molecular dynamics. Monte Carlo techniques facilitate the computations, which exploit advances in parallel and distributed computing (for example, the Folding@home project which performs molecular modeling on GPUs). \"In silico\" simulations discovered the folding of small α-helical protein domains such as the villin headpiece and the HIV accessory protein. Hybrid methods combining standard molecular dynamics with quantum mechanical mathematics explored the electronic states of rhodopsins.\n\nMany proteins (in Eucaryota ~33%) contain large unstructured but biologically functional segments and can be classified as intrinsically disordered proteins. Predicting and analysing protein disorder is, therefore, an important part of protein structure characterisation.\n\nMost microorganisms and plants can biosynthesize all 20 standard amino acids, while animals (including humans) must obtain some of the amino acids from the diet. The amino acids that an organism cannot synthesize on its own are referred to as essential amino acids. Key enzymes that synthesize certain amino acids are not present in animals — such as aspartokinase, which catalyses the first step in the synthesis of lysine, methionine, and threonine from aspartate. If amino acids are present in the environment, microorganisms can conserve energy by taking up the amino acids from their surroundings and downregulating their biosynthetic pathways.\n\nIn animals, amino acids are obtained through the consumption of foods containing protein. Ingested proteins are then broken down into amino acids through digestion, which typically involves denaturation of the protein through exposure to acid and hydrolysis by enzymes called proteases. Some ingested amino acids are used for protein biosynthesis, while others are converted to glucose through gluconeogenesis, or fed into the citric acid cycle. This use of protein as a fuel is particularly important under starvation conditions as it allows the body's own proteins to be used to support life, particularly those found in muscle. \n\nIn animals such as dogs and cats, protein maintains the health and quality of the skin by promoting hair follicle growth and keratinization, and thus reducing the likelihood of skin problems producing malodours. Poor-quality proteins also have a role regarding gastrointestinal health, increasing the potential for flatulence and odorous compounds in dogs because when proteins reach the colon in an undigested state, they are fermented producing hydrogen sulfide gas, indole, and skatole. Dogs and cats digest animal proteins better than those from plants but products of low-quality animal origin are poorly digested, including skin, feathers, and connective tissue.\n\nProteins were recognized as a distinct class of biological molecules in the eighteenth century by Antoine Fourcroy and others, distinguished by the molecules' ability to coagulate or flocculate under treatments with heat or acid. Noted examples at the time included albumin from egg whites, blood serum albumin, fibrin, and wheat gluten.\n\nProteins were first described by the Dutch chemist Gerardus Johannes Mulder and named by the Swedish chemist Jöns Jacob Berzelius in 1838. Mulder carried out elemental analysis of common proteins and found that nearly all proteins had the same empirical formula, CHNOPS. He came to the erroneous conclusion that they might be composed of a single type of (very large) molecule. The term \"protein\" to describe these molecules was proposed by Mulder's associate Berzelius; protein is derived from the Greek word πρώτειος (\"proteios\"), meaning \"primary\", \"in the lead\", or \"standing in front\", + \"-in\". Mulder went on to identify the products of protein degradation such as the amino acid leucine for which he found a (nearly correct) molecular weight of 131 Da. Prior to \"protein\", other names were used, like \"albumins\" or \"albuminous materials\" (\"Eiweisskörper\", in German).\n\nEarly nutritional scientists such as the German Carl von Voit believed that protein was the most important nutrient for maintaining the structure of the body, because it was generally believed that \"flesh makes flesh.\" Karl Heinrich Ritthausen extended known protein forms with the identification of glutamic acid. At the Connecticut Agricultural Experiment Station a detailed review of the vegetable proteins was compiled by Thomas Burr Osborne. Working with Lafayette Mendel and applying Liebig's law of the minimum in feeding laboratory rats, the nutritionally essential amino acids were established. The work was continued and communicated by William Cumming Rose. The understanding of proteins as polypeptides came through the work of Franz Hofmeister and Hermann Emil Fischer in 1902. The central role of proteins as enzymes in living organisms was not fully appreciated until 1926, when James B. Sumner showed that the enzyme urease was in fact a protein.\n\nThe difficulty in purifying proteins in large quantities made them very difficult for early protein biochemists to study. Hence, early studies focused on proteins that could be purified in large quantities, e.g., those of blood, egg white, various toxins, and digestive/metabolic enzymes obtained from slaughterhouses. In the 1950s, the Armour Hot Dog Co. purified 1 kg of pure bovine pancreatic ribonuclease A and made it freely available to scientists; this gesture helped ribonuclease A become a major target for biochemical study for the following decades.\n\nLinus Pauling is credited with the successful prediction of regular protein secondary structures based on hydrogen bonding, an idea first put forth by William Astbury in 1933. Later work by Walter Kauzmann on denaturation, based partly on previous studies by Kaj Linderstrøm-Lang, contributed an understanding of protein folding and structure mediated by hydrophobic interactions.\n\nThe first protein to be sequenced was insulin, by Frederick Sanger, in 1949. Sanger correctly determined the amino acid sequence of insulin, thus conclusively demonstrating that proteins consisted of linear polymers of amino acids rather than branched chains, colloids, or cyclols. He won the Nobel Prize for this achievement in 1958.\n\nThe first protein structures to be solved were hemoglobin and myoglobin, by Max Perutz and Sir John Cowdery Kendrew, respectively, in 1958. , the Protein Data Bank has over 126,060 atomic-resolution structures of proteins. In more recent times, cryo-electron microscopy of large macromolecular assemblies and computational protein structure prediction of small protein domains are two methods approaching atomic resolution.\n\n\n\n", "id": "23634", "title": "Protein"}
{"url": "https://en.wikipedia.org/wiki?curid=3487107", "text": "Real-time polymerase chain reaction\n\nA real-time polymerase chain reaction (Real-Time PCR), also known as quantitative polymerase chain reaction (qPCR), is a laboratory technique of molecular biology based on the polymerase chain reaction (PCR). It monitors the amplification of a targeted DNA molecule during the PCR, i.e. in real-time, and not at its end, as in conventional PCR. Real-time PCR can be used quantitatively (quantitative real-time PCR), and semi-quantitatively, i.e. above/below a certain amount of DNA molecules (semi quantitative real-time PCR).\n\nTwo common methods for the detection of PCR products in real-time PCR are: (1) non-specific fluorescent dyes that intercalate with any double-stranded DNA, and (2) sequence-specific DNA probes consisting of oligonucleotides that are labelled with a fluorescent reporter which permits detection only after hybridization of the probe with its complementary sequence.\n\nThe Minimum Information for Publication of Quantitative Real-Time PCR Experiments (MIQE) guidelines propose that the abbreviation \"qPCR\" be used for quantitative real-time PCR and that \"RT-qPCR\" be used for reverse transcription–qPCR. The acronym \"RT-PCR\" commonly denotes reverse transcription polymerase chain reaction and not real-time PCR, but not all authors adhere to this convention.\n\nCells in all organisms regulate gene expression by turnover of gene transcripts (single stranded RNA): The amount of an expressed gene in a cell can be measured by the number of copies of an RNA transcript of that gene present in a sample. In order to robustly detect and quantify gene expression from small amounts of RNA, amplification of the gene transcript is necessary. The polymerase chain reaction (PCR) is a common method for amplifying DNA; for RNA-based PCR the RNA sample is first reverse-transcribed to complementary DNA (cDNA) with reverse transcriptase.\n\nIn order to amplify small amounts of DNA, the same methodology is used as in conventional PCR using a DNA template, at least one pair of specific primers, deoxyribonucleotides, a suitable buffer solution and a thermo-stable DNA polymerase. A substance marked with a fluorophore is added to this mixture in a thermal cycler that contains sensors for measuring the fluorescence of the fluorophore after it has been excited at the required wavelength allowing the generation rate to be measured for one or more specific products. \nThis allows the rate of generation of the amplified product to be measured at each PCR cycle. The data thus generated can be analysed by computer software to calculate \"relative gene expression\" (or \"mRNA copy number\") in several samples. Quantitative PCR can also be applied to the detection and quantification of DNA in samples to determine the presence and abundance of a particular DNA sequence in these samples. This measurement is made after each amplification cycle, and this is the reason why this method is called real time PCR (that is, immediate or simultaneous PCR). In the case of RNA quantitation, the template is complementary DNA (cDNA), which is obtained by reverse transcription of ribonucleic acid (RNA). In this instance the technique used is quantitative RT-PCR or Q-RT-PCR.\n\nQuantitative PCR and DNA microarray are modern methodologies for studying gene expression. Older methods were used to measure mRNA abundance: Differential display, RNase protection assay and Northern blot. Northern blotting is often used to estimate the expression level of a gene by visualizing the abundance of its mRNA transcript in a sample. In this method, purified RNA is separated by agarose gel electrophoresis, transferred to a solid matrix (such as a nylon membrane), and probed with a specific DNA or RNA probe that is complementary to the gene of interest. Although this technique is still used to assess gene expression, it requires relatively large amounts of RNA and provides only qualitative or semi quantitative information of mRNA levels. Estimation errors arising from variations in the quantification method can be the result of DNA integrity, enzyme efficiency and many other factors. For this reason a number of standardization systems (often called normalization methods) have been developed. Some have been developed for quantifying total gene expression, but the most common are aimed at quantifying the specific gene being studied in relation to another gene called a normalizing gene, which is selected for its almost constant level of expression. These genes are often selected from housekeeping genes as their functions related to basic cellular survival normally imply constitutive gene expression. This enables researchers to report a ratio for the expression of the genes of interest divided by the expression of the selected normalizer, thereby allowing comparison of the former without actually knowing its absolute level of expression.\n\nThe most commonly used normalizing genes are those that code for the following molecules: tubulin, glyceraldehyde-3-phosphate dehydrogenase, albumin, cyclophilin, and ribosomal RNAs.\n\nReal-time PCR is carried out in a thermal cycler with the capacity to illuminate each sample with a beam of light of at least one specified wavelength and detect the fluorescence emitted by the excited fluorophore. The thermal cycler is also able to rapidly heat and chill samples, thereby taking advantage of the physicochemical properties of the nucleic acids and DNA polymerase.\n\nThe PCR process generally consists of a series of temperature changes that are repeated 25 – 50 times. These cycles normally consist of three stages: the first, at around 95 °C, allows the separation of the nucleic acid's double chain; the second, at a temperature of around 50-60 °C, allows the binding of the primers with the DNA template; the third, at between 68 - 72 °C, facilitates the polymerization carried out by the DNA polymerase. Due to the small size of the fragments the last step is usually omitted in this type of PCR as the enzyme is able to increase their number during the change between the alignment stage and the denaturing stage. In addition, in four step PCR the fluorescence is measured during short temperature phase lasting only a few seconds in each cycle, with a temperature of, for example, 80 °C, in order to reduce the signal caused by the presence of primer dimers when a non-specific dye is used. The temperatures and the timings used for each cycle depend on a wide variety of parameters, such as: the enzyme used to synthesize the DNA, the concentration of divalent ions and deoxyribonucleotides (dNTPs) in the reaction and the bonding temperature of the primers.\n\nReal-time PCR technique can be classified by the chemistry used to detect the PCR product, specific or non-specific fluorochromes.\n\nA DNA-binding dye binds to all double-stranded (ds) DNA in PCR, causing fluorescence of the dye. An increase in DNA product during PCR therefore leads to an increase in fluorescence intensity measured at each cycle. However, dsDNA dyes such as SYBR Green will bind to all dsDNA PCR products, including nonspecific PCR products (such as Primer dimer). This can potentially interfere with, or prevent, accurate monitoring of the intended target sequence.\n\nIn real-time PCR with dsDNA dyes the reaction is prepared as usual, with the addition of fluorescent dsDNA dye. Then the reaction is run in a real-time PCR instrument, and after each cycle, the intensity of fluorescence is measured with a detector; the dye only fluoresces when bound to the dsDNA (i.e., the PCR product). \nThis method has the advantage of only needing a pair of primers to carry out the amplification, which keeps costs down; however, only one target sequence can be monitored in a tube.\n\nFluorescent reporter probes detect only the DNA containing the sequence complementary to the probe; therefore, use of the reporter probe significantly increases specificity, and enables performing the technique even in the presence of other dsDNA. Using different-coloured labels, fluorescent probes can be used in multiplex assays for monitoring several target sequences in the same tube. The specificity of fluorescent reporter probes also prevents interference of measurements caused by primer dimers, which are undesirable potential by-products in PCR. However, fluorescent reporter probes do not prevent the inhibitory effect of the primer dimers, which may depress accumulation of the desired products in the reaction.\n\nThe method relies on a DNA-based probe with a fluorescent reporter at one end and a quencher of fluorescence at the opposite end of the probe. The close proximity of the reporter to the quencher prevents detection of its fluorescence; breakdown of the probe by the 5' to 3' exonuclease activity of the Taq polymerase breaks the reporter-quencher proximity and thus allows unquenched emission of fluorescence, which can be detected after excitation with a laser. An increase in the product targeted by the reporter probe at each PCR cycle therefore causes a proportional increase in fluorescence due to the breakdown of the probe and release of the reporter.\n\n\nReal-time PCR permits the identification of specific, amplified DNA fragments using analysis of their melting temperature (also called \"T\" value, from m\"elting\" t\"emperature\"). The method used is usually PCR with double-stranded DNA-binding dyes as reporters and the dye used is usually SYBR Green. The DNA melting temperature is specific to the amplified fragment. The results of this technique are obtained by comparing the dissociation curves of the analysed DNA samples.\n\nUnlike conventional PCR, this method avoids the previous use of electrophoresis techniques to demonstrate the results of all the samples. This is because, despite being a kinetic technique, quantitative PCR is usually evaluated at a distinct end point. The technique therefore usually provides more rapid results and / or uses fewer reactants than electrophoresis. If subsequent electrophoresis is required it is only necessary to test those samples that real time PCR has shown to be doubtful and / or to ratify the results for samples that have tested positive for a specific determinant.\n\nUnlike end point PCR (conventional PCR), real time PCR allows monitoring of the desired product at any point in the amplification process by measuring fluorescence (in real time frame, measurement is made of its level over a given threshold). A commonly employed method of DNA quantification by real-time PCR relies on plotting fluorescence against the number of cycles on a logarithmic scale. A threshold for detection of DNA-based fluorescence is set 3-5 times of the standard deviation of the signal noise above background. The number of cycles at which the fluorescence exceeds the threshold is called the threshold cycle (C) or, according to the MIQE guidelines, quantification cycle (C).\n\nDuring the exponential amplification phase, the quantity of the target DNA template (amplicon) doubles every cycle. For example, a DNA sample whose C precedes that of another sample by 3 cycles contained 2 = 8 times more template. However, the efficiency of amplification is often variable among primers and templates. Therefore, the efficiency of a primer-template combination is assessed in a titration experiment with serial dilutions of DNA template to create a standard curve of the change in (C) with each dilution. The slope of the linear regression is then used to determine the efficiency of amplification, which is 100% if a dilution of 1:2 results in a (C) difference of 1. The cycle threshold method makes several assumptions of reaction mechanism and has a reliance on data from low signal-to-noise regions of the amplification profile that can introduce substantial variance during the data analysis.\n\nTo quantify gene expression, the (C) for an RNA or DNA from the gene of interest is subtracted from the (C) of RNA/DNA from a housekeeping gene in the same sample to normalize for variation in the amount and quality of RNA between different samples. This normalization procedure is commonly called the \"ΔC-method\" and permits comparison of expression of a gene of interest among different samples. However, for such comparison, expression of the normalizing reference gene needs to be very similar across all the samples. Choosing a reference gene fulfilling this criterion is therefore of high importance, and often challenging, because only very few genes show equal levels of expression across a range of different conditions or tissues. Although cycle threshold analysis is integrated with many commercial software systems, there are more accurate and reliable methods of analysing amplification profile data that should be considered in cases where reproducibility is a concern.\n\nMechanism-based qPCR quantification methods have also been suggested, and have the advantage that they do not require a standard curve for quantification. Methods such as MAK2 have been shown to have equal or better quantitative performance to standard curve methods. These mechanism-based methods use knowledge about the polymerase amplification process to generate estimates of the original sample concentration. An extension of this approach includes an accurate model of the entire PCR reaction profile, which allows for the use of high signal-to-noise data and the ability to validate data quality prior to analysis.\n\nAccording to research of Ruijter et al. MAK2 assumes constant amplification efficiency during the PCR reaction. However, theoretical analysis of polymerase chain reaction, from which MAK2 was derived, has revealed that amplification efficiency is not constant throughout PCR. While MAK2 quantification provides reliable estimates of target DNA concentration in a sample under normal qPCR conditions, MAK2 does not reliably quantify target concentration for qPCR assays with competimeters.\n\nThere are numerous applications for quantitative polymerase chain reaction in the laboratory. It is commonly used for both diagnostic and basic research. Uses of the technique in industry include the quantification of microbial load in foods or on vegetable matter, the detection of GMOs (Genetically modified organisms) and the quantification and genotyping of human viral pathogens.\n\nQuantifying gene expression by traditional DNA detection methods is unreliable. Detection of mRNA on a Northern blot or PCR products on a gel or Southern blot does not allow precise quantification. For example, over the 20-40 cycles of a typical PCR, the amount of DNA product reaches a plateau that is not directly correlated with the amount of target DNA in the initial PCR.\n\nReal-time PCR can be used to quantify nucleic acids by two common methods: relative quantification and absolute quantification. Absolute quantification gives the exact number of target DNA molecules by comparison with DNA standards using a calibration curve. It is therefore essential that the PCR of the sample and the standard have the same amplification efficiency.\nRelative quantification is based on internal reference genes to determine fold-differences in expression of the target gene. The quantification is expressed as the change in expression levels of mRNA interpreted as complementary DNA (cDNA, generated by reverse transcription of mRNA). Relative quantification is easier to carry out as it does not require a calibration curve as the amount of the studied gene is compared to the amount of a control reference gene.\n\nAs the units used to express the results of relative quantification are unimportant the results can be compared across a number of different RT-Q-PCR. The reason for using one or more housekeeping genes is to correct non-specific variation, such as the differences in the quantity and quality of RNA used, which can affect the efficiency of reverse transcription and therefore that of the whole PCR process. However, the most crucial aspect of the process is that the reference gene must be stable.\n\nThe selection of these reference genes was traditionally carried out in molecular biology using qualitative or semi-quantitative studies such as the visual examination of RNA gels, Northern blot densitometry or semi-quantitative PCR (PCR mimics). Now, in the genome era, it is possible to carry out a more detailed estimate for many organisms using transcriptomic technologies. However, research has shown that amplification of the majority of reference genes used in quantifying the expression of mRNA varies according to experimental conditions. It is therefore necessary to carry out an initial statistically sound methodological study in order to select the most suitable reference gene.\n\nA number of statistical algorithms have been developed that can detect which gene or genes are most suitable for use under given conditions. Those like geNORM or BestKeeper can compare pairs or geometric means for a matrix of different reference genes and tissues.\n\nDiagnostic qualitative PCR is applied to rapidly detect nucleic acids that are diagnostic of, for example, infectious diseases, cancer and genetic abnormalities. The introduction of qualitative PCR assays to the clinical microbiology laboratory has significantly improved the diagnosis of infectious diseases, and is deployed as a tool to detect newly emerging diseases, such as new strains of flu, in diagnostic tests.\n\nQuantitative PCR is also used by microbiologists working in the fields of food safety, food spoilage and fermentation and for the microbial risk assessment of water quality (drinking and recreational waters) and in public health protection.\n\nIn research settings, quantitative PCR is mainly used to provide quantitative measurements of gene transcription. The technology may be used in determining how the genetic expression of a particular gene changes over time, such as in the response of tissue and cell cultures to an administration of a pharmacological agent, progression of cell differentiation, or in response to changes in environmental conditions.\nIt is also used for the determination of zygosity of transgenic animals used in research.\n\nThe agricultural industry is constantly striving to produce plant propagules or seedlings that are free of pathogens in order to prevent economic losses and safeguard health. Systems have been developed that allow detection of small amounts of the DNA of \"Phytophthora ramorum\", an oomycete that kills Oaks and other species, mixed in with the DNA of the host plant. Discrimination between the DNA of the pathogen and the plant is based on the amplification of ITS sequences, spacers located in ribosomal RNA gene's coding area, which are characteristic for each taxon. Field-based versions of this technique have also been developed for identifying the same pathogen.\n\nqPCR using reverse transcription (RT-qPCR) can be used to detect GMOs given its sensitivity and dynamic range in detecting DNA. Alternatives such as DNA or protein analysis are usually less sensitive. Specific primers are used that amplify not the transgene but the promoter, terminator or even intermediate sequences used during the process of engineering the vector. As the process of creating a transgenic plant normally leads to the insertion of more than one copy of the transgene its quantity is also commonly assessed. This is often carried out by relative quantification using a control gene from the treated species that is only present as a single copy.\n\nViruses can be present in humans due to direct infection or co-infections which makes diagnosis difficult using classical techniques and can result in an incorrect prognosis and treatment. The use of qPCR allows both the quantification and genotyping (characterization of the strain, carried out using melting curves) of a virus such as the Hepatitis B virus. \nThe degree of infection, quantified as the copies of the viral genome per unit of the patient's tissue, is relevant in many cases; for example, the probability that the type 1 herpes simplex virus reactivates is related to the number of infected neurons in the ganglia. This quantification is carried out either with reverse transcription or without it, as occurs if the virus becomes integrated in the human genome at any point in its cycle, such as happens in the case of HPV (human papillomavirus), where some of its variants are associated with the appearance of cervical cancer.\n\nqPCR is used to evaluate biodegradation potential or activity in contaminated groundwater. qPCR is used to detect and quantify bacteria such as \"dehalococcoides mccartyi\" as well as vinyl chloride reductase functional genes essential to the biodegradation of chlorinated solvents including trichloroethene and tetrachloroethylene. qPCR is also commonly used to quantify functional genes involved in the biodegradation of benzene, toluene, ethylbenzene, and xylenes (BTEX) and evaluate monitored natural attenuation (MNA) or other remedial methods at impacted petroleum hydrocarbon sites. Reverse transcriptase quantitative polymerase chain reaction (RT-qPCR), which is based on the analysis of RNA rather than DNA, is used to quantify gene expression and identify biodegradation activity.\n\n", "id": "3487107", "title": "Real-time polymerase chain reaction"}
{"url": "https://en.wikipedia.org/wiki?curid=97911", "text": "Size-exclusion chromatography\n\nSize-exclusion chromatography (SEC), also known as molecular sieve chromatography, is a chromatographic method in which molecules in solution are separated by their size, and in some cases molecular weight. It is usually applied to large molecules or macromolecular complexes such as proteins and industrial polymers. Typically, when an aqueous solution is used to transport the sample through the column, the technique is known as gel-filtration chromatography, versus the name gel permeation chromatography, which is used when an organic solvent is used as a mobile phase. SEC is a widely used polymer characterization method because of its ability to provide good molar mass distribution (Mw) results for polymers.\n\nThe main application of gel-filtration chromatography is the fractionation of proteins and other water-soluble polymers, while gel permeation chromatography is used to analyze the molecular weight distribution of organic-soluble polymers. Either technique should not be confused with gel electrophoresis, where an electric field is used to \"pull\" or \"push\" molecules through the gel depending on their electrical charges.\n\nThe advantages of this method include good separation of large molecules from the small molecules with a minimal volume of eluate, and that various solutions can be applied without interfering with the filtration process, all while preserving the biological activity of the particles to separate. The technique is generally combined with others that further separate molecules by other characteristics, such as acidity, basicity, charge, and affinity for certain compounds. With size exclusion chromatography, there are short and well-defined separation times and narrow bands, which lead to good sensitivity. There is also no sample loss because solutes do not interact with the stationary phase.\n\nThe other advantage to this experimental method is that in certain cases, it is feasible to determine the approximate molecular weight of a compound. The shape and size of the compound (eluent) determine how the compound interacts with the gel (stationary phase). To determine approximate molecular weight, the elution volumes of compounds with their corresponding molecular weights are obtained and then a plot of “K” vs “log(Mw)” is made, where K = (V-V)/(V-V) and Mw is the molecular mass. This plot acts as a calibration curve, which is used to approximate the desired compound’s molecular weight. The V component represents the volume at which the intermediate molecules elute such as molecules that have partial access to the beads of the column. In addition, V is the sum of the total volume between the beads and the volume within the beads. The V component represents the volume at which the larger molecules elute, which elute in the beginning. Disadvantages are, for example, that only a limited number of bands can be accommodated because the time scale of the chromatogram is short, and, in general, there must be a 10% difference in molecular mass to have a good resolution\n\nThe technique was invented by Grant Henry Lathe and Colin R Ruthven, working at Queen Charlotte’s Hospital, London. They later received the John Scott Award for this invention. While Lathe and Ruthven used starch gels as the matrix, Jerker Porath and Per Flodin later introduced dextran gels; other gels with size fractionation properties include agarose and polyacrylamide. A short review of these developments has appeared.\n\nThere were also attempts to fractionate synthetic high polymers; however, it was not until 1964, when J. C. Moore of the Dow Chemical Company published his work on the preparation of gel permeation chromatography (GPC) columns based on cross-linked polystyrene with controlled pore size, that a rapid increase of research activity in this field began. It was recognized almost immediately that with proper calibration, GPC was capable to provide molar mass and molar mass distribution information for synthetic polymers. Because the latter information was difficult to obtain by other methods, GPC came rapidly into extensive use.\n\nSEC is used primarily for the analysis of large molecules such as proteins or polymers. SEC works by trapping smaller molecules in the pores of the adsorbent materials adsorption (\"stationary phases\"). This process is usually performed with a column, which consists of a hollow tube tightly packed with extremely small porous polymer beads designed to have pores of different sizes. These pores may be depressions on the surface or channels through the bead. As the solution travels down the column some particles enter into the pores. Larger particles cannot enter into as many pores. The larger the particles, the faster the elution. The larger molecules simply pass by the pores because those molecules are too large to enter the pores. Larger molecules therefore flow through the column more quickly than smaller molecules, that is, the smaller the molecule, the longer the retention time.\n\nOne requirement for SEC is that the analyte does not interact with the surface of the stationary phases, with differences in elution time between analytes ideally being based solely on the solute volume the analytes can enter, rather than chemical or electrostatic interactions with the stationary phases. Thus, a small molecule that can penetrate every region of the stationary phase pore system can enter a total volume equal to the sum of the entire pore volume and the interparticle volume. This small molecule elutes late (after the molecule has penetrated all of the pore- and interparticle volume—approximately 80% of the column volume). At the other extreme, a very large molecule that cannot penetrate any the smaller pores can enter only the interparticle volume (~35% of the column volume) and elutes earlier when this volume of mobile phase has passed through the column. The underlying principle of SEC is that particles of different sizes elute (filter) through a stationary phase at different rates. This results in the separation of a solution of particles based on size. Provided that all the particles are loaded simultaneously or near-simultaneously, particles of the same size should elute together.\n\nHowever, as there are various measures of the size of a macromolecule (for instance, the radius of gyration and the hydrodynamic radius), a fundamental problem in the theory of SEC has been the choice of a proper molecular size parameter by which molecules of different kinds are separated. Experimentally, Benoit and co-workers found an excellent correlation between elution volume and a dynamically based molecular size, the hydrodynamic volume, for several different chain architecture and chemical compositions. The observed correlation based on the hydrodynamic volume became accepted as the basis of universal SEC calibration.\n\nStill, the use of the hydrodynamic volume, a size based on dynamical properties, in the interpretation of SEC data is not fully understood. This is because SEC is typically run under low flow rate conditions where hydrodynamic factor should have little effect on the separation. In fact, both theory and computer simulations assume a thermodynamic separation principle: the separation process is determined by the equilibrium distribution (partitioning) of solute macromolecules between two phases --- a dilute bulk solution phase located at the interstitial space and confined solution phases within the pores of column packing material. Based on this theory, it has been shown that the relevant size parameter to the partitioning of polymers in pores is the mean span dimension (mean maximal projection onto a line). Although this issue has not been fully resolved, it is likely that the mean span dimension and the hydrodynamic volume are strongly correlated.\nEach size exclusion column has a range of molecular weights that can be separated. The exclusion limit defines the molecular weight at the upper end of the column 'working' range and is where molecules are too large to get trapped in the stationary phase. The lower end of the range is defined by the permeation limit, which defines the molecular weight of a molecule that is small enough to penetrate all pores of the stationary phase. All molecules below this molecular mass are so small that they elute as a single band\n\nThe filtered solution that is collected at the end is known as the eluate. The void volume includes any particles too large to enter the medium, and the solvent volume is known as the column volume.\n\nIn real-life situations, particles in solution do not have a fixed size, resulting in the probability that a particle that would otherwise be hampered by a pore passing right by it. Also, the stationary-phase particles are not ideally defined; both particles and pores may vary in size. Elution curves, therefore, resemble Gaussian distributions. The stationary phase may also interact in undesirable ways with a particle and influence retention times, though great care is taken by column manufacturers to use stationary phases that are inert and minimize this issue.\n\nLike other forms of chromatography, increasing the column length enhances resolution, and increasing the column diameter increases column capacity. Proper column packing is important for maximum resolution: An over-packed column can collapse the pores in the beads, resulting in a loss of resolution. An under-packed column can reduce the relative surface area of the stationary phase accessible to smaller species, resulting in those species spending less time trapped in pores. Unlike affinity chromatography techniques, a solvent head at the top of the column can drastically diminish resolution as the sample diffuses prior to loading, broadening the downstream elution.\n\nIn simple manual columns, the eluent is collected in constant volumes, known as fractions. The more similar the particles are in size the more likely they are in the same fraction and not detected separately. More advanced columns overcome this problem by constantly monitoring the eluent.\n\nThe collected fractions are often examined by spectroscopic techniques to determine the concentration of the particles eluted. Common spectroscopy detection techniques are refractive index (RI) and ultraviolet (UV). When eluting spectroscopically similar species (such as during biological purification), other techniques may be necessary to identify the contents of each fraction. It is also possible to analyse the eluent flow continuously with RI, LALLS, Multi-Angle Laser Light Scattering MALS, UV, and/or viscosity measurements.\nThe elution volume (Ve) decreases roughly linear with the logarithm of the molecular hydrodynamic volume. Columns are often calibrated using 4-5 standard samples (e.g., folded proteins of known molecular weight), and a sample containing a very large molecule such as thyroglobulin to determine the void volume. (Blue dextran is not recommended for Vo determination because it is heterogeneous and may give variable results) The elution volumes of the standards are divided by the elution volume of the thyroglobulin (Ve/Vo) and plotted against the log of the standards' molecular weights.\n\nIn general, SEC is considered a low resolution chromatography as it does not discern similar species very well, and is therefore often reserved for the final step of a purification. The technique can determine the quaternary structure of purified proteins that have slow exchange times, since it can be carried out under native solution conditions, preserving macromolecular interactions. SEC can also assay protein tertiary structure, as it measures the hydrodynamic volume (not molecular weight), allowing folded and unfolded versions of the same protein to be distinguished. For example, the apparent hydrodynamic radius of a typical protein domain might be 14 Å and 36 Å for the folded and unfolded forms, respectively. SEC allows the separation of these two forms, as the folded form elutes much later due to its smaller size.\n\nSEC can be used as a measure of both the size and the polydispersity of a synthesised polymer, that is, the ability to find the distribution of the sizes of polymer molecules. If standards of a known size are run previously, then a calibration curve can be created to determine the sizes of polymer molecules of interest in the solvent chosen for analysis (often THF). In alternative fashion, techniques such as light scattering and/or viscometry can be used online with SEC to yield absolute molecular weights that do not rely on calibration with standards of known molecular weight. Due to the difference in size of two polymers with identical molecular weights, the absolute determination methods are, in general, more desirable. A typical SEC system can quickly (in about half an hour) give polymer chemists information on the size and polydispersity of the sample. The preparative SEC can be used for polymer fractionation on an analytical scale.\n\nIn SEC, mass is not measured so much as the hydrodynamic volume of the polymer molecules, that is, how much space a particular polymer molecule takes up when it is in solution. However, the approximate molecular weight can be calculated from SEC data because the exact relationship between molecular weight and hydrodynamic volume for polystyrene can be found. For this, polystyrene is used as a standard. But the relationship between hydrodynamic volume and molecular weight is not the same for all polymers, so only an approximate measurement can be obtained.\nAnother drawback is the possibility of interaction between the stationary phase and the analyte. Any interaction leads to a later elution time and thus mimics a smaller analyte size.\n\nWhen performing this method, the bands of the eluting molecules may be broadened. This can occur by turbulence caused by the flow of the mobile phase molecules passing through the molecules of the stationary phase. In addition, molecular thermal diffusion and friction between the molecules of the glass walls and the molecules of the eluent contribute to the broadening of the bands. Besides broadening, the bands also overlap with each other. As a result, the eluent usually gets considerably diluted. A few precautions can be taken to prevent the likelihood of the bands broadening. For instance, one can apply the sample in a narrow, highly concentrated band on the top of the column. The more concentrated the eluent is, the more efficient the procedure would be. However, it is not always possible to concentrate the eluent, which can be considered as one more disadvantage.\n\nAbsolute size-exclusion chromatography (ASEC) is a technique that couples a dynamic light scattering (DLS) instrument to a size exclusion chromatography system for absolute size measurements of proteins and macromolecules as they elute from the chromatography system.\n\nThe definition of absolute used here is that it does not require calibration to obtain hydrodynamic size, often referred to as hydrodynamic diameter (D in units of nm). The sizes of the macromolecules are measured as they elute into the flow cell of the DLS instrument from the size exclusion column set. It should be noted that the hydrodynamic size of the molecules or particles are measured and not their molecular weights. For proteins a Mark-Houwink type of calculation can be used to estimate the molecular weight from the hydrodynamic size.\n\nA big advantage of DLS coupled with SEC is the ability to obtain enhanced DLS resolution . Batch DLS is quick and simple and provides a direct measure of the average size, but the baseline resolution of DLS is 3 to 1 in diameter. Using SEC, the proteins and protein oligomers are separated, allowing oligomeric resolution. Aggregation studies can also be done using ASEC. Though the aggregate concentration may not be calculated, the size of the aggregate can be measured, only limited by the maximum size eluting from the SEC columns.\n\nLimitations of ASEC include flow-rate, concentration, and precision. Because a correlation function requires anywhere from 3–7 seconds to properly build, a limited number of data points can be collected across the peak.\n\n", "id": "97911", "title": "Size-exclusion chromatography"}
{"url": "https://en.wikipedia.org/wiki?curid=583438", "text": "Transformation (genetics)\n\nIn molecular biology, transformation is the genetic alteration of a cell resulting from the direct uptake and incorporation of exogenous genetic material from its surroundings through the cell membrane(s). For transformation to take place, the recipient bacteria must be in a state of competence, which might occur in nature as a time-limited response to environmental conditions such as starvation and cell density, and may also be induced in a laboratory.\n\nTransformation is one of three processes for horizontal gene transfer, in which exogenous genetic material passes from bacterium to another, the other two being conjugation (transfer of genetic material between two bacterial cells in direct contact) and transduction (injection of foreign DNA by a bacteriophage virus into the host bacterium). In transformation, the genetic material passes through the intervening medium, and uptake is completely dependent on the recipient bacterium.\n\nAs of 2014 about 80 species of bacteria were known to be capable of transformation, about evenly divided between Gram-positive and Gram-negative bacteria; the number might be an overestimate since several of the reports are supported by single papers.\n\n\"Transformation\" may also be used to describe the insertion of new genetic material into nonbacterial cells, including animal and plant cells; however, because \"transformation\" has a special meaning in relation to animal cells, indicating progression to a cancerous state, the process is usually called \"transfection\".\n\nTransformation in bacteria was first demonstrated in 1928 by British bacteriologist Frederick Griffith. Griffith discovered that a strain of \"Streptococcus pneumoniae\" could be made virulent after being exposed to heat-killed virulent strains. Griffith hypothesized that some \"transforming principle\" from the heat-killed strain was responsible for making the harmless strain virulent. In 1944 this \"transforming principle\" was identified as being genetic by Oswald Avery, Colin MacLeod, and Maclyn McCarty. They isolated DNA from a virulent strain of \"S. pneumoniae\" and using just this DNA were able to make a harmless strain virulent. They called this uptake and incorporation of DNA by bacteria \"transformation\" (See Avery-MacLeod-McCarty experiment). \nAccording to the textbook entitled, Microbiology an Introduction tenth edition; states \"Griffith was interested in determining whether injections of heat-killed bacteria of the encapsulated strain could be used to vaccinate mice against pneumonia. As he expected injections of living encapsulated bacteria killed the mouse. Injections of living nonencapsulated bacteria or injections of dead encapsulated bacteria did not kill the mouse. However, when the dead encapsulated bacteria were mixed with live nonencapsulated bacteria and injected into the mice, many of the mice died\". Subsequent investigations based on Griffith's research revealed that bacterial transformation could be carried out without mice.. The results of Avery et al.'s experiments were at first skeptically received by the scientific community and it was not until the development of genetic markers and the discovery of other methods of genetic transfer (conjugation in 1947 and transduction in 1953) by Joshua Lederberg that Avery's experiments were accepted.\n\nIt was originally thought that \"Escherichia coli\", a commonly used laboratory organism, was refractory to transformation. However, in 1970, Morton Mandel and Akiko Higa showed that \"E. coli\" may be induced to take up DNA from bacteriophage λ without the use of helper phage after treatment with calcium chloride solution. Two years later in 1972, Stanley Norman Cohen, Annie Chang and Leslie Hsu showed that treatment is also effective for transformation of plasmid DNA. The method of transformation by Mandel and Higa was later improved upon by Douglas Hanahan. The discovery of artificially induced competence in \"E. coli\" created an efficient and convenient procedure for transforming bacteria which allows for simpler molecular cloning methods in biotechnology and research, and it is now a routinely used laboratory procedure.\n\nTransformation using electroporation was developed in the late 1980s, increasing the efficiency of in-vitro transformation and increasing the number of bacterial strains that could be transformed. Transformation of animal and plant cells was also investigated with the first transgenic mouse being created by injecting a gene for a rat growth hormone into a mouse embryo in 1982. In 1907 a bacterium that caused plant tumors, \"Agrobacterium tumefaciens\", was discovered and in the early 1970s the tumor-inducing agent was found to be a DNA plasmid called the Ti plasmid. By removing the genes in the plasmid that caused the tumor and adding in novel genes, researchers were able to infect plants with \"A. tumefaciens\" and let the bacteria insert their chosen DNA into the genomes of the plants. Not all plant cells are susceptible to infection by \"A. tumefaciens\", so other methods were developed, including electroporation and micro-injection. Particle bombardment was made possible with the invention of the Biolistic Particle Delivery System (gene gun) by John Sanford in the 1980s.\n\nTransformation is one of three forms of horizontal gene transfer that occur in nature among bacteria, in which DNA encoding for a trait passes from one bacterium to another and is integrated into the recipient genome by homologous recombination; the other two are transduction, carried out by means of a bacteriophage, and conjugation, in which a gene is passed through direct contact between bacteria. In transformation, the genetic material passes through the intervening medium, and uptake is completely dependent on the recipient bacterium.\n\nCompetence refers to a temporary state of being able to take up exogenous DNA from the environment; it may be induced in a laboratory.\n\nIt appears to be an ancient process inherited from a common prokaryotic ancestor that is a beneficial adaptation for promoting recombinational repair of DNA damage, especially damage acquired under stressful conditions. Natural genetic transformation appears to be an adaptation for repair of DNA damage that also generates genetic diversity.\n\nTransformation has been studied in medically important Gram-negative bacteria species such as \"Helicobacter pylori\", \"Legionella pneumophila\", \"Neisseria meningitidis\", \"Neisseria gonorrhoeae\", \"Haemophilus influenzae\" and \"Vibrio cholerae\". It has also been studied in Gram-negative species found in soil such as \"Pseudomonas stutzeri\", \"Acinetobacter baylyi\", and Gram-negative plant pathogens such as \"Ralstonia solanacearum\" and \"Xylella fastidiosa\". Transformation among Gram-positive bacteria has been studied in medically important species such as \"Streptococcus pneumoniae\", \"Streptococcus mutans\", \"Staphylococcus aureus\" and \"Streptococcus sanguinis\" and in Gram-positive soil bacterium \"Bacillus subtilis\". It has also been reported in at least 30 species of \"Proteobacteria\" distributed in the classes alpha, beta, gamma and epsilon. The best studied \"Proteobacteria\" with respect to transformation are the medically important human pathogens \"Neisseria gonorrhoeae\" (class beta), \"Haemophilus influenzae\" (class gamma) and \"Helicobacter pylori\" (class epsilon)\n\n\"Transformation\" may also be used to describe the insertion of new genetic material into nonbacterial cells, including animal and plant cells; however, because \"transformation\" has a special meaning in relation to animal cells, indicating progression to a cancerous state, the process is usually called \"transfection\".\n\nAs of 2014 about 80 species of bacteria were known to be capable of transformation, about evenly divided between Gram-positive and Gram-negative bacteria; the number might be an overestimate since several of the reports are supported by single papers.\n\nNaturally competent bacteria carry sets of genes that provide the protein machinery to bring DNA across the cell membrane(s). The transport of the exogenous DNA into the cells may require proteins that are involved in the assembly of type IV pili and type II secretion system, as well as DNA translocase complex at the cytoplasmic membrane.\n\nDue to the differences in structure of the cell envelope between Gram-positive and Gram-negative bacteria, there are some differences in the mechanisms of DNA uptake in these cells, however most of them share common features that involve related proteins. The DNA first binds to the surface of the competent cells on a DNA receptor, and passes through the cytoplasmic membrane via DNA translocase. Only single-stranded DNA may pass through, the other strand being degraded by nucleases in the process. The translocated single-stranded DNA may then be integrated into the bacterial chromosomes by a RecA-dependent process. In Gram-negative cells, due to the presence of an extra membrane, the DNA requires the presence of a channel formed by secretins on the outer membrane. Pilin may be required for competence, but its role is uncertain. The uptake of DNA is generally non-sequence specific, although in some species the presence of specific DNA uptake sequences may facilitate efficient DNA uptake.\n\nNatural transformation is a bacterial adaptation for DNA transfer that depends on the expression of numerous bacterial genes whose products appear to be responsible for this process. In general, transformation is a complex, energy-requiring developmental process. In order for a bacterium to bind, take up and recombine exogenous DNA into its chromosome, it must become competent, that is, enter a special physiological state. Competence development in \"Bacillus subtilis\" requires expression of about 40 genes. The DNA integrated into the host chromosome is usually (but with rare exceptions) derived from another bacterium of the same species, and is thus homologous to the resident chromosome.\n\nIn \"B. subtilis\" the length of the transferred DNA is greater than 1271 kb (more than 1 million bases). The length transferred is likely double stranded DNA and is often more than a third of the total chromosome length of 4215 kb. It appears that about 7-9% of the recipient cells take up an entire chromosome.\n\nThe capacity for natural transformation appears to occur in a number of prokaryotes, and thus far 67 prokaryotic species (in seven different phyla) are known to undergo this process.\n\nCompetence for transformation is typically induced by high cell density and/or nutritional limitation, conditions associated with the stationary phase of bacterial growth. Transformation in \"Haemophilus influenzae\" occurs most efficiently at the end of exponential growth as bacterial growth approaches stationary phase. Transformation in \"Streptococcus mutans\", as well as in many other streptococci, occurs at high cell density and is associated with biofilm formation. Competence in \"B. subtilis\" is induced toward the end of logarithmic growth, especially under conditions of amino acid limitation.\n\nBy releasing intact host and plasmid DNA, certain bacteriophages are thought to contribute to transformation.\n\nCompetence is specifically induced by DNA damaging conditions. For instance, transformation is induced in \"Streptococcus pneumoniae\" by the DNA damaging agents mitomycin C (a DNA crosslinking agent) and fluoroquinolone (a topoisomerase inhibitor that causes double-strand breaks). In \"B. subtilis\", transformation is increased by UV light, a DNA damaging agent. In \"Helicobacter pylori\", ciprofloxacin, which interacts with DNA gyrase and introduces double-strand breaks, induces expression of competence genes, thus enhancing the frequency of transformation Using \"Legionella pneumophila\", Charpentier et al. tested 64 toxic molecules to determine which of these induce competence. Of these only six, all DNA damaging agents, caused strong induction. These DNA damaging agents were mitomycin C (which causes DNA inter-strand crosslinks), norfloxacin, ofloxacin and nalidixic acid (inhibitors of DNA gyrase that cause double-strand breaks), bicyclomycin (causes single- and double-strand breaks), and hydroxyurea (induces DNA base oxidation). UV light also induced competence in \"L. pneumophila\". Charpentier et al. suggested that competence for transformation probably evolved as a DNA damage response.\nLogarithmically growing bacteria differ from stationary phase bacteria with respect to the number of genome copies present in the cell, and this has implications for the capability to carry out an important DNA repair process. During logarithmic growth, two or more copies of any particular region of the chromosome may be present in a bacterial cell, as cell division is not precisely matched with chromosome replication. The process of homologous recombinational repair (HRR) is a key DNA repair process that is especially effective for repairing double-strand damages, such as double-strand breaks. This process depends on a second homologous chromosome in addition to the damaged chromosome. During logarithmic growth, a DNA damage in one chromosome may be repaired by HRR using sequence information from the other homologous chromosome. Once cells approach stationary phase, however, they typically have just one copy of the chromosome, and HRR requires input of homologous template from outside the cell by transformation.\nTo test whether the adaptive function of transformation is repair of DNA damages, a series of experiments were carried out using \"B. subtilis\" irradiated by UV light as the damaging agent (reviewed by Michod et al. and Bernstein et al.) The results of these experiments indicated that transforming DNA acts to repair potentially lethal DNA damages introduced by UV light in the recipient DNA. The particular process responsible for repair was likely HRR. Transformation in bacteria can be viewed as a primitive sexual process, since it involves interaction of homologous DNA from two individuals to form recombinant DNA that is passed on to succeeding generations. Bacterial transformation in prokaryotes may have been the ancestral process that gave rise to meiotic sexual reproduction in eukaryotes (see Evolution of sexual reproduction; Meiosis.)\n\nArtificial competence can be induced in laboratory procedures that involve making the cell passively permeable to DNA by exposing it to conditions that do not normally occur in nature. Typically the cells are incubated in a solution containing divalent cations (often calcium chloride) under cold conditions, before being exposed to a heat pulse (heat shock). Calcium chloride partially disrupts the cell membrane, which allows the recombinant DNA enter the host cell. Cells that are able to take up the DNA are called competent cells. \n\nIt has been found that growth of Gram-negative bacteria in 20 mM Mg reduces the number of protein-to-lipopolysaccharide bonds by increasing the ratio of ionic to covalent bonds, which increases membrane fluidity, facilitating transformation. The role of lipopolysaccharides here are verified from the observation that shorter O-side chains are more effectively transformed – perhaps because of improved DNA accessibility.\n\nThe surface of bacteria such as \"E. coli\" is negatively charged due to phospholipids and lipopolysaccharides on its cell surface, and the DNA is also negatively charged. One function of the divalent cation therefore would be to shield the charges by coordinating the phosphate groups and other negative charges, thereby allowing a DNA molecule to adhere to the cell surface.\n\nDNA entry into \"E. coli\" cells is through channels known as zones of adhesion or Bayer's junction, with a typical cell carrying as many as 400 such zones. Their role was established when cobalamine (which also uses these channels) was found to competitively inhibit DNA uptake. Another type of channel implicated in DNA uptake consists of poly (HB):poly P:Ca. In this poly (HB) is envisioned to wrap around DNA (itself a polyphosphate), and is carried in a shield formed by Ca ions.\n\nIt is suggested that exposing the cells to divalent cations in cold condition may also change or weaken the cell surface structure, making it more permeable to DNA. The heat-pulse is thought to create a thermal imbalance across the cell membrane, which forces the DNA to enter the cells through either cell pores or the damaged cell wall.\n\nElectroporation is another method of promoting competence. In this method the cells are briefly shocked with an electric field of 10-20 kV/cm, which is thought to create holes in the cell membrane through which the plasmid DNA may enter. After the electric shock, the holes are rapidly closed by the cell's membrane-repair mechanisms.\n\nMost species of yeast, including \"Saccharomyces cerevisiae\", may be transformed by exogenous DNA in the environment. Several methods have been developed to facilitate this transformation at high frequency in the lab.\n\nEfficiency – Different yeast genera and species take up foreign DNA with different efficiencies. Also, most transformation protocols have been developed for baker's yeast, \"S. cerevisiae\", and thus may not be optimal for other species. Even within one species, different strains have different transformation efficiencies, sometimes different by three orders of magnitude. For instance, when S. cerevisiae strains were transformed with 10 ug of plasmid YEp13, the strain DKD-5D-H yielded between 550 and 3115 colonies while strain OS1 yielded fewer than five colonies.\n\nA number of methods are available to transfer DNA into plant cells. Some vector-mediated methods are:\n\nSome vector-less methods include:\n\nIntroduction of DNA into animal cells is usually called transfection, and is discussed in the corresponding article.\n\nThe discovery of artificially induced competence in bacteria allow bacteria such as \"Escherichia coli\" to be used as a convenient host for the manipulation of DNA as well as expressing proteins. Typically plasmids are used for transformation in \"E. coli\". In order to be stably maintained in the cell, a plasmid DNA molecule must contain an origin of replication, which allows it to be replicated in the cell independently of the replication of the cell's own chromosome.\n\nThe efficiency with which a competent culture can take up exogenous DNA and express its genes is known as transformation efficiency and is measured in colony forming unit (cfu) per μg DNA used. A transformation efficiency of 1×10 cfu/μg for a small plasmid like pUC19 is roughly equivalent to 1 in 2000 molecules of the plasmid used being transformed.\n\nIn calcium chloride transformation, the cells are prepared by chilling cells in the presence of (in solution), making the cell become permeable to plasmid DNA. The cells are incubated on ice with the DNA, and then briefly heat-shocked (e.g., at 42 °C for 30–120 seconds). This method works very well for circular plasmid DNA. Non-commercial preparations should normally give 10 to 10 transformants per microgram of plasmid; a poor preparation will be about 10/μg or less, but a good preparation of competent cells can give up to ~10 colonies per microgram of plasmid. Protocols, however, exist for making supercompetent cells that may yield a transformation efficiency of over 10. The chemical method, however, usually does not work well for linear DNA, such as fragments of chromosomal DNA, probably because the cell's native exonuclease enzymes rapidly degrade linear DNA. In contrast, cells that are naturally competent are usually transformed more efficiently with linear DNA than with plasmid DNA.\nThe transformation efficiency using the method decreases with plasmid size, and electroporation therefore may be a more effective method for the uptake of large plasmid DNA. Cells used in electroporation should be prepared first by washing in cold double-distilled water to remove charged particles that may create sparks during the electroporation process.\n\nBecause transformation usually produces a mixture of relatively few transformed cells and an abundance of non-transformed cells, a method is necessary to select for the cells that have acquired the plasmid. The plasmid therefore requires a selectable marker such that those cells without the plasmid may be killed or have their growth arrested. Antibiotic resistance is the most commonly used marker for prokaryotes. The transforming plasmid contains a gene that confers resistance to an antibiotic that the bacteria are otherwise sensitive to. The mixture of treated cells is cultured on media that contain the antibiotic so that only transformed cells are able to grow. Another method of selection is the use of certain auxotrophic markers that can compensate for an inability to metabolise certain amino acids, nucleotides, or sugars. This method requires the use of suitably mutated strains that are deficient in the synthesis or utility of a particular biomolecule, and the transformed cells are cultured in a medium that allows only cells containing the plasmid to grow.\n\nIn a cloning experiment, a gene may be inserted into a plasmid used for transformation. However, in such experiment, not all the plasmids may contain a successfully inserted gene. Additional techniques may therefore be employed further to screen for transformed cells that contain plasmid with the insert. Reporter genes can be used as markers, such as the \"lacZ\" gene which codes for β-galactosidase used in blue-white screening. This method of screening relies on the principle of α-complementation, where a fragment of the \"lacZ\" gene (\"lacZα\") in the plasmid can complement another mutant \"lacZ\" gene (\"lacZΔM15\") in the cell. Both genes by themselves produce non-functional peptides, however, when expressed together, as when a plasmid containing \"lacZ-α\" is transformed into a \"lacZΔM15\" cells, they form a functional β-galactosidase. The presence of an active β-galactosidase may be detected when cells are grown in plates containing X-gal, forming characteristic blue colonies. However, the multiple cloning site, where a gene of interest may be ligated into the plasmid vector, is located within the \"lacZα\" gene. Successful ligation therefore disrupts the \"lacZα\" gene, and no functional β-galactosidase can form, resulting in white colonies. Cells containing successfully ligated insert can then be easily identified by its white coloration from the unsuccessful blue ones.\n\nOther commonly used reporter genes are green fluorescent protein (GFP), which produces cells that glow green under blue light, and the enzyme luciferase, which catalyzes a reaction with luciferin to emit light. The recombinant DNA may also be detected using other methods such as nucleic acid hybridization with radioactive RNA probe, while cells that expressed the desired protein from the plasmid may also be detected using immunological methods.\n\n", "id": "583438", "title": "Transformation (genetics)"}
{"url": "https://en.wikipedia.org/wiki?curid=27456923", "text": "List of restriction enzyme cutting sites: S\n\nThis article contains a list of the most studied restriction enzymes whose names start with S. It contains approximately 130 enzymes.\nThe following information is given:\n", "id": "27456923", "title": "List of restriction enzyme cutting sites: S"}
{"url": "https://en.wikipedia.org/wiki?curid=25672736", "text": "List of restriction enzyme cutting sites\n\nA restriction enzyme or restriction endonuclease is a special type of biological macromolecule that functions as part of the \"immune system\" in bacteria. One special kind of restriction enzymes is the class of \"homing endonucleases\", these being present in all three domains of life, although their function seems to be very different from one domain to another.\n\nThe classical restriction enzymes cut up, and hence render harmless, any unknown (non-cellular) DNA that enters a bacterial cell as a result of a viral infection. They recognize a specific DNA sequence, usually short (3 to 8 bp), and cut it, producing either blunt or overhung ends, either at or nearby the recognition site.\n\nRestriction enzymes are quite variable in the short DNA sequences they recognize. An organism often has several different enzymes, each specific to a distinct short DNA sequence.\n\nThe list includes some of the most studied examples of restriction endoncleases. The following information is given:\nThe whole list contains more than 1,200 enzymes, but databases register about 4,000.\n\nTo make a list that is accessible to navigation, this list has been divided into different pages. Each page contains somewhere between 120-150 entries. Choose a letter to go to a specific part of the list:\n\n\n\"Databases and lists of restriction enzymes:\"\n\"Databases of proteins:\"\n", "id": "25672736", "title": "List of restriction enzyme cutting sites"}
{"url": "https://en.wikipedia.org/wiki?curid=299901", "text": "Optical tweezers\n\nOptical tweezers (originally called \"single-beam gradient force trap\") are scientific instruments that use a highly focused laser beam to provide an attractive or repulsive force (typically on the order of piconewtons), depending on the refractive index mismatch to physically hold and move microscopic dielectric objects similar to tweezers. Optical tweezers have been particularly successful in studying a variety of biological systems in recent years.\n\nThe detection of optical scattering and gradient forces on micron sized particles was first reported in 1970 by Arthur Ashkin, a scientist working at Bell Labs. Years later, Ashkin and colleagues reported the first observation of what is now commonly referred to as an optical tweezer: a tightly focused beam of light capable of holding microscopic particles stable in three dimensions.\n\nOne of the authors of this seminal 1986 paper, former United States Secretary of Energy Steven Chu, would go on to use optical tweezing in his work on cooling and trapping neutral atoms. This research earned Chu the 1997 Nobel Prize in Physics along with Claude Cohen-Tannoudji and William D. Phillips. In an interview, Steven Chu described how Ashkin had first envisioned optical tweezing as a method for trapping atoms. Ashkin was able to trap larger particles (10 to 10,000 nanometers in diameter) but it fell to Chu to extend these techniques to the trapping of neutral atoms (0.1 nanometers in diameter) utilizing resonant laser light and a magnetic gradient trap (cf. Magneto-optical trap).\n\nIn the late 1980s, Arthur Ashkin and Joseph M. Dziedzic demonstrated the first application of the technology to the biological sciences, using it to trap an individual tobacco mosaic virus and \"Escherichia coli\" bacterium. Throughout the 1990s and afterwards, researchers like Carlos Bustamante, James Spudich, and Steven Block pioneered the use of optical trap force spectroscopy to characterize molecular-scale biological motors. These molecular motors are ubiquitous in biology, and are responsible for locomotion and mechanical action within the cell. Optical traps allowed these biophysicists to observe the forces and dynamics of nanoscale motors at the single-molecule level; optical trap force-spectroscopy has since led to greater understanding of the stochastic nature of these force-generating molecules.\n\nOptical tweezers have proven useful in other areas of biology as well. For instance, in 2003 the techniques of optical tweezers were applied in the field of cell sorting; by creating a large optical intensity pattern over the sample area, cells can be sorted by their intrinsic optical characteristics. Optical tweezers have also been used to probe the cytoskeleton, measure the visco-elastic properties of biopolymers, and study cell motility. A bio-molecular assay in which clusters of ligand coated nano-particles are both optically trapped and optically detected after target molecule induced clustering was proposed in 2011 and experimentally demonstrated in 2013.\n\nThe Kapitsa–Dirac effect effectively demonstrated during 2001 uses standing waves of light to affect a beam of particles.\n\nResearchers have also worked to convert optical tweezers from large, complex instruments to smaller, simpler ones, for use by those with smaller research budgets.\n\nOptical tweezers are capable of manipulating nanometer and micron-sized dielectric particles by exerting extremely small forces via a highly focused laser beam. The beam is typically focused by sending it through a microscope objective. The narrowest point of the focused beam, known as the beam waist, contains a very strong electric field gradient. Dielectric particles are attracted along the gradient to the region of strongest electric field, which is the center of the beam. The laser light also tends to apply a force on particles in the beam along the direction of beam propagation. This is due to conservation of momentum: photons that are absorbed or scattered by the tiny dielectric particle impart momentum to the dielectric particle. This is known as the scattering force and results in the particle being displaced slightly downstream from the exact position of the beam waist, as seen in the figure.\n\nOptical traps are very sensitive instruments and are capable of the manipulation and detection of sub-nanometer displacements for sub-micron dielectric particles. For this reason, they are often used to manipulate and study single molecules by interacting with a bead that has been attached to that molecule. DNA and the proteins and enzymes that interact with it are commonly studied in this way.\n\nFor quantitative scientific measurements, most optical traps are operated in such a way that the dielectric particle rarely moves far from the trap center. The reason for this is that the force applied to the particle is linear with respect to its displacement from the center of the trap as long as the displacement is small. In this way, an optical trap can be compared to a simple spring, which follows Hooke's law.\n\nProper explanation of optical trapping behavior depends upon the size of the trapped particle relative to the wavelength of light used to trap it. In cases where the dimensions of the particle are much greater than the wavelength, a simple ray optics treatment is sufficient. If the wavelength of light far exceeds the particle dimensions, the particles can be treated as electric dipoles in an electric field. For optical trapping of dielectric objects of dimensions within an order of magnitude of the trapping beam wavelength, the only accurate models involve the treatment of either time dependent or time harmonic Maxwell equations using appropriate boundary conditions.\n\nIn cases where the diameter of a trapped particle is significantly greater than the wavelength of light, the trapping phenomenon can be explained using ray optics. As shown in the figure, individual rays of light emitted from the laser will be refracted as it enters and exits the dielectric bead. As a result, the ray will exit in a direction different from which it originated. Since light has a momentum associated with it, this change in direction indicates that its momentum has changed. Due to Newton's third law, there should be an equal and opposite momentum change on the particle.\n\nMost optical traps operate with a Gaussian beam (TEM mode) profile intensity. In this case, if the particle is displaced from the center of the beam, as in the right part of the figure, the particle has a net force returning it to the center of the trap because more intense beams impart a larger momentum change towards the center of the trap than less intense beams, which impart a smaller momentum change away from the trap center. The net momentum change, or force, returns the particle to the trap center.\n\nIf the particle is located at the center of the beam, then individual rays of light are refracting through the particle symmetrically, resulting in no net lateral force. The net force in this case is along the axial direction of the trap, which cancels out the scattering force of the laser light. The cancellation of this axial gradient force with the scattering force is what causes the bead to be stably trapped slightly downstream of the beam waist.\n\nThe standard tweezers works with the trapping laser propagated in the\ndirection of gravity and the inverted tweezers works against gravity.\nIn cases where the diameter of a trapped particle is significantly smaller than the wavelength of light, the conditions for Rayleigh scattering are satisfied and the particle can be treated as a point dipole in an inhomogeneous electromagnetic field. The force applied on a single charge in an electromagnetic field is known as the Lorentz force,\n\nThe force on the dipole can be calculated by substituting two terms for the electric field in the equation above, one for each charge. The polarization of a dipole is formula_2 where formula_3 is the distance between the two charges. For a point dipole, the distance is infinitesimal, formula_4 Taking into account that the two charges have opposite signs, the force takes the form\n\nNotice that the formula_6 cancel out. Multiplying through by the charge, formula_7, converts position, formula_8, into polarization, formula_9,\n\nwhere in the second equality, it has been assumed that the dielectric particle is linear (i.e. formula_11).\n\nIn the final steps, two equalities will be used: (1) A Vector Analysis Equality, (2) One of Maxwell's Equations.\n\nFirst, the vector equality will be inserted for the first term in the force equation above. Maxwell's equation will be substituted in for the second term in the vector equality. Then the two terms which contain time derivatives can be combined into a single term.\n\nThe second term in the last equality is the time derivative of a quantity that is related through a multiplicative constant to the Poynting vector, which describes the power per unit area passing through a surface. Since the power of the laser is constant when sampling over frequencies much shorter than the frequency of the laser's light ~10 Hz, the derivative of this term averages to zero and the force can be written as \n\nwhere in the second part we have included the induced dipole of a spherical dielectric particle: formula_16, where formula_17 is the particle radius, formula_18 is the index of the medium and formula_19 is the relative index of the particle.\nThe square of the magnitude of the electric field is equal to the intensity of the beam as a function of position. Therefore, the result indicates that the force on the dielectric particle, when treated as a point dipole, is proportional to the gradient along the intensity of the beam. In other words, the gradient force described here tends to attract the particle to the region of highest intensity. In reality, the scattering force of the light works against the gradient force in the axial direction of the trap, resulting in an equilibrium position that is displaced slightly downstream of the intensity maximum. Under the Rayleigh approximation, we can also write the scattering force as\n\nSince the scattering is isotropic, the net momentum is transferred in the forward direction. On the quantum level, we picture the gradient force as forward Rayleigh scattering in which identical photons are created and annihilated concurrently, while in the scattering (radiation) force the incident photons travel in the same direction and ‘scatter’ isotropically. By conservation of momentum, the particle must accumulate the photons' original momenta, causing a forward force in the latter.\n\nA useful way to study the interaction of an atom in a Gaussian beam is to look at the harmonic potential approximation of the intensity profile the atom experiences. In the case of the two-level atom, the potential experienced is related to its AC Stark Shift,\n\nwhere formula_22 is the natural line width of the excited state, formula_23 is the electric dipole coupling, formula_24 is the frequency of the transition, and formula_25 is the defining or difference between the laser frequency and the transition frequency.\n\nThe intensity of a gaussian beam profile is characterized by the wavelength formula_26, minimum waist formula_27, and power of the beam formula_28. The following formulas define the beam profile:\n\nTo approximate this Gaussian potential in both the radial and axial directions of the beam, the intensity profile must be expanded to second order in formula_33 and formula_34 for formula_35 and formula_36 respectively and equated to the harmonic potential formula_37. These expansions are evaluated assuming fixed power.\n\nThis means that when solving for the harmonic frequencies (or trap frequencies when considering optical traps for atoms), the frequencies are given as:\n\nso that the relative trap frequencies for the radial and axial directions as a function of only beam waist scale as:\n\nThe most basic optical tweezer setup will likely include the following components: a laser (usually ), a beam expander, some optics used to steer the beam location in the sample plane, a microscope objective and condenser to create the trap in the sample plane, a position detector (e.g. quadrant photodiode) to measure beam displacements and a microscope illumination source coupled to a CCD camera.\n\nAn (1064 nm wavelength) is a common choice of laser for working with biological specimens. This is because such specimens (being mostly water) have a low absorption coefficient at this wavelength. A low absorption is advisable so as to minimise damage to the biological material, sometimes referred to as opticution. Perhaps the most important consideration in optical tweezer design is the choice of the objective. A stable trap requires that the gradient force, which is dependent upon the numerical aperture (NA) of the objective, be greater than the scattering force. Suitable objectives typically have an NA between 1.2 and 1.4.\n\nWhile alternatives are available, perhaps the simplest method for position detection involves imaging the trapping laser exiting the sample chamber onto a quadrant photodiode. Lateral deflections of the beam are measured similarly to how it is done using atomic force microscopy (AFM).\n\nExpanding the beam emitted from the laser to fill the aperture of the objective will result in a tighter, diffraction-limited spot. While lateral translation of the trap relative to the sample can be accomplished by translation of the microscope slide, most tweezer setups have additional optics designed to translate the beam to give an extra degree of translational freedom. This can be done by translating the first of the two lenses labelled as \"Beam Steering\" in the figure. For example, translation of that lens in the lateral plane will result in a laterally deflected beam from what is drawn in the figure. If the distance between the beam steering lenses and the objective is chosen properly, this will correspond to a similar deflection before entering the objective and a resulting \"lateral translation\" in the sample plane. The position of the beam waist, that is the focus of the optical trap, can be adjusted by an axial displacement of the initial lens. Such an axial displacement causes the beam to diverge or converge slightly, the end result of which is an axially displaced position of the beam waist in the sample chamber.\n\nVisualization of the sample plane is usually accomplished through illumination via a separate light source coupled into the optical path in the opposite direction using dichroic mirrors. This light is incident on a CCD camera and can be viewed on an external monitor or used for tracking the trapped particle position via video tracking.\n\nThe majority of optical tweezers make use of conventional TEM Gaussian beams. However a number of other beam types have been used to trap particles, including high order laser beams i.e. Hermite-Gaussian beams (TEM), Laguerre-Gaussian (LG) beams (TEM) and Bessel beams.\n\nOptical tweezers based on Laguerre-Gaussian beams have the unique capability of trapping particles that are optically reflective and absorptive. Laguerre-Gaussian beams also possess a well-defined orbital angular momentum that can rotate particles. This is accomplished without external mechanical or electrical steering of the beam.\n\nBoth zero and higher order Bessel Beams also possess a unique tweezing ability. They can trap and rotate multiple particles that are millimeters apart and even around obstacles.\n\nMicromachines can be driven by these unique optical beams due to their intrinsic rotating mechanism due to the spin and orbital angular momentum of light.\nA typical setup uses one laser to create one or two traps. Commonly, two traps are generated by splitting the laser beam into two orthogonally polarized beams. Optical tweezing operations with more than two traps can be realized either by time-sharing a single laser beam among several optical tweezers, or by diffractively splitting the beam into multiple traps. With acousto-optic deflectors or galvanometer-driven mirrors, a single laser beam can be shared among hundreds of optical tweezers in the focal plane, or else spread into an extended one-dimensional trap. Specially designed diffractive optical elements can divide a single input beam into hundreds of continuously illuminated traps in arbitrary three-dimensional configurations. The trap-forming hologram also can specify the mode structure of each trap individually, thereby creating arrays of optical vortices, optical tweezers, and holographic line traps, for example. When implemented with a spatial light modulator, such holographic optical traps also can move objects in three dimensions. Advanced forms of holographic optical traps with arbitrary spatial profiles, where smoothness of the intensity and the phase are controlled, find applications in many areas of science, from micromanipulation to ultracold atoms.\n\nThe standard fiber optical trap relies on the same principle as the optical trapping, but with the Gaussian laser beam delivered through an Optical fiber. If one end of the optical fiber is molded into a lens-like facet, the nearly gaussian beam carried by a single mode standard fiber will be focused at some distance from the fiber tip. The effective Numerical Aperture of such assembly is usually not enough to allow for a full 3D optical trap but only for a 2D trap (optical trapping and manipulation of objects will be possible only when, e.g., they are in contact with a surface ).\nA true 3D optical trapping based on a single fiber, with a trapping point which is not in nearly contact with the fiber tip, has been realized based on a not-standard annular-core fiber arrangement and a total-internal-reflection geometry.\n\nOn the other hand, if the ends of the fiber are not moulded, the laser exiting the fiber will be diverging and thus a stable optical trap can only be realised by balancing the gradient and the scattering force from two opposing ends of the fiber. The gradient force will trap the particles in the transverse direction, while the axial optical force comes from the scattering force of the two counter propagating beams emerging from the two fibers. The equilibrium z-position of such a trapped bead is where the two scattering forces equal each other. This work was pioneered by A. Constable \"et al.\", \"Opt. Lett.\" 18,1867 (1993), and followed by J.Guck \"et al.\", \"Phys. Rev. Lett.\" 84, 5451 (2000), who made use of this technique to stretch microparticles. By manipulating the input power into the two ends of the fiber, there will be an increase of a \"optical stretching\" that can be used to measure viscoelastic properties of cells, with sensitivity sufficient to distinguish between different individual cytoskeletal phenotypes. i.e. human erythrocytes and mouse fibroblasts. A recent test has seen great success in differentiating cancerous cells from non-cancerous ones from the two opposed, non-focused laser beams.\n\nWhile earlier version of fiber-based laser traps exclusively used single mode beams, M. Kreysing and colleagues recently showed that the careful excitation of further optical modes in a short piece of optical fiber allows the realization of non-trivial trapping geometries. By this the researchers were able to orient various human cell types (individual cells and clusters) on a microscope. The main advantage of the so-called \"optical cell rotator\" technology over standard optical tweezers is the decoupling of trapping from imaging optics. This, its modular design, and the high compatibility of divergent laser traps with biological material indicates the great potential of this new generation of laser traps in medical research and life science. Recently, the optical cell rotator technology was implemented on the basis of adaptive optics, allowing to dynamically reconfigure the optical trap during operation and adapt it to the sample.\n\nOne of the more common cell-sorting systems makes use of flow cytometry through fluorescent imaging. In this method, a suspension of biologic cells is sorted into two or more containers, based upon specific fluorescent characteristics of each cell during an assisted flow. By using an electrical charge that the cell is \"trapped\" in, the cells are then sorted based on the fluorescence intensity measurements. The sorting process is undertaken by an electrostatic deflection system that diverts cells into containers based upon their charge.\n\nIn the optically actuated sorting process, the cells are flowed through into an optical landscape i.e. 2D or 3D optical lattices. Without any induced electrical charge, the cells would sort based on their intrinsic refractive index properties and can be re-configurability for dynamic sorting. An optical lattice can be created using diffractive optics and optical elements.\n\nOn the other hand, K. Ladavac \"et al.\" used a spatial light modulator to project an intensity pattern to enable the optical sorting process. K. Xiao and D. G. Grier applied holographic video microscopy to demonstrate that this technique can sort colloidal spheres with part-per-thousand resolution for size and refractive index.\n\nThe main mechanism for sorting is the arrangement of the optical lattice points. As the cell flow through the optical lattice, there are forces due to the particles drag force that is competing directly with the optical gradient force (See Physics of optical tweezers) from the optical lattice point. By shifting the arrangement of the optical lattice point, there is a preferred optical path where the optical forces are dominant and biased. With the aid of the flow of the cells, there is a resultant force that is directed along that preferred optical path. Hence, there is a relationship of the flow rate with the optical gradient force. By adjusting the two forces, one will be able to obtain a good optical sorting efficiency.\n\nCompetition of the forces in the sorting environment need fine tuning to succeed in high efficient optical sorting. The need is mainly with regards to the balance of the forces; drag force due to fluid flow and optical gradient force due to arrangement of intensity spot.\n\nScientists at the University of St. Andrews have received considerable funding from the UK Engineering and Physical Sciences Research Council (EPSRC) for an optical sorting machine. This new technology could rival the conventional fluorescence-activated cell sorting.\n\nAn evanescent field is a residue optical field that \"leaks\" during total internal reflection. This \"leaking\" of light fades off at an exponential rate. The evanescent field has found a number of applications in nanometer resolution imaging (microscopy); optical micromanipulation (optical tweezers) are becoming ever more relevant in research.\n\nIn optical tweezers, a continuous evanescent field can be created when light is propagating through an optical waveguide (multiple total internal reflection). The resulting evanescent field has a directional sense and will propel microparticles along its propagating path. This work was first pioneered by S. Kawata and T. Sugiura, in 1992, who showed that the field can be coupled to the particles in proximity on the order of 100 nanometers.\n\nThis direct coupling of the field is treated as a type of photon tunnelling across the gap from prism to microparticles. The result is a directional optical propelling force.\n\nA recent updated version of the evanescent field optical tweezers makes use of extended optical landscape patterns to simultaneously guide a large number of particles into a preferred direction without using a waveguide. It is termed as Lensless Optical Trapping (“LOT”). The orderly movement of the particles is aided by the introduction of Ronchi Ruling that creates well-defined optical potential wells (replacing the waveguide). This means that particles are propelled by the evanescent field while being trapped by the linear bright fringes. At the moment, there are scientists working on focused evanescent fields as well.\n\nAnother approach that has been recently proposed makes use of surface plasmons, which is an enhanced evanescent wave localized at a metal/dielectric interface. The enhanced force field experienced by colloidal particles exposed to surface plasmons \nat a flat metal/dielectric interface has been for the first time measured using a photonic force microscope, the total force magnitude being found 40 times stronger compared to a normal evanescent wave. By patterning the surface with gold microscopic islands it is possible to have selective and parallel trapping in these islands. The forces of the latter optical tweezers lie in the femtonewton range.\n\nThe evanescent field can also be used to trap cold atoms and molecules near the surface of an optical waveguide or optical nanofiber.\n\nMing Wu, a UC Berkeley Professor of electrical engineering and computer sciences invented the new optoelectronic tweezers.\n\nWu transformed the optical energy from low powered light emitting diodes (LED) into electrical energy via a photoconductive surface. The idea is to allow the LED to switch on and off the photoconductive material via its fine projection. As the optical pattern can be easily transformable through optical projection, this method allows a high flexibility of switching different optical landscapes.\n\nThe manipulation/tweezing process is done by the variations between the electric field actuated by the light pattern. The particles will be either attracted or repelled from the actuated point due to the its induced electrical dipole. Particles suspended in a liquid will be susceptible to the electrical field gradient, this is known as dielectrophoresis.\n\nOne clear advantage is that the electrical conductivity is different between different kinds of cells. Living cells have a lower conductive medium while the dead ones have minimum or no conductive medium. The system may be able to manipulate roughly 10,000 cells or particles at the same time.\n\nSee comments by Professor Kishan Dholakia on this new technique, K. Dholakia, \"Nature Materials\" 4, 579–580 (01 Aug 2005) News and Views.\n\n\"The system was able to move live E. coli bacteria and 20-micrometre-wide particles, using an optical power output of less than 10 microwatts. This is one-hundred-thousandth of the power needed for [direct] optical tweezers\".\n\nWhen a cluster of microparticles are trapped within a monochromatic laser beam, the organization of the microparticles within the optical trapping is heavily dependent on the redistributing of the optical trapping forces amongst the microparticles. This redistribution of light forces amongst the cluster of microparticles provides a new force equilibrium on the cluster as a whole. As such we can say that the cluster of microparticles are somewhat bound together by light. One of the first experimental evidence of optical binding was reported by Michael M. Burns, Jean-Marc Fournier, and Jene A. Golovchenko, though it was originally predicted by T. Thirunamachandran. One of the many recent studies on optical binding has shown that for a system of chiral nanoparticles, the magnitude of the binding forces are dependent on the polarisation of the laser beam and the handedness of interacting particles themselves, with potential applications in areas such as enantiomeric separation and optical nanomanipulation.\n\nIn order to simultaneously manipulate and image samples that exhibit fluorescence, optical tweezers can be built alongside a fluorescence microscope. Such instruments are particularly useful when it comes to studying single or small numbers of biological molecules that have been fluorescently labelled, or in applications in which fluorescence is used to track and visualize objects that are to be trapped.\n", "id": "299901", "title": "Optical tweezers"}
{"url": "https://en.wikipedia.org/wiki?curid=2669197", "text": "Pulsed-field gel electrophoresis\n\nPulsed field gel electrophoresis is a technique used for the separation of large deoxyribonucleic acid (DNA) molecules by applying to a gel matrix an electric field that periodically changes direction.\n\nStandard gel electrophoresis techniques for separation of DNA molecules provided huge advantages for molecular biology research. However, it was unable to separate very large molecules of DNA effectively. DNA molecules larger than 15–20 kb migrating through a gel will essentially move together in a size-independent manner. At Columbia University in 1984, David C. Schwartz and Charles Cantor developed a variation on the standard protocol by introducing an alternating voltage gradient to improve the resolution of larger molecules.\nThis technique became known as pulsed-field gel electrophoresis (PFGE). The development of PFGE expanded the range of resolution for DNA fragments by as much as two orders of magnitude.\n\nThe procedure for this technique is relatively similar to performing a standard gel electrophoresis except that instead of constantly running the voltage in one direction, the voltage is periodically switched among three directions; one that runs through the central axis of the gel and two that run at an angle of 60 degrees either side. The pulse times are equal for each direction resulting in a net forward migration of the DNA. For extremely large molecules (up to around 2 Mb), switching-interval ramps can be used that increases the pulse time for each direction over the course of a number of hours—take, for instance, increasing the pulse linearly from 10 seconds at 0 hours to 60 seconds at 18 hours.\n\nThis procedure takes longer than normal gel electrophoresis due to the size of the fragments being resolved and the fact that the DNA does not move in a straight line through the gel.\n\nWhile in general small fragments can find their way through the gel matrix more easily than large DNA fragments, a threshold length exists above 30–50 kb where all large fragments will run at the same rate, and appear in a gel as a single large diffuse band.\n\nHowever, with periodic changing of field direction, the various lengths of DNA react to the change at differing rates. That is, larger pieces of DNA will be slower to realign their charge when field direction is changed, while smaller pieces will be quicker. Over the course of time with the consistent changing of directions, each band will begin to separate more and more even at very large lengths. Thus separation of very large DNA pieces using PFGE is made possible.\n\nPFGE may be used for genotyping or genetic fingerprinting. It is commonly considered a gold standard in epidemiological studies of pathogenic organisms. Subtyping has made it easier to discriminate among strains of \"Listeria monocytogenes\" and thus to link environmental or food isolates with clinical infections.\n\n", "id": "2669197", "title": "Pulsed-field gel electrophoresis"}
{"url": "https://en.wikipedia.org/wiki?curid=14572649", "text": "Double Helix (novel)\n\nDouble Helix (2004), a novel by Nancy Werlin, is about 18-year-old Eli Samuels, who works for a famous molecular biologist named Dr. Quincy Wyatt. There is a mysterious connection between Dr. Wyatt and Eli’s parents, and all Eli knows about the connection is that it has something to do with his mother, who has Huntington's disease. Because of the connection between Dr. Wyatt and the Samuels family, Eli's father is strongly against Eli working there. The job is perfect, and the wages are great, but Eli can't help but notice that Dr. Wyatt seems to be a little \"too\" interested in him. Later on, as Eli continues to work in the lab, he discovers with the help of Kayla Matheson, Dr. Wyatt's supposed \"niece,\" that he and Kayla are the product of a highly unethical eugenics experiment.\n\nEli Samuels: Eli is 18 years old, 6 feet and 7 inches (201 centimeters) tall. He´s an A student, and is the salutatorian in High School. His mother has Huntington's disease, which he could have too.\n\nJonathan Samuels: Jonathan Samuels is Eli's father, who loved his wife. He has a problem with Eli working at Wyatt Transgenic's, because of things that went down in the past. He is initially completely unwilling to tell Eli anything about his mother's relationships with Dr. Wyatt.\n\nDr. Quincy Wyatt: Dr. Quincy Wyatt is a famed geneticist, who is considered to be on par with Mendel, Watson, and Crick. He offers Eli a job. He displays an unexplained interest in Eli.\n\nVivian Fadiman: Vivian Fadiman is Eli's girlfriend and Valedictorian at his high school. All she wants is to be part of his life and she supports him in everything he does. It's hard for her to understand why Eli hides major parts of his life from her. Eli is devoted to her, though they do go through some rough times.\n\nKayla Matheson: A year older than Eli, he gets to know her via Dr. Wyatt. He is attracted to her because of her beauty and athleticism.\n\nAva Samuels: Ava Samuels was Eli's mother and lived in a nursing home because of her Huntington's disease. She had a mysterious connection with Dr. Wyatt.\n\nThe story addresses a lot of recent scientific breakthroughs, and uses them as plot devices. For instance, The Human Genome Project, which documents all the genes and DNA in the human genetic makeup, is talked about when Dr. Wyatt explains his genetic testing to Eli for the first time. Scientific American recently published an article detailing the possible uses for the information gathered from the Human Genome Project. Modern advancements in gene studies currently can detect and, in some cases, even predict the presence of a genetic abnormality. In Double Helix, this ability to detect flaws before birth was used to genetically engineer a “more perfect son, devoid of flaws and with a proper chance to live free of Huntington’s.”\n\nFor some people, the biggest issue with genetic engineering is whether or not to seek out nd act on knowledge about genetic flaws. Double Helix attempts to explore the life saving and life destroying aspects of genetic engineering. The book proposes that, while Eli’s life was saved by avoiding the Huntington’s disease gene, his concept of life and self were destroyed when he found out he was genetically engineered to be a certain way.\n", "id": "14572649", "title": "Double Helix (novel)"}
{"url": "https://en.wikipedia.org/wiki?curid=8643269", "text": "DPVweb\n\nDPVweb is a database for virologists working on plant viruses combining taxonomic, bioinformatic and symptom data.\n\nDPVweb is a central web-based source of information about viruses, viroids and satellites of plants, fungi and protozoa.\n\nIt provides comprehensive taxonomic information, including brief descriptions of each family and genus, and classified lists of virus sequences. It makes use of a large database that also holds detailed, curated, information for all sequences of viruses, viroids and satellites of plants, fungi and protozoa that are complete or that contain at least one complete gene. There are currently about 10,000 such sequences. For comparative purposes, DPVweb also contains a representative sequence of all other fully sequenced virus species with an RNA or single-stranded DNA genome. For each curated sequence the database contains the start and end positions of each feature (gene, non-translated region, etc.), and these have been checked for accuracy. As far as possible, the nomenclature for genes and proteins are standardized within genera and families. Sequences of features (either as DNA or amino acid sequences) can be directly downloaded from the website in FASTA format.\n\nThe sequence information can also be accessed via client software for personal computers.\n\nThe Descriptions of Plant Viruses (DPVs) were first published by the Association of Applied Biologists in 1970 as a series of leaflets, each one written by an expert describing a particular plant virus. In 1998 all of the 354 DPVs published in paper were scanned, and converted into an electronic format in a database and distributed on CDROM. In 2001 the descriptions were made available on the new DPVweb site, providing open access to the now 400+ DPVs (currently 415) as well as taxonomic and sequence data on all plant viruses.\n\nDPVweb is an aid to researchers in the field of plant virology as well as an educational resource for students of virology and molecular biology.\n\nThe site provides a single point of access for all known plant virus genome sequences making it easy to collect these sequences together for further analysis and comparison. Sequence data from the DPVweb database have proved valuable for a number of projects:\n\n\n", "id": "8643269", "title": "DPVweb"}
{"url": "https://en.wikipedia.org/wiki?curid=11882145", "text": "Hfq protein\n\nThe Hfq protein (also known as HF-I protein) encoded by the hfq gene was discovered in 1968 as an \"Escherichia coli\" host factor that was essential for replication of the bacteriophage Qβ. It is now clear that Hfq is an abundant bacterial RNA binding protein which has many important physiological roles that are usually mediated by interacting with Hfq binding sRNA.\n\nIn \"E. coli\", Hfq mutants show multiple stress response related phenotypes. The Hfq protein is now known to regulate the translation of two major stress transcription factors ( σS (RpoS) and σE (RpoE) ) in Enterobacteria. It also regulates sRNA in \"Vibrio cholerae\", a specific example being MicX sRNA.\nIn \"Salmonella typhimurium\", Hfq has been shown to be an essential virulence factor as its deletion attenuates the ability of \"S.typhimurium\" to invade epithelial cells, secrete virulence factors or survive in cultured macrophages. In \"Salmonella\", Hfq deletion mutants are also non motile and exhibit chronic activation of the sigma mediated envelope stress response. A CLIP-Seq study of Hfq in \"Salmonella\" has revealed 640 binding sites across the \"Salmonella\" transcriptome. The majority of these binding sites was found in mRNAs and sRNAs.\nIn \"Photorhabdus luminescens\", a deletion of the \"hfq\" gene causes loss of secondary metabolite production.\n\nHfq mediates its pleiotropic effects through several mechanisms. It interacts with regulatory sRNA and facilitates their antisense interaction with their targets. It also acts independently to modulate mRNA decay (directing mRNA transcripts for degradation) and also acts as a repressor of mRNA translation. Genomic SELEX has been used to show that Hfq binding RNAs are enriched in the sequence motif 5'-AAYAAYAA-3'.\n\nElectron microscopy imaging reveals that, in addition to the expected localization of this protein in cytoplasmic regions and in the nucleoid, an important fraction of Hfq is located in close proximity to the membrane.\n\nSix crystallographic structures of 4 different Hfq proteins have been published so far; \"E. coli\" Hfq (), \"P. aeruginosa\" Hfq in a low salt condition () and a high salt condition (), Hfq from \"S. aureus\" with bound RNA () and without (), and the Hfq(-like) protein from \"M. jannaschii\" ().\n\nAll six structures confirm the hexameric ring-shape of a Hfq protein complex.\n\n\n 11. Mol Cell. 2002 Jan;9(1):23-30. Hfq: a bacterial Sm-like protein that mediates RNA-RNA interaction.Møller T1, Franch T, Højrup P, Keene DR, Bächinger HP, Brennan RG, Valentin-Hansen P.\n", "id": "11882145", "title": "Hfq protein"}
{"url": "https://en.wikipedia.org/wiki?curid=7904662", "text": "PDE1\n\nPDE1 (phosphodiesterase type 1) is a phosphodiesterase enzyme also known as calcium- and calmodulin-dependent phosphodiesterase. It is one of the 11 families of phosphodiesterase (PDE1-PDE11). PDE1 has three subtypes, PDE1A, PDE1B and PDE1C which divide further into various isoforms. The various isoforms exhibit different affinities for cAMP and cGMP.\n\nThe existence of the Ca-stimulated PDE1 was first demonstrated by Cheung (1970), Kakiuchi and Yamazaki (1970) as a result of their research on bovine brain and rat brain respectively. It has since been found to be widely distributed in various mammalian tissues as well as in other eukaryotes. It is now one of the most intensively studied member of the PDE superfamily of enzymes, which today represents 11 gene families, and the best characterized one as well.\n\nFurther researches in the field along with increased availability of monoclonal antibodies have shown that various PDE1 isozymes exist and have been identified and purified. It is now known that PDE1 exists as tissue specific isozymes.\n\nThe PDE1 isozyme family belongs to a Class I enzymes, which includes all vertebrate PDEs and some yeast enzymes. Class I enzymes all have a catalytic core of at least 250 amino acids whereas Class II enzymes lack such a common feature.\n\nUsually vertebrate PDEs are dimers of linear 50–150 kDa proteins. They consist of three functional domains; a conserved catalytic core, a regulatory N-terminus and a C-terminus [3-5]. The proteins are chimeric and each domain is associated with their particular function.\n\nThe regulatory N-terminus is substantially different in various PDE types. They are flanked by the catalytic core and include regions that auto-inhibit the catalytic domains. They also target sequences that control subcellular localization. In PDE1 this region contains a calmodulin binding domain.\n\nThe catalytic domains of PDE1 (and other types of PDEs) have three helical subdomains: an N-terminal cyclin-fold region, a linker region and a C-terminal helical bundle. A deep hydrophobic pocket is formed at the interface of these subdomains. It is composed of four subsites. They are: a metal binding site (M site), core pocket (Q pocket), hydrophobic pocket (H pocket) and lid region (L region). The M site is placed at the bottom of the hydrophobic pocket with several metal atoms. The metal atoms bind to residues that are completely conserved in all PDE family members. The identity of the metal atoms is not known with absolute certainty. However, some evidence indicate that at least one of the metals is zinc and the other is likely to be magnesium. The zinc coordination sphere is composed of three histidines, one aspartate and two water molecules. The magnesium coordination sphere involves the same aspartate along with five water molecules, one of which is shared with the zinc molecule. The reputed role of the metal ions include structure stabilization as well as activation of hydroxide to mediate catalysis.\n\nThe domains are separated by ”hinge” regions where they can be experimentally separated by limited proteolysis.\n\nThe PDE1 isozyme family (along with the PDE4 family) is the most diverse one and includes numerous splice variant PDE1 isoforms. It has three subtypes, PDE1A, PDE1B and PDE1C which divide further into various isoforms.\n\nThe localization of PDE1 isoforms in different tissues/cells and their location within the cells is as follows:\n\"Table 1. Various PDE1s location in tissues and within cells.\"\n\nMost PDE1 isoforms are reported to be cytosolic. However, there are instances of PDE1s being localized to subcellular regions but little is known about the molecular mechanisms responsible for such localization. It is thought to be likely that the unique N-terminal or C-terminal regions of the various isoforms allow the different proteins to be targeted to specific subcellular domains.\n\nIntracellular second messengers such as cGMP and cAMP undergo rapid changes in concentration in a response to a wide variety of cell specific stimuli. The concentration of these second messengers is determined to a large extent by the relative synthetic activity of adenylate cyclase and degrative activity of cyclic nucleotide PDE. The role of PDE1 enzymes is to degrade both cGMP and cAMP.\n\nThe various isoforms exhibit different affinities for cAMP and cGMP. PDE1A and PDE1B preferentially hydrolyse cGMP, whereas PDE1C degrades both cAMP and cGMP with high affinity. For example, in airway smooth muscles of humans and other species, generic PDE1 accounts for more than 50% of the hydrolytic activity of cyclic nucleotides. It has been demonstrated that deletion and overexpression of PDE1 produces strong effects on agonist-induced cAMP signalling but has little effect on the basal cAMP level.\n\nBecause of in vitro regulation by Ca/calmodulin, PDE1s are believed to function as a mechanism for integrating cell signalling pathways mediated by cGMP and cAMP with pathways that regulate intracellular calcium levels. The precise function of PDE1 isozymes in various pathophysiological processes is not clear because most of the studies have been carried out in vitro. Therefore, it is essential to direct further research to in vivo studies.\n\nPDE1 has been implicated to play a role in a number of physiological and pathological processes:\n\n\nThe distinguishing feature of PDE1 as a family is their regulation by calcium (Ca) and calmodulin (CaM). Calmodulin has been shown to activate cyclic nucleotide PDE in a calcium-dependent manner and the cooperative binding of four Ca to calmodulin is required to fully activate PDE1 [2]. The binding of one Ca/CaM complex per monomer to binding sites near the N-terminus stimulates hydrolysis of cyclic nucleotides. In intact cells, PDE1 is almost exclusively activated by Ca entering the cell from the extracellular space. The regulation of PDE1 by Ca and CaM has been studied in vitro and these studies have shown that eight methionine residues within the hydrophobic clefts of Ca-CaM are required for the binding and activation of PDE1. Mutations in the N-terminal lobe of CaM affect its ability to activate PDE1 so it is believed that the C-terminal lobe of CaM serves to target CaM to PDE1, while the N-terminal lobe activates the enzyme. The presence of an aromatic residue, usually a tryptophan, in the CaM-binding region of Ca-CaM-regulated proteins may also be required for binding to PDE1.\n\nBetween different PDE1 isozymes there is a significant difference in affinity for Ca/CaM. In general, the PDE1 enzymes have high affinity for the complex but the affinity can be affected by phosphorylation. Phosphorylation of PDE1A1 and PDE1A2 by protein kinase A and of PDE1B1 by CaM Kinase II decreases their sensitivity to calmodulin activation. This phosphorylation can be reversed by the phosphatase, calcineurin. The phosphorylation of the isozymes is accompanied by a decrease in the isozymes affinity towards CaM, as well as an increase in the Ca concentrations required for CaM activation of the isozymes.\n\nPDEs have been pursued as therapeutic targets because of the basic pharmacological principle that regulation of degradation of any ligand or second messenger can often make a more rapid and larger percentage change in concentration than comparable rates of synthesis. Another reason is that PDEs do not have to compete with very high levels of endogenous substrate to be effective since the levels of cAMP and cGMP in most cells are typically in the micromolar range.\n\nThe availability of high-resolution crystal structures of the catalytic domains of PDEs makes the development of highly potent and specific inhibitors possible.\n\nMany compounds reported as PDE1 inhibitors do not interact directly with the catalytic site of PDE1 but interact during activation, either at the level of calmodulin binding sites such as compound KS505a or directly on Ca/calmodulin such as bepril, flunarizine and amiodarone.\n\nThose inhibitors that interact with the catalytic site occupy part of the active site, primarily around the Q pocket and occasionally close to the M pocket. A major point of interaction is a conserved hydrophobic pocket that is involved in orienting the substrate purine ring for interaction with a glutamine residue that is crucial for the catalytic mechanism of the PDEs.\n\nThe interactions of inhibitors can be split into three major types: interactions with the metal ions mediated through water, H-bond interactions with the protein residues involved in nucleotide recognition and most importantly the interaction with the hydrophobic residues lining the cavity of the active site. All known inhibitors seem to exploit these three types of interactions and hence these interactions should guide the design of new types of inhibitors.\n\nInitially PDE1 inhibitors were claimed to be effective vascular relaxants. With availability of purified cloned enzymes, however, it is now known that such inhibitors are in fact equally active against PDE5. Those inhibitors include e.g. zaprinast, 8-methoxymethyl IPMX and SCH 51866.\n\nAll therapeutically effective PDE inhibitors must be incorporated into the cell because all PDEs are localized in the cytoplasm and/or on intracellular membranes.\n\nToday, there is no real and effective specific PDE1 inhibitor that can be used to assess the functional role of PDE1 in tissues.\n\nNimodipine is a dihydropyridine that antagonizes/blocks specifically L-type Ca-channel, and was first described as a PDE1 inhibitor. This effect is not related to its calcium antagonist property since it inhibits, in micromolar range, basal and calmodulin stimulated purified PDE1. Since nimodipine at lower concentrations blocks the L-type calcium channel, it can only be used to estimate PDE1 participation in tissue and cell homogenates.\n\nVinpocetine was described as a specific inhibitor of basal and calmodulin-activated PDE1. This effect leads to an increase of cAMP over cGMP. It is mainly used as a pharmacological tool to implicate PDE1. Vinpocetine inhibits differently the various subtypes of PDE1 (IC from 8 to 50 µm) and it is also able to inhibit PDE7B. It can not be used as a specific tool to investigate the functional role of PDE1 due to its direct activator effects on BK (Ca) channels. Vinpocetine crosses the blood–brain barrier and is taken up by cerebral tissue. It has been hypothesized that vinpocetine can affect voltage-dependent calcium channels.\n\nIC224 inhibits PDE1 (IC = 0.08 µM) with a selective ratio of 127 (ratio of IC value for the next most sensitive PDE and for IC value for PDE1). It was developed by ICOS corporation. If IC224 similarly inhibits basal and calmodulin-activated PDE1 subtypes, this compound could be very helpful to characterize PDE1 activity and to clearly investigate the various roles of PDE1 in pathophysiology.\n\nNearly all the phosphodiesterases are expressed in the CNS, making this gene family an attractive source of new targets for the treatment of psychiatric and neurodegenerative disorders.\n\nPDE1A2 has a potential role in neurodegenerative diseases, including:\n\nPDE1C could have a role in the regulation of insulin release and may target proliferating smooth muscle cells in atherosclerotic lesions or during restenosis.\n", "id": "7904662", "title": "PDE1"}
{"url": "https://en.wikipedia.org/wiki?curid=1720640", "text": "Molecular ecology\n\nMolecular ecology is a field of evolutionary biology that is concerned with applying molecular population genetics, molecular phylogenetics, and more recently genomics to traditional ecological questions (e.g., species diagnosis, conservation and assessment of biodiversity, species-area relationships, and many questions in behavioral ecology). It is virtually synonymous with the field of \"Ecological Genetics\" as pioneered by Theodosius Dobzhansky, E. B. Ford, Godfrey M. Hewitt and others. These fields are united in their attempt to study genetic-based questions \"out in the field\" as opposed to the laboratory. Molecular ecology is related to the field of Conservation genetics.\n\nMethods frequently include using microsatellites to determine gene flow and hybridization between populations. The development of molecular ecology is also closely related to the use of DNA microarrays, which allows for the simultaneous analysis of the expression of thousands of different genes. Quantitative PCR may also be used to analyze gene expression as a result of changes in environmental conditions or different response by differently adapted individuals.\n\nMolecular ecological techniques have recently been used to study \"in situ\" questions of bacterial diversity. This stems from the fact that many microorganisms are not easily obtainable as cultured strains in the laboratory, which would allow for identification and characterisation. It also stems from the development of PCR technique, which allows for rapid amplification of genetic material.\n\nThe amplification of DNA from environmental samples using general of group-specific primers leads to a mix of genetic material that has to be sorted out before sequencing and identification. The classic technique to achieve this is through cloning, which involves incorporating the amplified DNA fragments into bacterial plasmids. Techniques such as temperature gradient gel electrophoresis, allow for a faster result. More recently, the advent of relatively low-cost, next-generation DNA sequencing technologies, such as 454 and Illumina platforms, has allowed exploration of bacterial ecology in relation to continental-scale environmental gradients such as pH that was not feasible with traditional technology.\n\nExploration of fungal diversity \"in situ\" has also benefited from next-generation DNA sequencing technologies. The use of high-throughput sequencing techniques has been widely adopted by the fungal ecology community since the first publication of their use in the field in 2009. Similar to exploration of bacterial diversity, these techniques have allowed high-resolution studies of fundamental questions in fungal ecology such as phylogeography, fungal diversity in forest soils, stratification of fungal communities in soil horizons, and fungal succession on decomposing plant litter.\n\nThe majority of fungal ecology research leveraging next-generation sequencing approaches involves sequencing of PCR amplicons of conserved regions of DNA (i.e. marker genes) to identify and describe the distribution of taxonomic groups in the fungal community in question, though more recent research has focused on sequencing functional gene amplicons (e.g. Baldrian et al. 2012). The locus of choice for description of the taxonomic structure of fungal communities has traditionally been the internal transcribed spacer (ITS) region of ribosomal RNA genes due to its utility in identifying fungi to genus or species taxonomic levels, and its high representation in public sequence databases. A second widely used locus (e.g. Amend et al. 2010, Weber et al. 2013), the D1-D3 region of 28S ribosomal RNA genes, may not allow the low taxonomic level classification of the ITS, but demonstrates superior performance in sequence alignment and phylogenetics. In addition, the D1-D3 region may be a better candidate for sequencing with Illumina sequencing technologies. Porras-Alfaro et al. showed that the accuracy of classification of either ITS or D1-D3 region sequences was largely based on the sequence composition and quality of databases used for comparison, and poor-quality sequences and sequence misidentification in public databases is a major concern. The construction of sequence databases that have broad representation across fungi, and that are curated by taxonomic experts is a critical next step.\n\nNext-generation sequencing technologies generate large amounts of data, and analysis of fungal marker-gene data is an active area of research. Two primary areas of concern are methods for clustering sequences into operational taxonomic units by sequence similarity, and quality control of sequence data. Currently there is no consensus on preferred methods for clustering, and clustering and sequence processing methods can have a significant impact on results, especially for the variable-length ITS region. In addition, fungal species vary in intra-specific sequence similarity of the ITS region. Recent research has been devoted to development of flexible clustering protocols that allow sequence similarity thresholds to vary by taxonomic groups, which are supported by well-annotated sequences in public sequence databases.\n\nIn recent years, molecular data and analyses have been able to supplement traditional approaches of behavioral ecology, the study of animal behavior in relation to its ecology and evolutionary history. One behavior that molecular data has helped scientists better understand is extra-pair fertilizations (EPFs), also known as extra-pair copulations (EPCs). These are mating events that occur outside of a social bond, like monogamy and are hard to observe. Molecular data has been key to understanding the prevalence of and the individuals participating in EPFs.\n\nWhile most bird species are socially monogamous, molecular data has revealed that less than 25% of these species are genetically monogamous. EPFs complicate matters, especially for male individuals, because it does not make sense for an individual to care for offspring that are not their own. Studies have found that males will adjust their parental care in response to changes in their paternity. Other studies have shown that in socially monogamous species, some individuals will employ an alternative strategy to be reproductively successful since a social bond does not always equal reproductive success.\n\nIt appears that EPFs in some species is driven by the good genes hypothesis (295). In red-back shrikes (\"Lanius collurio\") extra-pair males had significantly longer tarsi than within-pair males, and all of the extra-pair offspring were males, supporting the prediction that females will bias their clutch towards males when they mate with an \"attractive\" male. In house wrens (\"Troglodytes aedon)\", extra-pair offspring were also found to be male-biased compared to within-offspring.\n\nWithout molecular ecology, identifying individuals that participate in EPFs and the offspring that result from EPFs would be impossible.\n\nIsolation by distance (IBD), like reproductive isolation, is the effect of physical barriers to populations that limit migration and lower gene flow. The shorter the distance between populations the more likely individuals are to disperse and mate and thus, increase gene flow. The use of molecular data, specifically allele frequencies of individuals among populations in relation to their geographic distance help to explain concepts such as, sex-biased dispersal, speciation, and landscape genetics.\n\nThe Mantel test is an assessment that compares genetic distance with geographic distance and is most appropriate because it doesn't assume that the comparisons are independent of each other. There are three main factors that influence the chances of finding a correlation of IBD, which include sample size, metabolism, and taxa. For example, based on the meta-analysis, ectotherms are more likely than endotherms to display greater IBD.\n\nMetapopulation theory dictates that a metapopulation consists of spatially distinct populations that interact with one another on some level and move through a cycle of extinctions and recolonizations (i.e. through dispersal). The most common metapopulation model is the extinction-recolonization model which explains systems in which spatially distinct populations undergo stochastic changes in population sizes which may lead to extinction at the population level. Once this has occurred, dispersing individuals from other populations will immigrate and \"rescue\" the population at that site. Other metapopulation models include the source-sink model (island-mainland model) where one (or multiple) large central population(s) produces disperses to smaller satellite populations that have a population growth rate of less than one and could not persist without the influx from the main population.\n\nMetapopulation structure and the repeated extinctions and recolonizations can significantly affect a population's genetic structure. Recolonization by a few dispersers leads to population bottlenecks which will reduce the effective population size (Ne), accelerate genetic drift, and deplete genetic variation. However, dispersal between populations in the metapopulation can reverse or halt these processes over the long term. Therefore, in order for individual sub-populations to remain healthy, they must either have a large population size or have a relatively high rate of dispersal with other subpopulations. Molecular ecology focuses on using tests to determine the rates of dispersal between populations and can use molecular clocks to determine when historic bottlenecks occurred. As habitat becomes more fragmented, dispersal between populations will become increasingly rare. Therefore, subpopulations that may have historically been preserved by a metapopulation structure may start to decline. Using mitochondrial or nuclear markers to monitor dispersal coupled with population Fst values and allelic richness can provide insight into how well a population is performing and how it will perform into the future.\n\nThe molecular clock hypothesis states that DNA sequences roughly evolve at the same rate and because of this the dissimilarity between two sequences can be used to tell how long ago they diverged from one another. The first step in of using a molecular clock is it must be calibrated based on the approximate time the two lineages being study diverged. The sources usually used to calibrate the molecular clocks are fossils or known geological events in the past. After calibrating the clock the next step is to calculate divergence time by dividing the estimated time since the sequences diverged by the amount of sequence divergence. The resulting number is the estimated rate at which molecular evolution is occurring. The most widely cited molecular clock is a ‘universal’ mtDNA clock of approximately 2 per cent sequence divergence every million years. Although it is referred to as a universal clock, this idea of \"universal\" clock is not really possible considering rates of evolution differ within DNA regions. Another drawback to using molecular clocks is that they ideally need to be calibrated from an independent source of data other than the molecular data. This poses a big problem for taxa that don’t fossilize or preserve well because its almost impossible to calibrate their molecular clocks. Despite these inconveniences, the molecular clock hypothesis is still used today, and has been successful in dating events happening as long ago as 65 million years such as the emergence of ancestral mammals.\n\nThe concept of mate choice explains how organisms select their mates based on two main methods; The Good Genes Hypothesis and Genetic Compatibility. \nThe Good Genes Hypothesis, also referred to as the sexy son hypothesis, suggests that the females will choose a male that produce an offspring that will have increased fitness advantages and genetic viability. Therefore, the mates that are more 'attractive\" are more likely to be chosen for mating and pass on their genes to the next generation. In species which exhibit polyandry the females will search out for the most suitable males and re-mate until they have found the best sperm to fertilize their eggs.\nGenetic compatibility is where mates are choosing their partner based on the compatibility of their genotypes. The mate which is doing the selecting must know their own genotype as well as the genotypes of potential mates in order to select the appropriate partner. Genetic compatibility in most instances is limited to specific traits, such as the major histocompatibility complex in mammals, because of complex genetic interactions. This behavior is potentially seen in humans. A study looking at women's choice in men based on body odors concluded that the scent of the odors were influenced by the MHC and that they impact for mate choice in human populations.\n\nSex-biased dispersal, or the tendency of one sex to disperse between populations more frequently than the other, is a common behavior studied by researchers. Three major hypotheses currently exist to help explain sex-biased dispersal. The resource-competition hypothesis infers that the more philopatric sex (the sex more likely to remain at its natal grounds) benefits during reproduction simply by having familiarity with natal ground resources. A second proposal for sex-biased dispersal is the local mate competition hypothesis, which introduces the idea that individuals encounter less mate competition with relatives the farther from their natal grounds they disperse. And the inbreeding avoidance hypothesis suggests individuals disperse to decrease inbreeding.\n\nStudying these hypotheses can be arduous since it is nearly impossible to keep track of every individual and their whereabouts within and between populations. To combat this time-consuming method, scientists have recruited several molecular ecology techniques in order to study sex-biased dispersal. One method is the comparison of differences between nuclear and mitochondrial markers among populations. Markers showing higher levels of differentiation indicate the more philopatric sex; that is, the more a sex remains at natal grounds, the more their markers will take on a unique I.D, due to lack of gene flow with respect to that marker. Researchers can also quantify male-male and female-female pair relatedness within populations to understand which sex is more likely to disperse. Pairs with values consistently lower in one sex indicate the dispersing sex. This is because there is more gene flow in the dispersing sex and their markers are less similar than individuals of the same sex in the same population, which produces a low relatedness value. FST values are also used to understand dispersing behaviors by calculating an FST value for each sex. The sex that disperses more displays a lower FST value, which measures levels of inbreeding between the subpopulation and the total population. Additionally, assignment tests can be utilized to quantify the number of individuals of a certain sex dispersing to other populations. A more mathematical approach to quantifying sex-biased dispersal on the molecular level is the use of spatial autocorrelation. This correlation analyzes the relationship between geographic distance and spatial distance. A correlation coefficient, or r value, is calculated and the plot of r against distance provides an indication of individuals more related to or less related to one another than expected.\n\nA quantitative trait locus (QTL) refers to a suite of genes that controls a quantitative trait. A quantitative trait is one that is influenced by several different genes as opposed to just one or two. QTLs are analyzed using Qst. Qst looks at the relatedness of the traits in focus. In the case of QTLs, clines are analyzed by Qst. A cline (biology) is a change in allele frequency across a geographical distance. This change in allele frequency causes a series of intermediate varying phenotypes that when associated with certain environmental conditions can indicate selection. This selection causes local adaptation, but high gene flow is still expected to be present along the cline.\n\nFor example, barn owls in Europe exhibit a cline in reference to their plumage coloration. Their feathers range in coloration from white to reddish-brown across the geological range of the southwest to the northeast. This study sought to find if this phenotypic variation was due to selection by calculating the Qst values across the owl populations. Because high gene flow was still anticipated along this cline, selection was only expected to act upon the QTLs that incur locally adaptive phenotypic traits. This can be determined by comparing the Qst values to Fst (fixation index) values. If both of these values are similar and Fst is based on neutral markers then it can be assumed that the QTLs were based on neutral markers (markers not under selection or locally adapted) as well. However, in the case of the barn owls the Qst value was much higher than the Fst value. This means that high gene flow was present allowing the neutral markers to be similar, indicated by the low Fst value. But, local adaptation due to selection was present as well, in the form of varying plumage coloration since the Qst value was high, indicating differences in these non-neutral loci. In other words, this cline of plumage coloration has some sort of adaptive value to the birds.\n\nFixation indices are used when determining the level of genetic differentiation between sub-populations within a total population. F is the script used to represent this index when using the formula:\nIn this equation, H represents the expected heterozygosity of the total population and H is the expected heterozygosity of a sub-populations. Both measures of heterozygosity are measured at one loci. In the equation, heterozygosity values expected from the total population are compared to observed heterozygosity values of the sub-populations within this total population. Larger F values imply that the level of genetic differentiation between sub-populations within a total population is more significant. The level of differentiation is the result of a balance between gene flow amongst sub-populations (decreasing differentiation) and genetic drift within these sub-populations (increasing differentiation); however, some molecular ecologists note that it cannot be assumed that these factors are at equilibrium. F can also be viewed as a way of comparing the amount of inbreeding within sub-populations to the amount of inbreeding for the total population and is sometimes referred to as an inbreeding coefficient. In these cases, higher F values typically imply higher amounts of inbreeding within the sub-populations. Other factors such as selection pressures may also have an impact on F values.\n\nF values are accompanied by several analog equations (F, G, etc.).These additional measures are interpreted in a similar manner to F values; however, they are adjusted to accompany other factors that F may not, such as accounting for multiple loci.\n\nInbreeding depression is the reduced fitness and survival of offspring from closely related parents. Inbreeding is commonly seen in small populations because of the greater chance of mating with a relative due to limited mate choice. Inbreeding, especially in small populations, is more likely to result in higher rates of genetic drift, which leads to higher rates of homozygosity at all loci in the population and decreased heterozygosity. The rate of inbreeding is based on decreased heterozygosity. In other words, the rate at which heterozygosity is lost from a population due to genetic drift is equal to the rate of accumulating inbreeding in a population. In the absence of migration, inbreeding will accumulate at a rate that is inversely proportional to the size of the population.\n\nThere are two ways in which inbreeding depression can occur. The first of these is through dominance, where beneficial alleles are usually dominant and harmful alleles are usually recessive. The increased homozygosity resulting from inbreeding means that harmful alleles are more likely to be expressed as homozygotes, and the deleterious effects cannot be masked by the beneficial dominant allele. \nThe second method through which inbreeding depression occurs is through overdominance, or heterozygote advantage. Individuals that are heterozygous at a particular locus have a higher fitness than homozygotes at that locus. Inbreeding leads to decreased heterozygosity, and therefore decreased fitness.\n\nDeleterious alleles can be scrubbed by natural selection from inbred populations through genetic purging. As homozygosity increases, less fit individuals will be selected against and thus those harmful alleles will be lost from the population.\n\nOutbreeding depression is the reduced biological fitness in the offspring of distantly related parents. The decline in fitness due to outbreeding is attributed to a breakup of coadapted gene complexes or favorable epistatic relationships. Unlike inbreeding depression, outbreeding depression places emphasis on interactions between loci rather than within them.\n\nRisks of outbreeding depression increase with increased distance between populations. If outbreeding is limited and the population is large enough selective pressure acting on each generation may be able to restore fitness. Selection acts on out bred generations using increased diversity to adapt to the environment. This may result in greater fitness among offspring than the original parental type.\n\nConservation units are classifications often used in conservation biology, conservation genetics, and molecular ecology in order to separate and group different species or populations based on genetic variance and significance for protection. Two of the most common types of conservation units are:\nConservation units are often identified using both neutral and non-neutral genetic markers, with each having its own advantages. Using neutral markers during unit identification can provide unbiased assumptions of genetic drift and time since reproductive isolation within and among species and populations, while using non-neutral markers can provide more accurate estimations of adaptive evolutionary divergence, which can help determine the potential for a conservation unit to adapt within a certain habitat.\n\nBecause of conservation units, populations and species that have high or differing levels of genetic variation are can be distinguished in order to manage each individually, which can ultimately differ based on a number of factors. In one instance, Atlantic salmon located within the Bay of Fundy were given evolutionary significance based on the differences in genetic sequences found among different populations. This detection of evolutionary significance can allow each population of salmon to receive customized conservation and protection based on their adaptive uniqueness in response to geographic location.\n\nPhylogenies are the evolutionary history of an organism, also known as Phylogeography. A Phylogenetic tree is an tree that shows evolutionary relationships between different species based on similarities/differences among genetic or physical traits. Phylogenies embrace aspects of both time (evolutionary relationships) and space (geographic distribution). Typically phylogeny trees include tips, which represent groups of descendent species, and nodes, which represent the common ancestors of those descendants. If two descendants split from the same node, they are called Sister groups. They also may include an outgroup, a species outside of the group of interest. The trees depict clades, which is a group of organisms that include an ancestor and all descendants of that ancestor. The maximum parsimony tree is the simplest tree that has the minimum number of steps possible.\n\nPhylogenies confer important historical processes that shape current distributions of genes and species. When two species become isolated from each other they retain some of the same ancestral alleles also known as allele sharing. Alleles can be shared because of lineage sorting and hybridization. Lineage sorting is driven by genetic drift and must occur before alleles become species specific. Some of these alleles over time will simply be lost, or they may proliferate. Hybridization leads to introgression of alleles from one species to another.\n\nSpecies concepts are the subject of debate in the field of molecular ecology. Since the beginning of taxonomy, scientists have wanted to standardize and perfect the way species are defined. There are many species concepts that dictate how ecologists determine a good species. The most commonly used concept is the biological species concept which defines a species as groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups (Mayr, 1942). This concept is not always useful, particularly when it comes to hybrids. Other species concepts include phylogenetic species concept which describes a species as the smallest identifiable monophyletic group of organisms within which there is a parental pattern of ancestry and descent. This concept defines species on the identifiable. It would also suggest that until two identifiable groups actually produce offspring, they remain separate species. In 1999, John Avise and Glenn Johns suggested a standardized method for defining species based on past speciation and measuring biological classifications as time dependent. Their method used temporal banding to make genus, family and order based on how many tens of millions of years ago the speciation event that resulted in each species took place.\n\nLandscape genetics is a rapidly emerging interdisciplinary field within molecular ecology. Landscape genetics relates genetics to landscape characteristics, such as land-cover use (forests, agriculture, roads, etc.), presence of barriers and corridors, rivers, elevation, etc. Landscape genetics is used to answer how landscape affects dispersal and gene flow.\n\nBarriers are any landscape features that prevents dispersal. Barriers for terrestrial species can include mountains, rivers, roads, and unsuitable terrain, such as agriculture fields. Barriers for aquatic species can include islands or dams. Barriers are species specific; for example a river is a barrier to a field mouse, while a hawk can fly over a river. Corridors are areas over which dispersal is possible. Corridors are stretches of suitable habitat and can also be man-made, such as overpasses over roads and fish ladders on dams.\n\nGeographic data used for landscape genetics can include data collected by radars in planes, land satellite data, marine data collected by NOAA, as well as any other ecological data. In landscape genetics researchers often use different analyses to attempt to determine the best way for a species to travel from point A to point B. Least cost path analysis uses geographic data to determine the most efficient path from one point to another. Circuit scape analysis predicts all the possible paths and the probability of each path's use between point A and point B. These analyses are used to determine the route a dispersing individual is likely to travel.\n\nLandscape genetics is becoming an increasingly important tool in wildlife conservation efforts. It is being used to determine how habitat loss and fragmentation affects the movement of species. It is also used to determine which species need to be managed and whether to manage subpopulations the same or differently according to their gene flow.\n\n\n", "id": "1720640", "title": "Molecular ecology"}
{"url": "https://en.wikipedia.org/wiki?curid=23732455", "text": "Upstream open reading frame\n\nAn Upstream Open Reading Frame (uORF) is an open reading frame (ORF) within the 5' untranslated region (5'UTR) of an mRNA. uORFs can regulate eukaryotic gene expression. Translation of the uORF typically inhibits downstream expression of the primary ORF. In bacteria, uORFs are called leader peptides, and were originally discovered on the basis of their impact on the regulation of genes involved in the synthesis or transport of amino acids.\n\nApproximately 50% of human genes contain uORFs in their 5'UTR, and when present, these cause reductions in protein expression. Human peptides derived from translated uORFs can be detected from cellular material with a mass spectrometer.\n\n", "id": "23732455", "title": "Upstream open reading frame"}
{"url": "https://en.wikipedia.org/wiki?curid=105658", "text": "Pyrophosphate\n\nIn chemistry, a pyrophosphate is a phosphorus oxyanion. Compounds such as salts and esters are also called pyrophosphates. The group is also called diphosphate or dipolyphosphate, although this should not be confused with phosphates. As a food additive, diphosphates are known as E450. A number of hydrogen pyrophosphates also exist, such as NaHPO, as well as the normal pyrophosphates.\n\nPyrophosphates were originally prepared by heating phosphates (\"pyro\" from the Greek, meaning \"fire\"). They generally exhibit the highest solubilities among the phosphates; moreover, they are good complexing agents for metal ions (such as calcium and many transition metals) and have many uses in industrial chemistry. Pyrophosphate is the first member of an entire series of polyphosphates.\n\nThe term pyrophosphate is also the name of esters formed by the condensation of a phosphorylated biological compound with inorganic phosphate, as for dimethylallyl pyrophosphate. This bond is also referred to as a high-energy phosphate bond.\n\nThe synthesis of tetraethyl pyrophosphate was first described in 1854 by Philippe de Clermont at a meeting of the French Academy of Sciences.\n\nPyrophosphates are very important in biochemistry. The anion PO is abbreviated PP and is formed by the hydrolysis of ATP into AMP in cells.\n\nFor example, when a nucleotide is incorporated into a growing DNA or RNA strand by a polymerase, pyrophosphate (PP) is released. Pyrophosphorolysis is the reverse of the polymerization reaction in which pyrophosphate reacts with the 3'-nucleosidemonophosphate (NMP or dNMP), which is removed from the oligonucleotide to release the corresponding triphosphate (dNTP from DNA, or NTP from RNA).\n\nThe pyrophosphate anion has the structure PO, and is an acid anhydride of phosphate. It is unstable in aqueous solution and hydrolyzes into inorganic phosphate:\nor in biologists' shorthand notation:\n\nIn the absence of enzymic catalysis, hydrolysis reactions of simple polyphosphates such as pyrophosphate, linear triphosphate, ADP, and ATP normally proceed extremely slowly in all but highly acidic media.\n\nThis hydrolysis to inorganic phosphate effectively renders the cleavage of ATP to AMP and PP irreversible, and biochemical reactions coupled to this hydrolysis are irreversible as well.\n\nPP occurs in synovial fluid, blood plasma, and urine at levels sufficient to block calcification and may be a natural inhibitor of hydroxyapatite formation in extracellular fluid (ECF). Cells may channel intracellular PP into ECF. ANK is a nonenzymatic plasma-membrane PP channel that supports extracellular PP levels. Defective function of the membrane PP channel ANK is associated with low extracellular PP and elevated intracellular PP. Ectonucleotide pyrophosphatase/phosphodiesterase (ENPP) may function to raise extracellular PP.\n\nFrom the standpoint of high energy phosphate accounting, the hydrolysis of ATP to AMP and PP requires two high-energy phosphates, as to reconstitute AMP into ATP requires two phosphorylation reactions.\n\nThe plasma concentration of inorganic pyrophosphate has a reference range of 0.58-3.78 µM (95% prediction interval).\n\nVarious diphosphates are used as emulsifiers, stabilisers, acidity regulators, raising agents, sequestrants, and water retention agents in food processing. They are classified in the E number scheme under E450:\n\nIn particular, various formulations of diphosphates are used to stabilize whipped cream.\n", "id": "105658", "title": "Pyrophosphate"}
{"url": "https://en.wikipedia.org/wiki?curid=15418088", "text": "Compartment (chemistry)\n\nIn chemistry, a compartment is a part of a protein that serves a specific function.\n\nThere may be multiple compartments on one and the same protein. One example is the case of Pyruvate dehydrogenase complex. This is the enzyme which catalyses Pyruvate decarboxylation, the reaction of Pyruvate with Coenzyme A and the major entry point into the TCA cycle:\nPyruvate dehydrogenase has three chemical compartments; E1 (pyruvate decarboxylase), E2 (dihydrolipoyl transacetylase) and E3 (dihydrolipoyl dehydrogenase). Each one of the compartments has its own specific function.\n", "id": "15418088", "title": "Compartment (chemistry)"}
{"url": "https://en.wikipedia.org/wiki?curid=1475873", "text": "Plant virus\n\nPlant viruses are viruses that affect plants. Like all other viruses, plant viruses are obligate intracellular parasites that do not have the molecular machinery to replicate without a host. Plant viruses are pathogenic to higher plants.\n\nAlthough plant viruses are not nearly as well understood as the animal counterparts, one plant virus has become iconic. The first virus to be discovered (see below) was \"Tobacco mosaic virus\" (TMV). This and other viruses cause an estimated US$60 billion loss in crop yields worldwide each year. Plant viruses are grouped into 73 genera and 49 families. However, these figures relate only to cultivated plants that represent only a tiny fraction of the total number of plant species. Viruses in wild plants have been poorly studied, but those studies that exist almost overwhelmingly show that such interactions between wild plants and their viruses do not appear to cause disease in the host plants.\n\nTo transmit from one plant to another and from one plant cell to another, plant viruses must use strategies that are usually different from animal viruses. Plants do not move, and so plant-to-plant transmission usually involves vectors (such as insects). Plant cells are surrounded by solid cell walls, therefore transport through plasmodesmata is the preferred path for virions to move between plant cells. Plants have specialized mechanisms for transporting mRNAs through plasmodesmata, and these mechanisms are thought to be used by RNA viruses to spread from one cell to another.\n\nPlant defenses against viral infection include, among other measures, the use of siRNA in response to dsRNA. Most plant viruses encode a protein to suppress this response. Plants also reduce transport through plasmodesmata in response to injury.\n\nThe discovery of plant viruses causing disease is often accredited to A. Mayer (1886) working in the Netherlands demonstrated that the sap of mosaic obtained from tobacco leaves developed mosaic symptom when injected in healthy plants. However the infection of the sap was destroyed when it was boiled. He thought that the causal agent was the bacteria. However, after larger inoculation with a large number of bacteria, he failed to develop a mosaic symptom.\n\nIn 1898, Martinus Beijerinck, who was a Professor of Microbiology at the Technical University the Netherlands, put forth his concepts that viruses were small and determined that the \"mosaic disease\" remained infectious when passed through a Chamberland filter-candle. This was in contrast to bacteria microorganisms, which were retained by the filter. Beijerinck referred to the infectious filtrate as a \"contagium vivum fluidum\", thus the coinage of the modern term \"virus\".\n\nAfter the initial discovery of the ‘viral concept’ there was need to classify any other known viral diseases based on the mode of transmission even though microscopic observation proved fruitless. In 1939 Holmes published a classification list of 129 plant viruses. This was expanded and in 1999 there were 977 officially recognized, and some provisional, plant virus species.\n\nThe purification (crystallization) of TMV was first performed by Wendell Stanley, who published his findings in 1935, although he did not determine that the RNA was the infectious material. However, he received the Nobel Prize in Chemistry in 1946. In the 1950s a discovery by two labs simultaneously proved that the purified RNA of the TMV was infectious which reinforced the argument. The RNA carries genetic information to code for the production of new infectious particles.\n\nMore recently virus research has been focused on understanding the genetics and molecular biology of plant virus genomes, with a particular interest in determining how the virus can replicate, move and infect plants. Understanding the virus genetics and protein functions has been used to explore the potential for commercial use by biotechnology companies. In particular, viral-derived sequences have been used to provide an understanding of novel forms of resistance. The recent boom in technology allowing humans to manipulate plant viruses may provide new strategies for production of value-added proteins in plants.\n\nViruses are extremely small and can only be observed under an electron microscope. The structure of a virus is given by its coat of proteins, which surround the viral genome. Assembly of viral particles takes place spontaneously.\n\nOver 50% of known plant viruses are rod-shaped (flexuous or rigid). The length of the particle is normally dependent on the genome but it is usually between 300–500 nm with a diameter of 15–20 nm. Protein subunits can be placed around the circumference of a circle to form a disc. In the presence of the viral genome, the discs are stacked, then a tube is created with room for the nucleic acid genome in the middle.\n\nThe second most common structure amongst plant viruses are isometric particles. They are 25–50 nm in diameter. In cases when there is only a single coat protein, the basic structure consists of 60 T subunits, where T is an integer. Some viruses may have 2 coat proteins that associate to form an icosahedral shaped particle.\n\nThere are three genera of \"Geminiviridae\" that possess geminate particles which are like two isometric particles stuck together.\n\nA very small number of plant viruses have, in addition to their coat proteins, a lipid envelope. This is derived from the plant cell membrane as the virus particle buds off from the cell.\n\nViruses can be spread by direct transfer of sap by contact of a wounded plant with a healthy one. Such contact may occur during agricultural practices, as by damage caused by tools or hands, or naturally, as by an animal feeding on the plant. Generally TMV, potato viruses and cucumber mosaic viruses are transmitted via sap.\n\nPlant viruses need to be transmitted by a vector, most often insects such as leafhoppers. One class of viruses, the Rhabdoviridae, has been proposed to actually be insect viruses that have evolved to replicate in plants. The chosen insect vector of a plant virus will often be the determining factor in that virus's host range: it can only infect plants that the insect vector feeds upon. This was shown in part when the old world white fly made it to the United States, where it transferred many plant viruses into new hosts. Depending on the way they are transmitted, plant viruses are classified as non-persistent, semi-persistent and persistent. In non-persistent transmission, viruses become attached to the distal tip of the stylet of the insect and on the next plant it feeds on, it inoculates it with the virus. Semi-persistent viral transmission involves the virus entering the foregut of the insect. Those viruses that manage to pass through the gut into the haemolymph and then to the salivary glands are known as persistent. There are two sub-classes of persistent viruses: propagative and circulative. Propagative viruses are able to replicate in both the plant and the insect (and may have originally been insect viruses), whereas circulative can not. Circulative viruses are protected inside aphids by the chaperone protein symbionin, produced by bacterial symbionts. Many plant viruses encode within their genome polypeptides with domains essential for transmission by insects. In non-persistent and semi-persistent viruses, these domains are in the coat protein and another protein known as the helper component. A bridging hypothesis has been proposed to explain how these proteins aid in insect-mediated viral transmission. The helper component will bind to the specific domain of the coat protein, and then the insect mouthparts — creating a bridge. In persistent propagative viruses, such as tomato spotted wilt virus (TSWV), there is often a lipid coat surrounding the proteins that is not seen in other classes of plant viruses. In the case of TSWV, 2 viral proteins are expressed in this lipid envelope. It has been proposed that the viruses bind via these proteins and are then taken into the insect cell by receptor-mediated endocytosis.\n\nSoil-borne nematodes also have been shown to transmit viruses. They acquire and transmit them by feeding on infected roots. Viruses can be transmitted both non-persistently and persistently, but there is no evidence of viruses being able to replicate in nematodes. The virions attach to the stylet (feeding organ) or to the gut when they feed on an infected plant and can then detach during later feeding to infect other plants. Examples of viruses that can be transmitted by nematodes include tobacco ringspot virus and tobacco rattle virus.\n\nA number of virus genera are transmitted, both persistently and non-persistently, by soil borne zoosporic protozoa. These protozoa are not phytopathogenic themselves, but parasitic. Transmission of the virus takes place when they become associated with the plant roots. Examples include \"Polymyxa graminis\", which has been shown to transmit plant viral diseases in cereal crops and \"Polymyxa betae\" which transmits Beet necrotic yellow vein virus. Plasmodiophorids also create wounds in the plant's root through which other viruses can enter.\n\nPlant virus transmission from generation to generation occurs in about 20% of plant viruses. When viruses are transmitted by seeds, the seed is infected in the generative cells and the virus is maintained in the germ cells and sometimes, but less often, in the seed coat. When the growth and development of plants is delayed because of situations like unfavourable weather, there is an increase in the amount of virus infections in seeds. There does not seem to be a correlation between the location of the seed on the plant and its chances of being infected. Little is known about the mechanisms involved in the transmission of plant viruses via seeds, although it is known that it is environmentally influenced and that seed transmission occurs because of a direct invasion of the embryo via the ovule or by an indirect route with an attack on the embryo mediated by infected gametes. These processes can occur concurrently or separately depending on the host plant. It is unknown how the virus is able to directly invade and cross the embryo and boundary between the parental and progeny generations in the ovule. \nMany plants species can be infected through seeds including but not limited to the families Leguminosae, Solanaceae, Compositae, Rosaceae, Cucurbitaceae, Gramineae. Bean common mosaic virus is transmitted through seeds.\n\nResearchers from the University of the Mediterranean in Marseille, France have found tenuous evidence that suggest a virus common to peppers, the Pepper Mild Mottle Virus (PMMoV) may have moved on to infect humans. This is a very rare and highly unlikely event as, to enter a cell and replicate, a virus must \"bind to a receptor on its surface, and a plant virus would be highly unlikely to recognize a receptor on a human cell. One possibility is that the virus does not infect human cells directly. Instead, the naked viral RNA may alter the function of the cells through a mechanism similar to RNA interference, in which the presence of certain RNA sequences can turn genes on and off,\" according to Virologist Robert Garry from the Tulane University in New Orleans, Louisiana.\n\n75% of plant viruses have genomes that consist of single stranded RNA (ssRNA). 65% of plant viruses have +ssRNA, meaning that they are in the same sense orientation as messenger RNA but 10% have -ssRNA, meaning they must be converted to +ssRNA before they can be translated. 5% are double stranded RNA and so can be immediately translated as +ssRNA viruses. 3% require a reverse transcriptase enzyme to convert between RNA and DNA. 17% of plant viruses are ssDNA and very few are dsDNA, in contrast a quarter of animal viruses are dsDNA and three quarters of bacteriophage are dsDNA. Viruses use the plant ribosomes to produce the 4-10 proteins encoded by their genome. However, since many of the proteins are encoded on a single strand (that is, they are polycistronic) this will mean that the ribosome will either only produce one protein, as it will terminate translation at the first stop codon, or that a polyprotein will be produced. Plant viruses have had to evolve special techniques to allow the production of viral proteins by plant cells.\n\nFor translation to occur, eukaryotic mRNAs require a 5' Cap structure. This means that viruses must also have one. This normally consists of 7MeGpppN where N is normally adenine or guanine. The viruses encode a protein, normally a replicase, with a methyltransferase activity to allow this.\n\nSome viruses are cap-snatchers. During this process, a G-capped host mRNA is recruited by the viral transcriptase complex and subsequently cleaved by a virally encoded endonuclease. The resulting capped leader RNA is used to prime transcription on the viral genome.\n\nHowever some plant viruses do not use cap, yet translate efficiently due to cap-independent translation enhancers present in 5' and 3' untranslated regions of viral mRNA.\n\nSome viruses (e.g. tobacco mosaic virus (TMV)) have RNA sequences that contain a \"leaky\" stop codon. In TMV 95% of the time the host ribosome will terminate the synthesis of the polypeptide at this codon but the rest of the time it continues past it. This means that 5% of the proteins produced are larger than and different from the others normally produced, which is a form of translational regulation. In TMV, this extra sequence of polypeptide is an RNA polymerase that replicates its genome.\n\nSome viruses use the production of subgenomic RNAs to ensure the translation of all proteins within their genomes. In this process the first protein encoded on the genome, and this the first to be translated, is a replicase. This protein will act on the rest of the genome producing negative strand sub-genomic RNAs then act upon these to form positive strand sub-genomic RNAs that are essentially mRNAs ready for translation.\n\nSome viral families, such as the \"Bromoviridae\" instead opt to have multipartite genomes, genomes split between multiple viral particles. For infection to occur, the plant must be infected with all particles across the genome. For instance \"Brome mosaic virus\" has a genome split between 3 viral particles, and all 3 particles with the different RNAs are required for infection to take place.\n\nThis strategy is adopted by viral genera such as the Potyviridae and Tymoviridae. The ribosome translates a single protein from the viral genome. Within the polyprotein is an enzyme (or enzymes) with proteinase function that is able to cleave the polyprotein into the various single proteins or just cleave away the protease, which can then cleave other polypeptides producing the mature proteins.\n\nTobacco mosaic virus (TMV) and Cauliflower mosaic virus (CaMV) are frequently used in plant molecular biology. Of special interest is the CaMV 35S promoter, which is a very strong promoter most frequently used in plant transformations.\n\n\n\n", "id": "1475873", "title": "Plant virus"}
{"url": "https://en.wikipedia.org/wiki?curid=14168980", "text": "Deoxyribonucleoprotein\n\nDeoxyribonucleoprotein (DNP) is the complex of DNA and protein in which DNA is usually found upon cell disruption and isolation.\n\nThe most widespread deoxyribonucleoproteins are nucleosomes, in which the component is nuclear DNA. The proteins combined with DNA are histones and protamines; the resulting nucleoproteins are located in chromosomes. Thus, the entire chromosome, i.e. chromatin in eukaryotes consists of such nucleoproteins.\n\nMany viruses are little more than an organized collection of deoxyribonucleoproteins.\n\n", "id": "14168980", "title": "Deoxyribonucleoprotein"}
{"url": "https://en.wikipedia.org/wiki?curid=7851422", "text": "Single-strand conformation polymorphism\n\nSingle-strand conformation polymorphism (SSCP), or single-strand \"chain\" polymorphism, is defined as a conformational difference of single-stranded nucleotide sequences of identical length as induced by differences in the sequences under certain experimental conditions. This property allows sequences to be distinguished by means of gel electrophoresis, which separates fragments according to their different conformations.\n\nA single nucleotide change in a particular sequence, as seen in a double-stranded DNA, cannot be distinguished by electrophoresis, because the physical properties of the double strands are almost identical for both alleles. After denaturation, single-stranded DNA undergoes a 3-dimensional folding and may assume a unique conformational state based on its DNA sequence. The difference in shape between two single-stranded DNA strands with different sequences can cause them to migrate differently on an electrophoresis gel, even though the number of nucleotides is the same, which is, in fact, an application of SSCP.\n\nSSCP used to be a way to discover new DNA polymorphisms apart from DNA sequencing but is now being supplanted by sequencing techniques on account of efficiency and accuracy. These days, SSCP is most applicable as a diagnostic tool in molecular biology. It can be used in genotyping to detect homozygous individuals of different allelic states, as well as heterozygous individuals that should each demonstrate distinct patterns in an electrophoresis experiment. SSCP is also widely used in virology to detect variations in different strains of a virus, the idea being that a particular virus particle present in both strains will have undergone changes due to mutation, and that these changes will cause the two particles to assume different conformations and, thus, be differentiable on an SSCP gel.\n", "id": "7851422", "title": "Single-strand conformation polymorphism"}
{"url": "https://en.wikipedia.org/wiki?curid=27947144", "text": "Minimotif Miner\n\nMinimotif Miner is a program and database designed to identify minimotifs in any protein. Minimotifs are short contiguous peptide sequences that are known to have a function in at least one protein. Minimotifs are also called sequence motifs or short linear motifs or SLiMs. These are generally restricted to one secondary structure element and are less than 15 amino acids in length. Functions can be binding motifs that bind another macromolecule or small compound, that induce a covalent modification of minimotif, or are involved in protein trafficking of the protein containing the minimotif. The basic premise of Minimotif Miner is that is a short peptide sequence is known to have a function in one protein, may have a similar function in another query protein. The current release of the MnM 3.0 database has ~300,000 minimotifs and can be searched at the website.\n\nThere are two workflows that are of interest to scientists that use Minimotif Miner 1) Entering any query protein into Minimotif Miner returns a table with a list of minimotif sequence and functions that have a sequence pattern match with the protein query sequence. These provide potential new functions in the protein query. 2) By using the view single nucleotide polymorphism (SNP) function, SNPs from dbSNP are mapped in the sequence window. A user can select any set of the SNPs and then identify any minimotif that is introduced or eliminated by the SNP or mutation. This helps to identify minimotifs involved in generating organism diversity or those that may be associated with a disease.\n\nTypical results of MnM predict more than 50 new minimotifs for a protein query. A major limitation in this type of analysis is that the low sequence complexity of short minimotifs produces false positive predictions where the sequence occurs in a protein by random chance and not because it contains the predicted function. MnM 3.0 introduces a library of advanced heuristics and filters, which enable vast reduction of false positive predictions. These filters use minimotif complexity, protein surface location, molecular processes, cellular processes, protein-protein interactions, and genetic interactions. We recently combined all of these heuristics into a single, compound filter which makes significant progress toward solving this problem with high accuracy of minimotif prediction as measured by a performance benchmarking study which evaluated both sensitivity and specificity.\n\n\n\n", "id": "27947144", "title": "Minimotif Miner"}
{"url": "https://en.wikipedia.org/wiki?curid=27456286", "text": "List of restriction enzyme cutting sites: Ba–Bc\n\nThis article contains a list of the most studied restriction enzymes whose names start with Ba to Bc inclusive. It contains approximately 120 enzymes.\n\nThe following information is given:\n", "id": "27456286", "title": "List of restriction enzyme cutting sites: Ba–Bc"}
{"url": "https://en.wikipedia.org/wiki?curid=27456094", "text": "List of restriction enzyme cutting sites: A\n\nThis article contains a list of the most studied restriction enzymes whose names start with A. It contains approximately 30 enzymes.\n\nThe following information is given:\n", "id": "27456094", "title": "List of restriction enzyme cutting sites: A"}
{"url": "https://en.wikipedia.org/wiki?curid=174030", "text": "Western blot\n\nThe western blot (sometimes called the protein immunoblot) is a widely used analytical technique used in molecular biology, immunogenetics and other molecular biology disciplines to detect specific proteins in a sample of tissue homogenate or extract.\n\nSynthetic or animal-derived antibodies are created that react with a specific target protein. The sample material undergoes protein denaturation, followed by gel electrophoresis. Next, the electrophoresis membrane is washed in a solution containing the specific antibody. The excess antibody is then washed off, and a secondary antibody that reacts with the first antibody is added. Through various methods such as staining, immunofluorescence, and radioactivity, the secondary antibody can then allow visualisation of the protein.\n\nOther related techniques include dot blot analysis, quantitative dot blot, immunohistochemistry, and immunocytochemistry where antibodies are used to detect proteins in tissues and cells by immunostaining, and enzyme-linked immunosorbent assay (ELISA).\n\nThe name \"western blot\" is a play on the eponymously-named Southern blot, a technique for DNA detection developed earlier by Edwin Southern. Similarly, detection of RNA is termed northern blot and was developed by James Alwine, David Kemp, and George Stark at Stanford. The term \"western blot\" was given to the technique by W. Neal Burnette, although the method itself originated in the laboratory of Harry Towbin at the Friedrich Miescher Institute.\n\nThe western blot is extensively used in biochemistry for the qualitative detection of single proteins and protein-modifications (such as post-translational modifications). It is used as a general method to specifically prove the existence of a single protein in a mixture. A semi-quantitative estimation of a protein can be derived from the size and color intensity of a protein band on the blot membrane. By also applying a dilution series of a purified protein with known concentrations, a more precise estimate of the quantity can be obtained. The western blot is routinely used for verification of protein production after cloning. It is also used in medical diagnostics, e. g. in the HIV test or BSE-Test.\n\nThe confirmatory HIV test employs a western blot to detect anti-HIV antibody in a human serum sample. Proteins from known HIV-infected cells are separated and blotted on a membrane as above. Then, the serum to be tested is applied in the primary antibody incubation step; free antibody is washed away, and a secondary anti-human antibody linked to an enzyme signal is added. The stained bands then indicate the proteins to which the patient's serum contains antibody.\n\nA western blot is also used as the definitive test for variant Creutzfeldt-Jakob Disease, a type of prion disease linked to the consumption of contaminated beef from cattle with Bovine spongiform encephalopathy (BSE, commonly referred to as 'mad cow disease').\n\nSome forms of Lyme disease testing employ western blotting. A western blot can also be used as a confirmatory test for Hepatitis B infection and HSV-2 (Herpes Type 2) infection. In veterinary medicine, a western blot is sometimes used to confirm FIV+ status in cats. \n\nFurther applications of the western blot technique include its use by the World Anti-Doping Agency (WADA). Blood doping is the misuse of certain techniques and/or substances to increase one's red blood cell mass, which allows the body to transport more oxygen to muscles and therefore increase stamina and performance. There are three widely known substances or methods used for blood doping, namely, erythropoietin (EPO), synthetic oxygen carriers and blood transfusions. Each is prohibited under WADA's List of Prohibited Substances and Methods. The western blot technique was used during the 2014 FIFA World Cup in the anti-doping campaign for that event. In total, over 1000 samples were collected and analyzed by Reichel, et al. in the WADA accredited Laboratory of Lausanne, Switzerland. \nRecent research utilizing the western blot technique showed an improved detection of EPO in blood and urine based on novel Velum SAR precast horizontal gels optimized for routine analysis. With the adoption of the horizontal SAR-PAGE in combination with the precast film-supported Velum SAR gels the discriminatory capacity of micro-dose application of rEPO was significantly enhanced.\n\nThe western blot method is composed of a gel electrophoresis to separate native proteins by 3-D structure or denatured proteins by the length of the polypeptide, followed by an electrophoretic transfer onto a membrane (mostly PVDF or Nitrocellulose) and an immunostaining procedure to visualize a certain protein on the blot membrane. SDS-PAGE is generally used for the denaturing electrophoretic separation of proteins. SDS is generally used as a buffer (as well as in the gel) in order to give all proteins present a uniform negative charge, since proteins can be positively, negatively, or neutrally charged. This type of electrophoresis is known as SDS-PAGE (SDS-polyacrylamide gel electrophoresis). Prior to electrophoresis, protein samples are often boiled to denature the proteins present. This ensures that proteins are separated based on size and prevents proteases (enzymes that break down proteins) from degrading samples. Following electrophoretic separation, the proteins are transferred to a membrane (typically nitrocellulose or PVDF), where they are blocked with milk (or other blocking agents) to prevent non-specific antibody binding, and then stained with antibodies specific to the target protein. Lastly, the membrane will be stained with a secondary antibody that recognizes the first antibody staining, which can then be used for detection by a variety of methods. The gel electrophoresis step is included in western blot analysis to resolve the issue of the cross-reactivity of antibodies.\n\nSamples can be taken from whole tissue or from cell culture. Solid tissues are first broken down mechanically using a blender (for larger sample volumes), using a homogenizer (smaller volumes), or by sonication. Cells may also be broken open by one of the above mechanical methods. However, virus or environmental samples can be the source of protein and thus western blotting is not restricted to cellular studies only.\n\nAssorted detergents, salts, and buffers may be employed to encourage lysis of cells and to solubilize proteins. Protease and phosphatase inhibitors are often added to prevent the digestion of the sample by its own enzymes. Tissue preparation is often done at cold temperatures to avoid protein denaturing and degradation.\n\nA combination of biochemical and mechanical techniques – comprising various types of filtration and centrifugation – can be used to separate different cell compartments and organelles.\n\nThe proteins of the sample are separated using gel electrophoresis. Separation of proteins may be by isoelectric point (pI), molecular weight, electric charge, or a combination of these factors. The nature of the separation depends on the treatment of the sample and the nature of the gel. This is a very useful way to identify a protein.\n\nBy far the most common type of gel electrophoresis employs polyacrylamide gels and buffers loaded with sodium dodecyl sulfate (SDS). SDS-PAGE (SDS polyacrylamide gel electrophoresis) maintains polypeptides in a denatured state once they have been treated with strong reducing agents to remove secondary and tertiary structure (e.g. disulfide bonds [S-S] to sulfhydryl groups [SH and SH]) and thus allows separation of proteins by their molecular mass. Sampled proteins become covered in the negatively charged SDS and move to the positively charged electrode through the acrylamide mesh of the gel. Smaller proteins migrate faster through this mesh, and the proteins are thus separated according to size (usually measured in kilodaltons, kDa). The concentration of acrylamide determines the resolution of the gel - the greater the acrylamide concentration, the better the resolution of lower molecular weight proteins. The lower the acrylamide concentration, the better the resolution of higher molecular weight proteins. Proteins travel only in one dimension along the gel for most blots.\n\nSamples are loaded into \"wells\" in the gel. One lane is usually reserved for a \"marker\" or \"ladder\", a commercially available mixture of proteins having defined molecular weights, typically stained so as to form visible, coloured bands. When voltage is applied along the gel, proteins migrate through it at different speeds dependent on their size. These different rates of advancement (different \"electrophoretic mobilities\") separate into \"bands\" within each \"lane\".\n\nIt is also possible to use a two-dimensional (2-D) gel which spreads the proteins from a single sample out in two dimensions. Proteins are separated according to isoelectric point (pH at which they have a neutral net charge) in the first dimension, and according to their molecular weight in the second dimension.\n\nTo make the proteins accessible to antibody detection, they are moved from within the gel onto a membrane made of \"nitrocellulose or polyvinylidene difluoride (PVDF\"). The primary method for transferring the proteins is called electroblotting and uses an electric current to pull proteins from the gel into the PVDF or nitrocellulose membrane. The proteins move from within the gel onto the membrane while maintaining the organization they had within the gel. An older method of transfer involves placing a membrane on top of the gel, and a stack of filter papers on top of that. The entire stack is placed in a buffer solution which moves up the paper by capillary action, bringing the proteins with it. In practice this method is not used as it takes too much time; electroblotting is preferred. As a result of either \"blotting\" process, the proteins are exposed on a thin surface layer for detection (see below). Both varieties of membrane are chosen for their non-specific protein binding properties (i.e. binds all proteins equally well). Protein binding is based upon hydrophobic interactions, as well as charged interactions between the membrane and protein. Nitrocellulose membranes are cheaper than PVDF, but are far more fragile and do not stand up well to repeated probings.\n\nThis step visualizes the total protein that has been successfully transferred to the membrane. Total protein staining allows the user to check the uniformity of protein transfer and to perform subsequent normalization of the target protein with the actual protein amount per lane. Normalization with the so-called \"loading control\" has based on immunostaining of housekeeping proteins in the classical procedure, but is heading toward total protein staining recently, due to multiple benefits. At least seven different approaches for total protein staining have been described for western blot normalization: Ponceau S, stain-free techniques, Sypro Ruby, Epicocconone, Coomassie R-350, Amido Black, and Cy5. In order to avoid noise of signal, total protein staining should be performed before blocking of the membrane. Nevertheless, post-antibody stainings have been described as well. \n\nSince the membrane has been chosen for its ability to bind protein and as both antibodies and the target are proteins, steps must be taken to prevent the interactions between the membrane and the antibody used for detection of the target protein. Blocking of non-specific binding is achieved by placing the membrane in a dilute solution of protein – typically 3–5% bovine serum albumin (BSA) or non-fat dry milk (both are inexpensive) in tris-buffered saline (TBS) or I-Block, with a minute percentage (0.1%) of detergent such as Tween 20 or Triton X-100. Although non-fat dry milk is preferred due to its availability, an appropriate blocking solution is needed as not all proteins in milk are compatible with all the detection bands. The protein in the dilute solution attaches to the membrane in all places where the target proteins have not attached. Thus, when the antibody is added, there is no room on the membrane for it to attach other than on the binding sites of the specific target protein. This reduces background in the final product of the western blot, leading to clearer results, and eliminates false positives.\n\nDuring the detection process the membrane is \"probed\" for the protein of interest with a modified antibody which is linked to a reporter enzyme; when exposed to an appropriate substrate, this enzyme drives a colorimetric reaction and produces a color. For a variety of reasons, this traditionally takes place in a two-step process, although there are now one-step detection methods available for certain applications.\n\nThe primary antibodies are generated when a host species or immune cell culture is exposed to the protein of interest (or a part thereof). Normally, this is part of the immune response, whereas here they are harvested and used as sensitive and specific detection tools that bind the protein directly.\n\nAfter blocking, a dilute solution of primary antibody (generally between 0.5 and 5 micrograms/mL) is incubated with the membrane under gentle agitation. The primary antibody is then diluted with PBS or TBST wash buffers. Washing the membranes with the antibody-buffer solution helps minimize background and removes unbound antibodies. Typically, the solution is composed of buffered saline solution with a small percentage of detergent, and sometimes with powdered milk or BSA. The antibody solution and the membrane can be sealed and incubated together for anywhere from 30 minutes to overnight. It can also be incubated at different temperatures, with lesser temperatures being associated with more binding, both specific (to the target protein, the \"signal\") and non-specific (\"noise\").\n\nAfter rinsing the membrane to remove unbound primary antibody, the membrane is exposed to another antibody, directed at a species-specific portion of the primary antibody. Antibodies come from animal sources (or animal sourced hybridoma cultures); an anti-mouse secondary will bind to almost any mouse-sourced primary antibody, which allows some cost savings by allowing an entire lab to share a single source of mass-produced antibody, and provides far more consistent results. This is known as a secondary antibody, and due to its targeting properties, tends to be referred to as \"anti-mouse\", \"anti-goat\", etc. The secondary antibody is usually linked to biotin or a reporter enzyme such as alkaline phosphatase or horseradish peroxidase. This means that several secondary antibodies will bind to one primary antibody and enhance the signal.\n\nMost commonly, a horseradish peroxidase-linked secondary is used to cleave a chemiluminescent agent, and the reaction product produces luminescence in proportion to the amount of protein. A sensitive sheet of photographic film is placed against the membrane, and exposure to the light from the reaction creates an image of the antibodies bound to the blot. A cheaper but less sensitive approach utilizes a 4-chloronaphthol stain with 1% hydrogen peroxide; the reaction of peroxide radicals with 4-chloronaphthol produces a dark purple stain that can be photographed without using specialized photographic film.\n\nAs with the ELISPOT and ELISA procedures, the enzyme can be provided with a substrate molecule that will be converted by the enzyme to a colored reaction product that will be visible on the membrane (see the figure below with blue bands).\n\nAnother method of secondary antibody detection utilizes a near-infrared (NIR) fluorophore-linked antibody. The light produced from the excitation of a fluorescent dye is static, making fluorescent detection a more precise and accurate measure of the difference in the signal produced by labeled antibodies bound to proteins on a western blot. Proteins can be accurately quantified because the signal generated by the different amounts of proteins on the membranes is measured in a static state, as compared to chemiluminescence, in which light is measured in a dynamic state.\n\nA third alternative is to use a radioactive label rather than an enzyme coupled to the secondary antibody, such as labeling an antibody-binding protein like \"Staphylococcus\" Protein A or Streptavidin with a radioactive isotope of iodine. Since other methods are safer, quicker, and cheaper, this method is now rarely used; however, an advantage of this approach is the sensitivity of auto-radiography-based imaging, which enables highly accurate protein quantification when combined with optical software (e.g. Optiquant).\n\nHistorically, the probing process was performed in two steps because of the relative ease of producing primary and secondary antibodies in separate processes. This gives researchers and corporations huge advantages in terms of flexibility, and adds an amplification step to the detection process. Given the advent of high-throughput protein analysis and lower limits of detection, however, there has been interest in developing one-step probing systems that would allow the process to occur faster and with fewer consumables. This requires a probe antibody which both recognizes the protein of interest and contains a detectable label, probes which are often available for known protein tags. The primary probe is incubated with the membrane in a manner similar to that for the primary antibody in a two-step process, and then is ready for direct detection after a series of wash steps.\n\nAfter the unbound probes are washed away, the western blot is ready for detection of the probes that are labeled and bound to the protein of interest. In practical terms, not all westerns reveal protein only at one band in a membrane. Size approximations are taken by comparing the stained bands to that of the marker or ladder loaded during electrophoresis. The process is commonly repeated for a structural protein, such as actin or tubulin, that should not change between samples. The amount of target protein is normalized to the structural protein to control between groups. A superior strategy is the normalization to the total protein visualized with trichloroethanol or epicocconone. This practice ensures correction for the amount of total protein on the membrane in case of errors or incomplete transfers. (see western blot normalization)\n\nThe colorimetric detection method depends on incubation of the western blot with a substrate that reacts with the reporter enzyme (such as peroxidase) that is bound to the secondary antibody. This converts the soluble dye into an insoluble form of a different color that precipitates next to the enzyme and thereby stains the membrane. Development of the blot is then stopped by washing away the soluble dye. Protein levels are evaluated through densitometry (how intense the stain is) or spectrophotometry.\n\nChemiluminescent detection methods depend on incubation of the western blot with a substrate that will luminesce when exposed to the reporter on the secondary antibody. The light is then detected by CCD cameras which capture a digital image of the western blot or photographic film. The use of film for western blot detection is slowly disappearing because of non linearity of the image (non accurate quantification). The image is analysed by densitometry, which evaluates the relative amount of protein staining and quantifies the results in terms of optical density. Newer software allows further data analysis such as molecular weight analysis if appropriate standards are used.\nRadioactive labels do not require enzyme substrates, but rather, allow the placement of medical X-ray film directly against the western blot, which develops as it is exposed to the label and creates dark regions which correspond to the protein bands of interest (see image to the right). The importance of radioactive detections methods is declining due to its hazardous radiation , because it is very expensive, health and safety risks are high, and ECL (enhanced chemiluminescence) provides a useful alternative.\n\nThe fluorescently labeled probe is excited by light and the emission of the excitation is then detected by a photosensor such as a CCD camera equipped with appropriate emission filters which captures a digital image of the western blot and allows further data analysis such as molecular weight analysis and a quantitative western blot analysis. Fluorescence is considered to be one of the best methods for quantification but is less sensitive than chemiluminescence.\n\nOne major difference between nitrocellulose and PVDF membranes relates to the ability of each to support \"stripping\" antibodies off and reusing the membrane for subsequent antibody probes. While there are well-established protocols available for stripping nitrocellulose membranes, the sturdier PVDF allows for easier stripping, and for more reuse before background noise limits experiments. Another difference is that, unlike nitrocellulose, PVDF must be soaked in 95% ethanol, isopropanol or methanol before use. PVDF membranes also tend to be thicker and more resistant to damage during use.\n\n2-dimensional SDS-PAGE uses the principles and techniques outlined above. 2-D SDS-PAGE, as the name suggests, involves the migration of polypeptides in 2 dimensions. For example, in the first dimension, polypeptides are separated according to isoelectric point, while in the second dimension, polypeptides are separated according to their molecular weight. The isoelectric point of a given protein is determined by the relative number of positively (e.g. lysine, arginine) and negatively (e.g. glutamate, aspartate) charged amino acids, with negatively charged amino acids contributing to a low isoelectric point and positively charged amino acids contributing to a high isoelectric point. Samples could also be separated first under nonreducing conditions using SDS-PAGE, and under reducing conditions in the second dimension, which breaks apart disulfide bonds that hold subunits together. SDS-PAGE might also be coupled with urea-PAGE for a 2-dimensional gel.\n\nIn principle, this method allows for the separation of all cellular proteins on a single large gel. A major advantage of this method is that it often distinguishes between different isoforms of a particular protein - e.g. a protein that has been phosphorylated (by addition of a negatively charged group). Proteins that have been separated can be cut out of the gel and then analysed by mass spectrometry, which identifies the protein.\n\n", "id": "174030", "title": "Western blot"}
{"url": "https://en.wikipedia.org/wiki?curid=2073059", "text": "Mitotoxin\n\nA mitotoxin is a cytotoxic molecule targeted to specific cells by a mitogen. Generally found in snake venom\n", "id": "2073059", "title": "Mitotoxin"}
{"url": "https://en.wikipedia.org/wiki?curid=16161178", "text": "Effective number of codons\n\nEffective number of codons (abbreviated as ENC or \"Nc\") is a measure to study the state of codon usage biases in genes and genomes. The way that ENC is computed has obvious similarities to the computation of effective population size in population genetics. Although it is easy to compute ENC values, it has been shown that this measure is one of the best measures to show codon usage bias.\n\nSince the original suggestion of the ENC, several investigators have tried to improve the method, but it seems that there is much room to improve this measure.\n", "id": "16161178", "title": "Effective number of codons"}
{"url": "https://en.wikipedia.org/wiki?curid=5712711", "text": "Comet assay\n\nThe single cell gel electrophoresis assay (SCGE, also known as comet assay) is an uncomplicated and sensitive technique for the detection of DNA damage at the level of the individual eukaryotic cell. It was first developed by Östling & Johansson in 1984 and later modified by Singh \"et al.\" in 1988. It has since increased in popularity as a standard technique for evaluation of DNA damage/repair, biomonitoring and genotoxicity testing. It involves the encapsulation of cells in a low-melting-point agarose suspension, lysis of the cells in neutral or alkaline (pH>13) conditions, and electrophoresis of the suspended lysed cells. The term \"comet\" refers to the pattern of DNA migration through the electrophoresis gel, which often resembles a comet.\n\nThe comet assay (single-cell gel electrophoresis) is a simple method for measuring deoxyribonucleic acid (DNA) strand breaks in eukaryotic cells. Cells embedded in agarose on a microscope slide are lysed with detergent and high salt to form nucleoids containing supercoiled loops of DNA linked to the nuclear matrix. Electrophoresis at high pH results in structures resembling comets, observed by fluorescence microscopy; the intensity of the comet tail relative to the head reflects the number of DNA breaks. The likely basis for this is that loops containing a break lose their supercoiling and become free to extend toward the anode. This is followed by visual analysis with staining of DNA and calculating fluorescence to determine the extent of DNA damage. This can be performed by manual scoring or automatically by imaging software.\n\nA sample of cells, either derived from an \"in vitro\" cell culture or from an \"in vivo\" test subject is dispersed into individual cells and suspended in molten low-melting-point agarose at 37 °C. This mono-suspension is cast on a microscope slide. A glass cover slip is held at an angle and the mono-suspension applied to the point of contact between the coverslip and the slide. As the coverslip is lowered onto the slide the molten agarose spreads to form a thin layer. The agarose is gelled at 4 °C and the coverslip removed.\n\nThe agarose forms a matrix of carbohydrate fibres that encapsulate the cells, anchoring them in place. The agarose is considered to be osmotic-neutral, therefore solutions can penetrate the gel and affect the cells without cells shifting position.\n\nIn an \"in vitro\" study the cells would be exposed to a test agent – typically UV light, ionising radiation, or a genotoxic chemical – to induce DNA damage in the encapsulated cells. For calibration, hydrogen peroxide is usually used to provide a standardized level of DNA damage.\n\nThe slides are then immersed in a solution that cause the cells to lyse. The lysis solution often used in the comet assay consists of a highly concentrated aqueous salt (often, common table salt can be used) and a detergent (such as Triton X-100 or sarcosinate). The pH of the lysis solution can be adjusted (usually between neutral and alkaline pH) depending upon the type of damage the researcher is investigating.\n\nThe aqueous salt disrupts proteins and their bonding patterns within the cell as well as disrupting the RNA content of the cell. The detergent dissolves the cellular membranes. Through the action of the lysis solution the cells are destroyed. All proteins, RNA, membranes and cytoplasmic and nucleoplasmic constituents are disrupted and diffuse into the agarose matrix. Only the DNA of the cell remains, and unravels to fill the cavity in the agarose that the whole cell formerly filled. This structure is called nucleoid (a general term for a structure in which DNA is concentrated).\n\nAfter lysis of the cells (typically 1 to 2 hours at 4 °C) the slides are washed in distilled water to remove all salts and immersed in a second solution – an electrophoresis solution. Again this solution can have its pH adjusted depending upon the type of damage that is being investigated.\n\nThe slides are left for ~20 minutes in the electrophoresis solution prior to an electric field being applied. In alkaline conditions the DNA double helix is denatured and the nucleoid becomes single stranded.\n\nAn electric field is applied (typically 1 V/cm) for ~20 minutes. The slides are then neutralised to pH 7, stained with a DNA-specific fluorescent stain and analysed using a microscope with an attached CCD (charge-coupled device – essentially a digital camera) that is connected to a computer with image analysis software.\n\nThe concept underlying the SCGE assay is that undamaged DNA retains a highly organized association with matrix proteins in the nucleus. When damaged, this organization is disrupted. The individual strands of DNA lose their compact structure and relax, expanding out of the cavity into the agarose. When the electric field is applied the DNA, which has an overall negative charge, is drawn towards the positively charged anode. Undamaged DNA strands are too large and do not leave the cavity, whereas the smaller the fragments, the farther they are free to move in a given period of time. Therefore, the amount of DNA that leaves the cavity is a measure of the amount of DNA damage in the cell.\n\nThe image analysis measures the overall intensity of the fluorescence for the whole nucleoid and the fluorescence of the migrated DNA and compares the two signals. The stronger the signal from the migrated DNA the more damage there is present. The overall structure resembles a comet (hence \"comet assay\") with a circular head corresponding to the undamaged DNA that remains in the cavity and a tail of damaged DNA. The brighter and longer the tail, the higher the level of damage.\n\nThe comet assay is a versatile technique for detecting damage and with adjustments to the protocol can be used to quantify the presence of a wide variety of DNA altering lesions (damage). The damage usually detected are single strand breaks and double strand breaks. It is sometimes stated that alkaline conditions and complete denaturating of the DNA is necessary to detect single strand breaks. However this is not true, both single- and double strand breaks are also detected in neutral conditions. In alkaline conditions, however, additional DNA structures are detected as DNA damage: AP sites (abasic sites missing either a pyrimidine or purine nucleotide) and sites where excision repair is taking place.\n\nThe comet assay is an extremely sensitive DNA damage assay. This sensitivity needs to be handled carefully as it is also vulnerable to physical changes which can affect the reproducibility of results. Essentially, anything that can cause DNA damage or denaturation except the factor(s) being researched is to be avoided. The most common form of the assay is the alkaline version although there is as yet no definitive alkaline assay protocol. Due to its simple and inexpensive setup, it can be used in conditions where more complex assays are not available.\n\nThese include genotoxicity testing, human biomonitoring and molecular epidemiology, ecogenotoxicology, as well as fundamental research in DNA damage and repair.\n\nA Comet assay can determine the degree of DNA fragmentation in sperm cells. The degree of DNA fragmentation has been associated with outcomes of in vitro fertilization.\n\nThe Comet has been modified for use with sperm cells as a tool for male infertility diagnosis \n\nTo break down these tightly bound protamine proteins in order to use the Comet for sperm, additional steps in the de-condensation protocol are required.\n\n", "id": "5712711", "title": "Comet assay"}
{"url": "https://en.wikipedia.org/wiki?curid=29400", "text": "Structural biology\n\nStructural biology is a branch of molecular biology, biochemistry, and biophysics concerned with the molecular structure of biological macromolecules, especially amino and nucleic acids, how they acquire the structures they have, and how alterations in their structures affect their function. This subject is of great interest to biologists because macromolecules carry out most of the functions of cells, and only by coiling into specific three-dimensional shapes that they are able to perform these functions. This architecture, the \"tertiary structure\" of molecules, depends in a complicated way on the molecules' basic composition, or \"primary structures.\" \n\nBiomolecules are too small to see in detail even with the most advanced light microscopes. The methods that structural biologists use to determine their structures generally involve measurements on vast numbers of identical molecules at the same time. These methods include:\nMost often researchers use them to study the \"native states\" of macromolecules. But variations on these methods are also used to watch nascent or denatured molecules assume or reassume their native states. See protein folding. \n\nA third approach that structural biologists take to understanding structure is bioinformatics to look for patterns among the diverse sequences that give rise to particular shapes. Researchers often can deduce aspects of the structure of integral membrane proteins based on the membrane topology predicted by hydrophobicity analysis. See protein structure prediction. \nIn the past few years it has become possible for highly accurate physical molecular models to complement the \"in silico\" study of biological structures. Examples of these models can be found in the Protein Data Bank.\n\n\n", "id": "29400", "title": "Structural biology"}
{"url": "https://en.wikipedia.org/wiki?curid=25809437", "text": "Kenneth B. Storey\n\nKenneth B. Storey, Ph.D. (born October 23, 1949) is a Canadian scientist whose work draws from a variety of fields including biochemistry and molecular biology. He is a Professor of Biology and Chemistry at Carleton University in Ottawa, Canada. Storey has a world-wide reputation for his research on biochemical adaptation - the molecular mechanisms that allow animals to adapt to and endure severe environmental stresses such as deep cold, oxygen deprivation, and desiccation.\n\nKenneth Storey studied biochemistry at the University of Calgary (B.Sc. '71) and zoology at the University of British Columbia (Ph.D. '74). Storey is a Professor of Biochemistry, cross-appointed in the Departments of Biology, Chemistry and Neuroscience and holds the Canada Research Chair in Molecular Physiology at Carleton University in Ottawa, Canada.\n\nStorey is an elected fellow of the Royal Society of Canada, of the Society for Cryobiology and of the American Association for the Advancement of Science. He has won fellowships and awards for research excellence including the Fry medal from the Canadian Society of Zoologists (2011), the Flavelle medal from the Royal Society of Canada (2010), Ottawa Life Sciences Council Basic Research Award (1998), a Killam Senior Research Fellowship (1993–1995), the Ayerst Award from the Canadian Biochemical Society (1989), an E.W.R. Steacie Memorial Fellowship from the Natural Sciences and Engineering Research Council of Canada (1984–1986), and four Carleton University Research Achievement Awards. Storey is the author of over 700 research articles, the editor of seven books, has given over 500 talks at conferences and institutes worldwide, and organized numerous international symposia.\n\nStorey is one of the most cited researchers in the world. Storey's research includes studies of enzyme properties, gene expression, protein phosphorylation, and cellular signal transduction mechanisms to seek out the basic principles of how organisms endure and flourish under extreme conditions. He is particularly known within the field of cryobiology for his studies of animals that can survive freezing, especially the frozen \"frog-sicles\" (\"Rana sylvatica\") that have made his work popular with multiple TV shows and magazines. Storey's studies of the adaptations that allow frogs, insects, and other animals to survive freezing have made major advances in the understanding of how cells, tissues and organs can endure freezing. Storey was also responsible for the discovery that some turtle species are freeze tolerant: newly hatched painted turtles that spend their first winter on land (\"Chrysemys picta marginata & C. p. bellii\"). These turtles are unique as they are the only reptiles, and highest vertebrate life form, known to tolerate prolonged natural freezing of extracellular body fluids during winter hibernation. These advances may aid the development of organ cryopreservation technology. A second area of his research is metabolic rate depression - understanding the mechanisms by which some animals can sharply reduce their metabolism and enter a state of hypometabolism or torpor that allows them to survive over the long term under difficult environmental stresses. His studies have identified molecular mechanisms that underlie metabolic arrest across phylogeny and that support phenomena including mammalian hibernation, estivation, and anoxia and ischemia tolerance. Control mechanisms include transcription factor changes that alter gene expression, and reversible phosphorylation of key metabolic enzymes by protein kinases and protein phosphatases. These studies across multiple species also hold key applications for medical science, particularly for preservation technologies that aim to extend the survival time of excised organs in cold or frozen storage. Additional applications include insights into hyperglycemia in metabolic syndrome and diabetes, and anoxic and ischemic damage caused by heart attack and stroke.\n\nFull list on Google Scholar\n\n\n\n", "id": "25809437", "title": "Kenneth B. Storey"}
{"url": "https://en.wikipedia.org/wiki?curid=12318877", "text": "The Cosmic Serpent\n\nThe Cosmic Serpent: DNA and the Origins of Knowledge is a 1998 non-fiction book by anthropologist Dr. Jeremy Narby. \n\nNarby performed two years of field work in the Pichis Valley of the Peruvian Amazon researching the ecology of the Asháninka, an indigenous peoples in Peru.\n\nInvestigating the connections between shamanism and molecular biology, Narby hypothesizes that shamans may be able to access information at the molecular level through the ingestion of entheogens, specifically ayahuasca. Biophysicist Jacques Dubochet criticized Narby for not testing his hypothesis. \n\nNarby and three molecular biologists revisited the Peruvian Amazon to try to test the hypothesis, and their work is featured in the documentary film, \"Night of the Liana\".\n\n\n", "id": "12318877", "title": "The Cosmic Serpent"}
{"url": "https://en.wikipedia.org/wiki?curid=26865521", "text": "Fluorophore-assisted carbohydrate electrophoresis\n\nFluorophore assisted carbohydrate electrophoresis or FACE is a biochemical technology suited for detecting complex mixtures of high molecular weight N-glycans. A specialized form of this technique is the DSA-FACE, which is an acronym for DNA sequencer-assisted flurophore-assisted carbohydrate electrophoresis. DSA-FACE has higher resolution and sensitivity than classical FACE.\n", "id": "26865521", "title": "Fluorophore-assisted carbohydrate electrophoresis"}
{"url": "https://en.wikipedia.org/wiki?curid=961961", "text": "Gene gun\n\nA gene gun or a biolistic particle delivery system, originally designed for plant transformation, is a device for delivering exogenous DNA (transgenes) to cells. The payload is an elemental particle of a heavy metal coated with DNA (typically plasmid DNA). This technique is often simply referred to as biolistics.\n\nThis device is able to transform almost any type of cell, including plants, and is not limited to transformation of the nucleus; it can also transform organelles, including plastids.\n\nThe gene gun was originally a Crosman air pistol modified to fire dense tungsten particles. It was invented by John C Sanford, Ed Wolf and Nelson Allen at Cornell University, and Ted Klein of DuPont, between 1983 and 1986. The original target was onions (chosen for their large cell size) and it was used to deliver particles coated with a marker gene. Genetic transformation was then proven when the onion tissue expressed the gene.\n\nThe earliest custom manufactured gene guns (fabricated by Nelson Allen) used a 22 caliber nail gun cartridge to propel an extruded polyethylene cylinder (bullet) down a 22 cal. Douglas barrel. A droplet of the tungsten powder and genetic material was placed on the bullet and shot down the barrel at a lexan \"stopping\" disk with a petri dish below. The bullet welded to the disk and the genetic material blasted into the sample in the dish with a doughnut effect (devastation in the middle, a ring of good transformation and little around the edge). The gun was connected to a vacuum pump and was under vacuum while firing. The early design was put into limited production by a Rumsey-Loomis (a local machine shop then at Mecklenburg Rd in Ithaca, NY, USA). Later the design was refined by removing the \"surge tank\" and changing to nonexplosive propellants. DuPont added a plastic extrusion to the exterior to visually improve the machine for mass production to the scientific community. Biorad contracted with Dupont to manufacture and distribute the device. Improvements include the use of helium propellant and a multi-disk-collision delivery mechanism. Other heavy metals such as gold and silver are also used. Gold may be favored because it has better uniformity than tungsten and tungsten can be toxic to cells, but its use may be limited due to availability and cost.\n\nA construct is a piece of DNA inserted into the target's genome, including parts that are intended to be removed later. All biolistic transformations require a construct to proceed and while there is great variation among biolistic constructs, they can be broadly sorted into two categories: those which are designed to transform eukaryotic nuclei, and those designed to transform prokaryotic-type genomes such as mitochondria, plasmids or plastids. \n\nThose meant to transform prokaryotic genomes generally have the gene or genes of interest, at least one promoter and terminator sequence, and a reporter gene; which is a gene used to enable detection or removal of those cells which didn't integrate the construct into their DNA. These genes may each have their own promoter and terminator, or be grouped to produce multiple gene products from one transcript, in which case binding sites for translational machinery should be placed between each to ensure maximum translational efficiency. In any case the entire construct is flanked by regions called border sequences which are similar in sequence to locations within the genome, this allows the construct to target itself to a specific point in the existing genome.\n\nConstructs meant for integration into a eukaryotic nucleus follow a similar pattern except that: the construct contains no border sequences because the sequence rearrangement that prokaryotic constructs rely on rarely occurs in eukaryotes; and each gene contained within the construct must be expressed by its own copy of a promoter and terminator sequence.\n\nThough the above designs are generally followed, there are exceptions. For example, the construct might include a Cre-Lox system to selectively remove inserted genes; or a prokaryotic construct may insert itself downstream of a promoter, allowing the inserted genes to be governed by a promoter already in place and eliminating the need for one to be included in the construct.\n\nGene guns are so far mostly used with plant cells. However, there is much potential use in humans and other animals as well.\n\nThe target of a gene gun is often a callus of undifferentiated plant cells or a group of immature embryos growing on gel medium in a Petri dish. After the DNA-coated gold particles have been delivered to the cells, the DNA is used as a template for transcription (transient expression) and sometimes it integrates into a plant chromosome ('stable' transformation)\n\nIf the delivered DNA construct contains a selectable marker, then stably transformed cells can be selected and cultured using tissue culture methods. For example, if the delivered DNA construct contains a gene that confers resistance to an antibiotic or herbicide, then stably transformed cells may be selected by including that antibiotic or herbicide in the tissue culture media. \n\nTransformed cells can be treated with a series of plant hormones, such as auxins and gibberellins, and each may divide and differentiate into the organized, specialized, tissue cells of an entire plant. This capability of total re-generation is called totipotency. The new plant that originated from a successfully transformed cell may have new traits that are heritable. The use of the gene gun may be contrasted with the use of \"Agrobacterium tumefaciens\" and its Ti plasmid to insert DNA into plant cells. See transformation for different methods of transformation in different species.\n\nGene guns have also been used to deliver DNA vaccines.\n\nThe delivery of plasmids into rat neurons through the use of a gene gun, specifically DRG neurons, is also used as a pharmacological precursor in studying the effects of neurodegenerative diseases such as Alzheimer's disease.\n\nThe gene gun has become a common tool for labeling subsets of cells in cultured tissue. In addition to being able to transfect cells with DNA plasmids coding for fluorescent proteins, the gene gun can be adapted to deliver a wide variety of vital dyes to cells.\n\nGene gun bombardment has also been used to transform \"Caenorhabditis elegans\", as an alternative to microinjection.\n\nBiolistics has proven to be a versatile method of genetic modification and it is generally preferred to engineer transformation-resistant crops, such as cereals. Notably, \"Bt\" maize is a product of biolistics. Plastid transformation has also seen great success with particle bombardment when compared to other current techniques, such as \"Agrobacterium\" mediated transformation, which have difficulty targeting the vector to and stably expressing in the chloroplast. In addition, there are no reports of a chloroplast silencing a transgene inserted with a gene gun. Additionally, with only one firing of a gene gun, a skilled technician can generate two transformed organisms. This technology has even allowed for modification of specific tissues \"in situ\", although this is likely to damage large numbers of cells and transform only some, rather than all, cells of the tissue.\n\nBiolistics introduces DNA randomly into the target cells. Thus the DNA may be transformed into whatever genomes are present in the cell, be they nuclear, mitochondrial, plasmid or any others, in any combination, though proper construct design may mitigate this. Another issue is that the gene inserted may be overexpressed when the construct is inserted multiple times in either the same or different locations of the genome. This is due to the ability of the constructs to give and take genetic material from other constructs, causing some to carry no transgene and others to carry multiple copies; the number of copies inserted depends on both how many copies of the transgene an inserted construct has, and how many were inserted. Also, because eukaryotic constructs rely on illegitimate recombination, a process by which the transgene is integrated into the genome without similar genetic sequences, and not homologous recombination, which inserts at similar sequences, they cannot be targeted to specific locations within the genome, unless the transgene is co-delivered with genome editing reagents. \n\n", "id": "961961", "title": "Gene gun"}
{"url": "https://en.wikipedia.org/wiki?curid=26068545", "text": "Selection and amplification binding assay\n\nSelection and amplification binding assay (SAAB) is a molecular biology technique typically used to find the DNA binding site for proteins. It was developed by T. Keith Blackwell and Harold M. Weintraub in 1990.\n\nSAAB experimental procedure consists of several steps, depending upon the knowledge available about the binding site. A typical SAAB consists of the following steps:\n\n\nQuox1 is a homeobox gene involved in the regulation of patterns of development (morphogenesis) in animals, fungi and plants and was originally isolated from cDNA library of five week quail embryo. It is the only gene in the hox family that has been found to express in both prosencephalon and mesencephalon involved in the differentiation of the central and peripheral nerve cells. The optimal DNA binding site for Quox1 or its mammalian homologs was identified by SAAB in 2004. The amplified Quox1 DNA fragment obtained from PCR amplification from a human embryo cDNA librarywas digested with EcoRV and XhoI and cloned into the SmaI and XhoI restriction site of the expression vector pGEMEXxBal. The recombinant plasmids were transformed into competent Escherichia coli strain BL21 and Quox1 fusion proteins were isolated by chromatographic techniques.\n\nThe radio labeled probe was incubated with 25 pmol of purified Quox1 homeodomain fusion protein in binding buffer for EMSA. The protein bound DNA was detected by autoradiography, and the bands representing protein–DNA complexes were excised from the gel and the eluted DNA were amplified by PCR using primers complementary to the 20 bp nonrandom flanking sequences. After 5 set of the same procedure,the purified DNA was cloned into pMD 18T and sequenced. Finally the sequence CAATC was identified as the consensus binding sequence for Quox1 homeodomain.\n\nBy combining the power of random-sequence selection with pooled sequencing, the SAAB imprint assay makes possible simultaneous screening of a large number of binding site mutants. SAAB also allows the identification of sites with high relative binding affinity since the competition is inherent in the protocol. It can also identify site positions that are neutral or specific bases that can interfere with binding, such as a T at - 4 in the E47 half-site. We can apply the technique to less affinity binding sequence also, provided to keep high concentration of binding protein at each step of binding. It is also possible to identify the binding site even if both the protein and sequence is not known.\n", "id": "26068545", "title": "Selection and amplification binding assay"}
{"url": "https://en.wikipedia.org/wiki?curid=16136345", "text": "European Working Group for Legionella Infections\n\nThe European Working Group for Legionella Infections (EWGLI) was formed in 1986. Its members are scientists with an interest in improving knowledge and information on the epidemiological and microbiological (clinical and environmental) aspects of legionnaires' disease. This is achieved through international surveillance of the disease, as well as developments in diagnosis, management and treatment methods.\nEWGLI is based at the Health Protection Agency - Centre for Infections, Department of Respiratory Diseases (Legionella Section), 61 Colindale Avenue, London NW9 5EQ, United Kingdom\n\nExternal Quality Assessment (EQA) schemes are an important component in the operation of EWGLI. The following schemes are available:\n\n\n", "id": "16136345", "title": "European Working Group for Legionella Infections"}
{"url": "https://en.wikipedia.org/wiki?curid=2921986", "text": "Monopolin\n\nMonopolin is a protein complex that in yeast is composed of the four proteins CSM1, HRR25, LRS4, and MAM1. Monopolin is required for the segregation of homologous centromeres to opposite poles of a dividing cell during anaphase I of meiosis.\n", "id": "2921986", "title": "Monopolin"}
{"url": "https://en.wikipedia.org/wiki?curid=14101836", "text": "Nif gene\n\nThe \"nif\" genes are genes encoding enzymes involved in the fixation of atmospheric nitrogen into a form of nitrogen available to living organisms. The primary enzyme encoded by the \"nif\" genes is the nitrogenase complex which is in charge of converting atmospheric nitrogen (N) to other nitrogen forms such as ammonia which the organism can use for various purposes. Besides the nitrogenase enzyme, the \"nif\" genes also encode a number of regulatory proteins involved in nitrogen fixation. The \"nif\" genes are found in both free-living nitrogen-fixing bacteria and in symbiotic bacteria associated with various plants. The expression of the \"nif\" genes is induced as a response to low concentrations of fixed nitrogen and oxygen concentrations (the low oxygen concentrations are actively maintained in the root environment of host plants). The first Rhizobium genes for nitrogen fixation (nif) and for nodulation (nod) were cloned in the early 1980s by Gary Ruvkun and Sharon R. Long in Frederick M. Ausubel's laboratory.\n\nIn most bacteria, regulation of \"nif\" genes transcription is done by the nitrogen sensitive NifA protein. When there isn't enough fixed nitrogen available for the organism's use, NtrC triggers NifA expression, and NifA activates the rest of the \"nif\" genes. If there is a sufficient amount of reduced nitrogen or oxygen is present, another protein is activated: NifL. NifL inhibits NifA activity resulting in the inhibition of nitrogenase formation. NifL is regulated by the products of \"glnD\" and \"glnK\". The \"nif\" genes can be found on bacterial chromosomes, but in symbiotic bacteria they are often found on plasmids or symbiosis islands with other genes related to nitrogen fixation (such as the \"nod\" genes).\n\nThe expression and regulation of \"nif\" genes, while sharing common features in all or most of the nitrogen-fixing organisms in nature, have distinct characters and qualities that differ from one diazotroph to another. Examples of \"nif\" gene structure and regulation in different diazotrophs include:\n\n\"Klebsiella pneumoniae\"—a free-living anaerobic nitrogen-fixing bacterium. It contains a total of 20 \"nif\" genes located on the chromosome in a 24-Kb region. \"nifH\", \"nifD\", and \"nifK\" encode the nitrogenase subunits, while \"nifE\", \"nifN\", \"nifU\", \"nifS\", \"nifV\", \"nifW\", \"nifX\", \"nifB\", and \"nifQ\" encode proteins involved the assembly and incorporation of iron and molybdenum atoms into the nitrogenase subunits. \"nifF\" and \"nifJ\" encode proteins related to electron transfer taking place in the reduction process and \"nifA\" and \"nifL\" are regulatory proteins in charge of regulating the expression of the other \"nif\" genes.\n\"Rhodospirillum rubrum\"—a free-living anaerobic photosynthetic bacterium which, in addition to the transcriptional controls described above, regulates expression of the \"nif\" genes also in a metabolic way through a reversible ADP-ribosylation of a specific arginine residue in the nitrogenase complex. The ribosylation takes place when reduced nitrogen is present and it causes a barrier in the electron transfer flow and thereby inactivates nitrogenase activity. The enzymes catalyzing the ribosylation are called DraG and DraT.\n\n\"Rhodobacter capsulatus\"—a free-living anaerobic phototroph containing a transcriptional \"nif\" gene regulatory system. \"R. capsulatus\" regulates \"nif\" gene expression through \"nifA\" in the same manner described before, but it uses a different \"nifA\" activator which initiates the NtrC. NtrC activates a different expression of \"nifA\" and the other \"nif\" genes.\n\n\"Rhizobium\" spp.—Gram-negative, symbiotic nitrogen fixing bacteria that usually form a symbiotic relationship with legume species. In some rhizobia, the \"nif\" genes are located on plasmids called 'sym plasmids' (sym = symbiosis) which contain genes related to nitrogen fixation and metabolism, while the chromosomes contain most of the housekeeping genes of the bacteria. Regulation of the \"nif\" genes is at the transcriptional level and is dependent on colonization of the plant host.\n\n\n", "id": "14101836", "title": "Nif gene"}
{"url": "https://en.wikipedia.org/wiki?curid=25664757", "text": "Nonsynonymous substitution\n\nA nonsynonymous substitution is a nucleotide mutation that alters the amino acid sequence of a protein. Nonsynonymous substitutions differ from synonymous substitutions, which do not alter amino acid sequences and are silent mutations. As nonsynonymous substitutions result in a biological change in the organism, they are subject to natural selection.\n\nNonsynonymous substitutions at a certain loci can be compared to the synonymous substitutions at that loci to obtain the K/K ratio. This ratio is used to measure the evolutionary rate of gene sequences. If a gene has lower levels of nonsynonymous than synonymous nucleotide substitution, then it can be inferred to be functional because a K/K ratio < 1 is a hallmark of sequences that are being constrained to code for proteins.\n\nThere are several common types of nonsynonymous substitutions.\n\nMissense mutations are nonsynonymous substitutions that arise from point mutations, mutations in a single nucleotide that result in the substitution of a different amino acid, resulting in a change to the protein encoded.\n\nNonsense mutations are nonsynonymous substitutions that arise when a mutation in the DNA sequence causes a protein to terminate prematurely by changing the original amino acid to a stop codon. Another type of mutation that deals with stop codons is known as a readthrough mutation, which occurs when a stop codon is exchanged for an amino acid codon, causing the protein to be longer than specified.\n\nStudies have shown that diversity among nonsynonymous substitutions is significantly lower than among synonymous substitutions. This is due to the fact that nonsynonymous substitutions are subject to much higher selective pressures than synonymous mutations. Motoo Kimura (1968) determined that calculated mutation rates were impossibly high, unless most of the mutations that occurred were either neutral or \"nearly neutral\". He determined that if this were true, genetic drift would be a more powerful factor in molecular evolution than natural selection. The \"nearly neutral\" theory proposes that molecular evolution acting on nonsynonymous substitutions is driven by mutation, genetic drift, and very weak natural selection, and that it is extremely sensitive to population size. In order to determine whether natural selection is taking place at a certain loci, the McDonald–Kreitman test can be performed. The test consists of comparing ratios of synonymous and nonsynonymous genes between closely related species to the ratio of synonymous to nonsynonymous polymorphisms within species. If the ratios are the same, then Neutral theory of molecular evolution is true for that loci, and evolution is proceeding primarily through genetic drift. If there are more nonsynonymous substitutions between species than within a species, positive natural selection is occurring on beneficial alleles and natural selection is taking place. Nonsynonymous substitutions have been found to be more common in loci involving pathogen resistance, reproductive loci involving sperm competition or egg-sperm interactions, and genes that have replicated and gained new functions, indicating that positive selection is taking place.\n\nResearch on accurately modeling rates of mutation has been conducted for many years. A recent paper by Ziheng Yang and Rasmus Nielsen compared various methods and developed a new modeling method. They found that the new method was preferable for its smaller biases, which make it useful for large scale screening, but that the maximum-likelihood model was preferable in most scenarios because of its simplicity, and its flexibility in comparing multiple sequences while taking into account phylogeny.\n\nFurther research by Yang and Nielsen found that nonsynonymous to synonymous substitution ratios varied across loci in differing evolutionary lineages. During their study of nuclear loci of primates, even-toed ungulates, and rodents, they found that the ratio varied significantly at 22 of the 48 loci studied. This result provides strong evidence against a strictly neutral theory of molecular evolution, which states that mutations are mostly neutral or deleterious, and provides support for theories that include advantageous mutations.\n\n\n", "id": "25664757", "title": "Nonsynonymous substitution"}
{"url": "https://en.wikipedia.org/wiki?curid=7247837", "text": "Nesfatin-1\n\nNesfatin-1 is a neuropeptide produced in the hypothalamus of mammals. It participates in the regulation of hunger and fat storage. Increased nesfatin-1 in the hypothalamus contributes to diminished hunger, a 'sense of fullness', and a potential loss of body fat and weight.\n\nA study of metabolic effects of nesfatin-1 in rats have been done in which subjects administered nesfatin-1 ate less, used more stored fat and became more active. Nesfatin-1-induced inhibition of feeding may be mediated through the inhibition of orexigenic neurons. In addition, the protein stimulated insulin secretion from the pancreatic beta cells of both rats and mice.\n\nNesfatin-1 is a polypeptide encoded in the N-terminal region of the protein precursor, Nucleobindin2 (NUCB2).\nRecombinant human Nesfatin-1 is a 9.7 kDa protein containing 82 amino acid residues. Nesfatin-1 is expressed in the hypothalamus, in other areas of the brain, and in pancreatic islets, gastric endocrine cells and adipocytes.\n\nNesfatin/NUCB2 is expressed in the appetite-control hypothalamic nuclei such as paraventricular nucleus (PVN), arcuate nucleus (ARC), supraoptic nucleus (SON) of hypothalamus, lateral hypothalamic area (LHA), and zona incerta in rats. Nesfatin-1 immunoreactivity was also found in the brainstem nuclei such as nucleus of the solitary tract (NTS) and Dorsal nucleus of vagus nerve.\n\nNesfatin-1 can cross the blood–brain barrier without saturation.\n\nThe receptors within the brain are in the hypothalamus and the solitary nucleus, where nesfatin-1 is believed to be produced via peroxisome proliferator-activated receptors (PPARs). It appears there is a relationship between nesfatin-1 and cannabinoid receptors. Nesfatin-1-induced inhibition of feeding may be mediated through the inhibition of orexigenic NPY neurons.\n\nNesfatin/NUCB2 expression has been reported to be modulated by starvation and re-feeding in the Paraventricular nucleus (PVN) and supraoptic nucleus (SON) of the brain. Nesfatin-1 influences the excitability of a large proportion of different subpopulations of neurons located in the PVN. It is also reported that magnocellular oxytocin neurons are activated during feeding, and ICV infusion of oxytocin antagonist increases food intake, indicating a possible role of oxytocin in the regulation of feeding behavior. In addition, it is proposed that feeding-activated nesfatin-1 neurons in the PVN and SON could play an important role in the postprandial regulation of feeding behavior and energy homeostasis.\n\nNesfatin-1 immunopositive neurons are also located in the arcuate nucleus (ARC). Nesfatin-1 immunoreactive neurons in the ARC are activated by simultaneous injection of ghrelin and desacyl ghrelin, nesfatin-1 may be involved in the desacyl ghrelin-induced inhibition of the orexigenic effect of peripherally administered ghrelin in freely fed rat.\n\nNesfatin-1 was co-expressed with melanin concentrating hormone (MCH) in tuberal hypothalamic neurons. Nesfatin-1 co-expressed in MCH neurons may play a complex role not only in the regulation of food intake, but also in other essential integrative brain functions involving MCH signaling, ranging from autonomic regulation, stress, mood, cognition to sleep.\n\nThere is growing evidence that nesfatin-1 may play an important role in the regulation of food intake and glucose homeostasis. For instance, continuous infusion of nesfatin-1 into the third brain ventricle significantly decreased food intake and body weight gain in rats. In previous studies, we have also shown that plasma nesfatin-1 levels were elevated in patients with type 2 diabetes mellitus (T2DM) and associated with BMI, plasma insulin, and the homeostasis model assessment of insulin resistance.\n\nIt was found that central nesfatin-1 resulted in a marked suppression of hepatic PEPCK mRNA and protein levels in both standard diet (SD) and high fat diet (HFD) rats but failed to alter glucose 6-phosphatase (G-6-Pase) activity and protein expression. Central nesfatin-1 appeared to antagonize the effect of HFD on increasing PEPCK gene expression in vivo. In agreement with decreasing PEPCK gene expression, central nesfatin-1 also resulted in a reduced PEPCK enzyme activity, further confirming that it affected PEPCK rather than G-6-Pase.\n\nThe part of the glucose entering the liver is phosphorylated by glucokinase and then dephosphorylated by G-6-Pase. This futile cycle between glucokinase and G-6-Pase is named glucose cycling, and it accounts for the difference between the total flux through G-6-Pase and glucose production. G-6-Pase catalyzes the last step in both gluconeogenesis and glycogenolysis, and PEPCK is responsible only for gluconeogenesis. In this study, central nesfatin-1 led to a marked suppression of hepatic PEPCK protein and activity, but failed to alter hepatic G-6-Pase activity, suggesting that PEPCK may be more sensitive to short-term central nesfatin-1 exposure than G-6-Pase. In addition, we also considered that the suppression of HGP by central nesfatin-1 was dependent on an inhibition of the substrate flux through G-6-Pase and not on a decrease in the amount of G-6-Pase enzyme. Thus, in SD and HFD rats, central nesfatin-1 may have decreased glucose production mainly via decreasing gluconeogenesis and PEPCK activity.\n\nRecently, it has been reported that ICV nesfatin-1 produced a dose-dependent delay of gastric emptying.\n\nTo further delineate the mechanism by which central nesfatin-1 modulates glucose homeostasis, we assessed the effects of central nesfatin-1 on the phosphorylation of several proteins in the INSR → IRS-1 → AMPK → Akt signaling cascade in the liver. We found that central nesfatin-1 significantly augmented InsR and IRS-1 tyrosine phosphorylation. These results demonstrated that central nesfatin-1 in both SD and HFD rats resulted in a stimulation of liver insulin signaling that could account for the increased insulin sensitivity and improving glucose metabolism.\n\nAMPK is a key regulator of both lipid and glucose metabolism. It has been referred to as a metabolic master switch, because its activity is regulated by the energy status of the cell. In this study, we demonstrate that central nesfatin-1 resulted in increased phosphorylation of AMPK accompanied by a marked suppression of hepatic PEPCK activity, mRNA, and protein levels in both SD and HFD rats. Notably, central nesfatin-1 appears to prevent the obesity-driven decrease in phospho-AMPK levels in HFD-fed rats. Because hepatic AMPK controls glucose homeostasis mainly through the inhibition of gluconeogenic gene expression and glucose production, the suppressive effect of central nesfatin-1 on the HGP (Hepatic Glucose Production) can be attributed partly to its ability to suppress the expression of PEPCK mRNA and protein through AMPK activation. Furthermore, the activation of AMPK has been shown to enhance glucose uptake in skeletal muscle. Therefore, increased AMPK phosphorylation by central nesfatin-1 may also have been responsible for the improved glucose uptake in muscle.\n\nAkt is a key effector of insulin-induced inhibition of HGP and stimulation of muscle glucose uptake. We therefore examined the effects of central nesfatin-1 on Akt phosphorylation in vivo. We found that central nesfatin-1 produced a pronounced increase in insulin-mediated phosphorylation of Akt in the liver of HFD-fed rats. This increase was paralleled by an increase in muscle glucose uptake and inhibition of HGP. This provided correlative evidence that Akt activation may be involved in nesfatin-1 signaling and its effects on glucose homeostasis and insulin sensitivity.\n\nThe mTOR pathway has emerged as a molecular mediator of insulin resistance, which can be activated by both insulin and nutrients. It is needed to fully activate AKT and consists of two discrete protein complexes, TORC1 and TORC2, only one of which, TORC1, binds rapamycin. In addition to mTOR, the TORC2 complex contains RICTOR, mLST8, and SIN1 and regulates insulin action and Akt phosphorylation. Thus, mTOR sits at a critical juncture between insulin and nutrient signaling, making it important both for insulin signaling downstream from Akt and for nutrient sensing. Until now, it has not been known whether nesfatin-1 affects activation of mTOR. To gain further insight into the mechanism underlying the insulin-sensitizing effects of ICV nesfatin-1, we assessed mTOR and TORC2 phosphorylation in liver samples of SD- and HFD-fed animals. Both mTOR and TORC2 phosphorylations were increased in livers from these rats, demonstrating activation of mTOR and TORC2 by central nesfatin-1 in vivo. As mTOR kinase activity is required for Akt phosphorylation, the observed increased Akt phosphorylation may have been caused by the concomitant activation of the mTOR/TORC2. Thus, it's postulated that the mTOR/TORC2 plays a role as a negative-feedback mechanism in the regulation of metabolism and insulin sensitivity mediated by central nesfatin-1.\n\n", "id": "7247837", "title": "Nesfatin-1"}
{"url": "https://en.wikipedia.org/wiki?curid=61255", "text": "Bacterial artificial chromosome\n\nA bacterial artificial chromosome (BAC) is a DNA construct, based on a functional fertility plasmid (or F-plasmid), used for transforming and cloning in bacteria, usually \"E. coli\". F-plasmids play a crucial role because they contain partition genes that promote the even distribution of plasmids after bacterial cell division. The bacterial artificial chromosome's usual insert size is 150–350 kbp. A similar cloning vector called a PAC has also been produced from the DNA of P1 bacteriophage.\n\nBACs are often used to sequence the genome of organisms in genome projects, for example the Human Genome Project. A short piece of the organism's DNA is amplified as an insert in BACs, and then sequenced. Finally, the sequenced parts are rearranged \"in silico\", resulting in the genomic sequence of the organism. BACs were replaced with faster and less laborious sequencing methods like whole genome shotgun sequencing and now more recently next-gen sequencing.\n\n\nBACs are now being utilized to a greater extent in modelling genetic diseases, often alongside transgenic mice. BACs have been useful in this field as complex genes may have several regulatory sequences upstream of the encoding sequence, including various promoter sequences that will govern a gene's expression level. BACs have been used to some degree of success with mice when studying neurological diseases such as Alzheimer's disease or as in the case of aneuploidy associated with Down syndrome. There have also been instances when they have been used to study specific oncogenes associated with cancers. They are transferred over to these genetic disease models by electroporation/transformation, transfection with a suitable virus or microinjection. BACs can also be utilized to detect genes or large sequences of interest and then used to map them onto the human chromosome using BAC arrays. BACs are preferred for these kind of genetic studies because they accommodate much larger sequences without the risk of rearrangement, and are therefore more stable than other types of cloning vectors.\n\nThe genomes of several large DNA viruses and RNA viruses have been cloned as BACs. These constructs are referred to as \"infectious clones\", as transfection of the BAC construct into host cells is sufficient to initiate viral infection. The infectious property of these BACs has made the study of many viruses such as the herpesviruses, poxviruses and coronaviruses more accessible. Molecular studies of these viruses can now be achieved using genetic approaches to mutate the BAC while it resides in bacteria. Such genetic approaches rely on either linear or circular targeting vectors to carry out homologous recombination.\n\n\n", "id": "61255", "title": "Bacterial artificial chromosome"}
{"url": "https://en.wikipedia.org/wiki?curid=24172579", "text": "Activity-regulated cytoskeleton-associated protein\n\nActivity-regulated cytoskeleton-associated protein is a plasticity protein that in humans is ecoded by the ARC gene. It was first characterized in 1995. Arc is a member of the immediate-early gene (IEG) family, a rapidly activated class of genes functionally defined by their ability to be transcribed in the presence of protein synthesis inhibitors. Arc mRNA is localized to activated synaptic sites in an NMDA receptor-dependent manner, where the newly translated protein is believed to play a critical role in learning and memory-related molecular processes. Arc is widely considered to be an important protein in neurobiology because of its activity regulation, localization, and utility as a marker for plastic changes in the brain. Dysfunctions in the production of Arc protein has been implicated as an important factor in understanding of various neurological conditions including amnesia, Alzheimer's disease, Autism spectrum disorders, and Fragile X syndrome. Along with other IEGs such as zif268 and Homer 1a, Arc is also a significant tool for systems neuroscience as illustrated by the development of the \"cellular compartment analysis of temporal activity by fluorescence in situ hybridization\", or catFISH technique (see fluorescent in situ hybridization).\n\nThe Arc gene, located on chromosome 15 in the mouse, chromosome 7 in the rat, and chromosome 8 in the human genome, is conserved across vertebrate species and has low sequence homology to spectrin, a cytoskeletal protein involved in forming the actin cellular cortex. A number of promoter and enhancer regions have been identified that mediate activity-dependent Arc transcription: a serum response element (SRE; see serum response factor) at ~1.5 kb upstream of the initiation site. a second SRE at ~6.5 kb; and a synaptic activity response element (SARE) sequence at ~7 kb upstream that contains binding sites for cyclic AMP response element-binding protein (CREB), myocyte enhancer factor 2 (MEF2), and SRF.\n\nThe 3' UTR of the mRNA contains a cis-acting element required for the localization of Arc to neuronal dendrites, as well as sites for two exon junction complexes (EJCs) that make Arc a natural target for nonsense mediated decay (NMD). Also important for translocation of cytoplasmic Arc mRNA to activated synapses is an 11 nucleotide binding site for heterogeneous nuclear ribonucleoprotein A2 (hnRNP A2).\n\nOnce transported, the translated protein is 396 residues in length, with an N-terminus located at amino acids 1-25, a C-terminus at 155-396 (note that the spectrin homology located at 228-380 within the C-terminal), and a putative coiled coil domain at amino acids 26-154. Additionally, the protein has binding sites for endophilin 3 and dynamin 2 at amino acids 89-100 and 195-214, respectively. While Arc mRNA is subject to degradation by NMD, the translated protein contains a PEST sequence at amino acids 351-392, indicating proteasome-dependent degradation. The translated protein can be visualized with an immunoblot as a band at 55 kDa.\n\nFollowing transcription, Arc mRNA is transported out of the nucleus and localized to neuronal dendrites and activated synapses, a process dependent on the 3' UTR, polymerization of actin, and ERK phosphorylation. The mRNA (and aggregate protein) is carried along microtubules radiating out from the nucleus by kinesin (specifically KIF5) and likely translocated into dendritic spines by the actin-based motor protein myosin-Va. Arc has been shown to be associated with polyribosomes at synaptic sites,and is translated in isolated synaptoneurosomal fractions \"in vitro\" indicating that the protein is likely locally translated \"in vivo\".\n\nSynaptically localized Arc protein interacts with dynamin and endophilin, proteins involved in clathrin-mediated endocytosis, and facilitates the removal of AMPA receptors from the plasma membrane. Consistent with this, increased Arc levels reduce AMPA currents, while Arc KOs display increases in surface AMPA expression.\n\nArc is critical as a ubiquitous signaling factor in early embryonic development and is required for growth and patterning during gastrulation. The first knockouts (KOs) for Arc were therefore incompatible with life. Subsequent efforts produced homozygous knockout mice by targeting the entire Arc gene rather than portions of the coding region, eliminating dominant negative effects. These animals proved viable and exhibit no gross malformations in neuronal architecture, but express higher levels of the GluR1 subunit and increased miniature excitatory postsynaptic currents (mEPSCs) in addition to displaying deficiencies in long-term memory.\n\nThe Arc transcript is dependent upon activation of the mitogen-activated protein kinase or MAP kinase (MAPK) cascade, a pathway important for regulation of cell growth and survival. Extracellular signaling to neuronal dendrites activates postsynaptic sites to increase Arc levels through a wide variety of signaling molecules, including mitogens such as epidermal growth factor (EGF), nerve growth factor (NGF), and brain-derived neurotrophic factor (BDNF), glutamate acting at NMDA receptors, dopamine through activation of the D1 receptor subtype, and dihydroxyphenylglycine (DHPG). The common factor for these signaling molecules involves activation of cyclic-AMP and its downstream target protein kinase A (PKA). As such, direct pharmacological activation of cAMP by forskolin or 8-Br-cAMP robustly increases Arc levels while H89, a PKA antagonist, blocks these effects as does further downstream blockade of mitogen-activated protein kinase kinase [sic] (MEK). Note that the MAPK cascade is a signaling pathway involving multiple kinases acting sequentially [MAPKKK--> MAPKK--> MAPK].\n\nMAPK is able to enter the nucleus and perform its phosphotransferase activity on a number of gene regulatory components that have implications for the regulation of immediate-early genes. Several transcription factors are known to be involved in regulating the Arc gene (see above), including serum response factor (SRF), CREB, MEF2, and zif268.\n\nChanges in Arc mRNA and/or protein are correlated with a number of behavioral changes including cued fear conditioning, contextual fear conditioning, spatial memory, operant conditioning, and inhibitory avoidance. The mRNA is notably upregulated following electrical stimulation in LTP-induction procedures such as high frequency stimulation (HFS), and is massively and globally induced by maximal electroconvulsive shock (MECS).\n", "id": "24172579", "title": "Activity-regulated cytoskeleton-associated protein"}
{"url": "https://en.wikipedia.org/wiki?curid=10099472", "text": "Allele-specific oligonucleotide\n\nAn allele-specific oligonucleotide (ASO) is a short piece of synthetic DNA complementary to the sequence of a variable target DNA. It acts as a probe for the presence of the target in a Southern blot assay or, more commonly, in the simpler Dot blot assay. It is a common tool used in genetic testing, forensics, and Molecular Biology research.\n\nAn ASO is typically an oligonucleotide of 15–21 nucleotide bases in length. It is designed (and used) in a way that makes it specific for only one version, or allele, of the DNA being tested. The length of the ASO, which strand it is chosen from, and the conditions by which it is bound to (and washed from) the target DNA all play a role in its specificity. These probes can usually be designed to detect a difference of as little as 1 base in the target's genetic sequence, a basic ability in the assay of single-nucleotide polymorphisms (SNPs), important in genotype analysis and the Human Genome Project. To be detected after it has bound to its target, the ASO must be labeled with a radioactive, enzymatic, or fluorescent tag. The Illumina Methylation Assay technology takes advantage of ASO to detect one base pair difference (cytosine versus thymine) to measure methylation at a specific CpG site.\n\nThe human disease sickle cell anemia is caused by a genetic mutation in the codon for the sixth amino acid of the blood protein beta-hemoglobin. The normal DNA sequence G-A-G codes for the amino acid glutamate, while the mutation changes the middle adenine to a thymine, leading to the sequence G-T-G (G-U-G in the mRNA). This altered sequence substitutes a valine into the final protein, distorting its structure.\n\nTo test for the presence of the mutation in a DNA sample, an ASO probe would be synthesized to be complementary to the altered sequence, here labeled as \"S\". As a control, another ASO would be synthesized for the normal sequence \"A\". Each ASO is fully complementary to its target sequence (and will bind strongly), but has a single mismatch against its non-target allele (leading to weaker interaction). The first diagram shows how the \"S\" probe is fully complementary to the \"S\" target (top), but is partially mismatched against the \"A\" target (bottom).\nA segment of the beta-hemoglobin genes in the sample DNA(s) would be amplified by PCR, and the resulting products applied to duplicate support membranes as Dot blots. The sample's DNA strands are separated with alkali, and each ASO probe is applied to a different blot. After hybridization, a washing protocol is used which can discriminate between the fully complementary and the mismatched hybrids. The mismatched ASOs are washed off of the blots, while the matched ASOs (and their labels) remain.\n\nIn the second diagram, six samples of amplified DNA have been applied to each of the two blots. Detection of the ASO label that remains after washing allows a direct reading of the genotype of the samples, each with two copies of the beta-hemoglobin gene. Samples 1 and 4 only have the normal \"A\" allele, while samples 3 and 5 have both the \"A\" and \"S\" alleles (and are therefore heterozygous carriers of this recessive mutation). Samples 2 and 6 have only the \"S\" allele, and would be affected by the disease. The small amount of 'cross hybridization' shown is typical, and is considered in the process of interpreting the final results.\n\nASO analysis is only one of the methods used to detect genetic polymorphisms. Direct DNA sequencing is used to initially characterize the mutation, but is too laborious for routine screening. An earlier method, Restriction Fragment Length Polymorphism (RFLP) didn't need to know the sequence change beforehand, but required that the mutation affect the cleavage site of a Restriction Enzyme. The RFLP assay was briefly adapted to the use of oligonucleotide probes, but this technique was quickly supplanted by ASO analysis of polymerase chain reaction (PCR) amplified DNA. The PCR technique itself has been adapted to detect polymorphisms, as allele-specific PCR. However, the simplicity and versatility of the combined PCR/ASO method has led to its continued use, including with non-radioactive labels, and in a \"reverse dot blot\" format where the ASO probes are bound to the membrane and the amplified sample DNA is used for hybridization.\n\nThe use of synthetic oligonucleotides as specific probes for genetic sequence variations was pioneered by R. Bruce Wallace, working at the City of Hope National Medical Center in Duarte, California. In 1979 Wallace and his coworkers reported the use of ASO probes to detect variations in a single-stranded bacterial virus, and later applied the technique to cloned human genes. In 1983 and 1985 Wallace's lab reported the detection of the mutation for sickle cell anemia in samples of whole genomic DNA, although this application was hampered by the small amount of label that could be carried by the ASO.\n\nFortunately PCR, a method to greatly amplify a specific segment of DNA, was also reported in 1985. In less than a year PCR had been paired with ASO analysis. This combination solved the problem of ASO labeling, since the amount of target DNA could be amplified over a million-fold. Also, the specificity of the PCR process itself could be added to that of the ASO probes, greatly reducing the problem of spurious binding of the ASO to non-target sequences. The combination was specific enough that it could be used in a simple Dot blot, avoiding the laborious and inefficient Southern blot method.\n\n", "id": "10099472", "title": "Allele-specific oligonucleotide"}
{"url": "https://en.wikipedia.org/wiki?curid=12678511", "text": "BglII\n\n\"Bgl\"II (pronounced \"begel two\") is a type II restriction endonuclease enzyme isolated from certain strains of \"Bacillus globigii\".\n\nThe principal function of restriction enzymes is the protection of the host genome against foreign DNA, but they may also have some involvement in recombination and transposition.\n\nLike most type II restriction enzymes, \"Bgl\"II consists of two identical subunits that form a homodimer around the DNA double helix. Each monomer is 223 amino acids and symmetrically bind both sides of the unique palindromic nucleotide sequence AGATCT, cleaving the scissile phosphodiester bond between the first Adenine and Guanine nucleotides on both strands of the DNA molecule, creating sticky ends with 5' end overhangs.\n\nBeing a type II restriction enzyme, \"Bgl\"II does not require ATP (adenosine triphosphate) for its enzymatic function, but only requires association with a divalent metal cation, most likely Mg. Unlike other restriction enzymes of its class, \"Bgl\"II has been show to possess some unique structural characteristics, such as a β-sandwich subdomain, and appears to undergo a unique conformational change upon dimerization, but its overall structure and mechanism of catalysis remain consistent with other type II restriction enzymes.\n\nRestriction Endonuclease enzymes play a very important role in modern molecular cloning techniques. Because of their unique recognition/cut sites, restriction enzymes can be used to precisely cut DNA in specific locations in a controllable manner. Once cut, scientist can then insert the desired DNA fragment possessing \"sticky ends\" into a circular DNA molecule, and ligate them together to create an engineered cloning vector.\n\n\"Bgl\"II catalyses phosphodiester bond cleavage at the DNA backbone through a phosphoryl transfer to water. Studies on the mechanism of restriction enzymes have revealed several general features that seem to be true in almost all cases, although the actual mechanism for each enzyme is most likely some variation of this general mechanism. This mechanism requires a base to generate the hydroxide ion from water, which will act as the nucleophile and attack the phosphorus in the phosphodiester bond. Also required is a Lewis acid to stabilize the extra negative charge of the pentacoordinated transition state phosphorus, as well as a general acid or metal ion that stabilizes the leaving group (3’-O-).\n\nAlthough restriction endonucleases show little sequence similarity, crystal structures reveal that they all share a highly similar α/β core consisting of a six-stranded β-sheet flanked by five α-helicies, two of which mediate dimerization. This core carries the active site (catalytic center) and the residues that contact DNA in the major groove. \"Bgl\"II is unique in that its α/β core is augmented by a β-sandwich subdomain that has several projections that extend outward to grip the DNA, allowing \"Bgl\"II to completely encircle the DNA molecule. This atypical feature of \"Bgl\"II suggests a unique hinge motion for DNA binding and release.\nComparative structural studies of the free enzyme vs. the \"Bgl\"II-DNA complex showed that the enzyme opens by a dramatic scissor-like motion, accompanied by a complete rearrangement of the α-helicies at the dimer interface. These structural studies also revealed that within each monomer a set of residues lowers or raises to alternatively sequester or expose the active site residues. These dramatic differences in structure in the free vs. bound enzyme have yet to be observed in any other restriction endonuclease and may possibly represent a novel mechanism for capturing DNA that may extend to other proteins that encircle DNA.\n\nStructural studies of endonucleases have revealed a similar architecture for the [active site] with the residues following the weak consensus sequence (Glu/Asp)-X(9-20)-(Glu/Asp/Ser)-X-(Lys/Glu). \"Bgl\"II active site is similar to other endonucleases, but follows the sequence Asp 84-X9-Glu 93-X-Gln 95. In its active site there sits a divalent metal ion, most likely Mg, that allows interactions with Asp 84, Val 94, a phosphorus oxygen, and three water molecules. One of these water molecules, which we have labeled Nu, is well equipped to be the source of the hydroxide nucleophile because of its proximity to the scissile phosphate as well as a pKa lowered by its contact with the metal and its orientation fixed by a hydrogen bond with the side chain Oxygen of Gln 95.\n\n\nMolecular graphics images were produced using the UCSF Chimera package from the Resource for Biocomputing, Visualization, and Informatics at the University of California, San Francisco (supported by NIH P41 RR-01081).\n\n", "id": "12678511", "title": "BglII"}
{"url": "https://en.wikipedia.org/wiki?curid=22057524", "text": "Cap analysis gene expression\n\nCap analysis gene expression (CAGE) is a technique used in molecular biology to produce a snapshot of the 5′ end of the messenger RNA population in a biological sample. The small fragments (usually 27 nucleotides long) from the very beginnings of mRNAs (5' ends of capped transcripts) are extracted, reverse-transcribed to DNA, PCR amplified and sequenced. CAGE was first published by Hayashizaki, Carninci and co-workers in 2003.\nCAGE has been extensively used within the FANTOM research projects.\n\nThe output of CAGE is a set of short nucleotide sequences (often called \"tags\") with their observed counts.\nUsing a reference genome, a researcher can usually determine, with some confidence, the original mRNA (and therefore which gene) the tag was extracted from.\nCopy numbers of CAGE tags provide an easy way of digital quantification of the RNA transcript abundances in biological samples.\n\nUnlike a similar technique Serial Analysis of Gene Expression (SAGE, superSAGE) in which tags come from other parts of transcripts, CAGE is primarily used to locate exact transcription start sites in the genome. This knowledge in turn allows a researcher to investigate promoter structure necessary for gene expression.\n\nHowever, the CAGE protocol has a known bias with a nonspecific guanine (G) at the most 5′ end of the CAGE tags, which is attributed to the template-free 5′-extension during the first-strand cDNA synthesis. This would induce erroneous mapping of CAGE tags, for instance to nontranscribed pseudogenes. On the other hand, this addition of Gs was also utilised as a signal to filter more reliable TSS peaks.\n\nThe original CAGE method (Shiraki \"et al.\", 2003) was using CAP Trapper for capturing the 5′ ends, oligo-dT primers for synthesizing the cDNAs, the type IIs restriction enzyme MmeI for cleaving the tags, and the Sanger method for sequencing them.\n\nRandom reverse-transcription primers were introduced in 2006 by Kodzius \"et al.\" to better detect the non-polyadenylated RNAs.\n\nIn \"DeepCAGE\" (Valen \"et al.\", 2008), the tag concatemers were sequenced at a higher throughput on the 454 “\"next-generation\"” sequencing platform.\n\nIn 2008, barcode multiplexing was added to the DeepCAGE protocol (Maeda \"et al.\", 2008).\n\nIn \"nanoCAGE\" (Plessy \"et al.\", 2010), the 5′ ends or RNAs were captured with the template-switching method instead of CAP Trapper, in order to analyze smaller starting amounts of total RNA. Longer tags were cleaved with the type III restriction enzyme EcoP15I and directly sequenced on the Solexa (then Illumina) platform without concatenation.\n\nThe \"CAGEscan\" methodology (Plessy \"et al.\", 2010), where the enzymatic tag cleavage is skipped, and the 5′ cDNAs sequenced paired-end, was introduced in the same article to connect novel promoters to known annotations.\n\nWith \"HeliScopeCAGE\" (Kanamori-Katayama \"et al.\", 2011), the CAP-trapped CAGE protocol was changed to skip the enzymatic tag cleavage and sequence directly the capped 5′ ends on the HeliScope platform, without PCR amplification. It was then automated by Itoh \"et al.\" in 2012.\n\nIn 2012, the standard CAGE protocol was updated by Takahashi \"et al.\" to cleave tags with EcoP15I and sequence them on the Illumina-Solexa platform.\n\nIn 2013, Batut \"et al.\" combined CAP trapper, template switching, and 5′-phosphate-dependent exonuclease digestion in \"RAMPAGE\" to maximize promoter specificity.\n\nIn 2014, Murata \"et al.\" published the \"nAnTi-CAGE\" protocol, where capped 5′ ends are sequenced on the Illumina platform with no PCR amplification and no tag cleavage.\n\nIn 2017, Poulain \"et al.\" updated the \"nanoCAGE\" protocol to use the \"tagmentation\" method (based on Tn5 transposition) for multiplexing.\n\n\n", "id": "22057524", "title": "Cap analysis gene expression"}
{"url": "https://en.wikipedia.org/wiki?curid=16276793", "text": "Cell-free protein array\n\nCell-free protein array technology produces protein microarrays by performing in vitro synthesis of the target proteins from their DNA templates. This method of synthesizing protein microarrays overcomes the many obstacles and challenges faced by traditional methods of protein array production that have prevented widespread adoption of protein microarrays in proteomics. Protein arrays made from this technology can be used for testing protein–protein interactions, as well as protein interactions with other cellular molecules such as DNA and lipids. Other applications include enzymatic inhibition assays and screenings of antibody specificity.\n\nThe runaway success of DNA microarrays has generated much enthusiasm for protein microarrays. However, protein microarrays have not quite taken off as expected, even with the necessary tools and know-how from DNA microarrays being in place and ready for adaptation. One major reason is that protein microarrays are much more laborious and technically challenging to construct than DNA microarrays.\n\nThe traditional methods of producing protein arrays require the separate \"in vivo\" expression of hundreds or thousands of proteins, followed by separate purification and immobilization of the proteins on a solid surface. Cell-free protein array technology attempts to simplify protein microarray construction by bypassing the need to express the proteins in bacteria cells and the subsequent need to purify them. It takes advantage of available cell-free protein synthesis technology which has demonstrated that protein synthesis can occur without an intact cell as long as cell extracts containing the DNA template, transcription and translation raw materials and machinery are provided. Common sources of cell extracts used in cell-free protein array technology include wheat germ, \"Escherichia coli\", and rabbit reticulocyte. Cell extracts from other sources such as hyperthermophiles, hybridomas, Xenopus oocytes, insect, mammalian and human cells have also been used.\n\nThe target proteins are synthesized \"in situ\" on the protein microarray, directly from the DNA template, thus skipping many of the steps in traditional protein microarray production and their accompanying technical limitations. More importantly, the expression of the proteins can be done in parallel, meaning all the proteins can be expressed together in a single reaction. This ability to multiplex protein expression is a major time-saver in the production process.\n\nIn the \"in situ\" method, protein synthesis is carried out on a protein array surface that is pre-coated with a protein-capturing reagent or antibody. Once the newly synthesized proteins are released from the ribosome, the tag sequence that is also synthesized at the N- or C-terminus of each nascent protein will be bound by the capture reagent or antibody, thus immobilizing the proteins to form an array. Commonly used tags include polyhistidine (His)6 and glutathione s-transferase (GST).\n\nVarious research groups have developed their own methods, each differing in their approach, but can be summarized into 3 main groups.\nNAPPA uses DNA template that has already been immobilized onto the same protein capture surface. The DNA template is biotinylated and is bound to avidin that is pre-coated onto the protein capture surface. Newly synthesized proteins which are tagged with GST are then immobilized next to the template DNA by binding to the adjacent polyclonal anti-GST capture antibody that is also pre-coated onto the capture surface. The main drawback of this method is the extra and tedious preparation steps at the beginning of the process: (1) the cloning of cDNAs in an expression-ready vector; and (2) the need to biotinylate the plasmid DNA but not to interfere with transcription. Moreover, the resulting protein array is not ‘pure’ because the proteins are co-localized with their DNA templates and capture antibodies.\n\nUnlike NAPPA, PISA completely bypasses DNA immobilization as the DNA template is added as a free molecule in the reaction mixture. In 2006, another group refined and miniaturized this method by using multiple spotting technique to spot the DNA template and cell-free transcription and translation mixture on a high-density protein microarray with up to 13,000 spots. This was made possible by the automated system used to accurately and sequentially supply the reagents for the transcription/translation reaction occurs in a small, sub-nanolitre droplet.\n\nThis method is an adaptation of mRNA display technology. PCR DNA is first transcribed to mRNA, and a single-stranded DNA oligonucleotide modified with biotin and puromycin on each end is then hybridized to the 3’-end of the mRNA. The mRNAs are then arrayed on a slide and immobilized by the binding of biotin to streptavidin that is pre-coated on the slide. Cell extract is then dispensed on the slide for \"in situ\" translation to take place. When the ribosome reaches the hybridized oligonucleotide, it stalls and incorporates the puromycin molecule to the nascent polypeptide chain, thereby attaching the newly synthesized protein to the microarray via the DNA oligonucleotide. A pure protein array is obtained after the mRNA is digested with RNase. The protein spots generated by this method are very sharply defined and can be produced at a high density.\n\nNanowell array formats are used to express individual proteins in small volume reaction vessels or nanowells (Figure 4). This format is sometimes preferred because it avoids the need to immobilize the target protein which might result in the potential loss of protein activity. The miniaturization of the array also conserves solution and precious compounds that might be used in screening assays. Moreover, the structural properties of individual wells help to prevent cross-contamination among chambers. In 2012 an improved NAPPA was published, which used a nanowell array to prevent diffusion. Here the DNA was immobilized in the well together with an anti-GST antibody. Then cell-free expression mix was added and the wells closed by a lid. The nascent proteins containing a GST-tag were bound to the well surface enabling a NAPPA-array with higher density and nearly no cross-contaminations.\n\nDNA array to protein array (DAPA) is a method developed in 2007 to repeatedly produce protein arrays by ‘printing’ them from a single DNA template array, on demand (Figure 5). It starts with the spotting and immobilization of an array of DNA templates onto a glass slide. The slide is then assembled face-to-face with a second slide pre-coated with a protein-capturing reagent, and a membrane soaked with cell extract is placed between the two slides for transcription and translation to take place. The newly synthesized his-tagged proteins are then immobilized onto the slide to form the array. In the publication in 18 of 20 replications a protein microarray copy could be generated. Potentially the process can be repeated as often as needed, as long as the DNA is unharmed by DNAses, degradation or mechanical abrasion.\n\nMany of the advantages of cell-free protein array technology address the limitations of cell-based expression system used in traditional methods of protein microarray production.\n\nThe method avoids DNA cloning (with the exception of NAPPA) and can quickly convert genetic information into functional proteins by using PCR DNA. The reduced steps in production and the ability to miniaturize the system saves on reagent consumption and cuts production costs.\n\nMany proteins, including antibodies, are difficult to express in host cells due to problems with insolubility, disulfide bonds or host cell toxicity. Cell-free protein array makes many of such proteins available for use in protein microarrays.\n\nUnlike DNA, which is a highly stable molecule, proteins are a heterogeneous class of molecules with different stability and physiochemical properties. Maintaining the proteins’ folding and function in an immobilized state over long periods of storage is a major challenge for protein microarrays. Cell-free methods provide the option to quickly obtaining protein microarrays on demand, thus eliminating any problems associated with long-term storage.\n\nThe method is amenable to a range of different templates: PCR products, plasmids and mRNA. Additional components can be included during synthesis to adjust the environment for protein folding, disulfide bond formation, modification or protein activity.\n\n\nProtein interactions: To screen for protein–protein interactions and protein interactions with other molecules such as metabolites, lipids, DNA and small molecules.; enzyme inhibition assay: for high throughput drug candidate screening and to discover novel enzymes for use in biotechnology; screening antibody specificity.\n\n", "id": "16276793", "title": "Cell-free protein array"}
{"url": "https://en.wikipedia.org/wiki?curid=7265847", "text": "Chemotaxis assay\n\nChemotaxis assays are experimental tools for evaluation of chemotactic ability of prokaryotic or eukaryotic cells.\nA wide variety of techniques have been developed. Some techniques are qualitative - allowing an investigator to approximately determine a cell's chemotacic affinity for an analyte - while others are quantitative, allowing a precise measurement of this affinity.\n\nIn general, the most important requisite is to calibrate the incubation time of the assay both to the model cell and the ligand to be evaluated. Too short incubation time results no cells in the sample, while too long time perturbs the concentration gradients and measures more chemokinetic than chemotactic responses.\n\nThe most commonly used techniques are grouped into two main groups:\n\nThis way of evaluation deals with agar-agar or gelatine containing semi-solid layers made prior to the experiment. Small wells are cut into the layer and filled with cells and the test substance. Cells can migrate towards the chemical gradient in the semi solid layer or under the layer as well. Some variations of the technique deal also with wells and parallel channels connected by a cut at the start of the experiment (PP-technique). Radial arrangement of PP-technique (3 or more channels) provides the possibility to compare chemotactic activity of different cell populations or study preference between ligands.\n\nCounting of cells: positive responder cells could be counted from the front of migrating cells, after staining or in native conditions in light microscope.\n\nChambers isolated by filters are proper tools for accurate determination of chemotactic behavior. The pioneer type of these chambers was constructed by Boyden. The motile cells are placed into the upper chamber, while fluid containing the test substance is filled into the lower one. The size of the motile cells to be investigated determines the pore size of the filter; it is essential to choose a diameter which allows active transmigration. For modelling \"in vivo\" conditions, several protocols prefer coverage of filter with molecules of extracellular matrix (collagen, elastin etc.) Efficiency of the measurements was increased by development of multiwell chambers (e.g. NeuroProbe), where 24, 96, 384 samples are evaluated in parallel. Advantage of this variant is that several parallels are assayed in identical conditions.\n\nIn another setting the chambers are connected side by side horizontally (Zigmond chamber) or as concentric rings on a slide (Dunn chamber) Concentration gradient develops on a narrow connecting bridge between the chambers and the number of migrating cells is also counted on the surface of the bridge by light microscope. In some cases the bridge between the two chambers is filled with agar and cells have to \"glide\" in this semisolid layer.\n\nSome capillary techniques provide also a chamber like arrangement, however, there is no filter between the cells and the test substance. Quantitative results are gained by the multiwell type of this probe using 4-8-12-channel pipettes. Accuracy of the pipette and increased number of the parallel running samples is the great advantage of this test.\n\nCounting of cells: positive responder cells are count from the lower chamber (long incubation time) or from the filter (short incubation time). For detection of cells general staining techniques (e.g. trypan blue) or special probes (e.g. mt-dehydrogenase detection with MTT assay) are used. Labelled (e.g. fluorochromes) cells are also used, in some assays cells get labelled during transmigration the filter.\n\nBesides the above-mentioned two, most commonly used family of techniques a wide range of protocols were developed to measure chemotactic activity. Some of them are only qualitative, like aggregation tests, where small pieces of agar or filters are placed onto a slide and accumulation of cells around is measured.\n\nIn another semiquantitative technique cells are overlaid the test substance and changes in opalescence of the originally cell-free compartment is recorded during the incubation time.\n\nThe third very frequently used, however, qualitative technique is the T-maze and its adaptations for microplates. In the original version a container drilled in a peg is filled with cells. Then the peg is twisted and the cells get contact with two other containers filled with different substances. The incubation is stopped with resetting the peg, the cell number is counted from the containers.\n\nAlso, lately, microfluidic devices have been more and more frequently used to test quantitatively, and precisely, for chemotaxis.\n\n", "id": "7265847", "title": "Chemotaxis assay"}
{"url": "https://en.wikipedia.org/wiki?curid=9756354", "text": "ChIP-on-chip\n\nChIP-on-chip (also known as ChIP-chip) is a technology that combines chromatin immunoprecipitation ('ChIP') with DNA microarray (\"\"chip\"\"). Like regular ChIP, ChIP-on-chip is used to investigate interactions between proteins and DNA \"in vivo\". Specifically, it allows the identification of the cistrome, the sum of binding sites, for DNA-binding proteins on a genome-wide basis. Whole-genome analysis can be performed to determine the locations of binding sites for almost any protein of interest. As the name of the technique suggests, such proteins are generally those operating in the context of chromatin. The most prominent representatives of this class are transcription factors, replication-related proteins, like Origin Recognition Complex Protein (ORC), histones, their variants, and histone modifications.\n\nThe goal of ChIP-on-chip is to locate protein binding sites that may help identify functional elements in the genome. For example, in the case of a transcription factor as a protein of interest, one can determine its transcription factor binding sites throughout the genome. Other proteins allow the identification of promoter regions, enhancers, repressors and silencing elements, insulators, boundary elements, and sequences that control DNA replication. If histones are subject of interest, it is believed that the distribution of modifications and their localizations may offer new insights into the mechanisms of regulation.\n\nOne of the long-term goals ChIP-on-chip was designed for is to establish a catalogue of (selected) organisms that lists all protein-DNA interactions under various physiological conditions. This knowledge would ultimately help in the understanding of the machinery behind gene regulation, cell proliferation, and disease progression. Hence, ChIP-on-chip offers not only huge potential to complement our knowledge about the orchestration of the genome on the nucleotide level, but also on higher levels of information and regulation as it is propagated by research on epigenetics.\n\nThe technical platforms to conduct ChIP-on-chip experiments are DNA microarrays, or \"\"chips\"\". They can be classified and distinguished according to various characteristics:\n\n\"Probe type\": DNA arrays can comprise either mechanically spotted cDNAs or PCR-products, mechanically spotted oligonucleotides, or oligonucleotides that are synthesized \"in situ\". The early versions of microarrays were designed to detect RNAs from expressed genomic regions (open reading frames aka ORFs). Although such arrays are perfectly suited to study gene expression profiles, they have limited importance in ChIP experiments since most \"interesting\" proteins with respect to this technique bind in intergenic regions. Nowadays, even custom-made arrays can be designed and fine-tuned to match the requirements of an experiment. Also, any sequence of nucleotides can be synthesized to cover genic as well as intergenic regions.\n\n\"Probe size\": Early version of cDNA arrays had a probe length of about 200bp. Latest array versions use oligos as short as 70- (Microarrays, Inc.) to 25-mers (Affymetrix). (Feb 2007)\n\n\"Probe composition\": There are tiled and non-tiled DNA arrays. Non-tiled arrays use probes selected according to non-spatial criteria, i.e., the DNA sequences used as probes have no fixed distances in the genome. Tiled arrays, however, select a genomic region (or even a whole genome) and divide it into equal chunks. Such a region is called tiled path. The average distance between each pair of neighboring chunks (measured from the center of each chunk) gives the resolution of the tiled path. A path can be overlapping, end-to-end or spaced.\n\n\"Array size\": The first microarrays used for ChIP-on-Chip contained about 13,000 spotted DNA segments representing all ORFs and intergenic regions from the yeast genome. Nowadays, Affymetrix offers whole-genome tiled yeast arrays with a resolution of 5bp (all in all 3.2 million probes). Tiled arrays for the human genome become more and more powerful, too. Just to name example, Affymetrix offers a set of seven arrays with about 90 million probes, spanning the complete non-repetitive part of the human genome with about 35bp spacing. (Feb 2007)\nBesides the actual microarray, other hard- and software equipment is necessary to run ChIP-on-chip experiments. It is generally the case that one company’s microarrays can not be analyzed by another company’s processing hardware. Hence, buying an array requires also buying the associated workflow equipment. The most important elements are, among others, hybridization ovens, chip scanners, and software packages for subsequent numerical analysis of the raw data.\n\nStarting with a biological question, a ChIP-on-chip experiment can be divided into three major steps: The first is to set up and design the experiment by selecting the appropriate array and probe type. Second, the actual experiment is performed in the wet-lab. Last, during the dry-lab portion of the cycle, gathered data are analyzed to either answer the initial question or lead to new questions so that the cycle can start again.\n\nIn the first step, the protein of interest (POI) is cross-linked with the DNA site it binds to in an \"in vitro\" environment. Usually this is done by a gentle formaldehyde fixation that is reversible with heat.\n\nThen, the cells are lysed and the DNA is sheared by sonication or using micrococcal nuclease. This results in double-stranded chunks of DNA fragments, normally 1 kb or less in length. Those that were cross-linked to the POI form a POI-DNA complex.\n\nIn the next step, only these complexes are filtered out of the set of DNA fragments, using an antibody specific to the POI. The antibodies may be attached to a solid surface, may have a magnetic bead, or some other physical property that allows separation of cross-linked complexes and unbound fragments. This procedure is essentially an immunoprecipitation (IP) of the protein. This can be done either by using a tagged protein with an antibody against the tag (ex. FLAG, HA, c-myc) or with an antibody to the native protein.\n\nThe cross-linking of POI-DNA complexes is reversed (usually by heating) and the DNA strands are purified. For the rest of the workflow, the POI is no longer necessary.\n\nAfter an amplification and denaturation step, the single-stranded DNA fragments are labeled with a fluorescent tag such as Cy5 or Alexa 647.\n\nFinally, the fragments are poured over the surface of the DNA microarray, which is spotted with short, single-stranded sequences that cover the genomic portion of interest. Whenever a labeled fragment \"finds\" a complementary fragment on the array, they will hybridize and form again a double-stranded DNA fragment.\n\nAfter a sufficiently large time frame to allow hybridization, the array is illuminated with fluorescent light. Those probes on the array that are hybridized to one of the labeled fragments emit a light signal that is captured by a camera. This image contains all raw data for the remaining part of the workflow.\n\nThis raw data, encoded as false-color image, needs to be converted to numerical values before the actual analysis can be done. The analysis and information extraction of the raw data often remains the most challenging part for ChIP-on-chip experiments. Problems arise throughout this portion of the workflow, ranging from the initial chip read-out, to suitable methods to subtract background noise, and finally to appropriate algorithms that normalize the data and make it available for subsequent statistical analysis, which then hopefully lead to a better understanding of the biological question that the experiment seeks to address. Furthermore, due to the different array platforms and lack of standardization between them, data storage and exchange is a huge problem. Generally speaking, the data analysis can be divided into three major steps:\n\nDuring the first step, the captured fluorescence signals from the array are normalized, using control signals derived from the same or a second chip. Such control signals tell which probes on the array were hybridized correctly and which bound nonspecifically.\n\nIn the second step, numerical and statistical tests are applied to control data and IP fraction data to identify POI-enriched regions along the genome. The following three methods are used widely: Median percentile rank, Single-array error, and Sliding-window. These methods generally differ in how low-intensity signals are handled, how much background noise is accepted, and which trait for the data is emphasized during the computation. In the recent past, the sliding-window approach seems to be favored and is often described as most powerful.\n\nIn the third step, these regions are analyzed further. If, for example, the POI was a transcription factor, such regions would represent its binding sites. Subsequent analysis then may want to infer nucleotide motifs and other patterns to allow functional annotation of the genome.\n\nUsing tiled arrays, ChIP-on-chip allows for high resolution of genome-wide maps. These maps can determine the binding sites of many DNA-binding proteins like transcription factors and also chromatin modifications.\n\nAlthough ChIP-on-chip can be a powerful technique in the area of genomics, it is very expensive. Most published studies using ChIP-on-chip repeat their experiments at least three times to ensure biologically meaningful maps. The cost of the DNA microarrays is often a limiting factor to whether a laboratory should proceed with a ChIP-on-chip experiment. Another limitation is the size of DNA fragments that can be achieved. Most ChIP-on-chip protocols utilize sonication as a method of breaking up DNA into small pieces. However, sonication is limited to a minimal fragment size of 200 bp. For higher resolution maps, this limitation should be overcome to achieve smaller fragments, preferably to single nucleosome resolution. As mentioned previously, the statistical analysis of the huge amount of data generated from arrays is a challenge and normalization procedures should aim to minimize artifacts and determine what is really biologically significant. So far, application to mammalian genomes has been a major limitation, for example, due to the significant percentage of the genome that is occupied by repeats. However, as ChIP-on-chip technology advances, high resolution whole mammalian genome maps should become achievable.\n\nAntibodies used for ChIP-on-chip can be an important limiting factor. ChIP-on-chip requires highly specific antibodies that must recognize its epitope in free solution and also under fixed conditions. If it is demonstrated to successfully immunoprecipitate cross-linked chromatin, it is termed \"ChIP-grade\". Companies that provide ChIP-grade antibodies include Abcam, Cell Signaling Technology, Santa Cruz, and Upstate. To overcome the problem of specificity, the protein of interest can be fused to a tag like FLAG or HA that are recognized by antibodies. An alternative to ChIP-on-chip that does not require antibodies is DamID.\n\nAlso available are antibodies against a specific histone modification like H3 tri methyl K4. As mentioned before, the combination of these antibodies and ChIP-on-chip has become extremely powerful in determining whole genome analysis of histone modification patterns and will contribute tremendously to our understanding of the histone code and epigenetics.\n\nA study demonstrating the non-specific nature of DNA binding proteins has been published in PLoS Biology. This indicates that alternate confirmation of functional relevancy is a necessary step in any ChIP-chip experiment.\n\nA first ChIP-on-chip experiment was performed in 1999 to analyze the distribution of cohesin along budding yeast chromosome III. Although the genome was not completely represented, the protocol in this study remains equivalent as those used in later studies. The ChIP-on-chip technique using all of the ORFs of the genome (that nevertheless remains incomplete, missing intergenic regions) was then applied successfully in three papers published in 2000 and 2001. The authors identified binding sites for individual transcription factors in the budding yeast \"Saccharomyces cerevisiae\". In 2002, Richard Young’s group determined the genome-wide positions of 106 transcription factors using a c-Myc tagging system in yeast. The first demonstration of the mammalian ChIp-on-chip technique reported the isolation of nine chromatin fragments containing weak and strong E2F binding site was done by Peggy Farnham's lab in collaboration with Michael Zhang's lab and published in 2001. This study was followed several months later in a collaboration between the Young lab with the laboratory of Brian Dynlacht which used the ChIP-on-chip technique to show for the first time that E2F targets encode components of the DNA damage checkpoint and repair pathways, as well as factors involved in chromatin assembly/condensation, chromosome segregation, and the mitotic spindle checkpoint Other applications for ChIP-on-chip include DNA replication, recombination, and chromatin structure. Since then, ChIP-on-chip has become a powerful tool in determining genome-wide maps of histone modifications and many more transcription factors. ChIP-on-chip in mammalian systems has been difficult due to the large and repetitive genomes. Thus, many studies in mammalian cells have focused on select promoter regions that are predicted to bind transcription factors and have not analyzed the entire genome. However, whole mammalian genome arrays have recently become commercially available from companies like Nimblegen. In the future, as ChIP-on-chip arrays become more and more advanced, high resolution whole genome maps of DNA-binding proteins and chromatin components for mammals will be analyzed in more detail.\n\nChip-Sequencing is a recently developed technology that still uses chromatin immunoprecipitation to crosslink the proteins of interest to the DNA but then instead of using a micro-array, it uses the more accurate, higher throughput method of sequencing to localize interaction points.\n\nDamID is an alternative method that does not require antibodies.\n\nChIP-exo uses exonuclease treatment to achieve up to single base pair resolution.\n\n\n\n", "id": "9756354", "title": "ChIP-on-chip"}
{"url": "https://en.wikipedia.org/wiki?curid=28268694", "text": "DNA condensation\n\nDNA condensation refers to the process of compacting DNA molecules \"in vitro\" or \"in vivo\". Mechanistic details of DNA packing are essential for its functioning in the process of gene regulation in living systems. Condensed DNA often has surprising properties, which one would not predict from classical concepts of dilute solutions. Therefore, DNA condensation \"in vitro\" serves as a model system for many processes of physics, biochemistry and biology. In addition, DNA condensation has many potential applications in medicine and biotechnology.\n\nDNA diameter is about 2 nm, while the length of a stretched single molecule may be up to several dozens of centimetres depending on the organism. Many features of the DNA double helix contribute to its large stiffness, including the mechanical properties of the sugar-phosphate backbone, electrostatic repulsion between phosphates (DNA bears on average one elementary negative charge per each 0.17 nm of the double helix), stacking interactions between the bases of each individual strand, and strand-strand interactions. DNA is one of the stiffest natural polymers, yet it is also one of the longest molecules. This means that at large distances DNA can be considered as a flexible rope, and on a short scale as a stiff rod. Like a garden hose, unpacked DNA would randomly occupy a much larger volume than when it is orderly packed. Mathematically, for a non-interacting flexible chain randomly diffusing in 3D, the end-to-end distance would scale as a square root of the polymer length. For real polymers such as DNA, this gives only a very rough estimate; what is important, is that the space available for the DNA \"in vivo\" is much smaller than the space that it would occupy in the case of a free diffusion in the solution. To cope with volume constraints, DNA can pack itself in the appropriate solution conditions with the help of ions and other molecules. Usually, DNA condensation is defined as \"the collapse of extended DNA chains into compact, orderly particles containing only one or a few molecules\". This definition applies to many situations in vitro and is also close to the definition of DNA condensation in bacteria as \"adoption of relatively concentrated, compact state occupying a fraction of the volume available\". In eukaryotes, the DNA size and the number of other participating players are much larger, and a DNA molecule forms millions of ordered nucleoprotein particles, the nucleosomes, which is just the first of many levels of DNA packing.\n\nIn viruses and bacteriophages, the DNA or RNA is surrounded by a protein capsid, sometimes further enveloped by a lipid membrane. Double-stranded DNA is stored inside the capsid in the form of a spool, which can have different types of coiling leading to different types of liquid-crystalline packing. This packing can change from hexagonal to cholesteric to isotropic at different stages of the phage functioning. Although the double helices are always locally aligned, the DNA inside viruses does not represent real liquid crystals, because it lacks fluidity. On the other hand, DNA condensed \"in vitro\", e.g., with the help of polyamines also present in viruses, is both locally ordered and fluid.\n\nBacterial DNA is packed with the help of polyamines and proteins. Protein-associated DNA occupies about 1/4 of the intracellular volume forming a concentrated viscous phase with liquid crystalline properties, called the nucleoid. Similar DNA packaging exists also in chloroplasts and mitochondria. Bacterial DNA is sometimes referred to as the bacterial chromosome. Bacterial nucleoid evolutionary represents an intermediate engineering solution between the protein-free DNA packing in viruses and protein-determined packing in eukaryotes.\n\nSister chromosomes in the bacterium \"Escherichia coli\" are induced by stressful conditions to condense and undergo pairing. Stress-induced condensation occurs by a non-random, zipper-like convergence of sister chromosomes. This convergence appears to depend on the ability of identical double-stranded DNA molecules to specifically identify each other, a process that culminates in the proximity of homologous sites along the paired chromosomes. Diverse stress conditions appear to prime bacteria to effectively cope with severe DNA damages such as double-strand breaks. The apposition of homologous sites associated with stress-induced chromosome condensation helps explain how repair of double-strand breaks and other damages occurs.\n\nEukaryotic DNA with a typical length of dozens of centimeters should be orderly packed to be readily accessible inside the micrometer-size nucleus. In unicellular eukaryotes known as dinoflagellates, it is possible to distinguish liquid-crystalline chromosomal ordering, that they do not contain easily detectable histones, until recently it was thought that they were completely absent and that dinoflagellates were mesokaryotic though this is no longer the case and they are now classified as alveolates. In other eukaryotes, DNA is arranged in the cell nucleus with the help of histones. In this case, the basic level of DNA compaction is the nucleosome, where the double helix is wrapped around the histone octamer containing two copies of each histone H2A, H2B, H3 and H4. Linker histone H1 binds the DNA between nucleosomes and facilitates packaging of the 10 nm \"beads on the string\" nucleosomal chain into a more condensed 30 nm fiber. Most of the time, between cell divisions, chromatin is optimized to allow easy access of transcription factors to active genes, which are characterized by a less compact structure called euchromatin, and to alleviate protein access in more tightly packed regions called heterochromatin. During the cell division, chromatin compaction increases even more to form chromosomes, which can cope with large mechanical forces dragging them into each of the two daughter cells.\n\nDNA condensation can be induced \"in vitro\" either by applying external force to bring the double helices together, or by inducing attractive interactions between the DNA segments. The former can be achieved e.g. with the help of the osmotic pressure exerted by crowding neutral polymersin the presence of monovalent salts. In this case, the forces pushing the double helices together are coming from entropic random collisions with the crowding polymers surrounding DNA condensates, and salt is required to neutralize DNA charges and decrease DNA-DNA repulsion. The second possibility can be realized by inducing attractive interactions between the DNA segments by multivalent cationic charged ligands (multivalent metal ions, inorganic cations, polyamines, protamines, peptides, lipids, liposomes and proteins).\n\nCondensation of long double-helical DNAs is a sharp phase transition, which takes place within a narrow interval of condensing agent concentrations.[ref] Since the double helices come very closely to each other in the condensed phase, this leads to the restructuring of water molecules, which gives rise to the so-called hydration forces.[ref] To understand attraction between negatively charged DNA molecules, one also must account for correlations between counterions in the solution.[ref] DNA condensation by proteins can exhibit hysteresis, which can be explained using a modified Ising model.\n\nNowadays descriptions of gene regulation are based on the approximations of equilibrium binding in dilute solutions, although it is clear that these assumptions are in fact violated in chromatin. The dilute-solution approximation is violated for two reasons. First, the chromatin content is far from being dilute, and second, the numbers of the participating molecules are sometimes so small, that it does not make sense to talk about the bulk concentrations. Further differences from dilute solutions arise due to the different binding affinities of proteins to condensed and uncondensed DNA. Thus in condensed DNA both the reaction rates can be changed and their dependence on the concentrations of reactants may become nonlinear.\n\n", "id": "28268694", "title": "DNA condensation"}
{"url": "https://en.wikipedia.org/wiki?curid=5638621", "text": "DNA footprinting\n\nDNA footprinting is a method of investigating the sequence specificity of DNA-binding proteins in vitro. This technique can be used to study protein-DNA interactions both outside and within cells.\n\nThe regulation of transcription has been studied extensively, and yet there is still much that is not known. Transcription factors and associated proteins that bind promoters, enhancers, or silencers to drive or repress transcription are fundamental to understanding the unique regulation of individual genes within the genome. Techniques like DNA footprinting help elucidate which proteins bind to these associated regions of DNA and unravel the complexities of transcriptional control.\n\nIn 1978, David Galas and Albert Schmitz developed the DNA footprinting technique to study the binding specificity of the lac repressor protein. It was originally a modification of the Maxam-Gilbert chemical sequencing technique.\n\nThe simplest application of this technique is to assess whether a given protein binds to a region of interest within a DNA molecule. Polymerase chain reaction (PCR) amplify and label region of interest that contains a potential protein-binding site, ideally amplicon is between 50 and 200 base pairs in length. Add protein of interest to a portion of the labeled template DNA; a portion should remain separate without protein, for later comparison. Add a cleavage agent to both portions of DNA template. The cleavage agent is a chemical or enzyme that will cut at random locations in a sequence independent manner. The reaction should occur just long enough to cut each DNA molecule in only one location. A protein that specifically binds a region within the DNA template will protect the DNA it is bound to from the cleavage agent. Run both samples side by side on a polyacrylamide gel electrophoresis. The portion of DNA template without protein will be cut at random locations, and thus when it is run on a gel, will produce a ladder-like distribution. The DNA template with the protein will result in ladder distribution with a break in it, the \"footprint\", where the DNA has been protected from the cleavage agent.\nNote: Maxam-Gilbert chemical DNA sequencing can be run alongside the samples on the polyacrylamide gel to allow the prediction of the exact location of ligand binding site.\n\nThe DNA template labeled at the 3' or 5' end, depending on the location of the binding site(s). Labels that can be used are: radioactivity and fluorescence. Radioactivity has been traditionally used to label DNA fragments for footprinting analysis, as the method was originally developed from the Maxam-Gilbert chemical sequencing technique. Radioactive labeling is very sensitive and is optimal for visualizing small amounts of DNA. Fluorescence is a desirable advancement due to the hazards of using radio-chemicals. However, it has been more difficult to optimize because it is not always sensitive enough to detect the low concentrations of the target DNA strands used in DNA footprinting experiments. Electrophoretic sequencing gels or capillary electrophoresis have been successful in analyzing footprinting of fluorescent tagged fragments.\n\nA variety of cleavage agents can be chosen. a desirable agent is one that is sequence neutral, easy to use, and is easy to control. Unfortunately no available agents meet all of these standards, so an appropriate agent can be chosen, depending on your DNA sequence and ligand of interest. The following cleavage agents are described in detail:\nDNase I is a large protein that functions as a double-strand endonuclease. It binds the minor groove of DNA and cleaves the phosphodiester backbone. It is a good cleavage agent for footprinting because its size makes it easily physically hindered. Thus is more likely to have its action blocked by a bound protein on a DNA sequence. In addition, the DNase I enzyme is easily controlled by adding EDTA to stop the reaction. There are however some limitations in using DNase I. The enzyme does not cut DNA randomly; its activity is affected by local DNA structure and sequence and therefore results in an uneven ladder. This can limit the precision of predicting a protein’s binding site on the DNA molecule.\nHydroxyl radicals are created from the Fenton reaction, which involves reducing Fe with HO to form free hydroxyl molecules. These hydroxyl molecules react with the DNA backbone, resulting in a break. Due to their small size, the resulting DNA footprint has high resolution. Unlike DNase I they have no sequence dependence and result in a much more evenly distributed ladder. The negative aspect of using hydroxyl radicals is that they are more time consuming to use, due to a slower reaction and digestion time.\nUltraviolet irradiation can be used to excite nucleic acids and create photoreactions, which results in damaged bases in the DNA strand. Photoreactions can include: single strand breaks, interactions between or within DNA strands, reactions with solvents, or crosslinks with proteins. The workflow for this method has an additional step, once both your protected and unprotected DNA have been treated, there is subsequent primer extension of the cleaved products. The extension will terminate upon reaching a damaged base, and thus when the PCR products are run side-by-side on a gel; the protected sample will show an additional band where the DNA was crosslinked with a bound protein. Advantages of using UV are that it reacts very quickly and can therefore capture interactions that are only momentary. Additionally it can be applied to \"in vivo\" experiments, because UV can penetrate cell membranes. A disadvantage is that the gel can be difficult to interpret, as the bound protein does not protect the DNA, it merely alters the photoreactions in the vicinity.\n\n\"In vivo\" footprinting is a technique used to analyze the protein-DNA interactions that are occurring in a cell at a given time point. DNase I can be used as a cleavage agent if the cellular membrane has been permeabilized. However the most common cleavage agent used is UV irradiation because it penetrates the cell membrane without disrupting cell state and can thus capture interactions that are sensitive to cellular changes. Once the DNA has been cleaved or damaged by UV, the cells can be lysed and DNA purified for analysis of a region of interest. Ligation-mediated PCR is an alternative method to footprint \"in vivo\". Once a cleavage agent has been used on the genomic DNA, resulting in single strand breaks, and the DNA is isolated, a linker is added onto the break points. A region of interest is amplified between the linker and a gene-specific primer, and when run on a polyacrylamide gel, will have a footprint where a protein was bound. \"In vivo\" footprinting combined with immunoprecipitation can be used to assess protein specificity at many locations throughout the genome. The DNA bound to a protein of interest can be immunoprecipitated with an antibody to that protein, and then specific region binding can be assessed using the DNA footprinting technique.\n\nThe DNA footprinting technique can be modified to assess the binding strength of a protein to a region of DNA. Using varying concentrations of the protein for the footprinting experiment, the appearance of the footprint can be observed as the concentrations increase and the proteins binding affinity can then be estimated.\n\nTo adapt the footprinting technique to updated detection methods, the labelled DNA fragments are detected by a capillary electrophoresis device instead of being run on a polyacrylamide gel. If the DNA fragment to be analyzed is produced by polymerase chain reaction (PCR), it is straightforward to couple a fluorescent molecule such as carboxyfluorescein (FAM) to the primers. This way, the fragments produced by DNaseI digestion will contain FAM, and will be detectable by the capillary electrophoresis machine. Typically, carboxytetramethyl-rhodamine (ROX)-labelled size standards are also added to the mixture of fragments to be analyzed. Binding sites of transcription factors have been successfully identified this way.\n\nNext-generation sequencing has enabled a genome-wide approach to identify DNA footprints. Open chromatin assays such as DNase-Seq and FAIRE-Seq have proven to provide a robust regulatory landscape for many cell types. However, these assays require some downstream bioinformatics analyses in order to provide genome-wide DNA footprints. The computational tools proposed can be categorized in two classes: segmentation-based and site-centric approaches.\n\nSegmentation-based methods are based on the application of Hidden Markov models or sliding window methods to segment the genome into open/closed chromatin region. Examples of such methods are: HINT, Boyle method and Neph method. Site-centric methods, on the other hand, find footprints given the open chromatin profile around motif-predicted binding sites, i.e., regulatory regions predicted using DNA-protein sequence information (encoded in structures such as Position weight matrix). Examples of these methods are CENTIPEDE and Cuellar-Partida method.\n\n\n", "id": "5638621", "title": "DNA footprinting"}
{"url": "https://en.wikipedia.org/wiki?curid=3973930", "text": "Electrophoretic mobility shift assay\n\nAn electrophoretic mobility shift assay (EMSA) or mobility shift electrophoresis, also referred as a gel shift assay, gel mobility shift assay, band shift assay, or gel retardation assay, is a common affinity electrophoresis technique used to study protein–DNA or protein–RNA interactions. This procedure can determine if a protein or mixture of proteins is capable of binding to a given DNA or RNA sequence, and can sometimes indicate if more than one protein molecule is involved in the binding complex. Gel shift assays are often performed in vitro concurrently with DNase footprinting, primer extension, and promoter-probe experiments when studying transcription initiation, DNA replication, DNA repair or RNA processing and maturation, as well as pre-mRNA splicing. Although precursors can be found in earlier literature, most current assays are based on methods described by Garner and Revzin and Fried and Crothers.\n\nA mobility shift assay is electrophoretic separation of a protein–DNA or protein–RNA mixture on a polyacrylamide or agarose gel for a short period (about 1.5-2 hr for a 15- to 20-cm gel). The speed at which different molecules (and combinations thereof) move through the gel is determined by their size and charge, and to a lesser extent, their shape (see gel electrophoresis). The control lane (DNA probe without protein present) will contain a single band corresponding to the unbound DNA or RNA fragment. However, assuming that the protein is capable of binding to the fragment, the lane with protein present will contain another band that represents the larger, less mobile complex of nucleic acid probe bound to protein which is 'shifted' up on the gel (since it has moved more slowly).\n\nUnder the correct experimental conditions, the interaction between the DNA (or RNA) and protein is stabilized and the ratio of bound to unbound nucleic acid on the gel reflects the fraction of free and bound probe molecules as the binding reaction enters the gel. This stability is in part due to a \"caging effect\", in that the protein, surrounded by the gel matrix, is unable to diffuse away from the probe before they recombine. If the starting concentrations of protein and probe are known, and if the stoichiometry of the complex is known, the apparent affinity of the protein for the nucleic acid sequence may be determined. Unless the complex is very long lived under gel conditions, or dissociation during electrophoresis is taken into account, the number derived is an apparent Kd. If the protein concentration is not known but the complex stoichiometry is, the protein concentration can be determined by increasing the concentration of DNA probe until further increments do not increase the fraction of protein bound. By comparison with a set of standard dilutions of free probe run on the same gel, the number of moles of protein can be calculated.\n\nAn antibody that recognizes the protein can be added to this mixture to create an even larger complex with a greater shift. This method is referred to as a \"supershift assay\", and is used to unambiguously identify a protein present in the protein – nucleic acid complex.\n\nOften, an extra lane is run with a competitor oligonucleotide to determine the most favorable binding sequence for the binding protein. The use of different oligonucleotides of defined sequence allows the identification of the precise binding site by competition (not shown in diagram). Variants of the competition assay are useful for measuring the specificity of binding and for measurement of association and dissociation kinetics.\n\nOnce DNA-protein binding is determined \"in vitro\", a number of algorithms can narrow the search for identification of the transcription factor. Consensus sequence oligonucleotides for the transcription factor of interest will be able to compete for the binding, eliminating the shifted band, and must be confirmed by supershift. If the predicted consensus sequence fails to compete for binding, identification of the transcription factor may be aided by Multiplexed Competitor EMSA (MC-EMSA), whereby large sets of consensus sequences are multiplexed in each reaction, and where one set competes for binding, the individual consensus sequences from this set are run in a further reaction.\n\nFor visualization purposes, the nucleic acid fragment is usually labelled with a radioactive, fluorescent or biotin label. Standard ethidium bromide staining is less sensitive than these methods and can lack the sensitivity to detect the nucleic acid if small amounts of nucleic acid or single-stranded nucleic acid(s) are used in these experiments. When using a biotin label, streptavidin conjugated to an enzyme such as horseradish peroxidase is used to detect the DNA fragment. While isotopic DNA labeling has little or no effect on protein binding affinity, use of non-isotopic labels including flurophores or biotin can alter the affinity and/or stoichiometry of the protein interaction of interest. Competition between fluorophore- or biotin-labeled probe and unlabeled DNA of the same sequence can be used to determine whether the label alters binding affinity or stoichiometry.\n\n", "id": "3973930", "title": "Electrophoretic mobility shift assay"}
{"url": "https://en.wikipedia.org/wiki?curid=10438744", "text": "Gene targeting\n\nGene targeting (also, replacement strategy based on homologous recombination) is a genetic technique that uses homologous recombination to change an endogenous gene. The method can be used to delete a gene, remove exons, add a gene, and introduce point mutations. Gene targeting can be permanent or conditional. Conditions can be a specific time during development / life of the organism or limitation to a specific tissue, for example. Gene targeting requires the creation of a specific vector for each gene of interest. However, it can be used for any gene, regardless of transcriptional activity or gene size.\n\nGene targeting methods are established for several model organisms and may vary depending on the species used. In general, a\ntargeting construct made out of DNA is generated in bacteria. It typically contains part of the gene to be targeted, a reporter gene, and a (dominant) selectable marker.\n\nTo target genes in mice, this construct is then inserted into mouse embryonic stem cells in\nculture. After cells with the correct insertion have been selected, they can be used to contribute to a mouse's\ntissue via embryo injection. Finally, chimeric mice where the modified cells made up the reproductive organs are selected for via breeding. After this step the entire body of the mouse is based on the previously selected embryonic stem cell.\n\nTo target genes in moss, this construct is incubated together with freshly isolated protoplasts and\nwith Polyethylene glycol. As mosses are haploid organisms, regenerating moss filaments (protonema) can directly be screened for gene targeting, either by treatment with antibiotics or with PCR. Unique among plants, this procedure for reverse genetics is as efficient as in yeast.\nUsing modified procedures, gene targeting has also been successfully applied to cattle, sheep, swine, and many fungi.\n\nThe frequency of gene targeting can be significantly enhanced through the use of engineered endonucleases such as zinc finger nucleases, engineered homing endonucleases, and nucleases based on engineered TAL effectors. To date, this method has been applied to a number of species including Drosophila melanogaster, tobacco, corn, human cells, mice, and rats.\n\nGene trapping is based on random insertion of a cassette while gene targeting targets a specific gene. Cassettes can be used for many different things while the flanking homology regions of gene targeting cassettes need to be adapted for each gene. This makes gene trapping more easily amenable for large scale projects than targeting. On the other hand, gene targeting can be used for genes with low transcriptions that would go undetected in a trap screen. Also, the probability of trapping increases with intron size. For gene targeting these compact genes are just as easily altered.\n\nGene targeting has been widely used to study human genetic diseases by removing (\"knocking out\"), or adding (\"knocking in\"), specific mutations of interest to a variety of models. Previously used to engineer rat cell models, advances in gene targeting technologies are enabling the creation of a new wave of isogenic human disease models. These models are the most accurate in-vitro models available to researchers to date, and are facilitating the development of new personalized drugs and diagnostics, particularly in the field of cancer.\n\nMario R. Capecchi, Martin J. Evans and Oliver Smithies were declared laureates of the 2007 Nobel Prize in Physiology or Medicine for their work on \"principles for introducing specific gene modifications in mice by the use of embryonic stem cells\", or gene targeting.\n\n\n", "id": "10438744", "title": "Gene targeting"}
{"url": "https://en.wikipedia.org/wiki?curid=26403720", "text": "Geniom RT Analyzer\n\nGeniom RT Analyzer is an instrument used in molecular biology for diagnostic testing. The Geniom RT Analyzer utilizes the dynamic nature of tissue microRNA levels as a biomarker for disease progression. The Geniom analyzer incorporates microfluidic and biochip microarray technology in order to quantify microRNAs via a Microfluidic Primer Extension Assay (MPEA) technique (figure 2).\n\nMany human diseases such as cancer are thought to be involved at a molecular level with dynamic microRNA levels. microRNAs are important in regulating gene expression and can therefore have important implications on the activation or inactivation of oncogenes or tumour suppressor genes respectively. Certain microRNAs have been detected at differing levels throughout the progression of particular types of cancer. Early diagnostic testing has proved to be a challenge for many human diseases, as symptomatic phenotypes can be either ambiguous or subtle in nature. As a result, an extensive research area dedicated to the characterization of molecular biomarkers has blossomed. Biomarkers allow for the accurate measurement of biological molecules at various disease stages; these measurements will eventually contribute to an abundant data source useful for future early diagnostic testing. The Geniom RT Analyzer carries out automated microRNA biomarker profiling that will in turn contribute to this growing data source.\n\ndetection of microRNA levels can be useful over both space and time or a combination of the two.\n\nSpatial microRNA dynamics:\nRecent discoveries have shown the detection of free floating microRNAs in the blood. These free floating microRNAs are found to be protected from endogenous RNAase activity as compared to microRNAs within the cells of tissues. This protection renders these free floating microRNAs as suitable stable biomarkers. This stable biomarker can be used as a control when comparing microRNA levels between tissues. Comparing microRNA levels and thus assessing expression patterns across tissues can be used in a variety of applications such as cancer and disease classification.\n\nTemporal microRNA dynamics:\nA complication with designing and developing tumour suppressor drugs for cancer patients is that the molecular environment within and around the tumour does not seem to be constant throughout the process of tumour development. MicroRNAs are one class of molecules that contribute to this environmental fluctuation for the tumour. Detecting microRNA levels in tumour or regular tissues over time can help to gain insight on the nature of these environmental changes. This information can in turn be used in deciding when the proper timing is for various therapeutic interventions.\n\nThe platform for the Geniom RT Analyzer consists of a biochip containing 8 separate microarrays. Tissue derived microRNAs do not require treatment prior to introduction to the Biochip. The Biochip contains validated and optimized customizable capture probes and the on-chip Microfluidic Primer Extension Assay (MPEA) enables direct microRNA-capture probe hybridization. After biotin labelling, primer extension and washing, the Geniom Analyzer undergoes automated processing of the arrays. A Charge-coupled device (CCD) camera assists the biochip readout which displays a pictorial image of the microRNA quantification. This image is supported by the use of biotinylated nucleotides and subsequent staining with a streptavidin-phycoerythrin-conjugate. Due to the use of 8 separate microarrays per biochip, seven replicate intensity readings are made available and the median value is generally applied to the graphical results. An analysis of biomarker assessment is then made and data can be stored in the miRDBXP database.\n\nMPEA: the MPEA technique utilizes microfluidic technology to ensure the correct timing and consequently the correct alignment of the capture probes with the microRNA molecules. Conventional methods of probe-microRNA hybridization require prior treatment of microRNAs with potentially costly reagents. These conventional methods also require initial introduction of biotin prior to hybridization. Due to the application of microfluidics, neither initial biotin nor reagent treatment is necessary prior to primer extension. Rather the unlabelled, hybridized microRNA behaves as the primer for enzymatic elongation, a process in which biotinylated nucleotides are assembled.\n", "id": "26403720", "title": "Geniom RT Analyzer"}
{"url": "https://en.wikipedia.org/wiki?curid=10443283", "text": "Homing endonuclease\n\nThe homing endonucleases are a collection of endonucleases encoded either as freestanding genes within introns, as fusions with host proteins, or as self-splicing inteins. They catalyze the hydrolysis of genomic DNA within the cells that synthesize them, but do so at very few, or even singular, locations. Repair of the hydrolyzed DNA by the host cell frequently results in the gene encoding the homing endonuclease having been copied into the cleavage site, hence the term 'homing' to describe the movement of these genes. Homing endonucleases can thereby transmit their genes horizontally within a host population, increasing their allele frequency at greater than Mendelian rates.\n\nAlthough the origin and function of homing endonucleases is still being researched, the most established hypothesis considers them as selfish genetic elements, similar to transposons, because they facilitate the perpetuation of the genetic elements that encode them independent of providing a functional attribute to the host organism.\n\nHoming endonuclease recognition sequences are long enough to occur randomly only with a very low probability (approximately once every ), and are normally found in one or very few instances per genome. Generally, owing to the homing mechanism, the gene encoding the endonuclease (the HEG, \"homing endonuclease gene\") is located within the recognition sequence which the enzyme cuts, thus interrupting the homing endonuclease recognition sequence and limiting DNA cutting only to sites that do not (yet) carry the HEG.\n\nPrior to transmission, one allele carries the gene (HEG) while the other does not (HEG), and is therefore susceptible to being cut by the enzyme. Once the enzyme is synthesized, it breaks the chromosome in the HEG allele, initiating a response from the cellular DNA repair system. The damage is repaired using recombination, taking the pattern of the opposite, undamaged DNA allele, HEG, that contains the gene for the endonuclease. Thus, the gene is copied to the allele that initially did not have it and it is propagated through successive generations. This process is called \"homing\".\n\nHoming endonucleases are always indicated with a prefix that identifies their genomic origin, followed by a hyphen: \"I-\" for homing endonucleases encoded within an intron, \"PI-\" (for \"protein insert\") for those encoded within an intein. Some authors have proposed using the prefix \"F-\" (\"freestanding\") for viral enzymes and other natural enzymes not encoded by introns nor inteins, and \"H-\" (\"hybrid\") for enzymes synthesized in a laboratory. Next, a capital letter is derived from the first letter of the name of the genus of the natural source organism, and two lower case letters are derived from the name of the species of that organism. Finally, a Roman numeral distinguishes different enzymes found in the same organism.\n\nFor example, we can mention the enzyme PI-TliII that is the second enzyme encoded by an intein found in the archaea \"Thermococcus litoralis\", and H-DreI, the first synthetic homing endonuclease, created in a laboratory from the enzymes I-DmoI and I-CreI, taken respectively from \"Desulfurococcus mobilis\" and \"Chlamydomonas reinhardtii\".\n\nHoming endonucleases differ from Type II restriction enzymes in the several respects:\n\nCurrently there are six known structural families. Their conserved structural motifs are:\n\nThe crystal structure of the homing endonuclease PI-Sce revealed two domains: an endonucleolytic centre resembling the C-terminal domain of \"Drosophila melanogaster\" Hedgehog protein, and a second domain (Homing endonuclease-associated Hint domain) containing the protein-splicing active site.\n\n\n", "id": "10443283", "title": "Homing endonuclease"}
{"url": "https://en.wikipedia.org/wiki?curid=15056", "text": "Isoelectric point\n\nThe isoelectric point (pI, pH(I), IEP), is the pH at which a particular molecule carries no net electrical charge in the statistical mean. The standard nomenclature to represent the isoelectric point is pH(I), although pI is also commonly seen, and is used in this article for brevity. The net charge on the molecule is affected by pH of its surrounding environment and can become more positively or negatively charged due to the gain or loss, respectively, of protons (H).\n\nSurfaces naturally charge to form a double layer. In the common case when the surface charge-determining ions are H/OH, the net surface charge is affected by the pH of the liquid in which the solid is submerged.\n\nThe pI value can affect the solubility of a molecule at a given pH. Such molecules have minimum solubility in water or salt solutions at the pH that corresponds to their pI and often precipitate out of solution. Biological amphoteric molecules such as proteins contain both acidic and basic functional groups. Amino acids that make up proteins may be positive, negative, neutral, or polar in nature, and together give a protein its overall charge. At a pH below their pI, proteins carry a net positive charge; above their pI they carry a net negative charge. Proteins can, thus, be separated by net charge in a polyacrylamide gel using either preparative gel electrophoresis, which uses a constant pH to separate proteins or isoelectric focusing, which uses a pH gradient to separate proteins. Isoelectric focusing is also the first step in 2-D gel polyacrylamide gel electrophoresis.\n\nIn biomolecules, proteins can be separated by ion exchange chromatography. Biological proteins are made up of zwitterionic amino acid compounds; the net charge of these proteins can be positive or negative depending on the pH of the environment. The specific pI of the target protein can be used to model the process around and the compound can then be purified from the rest of the mixture. Buffers of various pH can be used for this purification process to change the pH of the environment. When a mixture containing a target protein is loaded into an ion exchanger, the stationary matrix can be either positively-charged (for mobile anions) or negatively-charged (for mobile cations). At low pH values, the net charge of most proteins in the mixture is positive - in cation exchangers, these positively-charged proteins bind to the negatively-charged matrix. At high pH values, the net charge of most proteins is negative, where they bind to the positively-charged matrix in anion exchangers. When the environment is at a pH value equal to the protein's pI, the net charge is zero, and the protein is not bound to any exchanger, and therefore, can be eluted out.\n\nFor an amino acid with only one amine and one carboxyl group, the pI can be calculated from the mean of the pKas of this molecule.\n\nThe pH of an electrophoretic gel is determined by the buffer used for that gel. If the pH of the buffer is above the pI of the protein being run, the protein will migrate to the positive pole (negative charge is attracted to a positive pole). If the pH of the buffer is below the pI of the protein being run, the protein will migrate to the negative pole of the gel (positive charge is attracted to the negative pole). If the protein is run with a buffer pH that is equal to the pI, it will not migrate at all. This is also true for individual amino acids.\n\nIn the two examples (on the right) the isoelectric point is shown by the green vertical line. In glycine the pK values are separated by nearly 7 units so the concentration of the neutral species, glycine (GlyH), is effectively 100% of the analytical glycine concentration. Glycine may exist as a zwitterion at the isoelectric point, but the equilibrium constant for the isomerization reaction in solution\nis not known.\n\nThe other example, adenosine monophosphate is shown to illustrate the fact that a third species may, in principle, be involved. In fact the concentration of (AMP)H is negligible at the isoelectric point in this case.\nIf the pI is greater than the pH, the molecule will have a positive charge.\n\nA number of algorithms for estimating isoelectric points of peptides and proteins have been developed. Most of them use Henderson–Hasselbalch equation with different pK values. For instance, within the model proposed by Bjellqvist and co-workers the pK's were determined between closely related immobilines, by focusing the same sample in overlapping pH gradients. Some improvements in the methodology (especially in the determination of the pK values for modified amino acids) have been also proposed. More advanced methods take into account the effect of adjacent amino acids ±3 residues away from a charged aspartic or glutamic acid, the effects on free C terminus, as well as they apply a correction term to the corresponding pK values using genetic algorithm. Other recent approaches are based on a support vector machine algorithm and pKa optimization against experimentally known protein/peptide isoelectric points.\n\nMoreover, experimentally measured isoelectric point of proteins were aggregated into the databases. Recently, a database of isoelectric points for all proteins predicted using most of the available methods had been also developed.\n\nThe isoelectric points (IEP) of metal oxide ceramics are used extensively in material science in various aqueous processing steps (synthesis, modification, etc.). In the absence of chemisorbed or physisorbed species particle surfaces in aqueous suspension are generally assumed to be covered with surface hydroxyl species, M-OH (where M is a metal such as Al, Si, etc.). At pH values above the IEP, the predominate surface species is M-O, while at pH values below the IEP, M-OH species predominate. Some approximate values of common ceramics are listed below:\n\n\"Note: The following list gives the isoelectric point at 25 °C for selected materials in water. The exact value can vary widely, depending on material factors such as purity and phase as well as physical parameters such as temperature. Moreover, the precise measurement of isoelectric points can be difficult, thus many sources often cite differing values for isoelectric points of these materials.\"\n\nMixed oxides may exhibit isoelectric point values that are intermediate to those of the corresponding pure oxides. For example, a synthetically prepared amorphous aluminosilicate (AlO-SiO) was initially measured as having IEP of 4.5 (the electrokinetic behavior of the surface was dominated by surface Si-OH species, thus explaining the relatively low IEP value). Significantly higher IEP values (pH 6 to 8) have been reported for 3AlO-2SiO by others. Similarly, also IEP of barium titanate, BaTiO was reported in the range 5-6 while others got a value of 3.\n\nThe terms isoelectric point (IEP) and point of zero charge (PZC) are often used interchangeably, although under certain circumstances, it may be productive to make the distinction.\n\nIn systems in which H/OH are the interface potential-determining ions, the point of zero charge is given in terms of pH. The pH at which the surface exhibits a neutral net electrical charge is the point of zero charge at the surface. Electrokinetic phenomena generally measure zeta potential, and a zero zeta potential is interpreted as the point of zero net charge at the shear plane. This is termed the isoelectric point. Thus, the isoelectric point is the value of pH at which the colloidal particle remains stationary in an electrical field. The isoelectric point is expected to be somewhat different than the point of zero charge at the particle surface, but this difference is often ignored in practice for so-called pristine surfaces, i.e., surfaces with no specifically adsorbed positive or negative charges. In this context, specific adsorption is understood as adsorption occurring in a Stern layer or chemisorption. Thus, point of zero charge at the surface is taken as equal to isoelectric point in the absence of specific adsorption on that surface.\n\nAccording to Jolivet, in the absence of positive or negative charges, the surface is best described by the point of zero charge. If positive and negative charges are both present in equal amounts, then this is the isoelectric point. Thus, the PZC refers to the absence of any type of surface charge, while the IEP refers to a state of neutral net surface charge. The difference between the two, therefore, is the quantity of charged sites at the point of net zero charge. Jolivet uses the intrinsic surface equilibrium constants, p\"K\" and p\"K\" to define the two conditions in terms of the relative number of charged sites:\n\nFor large Δp\"K\" (>4 according to Jolivet), the predominant species is MOH while there are relatively few charged species - so the PZC is relevant. For small values of Δp\"K\", there are many charged species in approximately equal numbers, so one speaks of the IEP.\n\n\n\n", "id": "15056", "title": "Isoelectric point"}
{"url": "https://en.wikipedia.org/wiki?curid=23455526", "text": "Fluorescence in the life sciences\n\nFluorescence is used in the life sciences generally as a non-destructive way of tracking or analysing biological molecules by means of fluorescence.\nSome proteins or small molecules in cells are naturally fluorescent, which is called intrinsic fluorescence or autofluorescence (such as NADH, tryptophan or endogenous Chlorophyll, Phycoerythrin or green fluorescent protein). Alternatively, specific or general proteins, nucleic acids, lipids or small molecules can be \"labelled\" with an extrinsic fluorophore, a fluorescent dye which can be a small molecule, protein or quantum dot. Several techniques exist to exploit additional properties of fluorophores, such as fluorescence resonance energy transfer, where the energy is passed non-radiatively to a particular neighbouring dye, allowing proximity or protein activation to be detected; another is the change in properties, such as intensity, of certain dyes depending on their environment allowing their use in structural studies.\n\nThe principle behind fluorescence is that the fluorescent moiety contains electrons which can absorb a photon and briefly enter an excited state before either dispersing the energy non-radiatively or emitting it as a photon, but with a lower energy, i.e., at a longer wavelength (wavelength and energy are inversely proportional).\nThe difference in the excitation and emission wavelengths is called the Stokes shift, and the time that an excited electron takes to emit the photon is called a lifetime. The quantum yield is an indicator of the efficiency of the dye (it is the ratio of emitted photons per absorbed photon), and the extinction coefficient is the amount of light that can be absorbed by a fluorophore. Both the quantum yield and extinction coefficient are specific for each fluorophore and multiplied together calculates the brightness of the fluorescent molecule.\n\nFluorophores can be attached to proteins via specific functional groups, such as\nor non-specificately (glutaraldehyde) or non-covalently (\"e.g.\" via hydrophobicity, etc.).\n\nThese fluorophores are either small molecules, protein or quantum dots.\n\nOrganic fluorophores fluoresce thanks to delocalized electrons which can jump a band and stabilize the energy absorbed, hence most fluorophores are conjugated systems. Several families exits and their excitations range from the infrared to the ultraviolet. <br>Lanthanides (chelated) are uniquely fluorescent metals, which emit thanks to transitions involving 4\"f\" orbits, which are forbidden, hence they have very low absorption coefficients and slow emissions, requiring exitation through fluorescent organic chelators (\"e.g.\" dipicolinate-based Terbium (III) chelators ). <br>A third class of small molecule fluorophore is that of the transition metal-ligand complexes, which display molecular fluorescence from a metal-to-ligand charge transfer state which is partially forbidden, these are generally complexes of Ruthenium, Rhenium or Osmium.\n\nQuantum dots are fluorescent semiconductor nanoparticles.\n\nSeveral fluorescent protein exist in nature, but the most important one as a research tool is Green Fluorescent Protein (GFP) from the jellyfish \"Aequorea victoria\", which spontaneously fluoresces upon folding via specific serine-tyrosine-glycine residues. The benefit that GFP and other fluorescent proteins have over organic dyes or quantum dots is that they can be expressed exogenously in cells alone or as a fusion protein, a protein that is created by ligating the fluorescent gene (e.g., GFP) to another gene and whose expression is driven by a housekeeping gene promoter or another specific promoter. This approach allows fluorescent proteins to be used as reporters for any number of biological events, such as sub-cellular localization and expression patterns.\nA variant of GFP is naturally found in corals, specifically the Anthozoa, and several mutants have been created to span the visible spectra and fluoresce longer and more stably.\nOther proteins are fluorescent but require a fluorophore cofactor, and hence can only be used \"in vitro\"; these are often found in plants and algae (phytofluors, phycobiliprotein such as allophycocyanin).\n\nFluorescence, chemiluminescence and phosphorescence are 3 different types of luminescence properties, i.e. emission of light from a substance.\nFluorescence is a property where light is absorbed and remitted within a few nanoseconds (approx. 10ns) at a lower energy (=higher wavelength), while bioluminescence is biological chemiluminescence, a property where light is generated by a chemical reaction of an enzyme on a substrate.\nPhosphorescence is a property of materials to absorb light and emit the energy several milliseconds or more later (due to forbidden transitions to the ground state of a triplet state, while fluorescence occurs in exited singlet states). Until recently was not applicable to life science research due to the size of the inorganic particles. However the boundary between the fluorescence and phosphorescence is not clean cut as transition metal-ligand complexes, which combine a metal and several organic moieties, have long lifetimes, up to several microseconds (as they display mixed singlet-triplet states).\n\nPrior to its widespread use in the past three decades radioactivity was the most common label.\n\nThe advantages of fluorescence over radioactive labels are as follows:\nNote: a channel is similar to \"colour\" but distinct, it is the pair of excitation and emission filters specific for a dye, e.g. agilent microarrays are dual channel, working on cy3 and cy5, these are colloquially referred to as green and red.\n\nFluorescence is not necessarily more convenient to use because it requires specialized detection equipment of its own. For non-quantitative or relative quantification applications it can be useful but it is poorly suited for making absolute measurement because of fluorescence quenching, whereas measuring radioactively labeled molecules is always direct and highly sensitive.\n\nDisadvantages of fluorophores include:\n\nThe basic property of fluorescence are extensively used, such as a marker of labelled components in cells (fluorescence microscopy) or as an indicator in solution (Fluorescence spectroscopy), but other additional properties, not found with radioactivity, make it even more extensively used.\n\nFRET (Förster resonance energy transfer) is a property in which the energy of the excited electron of one fluorphore, called the donor, is passed on to a nearby acceptor dye, either a dark quencher or another fluorophore, which has an excitation spectrum which overlaps with the emission spectrum of the donor dye resulting in a reduced fluorescence.\nThis can be used to\n\nEnvironment-sensitive dyes change their properties (intensity, half-life, and excitation and emission spectra) depending on the polarity (hydrophobicity and charge) of their environments. examples include: Indole, Cascade Yellow, prodan, Dansyl, Dapoxyl, NBD, PyMPO, Pyrene and diethylaminocumarin.\nThis change is most pronounced when electron-donating and electron-withdrawing groups are placed at opposite ends of an aromatic ring system, as this results in a large change in dipole moment when excited.\n\nWhen a fluorophore is excited, it generally has a larger dipole moment (μ) than in the ground state (μ). Absorption of a photon by a fluorophore takes a few picoseconds. Before this energy is released (emission: 1–10 ns), the solvent molecules surrounding the fluorophore reorient (10–100 ps) due to the change in polarity in the excited singlet state; this process is called solvent relaxation. As a result of this relaxation, the energy of the excited state of the fluorophore is lowered (longer wavelength), hence fluorophores that have a large change in dipole moment have larger stokes shift changes in different solvents. The difference between the energy levels can be roughly determined with the Lipper-Mataga equation.\n\nIt should be noted that a hydrophobic dye is a dye which is insoluble in water, a property independent of solvatochromism. <br>Additionally, The term environment-sensitive in chemistry actually describes changes due to one of a variety of different environmental factors, such as pH or temperature, not just polarity; however, in biochemistry environment-sensitive fluorphore and solvatochromic fluorophore are used interchangeably: this convention is so widespread that suppliers describe them as environment-sensitive over solvatochromic.\n\nFluorescent moieties emit photons several nanoseconds after absorption following an exponential decay curve, which differs between dyes and depends on the surrounding solvent. When the dye is attached to a macromolecules the decay curve becomes multiexponential. Conjugated dyes generally have a lifetime between 1–10 ns, a small amount of longer lived exceptions exist, notably pyrene with a lifetime of 400ns in degassed solvents or 100ns in lipids and coronene with 200ns. On a different category of fluorphores are the fluorescent organometals (lanthanides and transition metal-ligand complexes) which have been previously described, which have much longer lifetimes due to the restricted states: lanthanides have lifetimes of 0.5 to 3 ms, while transition metal-ligand complexes have lifetimes of 10 ns to 10 µs. Note that fluorescent lifetime should not be confused with the photodestruction lifetime or the \"shelf-life\" of a dye.\n\nMultiphoton excitation is a way of focusing the viewing plane of the microscope by taking advantage of the phenomenon where two simultaneous low energy photons are absorbed by a fluorescent moiety which normally absorbs one photon with double their individual energy: say two NIR photons (800 nm) to excite a UV dye (400 nm).\n\nA perfectly immobile fluorescent moiety when exited with polarized light will emit light which is also polarized. However, if a molecule is moving, it will tend to \"scramble\" the polarization of the light by radiating at a different direction from the incident light.\n\n\n\nAlso, many biological molecules have an intrinsic fluorescence that can sometimes be used without the need to attach a chemical tag. Sometimes this intrinsic fluorescence changes when the molecule is in a specific environment, so the distribution or binding of the molecule can be measured. Bilirubin, for instance, is highly fluorescent when bound to a specific site on serum albumin. Zinc protoporphyrin, formed in developing red blood cells instead of hemoglobin when iron is unavailable or lead is present, has a bright fluorescence and can be used to detect these problems.\n\nThe number of fluorescence applications in the biomedical, biological and related sciences continuously expands. Methods of analysis in these fields are also growing, often with nomenclature in the form of acronyms such as: FLIM, FLI, FLIP, CALI, FLIE, FRET, FRAP, FCS, PFRAP, smFRET, FIONA, FRIPS, SHREK, SHRIMP or TIRF. Most of these techniques rely on fluorescence microscopes, which use high intensity light sources, usually mercury or xenon lamps, LEDs, or lasers, to excite fluorescence in the samples under observation. Optical filters then separate excitation light from emitted fluorescence to be detected by eye or with a (CCD) camera or other light detector (e.g., photomultiplier tubes, spectrographs). Considerable research is underway to improve the capabilities of such microscopes, the fluorescent probes used, and the applications they are applied to. Of particular note are confocal microscopes, which use a pinhole to achieve optical sectioning, which affords a quantitative, 3D view of the sample.\n\n", "id": "23455526", "title": "Fluorescence in the life sciences"}
{"url": "https://en.wikipedia.org/wiki?curid=4116487", "text": "PComb3H\n\npComb3H, a derivative of pComb3 optimized for expression of human fragments, is a phagemid used to express proteins such as zinc finger proteins and antibody fragments on phage pili for the purpose of phage display selection. \n\nFor the purpose of phage production, it contains the bacterial ampicillin resistance gene (for B-lactamase), allowing the growth of only transformed bacteria.\n", "id": "4116487", "title": "PComb3H"}
{"url": "https://en.wikipedia.org/wiki?curid=12128493", "text": "Plant transformation vector\n\nPlant transformation vectors are plasmids that have been specifically designed to facilitate the generation of transgenic plants. The most commonly used plant transformation vectors are termed binary vectors because of their ability to replicate in both \"E. coli\", a common lab bacterium, and \"Agrobacterium tumefaciens\", a bacterium used to insert the recombinant (customized) DNA into plants.\nPlant Transformation vectors contain three key elements;\n\nPropagate binary vector in \"E. coli\"\n\nIsolate binary vector from \"E.coli\" and engineer (introduce a foreign gene)\n\nRe-introduce engineered binary vector into \"E. coli\" to amplify\n\nIsolate engineered binary vector and introduce into \"Agrobacteria\" containing a modified (relatively small) Ti plasmid\n\nInfect plant tissue with engineered \"Agrobacteria\" (T-DNA containing the foreign gene gets inserted into a plant cell genome)\n\nIn each cell T-DNA gets integrated at a different site in the genome\n\nNote: There are many variations to these steps. A custom DNA plasmid sequence can be created and replicated in more than one way.\n\nForeign DNA inserted\n\nInsertional mutagenesis (but not lethal for the plant cell – as the organism is diploid)\n\nWe want to transform the whole organism, not just one cell. This is done by transforming plant cells in culture, selecting transformed cells and regenerating an entire plant from the transformed cell (e.g. tobacco)\n\nWhen the bacteria with the desired, implanted gene are grown, they are made containing a selector. A selector is a way to isolate and distinguish the desired cells. A gene that makes the cells resistant to an antibiotic such as the antibiotics kanamycin, ampicillin, spectinomycin or tetracyclin, is an easy selector to use. The desired cells (along with any other organisms growing within the culture) can be treated with an antibiotic, allowing the desired cells to survive while other organisms cannot. The antibiotic gene is not usually transferred to the plant cell but remains within the bacterial cell.\n\nPlasmids replicate to produce many plasmid molecules in each host bacterial cell. The number of copies of each plasmid in a bacterial cell is determined by the replication origin. This is the position within the plasmids molecule where DNA replication is initiated. Most binary vectors have a higher number of plasmids copies when they replicate in \"E. coli\", the plasmid copy-number is usually less when the plasmid is resident within \"Agrobacterium tumefaciens\".\nPlasmids can also be replicated in the polymerase chain reaction (PCR).\n\nT-DNA contains two types of genes: the oncogenic genes, encoding for enzymes involved in the synthesis of auxins and cytokinins and responsible for tumor formation; and the genes encoding for the synthesis of opines. These compounds, produced by condensation between amino acids and sugars, are synthesized and excreted by the crown gall cells and consumed by A. tumefaciens as carbon and nitrogen sources. Outside the T-DNA, are located the genes for the opine catabolism, the genes involved in the process of T-DNA transfer from the bacterium to the plant cell and the genes involved in bacterium-bacterium plasmid conjugative transfer. (Hooykaas and Schilperoort, 1992; Zupan and Zambrysky, 1995).\nThe T-DNA fragment is flanked by 25-bp direct repeats, which act as a cis element signal for the transfer apparatus. The process of T-DNA transfer is mediated by the cooperative action of proteins encoded by genes determined in the Ti plasmid virulence region (vir genes) and in the bacterial chromosome. The Ti plasmid also contains the genes for opine catabolism produced by the crown gall cells, and regions for conjugative transfer and for its own integrity and stability. The 30 kb virulence (vir) region is a regulon organized in six operons that are essential for the T-DNA transfer (virA, virB, virD, and virG) or for the increasing of transfer efficiency (virC and virE) (Hooykaas and Schilperoort, 1992; Zupan and Zambryski, 1995, Jeon et al., 1998). Different chromosomal-determined genetic elements have shown their functional role in the attachment of A. tumefaciens to the plant cell and bacterial colonization: the loci chvA and chvB, involved in the synthesis and excretion of the b -1,2 glucan (Cangelosi et al., 1989); the chvE required for the sugar enhancement of vir genes induction and bacterial chemotaxis (Ankenbauer et al., 1990, Cangelosi et al., 1990, 1991); the cel locus, responsible for the synthesis of cellulose fibrils (Matthysse 1983); the pscA (exoC) locus, playing its role in the synthesis of both cyclic glucan and acid succinoglycan (Cangelosi et at., 1987, 1991); and the att locus, which is involved in the cell surface proteins (Matthysse, 1987).\n\n", "id": "12128493", "title": "Plant transformation vector"}
{"url": "https://en.wikipedia.org/wiki?curid=21715314", "text": "Multiple displacement amplification\n\nMultiple displacement amplification (MDA) is a non-PCR based DNA amplification technique. This method can rapidly amplify minute amounts of DNA samples to a reasonable quantity for genomic analysis. The reaction starts by annealing random hexamer primers to the template: DNA synthesis is carried out by a high fidelity enzyme, preferentially Φ29 DNA polymerase, at a constant temperature. Compared with conventional PCR amplification techniques, MDA generates larger sized products with a lower error frequency. This method has been actively used in whole genome amplification (WGA) and is a promising method for application to single cell genome sequencing and sequencing-based genetic studies.\n\nMany biological and forensic cases involving genetic analysis require sequencing of DNA from minute amounts of sample, such as DNA from uncultured single cells or trace amounts of tissue collected from crime scenes. Conventional Polymerase Chain Reaction (PCR)-based DNA amplification methods require sequence-specific oligonucleotide primers and heat-stable (usually \"Taq\") polymerase, and can be used to generate significant amounts of DNA from minute amounts of DNA. However, this is not sufficient for modern techniques which use sequencing-based DNA analysis. Therefore, a more efficient non-sequence-specific method to amplify minute amounts of DNA is necessary, especially in single cell genomic studies.\n\nBacteriophage Φ29 DNA polymerase is a high-processivity enzyme that can produce DNA amplicons greater than 70 kilobase pairs. Its high fidelity and 3’–5' proofreading activity reduces the amplification error rate to 1 in 10−10 bases compared to conventional \"Taq\" polymerase with a reported error rate of 1 in 9,000. The reaction can be carried out at a moderate isothermal condition of 30 °C and therefore does not require a thermocycler. It has been actively used in cell-free cloning, which is the enzymatic method of amplifying DNA \"in vitro\" without cell culturing and DNA extraction. The large fragment of \"Bst\" DNA polymerase is also used in MDA, but Ф29 is generally preferred due to its sufficient product yield and proofreading activity.\n\nHexamer primers are sequences composed of six random nucleotides. For MDA applications, these primers are usually thiophosphate-modified at their 3’ end to convey resistance to the 3’–5’ exonuclease activity of Ф29 DNA polymerase. MDA reactions start with the annealing of such primers to the DNA template followed by polymerase-mediated chain elongation. Increasing numbers of primer annealing events happen along the amplification reaction.\n\nThe amplification reaction initiates when multiple primer hexamers anneal to the template. When DNA synthesis proceeds to the next starting site, the polymerase displaces the newly produced DNA strand and continues its strand elongation. The strand displacement generates newly synthesized single stranded DNA template for more primers to anneal. Further primer annealing and strand displacement on the newly synthesized template results in a hyper-branched DNA network. The sequence debranching during amplification results in high yield of the products. To separate the DNA branching network, S1 nucleases are used to cleave the fragments at displacement sites. The nicks on the resulting DNA fragments are repaired by DNA polymerase I.\n\nMDA can generate 1–2 µg of DNA from single cell with genome coverage of up to 99%. Products also have lower error rate and larger sizes compared to PCR based \"Taq\" amplification.\n\nGeneral work flow of MDA:\n\nMDA generates sufficient yield of DNA products. It is a powerful tool of amplifying DNA molecules from samples, such as uncultured microorganism or single cells to the amount that would be sufficient for sequencing studies. The large size of MDA-amplified DNA products also provides desirable sample quality for identifying the size of polymorphic repeat alleles. Its high fidelity also makes it reliable to be used in the single-nucleotide polymorphism (SNP) allele detection. Due to its strand displacement during amplification, the amplified DNA has sufficient coverage of the source DNA molecules, which provides high quality product for genomic analysis. The products of displaced strands can be subsequently cloned into vectors to construct library for subsequent sequencing reactions.\n\nADO is defined as the random non-amplification of one of the alleles present in a heterozygous sample. Some studies have reported the ADO rate of the MDA products to be 0–60%. This drawback decreases the accuracy of genotyping of single sample and misdiagnosis in other MDA involved applications. ADO appears to be independent of the fragment sizes and has been reported to have similar rate in other single-cell techniques. Possible solutions are the use of different lysis conditions or to carry out multiple rounds of amplifications from the diluted MDA products since PCR mediated amplification from cultured cells has been reported to give lower ADO rates.\n\n'Preferential amplification' is over-amplification of one of the alleles in comparison to the other. Most studies on MDA have reported this issue. The amplification bias is currently observed to be random. It might affect the analysis of small stretches of genomic DNA in identifying Short Tandem Repeats (STR) alleles.\n\nEndogenous template-independent primer-primer interaction is due to the random design of hexamer primers. One possible solution is to design constrained-randomized hexanucleotide primers that do not cross-hybridize.\n\nSingle, uncultured bacteria, such as \"Prochlorococcus\", and single fungal spores have been sequenced with the help of MDA.\n\nThe ability to sequence individual cells is also useful in combating human disease. Genomes from single human embryonic cells have been successfully amplified for sequencing using MDA, allowing preimplantation genetic diagnosis (PGD): screening for genetic health issues in an early-stage embryo before implantation. Diseases with heterogeneous properties, such as cancer, also benefit from MDA-based genome sequencing's ability to study mutations in individual cells.\n\nThe MDA products from a single cell have also been successfully used in array-comparative genomic hybridization experiments, which usually require a relatively large amount of amplified DNA.\n\nChromatin Immunoprecipitation results in production of complex mixtures of relatively short DNA fragments, which is challenging to amplify with MDA without causing a bias in the fragment representation. A method to circumvent this problem was proposed, which is based on conversion of these mixtures to circular concatemers using ligation, followed by Φ29 DNA polymerase-mediated MDA.\n\nTrace amount of samples collected from crime scenes can be amplified by MDA to the quantity that is enough for forensic DNA analysis, which is commonly used in identifying victims and suspects.\n", "id": "21715314", "title": "Multiple displacement amplification"}
{"url": "https://en.wikipedia.org/wiki?curid=25799", "text": "Retrovirus\n\nA retrovirus is a single-stranded positive-sense RNA virus with a DNA intermediate and, as an obligate parasite, targets a host cell. Once inside the host cell cytoplasm, the virus uses its own reverse transcriptase enzyme to produce DNA from its RNA genome, the reverse of the usual pattern, thus \"retro\" (backwards). The new DNA is then incorporated into the host cell genome by an integrase enzyme, at which point the retroviral DNA is referred to as a provirus. The host cell then treats the viral DNA as part of its own genome, translating and transcribing the viral genes along with the cell's own genes, producing the proteins required to assemble new copies of the virus. It is difficult to detect the virus until it has infected the host. At that point, the infection will persist indefinitely.\n\nIn most viruses, DNA is transcribed into RNA, and then RNA is translated into protein. However, retroviruses function differently, as their RNA is reverse-transcribed into DNA, which is integrated into the host cell's genome (when it becomes a provirus), and then undergoes the usual transcription and translational processes to express the genes carried by the virus. The information contained in a retroviral gene is thus used to generate the corresponding protein via the sequence: RNA → DNA → RNA → polypeptide. This extends the fundamental process identified by Francis Crick (one gene-one peptide) in which the sequence is DNA → RNA → peptide (proteins are made of one or more polypeptide chains; for example, haemoglobin is a four-chain peptide).\n\nRetroviruses are valuable research tools in molecular biology, and they have been used successfully in gene delivery systems.\n\nVirions of retroviruses consist of enveloped particles about 100 nm in diameter. The virions also contain two identical single-stranded RNA molecules 7–10 kilobases in length. Although virions of different retroviruses do not have the same morphology or biology, all the virion components are very similar.\n\nThe main virion components are:\n\n\nWhen retroviruses have integrated their own genome into the germ line, their genome is passed on to a following generation. These endogenous retroviruses (ERVs), contrasted with exogenous ones, now make up 5-8% of the human genome. Most insertions have no known function and are often referred to as \"junk DNA\". However, many endogenous retroviruses play important roles in host biology, such as control of gene transcription, cell fusion during placental development in the course of the germination of an embryo, and resistance to exogenous retroviral infection. Endogenous retroviruses have also received special attention in the research of immunology-related pathologies, such as autoimmune diseases like multiple sclerosis, although endogenous retroviruses have not yet been proven to play any causal role in this class of disease.\n\nWhile transcription was classically thought to occur only from DNA to RNA, reverse transcriptase transcribes RNA into DNA. The term \"retro\" in retrovirus refers to this reversal (making DNA from RNA) of the central dogma of molecular biology. Reverse transcriptase activity outside of retroviruses has been found in almost all eukaryotes, enabling the generation and insertion of new copies of retrotransposons into the host genome. These inserts are transcribed by enzymes of the host into new RNA molecules that enter the cytosol. Next, some of these RNA molecules are translated into viral proteins. For example, the \"gag\" gene is translated into molecules of the capsid protein, the \"pol\" gene is translated into molecules of reverse transcriptase, and the \"env\" gene is translated into molecules of the envelope protein. It is important to note that a retrovirus must \"bring\" its own reverse transcriptase in its capsid, otherwise it is unable to utilize the enzymes of the infected cell to carry out the task, due to the unusual nature of producing DNA from RNA.\n\nIndustrial drugs that are designed as protease and reverse transcriptase inhibitors are made such that they target specific sites and sequences within their respective enzymes. However these drugs can quickly become ineffective due to the fact that the gene sequences that code for the protease and the reverse transcriptase quickly mutate. These changes in bases cause specific codons and sites with the enzymes to change and thereby avoid drug targeting by losing the sites that the drug actually targets.\n\nBecause reverse transcription lacks the usual proofreading of DNA replication, a retrovirus mutates very often. This enables the virus to grow resistant to antiviral pharmaceuticals quickly, and impedes the development of effective vaccines and inhibitors for the retrovirus.\n\nOne difficulty faced with some retroviruses, such as the Moloney retrovirus, involves the requirement for cells to be actively dividing for transduction. As a result, cells such as neurons are very resistant to infection and transduction by retroviruses. This gives rise to a concern that insertional mutagenesis due to integration into the host genome might lead to cancer or leukemia. This is unlike \"Lentivirus\", a genus of \"Retroviridae\", which are able to integrate their RNA into the genome of non-dividing host cells.\n\n\nThis DNA can be incorporated into host genome as a provirus that can be passed on to progeny cells. The retrovirus DNA is inserted at random into the host genome. Because of this, it can be inserted into oncogenes. In this way some retroviruses can convert normal cells into cancer cells. Some provirus remains latent in the cell for a long period of time before it is activated by the change in cell environment.\n\nStudies of retroviruses led to the first demonstrated synthesis of DNA from RNA templates, a fundamental mode for transferring genetic material that occurs in both eukaryotes and prokaryotes. It has been speculated that the RNA to DNA transcription processes used by retroviruses may have first caused DNA to be used as genetic material. In this model, the RNA world hypothesis, cellular organisms adopted the more chemically stable DNA when retroviruses evolved to create DNA from the RNA templates.\n\nAn estimate of the date of evolution of the foamy-like endogenous retroviruses placed the time of the most recent common ancestor at > .\n\nGammaretroviral and lentiviral vectors for gene therapy have been developed that mediate stable genetic modification of treated cells by chromosomal integration of the transferred vector genomes. This technology is of use, not only for research purposes, but also for clinical gene therapy aiming at the long-term correction of genetic defects, e.g., in stem and progenitor cells. Retroviral vector particles with tropism for various target cells have been designed. Gammaretroviral and lentiviral vectors have so far been used in more than 300 clinical trials, addressing treatment options for various diseases. Retroviral mutations can be developed to make transgenic mouse models to study various cancers and their metastatic models.\n\nRetroviruses that cause tumor growth include \"Rous sarcoma virus\" and \"Mouse mammary tumor virus\". Cancer can be triggered by proto-oncogenes that were mistakenly incorporated into proviral DNA or by the disruption of cellular proto-oncogenes. Rous sarcoma virus contains the src gene that triggers tumor formation. Later it was found that a similar gene in cells is involved in cell signaling, which was most likely excised with the proviral DNA. Nontransforming viruses can randomly insert their DNA into proto-oncogenes, disrupting the expression of proteins that regulate the cell cycle. The promoter of the provirus DNA can also cause over expression of regulatory genes.\n\nThese are infectious RNA-containing viruses which are transmitted from human to human.\n\nThe following genera are included here:\n\nThese were previously divided into three subfamilies (\"Oncovirinae\", \"Lentivirinae\", and \"Spumavirinae\"), but are now divided into two: \"Orthoretrovirinae\" and \"Spumaretrovirinae\". The term oncovirus is now commonly used to describe a cancer-causing virus.\n\nRetroviruses were in 2 groups of the Baltimore classification.\n\nAll members of Group VI use virally encoded reverse transcriptase, an RNA-dependent DNA polymerase, to produce DNA from the initial virion RNA genome. This DNA is often integrated into the host genome, as in the case of retroviruses and pseudoviruses, where it is replicated and transcribed by the host.\n\nGroup VI includes:\n\nBoth families in Group VII have DNA genomes contained within the invading virus particles. The DNA genome is transcribed into both mRNA, for use as a transcript in protein synthesis, and pre-genomic RNA, for use as the template during genome replication. Virally encoded reverse transcriptase uses the pre-genomic RNA as a template for the creation of genomic DNA.\n\nGroup VII includes:\n\nEndogenous retroviruses are not formally included in this classification system, and are broadly classified into three classes, on the basis of relatedness to exogenous genera:\n\nAntiretroviral drugs are medications for the treatment of infection by retroviruses, primarily HIV. Different classes of antiretroviral drugs act on different stages of the HIV life cycle. Combination of several (typically three or four) antiretroviral drugs is known as highly active anti-retroviral therapy (HAART).\n\n\"Feline leukemia virus\" and \"Feline immunodeficiency virus\" infections are treated with biologics, including the only immunomodulator currently licensed for sale in the United States, Lymphocyte T-Cell Immune Modulator (LTCI).\n\n", "id": "25799", "title": "Retrovirus"}
{"url": "https://en.wikipedia.org/wiki?curid=26964637", "text": "SDD-AGE\n\nSDD-AGE is short for Semi-Denaturating Detergent Agarose Gel Electrophoresis. This is a method for detecting and characterizing large protein polymers which are stable in 2% SDS at room temperature, unlike most large protein complexes. This method is very useful for studying prions and amyloids, which are characterized by the formation of proteinaceous polymers. Agarose is used for the gel since the SDS-resistant polymers are large (in the 200-4000+ КDa range) and cannot enter a conventional polyacrylamide gel, which has small pores. Agarose on the other hand has large pores, which allows for the separation of polymers.\n\nUse of this method allowed researchers to understand that at least some types of prion aggregates existed in a two-level structure - protein molecules grouped into polymers, which are very stable and withstand treatment with 2% SDS at room temperature, and aggregates, which are bundles of polymers, that dissociate under these conditions.\n\nDifferences in the size of polymers can indicate the efficiency of polymer fragmentation \"in vivo\".\n\nThe method was created in the Molecular Genetics laboratory of the Russian Cardiology Research Institute and was published in 2003 by Kryndushkin et al. The original method used a TAE buffering system and incorporated a modified vacuum blotting system for the transfer of proteins onto a membrane (originally PVDF). The modified vacuum blotting system is actually a vacuum-assisted capillary transfer, since the vacuum only helps fluid that has already gone through the gel and membrane to leave the system.\n\nOther modifications have also been used, such as the one described in Bagriantsev et al., using traditional wet transfer and a TGB buffering system, and others using semi-dry transfer or capillary transfer.\n", "id": "26964637", "title": "SDD-AGE"}
{"url": "https://en.wikipedia.org/wiki?curid=24950345", "text": "Microscale thermophoresis\n\nMicroscale thermophoresis (MST) is a technology for the interaction analysis of biomolecules. Microscale thermophoresis is the directed movement of particles in a microscopic temperature gradient. Any change of the hydration shell of biomolecules due to changes in their structure/conformation results in a relative change of the movement along the temperature gradient and is used to determine binding affinities. MST allows measurement of interactions directly in solution without the need of immobilization to a surface (immobilization-free technology).\n\n\"Affinity\"\n\n\"Stoichiometry\"\n\n\"Thermodynamic parameters\"\n\n\"Additional information\"\n\nMST is based on the directed movement of molecules along temperature gradients, an effect termed thermophoresis. A spatial temperature difference ΔT leads to a depletion of molecule concentration in the region of elevated temperature, quantified by the Soret coefficient S: c/c = exp(-S ΔT)\n\nThermophoresis depends on the interface between molecule and solvent. Under constant buffer conditions, thermophoresis probes the size, charge and solvation entropy of the molecules. The thermophoresis of a fluorescently labeled molecule A typically differs significantly from the thermophoresis of a molecule-target complex AT due to size, charge and solvation entropy differences. This difference in the molecule's thermophoresis is used to quantify the binding in titration experiments under constant buffer conditions.\n\nThe thermophoretic movement of the fluorescently labelled molecule is measured by monitoring the fluorescence distribution F inside a capillary. The microscopic temperature gradient is generated by an IR-Laser, which is focused into the capillary and is strongly absorbed by water. The temperature of the aqueous solution in the laser spot is raised by up to ΔT=5 K. Before the IR-Laser is switched on a homogeneous fluorescence distribution F is observed inside the capillary. When the IR-Laser is switched on, two effects, separated by their time-scales, contribute to the new fluorescence distribution F. The thermal relaxation time is fast and induces a binding-dependent drop in the fluorescence of the dye due to its local environmental-dependent response to the temperature jump. On the slower diffusive time scale (10 s), the molecules move from the locally heated region to the outer cold regions. The local concentration of molecules decreases in the heated region until it reaches a steady-state distribution.\n\nWhile the mass diffusion D dictates the kinetics of depletion, S determines the steady-state concentration ratio c/c=exp(-S ΔT) ≈ 1-S ΔT under a temperature increase ΔT. The normalized fluorescence F=F/F measures mainly this concentration ratio, in addition to the temperature jump ∂F/∂T. In the linear approximation we find: F=1+(∂F/∂T-S)ΔT. Due to the linearity of the fluorescence intensity and the thermophoretic depletion, the normalized fluorescence from the unbound molecule F(A) and the bound complex F(AT) superpose linearly. By denoting x the fraction of molecules bound to targets, the changing fluorescence signal during the titration of target T is given by: F=(1-x) F(A)+x F(AT).\n\nQuantitative binding parameters are obtained by using a serial dilution of the binding substrate. By plotting F against the logarithm of the different concentrations of the dilution series, a sigmoidal binding curve is obtained. This binding curve can directly be fitted with the nonlinear solution of the law of mass action, with the dissociation constant K as result.\n\n", "id": "24950345", "title": "Microscale thermophoresis"}
{"url": "https://en.wikipedia.org/wiki?curid=17259468", "text": "Clone (cell biology)\n\nA clone is a group of \"identical\" cells that share a \"common ancestry\", meaning they are derived from the same cell.\n\nClonality implies the state of a cell or a substance being derived from one source or the other. Thus there are terms like \"polyclonal\"—derived from many clones; \"oligoclonal\"—derived from a few clones; and \"monoclonal\"—derived from one clone. These terms are most commonly used in context of antibodies or immunocytes.\n\nThis concept of clone assumes importance as all the cells that form a clone share common ancestry, which has a very significant consequence: shared genotype.\n\n\nMost other cells cannot divide indefinitely as after a few cycles of cell division the cells stop expressing an enzyme telomerase. The genetic material, in the form of deoxyribonucleic acid (DNA), continues to shorten with each cell division, and cells eventually stop dividing when they sense that their DNA is critically shortened. However, this enzyme in \"youthful\" cells replaces these lost bits (nucleotides) of DNA, thus making almost unlimited cycles of cell division possible. It is believed that the above-mentioned tissues have a constitutional elevated expression of telomerase. When ultimately many cells are produced by a single cell, \"clonal expansion\" is said to have taken place.\n\nA somewhat similar concept is that of \"clonal colony\" (also called a \"genet\"), wherein the cells (usually unicellular) also share a common ancestry, but which also requires the products of clonal expansion to reside at \"one place\", or in close proximity. A clonal colony would be well exemplified by a bacterial culture colony, or the bacterial films that are more likely to be found \"in vivo\" (e.g., in infected multicellular hosts). Whereas, the cells of clones dealt with here are specialized cells of a multicellular organism (usually vertebrates), and reside at quite distant places. For instance, two plasma cells belonging to the same clone could be derived from different memory cells (in turn with shared clonality) and could be residing in quite distant locations, such as the cervical (in the neck) and inguinal (in the groin) lymph nodes.\n\n", "id": "17259468", "title": "Clone (cell biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=5545351", "text": "Recombinase\n\nRecombinases are genetic recombination enzymes. DNA recombinases are widely used in multicellular organisms to manipulate the structure of genomes, and to control gene expression. These enzymes, derived from bacteria and fungi, catalyze directionally sensitive DNA exchange reactions between short (30–40 nucleotides) target site sequences that are specific to each recombinase. These reactions enable four basic functional modules, excision/insertion, inversion, translocation and cassette exchange, which have been used individually or combined in a wide range of configurations to control gene expression.\n\nTypes include:\n", "id": "5545351", "title": "Recombinase"}
{"url": "https://en.wikipedia.org/wiki?curid=26194", "text": "Restriction enzyme\n\nA restriction enzyme or restriction endonuclease is an enzyme that cleaves DNA into fragments at or near specific recognition sites within the molecule known as restriction sites. Restriction enzymes are commonly classified into four types, which differ in their structure and whether they cut their DNA substrate at their recognition site, or if the recognition and cleavage sites are separate from one another. To cut DNA, all restriction enzymes make two incisions, once through each sugar-phosphate backbone (i.e. each strand) of the DNA double helix.\n\nThese enzymes are found in bacteria and archaea and provide a defense mechanism against invading viruses. Inside a prokaryote, the restriction enzymes selectively cut up \"foreign\" DNA in a process called \"restriction\"; meanwhile, host DNA is protected by a modification enzyme (a methyltransferase) that modifies the prokaryotic DNA and blocks cleavage. Together, these two processes form the restriction modification system.\n\nOver 3000 restriction enzymes have been studied in detail, and more than 600 of these are available commercially. These enzymes are routinely used for DNA modification in laboratories, and they are a vital tool in molecular cloning.\n\nThe term restriction enzyme originated from the studies of phage λ, a virus that infects bacteria, and the phenomenon of host-controlled restriction and modification of such bacterial phage or bacteriophage. The phenomenon was first identified in work done in the laboratories of Salvador Luria and Giuseppe Bertani in the early 1950s. It was found that, for a bacteriophage λ that can grow well in one strain of \"Escherichia coli\", for example \"E. coli\" C, when grown in another strain, for example \"E. coli\" K, its yields can drop significantly, by as much as 3-5 orders of magnitude. The host cell, in this example \"E. coli\" K, is known as the restricting host and appears to have the ability to reduce the biological activity of the phage λ. If a phage becomes established in one strain, the ability of that phage to grow also becomes restricted in other strains. In the 1960s, it was shown in work done in the laboratories of Werner Arber and Matthew Meselson that the restriction is caused by an enzymatic cleavage of the phage DNA, and the enzyme involved was therefore termed a restriction enzyme.\n\nThe restriction enzymes studied by Arber and Meselson were type I restriction enzymes, which cleave DNA randomly away from the recognition site. In 1970, Hamilton O. Smith, Thomas Kelly and Kent Wilcox isolated and characterized the first type II restriction enzyme, \"Hin\"dII, from the bacterium \"Haemophilus influenzae\". Restriction enzymes of this type are more useful for laboratory work as they cleave DNA at the site of their recognition sequence. Later, Daniel Nathans and Kathleen Danna showed that cleavage of simian virus 40 (SV40) DNA by restriction enzymes yields specific fragments that can be separated using polyacrylamide gel electrophoresis, thus showing that restriction enzymes can also be used for mapping DNA. For their work in the discovery and characterization of restriction enzymes, the 1978 Nobel Prize for Physiology or Medicine was awarded to Werner Arber, Daniel Nathans, and Hamilton O. Smith. The discovery of restriction enzymes allows DNA to be manipulated, leading to the development of recombinant DNA technology that has many applications, for example, allowing the large scale production of proteins such as human insulin used by diabetics.\n\nRestriction enzymes likely evolved from a common ancestor and became widespread via horizontal gene transfer. In addition, there is mounting evidence that restriction endonucleases evolved as a selfish genetic element.\n\nRestriction enzymes recognize a specific sequence of nucleotides and produce a double-stranded cut in the DNA. The recognition sequences can also be classified by the number of bases in its recognition site, usually between 4 and 8 bases, and the number of bases in the sequence will determine how often the site will appear by chance in any given genome, e.g., a 4-base pair sequence would theoretically occur once every 4^4 or 256bp, 6 bases, 4^6 or 4,096bp, and 8 bases would be 4^8 or 65,536bp.<ref name=\"http://bioweb.uwlax.edu/genweb/molecular/seq_anal/restriction_map/restriction_map.htm\">Restriction Map</ref> Many of them are palindromic, meaning the base sequence reads the same backwards and forwards. In theory, there are two types of palindromic sequences that can be possible in DNA. The \"mirror-like\" palindrome is similar to those found in ordinary text, in which a sequence reads the same forward and backward on a single strand of DNA, as in GTAATG. The \"inverted repeat\" palindrome is also a sequence that reads the same forward and backward, but the forward and backward sequences are found in complementary DNA strands (i.e., of double-stranded DNA), as in GTATAC (GTATAC being complementary to CATATG). Inverted repeat palindromes are more common and have greater biological importance than mirror-like palindromes.\n\n\"Eco\"RI digestion produces \"sticky\" ends,\n\nwhereas SmaI restriction enzyme cleavage produces \"blunt\" ends:\n\nRecognition sequences in DNA differ for each restriction enzyme, producing differences in the length, sequence and strand orientation (5' end or 3' end) of a sticky-end \"overhang\" of an enzyme restriction.\n\nDifferent restriction enzymes that recognize the same sequence are known as neoschizomers. These often cleave in different locales of the sequence. Different enzymes that recognize and cleave in the same location are known as isoschizomers.\n\nNaturally occurring restriction endonucleases are categorized into four groups (Types I, II III, and IV) based on their composition and enzyme cofactor requirements, the nature of their target sequence, and the position of their DNA cleavage site relative to the target sequence. DNA sequence analyses of restriction enzymes however show great variations, indicating that there are more than four types. All types of enzymes recognize specific short DNA sequences and carry out the endonucleolytic cleavage of DNA to give specific fragments with terminal 5'-phosphates. They differ in their recognition sequence, subunit composition, cleavage position, and cofactor requirements, as summarised below:\n\n\nType I restriction enzymes were the first to be identified and were first identified in two different strains (K-12 and B) of \"E. coli\". These enzymes cut at a site that differs, and is a random distance (at least 1000 bp) away, from their recognition site. Cleavage at these random sites follows a process of DNA translocation, which shows that these enzymes are also molecular motors. The recognition site is asymmetrical and is composed of two specific portions—one containing 3–4 nucleotides, and another containing 4–5 nucleotides—separated by a non-specific spacer of about 6–8 nucleotides. These enzymes are multifunctional and are capable of both restriction and modification activities, depending upon the methylation status of the target DNA. The cofactors S-Adenosyl methionine (AdoMet), hydrolyzed adenosine triphosphate (ATP), and magnesium (Mg) ions, are required for their full activity. Type I restriction enzymes possess three subunits called HsdR, HsdM, and HsdS; HsdR is required for restriction; HsdM is necessary for adding methyl groups to host DNA (methyltransferase activity), and HsdS is important for specificity of the recognition (DNA-binding) site in addition to both restriction (DNA cleavage) and modification (DNA methyltransferase) activity.\n\nTypical type II restriction enzymes differ from type I restriction enzymes in several ways. They form homodimers, with recognition sites that are usually undivided and palindromic and 4–8 nucleotides in length. They recognize and cleave DNA at the same site, and they do not use ATP or AdoMet for their activity—they usually require only Mg as a cofactor. These enzymes cleave the phosphodiester bond of double helix DNA. It can either cleave at the center of both strands to yield a blunt end. Or it can cleave at a staggered position leaving overhangs called sticky ends. These are the most commonly available and used restriction enzymes. In the 1990s and early 2000s, new enzymes from this family were discovered that did not follow all the classical criteria of this enzyme class, and new subfamily nomenclature was developed to divide this large family into subcategories based on deviations from typical characteristics of type II enzymes. These subgroups are defined using a letter suffix.\n\nType IIB restriction enzymes (e.g., BcgI and BplI) are multimers, containing more than one subunit. They cleave DNA on both sides of their recognition to cut out the recognition site. They require both AdoMet and Mg cofactors. Type IIE restriction endonucleases (e.g., NaeI) cleave DNA following interaction with two copies of their recognition sequence. One recognition site acts as the target for cleavage, while the other acts as an allosteric effector that speeds up or improves the efficiency of enzyme cleavage. Similar to type IIE enzymes, type IIF restriction endonucleases (e.g. NgoMIV) interact with two copies of their recognition sequence but cleave both sequences at the same time. Type IIG restriction endonucleases (e.g., Eco57I) do have a single subunit, like classical Type II restriction enzymes, but require the cofactor AdoMet to be active. Type IIM restriction endonucleases, such as DpnI, are able to recognize and cut methylated DNA. Type IIS restriction endonucleases (e.g., \"Fok\"I) cleave DNA at a defined distance from their non-palindromic asymmetric recognition sites; this characteristic is widely used to perform in-vitro cloning techniques such as Golden Gate cloning. These enzymes may function as dimers. Similarly, Type IIT restriction enzymes (e.g., Bpu10I and BslI) are composed of two different subunits. Some recognize palindromic sequences while others have asymmetric recognition sites.\n\nType III restriction enzymes (e.g., EcoP15) recognize two separate non-palindromic sequences that are inversely oriented. They cut DNA about 20–30 base pairs after the recognition site. These enzymes contain more than one subunit and require AdoMet and ATP cofactors for their roles in DNA methylation and restriction, respectively. They are components of prokaryotic DNA restriction-modification mechanisms that protect the organism against invading foreign DNA. Type III enzymes are hetero-oligomeric, multifunctional proteins composed of two subunits, Res and Mod. The Mod subunit recognises the DNA sequence specific for the system and is a modification methyltransferase; as such, it is functionally equivalent to the M and S subunits of type I restriction endonuclease. Res is required for restriction, although it has no enzymatic activity on its own. Type III enzymes recognise short 5–6 bp-long asymmetric DNA sequences and cleave 25–27 bp downstream to leave short, single-stranded 5' protrusions. They require the presence of two inversely oriented unmethylated recognition sites for restriction to occur. These enzymes methylate only one strand of the DNA, at the N-6 position of adenosyl residues, so newly replicated DNA will have only one strand methylated, which is sufficient to protect against restriction. Type III enzymes belong to the beta-subfamily of N6 adenine methyltransferases, containing the nine motifs that characterise this family, including motif I, the AdoMet binding pocket (FXGXG), and motif IV, the catalytic region (S/D/N (PP) Y/F).\n\nType IV enzymes recognize modified, typically methylated DNA and are exemplified by the McrBC and Mrr systems of \"E. coli\".\n\nType V restriction enzymes (e.g., the cas9-gRNA complex from CRISPRs) utilize guide RNAs to target specific non-palindromic sequences found on invading organisms. They can cut DNA of variable length, provided that a suitable guide RNA is provided. The flexibility and ease of use of these enzymes make them promising for future genetic engineering applications.\n\nArtificial restriction enzymes can be generated by fusing a natural or engineered DNA binding domain to a nuclease domain (often the cleavage domain of the type IIS restriction enzyme \"Fok\"I). Such artificial restriction enzymes can target large DNA sites (up to 36 bp) and can be engineered to bind to desired DNA sequences. Zinc finger nucleases are the most commonly used artificial restriction enzymes and are generally used in genetic engineering applications, but can also be used for more standard gene cloning applications. Other artificial restriction enzymes are based on the DNA binding domain of TAL effectors.\n\nIn 2013, a new technology CRISPR-Cas9, based on a prokaryotic viral defense system, was engineered for editing the genome, and it was quickly adopted in laboratories. For more detail, read CRISPR (Clustered regularly interspaced short palindromic repeats).\n\nIn 2017 a group in Illinois announced using an Argonaute protein taken from Pyrococcus furiosus (PfAgo) along with guide DNA to edit DNA as artificial restriction enzymes.\n\nArtificial ribonucleases that act as restriction enzymes for RNA are also being developed. A PNA-based system, called PNAzymes, has a Cu(II)-2,9-dimethylphenanthroline group that mimics ribonucleases for specific RNA sequence and cleaves at a non-base-paired region (RNA bulge) of the targeted RNA formed when the enzyme binds the RNA. This enzyme shows selectivity by cleaving only at one site that either does not have a mismatch or is kinetically preferred out of two possible cleavage sites.\n\nSince their discovery in the 1970s, many restriction enzymes have been identified; for example, more than 3500 different Type II restriction enzymes have been characterized. Each enzyme is named after the bacterium from which it was isolated, using a naming system based on bacterial genus, species and strain. For example, the name of the \"Eco\"RI restriction enzyme was derived as shown in the box.\n\nIsolated restriction enzymes are used to manipulate DNA for different scientific applications.\n\nThey are used to assist insertion of genes into plasmid vectors during gene cloning and protein production experiments. For optimal use, plasmids that are commonly used for gene cloning are modified to include a short \"polylinker\" sequence (called the multiple cloning site, or MCS) rich in restriction enzyme recognition sequences. This allows flexibility when inserting gene fragments into the plasmid vector; restriction sites contained naturally within genes influence the choice of endonuclease for digesting the DNA, since it is necessary to avoid restriction of wanted DNA while intentionally cutting the ends of the DNA. To clone a gene fragment into a vector, both plasmid DNA and gene insert are typically cut with the same restriction enzymes, and then glued together with the assistance of an enzyme known as a DNA ligase.\n\nRestriction enzymes can also be used to distinguish gene alleles by specifically recognizing single base changes in DNA known as single nucleotide polymorphisms (SNPs). This is however only possible if a SNP alters the restriction site present in the allele. In this method, the restriction enzyme can be used to genotype a DNA sample without the need for expensive gene sequencing. The sample is first digested with the restriction enzyme to generate DNA fragments, and then the different sized fragments separated by gel electrophoresis. In general, alleles with correct restriction sites will generate two visible bands of DNA on the gel, and those with altered restriction sites will not be cut and will generate only a single band. A DNA map by restriction digest can also be generated that can give the relative positions of the genes. The different lengths of DNA generated by restriction digest also produce a specific pattern of bands after gel electrophoresis, and can be used for DNA fingerprinting.\n\nIn a similar manner, restriction enzymes are used to digest genomic DNA for gene analysis by Southern blot. This technique allows researchers to identify how many copies (or paralogues) of a gene are present in the genome of one individual, or how many gene mutations (polymorphisms) have occurred within a population. The latter example is called restriction fragment length polymorphism (RFLP).\n\nArtificial restriction enzymes created by linking the \"Fok\"I DNA cleavage domain with an array of DNA binding proteins or zinc finger arrays, denoted zinc finger nucleases (ZFN), are a powerful tool for host genome editing due to their enhanced sequence specificity. ZFN work in pairs, their dimerization being mediated in-situ through the \"Fok\"I domain. Each zinc finger array (ZFA) is capable of recognizing 9–12 base pairs, making for 18–24 for the pair. A 5–7 bp spacer between the cleavage sites further enhances the specificity of ZFN, making them a safe and more precise tool that can be applied in humans. A recent Phase I clinical trial of ZFN for the targeted abolition of the CCR5 co-receptor for HIV-1 has been undertaken.\n\nOthers have proposed using the bacteria R-M system as a model for devising human anti-viral gene or genomic vaccines and therapies since the RM system serves an innate defense-role in bacteria by restricting tropism by bacteriophages. There is research on REases and ZFN that can cleave the DNA of various human viruses, including HSV-2, high-risk HPVs and HIV-1, with the ultimate goal of inducing target mutagenesis and aberrations of human-infecting viruses. Interestingly, the human genome already contains remnants of retroviral genomes that have been inactivated and harnessed for self-gain. Indeed, the mechanisms for silencing active L1 genomic retroelements by the three prime repair exonuclease 1 (TREX1) and excision repair cross complementing 1(ERCC) appear to mimic the action of RM-systems in bacteria, and the non-homologous end-joining (NHEJ) that follows the use of ZFN without a repair template.\n\nExamples of restriction enzymes include:\nKey:\n<nowiki>*</nowiki> = blunt ends\nN = C or G or T or A\nW = A or T\n\n\n\"General Information:\"\n\"Databases:\"\n\"Software:\"\n", "id": "26194", "title": "Restriction enzyme"}
{"url": "https://en.wikipedia.org/wiki?curid=3264380", "text": "Prokaryotic translation\n\nProkaryotic translation is the process by which messenger RNA is translated into proteins in prokaryotes.\n\nInitiation of translation in prokaryotes involves the assembly of the components of the translation system, which are: the two ribosomal subunits (50S and 30S subunits); the mature mRNA to be translated; the tRNA charged with N-formylmethionine (the first amino acid in the nascent peptide); guanosine triphosphate (GTP) as a source of energy, and the three prokaryotic initiation factors IF1, IF2, and IF3, which help the assembly of the initiation complex. Variations in the mechanism can be anticipated.\n\nThe ribosome has three active sites: the A site, the P site, and the E site. The \"A site\" is the point of entry for the aminoacyl tRNA (except for the first aminoacyl tRNA, which enters at the P site). The \"P site\" is where the peptidyl tRNA is formed in the ribosome. And the \"E site\" which is the exit site of the now uncharged tRNA after it gives its amino acid to the growing peptide chain.\n\nThe selection of an initiation site (usually an AUG codon) depends on the interaction between the 30S subunit and the mRNA template. The 30S subunit binds to the mRNA template at a purine-rich region (the Shine-Dalgarno sequence) upstream of the AUG initiation codon. The Shine-Dalgarno sequence is complementary to a pyrimidine rich region on the 16S rRNA component of the 30S subunit. This sequence has been evolutionarily conserved and plays a major role in the microbial world we know today. During the formation of the initiation complex, these complementary nucleotide sequences pair to form a double stranded RNA structure that binds the mRNA to the ribosome in such a way that the initiation codon is placed at the P site.\n\nWell-known coding regions that do not have AUG initiation codons are those of \"lacI\" (GUG) and \"lacA\" (UUG) in the \"E. coli\" lac operon. Two more recent studies have independently shown that 17 or more non-AUG start codons may initiate translation in \"E. coli\" .\n\nElongation of the polypeptide chain involves addition of amino acids to the carboxyl end of the growing chain. The growing protein exits the ribosome through the polypeptide exit tunnel in the large subunit.\n\nElongation starts when the fMet-tRNA enters the P site, causing a conformational change which opens the A site for the new aminoacyl-tRNA to bind. This binding is facilitated by elongation factor-Tu (EF-Tu), a small GTPase. For fast and accurate recognition of the appropriate tRNA, the ribosome utilizes large conformational changes (conformational proofreading) \nNow the P site contains the beginning of the peptide chain of the protein to be encoded and the A site has the next amino acid to be added to the peptide chain. The growing polypeptide connected to the tRNA in the P site is detached from the tRNA in the P site and a peptide bond is formed between the last amino acids of the polypeptide and the amino acid still attached to the tRNA in the A site. This process, known as \"peptide bond formation\", is catalyzed by a ribozyme (the 23S ribosomal RNA in the 50S ribosomal subunit). Now, the A site has the newly formed peptide, while the P site has an uncharged tRNA (tRNA with no amino acids). The newly formed peptide in the A site tRNA is known as \"dipeptide\" and the whole assembly is called \"dipeptidyl-tRNA\". The tRNA in the P site minus the amino acid is known to be \"deacylated\". In the final stage of elongation, called \"translocation\", the \"deacylated\" tRNA (in the P site) and the \"dipeptidyl-tRNA\" (in the A site) along with its corresponding codons move to the E and P sites, respectively, and a new codon moves into the A site. This process is catalyzed by elongation factor G (EF-G). The deacylated tRNA at the E site is released from the ribosome during the next A-site occupation by an aminoacyl-tRNA again facilitated by EF-Tu.\n\nThe ribosome continues to translate the remaining codons on the mRNA as more aminoacyl-tRNA bind to the A site, until the ribosome reaches a stop codon on mRNA(UAA, UGA, or UAG).\n\nThe translation machinery works relatively slowly compared to the enzyme systems that catalyze DNA replication. Proteins in prokaryotes are synthesized at a rate of only 18 amino acid residues per second, whereas bacterial replisomes synthesize DNA at a rate of 1000 nucleotides per second. This difference in rate reflects, in part, the difference between polymerizing four types of nucleotides to make nucleic acids and polymerizing 20 types of amino acids to make proteins. Testing and rejecting incorrect aminoacyl-tRNA molecules takes time and slows protein synthesis. In bacteria, translation initiation occurs as soon as the 5' end of an mRNA is synthesized, and translation and transcription are coupled. This is not possible in eukaryotes because transcription and translation are carried out in separate compartments of the cell (the nucleus and cytoplasm).\n\nTermination occurs when one of the three termination codons\nmoves into the A site. These codons are not recognized by any tRNAs. Instead, they are recognized by proteins called release factors, namely RF1 (recognizing the UAA and UAG stop codons) or RF2 (recognizing the UAA and UGA stop codons). These factors trigger the hydrolysis of the ester bond in peptidyl-tRNA and the release of the newly synthesized protein from the ribosome. A third release factor RF-3 catalyzes the release of RF-1 and RF-2 at the end of the termination process.\n\nThe post-termination complex formed by the end of the termination step consists of mRNA with the termination codon at the A-site, an uncharged tRNA in the P site, and the intact 70S ribosome. Ribosome recycling step is responsible for the disassembly of the post-termination ribosomal complex. Once the nascent protein is released in termination, Ribosome Recycling Factor and Elongation Factor G (EF-G) function to release mRNA and tRNAs from ribosomes and dissociate the 70S ribosome into the 30S and 50S subunits. IF3 then replaces the deacylated tRNA releasing the mRNA. All translational components are now free for additional rounds of translation.\n\nTranslation is carried out by more than one ribosome simultaneously. Because of the relatively large size of ribosomes, they can only attach to sites on mRNA 35 nucleotides apart. The complex of one mRNA and a number of ribosomes is called a polysome or polyribosome.\n\nWhen bacterial cells run out of nutrients, they enter stationary phase and downregulate protein synthesis. Several processes mediate this transition. For instance, in \"E. coli\", 70S ribosomes form 90S dimers upon binding with a small 6.5 kDa protein, ribosome modulation factor RMF. These intermediate ribosome dimers can subsequently bind a hibernation promotion factor (the 10.8 kDa protein, HPF) molecule to form a mature 100S ribosomal particle, in which the dimerization interface is made by the two 30S subunits of the two participating ribosomes. The ribosome dimers represent a hibernation state and are translationally inactive. A third protein that can bind to ribosomes when \"E. coli\" cells enter the stationary phase is YfiA (previously known as RaiA). HPF and YfiA are structurally similar, and both proteins can bind to the catalytic A- and P-sites of the ribosome. RMF blocks ribosome binding to mRNA by preventing interaction of the messenger with 16S rRNA. When bound to the ribosomes the C-terminal tail of \"E. coli \"YfiA interferes with the binding of RMF, thus preventing dimerization and resulting in the formation of translationally inactive monomeric 70S ribosomes.\nIn addition to ribosome dimerization, the joining of the two ribosomal subunits can be blocked by RsfS (formerly called RsfA or YbeB). RsfS binds to L14, a protein of the large ribosomal subunit, and thereby blocks joining of the small subunit to form a functional 70S ribosome, slowing down or blocking translation entirely. RsfS proteins are found in almost all eubacteria (but not archaea) and homologs are present in mitochondria and chloroplasts (where they are called C7orf30 and \"iojap\", respectively). However, it is not known yet how the expression or activity of RsfS is regulated.\n\nAnother ribosome-dissociation factor in \"Escherichia coli\" is HflX, previously a GTPase of unknown function. Zhang et al. (2015) showed that HflX is a heat shock–induced ribosome-splitting factor capable of dissociating vacant as well as mRNA-associated ribosomes. The N-terminal effector domain of HflX binds to the peptidyl transferase center in a strikingly similar manner as that of the class I release factors and induces dramatic conformational changes in central intersubunit bridges, thus promoting subunit dissociation. Accordingly, loss of HflX results in an increase in stalled ribosomes upon heat shock and possibly other stress conditions.\n\nSeveral antibiotics exert their action by targeting the translation process in bacteria. They exploit the differences between prokaryotic and eukaryotic translation mechanisms to selectively inhibit protein synthesis in bacteria without affecting the host.\n\n", "id": "3264380", "title": "Prokaryotic translation"}
{"url": "https://en.wikipedia.org/wiki?curid=3264389", "text": "Eukaryotic translation\n\nEukaryotic translation is the biological process by which messenger RNA is translated into proteins in eukaryotes. It consists of four phases: initiation, elongation, termination, and recycling.\n\nInitiation of translation usually involves the interaction of certain key proteins with a special tag bound to the 5'-end of an mRNA molecule, the 5' cap, as well as with the 5' UTR. The protein factors bind the small ribosomal subunit (also referred to as the 40S subunit), and these initiation factors hold the mRNA in place. eIF3 is associated with the 40S ribosomal subunit, and plays a role in keeping the large (60S) ribosomal subunit from prematurely binding. eIF3 also interacts with the eIF4F complex, which consists of three other initiation factors: eIF4A, eIF4E, and eIF4G. eIF4G is a scaffolding protein that directly associates with both eIF3 and the other two components. eIF4E is the cap-binding protein. Binding of the cap by eIF4E is often considered the rate-limiting step of cap-dependent initiation, and the concentration of eIF4E is a regulatory nexus of translational control. Certain viruses cleave a portion of eIF4G that binds eIF4E, thus preventing cap-dependent translation to hijack the host machinery in favor of the viral (cap-independent) messages. eIF4A is an ATP-dependent RNA helicase, which aids the ribosome in resolving certain secondary structures formed along the mRNA transcript.\n\nThe poly(A)-binding protein (PABP) also associates with the eIF4F complex via eIF4G, and binds the poly-A tail of most eukaryotic mRNA molecules. This protein has been implicated in playing a role in circularization of the mRNA during translation. This 43S preinitiation complex (43S PIC) accompanied by the protein factors moves along the mRNA chain toward its 3'-end, in a process known as 'scanning', to reach the start codon (typically AUG). In eukaryotes and archaea, the amino acid encoded by the start codon is methionine. The Met-charged initiator tRNA (Met-tRNA) is brought to the P-site of the small ribosomal subunit by eukaryotic initiation factor 2 (eIF2). It hydrolyzes GTP, and signals for the dissociation of several factors from the small ribosomal subunit, eventually leading to the association of the large subunit (or the 60S subunit). The complete ribosome (80S) then commences translation elongation.\n\nRegulation of protein synthesis partly influenced by phosphorylation of eIF2 (via the α subunit), which is a part of the eIF2-GTP-Met-tRNA ternary complex (eIF2-TC). When large numbers of eIF2 are phosphorylated, protein synthesis is inhibited. This occurs if there is amino acid starvation or there has been a virus infection. However, naturally a small percentage of this initiation factor is phosphorylated. Another regulator is 4EBP, which binds to the initiation factor eIF4E and inhibits its interactions with eIF4G, thus preventing cap-dependent initiation. To oppose the effects of the 4EBP, growth factors phosphorylate 4EBP, reducing its affinity for eIF4E and permitting protein synthesis.\n\nWhile the global regulation of protein synthesis is achieved by modulating the expression of key initiation factors as well as the number of ribosomes, individual mRNAs can have different translation rates due to presence of regulatory sequence elements. This has been shown to be important in a variety of settings including yeast meiosis and ethylene response in plants. In addition, recent work in yeast and humans suggest that evolutionary divergence in cis-regulatory sequences can impact translation regulation. Additionally, RNA helicases such as DHX29 and Ded1/DDX3 may participate in the process of translation initiation, especially for mRNAs with structured 5'UTRs.\n\nThe best-studied example of cap-independent translation initiation in eukaryotes is that by the Internal ribosome entry site (IRES). What differentiates cap-independent translation from cap-dependent translation is that cap-independent translation does not require the 5' cap to initiate scanning from the 5' end of the mRNA until the start codon. The ribosome can be trafficked to the start site by direct binding, initiation factors, and/or ITAFs (IRES trans-acting factors) bypassing the need to scan the entire 5' UTR. This method of translation has been found important in conditions that require the translation of specific mRNAs during cellular stress, when overall translation is reduced. Examples include factors responding to apoptosis and stress-induced responses.\n\nElongation depends on eukaryotic elongation factors. At the end of the initiation step, the mRNA is positioned so that the next codon can be translated during the elongation stage of protein synthesis. The initiator tRNA occupies the P site in the ribosome, and the A site is ready to receive an aminoacyl-tRNA. During chain elongation, each additional amino acid is added to the nascent polypeptide chain in a three-step microcycle. The steps in this microcycle are (1) positioning the correct aminoacyl-tRNA in the A site of the ribosome, (2) forming the peptide bond and (3) shifting the mRNA by one codon relative to the ribosome.\n\nUnlike bacteria, in which translation initiation occurs as soon as the 5' end of an mRNA is synthesized, in eukaryotes such tight coupling between transcription and translation is not possible because transcription and translation are carried out in separate compartments of the cell (the nucleus and cytoplasm). Eukaryotic mRNA precursors must be processed in the nucleus (e.g., capping, polyadenylation, splicing) before they are exported to the cytoplasm for translation.\n\nTranslation can also be affected by ribosomal pausing, which can trigger endonucleolytic attack of the mRNA, a process termed mRNA no-go decay. Ribosomal pausing also aids co-translational folding of the nascent polypeptide on the ribosome, and delays protein translation while it is encoding mRNA. This can trigger ribosomal frameshifting.\n\nTermination of elongation depends on eukaryotic release factors. The process is similar to that of prokaryotic termination, but unlike prokaryotic termination, there is a universal release factor, eRF1, that recognizes all three stop codons. Upon termination, the ribosome is disassembled and the completed polypeptide is released. eRF3 is a ribosome-dependent GTPase that helps eRF1 release the completed polypeptide. The human genome encodes a few genes whose mRNA stop codon are surprisingly leaky: In these genes, termination of translation is inefficient due to special RNA bases in the vicinity of the stop codon. Leaky termination in these genes leads to translational readthrough of up to 10% of the stop codons of these genes. Some of these genes encode functional protein domains in their readthrough extension so that new protein isoforms can arise. This process has been termed 'functional translational readthrough'.\n\n\n", "id": "3264389", "title": "Eukaryotic translation"}
{"url": "https://en.wikipedia.org/wiki?curid=4414307", "text": "Overlap extension polymerase chain reaction\n\nThe overlap extension polymerase chain reaction (or OE-PCR) is a variant of PCR. It is also referred to as Splicing by overlap extension / Splicing by overhang extension (SOE) PCR. It is used to insert specific mutations at specific points in a sequence or to splice smaller DNA fragments into a larger polynucleotide.\n\nAs in most PCR reactions, two primers—one for each end—are used per sequence. To splice two DNA molecules, special primers are used at the ends that are to be joined. For each molecule, the primer at the end to be joined is constructed such that it has a 5' overhang complementary to the end of the other molecule. Following annealing when replication occurs, the DNA is extended by a new sequence that is complementary to the molecule it is to be joined to. Once both DNA molecules are extended in such a manner, they are mixed and a PCR is carried out with only the primers for the far ends. The overlapping complementary sequences introduced will serve as primers and the two sequences will be fused. This method has an advantage over other gene splicing techniques in not requiring restriction sites.\n\nTo get higher yields, some primers are used in excess as in asymmetric PCR.\n\nTo insert a mutation into a DNA sequence, a specific primer is designed. The primer may contain a single substitution or contain a new sequence at its 5' end. If a deletion is required, a sequence that is 5' of the deletion is added, because the 3' end of the primer must have complementarity to the template strand so that the primer can sufficiently anneal to the template DNA.\n\nFollowing annealing of the primer to the template, DNA replication proceeds to the end of the template. The duplex is denatured and the second primer anneals to the newly formed DNA strand, containing sequence from the first primer. Replication proceeds to produce a strand of the required sequence, containing the mutation.\n\nThe duplex is denatured again and the first primer can now bind to the latest DNA strand. The replication reaction continues to produce a fully dimerised DNA fragment. After further PCR cycles, to amplify the DNA, the sample can be separated by agarose gel electrophoresis, followed by electroelution for collection.\n\nEfficiently generating oligonucleotides beyond ~110 nucleotides in length is very difficult, so to insert a mutation further into a sequence than a 110 nt primer will allow, it is necessary to employ overlap extension PCR. In OE-PCR the sequence being modified is used to make two modified strands with the mutation at opposite ends, using the technique described above. After mixing and denaturation, the strands are allowed to anneal to produce three different combinations as detailed in the diagram. Only the duplex without overlap at the 5' end will allow extension by DNA polymerase in 3' to 5' direction.\n\nFollowing separation, the eluted fragments of appropriate size are subject to normal PCR, using the outermost primers used in the initial, mutagenic PCR reactions.\n", "id": "4414307", "title": "Overlap extension polymerase chain reaction"}
{"url": "https://en.wikipedia.org/wiki?curid=27557852", "text": "Stable nucleic acid lipid particle\n\nStable nucleic acid lipid particles (SNALPs) are microscopic particles approximately 120 nanometers in diameter, smaller than the wavelengths of visible light. They have been used to deliver siRNAs therapeutically to mammals \"in vivo\". In SNALPs, the siRNA is surrounded by a lipid bilayer containing a mixture of cationic and fusogenic lipids, coated with diffusible polyethylene glycol.\n\nRNA interference(RNAi) is a process that occurs naturally within the cytoplasm inhibiting gene expression at specific sequences. Regulation of gene expression through RNAi is possible by introducing small interfering RNAs(siRNAs), which effectively silence expression of a targeted gene. RNAi activates the RNA-induced silencing complex(RISC) containing siRNA, siRNA derived from cleaved dsRNA. The siRNA guides the RISC complex to a specific sequence on the mRNA that is cleaved by RISC and, consequently, silences those genes.\n\nHowever, without modifications to the RNA backbone or inclusion of inverted bases at either end, siRNA instability in the plasma makes it extremely difficult to apply this technique \"in vivo\". Pattern recognition receptors(PRRs), which can be grouped as endocytic PRRs or signaling PRRs, are expressed in all cells of the innate immune system. Signaling PRRs, in particular, include Toll-like receptors(TLRs) and are involved primarily with identifying pathogen-associated molecular patterns(PAMPs). For example, TLRs can recognize specific regions conserved in various pathogens, recognition stimulating an immune response with potentially devastating effects to the organism. In particular, TLR 3 recognizes both dsRNA characteristic of viral replication and siRNA, which is also double-stranded. In addition to this instability, another limitation of siRNA therapy concerns the inability to target a tissue with any specificity.\n\nSNALPs, though, may provide the stability and specificity required for this mode of RNAi therapy to be effective. Consisting of a lipid bilayer, SNALPs are able to provide stability to siRNAs by protecting them from nucleases within the plasma that would degrade them. In addition, delivery of siRNAs is subject to endosomal trafficking, potentially exposing them to TLR3 and TLR7, and can lead to activation of interferons and proinflammatory cytokines. However, SNALPs allow siRNA uptake into the endosome without activating Toll-like receptors and consequently stimulating an impeding immune response, thus enabling siRNA escape from the endosome.\n\nDownregulation of gene expression via siRNA has been an important research tool in \"in vitro\" studies. Susceptibility of siRNAs to nuclease degradation, though, makes use of them \"in vivo\" problematic. In 2005, researchers working with hepatitis B virus(HBV) in rodents, determined that certain modifications of the siRNA prevented degradation by nucleases within the plasma and lead to increased gene silencing compared to unmodified siRNA. Modifications to the sense and antisense strands were made differentially. With respect to both sense and antisense strands, 2'-OH was substituted with 2'-fluoro at all pyrimidine positions. In addition, sense strands were modified at all purine positions with deoxyribose, antisense strands modified with 2'-\"O\"-methyl at the same positions. The 5' and 3' ends of the sense strand were capped with abasic inverted repeats, while a phosphorothioate linkage was incorporated at the 3' end of the antisense strand.\n\nAlthough this research demonstrated a potential RNAi therapy using modified siRNA, the 90% reduction in HBV DNA in rodents resulted from a 30 mg/kg dosage with frequent administration. Because this is not a viable dosing regime, this same group looked at the effects of encapsulating the siRNA in a PEGylated lipid bilayer, or SNALP. Specifically, the lipid bilayer facilitates uptake into the cell and subsequent release from the endosome, the PEGylated outer layer providing stability during formulation due to the resulting hydrophilicity of the exterior. According to this 2005 study, researchers obtained 90% reduction in HBV DNA with a 3 mg/kg/day dose of siRNA for three days, a dose substantially lower than the earlier study. In addition, in contrast to unmodified or modified and non-encapsulated siRNA, administration of SNALP-delivered siRNA resulted in no detectable levels of interferons, such as IFN-a, or inflammatory cytokines associated with immunostimulation. Even so, researchers acknowledged that more work was necessary in order to reach a feasible dose and dosing regime.\n\nIn 2006, researchers working on silencing of apolipoprotein B(ApoB) in non-human primates achieved 90% silencing with a single dose of 2.5 mg/kg of SNALP-delivered APOB-specific siRNA. ApoB is a protein involved with the assembly and secretion of very-low-density lipoprotein(VLDL) and low-density lipoprotein(LDL), and it is expressed primarily in the liver and jejunum. Both VLDL and LDL are important in cholesterol transport and its metabolism. Not only was this degree of silencing observed very quickly, in about 24 hours post-administration, but the silencing effects maintained for over 22 days after only a single dose. Researchers tested a 1 mg/kg single dose, too, obtaining a 68% silencing of the target gene, indicating dose-dependent silencing. This dose-dependent silencing was evident not only on the degree of silencing but the duration of silencing, expression of the target gene recovering 72 hours post-administration.\n\nAlthough SNALPs having a 100 nm diameter have been used effectively to target specific genes for silencing, there are a variety of systemic barriers that relate specifically to size. For example, diffusion into solid tumors is impeded by large SNALPs and, similarly, inflamed cells having enhanced permeation and retention make it difficult for large SNALPs to enter. In addition, reticuloendothelial elimination, blood-brain barrier size-selectivity and limitations of capillary fenestrae all necessitate a smaller SNALP in order to effectively deliver target-specific siRNA. In 2012, scientists in Germany developed what they termed \"mono-NALPs\" using a fairly simple solvent exchange method involving progressive dilution of a 50% isopropanol solution. What results is a very stable delivery system similar to traditional SNALPs, but one having only a diameter of 30 nm. The mono-NALPs developed here, however, are inactive, but can become active carriers by implementing specific targeting and release mechanisms used by similar delivery systems.\n\nIn May 2010, an application of SNALPs to the Ebola Zaire virus made headlines, as the preparation was able to cure rhesus macaques when administered shortly after their exposure to a lethal dose of the virus, which can be up to 90% lethal to humans in sporadic outbreaks in Africa. The treatment used for rhesus macaques consisted of three siRNAs (staggered duplexes of RNA) targeting three viral genes. The SNALPs (around 81 nm in size here) were formulated by spontaneous vesiculation from a mixture of cholesterol, dipalmitoyl phosphatidylcholine, 3-N-[(ω-methoxy\npoly(ethylene glycol)2000)carbamoyl]-1,2-dimyrestyloxypropylamine, and cationic 1,2-dilinoleyloxy-3-N,N-dimethylaminopropane.\n\nIn addition to the rhesus macaque application, SNALPs have also been proven to protect cavia porcellua from viremia and death when administered shortly after postexposure to ZEBOV. A polymerase (L) gene-specific siRNAs delivery system was imposed upon four genes associated with the viral genomic RNA in the ribonucleoprotein complex found within EBOV particles (three of which match the application above): NP, VP30, VP35, and the L protein. The SNALPs ranged from 71 – 84 nm in size and were composed of synthetic cholesterol, phospholipid DSPC, PEG lipid PEGC-DMA, and cationic lipid DLinDMA at the molar ratio of 48:20:2:30. The results confirm complete protection against viremia and death in guinea pigs when administered a SNALP-siRNA delivery system after diagnosis of the Ebola virus, thus proving this technology to be an effective treatment. Future studies will focus mainly upon evaluating the effects of siRNA ‘cocktails’ on EBOV genes to increase antiviral effects.\n\nIn 2010, researchers developed an applicable targeting therapy for hepatocellular carcinoma (HCC) in humans. The identification of CSN5, the fifth subunit of the COP9 signalosome complex found in early HCC, was used as a therapeutic target for siRNA induction. Systemic delivery of modified CSN5siRNA encapsulated in SNALPs significantly inhibited hepatic tumor growth in the Huh7-luc+ orthotopic xenograft model of human liver cancer. SiRNA-mediated CSN5 knockdown was also proven to inhibit cell-cycle progression and increases the rate of apoptosis in HCC cells in vitro. Not only do these results demonstrate the role of CSN5 in liver cancer progression, they also indicate that CSN5 has an essential role in HCC pathogenesis. In conclusion, SNALPs have been proven to significantly reduce hepatocellular carcinoma tumor growth in human Huh7-luc* cells through therapeutic silencing.\n\nIn 2009, researchers developed siRNAs capable of targeting both polo-like kinase 1(PLK1) and kinesin spindle protein(KSP). Both proteins are important to the cell-cycle of tumor cells, PLK1 involved with phosphorylation of a variety of proteins and KSP integral to chromosome segregation during mitosis. Specifically, bipolar mitotic spindles are unable to form when KSP is inhibited, leading to arrest of the cell cycle and, eventually, apoptosis. Likewise, inhibition of PLK1 facilitates mitotic arrests and cell apoptosis. According to the study, a 2 mg/kg dose of PLK1-specific siRNA administered for 3 weeks to mice implanted with tumors resulted in increased survival times and obvious reduction of tumors. In fact, the median survival time of treated mice was 51 days as opposed to 32 days for the controls. Further, only 2 of the 6 mice treated had noticeable tumors around the implantation site. Even so, GAPDH, a tumor-derived signal, was present at low levels, indicating significant suppression of tumor growth but not complete elimination. Still, the results suggested minimal toxicity and no significant dysfunction of the bone marrow. Animals treated with KSP-specific siRNA, too, exhibited increased survival times of 28 days compared to 20 days in the controls.\n", "id": "27557852", "title": "Stable nucleic acid lipid particle"}
{"url": "https://en.wikipedia.org/wiki?curid=159266", "text": "Gene expression\n\nGene expression is the process by which information from a gene is used in the synthesis of a functional gene product. These products are often proteins, but in non-protein coding genes such as transfer RNA (tRNA) or small nuclear RNA (snRNA) genes, the product is a functional RNA.\n\nThe process of gene expression is used by all known life—eukaryotes (including multicellular organisms), prokaryotes (bacteria and archaea), and utilized by viruses—to generate the macromolecular machinery for life.\n\nSeveral steps in the gene expression process may be modulated, including the transcription, RNA splicing, translation, and post-translational modification of a protein. Gene regulation gives the cell control over structure and function, and is the basis for cellular differentiation, morphogenesis and the versatility and adaptability of any organism. Also it depends on the state of activator RNA. Gene regulation may also serve as a substrate for evolutionary change, since control of the timing, location, and amount of gene expression can have a profound effect on the functions (actions) of the gene in a cell or in a multicellular organism.\n\nIn genetics, gene expression is the most fundamental level at which the genotype gives rise to the phenotype, i.e. observable trait. The genetic code stored in DNA is \"interpreted\" by gene expression, and the properties of the expression give rise to the organism's phenotype. Such phenotypes are often expressed by the synthesis of proteins that control the organism's shape, or that act as enzymes catalysing specific metabolic pathways characterising the organism. Regulation of gene expression is thus critical to an organism's development.\n\nA gene is a stretch of DNA that encodes information. Genomic DNA consists of two antiparallel and reverse complementary strands, each having 5' and 3' ends. With respect to a gene, the two strands may be labeled the \"template strand,\" which serves as a blueprint for the production of an RNA transcript, and the \"coding strand,\" which includes the DNA version of the transcript sequence. (Perhaps surprisingly, the \"coding strand\" is not physically involved in the coding process because it is the \"template strand\" that is read during transcription.)\n\nThe production of the RNA copy of the DNA is called transcription, and is performed in the nucleus by RNA polymerase, which adds one RNA nucleotide at a time to a growing RNA strand as per the complementarity law of the bases. This RNA is complementary to the template 3' → 5' DNA strand, which is itself complementary to the coding 5' → 3' DNA strand. Therefore, the resulting 5' → 3' RNA strand is identical to the coding DNA strand with the exception that thymines (T) are replaced with uracils (U) in the RNA. A coding DNA strand reading \"ATG\" is indirectly transcribed through the non-coding strand as \"AUG\" in RNA.\n\nIn prokaryotes, transcription is carried out by a single type of RNA polymerase, which needs a DNA sequence called a Pribnow box as well as a sigma factor (σ factor) to start transcription. In eukaryotes, transcription is performed by three types of RNA polymerases, each of which needs a special DNA sequence called the promoter and a set of DNA-binding proteins—transcription factors—to initiate the process. RNA polymerase I is responsible for transcription of ribosomal RNA (rRNA) genes. RNA polymerase II (Pol II) transcribes all protein-coding genes but also some non-coding RNAs (e.g., snRNAs, snoRNAs or long non-coding RNAs). Pol II includes a C-terminal domain (CTD) that is rich in serine residues. When these residues are phosphorylated, the CTD binds to various protein factors that promote transcript maturation and modification. RNA polymerase III transcribes 5S rRNA, transfer RNA (tRNA) genes, and some small non-coding RNAs (e.g., 7SK). Transcription ends when the polymerase encounters a sequence called the terminator.\n\nWhile transcription of prokaryotic protein-coding genes creates messenger RNA (mRNA) that is ready for translation into protein, transcription of eukaryotic genes leaves a primary transcript of RNA (pre-mRNA), which first has to undergo a series of modifications to become a mature mRNA.\n\nThese include 5' \"capping\", which is set of enzymatic reactions that add 7-methylguanosine (mG) to the 5' end of pre-mRNA and thus protect the RNA from degradation by exonucleases. The mG cap is then bound by cap binding complex heterodimer (CBC20/CBC80), which aids in mRNA export to cytoplasm and also protect the RNA from decapping.\n\nAnother modification is 3' \"cleavage and polyadenylation\". They occur if polyadenylation signal sequence (5'- AAUAAA-3') is present in pre-mRNA, which is usually between protein-coding sequence and terminator. The pre-mRNA is first cleaved and then a series of ~200 adenines (A) are added to form poly(A) tail, which protects the RNA from degradation. Poly(A) tail is bound by multiple poly(A)-binding proteins (PABP) necessary for mRNA export and translation re-initiation.\nA very important modification of eukaryotic pre-mRNA is \"RNA splicing\". The majority of eukaryotic pre-mRNAs consist of alternating segments called exons and introns. During the process of splicing, an RNA-protein catalytical complex known as spliceosome catalyzes two transesterification reactions, which remove an intron and release it in form of lariat structure, and then splice neighbouring exons together. In certain cases, some introns or exons can be either removed or retained in mature mRNA. This so-called alternative splicing creates series of different transcripts originating from a single gene. Because these transcripts can be potentially translated into different proteins, splicing extends the complexity of eukaryotic gene expression.\n\nExtensive RNA processing may be an evolutionary advantage made possible by the nucleus of eukaryotes. In prokaryotes, transcription and translation happen together, whilst in eukaryotes, the nuclear membrane separates the two processes, giving time for RNA processing to occur.\n\nIn most organisms non-coding genes (ncRNA) are transcribed as precursors that undergo further processing. In the case of ribosomal RNAs (rRNA), they are often transcribed as a pre-rRNA that contains one or more rRNAs. The pre-rRNA is cleaved and modified (2′-O-methylation and pseudouridine formation) at specific sites by approximately 150 different small nucleolus-restricted RNA species, called snoRNAs. SnoRNAs associate with proteins, forming snoRNPs. While snoRNA part basepair with the target RNA and thus position the modification at a precise site, the protein part performs the catalytical reaction. In eukaryotes, in particular a snoRNP called RNase, MRP cleaves the 45S pre-rRNA into the 28S, 5.8S, and 18S rRNAs. The rRNA and RNA processing factors form large aggregates called the nucleolus.\n\nIn the case of transfer RNA (tRNA), for example, the 5' sequence is removed by RNase P, whereas the 3' end is removed by the tRNase Z enzyme and the non-templated 3' CCA tail is added by a nucleotidyl transferase. In the case of micro RNA (miRNA), miRNAs are first transcribed as primary transcripts or pri-miRNA with a cap and poly-A tail and processed to short, 70-nucleotide stem-loop structures known as pre-miRNA in the cell nucleus by the enzymes Drosha and Pasha. After being exported, it is then processed to mature miRNAs in the cytoplasm by interaction with the endonuclease Dicer, which also initiates the formation of the RNA-induced silencing complex (RISC), composed of the Argonaute protein.\n\nEven snRNAs and snoRNAs themselves undergo series of modification before they become part of functional RNP complex. This is done either in the nucleoplasm or in the specialized compartments called Cajal bodies. Their bases are methylated or pseudouridinilated by a group of small Cajal body-specific RNAs (scaRNAs), which are structurally similar to snoRNAs.\n\nIn eukaryotes most mature RNA must be exported to the cytoplasm from the nucleus. While some RNAs function in the nucleus, many RNAs are transported through the nuclear pores and into the cytosol. Notably this includes all RNA types involved in protein synthesis. In some cases RNAs are additionally transported to a specific part of the cytoplasm, such as a synapse; they are then towed by motor proteins that bind through linker proteins to specific sequences (called \"zipcodes\") on the RNA.\n\nFor some RNA (non-coding RNA) the mature RNA is the final gene product. In the case of messenger RNA (mRNA) the RNA is an information carrier coding for the synthesis of one or more proteins. mRNA carrying a single protein sequence (common in eukaryotes) is monocistronic whilst mRNA carrying multiple protein sequences (common in prokaryotes) is known as polycistronic.\n\nEvery mRNA consists of three parts: a 5' untranslated region (5'UTR), a protein-coding region or open reading frame (ORF), and a 3' untranslated region (3'UTR). The coding region carries information for protein synthesis encoded by the genetic code to form triplets. Each triplet of nucleotides of the coding region is called a codon and corresponds to a binding site complementary to an anticodon triplet in transfer RNA. Transfer RNAs with the same anticodon sequence always carry an identical type of amino acid. Amino acids are then chained together by the ribosome according to the order of triplets in the coding region. The ribosome helps transfer RNA to bind to messenger RNA and takes the amino acid from each transfer RNA and makes a structure-less protein out of it. Each mRNA molecule is translated into many protein molecules, on average ~2800 in mammals.\n\nIn prokaryotes translation generally occurs at the point of transcription (co-transcriptionally), often using a messenger RNA that is still in the process of being created. In eukaryotes translation can occur in a variety of regions of the cell depending on where the protein being written is supposed to be. Major locations are the cytoplasm for soluble cytoplasmic proteins and the membrane of the endoplasmic reticulum for proteins that are for export from the cell or insertion into a cell membrane. Proteins that are supposed to be expressed at the endoplasmic reticulum are recognised part-way through the translation process. This is governed by the signal recognition particle—a protein that binds to the ribosome and directs it to the endoplasmic reticulum when it finds a signal peptide on the growing (nascent) amino acid chain.\nTranslation is the communication of the meaning of a source-language text by means of an equivalent target-language text\n\nThe polypeptide folds into its characteristic and functional three-dimensional structure from a random coil.\nEach protein exists as an unfolded polypeptide or random coil when translated from a sequence of mRNA into a linear chain of amino acids. This polypeptide lacks any developed three-dimensional structure (the left hand side of the neighboring figure). Amino acids interact with each other to produce a well-defined three-dimensional structure, the folded protein (the right hand side of the figure) known as the native state. The resulting three-dimensional structure is determined by the amino acid sequence (Anfinsen's dogma).\n\nThe correct three-dimensional structure is essential to function, although some parts of functional proteins may remain unfolded. Failure to fold into the intended shape usually produces inactive proteins with different properties including toxic prions. Several neurodegenerative and other diseases are believed to result from the accumulation of \"misfolded\" proteins. Many allergies are caused by the folding of the proteins, for the immune system does not produce antibodies for certain protein structures.\n\nEnzymes called chaperones assist the newly formed protein to attain (fold into) the 3-dimensional structure it needs to function. Similarly, RNA chaperones help RNAs attain their functional shapes. Assisting protein folding is one of the main roles of the endoplasmic reticulum in eukaryotes.\n\nSecretory proteins of eukaryotes or prokaryotes must be translocated to enter the secretory pathway. Newly synthesized proteins are directed to the eukaryotic Sec61 or prokaryotic SecYEG translocation channel by signal peptides. The efficiency of protein secretion in eukaryotes is very dependent on the signal peptide which has been used.\n\nMany proteins are destined for other parts of the cell than the cytosol and a wide range of signalling sequences or (signal peptides) are used to direct proteins to where they are supposed to be. In prokaryotes this is normally a simple process due to limited compartmentalisation of the cell. However, in eukaryotes there is a great variety of different targeting processes to ensure the protein arrives at the correct organelle.\n\nNot all proteins remain within the cell and many are exported, for example, digestive enzymes, hormones and extracellular matrix proteins. In eukaryotes the export pathway is well developed and the main mechanism for the export of these proteins is translocation to the endoplasmic reticulum, followed by transport via the Golgi apparatus.\n\nRegulation of gene expression refers to the control of the amount and timing of appearance of the functional product of a gene. Control of expression is vital to allow a cell to produce the gene products it needs when it needs them; in turn, this gives cells the flexibility to adapt to a variable environment, external signals, damage to the cell, and other stimuli. More generally, gene regulation gives the cell control over all structure and function, and is the basis for cellular differentiation, morphogenesis and the versatility and adaptability of any organism.\n\nNumerous terms are used to describe types of genes depending on how they are regulated; these include:\n\nAny step of gene expression may be modulated, from the DNA-RNA transcription step to post-translational modification of a protein. The stability of the final gene product, whether it is RNA or protein, also contributes to the expression level of the gene—an unstable product results in a low expression level. In general gene expression is regulated through changes in the number and type of interactions between molecules that collectively influence transcription of DNA and translation of RNA.\n\nSome simple examples of where gene expression is important are:\n\nRegulation of transcription can be broken down into three main routes of influence; genetic (direct interaction of a control factor with the gene), modulation interaction of a control factor with the transcription machinery and epigenetic (non-sequence changes in DNA structure that influence transcription).\n\nDirect interaction with DNA is the simplest and the most direct method by which a protein changes transcription levels. Genes often have several protein binding sites around the coding region with the specific function of regulating transcription. There are many classes of regulatory DNA binding sites known as enhancers, insulators and silencers. The mechanisms for regulating transcription are very varied, from blocking key binding sites on the DNA for RNA polymerase to acting as an activator and promoting transcription by assisting RNA polymerase binding.\n\nThe activity of transcription factors is further modulated by intracellular signals causing protein post-translational modification including phosphorylated, acetylated, or glycosylated. These changes influence a transcription factor's ability to bind, directly or indirectly, to promoter DNA, to recruit RNA polymerase, or to favor elongation of a newly synthesized RNA molecule.\n\nThe nuclear membrane in eukaryotes allows further regulation of transcription factors by the duration of their presence in the nucleus, which is regulated by reversible changes in their structure and by binding of other proteins. Environmental stimuli or endocrine signals may cause modification of regulatory proteins eliciting cascades of intracellular signals, which result in regulation of gene expression.\n\nMore recently it has become apparent that there is a significant influence of non-DNA-sequence specific effects on transcription. These effects are referred to as epigenetic and involve the higher order structure of DNA, non-sequence specific DNA binding proteins and chemical modification of DNA. In general epigenetic effects alter the accessibility of DNA to proteins and so modulate transcription.\n\nDNA methylation is a widespread mechanism for epigenetic influence on gene expression and is seen in bacteria and eukaryotes and has roles in heritable transcription silencing and transcription regulation. In eukaryotes the structure of chromatin, controlled by the histone code, regulates access to DNA with significant impacts on the expression of genes in euchromatin and heterochromatin areas.\n\nThe majority of gene promoters contain a CpG island with numerous CpG sites. When many of a gene's promoter CpG sites are methylated the gene becomes silenced. Colorectal cancers typically have 3 to 6 driver mutations and 33 to 66 hitchhiker or passenger mutations. However, transcriptional silencing may be of more importance than mutation in causing progression to cancer. For example, in colorectal cancers about 600 to 800 genes are transcriptionally silenced by CpG island methylation (see regulation of transcription in cancer). Transcriptional repression in cancer can also occur by other epigenetic mechanisms, such as altered expression of microRNAs. In breast cancer, transcriptional repression of BRCA1 may occur more frequently by over-expressed microRNA-182 than by hypermethylation of the BRCA1 promoter (see Low expression of BRCA1 in breast and ovarian cancers).\n\nIn eukaryotes, where export of RNA is required before translation is possible, nuclear export is thought to provide additional control over gene expression. All transport in and out of the nucleus is via the nuclear pore and transport is controlled by a wide range of importin and exportin proteins.\n\nExpression of a gene coding for a protein is only possible if the messenger RNA carrying the code survives long enough to be translated. In a typical cell, an RNA molecule is only stable if specifically protected from degradation. RNA degradation has particular importance in regulation of expression in eukaryotic cells where mRNA has to travel significant distances before being translated. In eukaryotes, RNA is stabilised by certain post-transcriptional modifications, particularly the 5' cap and poly-adenylated tail.\n\nIntentional degradation of mRNA is used not just as a defence mechanism from foreign RNA (normally from viruses) but also as a route of mRNA \"destabilisation\". If an mRNA molecule has a complementary sequence to a small interfering RNA then it is targeted for destruction via the RNA interference pathway.\n\nThree prime untranslated regions (3'UTRs) of messenger RNAs (mRNAs) often contain regulatory sequences that post-transcriptionally influence gene expression. Such 3'-UTRs often contain both binding sites for microRNAs (miRNAs) as well as for regulatory proteins. By binding to specific sites within the 3'-UTR, miRNAs can decrease gene expression of various mRNAs by either inhibiting translation or directly causing degradation of the transcript. The 3'-UTR also may have silencer regions that bind repressor proteins that inhibit the expression of a mRNA.\n\nThe 3'-UTR often contains microRNA response elements (MREs). MREs are sequences to which miRNAs bind. These are prevalent motifs within 3'-UTRs. Among all regulatory motifs within the 3'-UTRs (e.g. including silencer regions), MREs make up about half of the motifs.\n\nAs of 2014, the miRBase web site, an archive of miRNA sequences and annotations, listed 28,645 entries in 233 biologic species. Of these, 1,881 miRNAs were in annotated human miRNA loci. miRNAs were predicted to have an average of about four hundred target mRNAs (affecting expression of several hundred genes). Freidman et al. estimate that >45,000 miRNA target sites within human mRNA 3'UTRs are conserved above background levels, and >60% of human protein-coding genes have been under selective pressure to maintain pairing to miRNAs.\n\nDirect experiments show that a single miRNA can reduce the stability of hundreds of unique mRNAs. Other experiments show that a single miRNA may repress the production of hundreds of proteins, but that this repression often is relatively mild (less than 2-fold).\n\nThe effects of miRNA dysregulation of gene expression seem to be important in cancer. For instance, in gastrointestinal cancers, nine miRNAs have been identified as epigenetically altered and effective in down regulating DNA repair enzymes.\n\nThe effects of miRNA dysregulation of gene expression also seem to be important in neuropsychiatric disorders, such as schizophrenia, bipolar disorder, major depression, Parkinson's disease, Alzheimer's disease and autism spectrum disorders.\n\nDirect regulation of translation is less prevalent than control of transcription or mRNA stability but is occasionally used. Inhibition of protein translation is a major target for toxins and antibiotics, so they can kill a cell by overriding its normal gene expression control. Protein synthesis inhibitors include the antibiotic neomycin and the toxin ricin.\n\nOnce protein synthesis is complete, the level of expression of that protein can be reduced by protein degradation. There are major protein degradation pathways in all prokaryotes and eukaryotes, of which the proteasome is a common component. An unneeded or damaged protein is often labeled for degradation by addition of ubiquitin.\n\nMeasuring gene expression is an important part of many life sciences, as the ability to quantify the level at which a particular gene is expressed within a cell, tissue or organism can provide a lot of valuable information. For example, measuring gene expression can:\nSimilarly, the analysis of the location of protein expression is a powerful tool, and this can be done on an organismal or cellular scale. Investigation of localization is particularly important for the study of development in multicellular organisms and as an indicator of protein function in single cells. Ideally, measurement of expression is done by detecting the final gene product (for many genes, this is the protein); however, it is often easier to detect one of the precursors, typically mRNA and to infer gene-expression levels from these measurements.\n\nLevels of mRNA can be quantitatively measured by northern blotting, which provides size and sequence information about the mRNA molecules. A sample of RNA is separated on an agarose gel and hybridized to a radioactively labeled RNA probe that is complementary to the target sequence. The radiolabeled RNA is then detected by an autoradiograph. Because the use of radioactive reagents makes the procedure time consuming and potentially dangerous, alternative labeling and detection methods, such as digoxigenin and biotin chemistries, have been developed. Perceived disadvantages of Northern blotting are that large quantities of RNA are required and that quantification may not be completely accurate, as it involves measuring band strength in an image of a gel. On the other hand, the additional mRNA size information from the Northern blot allows the discrimination of alternately spliced transcripts.\n\nAnother approach for measuring mRNA abundance is RT-qPCR. In this technique, reverse transcription is followed by quantitative PCR. Reverse transcription first generates a DNA template from the mRNA; this single-stranded template is called cDNA. The cDNA template is then amplified in the quantitative step, during which the fluorescence emitted by labeled hybridization probes or intercalating dyes changes as the DNA amplification process progresses. With a carefully constructed standard curve, qPCR can produce an absolute measurement of the number of copies of original mRNA, typically in units of copies per nanolitre of homogenized tissue or copies per cell. qPCR is very sensitive (detection of a single mRNA molecule is theoretically possible), but can be expensive depending on the type of reporter used; fluorescently labeled oligonucleotide probes are more expensive than non-specific intercalating fluorescent dyes.\n\nFor expression profiling, or high-throughput analysis of many genes within a sample, quantitative PCR may be performed for hundreds of genes simultaneously in the case of low-density arrays. A second approach is the hybridization microarray. A single array or \"chip\" may contain probes to determine transcript levels for every known gene in the genome of one or more organisms. Alternatively, \"tag based\" technologies like Serial analysis of gene expression (SAGE) and RNA-Seq, which can provide a relative measure of the cellular concentration of different mRNAs, can be used. An advantage of tag-based methods is the \"open architecture\", allowing for the exact measurement of any transcript, with a known or unknown sequence. Next-generation sequencing (NGS) such as RNA-Seq is another approach, producing vast quantities of sequence data that can be matched to a reference genome. Although NGS is comparatively time-consuming, expensive, and resource-intensive, it can identify single-nucleotide polymorphisms, splice-variants, and novel genes, and can also be used to profile expression in organisms for which little or no sequence information is available.\n\nProfiles like these are found for almost all proteins listed in Wikipedia. They are generated by organizations such as the Genomics Institute of the Novartis Research Foundation and the European Bioinformatics Institute. Additional information can be found by searching their databases (for an example of the GLUT4 transporter pictured here, see citation). These profiles indicate the level of DNA expression (and hence RNA produced) of a certain protein in a certain tissue, and are color-coded accordingly in the images located in the Protein Box on the right side of each Wikipedia page.\n\nFor genes encoding proteins, the expression level can be directly assessed by a number of methods with some clear analogies to the techniques for mRNA quantification.\n\nThe most commonly used method is to perform a Western blot against the protein of interest—this gives information on the size of the protein in addition to its identity. A sample (often cellular lysate) is separated on a polyacrylamide gel, transferred to a membrane and then probed with an antibody to the protein of interest. The antibody can either be conjugated to a fluorophore or to horseradish peroxidase for imaging and/or quantification. The gel-based nature of this assay makes quantification less accurate, but it has the advantage of being able to identify later modifications to the protein, for example proteolysis or ubiquitination, from changes in size.\n\nAnalysis of expression is not limited to quantification; localisation can also be determined. mRNA can be detected with a suitably labelled complementary mRNA strand and protein can be detected via labelled antibodies. The probed sample is then observed by microscopy to identify where the mRNA or protein is.\n\nBy replacing the gene with a new version fused to a green fluorescent protein (or similar) marker, expression may be directly quantified in live cells. This is done by imaging using a fluorescence microscope. It is very difficult to clone a GFP-fused protein into its native location in the genome without affecting expression levels so this method often cannot be used to measure endogenous gene expression. It is, however, widely used to measure the expression of a gene artificially introduced into the cell, for example via an expression vector. It is important to note that by fusing a target protein to a fluorescent reporter the protein's behavior, including its cellular localization and expression level, can be significantly changed.\n\nThe enzyme-linked immunosorbent assay works by using antibodies immobilised on a microtiter plate to capture proteins of interest from samples added to the well. Using a detection antibody conjugated to an enzyme or fluorophore the quantity of bound protein can be accurately measured by fluorometric or colourimetric detection. The detection process is very similar to that of a Western blot, but by avoiding the gel steps more accurate quantification can be achieved.\n\nAn expression system is a system specifically designed for the production of a gene product of choice. This is normally a protein although may also be RNA, such as tRNA or a ribozyme. An expression system consists of a gene, normally encoded by DNA, and the molecular machinery required to transcribe the DNA into mRNA and translate the mRNA into protein using the reagents provided. In the broadest sense this includes every living cell but the term is more normally used to refer to expression as a laboratory tool. An expression system is therefore often artificial in some manner. Expression systems are, however, a fundamentally natural process. Viruses are an excellent example where they replicate by using the host cell as an expression system for the viral proteins and genome.\n\nDoxycycline is also used in \"Tet-on\" and \"Tet-off\" tetracycline controlled transcriptional activation to regulate transgene expression in organisms and cell cultures.\n\nIn addition to these biological tools, certain naturally observed configurations of DNA (genes, promoters, enhancers, repressors) and the associated machinery itself are referred to as an expression system. This term is normally used in the case where a gene or set of genes is switched on under well defined conditions, for example, the simple repressor switch expression system in Lambda phage and the lac operator system in bacteria. Several natural expression systems are directly used or modified and used for artificial expression systems such as the Tet-on and Tet-off expression system.\n\nGenes have sometimes been regarded as nodes in a network, with inputs being proteins such as transcription factors, and outputs being the level of gene expression. The node itself performs a function, and the operation of these functions have been interpreted as performing a kind of information processing within cells and determines cellular behavior.\n\nGene networks can also be constructed without formulating an explicit causal model. This is often the case when assembling networks from large expression data sets. Covariation and correlation of expression is computed across a large sample of cases and measurements (often transcriptome or proteome data). The source of variation can be either experimental or natural (observational). There are several ways to construct gene expression networks, but one common approach is to compute a matrix of all pair-wise correlations of expression across conditions, time points, or individuals and convert the matrix (after thresholding at some cut-off value) into a graphical representation in which nodes represent genes, transcripts, or proteins and edges connecting these nodes represent the strength of association (see ).\nWeighted correlation network analysis (WGCNA) involves weighted networks defined by soft-thresholding the pairwise correlations among variables (e.g. measures of transcript abundance). WGCNA can be applied to compute eigengenes, which are highly robust biomarkers (features) useful for diagnosis and prognosis.\n\nThe following experimental techniques are used to measure gene expression and are listed in roughly chronological order, starting with the older, more established technologies. They are divided into two groups based on their degree of multiplexity.\n\n\n", "id": "159266", "title": "Gene expression"}
{"url": "https://en.wikipedia.org/wiki?curid=30965725", "text": "Ion semiconductor sequencing\n\nIon semiconductor sequencing is a method of DNA sequencing based on the detection of hydrogen ions that are released during the polymerization of DNA. This is a method of \"sequencing by synthesis\", during which a complementary strand is built based on the sequence of a template strand.\n\nA microwell containing a template DNA strand to be sequenced is flooded with a single species of deoxyribonucleotide triphosphate (dNTP). If the introduced dNTP is complementary to the leading template nucleotide, it is incorporated into the growing complementary strand. This causes the release of a hydrogen ion that triggers an ISFET ion sensor, which indicates that a reaction has occurred. If homopolymer repeats are present in the template sequence, multiple dNTP molecules will be incorporated in a single cycle. This leads to a corresponding number of released hydrogens and a proportionally higher electronic signal.\n\nThis technology differs from other sequencing technologies in that no modified nucleotides or optics are used. Ion semiconductor sequencing may also be referred to as Ion Torrent sequencing, pH-mediated sequencing, silicon sequencing, or semiconductor sequencing.\n\nThe technology was licensed from DNA Electronics Ltd, developed by Ion Torrent Systems Inc. and was released in February 2010. Ion Torrent have marketed their machine as a rapid, compact and economical sequencer that can be utilized in a large number of laboratories as a bench top machine. Roche's 454 Life Sciences is partnering with DNA Electronics on the development of a long-read, high-density semiconductor sequencing platform using this technology.\n\nIn nature, the incorporation of a deoxyribonucleoside triphosphate (dNTP) into a growing DNA strand involves the formation of a covalent bond and the release of pyrophosphate and a positively charged hydrogen ion. A dNTP will only be incorporated if it is complementary to the leading unpaired template nucleotide. Ion semiconductor sequencing exploits these facts by determining if a hydrogen ion is released upon providing a single species of dNTP to the reaction.\n\nMicrowells on a semiconductor chip that each contain many copies of one single-stranded template DNA molecule to be sequenced and DNA polymerase are sequentially flooded with unmodified A, C, G or T dNTP. If an introduced dNTP is complementary to the next unpaired nucleotide on the template strand it is incorporated into the growing complementary strand by the DNA polymerase. If the introduced dNTP is not complementary there is no incorporation and no biochemical reaction. The hydrogen ion that is released in the reaction changes the pH of the solution, which is detected by an ISFET. The unattached dNTP molecules are washed out before the next cycle when a different dNTP species is introduced.\n\nBeneath the layer of microwells is an ion sensitive layer, below which is an ISFET ion sensor. All layers are contained within a CMOS semiconductor chip, similar to that used in the electronics industry.\n\nEach chip contains an array of microwells with corresponding ISFET detectors.\nEach released hydrogen ion then triggers the ISFET ion sensor. The series of electrical pulses transmitted from the chip to a computer is translated into a DNA sequence, with no intermediate signal conversion required. Because nucleotide incorporation events are measured directly by electronics, the use of labeled nucleotides and optical measurements are avoided. Signal processing and DNA assembly can then be carried out in software.\n\nThe per base accuracy achieved in house by Ion Torrent on the Ion Torrent Ion semiconductor sequencer as of February 2011 was 99.6% based on 50 base reads, with 100 Mb per run. The read-length as of February 2011 was 100 base pairs. The accuracy for homopolymer repeats of 5 repeats in length was 98%. Later releases show a read length of 400 base pairs It should be noted that these figures have not yet been independently verified outside of the company.\n\nThe major benefits of ion semiconductor sequencing are rapid sequencing speed and low upfront and operating costs. This has been enabled by the avoidance of modified nucleotides and optical measurements.\n\nBecause the system records natural polymerase-mediated nucleotide incorporation events, sequencing can occur in real-time. In reality, the sequencing rate is limited by the cycling of substrate nucleotides through the system. Ion Torrent Systems Inc., the developer of the technology, claims that each incorporation measurement takes 4 seconds and each run takes about one hour, during which 100-200 nucleotides are sequenced. If the semiconductor chips are improved (as predicted by Moore’s law), the number of reads per chip (and therefore per run) should increase.\n\nThe cost of acquiring a pH-mediated sequencer from Ion Torrent Systems Inc. at time of launch was priced at around $50,000 USD, excluding sample preparation equipment and a server for data analysis. The cost per run is also significantly lower than that of alternative automated sequencing methods, at roughly $1,000.\n\nIf homopolymer repeats of the same nucleotide (e.g. TTTTT) are present on the template strand (strand to be sequenced) then multiple introduced nucleotides are incorporated and more hydrogen ions are released in a single cycle. This results in a greater pH change and a proportionally greater electronic signal. This is a limitation of the system in that it is difficult to enumerate long repeats. This limitation is shared by other techniques that detect single nucleotide additions such as pyrosequencing. Signals generated from a high repeat number are difficult to differentiate from repeats of a similar but different number; \"e.g.\", homorepeats of length 7 are difficult to differentiate from those of length 8.\n\nAnother limitation of this system is the short read length compared to other sequencing methods such as Sanger sequencing or pyrosequencing. Longer read lengths are beneficial for \"de novo\" genome assembly. Ion Torrent semiconductor sequencers produce an average read length of approximately 400 nucleotides per read.\n\nThe throughput is currently lower than that of other high-throughput sequencing technologies, although the developers hope to change this by increasing the density of the chip.\n\nThe developers of Ion Torrent semiconductor sequencing have marketed it as a rapid, compact and economical sequencer that can be utilized in a large number of laboratories as a bench top machine. The company hopes that their system will take sequencing outside of specialized centers and into the reach of hospitals and smaller laboratories. A January 2011 New York Times article, \"Taking DNA Sequencing to the Masses\", underlines these ambitions.\n\nDue to the ability of alternative sequencing methods to achieve a greater read length (and therefore being more suited to whole genome analysis) this technology may be best suited to small scale applications such as microbial genome sequencing, microbial transcriptome sequencing, targeted sequencing, amplicon sequencing, or for quality testing of sequencing libraries.\n\n", "id": "30965725", "title": "Ion semiconductor sequencing"}
{"url": "https://en.wikipedia.org/wiki?curid=30992262", "text": "Partial cloning\n\nIn the field of cell biology, the method of partial cloning (PCL) converts a fully differentiated \"old\" somatic cell into a partially reprogrammed \"young\" cell that retains all the specialised functions of the differentiated \"old\" cell but is simply younger. The method of PCL reverses characteristics associated with old cells. For example, old, senescent, cells \"rejuvenated\" by PCL are free of highly condensed senescence-associated heterochromatin foci (SAHF) and re-acquire the proliferation potential of young cells. The method of PCL thus rejuvenates old cells without de-differentiation and passage through an embryonic, pluripotent, stage.\n\nPCL consists in introducing a somatic adult or senescent cell nucleus or entire cell with enlarged membrane pores in an (activated) oocyte and to withdraw this treated cell before its de-differentiation and first cell division occurs. Thus, the progressive rejuvenation capability of the oocyte is used only temporarily in order to obtain a partial natural rejuvenation. PCL permits to envisage a chosen degree of partial rejuvenation in changing the duration of the introduction of the treated cell in the oocyte. Using PCL cell de-differentiation and its age reprogramming might be, at least partially, separable. Thus the existence of an isolated ageing clock would be confirmed at least during a certain part of the cellular evolution and involution.\n\nFirst experimental result shows a possible high efficiency in partial rejuvenation of senescent mouse cells. Notably PCL rejuvenates exclusively one single tissue or organ, in contrast to classical cloning PCL is therefore unable to reconstitute an entire organism. Furthermore, PCL is feasible in a few hours in opposition to classical cloning or induced pluripotent stem cells (iPS) which all need weeks or months.\n\nClassical cloning can rejuvenate old cells but the process demands that the old cells must artificially pass through an embryonic cell stage. Partial cloning affords the advantage that the old cells to be rejuvenated do not have to pass through the embryonic cell stage and are simply made younger.\n\nThe extension of human lifespan, in terms of useful, quality, years added to life, has been a goal for many since time immemorial. And while a goal whose attainment was thought improbable, or at least achievable only in the far distant future, the discovery that animals can be cloned has brought the goal of rejuvenation much closer. The remarkable discovery that animals can be cloned showed that the nucleus of an old cell can be used as a donor in so-called “nuclear transfer” experiments where an old nucleus is transferred into a recipient egg whose own nuclear material has been removed. The “reconstructed” egg is then prompted to engage development and develops through an embryonic stage that results, once the embryo is implanted into a surrogate mother, into a new born. Thus an old cell can give rise to a newborn, which has a typical lifespan: the age of the donor cell is “wiped clean” and returned to a youthful state. Notably, in classical animal cloning the rejuvenation process involves a return to an embryonic form. Thus the specialized functions of the adult cell are also “wiped clean” and returned to an embryonic cell type. And in classical cloning passage through this embryonic state is a must for the age of the cell to be “wiped clean”.\n\nThe key notion that exemplifies “partial” cloning from “classical” cloning is the separation of the mechanism(s) that “wipe clean” the specialization of a cell from those that “wipe-clean” the age of the cell. In short, partial cloning aims to retain the specialized functions of a cell and simply make it younger, e.g., a skin cell is rejuvenated without having to pass through the embryonic stage that is a must for rejuvenation via the classical cloning technique (see diagram).\n\nIn a new laboratory at the Forschungszentrum Borstel our work on partial cloning focuses, inter alia, on the restricted, temporary, incubation of an “old” cell within the egg. In this way only the age of the cell is “wiped clean” and its specialized, differentiated, state is retained. It is simply made younger – rejuvenated - without going through the embryonic state. The measure of Diagram showing the difference between “Classical” and “Partial” cloning: Classical cloning (the route given by the black arrows) can rejuvenate an old cell but requires passage through an embryonic stage. “Partial cloning” (given by the red arrow) rejuvenates old cells without passage through an embryonic stage.“Partial cloning” (given by the red arrow)rejuvenates old cells without passage through an embryonic stage.\nIn a new laboratory at the Forschungszentrum Borstel our work on partial cloning focuses, inter alia, on the restricted, temporary, incubation of an “old” cell within the egg. In this way only the age of the cell is “wiped clean” and its specialized, differentiated, state is retained. It is simply made younger – rejuvenated - without going through the embryonic state. The measure of rejuvenation in our system is, first, the re-acquisition of the ability of an old cell to divide, something that is lost in old cells and, second, the loss of characteristics that are associated with old cells.\n\nShould such rejuvenation be achievable the consequences for medicine would be profound. It would avoid the need to artificially pass through an embryonic stage – either by nuclear transfer or by the so-called iPS cells method - to rejuvenate cells. One would simply be able to take aged cells from a patient and then return to the patient their own, histocompatible, rejuvenated heart cells, liver cells etc. In sharp contrast to the cycle of artificial de-differentiation of somatic cells to stem cells and then the artificial re-differentiation of stem cells to the desired differentiated cell type, which is highly inefficient, time-consuming and results in unstable cell types. The process of partial cloning would be efficient and rapid and thus cheap both in terms of materials and manpower. In short, partial cloning has enormous potential to relieve human suffering and disease: it is the most rapid and cheap route to successful regenerative medicine. Partial cloning also avoids the ethical problems associated with “classical” cloning in that it does not result in live born – it mere uses the oocyte briefly as a means to condition and thereby rejuvenate the old cell exclusively.\n\n", "id": "30992262", "title": "Partial cloning"}
{"url": "https://en.wikipedia.org/wiki?curid=5510125", "text": "Transrepression\n\nIn the field of molecular biology, transrepression is a process whereby one protein represses (i.e., inhibits) the activity of a second protein through a protein-protein interaction. Since this repression occurs between two different protein molecules (intermolecular), it is referred to as a trans-acting process.\n\nThe protein that is repressed is usually a transcription factor whose function is to up-regulate (i.e., increase) the rate of gene transcription. Hence the net result of transrepression is down regulation of gene transcription.\n\nAn example of transrepression is the ability of the glucocorticoid receptor to inhibit the transcriptional promoting activity of the AP-1 and NF-κB transcription factors. In addition to transactivation, transrepression is an important pathway for the anti-inflammatory effects of glucocorticoids. Other nuclear receptors such as LXR and PPAR have been demonstrated to also have the ability to transrepress the activity of other proteins.\n\n", "id": "5510125", "title": "Transrepression"}
{"url": "https://en.wikipedia.org/wiki?curid=19657997", "text": "EIF2\n\nEukaryotic Initiation Factor 2 (eIF2) is a eukaryotic initiation factor. It is required for most forms of eukaryotic translation initiation. eIF2 mediates the binding of tRNA to the ribosome in a GTP-dependent manner. eIF2 is a heterotrimer consisting of an alpha (also called subunit 1), a beta (subunit 2), and a gamma (subunit 3) subunit.\n\nOnce the initiation phase has completed, eIF2 is released from the ribosome bound to GDP as an inactive binary complex. To participate in another round of translation initiation, this GDP must be exchanged for GTP.\n\neIF2 is an essential factor for protein synthesis that forms a ternary complex (TC) with GTP and the initiator Met-tRNA. After its formation, the TC binds the 40S ribosomal subunit to form the 43S preinitiation complex (43S PIC). 43S PIC assembly is believed to be stimulated by the initiation factors eIF1, eIF1A, and the eIF3 complex according to \"in vitro\" experiments. The 43S PIC then binds mRNA that has previously been unwound by the eIF4F complex. The 43S PIC and the eIF4F proteins form a new 48S complex on the mRNA, which starts searching along the mRNA for the start codon (AUG). Upon base pairing of the AUG-codon with the Met-tRNA, eIF5 (which is a GTPase-activating protein) is recruited to the complex and induces eIF2 to hydrolyse its GTP. This causes eIF2-GDP to be released from this 48S complex and translation begins after recruitment of the 60S ribosomal sub-unit and formation of the 80S initiation complex. Finally, with the help of the Guanine nucleotide exchange factor eIF2B, the GDP in eIF2 is exchanged for a GTP and the ternary complex reforms for a new round of translation initiation.\neIF2 is a heterotrimer of a total molar mass of 126 kDa that is composed of the three sub-units: α (sub-unit 1), β (sub-unit 2), and γ (sub-unit 3). \nThe sequences of all three sub-units are highly conserved (pairwise amino acid identities for each sub-unit range from 47–72% when comparing the proteins of \"Homo sapiens\" and \"Saccharomyces cerevisiae\").\nThe α-subunit contains the main target for phosphorylation, a serine at position 51. It also contains a S1 motif domain, which is a potential RNA binding-site. Therefore, the α-subunit can be considered the regulatory subunit of the trimer.\n\nThe β-subunit contains multiple phosphorylation sites (residues 2, 13, 67, 218). What is important to consider is that there are also three lysine clusters in the N-terminal domain (NTD), which are important for the interaction with eIF2B. Moreover, the sequence of the protein comprises a zinc finger motif that was shown to play a role in both ternary complex and 43S preinitiation complex formation. There are also two guanine nucleotide-binding sequences that have not been shown to be involved in the regulation of eIF2 activity. The β-subunit is also believed to interact with both tRNA and mRNA.\n\nThe γ-sub-unit comprises three guanine nucleotide-binding sites and is known to be the main docking site for GTP/GDP. It also contains a tRNA-binding cavity that has been shown by X-ray crystallography. A zinc knuckle motif is able to bind one Zn cation.\n\neIF2 activity is regulated by a mechanism involving both guanine nucleotide exchange and phosphorylation. Phosphorylation takes place at the α-subunit, which is a target for a number of serine kinases that phosphorylate serine 51. Those kinases act as a result of stress such as amino acid deprivation (GCN2), ER stress (PERK), the presence of dsRNA (PKR) heme deficiency (HRI), or interferon. Once phosphorylated, eIF2 shows increased affinity for its Guanine nucleotide exchange factor eIF2B. However, eIF2B is able to exchange GDP for GTP only if eIF2 is in its unphosphorylated state. Phosphorylated eIF2, however, due to its stronger binding, acts as an inhibitor of its own GEF (eIF2B). Since the cellular concentration of eIF2B is much lower than that of eIF2, even a small amount of phosphorylated eIF2 can completely abolish eIF2B activity by sequestration. Without the GEF, eIF2 can no longer be returned to its active (GTP-bound) state. As a consequence, translation comes to a halt since initiation is no longer possible without any available ternary complex. Furthermore, low concentration of ternary complex allows the expression of GCN4 (starved condition), which, in turn, results in increased activation of amino acid synthesis genes\n\nSince eIF2 is essential for most forms of translation initiation and therefore protein synthesis, defects in eIF2 are often lethal. The protein is highly conserved among evolutionary remote species - indicating a large impact of mutations on cell viability. Therefore, no diseases directly related to mutations in eIF2 can be observed. However, there are many illnesses caused by down-regulation of eIF2 through its upstream kinases. For example, increased concentrations of active PKR and inactive (phosphorylated) eIF2 were found in patients suffering from neurodegenerative diseases such as Alzheimer’s, Parkinson’s, and Huntington’s disease. There is also one proven example of a disease related to the GEF eIF2B. Mutations in all of the five subunits of eIF2B could be linked with leukoencephalopathy, an illness that causes the brain’s white matter to disappear. It is still not fully understood why only brain cells seem to be affected by these defects. Potentially reduced levels of unstable regulatory proteins might play a role in the development of the diseases mentioned.\n\n\n", "id": "19657997", "title": "EIF2"}
{"url": "https://en.wikipedia.org/wiki?curid=9346573", "text": "Multiplex ligation-dependent probe amplification\n\nMultiplex ligation-dependent probe amplification (MLPA) is a variation of the multiplex polymerase chain reaction that permits amplification of multiple targets with only a single primer pair.\n\nEach probe consists of two oligonucleotides which recognize adjacent target sites on the DNA. One probe oligonucleotide contains the sequence recognized by the forward primer, the other contains the sequence recognized by the reverse primer. Only when both probe oligonucleotides are hybridized to their respective targets, can they be ligated into a complete probe. The advantage of splitting the probe into two parts is that only the ligated oligonucleotides, but not the unbound probe oligonucleotides, are amplified. If the probes were not split in this way, the primer sequences at either end would cause the probes to be amplified regardless of their hybridization to the template DNA, and the amplification product would not be dependent on the number of target sites present in the sample DNA. Each complete probe has a unique length, so that its resulting amplicons can be separated and identified by (capillary) electrophoresis. This avoids the resolution limitations of multiplex PCR. Because the forward primer used for probe amplification is fluorescently labeled, each amplicon generates a fluorescent peak which can be detected by a capillary sequencer. Comparing the peak pattern obtained on a given sample with that obtained on various reference samples, the relative quantity of each amplicon can be determined. This ratio is a measure for the ratio in which the target sequence is present in the sample DNA.\n\nVarious techniques including DGGE (Denaturing Gradient Gel Electrophoresis), DHPLC (Denaturing High Performance Liquid Chromatography), and SSCA (Single Strand Conformation Analysis) effectively identify SNPs and small insertions and deletions. MLPA, however, is one of the only accurate, time-efficient techniques to detect genomic deletions and insertions (one or more entire exons), which are frequent causes of cancers such as hereditary non-polyposis colorectal cancer (HNPCC), breast, and ovarian cancer. MLPA can successfully and easily determine the relative copy number of all exons within a gene simultaneously with high sensitivity.\n\nAn important use of MLPA is to determine relative ploidy. For example, probes may be designed to target various regions of chromosome 21 of a human cell. The signal strengths of the probes are compared with those obtained from a reference DNA sample known to have two copies of the chromosome. If an extra copy is present in the test sample, the signals are expected to be 1.5 times the intensities of the respective probes from the reference. If only one copy is present the proportion is expected to be 0.5. If the sample has two copies, the relative probe strengths are expected to be equal.\n\nDosage quotient analysis is the usual method of interpreting MLPA data. If a and b are the signals from two amplicons in the patient sample, and A and B are the corresponding amplicons in the experimental control, then the dosage quotient DQ = (a/b) / (A/B). Although dosage quotients may be calculated for any pair of amplicons, it is usually the case that one of the pair is an internal reference probe.\n\nMLPA facilitates the amplification and detection of multiple targets with a single primer pair. In a standard multiplex PCR reaction, each fragment needs a unique amplifying primer pair. These primers being present in a large quantity result in various problems such as dimerization and false priming. With MLPA, amplification of probes can be achieved. Thus, many sequences (up to 40) can be amplified and quantified using just a single primer pair. MLPA reaction is fast, inexpensive and very simple to perform.\n\nMLPA has a variety of applications including detection of mutations and single nucleotide polymorphisms, analysis of DNA methylation, relative mRNA quantification, chromosomal characterisation of cell lines and tissue samples, detection of gene copy number, detection of duplications and deletions in human cancer predisposition genes such as BRCA1, BRCA2, hMLH1 and hMSH2 and aneuploidy determination. MLPA has potential application in prenatal diagnosis both invasive and noninvasive.\n\n", "id": "9346573", "title": "Multiplex ligation-dependent probe amplification"}
{"url": "https://en.wikipedia.org/wiki?curid=30865488", "text": "Complementarity (molecular biology)\n\nIn molecular biology, complementarity describes a relationship between two structures each following the lock-and-key principle. In nature complementarity is the base principle of DNA replication and transcription as it is a property shared between two DNA or RNA sequences, such that when they are aligned antiparallel to each other, the nucleotide bases at each position in the sequences will be complementary, much like looking in the mirror and seeing the reverse of things.\nThis complementary base pairing allows cells to copy information from one generation to another and even find and repair damage to the information stored in the sequences.\nThe degree of complementarity between two nucleic acid strands may vary, from complete complementarity (each nucleotide is across from its opposite) to no complementarity (each nucleotide is not across from its opposite) and determines the stability of the sequences to be together. Furthermore, various DNA repair functions as well as regulatory functions are based on base pair complementarity. In biotechnology, the principle of base pair complementarity allows the generation of DNA hybrids between RNA and DNA, and opens the door to modern tools such as cDNA libraries.\nWhile most complementarity is seen between two separate strings of DNA or RNA, it is also possible for a sequence to have internal complementarity resulting in the sequence binding to itself in a folded configuration.\n\nComplementarity is achieved by distinct interactions between nucleobases: adenine, thymine (uracil in RNA), guanine and cytosine. Adenine and guanine are purines, while thymine, cytosine and uracil are pyrimidines. Purines are larger than pyrimidines. Both types of molecules complement each other and can only base pair with the opposing type of nucleobase. In nucleic acid, nucleobases are held together by hydrogen bonding, which only works efficiently between adenine and thymine and between guanine and cytosine. The base complement A=T shares two hydrogen bonds, while the base pair G≡C has three hydrogen bonds. All other configurations between nucleobases would hinder double helix formation. DNA strands are oriented in opposite directions, they are said to be antiparallel.\n\nA complementary strand of DNA or RNA may be constructed based on nucleobase complementarity. Each base pair, A=T vs. G≡C, takes up roughly the same space, thereby enabling a twisted DNA double helix formation without any spatial distortions. Hydrogen bonding between the nucleobases also stabilizes the DNA double helix.\n\nComplementarity of DNA strands in a double helix make it possible to use one strand as a template to construct the other. This principle plays an important role in DNA replication, setting the foundation of heredity by explaining how genetic information can be passed down to the next generation. Complementarity is also utilized in DNA transcription, which generates an RNA strand from a DNA template.\n\nDNA repair mechanisms such as proof reading are complementarity based and allow for error correction during DNA replication by removing mismatched nucleobases.\n\nNucleic acids strands may also form hybrids in which single stranded DNA may readily anneal with complementary DNA or RNA. This principle is the basis of commonly performed laboratory techniques such as the polymerase chain reaction, PCR.\n\nTwo strands of complementary sequence are referred to as sense and anti-sense. The sense strand is, generally, the transcribed sequence of DNA or the RNA that was generated in transcription. While the anti-sense strand is the strand that is complementary to the sense sequence.\n\nSelf-complementarity refers to the fact that a sequence of DNA or RNA may fold back on itself, creating a double-strand like structure. Depending on how close together the parts of the sequence are that are self-complementary, the strand may form hairpin loops, junctions, bulges or internal loops. RNA is more likely to form these kinds of structures due to base pair binding not seen in DNA, such as guanine binding with uracil.\nComplementarity can be found between short nucleic acid stretches and a coding region or an transcribed gene, and results in base pairing. These short nucleic acid sequences are commonly found in nature and have regulatory functions such as gene silencing.\n\nAntisense transcripts are stretches of non coding mRNA that are complementary to the coding sequence. Genome wide studies have shown that RNA antisense transcripts occur commonly within nature. They are generally believed to increase the coding potential of the genetic code and add an overall layer of complexity to gene regulation. So far, it is known that 40% of the human genome is transcribed in both directions, underlining the potential significance of reverse transcription. \nIt has been suggested that complementary regions between sense and antisense transcripts would allow generation of double stranded RNA hybrids, which may play an important role in gene regulation. For example, \"hypoxia-induced factor 1α mRNA\" and \"β-secretase mRNA\" are transcribed bidirectionally, and it has been shown that the antisense transcript acts as a stabilizer to the sense script.\n\nmiRNAs, microRNA, are short RNA sequences that are complementary to regions of a transcribed gene and have regulatory functions. Current research indicates that circulating miRNA may be utilized as novel biomarkers, hence show promising evidence to be utilized in disease diagnostics. MiRNAs are formed from longer sequences of RNA that are cut free by a Dicer enzyme from an RNA sequence that is from a regulator gene. These short strands bind to a RISC complex. They match up with sequences in the upstream region of a transcribed gene due to their complementarity to act as a silencer for the gene in three ways. One is by preventing a ribosome from binding and initiating translation. Two is by degrading the mRNA that the complex has bound to. And three is by providing a new double-stranded RNA (dsRNA) sequence that Dicer can act upon to create more miRNA to find and degrade more copies of the gene. Small interfering RNAs (siRNAs) are similar in function to miRNAs; they come from other sources of RNA, but serve a similar purpose to miRNAs.\nGiven their short length, the rules for complementarity means that they can still be very discriminating in their targets of choice. Given that there are four choices for each base in the strand and a 20bp - 22bp length for a mi/siRNA, that leads to more than possible combinations. Given that the human genome is ~3.1 billion bases in length, this means that each miRNA should only find a match once in the entire human genome by accident.\n\nKissing hairpins are formed when a single strand of nucleic acid complements with itself creating loops of RNA in the form of a hairpin. When two hairpins come into contact with each other in vivo, the complementary bases of the two strands form up and begin to unwind the hairpins until a double-stranded RNA (dsRNA) complex is formed or the complex unwinds back to two separate strands due to mismatches in the hairpins. The secondary structure of the hairpin prior to kissing allows for a stable structure with a relatively fixed change in energy. The purpose of these structures is a balancing of stability of the hairpin loop vs binding strength with a complementary strand. Too strong of an initial binding to a bad location and the strands will not unwind quickly enough. Too weak of an initial binding and the strands won't ever fully form the desired complex. These hairpin structures allow for the exposure of enough bases to provide a strong enough check on the initial binding and a weak enough internal binding to allow the unfolding once a favorable match has been found.\n\nComplementarity allows information found in DNA or RNA to be stored in a single strand. The complementing strand can be determined from the template and vice versa as in cDNA libraries. This also allows for analysis, like comparing the sequences of two different species. Shorthands have been developed for writing down sequences when there are mismatches (ambiguity codes) or to speed up how to read the opposite sequence in the complement (ambigrams).\n\nA cDNA library is a collection of expressed DNA genes that are seen as a useful reference tool in gene identification and cloning processes. cDNA libraries are constructed from mRNA using RNA-dependent DNA polymerase reverse transcriptase (RT), which transcribes an mRNA template into DNA. Therefore, a cDNA library can only contain inserts that are meant to be transcribed into mRNA. This process relies on the principle of DNA/RNA complementarity. The end product of the libraries is double stranded DNA, which may be inserted into plasmids. Hence, cDNA libraries are a powerful tool in modern research.\n\nWhen writing sequences for systematic biology it may be necessary to have IUPAC codes that mean \"any of the two\" or \"any of the three\". The IUPAC code R (any purine) is complementary to Y (any pyrimidine) and M (amino) to K (keto). W (weak) and S (strong) are usually not swapped but have been swapped in the past by some tools. W and S denote \"weak\" and \"strong\", respectively, and indicate a number of the hydrogen bonds that a nucleotide uses to pair with its complementing partner. A partner uses the same number of the bonds to make a complementing pair.\n\nAn IUPAC code that specifically excludes one of the three nucleotides can be complementary to an IUPAC code that excludes the complementary nucleotide. For instance, V (A,C or G - \"not T\") can be complementary to B (C, G or T - \"not A\").\n\nSpecific characters may be used to create a suitable (ambigraphic) nucleic acid notation for complementary bases (i.e. guanine = \"b\", cytosine = \"q\", adenine = \"n\", and thymine = \"u\"), which makes it is possible to complement entire DNA sequences by simply rotating the text \"upside down\". For instance, with the previous alphabet, buqn(GTCA) would read as ubnq (TGAC, reverse complement) if turned upside down.\n\nAmbigraphic notations readily visualize complementary nucleic acid stretches such as palindromic sequences. This feature is enhanced when utilizing custom fonts or symbols rather than ordinary ASCII or even Unicode characters.\n\n\n", "id": "30865488", "title": "Complementarity (molecular biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=31179919", "text": "NeXtProt\n\nneXtProt is an on-line knowledge platform on human proteins.\nIt strives to be a comprehensive resource that provides a variety of\ntypes of information on human proteins, such as their function, \nsubcellular location, expression, interactions and role in diseases.\nThe major part of the information in neXtProt\n\nis obtained from the UniProt Swiss-Prot database but it is complemented by data originating from high-throughput studies with an emphasis on proteomics. neXtProt offers also an advanced search capacity based on the SPARQL technology as well as an API that allows to programatically extract the data stored in the resource. It is developed by the CALIPHO group directed by Amos Bairoch and Lydie Lane of the Swiss Institute of Bioinformatics (SIB).\n\n", "id": "31179919", "title": "NeXtProt"}
{"url": "https://en.wikipedia.org/wiki?curid=25008405", "text": "Primer dimer\n\nA Primer dimer (PD) is a potential by-product in PCR, a common biotechnological method. As its name implies, a PD consists of primer molecules that have attached (hybridized) to each other because of strings of complementary bases in the primers. As a result, the DNA polymerase amplifies the PD, leading to competition for PCR reagents, thus potentially inhibiting amplification of the DNA sequence targeted for PCR amplification. In quantitative PCR, PDs may interfere with accurate quantification.\n\nA primer dimer is formed and amplified in three steps. In the first step, two primers anneal at their respective 3' ends (step I in the figure). If this construct is stable enough, the DNA polymerase will bind and extend the primers according to the complementary sequence (step II in the figure). An important factor contributing to the stability of the construct in step I is a high GC-content at the 3' ends and length of the overlap. The third step occurs in the next cycle, when a single strand of the product of step II is used as a template to which fresh primers anneal leading to synthesis of more PD product.\n\nPrimer dimers may be visible after gel electrophoresis of the PCR product. PDs in ethidium bromide-stained gels are typically seen as a 30-50 base-pair (bp) band or smear of moderate to high intensity and distinguishable from the band of the target sequence, which is typically longer than 50 bp.\n\nIn quantitative PCR, PDs may be detected by melting curve analysis with intercalating dyes, such as SYBR Green I, a nonspecific dye for detection of double-stranded DNA. Because they usually consist of short sequences, the PDs denaturate at lower temperature than the target sequence and hence can be distinguished by their melting-curve characteristics.\n\nOne approach to prevent PDs consists of physical-chemical optimization of the PCR system, i.e., changing the concentrations of primers, magnesium chloride, nucleotides, ionic strength and temperature of the reaction. This method is somewhat limited by the physical-chemical characteristics that also determine the efficiency of amplification of the target sequence in the PCR. Therefore, reducing PDs formation may also result in reduced PCR efficiency. To overcome this limitation, other methods aim to reduce the formation of PDs only, including primer design, and use of different PCR enzyme systems or reagents. \n\nPrimer-design software uses algorithms that check for the potential of DNA secondary structure formation and annealing of primers to itself or within primer pairs. Physical parameters that are taken into account by the software are potential self-complementarity and GC content of the primers; similar melting temperatures of the primers; and absence of secondary structures, such as stem-loops, in the DNA target sequence.\n\nBecause primers are designed to have low complementarity to each other, they may anneal (step I in the figure) only at low temperature, e.g. room temperature, such as during the preparation of the reaction mixture. Although DNA polymerases used in PCR are most active around 70 °C, they have some polymerizing activity also at lower temperatures, which can cause DNA synthesis from primers after annealing to each other. Several methods have been developed to prevent PDs formation until the reaction reaches working temperature (60-70 °C), and these include initial inhibition of the DNA polymerase, or physical separation of reaction components reaction until the reaction mixture reaches the higher temperatures. These methods are referred to as \"hot-start PCR\".\n\n\"Wax\": in this method the enzyme is spatially separated from the reaction mixture by wax that melts when the reaction reaches high temperature.\n\n\"Slow release of magnesium\": DNA polymerase requires magnesium ions for activity, so the magnesium is chemically separated from the reaction by binding to a chemical compound, and is released into the solution only at high temperature \n\n\"Non-covalent binding of inhibitor\": in this method a peptide, antibody or aptamer are non-covalently bound to the enzyme at low temperature and inhibit its activity. After an incubation of 1–5 minutes at 95 °C, the inhibitor is released and the reaction starts.\n\n\"Cold-sensitive Taq polymerase\": is a modified DNA polymerase with almost no activity at low temperature.\n\n\"Chemical modification\": in this method a small molecule is covalently bound to the side chain of an amino acid in the active site of the DNA polymerase. The small molecule is released from the enzyme by incubation of the reaction mixture for 10–15 minutes at 95 °C. Once the small molecule is released, the enzyme is activated.\n\nAnother approach to prevent or reduce PD formation is by modifying the primers so that annealing with themselves or each other does not cause extension.\n\n\"HANDS\" (Homo-Tag Assisted Non-Dimer System): a nucleotide tail, complementary to the 3' end of the primer is added to the 5' end of the primer. Because of the close proximity of the 5' tail it anneals to the 3' end of the primer. The result is a stem-loop primer that excludes annealing involving shorter overlaps, but permits annealing of the primer to its fully complementary sequence in the target.\n\n\"Chimeric primers\": some DNA bases in the primer are replaced with RNA bases, creating a \"chimeric sequence\". The melting temperature of a chimeric sequence with another chimeric sequence is lower than that of chimeric sequence with DNA. This difference enables setting the annealing temperature such that the primer will anneal to its target sequence, but not to other chimeric primers.\n\n\"Blocked-cleavable primers\": a method known as RNase H-dependent PCR (rhPCR), utilizes a thermostable RNase HII to remove a blocking group from the PCR primers at high temperature. This RNase HII enzyme displays almost no activity at low temperature, making the removal of the block only occur at high temperature. The enzyme also possess inherent primer:template mismatch discrimination, resulting in additional selection against primer-dimers.\n\nWhile the methods above are designed to reduce PD formation, another approach aims to minimize signal generated from PDs in quantitative PCR. This approach is useful as long as there are few PDs formed and their inhibitory effect on product accumulation is minor.\n\n\"Four steps PCR\": used when working with nonspecific dyes, such as SYBR Green I. It is based on the different length, and hence, different melting temperature of the PDs and the target sequence. In this method the signal is acquired below the melting temperature of the target sequence, but above the melting temperature of the PDs.\n\n\"Sequence specific probes\": TaqMan and Molecular beacon probes generate signal only in the presence of their target (complementary) sequence, and this enhanced specificity precludes signal acquisition (but not possible inhibitory effects on product accumulation) from PDs.\n", "id": "25008405", "title": "Primer dimer"}
{"url": "https://en.wikipedia.org/wiki?curid=31182642", "text": "WebGeSTer\n\nWebGeSTer DB is a database of intrinsic transcription terminators\n\n\n", "id": "31182642", "title": "WebGeSTer"}
{"url": "https://en.wikipedia.org/wiki?curid=15690228", "text": "DERB\n\nDual expression recombinase based (DERB) single vector system is a method of efficient cloning and subcloning of plasmid vectors for high throughput screening (HTS) and verification of protein-protein interactions inside living cells. DERB was developed by Lu JP et al.\n\nPlasmid Vectors are deliberately constructed circular double-strand DNA loops capable of self-amplification and protein production. They are widely used in laboratories and the bio-medical and pharmaceutical industries to produce of DNA or protein in quantity. The DERB vector system consists of a series of vectors, each of which produces two or more proteins which are labeled or tagged for screening and verification of molecular interactions such as protein–protein interactions.\n\nThe following characteristics of the vectors ensure highly efficient cloning or subcloning of the protein of interest ORFs into the DERB vectors for high-throughput screening (HTS) and verification of protein–protein interactions after single introducing of the vectors into eukaryotic or prokaryotic cells:\n\n\nThe system eliminates the requirements of multiple step restriction-purification-ligation subcloning and co-transfection, which are bottle-neck in automation in HTS protein–protein interactions.\n\nThe examination of the protein–protein interactions of interest with the DERB vector system can use standard laboratory equipment, e.g. plate reader, flow cytometer, fluorescent microscope and confocal microscope.\n\nDERB is a new tool in high-throughput screening (HTS) and verification of protein–protein interactions and their modulators for both basic research and development such as screen new drugs or medicines for prevention and treatment of disease.\n", "id": "15690228", "title": "DERB"}
{"url": "https://en.wikipedia.org/wiki?curid=505422", "text": "Ribonuclease H\n\nRibonuclease H (abbreviated RNase H or RNH) is a family of non-sequence-specific endonuclease enzymes that catalyze the cleavage of RNA in an RNA/DNA substrate via a hydrolytic mechanism. Members of the RNase H family can be found in nearly all organisms, from bacteria to archaea to eukaryotes.\n\nThe family is divided into evolutionarily related groups with slightly different substrate preferences, broadly designated ribonuclease H1 and H2. The human genome encodes both H1 and H2. Human ribonuclease H2 is a heterotrimeric complex composed of three subunits, mutations in any of which are among the genetic causes of a rare disease known as Aicardi–Goutières syndrome. A third type, closely related to H2, is found only in a few prokaryotes, whereas H1 and H2 occur in all domains of life. Additionally, RNase H1-like retroviral ribonuclease H domains occur in multidomain reverse transcriptase proteins, which are encoded by retroviruses such as HIV and are required for viral replication.\n\nIn eukaryotes, ribonuclease H1 is involved in DNA replication of the mitochondrial genome. Both H1 and H2 are involved in genome maintenance tasks such as processing of R-loop structures.\n\nRibonuclease H is a family of endonuclease enzymes with a shared substrate specificity for the RNA strand of RNA-DNA duplexes. By definition, RNases H cleave RNA backbone phosphodiester bonds to leave a 3' hydroxyl and a 5' phosphate group. RNases H have been proposed as members of an evolutionarily related superfamily encompassing other nucleases and nucleic acid processing enzymes such as retroviral integrases, DNA transposases, Holliday junction resolvases, Piwi and Argonaute proteins, various exonucleases, and the spliceosomal protein Prp8.\n\nRNases H can be broadly divided into two subtypes, H1 and H2, which for historical reasons are given Arabic numeral designations in eukaryotes and Roman numeral designations in prokaryotes. Thus the \"Escherichia coli\" RNase HI is a homolog of the \"Homo sapiens\" RNase H1. In \"E. coli\" and many other prokaryotes, the \"rnhA\" gene encodes HI and the \"rnhB\" gene encodes HII. A third related class, called HIII, occurs in a few bacteria and archaea; it is closely related to prokaryotic HII enzymes.\n\nThe structure of RNase H commonly consists of a 5-stranded β-sheet surrounded by a distribution of α-helices. All RNases H have an active site centered on a conserved sequence motif composed of aspartate and glutamate residues, often referred to as the DEDD motif. These residues interact with catalytically required magnesium ions.\n\nRNases H2 are larger than H1 and usually have additional helices. The domain organization of the enzymes varies; some prokaryotic and most eukaryotic members of the H1 group have an additional small domain at the N-terminus known as the \"hybrid binding domain\", which facilitates binding to RNA:DNA hybrid duplexes and sometimes confers increased processivity. While all members of the H1 group and the prokaryotic members of the H2 group function as monomers, eukaryotic H2 enzymes are obligate heterotrimers. Prokaryotic HIII enzymes are members of the broader H2 group and share most structural features with H2, with the addition of an N-terminal TATA box binding domain. Retroviral RNase H domains occurring in multidomain reverse transcriptase proteins have structures closely resembling the H1 group.\n\nRNases H1 have been extensively studied to explore the relationships between structure and enzymatic activity. They are also used, especially the \"E. coli\" homolog, as model systems to study protein folding. Within the H1 group, a relationship has been identified between higher substrate-binding affinity and the presence of structural elements consisting of a helix and flexible loop providing a larger and more basic substrate-binding surface. The C-helix has a scattered taxonomic distribution; it is present in the \"E. coli\" and human RNase H1 homologs and absent in the HIV RNase H domain, but examples of retroviral domains with C-helices do exist.\n\nRibonuclease H enzymes cleave the phosphodiester bonds of RNA in a double-stranded RNA:DNA hybrid, leaving a 3' hydroxyl and a 5' phosphate group on either end of the cut site. RNase H1 and H2 have distinct substrate preferences and distinct but overlapping functions in the cell. In prokaryotes and lower eukaryotes, neither enzyme is essential, whereas both are believed to be essential in higher eukaryotes. The combined activity of both H1 and H2 enzymes is associated with maintenance of genome stability due to the enzymes' degradation of the RNA component of R-loops.\n\nRibonuclease H1 enzymes require at least four ribonucleotide-containing base pairs in a substrate and cannot remove a single ribonucleotide from a strand that is otherwise composed of deoxyribonucleotides. For this reason, it is considered unlikely that RNase H1 enzymes are involved in the processing of RNA primers from Okazaki fragments during DNA replication. RNase H1 is not essential in unicellular organisms where it has been investigated; in \"E. coli\", RNase H1 knockouts confer a temperature-sensitive phenotype, and in \"S. cerevisiae\", they produce defects in stress response.\n\nIn many eukaryotes, including mammals, RNase H1 genes include a mitochondrial targeting sequence, leading to expression of isoforms with and without the MTS present. As a result, RNase H1 is localized to both mitochondria and the nucleus. In knockout mouse models, RNase H1-null mutants are lethal during embryogenesis due to defects in replicating mitochondrial DNA. The defects in mitochondrial DNA replication induced by loss of RNase H1 are likely due to defects in R-loop processing.\n\nIn prokaryotes, RNase H2 is enzymatically active as a monomeric protein. In eukaryotes, it is an obligate heterotrimer composed of a catalytic subunit A and structural subunits B and C. While the A subunit is closely homologous to the prokaryotic RNase H2, the B and C subunits have no apparent homologs in prokaryotes and are poorly conserved at the sequence level even among eukaryotes. The B subunit mediates protein-protein interactions between the H2 complex and PCNA, which localizes H2 to replication foci.\n\nBoth prokaryotic and eukaryotic H2 enzymes can cleave single ribonucleotides in a strand. however, they have slightly different cleavage patterns and substrate preferences: prokaryotic enzymes have lower processivity and hydrolyze successive ribonucleotides more efficiently than ribonucleotides with a 5' deoxyribonucleotide, while eukaryotic enzymes are more processive and hydrolyze both types of substrate with similar efficiency. The substrate specificity of RNase H2 gives it a role in ribonucleotide excision repair, removing misincorporated ribonucleotides from DNA, in addition to R-loop processing. Although both H1 and H2 are present in the mammalian cell nucleus, H2 is the dominant source of RNase H activity there and is important for maintaining genome stability.\n\nSome prokaryotes possess an additional H2-type gene designated RNase HIII in the Roman-numeral nomenclature used for the prokaryotic genes. HIII proteins are more closely related to the H2 group by sequence identity and structural similarity, but have substrate preferences that more closely resemble H1. Unlike HI and HII, which are both widely distributed among prokaryotes, HIII is found in only a few organisms with a scattered taxonomic distribution; it is somewhat more common in archaea and is rarely or never found in the same prokaryotic genome as HI.\n\nThe active site of nearly all RNases H contains four negatively charged amino acid residues, known as the DEDD motif; often histidine is also present.\n\nThe charged residues bind either one or two metal ions that are required for catalysis; under physiological conditions these are magnesium ions, but manganese also usually supports enzymatic activity, while calcium may inhibit it. Although two-metal-ion catalytic mechanisms are very common in enzymes involved in phosphate biochemistry, it has been a subject of debate in the literature whether one or two ions are used in RNase H catalysis. In either proposed mechanism, at least one water molecule participates in the reaction.\n\nMost experimental evidence for the mechanism of RNase H catalysis comes from measurements performed on members of the H1 group, usually the \"E. coli\" homolog. According to measurements of this protein, one of the aspartate residues has an elevated pKa, while another has an abnormally low pKa. It is unclear whether any of the active-site residues participates in the reaction as a general base. In addition, it is possible that one of the substrate's oxygen atoms participates directly in the reaction as a base.\n\nThe human genome contains four genes encoding RNase H:\n\nIn addition, genetic material of retroviral origin appears frequently in the genome, reflecting integration of the genomes of human endogenous retroviruses. Such integration events result in the presence of genes encoding retroviral reverse transcriptase, which includes an RNase H domain. An example is ERVK6. Long terminal repeat retrotransposons are also common in the genome and often include their own RNase H domains, with a complex evolutionary history.\n\nIn small studies, mutations in human RNase H1 have been associated with chronic progressive external ophthalmoplegia, a common feature of mitochondrial disease.\n\nMutations in any of the three RNase H2 subunits are well-established as causes of a rare genetic disorder known as Aicardi–Goutières syndrome (AGS), which manifests as neurological and dermatological symptoms at an early age. The symptoms of AGS closely resemble those of congenital viral infection and are associated with inappropriate upregulation of type I interferon. AGS can also be caused by mutations in other genes: TREX1, SAMHD1, ADAR, and MDA5/IFIH1, all of which are involved in nucleic acid processing. Characterization of mutational distribution in an AGS patient population found 5% of all AGS mutations in RNASEH2A, 36% in 2B, and 12% in 2C. Mutations in 2B have been associated with somewhat milder neurological impairment and with an absence of interferon-induced gene upregulation that can be detected in patients with other AGS-associated genotypes.\n\nTwo groups of viruses use reverse transcription as part of their life cycles: retroviruses, which encode their genomes in single-stranded RNA and replicate through a double-stranded DNA intermediate; and dsDNA-RT viruses, which replicate their double-stranded DNA genomes through an RNA \"pregenome\" intermediate. Pathogenic examples include human immunodeficiency virus and hepatitis B virus, respectively. Both encode large multifunctional reverse transcriptase (RT) proteins containing RNase H domains.\n\nRetroviral RT proteins from HIV-1 and murine leukemia virus are the best-studied members of the family. Retroviral RT is responsible for converting the virus' single-stranded RNA genome into double-stranded DNA. This process requires three steps: first, RNA-dependent DNA polymerase activity produces minus-strand DNA from the plus-strand RNA template, generating an RNA:DNA hybrid intermediate; second, the RNA strand is destroyed; and third, DNA-dependent DNA polymerase activity synthesizes plus-strand DNA, generating double-stranded DNA as the final product. The second step of this process is carried out by an RNase H domain located at the C-terminus of the RT protein.\n\nRNase H performs three types of cleaving actions: non-specific degradation of the plus-strand RNA genome, specific removal of the minus-strand tRNA primer, and removal of the plus-strand purine-rich polypurine tract (PPT) primer. RNase H plays a role in the priming of the plus-strand, but not in the conventional method of synthesizing a new primer sequence. Rather RNase H creates a \"primer\" from the PPT that is resistant to RNase H cleavage. By removing all bases but the PPT, the PPT is used as a marker for the end of the U3 region of its long terminal repeat.\n\nBecause RNase H activity is required for viral proliferation, this domain has been considered a drug target for the development of antiretroviral drugs used in the treatment of HIV/AIDS and other conditions caused by retroviruses. Inhibitors of retroviral RNase H of several different chemotypes have been identified, many of which have a mechanism of action based on chelation of the active-site cations. Reverse-transcriptase inhibitors that specifically inhibit the polymerase function of RT are in widespread clinical use, but not inhibitors of the RNase H function; it is the only enzymatic function encoded by HIV that is not yet targeted by drugs in clinical use.\n\nRNases H are widely distributed and occur in all domains of life. The family belongs to a larger superfamily of nuclease enzymes and is considered to be evolutionarily ancient. In prokaryotic genomes, multiple RNase H genes are often present, but there is little correlation between occurrence of HI, HII, and HIII genes and overall phylogenetic relationships, suggesting that horizontal gene transfer may have played a role in establishing the distribution of these enzymes. RNase HI and HIII rarely or never appear in the same prokaryotic genome. When an organism's genome contains more than one RNase H gene, they sometimes have significant differences in activity level. These observations have been suggested to reflect an evolutionary pattern that minimizes functional redundancy among RNase H genes. RNase HIII, which is unique to prokaryotes, has a scattered taxonomic distribution and is found in both bacteria and archaea; it is believed to have diverged from HII fairly early.\n\nThe evolutionary trajectory of RNase H2 in eukaryotes, especially the mechanism by which eukaryotic homologs became obligate heterotrimers, is unclear; the B and C subunits have no apparent homologs in prokaryotes.\n\nBecause RNase H specifically degrades only the RNA in double-stranded RNA:DNA hybrids, it is commonly used as a laboratory reagent in molecular biology. Purified preparations of \"E. coli\" RNase HI and HII are commercially available. RNase HI is often used to destroy the RNA template after first-strand complementary DNA (cDNA) synthesis by reverse transcription. It can also be used to cleave specific RNA sequences in the presence of short complementary segments of DNA. Highly sensitive techniques such as surface plasmon resonance can be used for detection. RNase HII can be used to degrade the RNA primer component of an Okazaki fragment or to introduce single-stranded nicks at positions containing a ribonucleotide. A variant of hot start PCR, known as RNase H-dependent PCR or rhPCR, has been described using a thermostable RNase HII from the hyperthermophilic archaeon \"Pyrococcus abyssi\". Of note, the ribonuclease inhibitor protein commonly used as a reagent is not effective at inhibiting the activity of either HI or HII.\n\nRibonucleases H were first discovered in the laboratory of Peter Hausen when researchers found RNA:DNA hybrid endonuclease activity in calf thymus in 1969 and gave it the name \"ribonuclease \"H\"\" to designate its \"hybrid\" specificity. RNase H activity was subsequently discovered in \"E. coli\" and in a sample of oncoviruses with RNA genomes during early studies of viral reverse transcription. It later became clear that calf thymus extract contained more than one protein with RNase H activity and that \"E. coli\" contained two RNase H genes. Originally, the enzyme now known as RNase H2 in eukaryotes was designated H1 and vice versa, but the names of the eukaryotic enzymes were switched to match those in \"E. coli\" to facilitate comparative analysis, yielding the modern nomenclature in which the prokaryotic enzymes are designated with Roman numerals and the eukaryotic enzymes with Arabic numerals. The prokaryotic RNase HIII, reported in 1999, was the last RNase H subtype to be identified.\n\nCharacterizing eukaryotic RNase H2 was historically a challenge, in part due to its low abundance. Careful efforts at purification of the enzyme suggested that, unlike the \"E. coli\" RNase H2, the eukaryotic enzyme had multiple subunits. The \"S. cerevisiae\" homolog of the \"E. coli\" protein (that is, the H2A subunit) was easily identifiable by bioinformatics when the yeast genome was sequenced, but the corresponding protein was found not to have enzymatic activity in isolation. Eventually, the yeast B and C subunits were isolated by co-purification and found to be required for enzymatic activity. However, the yeast B and C subunits have very low sequence identity to their homologs in other organisms, and the corresponding human proteins were conclusively identified only after mutations in all three were found to cause Aicardi–Goutières syndrome.\n\n", "id": "505422", "title": "Ribonuclease H"}
{"url": "https://en.wikipedia.org/wiki?curid=450993", "text": "Polyhistidine-tag\n\nA polyhistidine-tag is an amino acid motif in proteins that consists of at least six histidine (\"His\") residues, often at the N- or C-terminus of the protein. It is also known as hexa histidine-tag, 6xHis-tag, His6 tag and by the trademarked name His-tag (registered by EMD Biosciences). The tag was invented by Roche, although the use of histidines and its vectors are distributed by Qiagen. Various purification kits for histidine-tagged proteins are available from Qiagen, Sigma, Thermo Scientific, GE Healthcare, Macherey-Nagel, Clontech, Bio-Rad, and others.\n\nThe use of the tag for academic users is unrestricted; however, commercial users must pay royalties to Roche. The original patent expired on 11 Feb 2003, and thus should be now public property; current claims to royalties are based on a much narrower set of more recent patents. Suitable tag sequences are available free for commercial use; for example, MK(HQ)6 may be used for enhanced expression in \"E. coli\" and tag removal. The total number of histidine residues may vary in the tag. N- or C-terminal his-tags may also be followed or preceded, respectively, by a suitable amino acid sequence that facilitates a removal of the polyhistidine-tag using endopeptidases. This extra sequence is not necessary if exopeptidases are used to remove N-terminal His-tags (e.g., Qiagen TAGZyme). Furthermore, exopeptidase cleavage may solve the unspecific cleavage observed when using endoprotease-based tag removal. Polyhistidine-tags are often used for affinity purification of genetically modified proteins.\n\nPolyhistidine-tags are often used for affinity purification of polyhistidine-tagged recombinant proteins expressed in \"Escherichia coli\" and other prokaryotic expression systems. Bacterial cells are harvested via centrifugation and the resulting cell pellet lysed either by physical means or by means of detergents and enzymes such as lysozyme or any combination of these. At this stage raw lysate contains the recombinant protein among many other proteins originating from the bacterial host. This mixture is incubated with an affinity resin containing bound divalent nickel or cobalt ions, which are available commercially in different varieties. Nickel and cobalt have similar properties and as they are adjacent period 4 transition metals ((v. iron triad)). These resins are generally sepharose/agarose functionalised with a chelator, such as iminodiacetic acid (Ni-IDA) and nitrilotriacetic acid (Ni-NTA) for nickel and carboxylmethylaspartate (Co-CMA) for cobalt, which the polyhistidine-tag binds with micromolar affinity. The resin is then washed with phosphate buffer to remove proteins that do not specifically interact with the cobalt or nickel ion. With Ni-based methods, washing efficiency can be improved by the addition of 20 mM imidazole (proteins are usually eluted with 150-300 mM imidazole). Generally nickel-based resins have higher binding capacity, while cobalt-based resins offer the highest purity. The purity and amount of protein can be assessed by SDS-PAGE and Western blotting.\n\nAffinity purification using a polyhistidine-tag usually results in relatively pure protein when the recombinant protein is expressed in prokaryotic organisms. Depending on downstream applications, including the purification of protein complexes to study protein interactions, purification from higher organisms such as yeasts or other eukaryotes may require a tandem affinity purification using two tags to yield higher purity. Alternatively, single-step purification using immobilized cobalt ions rather than nickel ions generally yields a substantial increase in purity and requires lower imidazole concentrations for elution of the his-tagged protein.\n\nPolyhistidine-tagging is the option of choice for purifying recombinant proteins in denaturing conditions because its mode of action is dependent only on the primary structure of proteins. Generally for this sort of a technique, histidine binding is titrated using pH instead of imidazole binding—at a high pH, histidine binds to nickel or cobalt, but at low pH (~6 for cobalt and ~4 for nickel), histidine becomes protonated and is competed off of the metal ion. Compare this to antibody purification and GST purification, a prerequisite to which is the proper (native) folding of proteins involved.\n\nPolyhistidine-tag columns retain several well known proteins as impurities. One of them is FKBP-type peptidyl prolyl isomerase, which appears around 25kDa (SlyD). Impurities are generally eliminated using a secondary chromatographic technique, or by expressing the recombinant protein in a SlyD-deficient \"E. coli\" strain. Alternatively, comparing with nickel-based, cobalt-based resins have less affinity with SlyD from E. coli, but in several cases, it is moderately helpful.\n\nProteins with different numbers of polyhistidine tags elute differently from nickel-affinity resin. For proteins with a single hexahistidine tag, 75 mM imidazole enables elution from Ni-NTA, whereas for proteins with two hexahistidine tags, 100 mM imidazole is required for elution. This step-wise elution may be used to isolate specific protein assemblies from a mixture, such as defined heteromultimers (e.g. an AB heterodimer from a mixture including AA and BB homodimers, if only subunit B has a polyhistidine tag). Such an approach was used in isolation of monovalent streptavidin.\n\nPolyhistidine-tagging can be used to detect protein-protein interactions in the same way as a pull-down assay. However, this technique is generally considered to be less sensitive, and also restricted by some of the more finicky aspects of this technique. For example, reducing conditions cannot be used, EDTA and many types of detergents cannot be used. Recent advances in dual polarisation interferometry is amenable to EDTA and a wider use of reagents, and the use of such site-specific tags greatly simplifies the direct measurement of associated conformational change.\n\nHexahistadine CyDye tags have also been developed. These use Nickel covalent coordination to EDTA groups attached to fluorophores in order to create dyes that attach to the polyhistidine tag. This technique has been shown to be effective for following protein migration and trafficking. There has also been recent discoveries that show this technique may be effective in order to measure distance via Fluorescent Resonance Energy Transfer.\n\nA polyfluorohistidine tag has been reported for use in \"in vitro\" translation systems. In this system, an expanded genetic code is used in which histidine is replaced by 4-fluorohistidine. The fluorinated analog is incorporated into peptides via the relaxed substrate specificity of histidine-tRNA ligase and lowers the overall pK of the tag. This allows for the selective enrichment of polyfluorohistidine tagged peptides in the presence of complex mixtures of traditional polyhistidine tags by altering the pH of the wash buffers.\n\nThe most common polyhistidine tags are formed of six histidine (6xHis tag) residues - which are added at the N-terminus preceded by Methionine or C-terminus before a stop codon, in the coding sequence of the protein of interest. The choice of the end where His-tag is added will depend mainly on the characteristics of the protein and the methods chosen to remove the tag. Some ends are buried inside the protein core and others are important for the protein function or structure. In those cases the choice is limited to the other end. On the other hand, most available exopeptidases can only remove the His-tag from the N-terminus; removing the tag from the C-terminus will require the use of other techniques. It is important to take into account that the computer simulation (by molecular dynamic) will help you to choose between options, for example, whether the His-tag must be digested or engineered to the N- or C-terminal.\nThere are two ways to add polyhistidines. The most simple is to insert the DNA encoding the protein in a vector encoding a His-tag so that it will be automatically attached to one of its ends . Another technique is to perform a PCR with primers that have repetitive histidine codons (CAT or CAC) right next to the START or STOP codon in addition to several (16 or more) bases from one end of the DNA encoding the protein to be tagged (see primer example below).\n\n\"Example of primer designed to add a 6xHis-tag using PCR\". Eighteen bases coding six histidines are inserted right after the START codon or right before the STOP codon. At least 16 bases specific to the gene of interest are needed next to the His-tag. With 6 His, the protein will have an added 1 kDa of molecular weight. Note: often, a linker (such as gly-gly-gly or gly-ser-gly) is placed between the protein of interest and the 6 His tag. This is to prevent the polyhistidine tag from affecting the activity of the protein being tagged.\n\nThe polyhistidine-tag can also be used to detect the protein via anti-polyhistidine-tag antibodies or alternatively by in-gel staining (SDS-PAGE) with fluorescent probes bearing metal ions. This can be useful in subcellular localization, ELISA, western blotting or other immuno-analytical methods.\n\nThe polyhistidine-tag can be successfully used for the immobilization of proteins on a surface such as on a nickel- or cobalt-coated microtiter plate or on a protein array.\n\n", "id": "450993", "title": "Polyhistidine-tag"}
{"url": "https://en.wikipedia.org/wiki?curid=1053858", "text": "Functional genomics\n\nFunctional genomics is a field of molecular biology that attempts to make use of the vast wealth of data given by genomic and transcriptomic projects (such as genome sequencing projects and RNA sequencing) to describe gene (and protein) functions and interactions. Unlike structural genomics, functional genomics focuses on the dynamic aspects such as gene transcription, translation, regulation of gene expression and protein–protein interactions, as opposed to the static aspects of the genomic information such as DNA sequence or structures. Functional genomics attempts to answer questions about the function of DNA at the levels of genes, RNA transcripts, and protein products. A key characteristic of functional genomics studies is their genome-wide approach to these questions, generally involving high-throughput methods rather than a more traditional “gene-by-gene” approach.\nThe goal of functional genomics is to understand the function of larger numbers of genes or proteins, eventually all components of a genome. A more long-term goal is to understand the relationship between an organism's genome and its phenotype. The term functional genomics is often used broadly to refer to the many technical approaches to study an organism's genes and proteins, including the \"biochemical, cellular, and/or physiological properties of each and every gene product\" while some authors include the study of nongenic elements in his definition. Functional genomics may also include studies of natural genetic variation over time (such as an organism's development) or space (such as its body regions), as well as functional disruptions such as mutations.\n\nThe promise of functional genomics is to generate and synthesize genomic and proteomic knowledge into an understanding of the dynamic properties of an organism. This would provide a more complete picture than studies of single genes. Integration of functional genomics data is also the goal of systems biology.\n\nFunctional genomics includes function-related aspects of the genome itself such as mutation and polymorphism (such as single nucleotide polymorphism (SNP) analysis), as well as measurement of molecular activities. The latter comprise a number of \"-omics\" such as transcriptomics (gene expression), proteomics (protein production), and metabolomics. Functional genomics uses mostly multiplex techniques to measure the abundance of many or all gene products such as mRNAs or proteins within a biological sample. Together these measurement modalities endeavor to quantitate the various biological processes and improve our understanding of gene and protein functions and interactions.\n\nSystematic pairwise deletion of genes or inhibition of gene expression can be used to identify genes with related function, even if they do not interact physically. Epistasis refers to the fact that effects for two different gene knockouts may not be additive; that is, the phenotype that results when two genes are inhibited may be different from the sum of the effects of single knockouts.\n\nThe ENCODE (Encyclopedia of DNA elements) project is an in-depth analysis of the human genome whose goal is to identify all the functional elements of genomic DNA, in both coding and noncoding regions. To this point, only the pilot phase of the study has been completed, involving hundreds of assays performed on 44 regions of known or unknown function comprising 1% of the human genome. Important results include evidence from genomic tiling arrays that most nucleotides are transcribed as coding transcripts, noncoding RNAs, or random transcripts, the discovery of additional transcriptional regulatory sites, further elucidation of chromatin-modifying mechanisms.\n\nMicroarrays measure the amount of mRNA in a sample that corresponds to a given gene or probe DNA sequence. Probe sequences are immobilized on a solid surface and allowed to hybridize with fluorescently labeled “target” mRNA. The intensity of fluorescence of a spot is proportional to the amount of target sequence that has hybridized to that spot, and therefore to the abundance of that mRNA sequence in the sample. Microarrays allow for identification of candidate genes involved in a given process based on variation between transcript levels for different conditions and shared expression patterns with genes of known function.\n\nSAGE (serial analysis of gene expression) is an alternate method of gene expression analysis based on RNA sequencing rather than hybridization. SAGE relies on the sequencing of 10–17 base pair tags which are unique to each gene. These tags are produced from poly-A mRNA and ligated end-to-end before sequencing. SAGE gives an unbiased measurement of the number of transcripts per cell, since it does not depend on prior knowledge of what transcripts to study (as microarrays do).\n\nRNA sequencing has taken over microarray and SAGE technology in recent years and has become the most efficient way to study transcription and gene expression. This is typically done by next-generation sequencing.\n\nA subset of sequenced RNAs are small RNAs, a class of non-coding RNA molecules that are key regulators of transcriptional and post-transcriptional gene silencing, or RNA silencing. Next generation sequencing is the gold standard tool for non-coding RNA discovery, profiling and expression analysis.\n\nA yeast two-hybrid (Y2H) screen tests a \"bait\" protein against many potential interacting proteins (\"prey\") to identify physical protein–protein interactions. This system is based on a transcription factor, originally GAL4, whose separate DNA-binding and transcription activation domains are both required in order for the protein to cause transcription of a reporter gene. In a Y2H screen, the \"bait\" protein is fused to the binding domain of GAL4, and a library of potential \"prey\" (interacting) proteins is recombinantly expressed in a vector with the activation domain. In vivo interaction of bait and prey proteins in a yeast cell brings the activation and binding domains of GAL4 close enough together to result in expression of a reporter gene. It is also possible to systematically test a library of bait proteins against a library of prey proteins to identify all possible interactions in a cell.\n\nAffinity purification and mass spectrometry (AP/MS) is able to identify proteins that interact with one another in complexes. Complexes of proteins are allowed to form around a particular “bait” protein. The bait protein is identified using an antibody or a recombinant tag which allows it to be extracted along with any proteins that have formed a complex with it. The proteins are then digested into short peptide fragments and mass spectrometry is used to identify the proteins based on the mass-to-charge ratios of those fragments.\n\nGene function can be investigated by systematically “knocking out” genes one by one. This is done by either deletion or disruption of function (such as by insertional mutagenesis) and the resulting organisms are screened for phenotypes that provide clues to the function of the disrupted gene.\n\nRNA interference (RNAi) methods can be used to transiently silence or knock down gene expression using ~20 base-pair double-stranded RNA typically delivered by transfection of synthetic ~20-mer short-interfering RNA molecules (siRNAs) or by virally encoded short-hairpin RNAs (shRNAs). RNAi screens, typically performed in cell culture-based assays or experimental organisms (such as C. elegans) can be used to systematically disrupt nearly every gene in a genome or subsets of genes (sub-genomes); possible functions of disrupted genes can be assigned based on observed phenotypes.\n\nPutative genes can be identified by scanning a genome for regions likely to encode proteins, based on characteristics such as long open reading frames, transcriptional initiation sequences, and polyadenylation sites. A sequence identified as a putative gene must be confirmed by further evidence, such as similarity to cDNA or EST sequences from the same organism, similarity of the predicted protein sequence to known proteins, association with promoter sequences, or evidence that mutating the sequence produces an observable phenotype.\n\nThe Rosetta stone approach is a computation method of de novo protein function prediction, based on the hypothesis that some proteins involved in a given physiological process may exist as two separate genes in one organism and as a single gene in another. Genomes are scanned for sequences that are independent in one organism and in a single open reading frame in another. If two genes have fused, it is predicted that they have similar biological functions that make such co-regulation advantageous.\n\nBecause of the large quantity of data produced by these techniques and the desire to find biologically meaningful patterns, bioinformatics is crucial to analysis of functional genomics data. Examples of techniques in this class are data clustering or principal component analysis for unsupervised machine learning (class detection) as well as artificial neural networks or support vector machines for supervised machine learning (class prediction, classification). Functional enrichment analysis is used to determine the extent of over- or under-expression (positive- or negative- regulators in case of RNAi screens) of functional categories relative to a background sets. Gene ontology based enrichment analysis are provided by DAVID and gene set enrichment analysis (GSEA), pathway based analysis by Ingenuity and Pathway studio and protein complex based analysis by COMPLEAT.\n\n\n", "id": "1053858", "title": "Functional genomics"}
{"url": "https://en.wikipedia.org/wiki?curid=1743649", "text": "Serial analysis of gene expression\n\nSerial analysis of gene expression (SAGE) is a transcriptomic technique used by molecular biologists to produce a snapshot of the messenger RNA population in a sample of interest in the form of small tags that correspond to fragments of those transcripts. Several variants have been developed since, most notably a more robust version, LongSAGE, RL-SAGE and the most recent SuperSAGE. Many of these have improved the technique with the capture of longer tags, enabling more confident identification of a source gene.\n\nBriefly, SAGE experiments proceed as follows:\n\n\nThe output of SAGE is a list of short sequence tags and the number of times it is observed. Using sequence databases a researcher can usually determine, with some confidence, from which original mRNA (and therefore which gene) the tag was extracted.\n\nStatistical methods can be applied to tag and count lists from different samples in order to determine which genes are more highly expressed. For example, a normal tissue sample can be compared against a corresponding tumor to determine which genes tend to be more (or less) active.\n\nIn 1979 teams at Harvard and Caltech extended the basic idea of making DNA copies of mRNAs in vitro to amplifying a library of such in bacterial plasmids. In 1982-3, the idea of selecting random or semi-random clones from such a cDNA library for sequencing was explored by Greg Sutcliffe and coworkers. and Putney et al. who sequenced 178 clones from a rabbit muscle cDNA library. In 1991 Adams and co-workers coined the term Expressed Sequence Tag (EST) and initiated more systematic sequencing of cDNAs as a project (starting with 600 brain cDNAs). The identification of ESTs proceeded rapidly, millions of ESTs now available in public databases (e.g. GenBank).\n\nIn 1995, the idea of reducing the tag length from 100 to 800 bp down to tag length of 10 to 22 bp helped reduce the cost of mRNA surveys. In this year, the original SAGE protocol was published by Victor Velculescu at the Oncology Center of Johns Hopkins University. Although SAGE was originally conceived for use in cancer studies, it has been successfully used to describe the transcriptome of other diseases and in a wide variety of organisms.\n\nThe general goal of the technique is similar to the DNA microarray. However, SAGE sampling is based on sequencing mRNA output, not on hybridization of mRNA output to probes, so transcription levels are measured more quantitatively than by microarray. In addition, the mRNA sequences do not need to be known \"a priori\", so genes or gene variants which are not known can be discovered. Microarray experiments are much cheaper to perform, so large-scale studies do not typically use SAGE. Quantifying gene expressions is more exact in SAGE because it involves directly counting the number of transcripts whereas spot intensities in microarrays fall in non-discrete gradients and are prone to background noise.\n\nMicroRNAs, or miRNAs for short, are small (~22nt) segments of RNA which have been found to play a crucial role in gene regulation. One of the most commonly used methods for cloning and identifying miRNAs within a cell or tissue was developed in the Bartel Lab and published in a paper by Lau \"et al.\" (2001). Since then, several variant protocols have arisen, but most have the same basic format. The procedure is quite similar to SAGE: The small RNA are isolated, then linkers are added to each, and the RNA is converted to cDNA by RT-PCR. Following this, the linkers, containing internal restriction sites, are digested with the appropriate restriction enzyme and the sticky ends are ligated together into concatamers. Following concatenation, the fragments are ligated into plasmids and are used to transform bacteria to generate many copies of the plasmid containing the inserts. Those may then be sequenced to identify the miRNA present, as well as analysing expression levels of a given miRNA by counting the number of times it is present, similar to SAGE.\n\nLongSAGE was a more robust version of the original SAGE developed in 2002 which had a higher throughput, using 20 μg of mRNA to generate a cDNA library of thousands of tags. Robust LongSage (RL-SAGE) Further improved on the LongSAGE protocol with the ability to generate a library with an insert size of 50 ng mRNA, much smaller than previous LongSAGE insert size of 2 μg mRNA and using a lower number of ditag polymerase chain reactions (PCR) to obtain a complete cDNA library. \n\nSuperSAGE is a derivative of SAGE that uses the type III-endonuclease EcoP15I of phage P1, to cut 26 bp long sequence tags from each transcript's cDNA, expanding the tag-size by at least 6 bp as compared to the predecessor techniques SAGE and LongSAGE. The longer tag-size allows for a more precise allocation of the tag to the corresponding transcript, because each additional base increases the precision of the annotation considerably.\n\nLike in the original SAGE protocol, so-called ditags are formed, using blunt-ended tags. However, SuperSAGE avoids the bias observed during the less random LongSAGE 20 bp ditag-ligation. By direct sequencing with high-throughput sequencing techniques (next-generation sequencing, i.e. pyrosequencing), hundred thousands or millions of tags can be analyzed simultaneously, producing very precise and quantitative gene expression profiles. Therefore, tag-based gene expression profiling also called \"digital gene expression profiling\" (DGE) can today provide most accurate transcription profiles that overcome the limitations of microarrays.\n\nin the mid 2010s several techniques combined with Next Generation Sequencing were developed that employ the \"tag\" principle for \"digital gene expression profiling\" but without the use of the tagging enzyme. The \"MACE\" approach, (=Massive Analysis of cDNA Ends) generates tags somewhere in the last 1500 bps of a transcript. The technique does not depend on restriction enzymes anymore and thereby circumvents bias that is related to the absence or location of the restriction site within the cDNA. Instead, the cDNA is randomly fragmented and the 3'ends are sequenced from the 5' end of the cDNA molecule that carries the poy-A tail. The sequencing length of the tag can be freely chosen. Because of this, the tags can be assembled into contigs and the annotation of the tags can be drastically improved. Therefore, MACE is also use for the analyses of non-model organisms. In addition, the longer contigs can be screened for polymorphisms. As UTRs show a large number of polymorphisms between individuals, the MACE approach can be applied for allele determination, allele specific gene expression profiling and the search for molecular markers for breeding. In addition, the approach allows determining alternative polyadenylation of the transcripts. Because MACE does only require 3’ ends of transcripts, even partly degraded RNA can be analyzed with less degradation dependent bias. The MACE approach uses unique molecular identifiers to allow for identification of PCR bias. \n\n", "id": "1743649", "title": "Serial analysis of gene expression"}
{"url": "https://en.wikipedia.org/wiki?curid=21731590", "text": "RNA-Seq\n\nRNA-Seq (RNA sequencing), also called whole transcriptome shotgun sequencing (\"WTSS\"), uses next-generation sequencing (NGS) to reveal the presence and quantity of RNA in a biological sample at a given moment in time.\n\nRNA-Seq is used to analyze the continuously changing cellular transcriptome. Specifically, RNA-Seq facilitates the ability to look at alternative gene spliced transcripts, post-transcriptional modifications, gene fusion, mutations/SNPs and changes in gene expression over time, or differences in gene expression in different groups or treatments. In addition to mRNA transcripts, RNA-Seq can look at different populations of RNA to include total RNA, small RNA, such as miRNA, tRNA, and ribosomal profiling. RNA-Seq can also be used to determine exon/intron boundaries and verify or amend previously annotated 5' and 3' gene boundaries.\n\nPrior to RNA-Seq, gene expression studies were done with hybridization-based microarrays. Issues with microarrays include cross-hybridization artifacts, poor quantification of lowly and highly expressed genes, and needing to know the sequence \"a priori\". Because of these technical issues, transcriptomics transitioned to sequencing-based methods. These progressed from Sanger sequencing of Expressed Sequence Tag libraries, to chemical tag-based methods (e.g., serial analysis of gene expression), and finally to the current technology, next-gen sequencing of cDNA (notably RNA-Seq).\n\nThe general steps to prepare a complementary DNA (cDNA) library for sequencing are described below, but often vary between platforms.\n\nWhen sequencing RNA other than mRNA, the library preparation is modified. The cellular RNA is selected based on the desired size range. For small RNA targets, such as miRNA, the RNA is isolated through size selection. This can be performed with a size exclusion gel, through size selection magnetic beads, or with a commercially developed kit. Once isolated, linkers are added to the 3' and 5' end then purified. The final step is cDNA generation through reverse transcription.\n\nAs converting RNA into cDNA using reverse transcriptase has been shown to introduce biases and artifacts that may interfere with both the proper characterization and quantification of transcripts, single molecule Direct RNA Sequencing (DRSTM) technology was under development by Helicos (now bankrupt). DRSTM sequences RNA molecules directly in a massively-parallel manner without RNA conversion to cDNA or other biasing sample manipulations such as ligation and amplification.\n\nA variety of parameters are considered when designing and conducting RNA-Seq experiments:\n\n\nTwo methods are used to assign raw sequence reads to genomic features (i.e., assemble the transcriptome):\n\n\n\n\"A note on assembly quality:\" The current consensus is that 1) assembly quality can vary depending on which metric is used, 2) assemblies that scored well in one species do not necessarily perform well in the other species, and 3) combining different approaches might be the most reliable.\n\nExpression is quantified to study cellular changes in response to external stimuli, differences between healthy and diseased states, and other research questions. Gene expression is often used as a proxy for protein abundance, but these are often not equivalent due to post transcriptional events such as RNA interference and nonsense-mediated decay.\n\nExpression is quantified by counting the number of reads that mapped to each locus in the transcriptome assembly step. Expression can be quantified for exons or genes using contigs or reference transcript annotations. These observed RNA-Seq read counts have been robustly validated against older technologies, including expression microarrays and qPCR. Tools that quantify counts are HTSeq, FeatureCounts, Rcount, maxcounts, FIXSEQ, and Cuffquant. The read counts are then converted into appropriate metrics for hypothesis testing, regressions, and other analyses. Parameters for this conversion are:\n\nRNA-Seq is generally used to compare gene expression between conditions, such as a drug treatment \"vs\" non-treated, and find out which genes are up- or down-regulated in each condition. In principle, RNA-Seq will make it possible to account for all the transcripts in the cell for each condition. Differently expressed genes can be identified using tools that count the sequencing reads per gene and compare them between samples. Many packages are available for this type of analysis; some of the most commonly used tools are DESeq and edgeR, packages from Bioconductor. Both these tools use a model based on the negative binomial distribution.\n\nIt is not possible to do absolute quantification using the common RNA-Seq pipeline, because it only provides RNA levels relative to all transcripts. If the total amount of RNA in the cell changes between conditions, relative normalization will misrepresent the changes for individual transcripts. Absolute quantification of mRNAs is possible by performing RNA-Seq with added spike ins, samples of RNA at known concentrations. After sequencing, the read count of the spike ins sequences is used to determine the direct correspondence between read count and biological fragments. In developmental studies, this technique has been used in \"Xenopus tropicalis\" embryos at a high temporal resolution, to determine transcription kinetics.\n\nCoexpression networks are data-derived representations of genes behaving in a similar way across tissues and experimental conditions. Their main purpose lies in hypothesis generation and guilt-by-association approaches for inferring functions of previously unknown genes. RNASeq data has been recently used to infer genes involved in specific pathways based on Pearson correlation, both in plants and mammals. The main advantage of RNASeq data in this kind of analysis over the microarray platforms is the capability to cover the entire transcriptome, therefore allowing the possibility to unravel more complete representations of the gene regulatory networks. Differential regulation of the splice isoforms of the same gene can be detected and used to predict and their biological functions. \nWeighted gene co-expression network analysis has been successfully used to identify co-expression modules and intramodular hub genes based on RNA seq data. Co-expression modules may corresponds to cell types or pathways. Highly connected intramodular hubs can be interpreted as representatives of their respective module. Variance-Stabilizing Transformation approaches for estimating correlation coefficients based on RNA seq data have been proposed.\n\nTranscriptome single nucleotide variation has been analyzed in maize on the Roche 454 sequencing platform. Directly from the transcriptome analysis, around 7000 single nucleotide polymorphisms (SNPs) were recognized. Following Sanger sequence validation, the researchers were able to conservatively obtain almost 5000 valid SNPs covering more than 2400 maize genes. RNA-seq is limited to transcribed regions however, since it will only discover sequence variations in exon regions. This misses many subtle but important intron alleles that affect disease such as transcription regulators, leaving analysis to only large effectors. While some correlation exists between exon to intron variation, only whole genome sequencing would be able to capture the source of all relevant SNPs.\n\nThe only way to be absolutely sure of the individual's mutations is to compare the transcriptome sequences to the germline DNA sequence. This enables the distinction of homozygous genes versus skewed expression of one of the alleles and it can also provide information about genes that were not expressed in the transcriptomic experiment. An R-based statistical package known as CummeRbund can be used to generate expression comparison charts for visual analysis.\n\nHaving the matching genomic and transcriptomic sequences of an individual can also help in detecting post-transcriptional edits, where, if the individual is homozygous for a gene, but the gene's transcript has a different allele, then a post-transcriptional modification event is determined.\n\nmRNA centric single nucleotide variants (SNVs) are generally not considered as a representative source of functional variation in cells, mainly due to the fact that these mutations disappear with the mRNA molecule, however the fact that efficient DNA correction mechanisms do not apply to RNA molecules can cause them to appear more often. This has been proposed as the source of certain prion diseases, also known as TSE or transmissible spongiform encephalopathies.\n\nCaused by different structural modifications in the genome, fusion genes have gained attention because of their relationship with cancer. The ability of RNA-seq to analyze a sample's whole transcriptome in an unbiased fashion makes it an attractive tool to find these kinds of common events in cancer.\n\nThe idea follows from the process of aligning the short transcriptomic reads to a reference genome. Most of the short reads will fall within one complete exon, and a smaller but still large set would be expected to map to known exon-exon junctions. The remaining unmapped short reads would then be further analyzed to determine whether they match an exon-exon junction where the exons come from different genes. This would be evidence of a possible fusion event, however, because of the length of the reads, this could prove to be very noisy. An alternative approach is to use pair-end reads, when a potentially large number of paired reads would map each end to a different exon, giving better coverage of these events (see figure). Nonetheless, the end result consists of multiple and potentially novel combinations of genes providing an ideal starting point for further validation.\n\nThe past five years have seen a flourishing of NGS-based methods for genome analysis leading to the discovery of a number of new mutations and fusion transcripts in cancer. RNA-Seq data could help researchers interpreting the \"personalized transcriptome\" so that it will help understanding the transcriptomic changes happening therefore, ideally, identifying gene drivers for a disease. The feasibility of this approach is however dictated by the costs in terms of money and time.\n\nA basic search on PubMed reveals that the term RNA Seq, queried as \"\"RNA Seq\" OR \"RNA-Seq\" OR \"RNA sequencing\" OR \"RNASeq\"\" in order to capture the most common ways of phrasing it, gives 5,425 hits demonstrating usage statistics of this technology. A few examples will be taken into consideration to explain that RNA-Seq applications to the clinic have the potentials to significantly affect patient's life and, on the other hand, requires a team of specialists (bioinformaticians, physicians/clinicians, basic researchers, technicians) to fully interpret the huge amount of data generated by this analysis.\n\nAs an example of clinical applications, researchers at the Mayo Clinic used an RNA-Seq approach to identify differentially expressed transcripts between oral cancer and normal tissue samples. They also accurately evaluated the allelic imbalance (AI), ratio of the transcripts produced by the single alleles, within a subgroup of genes involved in cell differentiation, adhesion, cell motility and muscle contraction identifying a unique transcriptomic and genomic signature in oral cancer patients. Novel insight on skin cancer (melanoma) also come from RNA-Seq of melanoma patients. This approach led to the identification of eleven novel gene fusion transcripts originated from previously unknown chromosomal rearrangements. Twelve novel chimeric transcripts were also reported, including seven of those that confirmed previously identified data in multiple melanoma samples. Furthermore, this approach is not limited to cancer patients. RNA-Seq has been used to study other important chronic diseases such as Alzheimer (AD) and diabetes. In the former case, Twine and colleagues compared the transcriptome of different lobes of deceased AD's patient's brain with the brain of healthy individuals identifying a lower number of splice variants in AD's patients and differential promoter usage of the APOE-001 and -002 isoforms in AD's brains. In the latter case, different groups showed the unicity of the beta-cells transcriptome in diabetic patients in terms of transcripts accumulation and differential promoter usage and long non coding RNAs (lncRNAs) signature.\n\nCompared with microarrays, NGS technology has identified novel and low frequency RNAs associated with disease processes. This advantage aids in the diagnosis and possible future treatments of diseases, including cancer. For example, NGS technology identified several previously undocumented differentially-expressed transcripts in rats treated with AFB1, a potent hepatocarcinogen. Nearly 50 new differentially-expressed transcriptions were identified between the controls and AFB1-treated rats. Additionally potential new exons were identified, including some that are responsive to AFB1. The next-generation sequencing pipeline identified more differential gene expressions compared with microarrays, particularly when DESeq software was utilized. Cufflinks identified two novel transcripts that were not previously annotated in the Ensembl database; these transcripts were confirmed using cloning PCR. Numerous other studies have demonstrated NGS's ability to detect aberrant mRNA and small non-coding RNA expression in disease processes above that provided by microarrays. The lower cost and higher throughput offered by NGS confers another advantage to researchers.\n\nThe role of small non-coding RNAs in disease processes has also been explored in recent years. For example, Han et al. (2011) examined microRNA expression differences in bladder cancer patients in order to understand how changes and dysregulation in microRNA can influence mRNA expression and function. Several microRNAs were differentially expressed in the bladder cancer patients. Upregulation in the aberrant microRNAs was more common than downregulation in the cancer patients. One of the upregulated microRNAs, has-miR-96, has been associated with carcinogenesis, and several of the overexpressed microRNAs have also been observed in other cancers, including ovarian and cervical. Some of the downregulated microRNAs in cancer samples were hypothesized to have inhibitory roles.\n\nA lot of emphasis has been given to RNA-Seq data after the Encyclopedia of DNA Elements (ENCODE) and The Cancer Genome Atlas (TCGA) projects have used this approach to characterize dozens of cell lines and thousands of primary tumor samples, respectively. ENCODE aimed to identify genome-wide regulatory regions in different cohort of cell lines and transcriptomic data are paramount in order to understand the downstream effect of those epigenetic and genetic regulatory layers. TCGA, instead, aimed to collect and analyze thousands of patient's samples from 30 different tumor types in order to understand the underlying mechanisms of malignant transformation and progression. In this context RNA-Seq data provide a unique snapshot of the transcriptomic status of the disease and look at an unbiased population of transcripts that allows the identification of novel transcripts, fusion transcripts and non-coding RNAs that could be undetected with different technologies.\n\n\n", "id": "21731590", "title": "RNA-Seq"}
{"url": "https://en.wikipedia.org/wiki?curid=31428932", "text": "Relative fluorescence units\n\nThe terms \"relative fluorescence units\" (RFU) and \"RFU peak\" refer to measurements in electrophoresis methods, such as for DNA analysis. A \"relative fluorescence unit\" is a unit of measurement used in analysis which employs fluorescence detection. Fluorescence is detected using a charged coupled device (CCD) array, when the labeled fragments, which are separated within a capillary by using electrophoresis, are energized by laser light and travel across the detection window. A computer program measures the results, determining the quantity or size of the fragments, at each data point, from the level of fluorescence intensity. Samples which contain higher quantities of amplified DNA will have higher corresponding RFU values.\n\nAn \"RFU peak\" is a relative maximum point along a graph of the analyzed data. The data can be normalized to DNA input or additional normalizing genes. The RFU heights can range from 0 to several thousands.\n\nThe RFU measurements are used, for DNA profiling, in a real-time polymerase chain reaction (PCR). Two common methods for detection of products in real-time PCR are: (1) non-specific fluorescent dyes that intercalate with any double-stranded DNA, and (2) sequence-specific DNA probes consisting of oligonucleotides that are labeled with a fluorescent reporter which permits detection only after hybridization of the probe with its complementary DNA target. Frequently, real-time PCR is combined with reverse transcription to quantify messenger RNA and Non-coding RNA in cells or tissues.\n\nThe RFU peak height depends on the amount of DNA being analyzed. When the amount of DNA is very low, then\nit can be difficult to separate a true low-level RFU peak from signal noise or other technical artifacts. As a result, many forensic DNA laboratories have set minimum RFU peak-height levels in \"scoring\" the analysis of alleles.\n\nThere are no firm industry-wide rules for establishing minimum RFU threshold values. Each laboratory, in general, has established its own threshold levels as one aspect of its particular validation procedure. Many laboratories have established both lower and upper thresholds for data interpretation, as a window of minimum and maximum readings.\n\nSome threshold levels can be derived experimentally based on the equipment's known signal-to-noise ratios, or a threshold can be defined to match published data or the manufacturer specifications. The company which sells the most widely used equipment for STR typing, Applied Biosystems, Inc. (ABI), has recommended a peak-height minimum of 150 RFU, advising how peaks below that level should be judged with caution. However, many forensic laboratories which have ABI systems have defined lower thresholds, often only 50 to 100 RFU, as determined by their own studies.\n\nMany different factors can affect a laboratory's choice of thresholds. For instance, there might be regulatory guidelines in specific jurisdictions. Also, different kinds of instruments vary in sensitivity (such as slab gel instruments being less sensitive than capillary electrophoresis (CE) instruments). Individual instruments, of a particular model type, have also been known to differ in performance (e.g. differences among various ABI 310 units, all of the same model). Capillary electrophoresis instruments generally provide better resolution compared gel-based systems, as well having better sensitivity. In addition, some laboratories have set different threshold standards depending on which instruments in the lab are used for an analysis.\n\nSetting an upper maximum threshold is critical when analyzing DNA data within high quantity samples. Samples with large amounts of amplified DNA will report high RFU levels that might oversaturate an instrument's sensitivity to measure the results. In such cases, an accurate measurement of the relative peak heights and/or areas might be unattainable. Oversaturation can be a problem when analyzing mixed samples.\n", "id": "31428932", "title": "Relative fluorescence units"}
{"url": "https://en.wikipedia.org/wiki?curid=31548061", "text": "Affinity magnetic separation\n\nAffinitymagnetic separation (AMS) is a laboratory tool that can efficiently isolate bacterial cells out of body fluid or cultured cells. It can also be used as a method of quantifying the pathogenicity of food, blood or feces. Another laboratory separation tool is the immunomagnetic separation (IMS), which is more suitable for the isolation of eucaryotic cells.\n\nHost recognition of bacteriophages occur via bacteria-binding proteins that have strong binding affinities to specific protein or carbohydrate structures on the surface of the bacterial host. Bacteria-binding proteins derived from bacteriophage coating paramagnetic beads will bind to specific cell components present on the surface of host thus \"capturing\" the cells and facilitate the concentration of these bead-attached cells. The concentration process is created by a magnet placed on the side of the test tube bringing the beads to it. Due to the phage-ligand technology, AMS is superior to the antibody based immunomagnetic separation (IMS) on sorting bacterial cells.\n", "id": "31548061", "title": "Affinity magnetic separation"}
{"url": "https://en.wikipedia.org/wiki?curid=31541379", "text": "Expected progeny difference\n\nExpected progeny differences (EPD) are an evaluation of an animal’s genetic worth as a parent. They are based on animal models which combine all information known about an individual and its relatives to create a genetic profile of the animal’s merits. These profiles are then compared only to other individuals of the same breed.\n\nAn example of a set of EPDs looks like the following chart. Each set of letters stands for a specific measurement with an accuracy reading and percent rank below it. Each EPD is compared to the breed average of a given year. The number given by the EPD is the amount above or below this given average.\nGrowth EPDs measure the amount of weight a given offspring will gain due to the parent’s genetics.\n\nCalving ease predicts the level of difficulty first time heifers will have during birth. These are determined by the percentage of unassisted births for that particular animal.\n\nBirth weight measures how much above or below the breed’s average an offspring will gain due to the parent. It does not necessarily predict the exact weight of all offspring, but instead gives a general prediction of how much extra or less weight an offspring will weigh compared to if it had been sired by another bull. High birth weight is the biggest cause of difficulty in calving, so having a bull with a low birth weight EPD is high beneficial.\n\nWeaning and yearling weight measure the amount of weight an offspring has gained by the time it is weaned and at the one year mark. Typically the weaning weight is measured at the 205-day mark and the yearling weight is taken at the 365-day mark. Typically a larger number is favored for both of these traits.\n\nMilk EPDs give an estimate for the maternal portion of the weaning weight that determined by milk production of the dam. It is measured in pounds of weaning weight of a bull’s grandprogeny due to the milk production of the bull’s daughters.\n\nScrotal circumference is an indicator of a bull’s fertility. A larger circumference is preferred and is an indicator for his sons to have a larger scrotal circumference and his daughters to reach puberty sooner and therefore have calves sooner.\n\nGestation length is an indicator of the probability of dystocia. The longer a calf is in utero the larger it will be at birth and the greater the chance of it having dystocia. It also gives the cow a larger postpartum interval between pregnancies. A shorter gestation length is usually preferred because of this.\n\nStayability is an indicator EPD of longevity of a bull’s daughter in a cow herd. The higher the EPD value the greater chance a cow will stay in a herd over six years and continue producing quality offspring.\n\nCarcass weight predicts what an animal’s total retail product will be compared to other animals of that breed. It does not however predict percent retail value, or the actual amount of sellable meat that can be produced from the carcass.\n\nFat thickness determined the expected external and seam fat the animal will contain. These two factors contribute to the greatest waste in an animal and best way to reduce economic loss. Fat thickness EPDs can help producers reduce this loss, by using animals with mid range EPDs, so as not to have too little or too much fat.\n\nMarbling EPDs are also important in the beef industry for predicting palatability in a beef carcass. They show the estimated USDA Quality grading System and marbling score an animal would receive if it were slaughtered. This EDP is different in that it is measured in units of marbling score, and not in weight gained by an animal. Higher values indicate the presence of genes that will produce more intramuscular fat.\n\nEPDs are used in both scientific research and in typical farm usage. Through mathematical equations and computing power EPDs can be generated for use in both situations. Through EPDs certain phenotypes can be chosen for over others. Use of EPSs and DNA markers can help when choosing certain traits over others. By using certain EPDs of an animal one can rapidly improve genetics of a herd, especially for highly heritable traits such as marbling. In addition, by using a combination of EPDs and DNA markers, EPDs can be made more accurate. When new DNA markers are used, they can help adjust the EPD according to the genotype and therefore produce a more accurate measurement.\n\nTo determine how accurate EPDs are for an individual, samples of all of a bull’s offspring are looked at and compared to what their expected outcome should have been. The EPDs can then be changed based on the values that are gathered. The higher the accuracy rate, the closer most of the progeny will be to the EPD values listed. Accuracy is not an indicator of beneficial EPDs, but rather shows how close the EPD is to the true genetic potential of the animal. The closer an accuracy value is to 1, the more accurate the EPDs can be thought to be. Other factors can affect the progeny as well, such as non genetic effects including feed, weather, environment, as well as random Mendelian sampling.\n\nIn addition to the EPD and accuracy shown in a chart, often the percent rank is also given. This shows what percentile the animal ranks for the given EPD. The higher the percentile, the better the EPD is for that characteristic. The better the EPDs for a given bull, the higher chance its progeny will have a given characteristic.\n", "id": "31541379", "title": "Expected progeny difference"}
{"url": "https://en.wikipedia.org/wiki?curid=31629357", "text": "Phage-ligand technology\n\nThe Phage-ligand technology is a technology to detect, bind and remove bacteria and bacterial toxins by using highly specific bacteriophage derived proteins.\n\nThe host recognition of bacteriophages occur via bacteria-binding proteins that have strong binding affinities to specific protein or carbohydrate structures on the surface of the bacterial host. At the end of the infection life cycle the bacteria-lysing Endolysin is synthesized and degrades the bacterial peptidoglycan cell wall, resulting in lysis (and therefore killing) of the bacterial cell.\n\nBacteriophage derived proteins are used for detection and removal of bacteria and bacterial components (especially endotoxin contaminations) in pharmaceutical and biological products, human diagnostics, food, and decolonization of bacteria causing nosocomial infections (e.g. MRSA). \nProtein modifications allow the biotechnological adaption to specific requirements.\n\nAffinity magnetic separation\n", "id": "31629357", "title": "Phage-ligand technology"}
{"url": "https://en.wikipedia.org/wiki?curid=31216882", "text": "Insulin signal transduction pathway\n\nThe insulin transduction pathway is a biochemical pathway by which insulin increases the uptake of glucose into fat and muscle cells and reduces the synthesis of glucose in the liver and hence is involved in maintaining glucose homeostasis. This pathway is also influenced by fed versus fasting states, stress levels, and a variety of other hormones.\n\nWhen carbohydrates are consumed, digested, and absorbed the pancreas senses the subsequent rise in blood glucose concentration and releases insulin to promote an uptake of glucose from the blood stream. When insulin binds to the insulin receptor, it leads to a cascade of cellular processes that promote the usage or, in some cases, the storage of glucose in the cell. The effects of insulin vary depending on the tissue involved, e.g., insulin is most important in the uptake of glucose by muscle and adipose tissue.\n\nThis insulin signal transduction pathway is composed of trigger mechanisms (e.g., autophosphorylation mechanisms) that serve as signals throughout the cell. There is also a counter mechanism in the body to stop the secretion of insulin beyond a certain limit. Namely, those counter-regulatory mechanisms are glucagon and epinephrine. The process of the regulation of blood glucose (also known as glucose homeostasis) also exhibits oscillatory behavior.\n\nOn a pathological basis, this topic is crucial to understanding certain disorders in the body such as diabetes, hyperglycemia and hypoglycemia.\n\nThe functioning of a signal transduction pathway is based on extra-cellular signaling that in turn creates a response which causes other subsequent responses, hence creating a chain reaction, or cascade. During the course of signaling, the cell uses each response for accomplishing some kind of a purpose along the way. Insulin secretion mechanism is a common example of signal transduction pathway mechanism.\n\nInsulin is produced by the pancreas in a region called Islets of Langerhans. In the islets of Langerhans, there are beta-cells, which are responsible for production and storage of insulin. Insulin is secreted as a response mechanism for counteracting the increasing excess amounts of glucose in the blood.\n\nGlucose in the body increases after food consumption. This is primarily due to carbohydrate intake, but to a much lesser degree protein intake ()(). Depending on the tissue type, the glucose enters the cell through facilitated or passive diffusion. In muscle and adipose tissue, glucose enters through GLUT 4 receptors via facilitated diffusion (). In brain, kidney and retina, glucose enters passively. In the beta-cells of the pancreas, glucose enters through the GLUT 2 receptors (process described below).\n\nInsulin biosynthesis is regulated by transcriptional and translational levels. The β-cells promote their protein transcription in response to nutrients. The exposure of rat Langerhans islets to glucose for 1 hour is able to remarkably induce the intracellular proinsulin levels. It was noted that the proinsulin mRNA remained stable. This suggests that the acute response to glucose of the insulin synthesis is independent of mRNA synthesis in the first 45 minutes, because the blockage of the transcription decelerated the insulin accumulation during that time . \nPTBPs, also called Polypyrimidine tract binding proteins, are proteins that regulate the translation of mRNA. They increase the viability of mRNA and provoke the initiation of the translation. PTBP1 enable the insulin gene specific activation and insulin granule protein mRNA by glucose . \n\nTwo aspects of the transduction pathway process are explained below: insulin secretion and insulin action on the cell.\n\nThe glucose that goes in the bloodstream after food consumption also enters the beta cells in the Islets of Langerhans in the pancreas. The glucose passively diffuses in the beta cell through a GLUT-2 vesicle. Inside the beta cell, the following process occurs:\n\nGlucose gets converted to Glucose-6-Phosphate (G6P) through Glucokinase; and G6P is subsequently oxidized to form ATP. This process inhibits the ATP-sensitive potassium ion channels of the cell causing the Potassium ion channel to close and not function anymore. The closure of the ATP-sensitive potassium channels causes depolarization of the cell membrane causing the cell membrane to stretch which causes the voltage-gated calcium channel on the membrane to open causing an influx of Ca2+ ions.\nThis influx then stimulates fusion of the insulin vesicles to the cell membrane and secretion of insulin in the extracellular fluid outside the beta cell; thus making it enter the bloodstream. [Also Illustrated in Figure 1.1.1].\n\nThere are 3 subfamilies of Ca+2 channels; L-type Ca+2 channels, Non-L-Type Ca+2 channels (including R-Type) and the T-type Ca+2 channels. There are two phases of the insulin secretion, the first phase involves the L-type Ca+2 channels and the second phase involves the R-type Ca+2 channels. The Ca+2 influx generated by R-type Ca+2 channels is not enough to cause insulin exocytosis, however, it increases the mobilization of the vesicles towards the cell membrane .\n\nFatty acids also affect insulin secretion. In type 2 diabetes, fatty acids are able to potentiate insulin release to compensate the increment need of insulin. It was found that the β-cells express free fatty acid receptors at their surface, through which fatty acids can impact the function of β-cells. Long-chain acyl-CoA and DAG are the metabolites resulting from the intracellular metabolism of fatty acids. Long-chain acyl-CoA has the ability to acylate proteins that are essential in the insulin granule fusion. On the other hand, DAG activates PKC that is involved in the insulin secretion . \n\nSeveral hormones can affect the insulin secretion. Estrogen is correlated with an increase of insulin secretion by depolarizing the β-cells membrane and enhancing the entry of Ca+2. In contrast, growth hormone is known to lower the serum level of insulin by promoting the production of insulin-like growth factor-I (IGF-I). IGF-I in turn, suppresses the insulin secretion .\n\nAfter insulin enters the bloodstream, it binds to a membrane-spanning glycoprotein receptor. This glycoprotein is embedded in the cellular membrane and has an extracellular receptor domain, made up of two α-subunits, and an intracellular catalytic domain, made up of two β-subunits. The α-subunits act as insulin receptors and the insulin molecule acts as a ligand. Together, they form a receptor-ligand complex.\n\nBinding of insulin to the α-subunit results in a conformational change in the membrane-bound glycoprotein, which activates tyrosine kinase domains on each β-subunit. The tyrosine kinase activity causes an auto phosphorylation of several tyrosine residues in the β-subunit. The phosphorylation of 3 residues of tyrosine is necessary for the amplification of the kinase activity . \n\nOnce the tyrosine kinase is activated in the insulin receptor, it triggers the activation of the docking proteins, also called IRS (1-4) that are important in the signaling pathway, and then the activation of the PI-3k \nThe two enzymes Mitogen-activated Protein Kinase (MAP-Kinase) and Phosphatidylinositol-3-Kinase (PI-3K, Phosphoinositide 3-kinase) are responsible for expressing the mitogenic and metabolic actions of Insulin, respectively.\n\nThe activation of MAP-Kinase leads to completion of mitogenic functions like cell growth and gene expression.\n\nThe activation of PI-3K leads to crucial metabolic functions such as synthesis of lipids, proteins and glycogen. It also leads to cell survival and cell proliferation. Most importantly, the PI-3K pathway is responsible for the distribution of glucose for important cell functions.The activation of PI-3K leads to the activation of PKB (AKT) that induces the impact of insulin on the liver. For example, the suppression of hepatic glucose synthesis and the activation glycogen synthesis. Hence, PKB possesses a crucial role in the linkage of the glucose transporter (GLUT4) to the insulin signaling pathway. The activated GLUT4 will translocate to the cell membrane and promotes the transportation of glucose into the intracellular medium .\n\nThus, insulin’s role is more of a promoter for the usage of glucose in the cells rather than neutralizing or counteracting it.\n\nPI-3K is one of the important components in the regulation of the insulin signaling pathway. It maintains the insulin sensitivity in the liver. PI-3K is composed of a regulatory subunit (P85) and a catalytic subunit (P110). P85 regulates the activation of PI-3K enzyme . In the PI-3K heterodimer (P85-p110), P85 is responsible for the PI-3K activity, by binding to the binding site on the insulin receptor substrates (IRS). It was noted that an increase of P85 a (isoform of P85) results in a competition between the later and the P85-P110 complex to the IRS binding site, reducing the PI-3k activity and leading to insulin resistance. Insulin resistance refers also to Type 2 diabetes. \nIt was also noted that increased serine phosphorylation of IRS is involved in the insulin resistance by reducing their ability to attract PI3K. The serine phosphorylation can also lead to degradation of IRS-1 .\n\nSignal transduction is a mechanism in which the cell responds to a signal from the environment by activating several proteins and enzymes that will give a response to the signal.\nFeedback mechanism might involve negative and positive feedbacks. In the negative feedback, the pathway is inhibited and the final result of the transduction pathway is reduced or limited. In positive feedback, the transduction pathway is promoted and stimulated to produce more products.\n\nInsulin secretion results in positive feedback in different ways. Firstly, insulin increases the uptake of glucose from blood by the translocation and exocytosis of GLUT4 storage vesicles in the muscle and fat cells. Secondly, it promotes the conversion of glucose into triglyceride in the liver, fat, and muscle cells. Finally, the cell will increase the rate of glycolysis within itself to break glucose in the cell into other components for tissue growth purposes.\n\nAn example of positive feedback mechanism in the insulin transduction pathway is the activation of some enzymes that inhibit other enzymes from slowing or stopping the insulin transduction pathway which results in improved intake of the glucose.\n\nOne of these pathways, involves the PI(3)K enzyme (Phosphoinositide 3-kinase). This pathway is responsible for activating glycogen, lipid-protein synthesis, and specific gene expression of some proteins which will help in the intake of glucose.\nDifferent enzymes control this pathway. Some of these enzymes constrict the pathway causing a negative feedback like the GSK-3 pathway. Other enzymes will push the pathway forward causing a positive feedback like the AKT and P70 enzymes.\nWhen insulin binds to its receptor, it activates the glycogen synthesis by inhibiting the enzymes that slow down the PI(3)K pathway such as PKA enzyme. At the same time, it will promote the function of the enzymes that provide a positive feedback for the pathway like the AKT and P70 enzymes. The inactivation of the enzymes that stop the reaction and activating of enzymes that provide a positive feedback will increase glycogen, lipid & protein syntheses and promote glucose intake.\n\nWhen insulin binds to the cell's receptor, it results in negative feedback by limiting or stopping some other actions in the cell. It inhibits the release and production of glucose from the cells which is an important part in reducing the glucose blood level. Insulin will also inhibit the breakdown of glycogen into glucose by inhibiting the expression of the enzymes that catalyzes the degradation of Glycogen.\n\nAn example of negative feedback is slowing or stopping the intake of glucose after the pathway was activated. Negative feedback is shown in the insulin signal transduction pathway by constricting the phosphorylation of the insulin-stimulated tyrosine. The enzyme that deactivates or phosphorylates the insulin-stimulated tyrosine is called tyrosine phosphatases (PTPases). When activated, this enzyme provides a negative feedback by catalyzing the dephosphorylation of the insulin receptors. The dephosphorylation of the insulin receptor slows down glucose intake by inhibiting the activation (phosphorylation) of proteins responsible for further steps of the insulin transduction pathway.\n\nInsulin is synthesized and secreted in the beta cells of the islets of Langerhans. Once insulin is synthesized, the beta cells are ready to release it in two different phases. As for the first phase, insulin release is triggered rapidly when the blood glucose level is increased. The second phase is a slow release of newly formed vesicles that are triggered regardless of the blood sugar level.\nGlucose enters the beta cells and goes through glycolysis to form ATP that eventually causes depolarization of the beta cell membrane (as explained in Insulin secretion section of this article). The depolarization process causes voltage-controlled calcium channels (Ca2+) opening, allowing the calcium to flow into the cells. An increased calcium level activates phospholipase C, which cleaves the membrane phospholipid phosphtidylinositol 4,5-bisphosphate into Inositol 1,4,5-triphosphate (IP3) and diacylglycerol (DAG). IP3 binds to receptor proteins in the membrane of endplasmic reticulum (ER). This releases (Ca2+) from the ER via IP3 gated channels, and raises the cell concentration of calcium even more. The influx of Ca2+ ions causes the secretion of insulin stored in vesicles through the cell membrane. The process of insulin secretion is an example of a trigger mechanism in a signal transduction pathway, because insulin is secreted after glucose enters the beta cell and that triggers several other processes in a chain reaction.\n\nWhile insulin is secreted by the pancreas to lower blood glucose levels, glucagon is secreted to raise blood glucose levels. This is why glucagon has been known for decades as a counter-regulatory hormone. When blood glucose levels are low, the pancreas secretes glucagon, which in turn causes the liver to convert stored glycogen polymers into glucose monomers, which is then released into the blood. This process is called glycogenolysis. Liver cells, or hepatocytes, have glucagon receptors which allow for glucagon to attach to them and thus stimulate glycogenolysis. Contrary to insulin, which is produced by pancreatic β-cells, glucagon is produced by pancreatic α-cells. It is also known that an increase in insulin suppresses glucagon secretion, and a decrease in insulin, along with low glucose levels, stimulates the secretion of glucagon.\n\nWhen blood-glucose levels are too low, the pancreas is signaled to release glucagon, which has essentially the opposite effect of insulin and therefore opposes the reduction of glucose in the blood. Glucagon is delivered directly to the liver, where it connects to the glucagon receptors on the membranes of the liver cells, signals the conversion of the glycogen already stored in the liver cells into glucose. This process is called glycogenolysis.\n\nConversely, when the blood-glucose levels are too high, the pancreas is signaled to release insulin. Insulin is delivered to the liver and other tissues throughout the body (e.g., muscle, adipose). When the insulin is introduced to the liver, it connects to the insulin receptors already present, that is tyrosine kinase receptor. These receptors have two alpha subunits (extracellular) and two beta subunits (intercellular) which are connected through the cell membrane via disulfide bonds. When the insulin binds to these alpha subunits, 'glucose transport 4' (GLUT4) is released and transferred to the cell membrane to regulate glucose transport in and out of the cell. With the release of GLUT4, the allowance of glucose into cells is increased, and therefore the concentration of blood glucose might decrease. This, in other words, increases the utilization of the glucose already present in the liver. This is shown in the adjacent image. As glucose increases, production of insulin increases, which thereby increases the utilization of the glucose, which maintains the glucose levels in an efficient manner and creates an oscillatory behavior.\n", "id": "31216882", "title": "Insulin signal transduction pathway"}
{"url": "https://en.wikipedia.org/wiki?curid=21227807", "text": "DNA demethylation\n\nDNA demethylation is the process of removal of a methyl group from nucleotides in DNA. DNA demethylation could be passive or active. The passive process takes place in the absence of methylation of newly synthesised DNA strands by DNMT1 during several replication rounds (for example, upon 5-Azacytidine treatment). Active DNA demethylation occurs via direct removal of a methyl group independently of DNA replication.\n\nAll the cases of DNA demethylation can be divided as global (genome wide) or locus-specific (when just specific sequences are demethylated).\nThe genome-wide DNA demethylation occurs:\n\nExamples of specific DNA demethylation:\n\nThere are several proposed hypothetical mechanisms of active DNA demethylation:\n\nA Direct removal of 5-methylcytosine\n\nB Removal of 5-methylcytosine via further modified cytosine bases\n\nOxidation of the methyl group generates 5-Hydroxymethylcytosine. Several mechanisms have been proposed to mediate demethylation of 5-hydroxymethylcytosines. This base can be either deaminated by AID/Apobec enzymes to give 5-Hydroxymethyluracil. Alternatively, TET enzymes can further oxidize 5-hydroxymethylcytosine to 5-Formylcytosine and 5-Carboxylcytosine.\n\nDNA hydroxymethylation has been proposed to act as a specific epigenetic mark opposing DNA methylation, rather than a passive intermediate in the de-methylation pathway. DNA hydroxymethylation \"in vivo\" is sometimes associated with labile nucleosomes, which are more easy to disassemble and to out-compete by transcription factors during cell development Hydroxymethylation has been associated with pluripotency of stem cells. Further, Changes in hydroxymethylation have been associated with cancer.\n\n\n", "id": "21227807", "title": "DNA demethylation"}
{"url": "https://en.wikipedia.org/wiki?curid=513091", "text": "Single-nucleotide polymorphism\n\nA single-nucleotide polymorphism, often abbreviated to SNP (; plural ), is a variation in a single nucleotide that occurs at a specific position in the genome, where each variation is present to some appreciable degree within a population (e.g. > 1%).\n\nFor example, at a specific base position in the human genome, the C nucleotide may appear in most individuals, but in a minority of individuals, the position is occupied by an A. This means that there is an SNP at this specific position, and the two possible nucleotide variations – C or A – are said to be alleles for this position.\n\nSNPs underlie differences in our susceptibility to disease; a wide range of human diseases, e.g. sickle-cell anemia, β-thalassemia and cystic fibrosis result from SNPs. The severity of illness and the way our body responds to treatments are also manifestations of genetic variations. For example, a single-base mutation in the APOE (apolipoprotein E) gene is associated with a higher risk for Alzheimer's disease.\n\nA single-nucleotide variant (SNV) is a variation in a single nucleotide without any limitations of frequency and may arise in somatic cells. A somatic single-nucleotide variation (e.g., caused by cancer) may also be called a single-nucleotide alteration.\n\nSingle-nucleotide polymorphisms may fall within coding sequences of genes, non-coding regions of genes, or in the intergenic regions (regions between genes). SNPs within a coding sequence do not necessarily change the amino acid sequence of the protein that is produced, due to degeneracy of the genetic code.\n\nSNPs in the coding region are of two types: synonymous and nonsynonymous SNPs. Synonymous SNPs do not affect the protein sequence, while nonsynonymous SNPs change the amino acid sequence of protein. The nonsynonymous SNPs are of two types: missense and nonsense.\n\nSNPs that are not in protein-coding regions may still affect gene splicing, transcription factor binding, messenger RNA degradation, or the sequence of non-coding RNA. Gene expression affected by this type of SNP is referred to as an eSNP (expression SNP) and may be upstream or downstream from the gene.\n\n\nThe genomic distribution of SNPs is not homogenous; SNPs occur in non-coding regions more frequently than in coding regions or, in general, where natural selection is acting and \"fixing\" the allele (eliminating other variants) of the SNP that constitutes the most favorable genetic adaptation. Other factors, like genetic recombination and mutation rate, can also determine SNP density.\n\nSNP density can be predicted by the presence of microsatellites: AT microsatellites in particular are potent predictors of SNP density, with long (AT)(n) repeat tracts tending to be found in regions of significantly reduced SNP density and low GC content.\n\nThere are variations between human populations, so a SNP allele that is common in one geographical or ethnic group may be much rarer in another. Within a population, SNPs can be assigned a minor allele frequency — the lowest allele frequency at a locus that is observed in a particular population. This is simply the lesser of the two allele frequencies for single-nucleotide polymorphisms.\n\nVariations in the DNA sequences of humans can affect how humans develop diseases and respond to pathogens, chemicals, drugs, vaccines, and other agents. SNPs are also critical for personalized medicine.\n\nSNPs' greatest importance in biomedical research is for comparing regions of the genome between cohorts (such as with matched cohorts with and without a disease) in genome-wide association studies. SNPs have been used in genome-wide association studies as high-resolution markers in gene mapping related to diseases or normal traits. SNPs without an observable impact on the phenotype (so called silent mutations) are still useful as genetic markers in genome-wide association studies, because of their quantity and the stable inheritance over generations.\n\nSNPs were used initially for matching a forensic DNA sample to a suspect but it has been phased out with development of STR-based DNA fingerprinting techniques. In the future SNPs may be used in forensics for some phenotypic clues like eye color, hair color, ethnicity etc. Kidd et al. have demonstrated that a panel of 19 SNPs can identify the ethnic group with good probability of match (Pm = 10^-7) in 40 population groups studied. One example of how this might potentially be useful is in the area of artistic reconstruction of possible premortem appearances of skeletonized remains of unknown individuals. Although a facial reconstruction can be fairly accurate based strictly upon anthropological features, other data that might allow a more accurate representation include eye color, skin color, hair color, etc.\n\nIn a situation with a low amount of forensic sample or a degraded sample, SNP methods can be a good alternative to STR methods due to the abundance of potential markers, amenability to automation, and potential reduction of required fragment length to only 60-80 bp. In the absence of a STR match in DNA profile database; different SNPs can be used to get clues regarding ethnicity, phenotype, lineage, and even identity.\n\nSome SNPs are associated with the metabolism of different drugs. The association of a wide range of human diseases like cancer, infectious diseases (AIDS, leprosy, hepatitis, etc.) autoimmune, neuropsychiatric and many other diseases with different SNPs can be made as relevant pharmacogenomic targets for drug therapy.\n\nA single SNP may cause a Mendelian disease, though for complex diseases, SNPs do not usually function individually, rather, they work in coordination with other SNPs to manifest a disease condition as has been seen in Osteoporosis. One of the earliest successes in this field was finding a single base mutation in the non-coding region of the APOC3 (apolipoprotein C3 gene) that associated with  higher risks of hypertriglyceridemia and atherosclerosis (see David J. Galton Lancet 1983 pp444-446).\n\nAll types of SNPs can have an observable phenotype or can result in disease:\n\n\nAs there are for genes, bioinformatics databases exist for SNPs. \nThe International SNP Map working group mapped the sequence flanking each SNP by alignment to the genomic sequence of large-insert clones in Genebank. These alignments were converted to chromosomal coordinates that is shown in Table 1. \nThe nomenclature for SNPs can be confusing: several variations can exist for an individual SNP and consensus has not yet been achieved. One approach is to write SNPs with a prefix, period and \"greater than\" sign showing the wild-type and altered nucleotide or amino acid; for example, c.76A>T. SNPs are frequently referred to by their dbSNP rs number, as in the examples above.\n\nSNPs are usually biallelic and thus easily assayed. Analytical methods to discover novel SNPs and detect known SNPs include:\n\n\nAn important group of SNPs are those that corresponds to missense mutations causing amino acid change on protein level. Point mutation of particular residue can have different effect on protein function (from no effect to complete disruption its function). Usually, change in amino acids with similar size and physico-chemical properties (e.g. substitution from leucine to valine) has mild effect, and opposite. Similarly, if SNP disrupts secondary structure elements (e.g. substitution to proline in alpha helix region) such mutation usually may affect whole protein structure and function. Using those simple and many other machine learning derived rules a group of programs for the prediction of SNP effect was developed:\n\n\n", "id": "513091", "title": "Single-nucleotide polymorphism"}
{"url": "https://en.wikipedia.org/wiki?curid=32170572", "text": "BpuJI\n\nIn molecular biology, BpuJI is a type II restriction endonuclease which recognises the asymmetric sequence 5'-CCCGT and cuts at multiple sites in the surrounding area of the target sequence. The BpuJI protein consists of two distinct modules; an N-terminal DNA recognition domain, and a C-terminal dimerisation and catalysis domain. The N-terminal domain is composed of two winged-helix subdomains and a disrupted linker subdomain. Target sequence recognition occurs through major groove contacts of amino acids in the winged-helix subdomains.\n", "id": "32170572", "title": "BpuJI"}
{"url": "https://en.wikipedia.org/wiki?curid=32171476", "text": "Btk-type zinc finger\n\nIn molecular biology, the Btk-type zinc finger or Btk motif (BM) is a conserved zinc-binding motif containing conserved cysteines and a histidine that is present in certain eukaryotic signalling proteins. The motif is named after Bruton's tyrosine kinase (Btk), an enzyme which is essential for B cell maturation in humans and mice. Btk is a member of the Tec family of protein tyrosine kinases (PTK). These kinases contain a conserved Tec homology (TH) domain between the N-terminal pleckstrin homology (PH) domain and the Src homology 3 (SH3) domain. The N-terminal of the TH domain is highly conserved and known as the Btf motif, while the C-terminal region of the TH domain contains a proline-rich region (PRR). The Btk motif contains a conserved His and three Cys residues that form a zinc finger (although these differ from known zinc finger topologies), while PRRs are commonly involved in protein-protein interactions, including interactions with G proteins. The TH domain may be of functional importance in various signalling pathways in different species. A complete TH domain, containing both the Btk and PRR regions, has not been found outside the Tec family; however, the Btk motif on its own does occur in other proteins, usually C-terminal to a PH domain (note that although a Btk motif always occurs C-terminal to a PH domain, not all PH domains are followed by a Btk motif).\n\nThe crystal structures of Btk show that the Btk-type zinc finger has a globular core, formed by a long loop which is held together by a zinc ion, and that the Btk motif is packed against the PH domain. The zinc-binding residues are a histidine and three cysteines, which are fully conserved in the Btk motif.\n\nProteins known to contain a Btk-type zinc finger include:\n\n", "id": "32171476", "title": "Btk-type zinc finger"}
{"url": "https://en.wikipedia.org/wiki?curid=619632", "text": "Transfection\n\nTransfection is the process of deliberately introducing naked or purified nucleic acids into eukaryotic cells. It may also refer to other methods and cell types, although other terms are often preferred: \"transformation\" is typically used to describe non-viral DNA transfer in bacteria and non-animal eukaryotic cells, including plant cells. In animal cells, transfection is the preferred term as transformation is also used to refer to progression to a cancerous state (carcinogenesis) in these cells. Transduction is often used to describe virus-mediated gene transfer into eukaryotic cells.\n\nThe word \"transfection\" is a blend of \"trans-\" and \"infection\". Genetic material (such as supercoiled plasmid DNA or siRNA constructs), or even proteins such as antibodies, may be transfected.\n\nTransfection of animal cells typically involves opening transient pores or \"holes\" in the cell membrane to allow the uptake of material. Transfection can be carried out using calcium phosphate (i.e. tricalcium phosphate), by electroporation, by cell squeezing or by mixing a cationic lipid with the material to produce liposomes which fuse with the cell membrane and deposit their cargo inside.\n\nTransfection can result in unexpected morphologies and abnormalities in target cells.\n\nThe meaning of the term has evolved. The original meaning of transfection was \"infection by transformation\", i.e., introduction of genetic material, DNA or RNA, from a prokaryote-infecting virus or bacteriophage into cells, resulting in an infection. Because the term transformation had another sense in animal cell biology (a genetic change allowing long-term propagation in culture, or acquisition of properties typical of cancer cells), the term transfection acquired, for animal cells, its present meaning of a change in cell properties caused by introduction of DNA.\n\nThere are various methods of introducing foreign DNA into a eukaryotic cell: some rely on physical treatment (electroporation, cell squeezing, nanoparticles, magnetofection); others rely on chemical materials or biological particles (viruses) that are used as carriers. Gene delivery is, for example, one of the steps necessary for gene therapy and the genetic modification of crops. There are many different methods of gene delivery developed for various types of cells and tissues, from bacterial to mammalian. Generally, the methods can be divided into two categories: non-viral and viral.\n\nNon-viral methods include physical methods such as electroporation, microinjection, gene gun, impalefection, hydrostatic pressure, continuous infusion, and sonication and chemical, such as lipofection, which is a lipid-mediated DNA-transfection process utilizing liposome vectors. It can also include the use of polymeric gene carriers (polyplexes).\n\nVirus mediated gene delivery utilizes the ability of a virus to inject its DNA inside a host cell. A gene that is intended for delivery is packaged into a replication-deficient viral particle. Viruses used to date include retrovirus, lentivirus, adenovirus, adeno-associated virus and herpes simplex virus. However, there are drawbacks to using viruses to deliver genes into cells. Viruses can only deliver very small pieces of DNA into the cells, it is labor-intensive and there are risks of random insertion sites, cytopathic effects and mutagenesis.\n\nChemical-based transfection can be divided into several kinds: cyclodextrin, polymers, liposomes, or nanoparticles (with or without chemical or viral functionalization. See below).\n\n\n\n\nOther methods of transfection include nucleofection, which has proved very efficient in transfection of the THP-1 cell line, creating a viable cell line that was able to be differentiated into mature macrophages, and heat shock.\n\nDNA can also be introduced into cells using viruses as a carrier. In such cases, the technique is called viral transduction, and the cells are said to be transduced. Adenoviral vectors can be useful for viral transfection methods because they can transfer genes into a wide variety of human cells and have high transfer rates. Lentiviral vectors are also helpful due to their ability to transduce cells not currently undergoing mitosis.\n\nStable and transient transfection differ in their long term effects on a cell; a stably-transfected cell will continuously express transfected DNA and pass it on to daughter cells, while a transiently-transfected cell will express transfected DNA for a short amount of time and not pass it on to daughter cells.\n\nFor some applications of transfection, it is sufficient if the transfected genetic material is only transiently expressed. Since the DNA introduced in the transfection process is usually not integrated into the nuclear genome, the foreign DNA will be diluted through mitosis or degraded. Cell lines expressing the Epstein–Barr virus (EBV) nuclear antigen 1 (EBNA1) or the SV40 large-T antigen, allow episomal amplification of plasmids containing the viral EBV (293E) or SV40 (293T) origins of replication, greatly reducing the rate of dilution.\n\nIf it is desired that the transfected gene actually remain in the genome of the cell and its daughter cells, a stable transfection must occur. To accomplish this, a marker gene is co-transfected, which gives the cell some selectable advantage, such as resistance towards a certain toxin. Some (very few) of the transfected cells will, by chance, have integrated the foreign genetic material into their genome. If the toxin is then added to the cell culture, only those few cells with the marker gene integrated into their genomes will be able to proliferate, while other cells will die. After applying this selective stress (selection pressure) for some time, only the cells with a stable transfection remain and can be cultivated further.\n\nCommon agents for selecting stable transfection are:\n\n\nRNA can also be transfected into cells to transiently express its coded protein, or to study RNA decay kinetics. RNA transfection is often used in primary cells that do not divide.\n\nsiRNAs can also be transfected to achieve RNA silencing (i.e. loss of RNA and protein from the targeted gene). This has become a major application in research to achieve \"knock-down\" of proteins of interests (e.g. Endothelin-1) with potential applications in gene therapy. Limitation of the silencing approach are the toxicity of the transfection for cells and potential \"off-target\" effects on the expression of other genes/proteins.\n\n\n\n", "id": "619632", "title": "Transfection"}
{"url": "https://en.wikipedia.org/wiki?curid=505525", "text": "DNase footprinting assay\n\nA DNase footprinting assay is a DNA footprinting technique from molecular biology/biochemistry that detects DNA-protein interaction using the fact that a protein bound to DNA will often protect that DNA from enzymatic cleavage. This makes it possible to locate a protein binding site on a particular DNA molecule. The method uses an enzyme, deoxyribonuclease (DNase, for short), to cut the radioactively end-labeled DNA, followed by gel electrophoresis to detect the resulting cleavage pattern.\n\nFor example, the DNA fragment of interest may be PCR amplified using a P 5' labeled primer, with the result being many DNA molecules with a radioactive label on one end of one strand of each double stranded molecule. Cleavage by DNase will produce fragments. The fragments which are smaller with respect to the P-labelled end will appear further on the gel than the longer fragments. The gel is then used to expose a special photographic film.\n\nThe cleavage pattern of the DNA in the absence of a DNA binding protein, typically referred to as free DNA, is compared to the cleavage pattern of DNA in the presence of a DNA binding protein. If the protein binds DNA, the binding site is protected from enzymatic cleavage. This protection will result in a clear area on the gel which is referred to as the \"footprint\".\n\nBy varying the concentration of the DNA-binding protein, the binding affinity of the protein can be estimated according to the minimum concentration of protein at which a footprint is observed.\n\nThis technique was developed by David Galas and Albert Schmitz at Geneva in 1977\n\n", "id": "505525", "title": "DNase footprinting assay"}
{"url": "https://en.wikipedia.org/wiki?curid=23278041", "text": "Optical transfection\n\nOptical transfection is the process of introducing nucleic acids into cells using light. Typically, a laser is focussed to a diffraction limited spot (~1 µm diameter) using a high numerical aperture microscope objective. The plasma membrane of a cell is then exposed to this highly focussed light for a small amount of time (typically tens of milliseconds to seconds), generating a transient pore on the membrane. The generation of a photopore allows exogenous plasmid DNA, RNA, organic fluorophores, or larger objects such as semiconductor quantum nanodots to enter the cell. In this technique, one cell at a time is treated, making it particularly useful for single cell analysis.\n\nTo put the above simply, cells do not usually allow certain types of substances into their interior space. Lasers can be used to burn a tiny hole on the cell surface, allowing those substances to enter. This is tremendously useful to biologists who are studying disease, as a common experimental requirement is to put things (such as DNA) into cells.\n\nThis technique was first demonstrated in 1984 by Tsukakoshi et al., who used a frequency tripled Nd:YAG to generate stable and transient transfection of normal rat kidney cells. Since this time, the optical transfection of a host of mammalian cell types has been demonstrated using a variety of laser sources, including the 405 nm continuous wave (cw), 488 nm cw, or pulsed sources such as the 800 nm femtosecond pulsed Ti:Sapphire or 1064 nm nanosecod pulsed Nd:YAG.\n\nThe meaning of the term transfection has evolved. The original meaning of transfection was \"infection by transformation\", \"i.e.\" introduction of DNA (or RNA) from a eukaryote-infecting virus or bacteriophage into cells, resulting in an infection. Because the term transformation had another sense in animal cell biology (a genetic change allowing long-term propagation in culture, or acquisition of properties typical of cancer cells), the term transfection acquired, for animal cells, its present meaning of a change in cell properties caused by introduction of DNA (or other nucleic acid species such as RNA or SiRNA).\n\nBecause of this strict definition of transfection, optical transfection also refers only to the introduction of nucleic acid species. The introduction of other impermeable compounds into a cell, such as organic fluorophores or semiconductor quantum nanodots is not strictly speaking \"transfection,\" and is therefore referred to as \"optical injection\" or one of the other many terms now outlined.\n\nThe lack of a unified name for this technology makes reviewing the literature on the subject very difficult. Optical injection has been described using over a dozen different names or phrases (see bulleted lists below). Some trends in the literature are clear. The first term of the technique is invariably a derivation of word laser, optical, or photo, and the second term is usually in reference to injection, transfection, poration, perforation or puncture. Like many cellular perturbations, when a single cell or group of cells is treated with a laser, three things can happen: the cell dies (overdose), the cell membrane is permeabilised, substances enter, and the cell recovers (therapeutic dose), or nothing happens (underdose). There have been suggestions in the literature to reserve the term optoinjection for when a therapeutic dose is delivered upon a single cell, and the term optoporation for when a laser generated shockwave treats a cluster of many (10s to 100s) cells. The first definition of optoinjection is uncontroversial. The definition of optoporation, however, has failed to be adopted, with a similar number of references using the term to denote the dosing of single cells as those using the term to denote the simultaneous dosing of clusters of many cells \nAs the field stands, it is the opinion of the authors of a review article on the subject that the term optoinjection always be included as a keyword in future publications, regardless of their own naming preferences.\n\nTerms agreed by consensus\n\n\nTerms under deliberation\n\nSome of the above was reproduced with permission from.\n\nA typical optical transfection protocol is as follows:\n1) Build an optical tweezers system with a high NA objective\n2) Culture cells to 50-60% confluency\n3) Expose cells to at least 10 µg/ml of plasmid DNA\n4) Dose the plasma membrane of each cell with 10-40 ms of focussed laser, at a power of <100 mW at focus\n5) Observe transient transfection 24-96h later\n6) Add selective medium if the generation of stable colonies is desired\n\n\n", "id": "23278041", "title": "Optical transfection"}
{"url": "https://en.wikipedia.org/wiki?curid=32571371", "text": "FAD dependent oxidoreductase family\n\nIn molecular biology, the FAD dependent oxidoreductase family of proteins is a family of FAD dependent oxidoreductases. Members of this family include Glycerol-3-phosphate dehydrogenase , Sarcosine oxidase beta subunit , D-amino-acid dehydrogenase , D-aspartate oxidase .\n\nD-amino acid oxidase (DAMOX or DAO) is an FAD flavoenzyme that catalyses the oxidation of neutral and basic D-amino acids into their corresponding keto acids. DAOs have been characterised and sequenced in fungi and vertebrates where they are known to be located in the peroxisomes.\n\nD-aspartate oxidase (DASOX) is an enzyme, structurally related to DAO, which catalyses the same reaction but is active only toward dicarboxylic D-amino acids. In DAO, a conserved histidine has been shown to be important for the enzyme's catalytic activity.\n\n", "id": "32571371", "title": "FAD dependent oxidoreductase family"}
{"url": "https://en.wikipedia.org/wiki?curid=32709889", "text": "Methylation induced premeiotically\n\nIn molecular biology, methylation induced premeiotically (MIP) is a process by which cytosines within repeated DNA sequences are \"de novo\" methylated prior to the sexual cycle. This process was first described in the ascomycete \"Ascobolus immersens\". MIP is dependent upon the gene \"masc1\" which encodes a cytosine methyltransferase-like protein.\nAt least one major function of the process appears to be genome defense. Related functions have been found in other fungi, including \"Neurospora\" and \"Aspergillus\" species.\n", "id": "32709889", "title": "Methylation induced premeiotically"}
{"url": "https://en.wikipedia.org/wiki?curid=32823089", "text": "Surround optical-fiber immunoassay\n\nSurround optical-fiber immunoassay (SOFIA) is an ultrasensitive, \"in vitro\" diagnostic platform incorporating a surround optical-fiber assembly that captures fluorescence emissions from an entire sample. The technology's defining characteristics are its extremely high limit of detection, sensitivity, and dynamic range. SOFIA’s sensitivity is measured at the attogram level (10 g), making it about one billion times more sensitive than conventional diagnostic techniques. Based on its enhanced dynamic range, SOFIA is able to discriminate levels of analyte in a sample over 10 orders of magnitude, facilitating accurate titering.\n\nAs a diagnostic platform, SOFIA has a broad range of applications. Several studies have already demonstrated SOFIA’s unprecedented ability to detect naturally occurring prions in the blood and urine of disease carriers. This is expected to lead to the first reliable \"ante mortem\" screening test for vCJD, BSE, scrapie, CWD, and other transmissible spongiform encephalopathies. Given the technology’s extreme sensitivity, additional unique applications are anticipated, including \"in vitro\" tests for other neurodegenerative diseases, such as Alzheimer's and Parkinson's disease.\n\nSOFIA was developed as a result of a joint-collaborative research project between Los Alamos National Laboratory and State University of New York, and was supported by the Department of Defense's National Prion Research Program.\n\nThe conventional method of performing laser-induced fluorescence, as well as other types of spectroscopic measurements, such as infrared, ultraviolet-visible spectroscopy, phosphorescence, etc., is to use a small transparent laboratory vessel, a cuvette, to contain the sample to be analyzed.\n\nTo perform a measurement, the cuvette is filled with the liquid to be investigated and then illuminated with a laser focused through one of the cuvette’s faces. A lens is placed in line with one of the faces of the cuvette located at 90° from the input window to collect the laser-induced fluorescent light. Only a small volume of the cuvette is actually illuminated by the laser and produces a detectable spectroscopic emission. The output signal is significantly reduced because the lens picks up only about 10% of the spectroscopic emission due to solid angle considerations. This technique has been used for at least 75 years; even before the laser existed, when conventional light sources were used to excite the fluorescence.\n\nSOFIA solves the problem of low collection efficiency, as it collects nearly all of the fluorescent light produced from the sample being analyzed, increasing the amount of fluorescence signal by around a factor of 10 over conventional apparatus.\n\nSOFIA is an apparatus and method for improved optical geometry for enhancement of spectroscopic detection of analytes in a sample. The invention has already demonstrated its proof-of-concept functionality as an apparatus and method for ultrasensitive detection of prions and other low-level analytes.\n\nSOFIA combines the specificity inherent in monoclonal antibodies for antigen capture with the sensitivity of surround optical detection technology. To detect extremely low signal levels, a low-noise, photovoltaic diode is used as the detector for the system. SOFIA uses a laser to illuminate a microcapillary tube holding the sample. Then, the light collected from the sample is directed to transfer optics from optical fibers. Next, the light is optically filtered for detection, which is performed as a current measurement amplified against noise by a digital signal processing lock-in amplified. The results are displayed on a computer and software designed for data acquisition.\n\nThe advantages of such a detection array are numerous. Primarily, it permits the use of very small samples at low concentration to be optimally interrogated using the laser-induced fluorescence technique. This fiber-based detection system is adaptable to existing short-pulsed detection hardware that was originally developed for sequencing single DNA molecules. The geometry is also amenable to deployment for short-pulse laser, single-molecule detection schemes. The multiport geometry of the system allows efficient electronic processing of the signals from each arm of the device. Finally, and perhaps most importantly, fiberoptic cables are essentially 100% efficient in optical transmission, having an attenuation less than 10 dB/km. Thus, once deployed for use in a facility, the fluorescence information can be fiberoptically transmitted to a remote location, where data processing and analysis can be performed.\n\nSOFIA comprises a multiwell plate sample container, an automated means for successively transporting samples from the multiwell plate sample container to a transparent capillary contained within a sample holder, an excitation source in optical communication with the sample, wherein radiation from the excitation source is directed along the length of the capillary, and wherein the radiation induces a signal which is emitted from the sample, and, at least one linear array.\n\nAfter amplifying and then concentrating the target analyte, the samples are labeled with a fluorescent dye using an antibody for specificity and then finally loaded into a microcapillary tube. This tube is placed in a specially constructed apparatus so it is totally surrounded by optical fibers to capture all light emitted once the dye is excited using a laser.\nThis equipment is a spectroscopic (light gathering) apparatus and corresponding method for rapidly detecting and analyzing analytes in a sample. The sample is irradiated by an excitation source in optical communication with the sample. The excitation source may include, but is not limited to, a laser, a flash lamp, an arc lamp, a light-emitting diode, or the like.\n\nFigure 1 depicts the current version of the SOFIA system. Four linear arrays (101) extend from a sample holder (102), which houses an elongated, transparent sample container which is open at both ends, to an end port (103). The distal end of the endport (104) is inserted into an end port assembly (200). The linear arrays (101) comprise a plurality of optical fibers having a first end and a second end, the plurality of optical fibers optionally surrounded by a protective and/or insulating sheath. The optical fibers are linearly arranged, meaning that they are substantially coplanar with respect to one another so as to form an elongated row of fibers.\n\nThe analyte of interest may be biological or chemical in nature, and by way of example, only may include chemical moieties (toxins, metabolites, drugs and drug residues), peptides, proteins, cellular components, viruses, and combinations thereof. The analyte of interest may be in either a fluid or a supporting medium, such as a gel.\n\nSOFIA has demonstrated its potential as a device with a wide range of applications. These include clinical applications, such as detecting diseases, discovering predispositions to pathologies, establishing a diagnosis and tracking the effectiveness of prescribed treatments, and nonclinical applications, such as preventing the entry of toxins and other pathogenic agents into products intended for human consumption:\n\n\nSOFIA has been used to rapidly detect the abnormal form of the prion protein (PrP) in samples of bodily fluids, such as blood or urine. PrP is the marker protein used in diagnostics for transmissible spongiform encephalopathies (TSEs), examples of which include bovine spongiform encephalopathy in cattle (i.e. “mad cow” disease), scrapie in sheep, and Creutzfeldt–Jakob disease in humans. Currently, no rapid means exists for the \"ante mortem\" detection of PrP in the dilute quantities in which it usually appears in bodily fluids. SOFIA has the advantages of requiring little sample preparation, and allowing for electronic diagnostic equipment to be placed outside the containment area.\n\nTSEs, or prion diseases, are infectious neurodegenerative diseases of mammals that include bovine spongiform encephalopathy, chronic wasting disease of deer and elk, scrapie in sheep, and Creutzfeldt–Jakob disease (CJD) in humans. TSEs may be passed from host to host by ingestion of infected tissues or blood transfusions. Clinical symptoms of TSEs include loss of movement and coordination and dementia in humans. They have incubation periods of months to years, but after the appearance of clinical signs, they progress rapidly, are untreatable and invariably are fatal. Attempts at TSE risk-reduction have led to significant changes in the production and trade of agricultural goods, medicines, cosmetics, blood and tissue donations, and biotechnology products. \"Post mortem\" neuropathological examination of brain tissue from an animal or human has remained the ‘gold standard’ of TSE diagnosis and is very specific, but not as sensitive as other techniques.\n\nTo improve food safety, it would be beneficial to screen all the animals for prion diseases using \"ante mortem\", preclinical testing, i.e., testing prior to presentation of symptoms. However, PrP levels are very low in presymptomatic hosts. In addition, PrPs are generally unevenly distributed in body tissues, with highest concentration consistently found in nervous system tissues and very low concerntrations in easily accessible body fluids such as blood or urine. Therefore, any such test would be required to detect extremely small amounts of PrP and would have to differentiate PrP and PrP.\n\nCurrent PrP detection methods are time-consuming and employ \"post mortem\" analysis after suspicious animals manifest one or more symptoms of the disease. Current diagnostic methods are based mainly on detection of physiochemical differences between PrP and PrP which, to date, are the only reliable markers for TSEs. For example, the most widely used diagnostic tests exploit the relative protease resistance of PrP in brain samples to discriminate between PrP and PrP, in combination with antibody-based detection of the PK-resistant portion of PrP. It has as yet not been possible to detect prion diseases by using conventional methods, such as polymerase chain reaction, serology, or cell culture assays. An agent-specific nucleic acid has not yet been identified, and the infected host does not elicit an antibody response.\n\nThe conformationally altered form of PrP is PrP. Some groups believe PrP is the infectious agent (prion agent) in TSEs, while other groups do not. PrP could be a neuropathological product of the disease process, a component of the infectious agent, the infectious agent itself, or something else altogether. Regardless of what its actual function in the disease state is, PrP is clearly specifically associated with the disease process, and detection of it indicates infection with the agent causing prion diseases.\n\nSOFIA provides, among other things, methods to diagnose prion diseases by detection of PrP in a biological sample. This biological sample can be brain tissue, nerve tissue, blood, urine, lymphatic fluid, cerebrospinal fluid, or a combination thereof. Absence of PrP indicates no infection with the infectious agent up to the detection limits of the methods. Detection of a presence of PrP indicates infection with the infectious agent associated with prion disease. Infection with the prion agent may be detected in both presymptomatic and symptomatic stages of disease progression.\n\nThese and other improvements have been achieved with SOFIA. SOFIA’s sensitivity and specificity eliminates the need for PK digestion to distinguish between the normal and abnormal PrP isoforms. Further detection of PrP in blood plasma has now been addressed by limited PMCA followed by SOFIA. Because of the sensitivity of SOFIA, PMCA cycles can be reduced, thus decreasing the chances of spontaneous PrP formation and the detection of false-positive samples.\n\nSOFIA meets the needs of increased sensitivity in the detection of prion diseases in both presymptomatic and symptomatic TSE infected animals, including humans, by providing methods of analysis using highly sensitive instrumentation, which requires less sample preparation than previously described methods, in combination with recently developed Mabs against PrP. The method of the present version of SOFIA provides sensitivity levels sufficient to detect PrP in brain tissue. When coupled with limited sPMCA, the methods of the present inventions provide sensitivity levels sufficient to detect PrP in blood plasma, tissue and other fluids collected antemortem.\n\nThe methods combine the specificity of the Mabs for antigen capture and concentration with the sensitivity of a surround optical fiber detection technology. In contrast to previously described methods for detection of PrP in brain homogenates, these techniques, when used to study brain homogenates, do not use seeded polymerization, amplification, or enzymatic digestion (for example, by proteinase K, or “PK”). This is important in that previous reports have indicated the existence of PrP isoforms with varied PK sensitivity, which decreases reliability of the assay. The sensitivity of this assay makes it suitable as a platform for rapid prion detection assay in biological fluids. In addition to prion diseases, the method may provide a means for rapid, high-throughput testing for a wide spectrum of infections and disorders.\n\nWhile about 40 cycles of sPMCA combined with immunoprecipitation were found to be inadequate for PrP detection in plasma by ELISA or western blotting, the PrP has also been found to be readily measured by SOFIA methods. The limited numbers of cycles necessary for the present assay platform virtually eliminates the possibility of obtaining PMCA-related false-positive results such as those previously reported (Thorne and Terry, 2008).\n\nWith rapid developments in the field of biomarker research, many infections and disorders that have not been possible to diagnose via \"in vitro\" testing, are becoming increasingly possible. SOFIA is predicted to be of broader use in diagnostic assay development for infections and disorders beyond the scope of prion diseases. A major potential application is for other protein misfolding diseases, in particular Alzheimer's.\n\nA 2011 study reported the detection of prions in urine from naturally and orally infected sheep with clinical scrapie agent and orally infected preclinical and infected white-tailed deer with clinical chronic wasting disease (CWD). This is the first report on prion detection of PrP from the urine of naturally or preclinical prion-diseased ovines or cervids.\n\nA 2010 study demonstrated a moderate amount of protein misfolding cyclic amplification (PMCA) coupled to a novel SOFIA detection scheme, can be used to detect PrP in protease-untreated plasma from preclinical and clinical scrapie sheep, and white-tailed deer with chronic wasting disease, following natural and experimental infection. The disease-associated form of the prion protein (PrP), resulting from a conformational change of the normal (cellular) form of prion protein (PrP), is considered central to neuropathogenesis and serves as the only reliable molecular marker for prion disease diagnosis. While the highest levels of PrP are present in the CNS, the development of a reasonable diagnostic assay requires the use of body fluids which characteristically contains extremely low levels of PrP. PrP has been detected in the blood of sick animals by means of PMCA technology. However, repeated cycling over several days, which is necessary for PMCA of blood material, has been reported to result in decreased specificity (false positives). To generate an assay for PrP in blood that is both highly sensitive and specific, the researchers used limited serial PMCA (sPMCA) with SOFIA. They did not find any enhancement of sPMCA with the addition of polyadenylic acid, nor was it necessary to match the genotypes of the PrP and PrP sources for efficient amplification.\n\nA 2009 study found SOFIA, in its current format, is capable of detecting less than 10 attogram (ag) of hamster, sheep and deer recombinant PrP. About 10 ag of PrP from 263K-infected hamster brains can be detected with similar lower limits of PrP detection from the brains of scrapie-infected sheep and deer infected with chronic wasting disease. These detection limits allow protease-treated and untreated material to be diluted beyond the point where PrP, nonspecific proteins or other extraneous material may interfere with PrP signal detection and/or specificity. This not only eliminates the issue of specificity of PrP detection, but also increases sensitivity, since the possibility of partial PrP proteolysis is no longer a concern. SOFIA will likely lead to early \"ante mortem\" detection of transmissible encephalopathies and is also amenable for use with additional target amplification protocols. SOFIA represents a sensitive means for detecting specific proteins involved in disease pathogenesis and/or diagnosis that extends beyond the scope of the transmissible spongiform encephalopathies.\n\n\n", "id": "32823089", "title": "Surround optical-fiber immunoassay"}
{"url": "https://en.wikipedia.org/wiki?curid=32967604", "text": "Glycomimetic\n\nGlycomimetic is a term used to refer to molecules that have structures similar to carbohydrates, but with some variation. This will normally result in modified biological properties.\n\nOften, modification of the structure will take place around the glycosidic linkage. Replacement of one or other of the glycosidic oxygen atoms by carbon, sulfur, nitrogen etc. will alter the properties of the glycosidic bond. The molecules produced in this way would be called carbasugars or C-glycosides, thiosugars or thioglycosides, or iminosugars or glycosylamines.\n\nWhen nitrogen is introduced, the glycomimetic may become positively charged at physiological pH, meaning that it may act as an enzyme inhibitor, either by Coulombic interaction with carboxylate amino acid side-chains in the enzyme active site, or by mimicking positive-charge build-up at the transition state of the reaction, or both. Iminosugars (sometimes referred to erroneously as azasugars) are classic examples of molecules with this behaviour. Glycosylamines typically have a lower stability, being easily hydrolysed, which means that to exploit an exocyclic nitrogen substituent at C-1, further modification is necessary. An example of this would be the additional substitution of the ring-oxygen for carbon as is seen in valienamine.\n\nAltering the structure of a carbohydrate will normally result in several changes to the properties of the molecule. As well as changing the stability of the glycosidic bond, the ring-conformation may be affected. Also the conformation of the glycosidic bond may be affected. As well as obvious changes in the immediate vicinity of the substitution, e.g. that replacement of an acetal oxygen by methylene (CH2) would result in loss of a hydrogen-bond participatory atom, such a substitution is expected to have more subtle effects resulting from a change in the dipole of the molecule, such as slight changes in hydrogen bonding or pKa values of the unchanged hydroxyl groups. Substitution by CF2 rather than methylene has been explored in efforts to address this and come up with better mimetics while still retaining the hydrolytic stability gained by the modification.\n\nTamiflu is a carbocyclic mimic of the cell-surface carbohydrate sialic acid. Tamiflu is an enzyme inhibitor that blocks the action of influenza virus neuraminidases (sialidases).\n\nAcarbose is a pseudotetrasaccharide mimicking maltotetraose (a substructure of starch). One of the glucose units has been replaced by valienamine - a carbasugar, linked to the next carbohydrate by an amine bridge. Another of the glucose units appears as a 6-deoxy variant. Acarbose is an enzyme inhibitor that is used as a drug against type 2 diabetes.\n\nMiglustat is an iminosugar in which the ring oxygen is replaced by nitrogen. Miglustat a drug used to treat some rare lysosomal storage disorder diseases.\n", "id": "32967604", "title": "Glycomimetic"}
{"url": "https://en.wikipedia.org/wiki?curid=33178845", "text": "TopFIND\n\nTopFIND is the Termini oriented protein Function Inferred Database (TopFIND) is an integrated knowledgebase focused on protein termini, their formation by proteases and functional implications. It contains information about the processing and the processing state of proteins and functional implications thereof derived from research literature, contributions by the scientific community and biological databases.\n\nAmong the most fundamental characteristics of a protein are the N- and C-termini defining the start and end of the polypeptide chain. While genetically encoded, protein termini isoforms are also often generated during translation, following which, termini are highly dynamic, being frequently trimmed at their ends by a large array of exopeptidases. Neo-termini can also be generated by endopeptidases after precise and limited proteolysis, termed processing. Necessary for the maturation of many proteins, processing can also occur afterwards, often resulting in dramatic functional consequences. Aberrant proteolysis can cause wide range of diseases like arthritis or cancer. Hence, proteolytic generation of pleiotrophic stable forms of proteins, the universal susceptibility of proteins to proteolysis, and its irreversibility, distinguishes proteolysis from many highly studied posttranslational modifications. Proteases are tightly interconnected in the protease web and their aberrant activity in disease can lead to diagnostic fragment profiles with characteristic protein termini. Following proteolysis, the newly formed protein termini can be further modified, a process that affects protein function and stability.\n\nTopFIND is a resource for comprehensive coverage of protein N- and C-termini discovered by all available in silico, in vitro as well as in vivo methodologies. It makes use of existing knowledge by seamless integration of data from UniProt and MEROPS and provides access to new data from community submission and manual literature curating. It renders modifications of protein termini, such as acetylation and citrullination, easily accessible and searchable and provides the means to identify and analyse extend and distribution of terminal modifications across a protein. Since its inception TopFIND has been expanded to further species.\n\nThe data is presented to the user with a strong emphasis on the relation to curated background information and underlying evidence that led to the observation of a terminus, its modification or proteolytic cleavage. In brief the protein information, its domain structure, protein termini, terminus modifications and proteolytic processing of and by other proteins is listed. All information is accompanied by metadata like its original source, method of identification, confidence measurement or related publication. A positional cross correlation evaluation matches termini and cleavage sites with protein features (such as amino acid variants) and domains to highlight potential effects and dependencies in a unique way. Also, a network view of all proteins showing their functional dependency as protease, substrate or protease inhibitor tied in with protein interactions is provided for the easy evaluation of network wide effects. A powerful yet user friendly filtering mechanism allows the presented data to be filtered based on parameters like methodology used, in vivo relevance, confidence or data source (e.g. limited to a single laboratory or publication). This provides means to assess physiological relevant data and to deduce functional information and hypotheses relevant to the bench scientist. In a later release analysis tools for the evaluation of proteolytic pathways in experimental data have been added.\n\n\n", "id": "33178845", "title": "TopFIND"}
{"url": "https://en.wikipedia.org/wiki?curid=17385860", "text": "Foldit\n\nFoldit is an online puzzle video game about protein folding. It is part of an experimental research project developed by the University of Washington, Center for Game Science, in collaboration with the UW Department of Biochemistry. The objective of Foldit is to fold the structures of selected proteins as perfectly as possible, using tools provided in the game. The highest scoring solutions are analyzed by researchers, who determine whether or not there is a native structural configuration (native state) that can be applied to relevant proteins in the real world. Scientists can then use these solutions to target and eradicate diseases and create biological innovations.\nA 2010 paper in the science journal \"Nature\" credited Foldit's 57,000 players with providing useful results that matched or outperformed algorithmically computed solutions.\n\nProf. David Baker, a protein research scientist at the University of Washington, founded the Foldit project. Seth Cooper was the lead game designer. Before starting the project, Baker and his laboratory coworkers relied on another research project named Rosetta to predict the native structures of various proteins using special computer protein structure prediction algorithms. Rosetta was eventually extended to use the power of distributed computing: The Rosetta@home program was made available for public download, and displayed its protein-folding progress as a screensaver. Its results were sent to a central server for verification.\n\nSome Rosetta@home users became frustrated when they saw ways to solve protein structures, but could not interact with the program. Hoping that humans could improve the computers' attempts to solve protein structures, Baker approached David Salesin and Zoran Popović, computer science professors at the same university, to help conceptualize and build an interactive program, a video game, that would appeal to the public and help efforts to find native protein structures.\n\nMany of the same people who created Rosetta@home worked on Foldit. The public beta version was released in May 2008 and has 240,000 registered players.\n\nSince 2008, Foldit has participated in Critical Assessment of Techniques for Protein Structure Prediction (CASP) experiments, submitting its best solutions to targets based on unknown protein structures. CASP is an international program to assess methods of protein structure prediction and identify those that are most productive.\n\nProtein structure prediction is important in several fields of science, including bioinformatics, molecular biology, and medicine. Identifying natural proteins' structural configurations enables scientists to understand them better. This can lead to creating novel proteins by design, advances in treating disease, and solutions for other real-world problems such as invasive species, waste, and pollution.\n\nThe process by which living beings create the primary structure of proteins, protein biosynthesis, is reasonably well understood, as is the means by which proteins are encoded as DNA. However, determining how a given protein's primary structure becomes a functioning three-dimensional structure, how the molecule \"folds\", is more difficult. The general process is understood, but predicting a protein's eventual, functioning structure is computationally demanding.\n\nSimilarly to Rosetta@home, Foldit is a means to discover native protein structures faster through distributed computing. However, Foldit has a greater emphasis on community collaboration through its forums, where users can collaborate on certain folds. Furthermore, Foldit's crowdsourced approach places a greater emphasis on the user. Foldit's virtual interaction and gamification create a unique and innovative environment with the potential to greatly advance protein folding research.\n\nFoldit attempts to apply the human brain's three-dimensional pattern matching and spatial reasoning abilities to help solve the problem of protein structure prediction. Current puzzles are based on well-understood proteins. By analysing how humans intuitively approach these puzzles, researchers hope to improve the algorithms used by protein-folding software.\n\nFoldit includes a series of tutorials where users manipulate simple protein-like structures and a periodically updated set of puzzles based on real proteins. It shows a graphical representation of each protein which users can manipulate using a set of tools.\n\nFoldit's developers wanted to attract as many people as possible to the cause of protein folding. So, rather than only building a useful science tool, they used gamification (the inclusion of gaming elements) to make Foldit appealing and engaging to the general public.\n\nAs a protein structure is modified, a \"score\" is calculated based on how well-folded the protein is, and a list of high scores for each puzzle is maintained. Foldit users may create and join groups, and share puzzle solutions. A separate list of group high scores is maintained.\n\n\nFoldit's toolbox is mainly for the design of protein molecules. The game's creator announced the plan to add, by 2013, the chemical building blocks of organic subcomponents to enable players to design small molecules.\n\n\n\n", "id": "17385860", "title": "Foldit"}
{"url": "https://en.wikipedia.org/wiki?curid=33496499", "text": "Codon Adaptation Index\n\nThe Codon Adaptation Index (CAI) is the most widespread technique for analyzing Codon usage bias. As opposed to other measures of codon usage bias, such as the 'effective number of codons' (Nc), which measure deviation from a uniform bias (null hypothesis), CAI measures the deviation of a given protein coding gene sequence with respect to a reference set of genes.\n\nIdeally, the reference set in CAI is composed of highly expressed genes, so that CAI provides an indication of gene expression level under the assumption that there is translational selection to optimize gene sequences according to their expression levels. The rationale for this is dual: highly expressed genes need to compete for resources (i.e. ribosomes) in fast-growing organisms and it makes sense for them to be also more accurately translated. Both hypotheses lead to highly expressed genes using mostly codons for tRNA species that are abundant in the cell.\n\nCAI is simply defined as the geometric mean of the weight associated to each codon over the length of the gene sequence (measured in codons).\n\nFor each amino acid, the weight of each of its codons, in CAI, is computed from the reference sequence set, as the ratio between the observed frequency of the codon and the frequency of the most frequent synonymous codon for that amino acid.\n\n", "id": "33496499", "title": "Codon Adaptation Index"}
{"url": "https://en.wikipedia.org/wiki?curid=10175256", "text": "Geranylgeranylation\n\nGeranylgeranylation is a form of prenylation, which is a post-translational modification of proteins that involves the attachment of one or two 20-carbon lipophilic geranylgeranyl isoprene units from geranylgeranyl diphosphate to one or two cysteine residue(s) at the C-terminus of specific proteins. Prenylation (including geranylgeranylation) is thought to function, at least in part, as a membrane anchor for proteins. \n\nThe process of geranylgeranylation can be catalyzed by either geranylgeranyl transferase I (GGTase I) or Rab GGTase (also GGTase II). GGTase I catalyzes the addition of one geranylgeranyl group onto the C-terminal consensus sequence CAAL (somewhat similar to farnesyltransferase reactions), where C=cysteine, A=any aliphatic amino acid, and L=leucine. Rab GGTase adds a total of two geranylgeranyl groups onto two cysteine residues at the C-terminal consensus sequence CXC or XXCC. The source of the geranylgeranyl group is geranylgeranyl diphosphate, which is synthesized by GGPS1 within the isoprenoid biosynthetic pathway. \n\nAn example of this can be seen in the lipid anchoring of the Rho GTPase family of signaling molecules and the gamma subunit of heterotrimeric G proteins. \n", "id": "10175256", "title": "Geranylgeranylation"}
