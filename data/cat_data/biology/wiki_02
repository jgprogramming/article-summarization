{"url": "https://en.wikipedia.org/wiki?curid=18393", "text": "Life\n\nLife is a characteristic that distinguishes physical entities that do have biological processes, such as signaling and self-sustaining processes, from those that do not, either because such functions have ceased, or because they never had such functions and are classified as inanimate. Various forms of life exist, such as plants, animals, fungi, protists, archaea, and bacteria. The criteria can at times be ambiguous and may or may not define viruses, viroids, or potential artificial life as \"living\". Biology is the primary science concerned with the study of life, although many other sciences are involved.\n\nThe definition of life is controversial. The current definition is that organisms maintain homeostasis, are composed of cells, undergo metabolism, can grow, adapt to their environment, respond to stimuli, and reproduce. However, many other biological definitions have been proposed, and there are some borderline cases of life, such as viruses. Throughout history, there have been many attempts to define what is meant by \"life\" and many theories on the properties and emergence of living things, such as materialism, the belief that everything is made out of matter and that life is merely a complex form of it; hylomorphism, the belief that all things are a combination of matter and form, and the form of a living thing is its soul; spontaneous generation, the belief that life repeatedly emerges from non-life; and vitalism, a now largely discredited hypothesis that living organisms possess a \"life force\" or \"vital spark\". Modern definitions are more complex, with input from a diversity of scientific disciplines. Biophysicists have proposed many definitions based on chemical systems; there are also some living systems theories, such as the Gaia hypothesis, the idea that the Earth itself is alive. Another theory is that life is the property of ecological systems, and yet another is elaborated in complex systems biology, a branch or subfield of mathematical biology. Abiogenesis describes the natural process of life arising from non-living matter, such as simple organic compounds. Properties common to all organisms include the need for certain core chemical elements to sustain biochemical functions.\n\nLife on Earth first appeared as early as 4.28 billion years ago, soon after ocean formation 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. Earth's current life may have descended from an RNA world, although RNA-based life may not have been the first. The mechanism by which life began on Earth is unknown, though many hypotheses have been formulated and are often based on the Miller–Urey experiment. The earliest known life forms are microfossils of bacteria. 3.45 billion year old Australian rocks are reported to have contained microorganisms. In 2016, scientists reported identifying a set of 355 genes thought to be present in the last universal common ancestor (LUCA) of all living organisms, already a complex organism and not the first living thing.\n\nSince its primordial beginnings, life on Earth has changed its environment on a geologic time scale. To survive in most ecosystems, life must often adapt to a wide range of conditions. Some microorganisms, called extremophiles, thrive in physically or geochemically extreme environments that are detrimental to most other life on Earth. Aristotle was the first person to classify organisms. Later, Carl Linnaeus introduced his system of binomial nomenclature for the classification of species. Eventually new groups and categories of life were discovered, such as cells and microorganisms, forcing dramatic revisions of the structure of relationships between living organisms. Cells are sometimes considered the smallest units and \"building blocks\" of life. There are two kinds of cells, prokaryotic and eukaryotic, both of which consist of cytoplasm enclosed within a membrane and contain many biomolecules such as proteins and nucleic acids. Cells reproduce through a process of cell division, in which the parent cell divides into two or more daughter cells.\n\nThough currently only known on Earth, life need not be restricted to it, and many scientists speculate in the existence of extraterrestrial life. Artificial life is a computer simulation or man-made reconstruction of any aspect of life, which is often used to examine systems related to natural life. Death is the permanent termination of all biological functions which sustain an organism, and as such, is the end of its life. Extinction is the process by which an entire group or taxon, normally a species, dies out. Fossils are the preserved remains or traces of organisms.\n\nIt is a challenge for scientists and philosophers to define life. This is partially because life is a process, not a substance. Any definition must be general enough to both encompass all known life and any unknown life that may be different from life on Earth.\n\nSince there is no unequivocal definition of life, most current definitions in biology are descriptive. Life is considered a characteristic of something that preserves, furthers or reinforces its existence in the given environment. This characteristic exhibits all or most of the following traits: \n\nThese complex processes, called physiological functions, have underlying physical and chemical bases, as well as signaling and control mechanisms that are essential to maintaining life.\n\nFrom a physics perspective, living beings are thermodynamic systems with an organized molecular structure that can reproduce itself and evolve as survival dictates. Thermodynamically, life has been described as an open system which makes use of gradients in its surroundings to create imperfect copies of itself. Hence, life is a self-sustained chemical system capable of undergoing Darwinian evolution. A major strength of this definition is that it distinguishes life by the evolutionary process rather than its chemical composition.\n\nOthers take a systemic viewpoint that does not necessarily depend on molecular chemistry. One systemic definition of life is that living things are self-organizing and autopoietic (self-producing). Variations of this definition include Stuart Kauffman's definition as an autonomous agent or a multi-agent system capable of reproducing itself or themselves, and of completing at least one thermodynamic work cycle. This definition is extended by the apparition of novel functions over time.\n\nWhether or not viruses should be considered as alive is controversial. They are most often considered as just replicators rather than forms of life. They have been described as \"organisms at the edge of life\" because they possess genes, evolve by natural selection, and replicate by creating multiple copies of themselves through self-assembly. However, viruses do not metabolize and they require a host cell to make new products. Virus self-assembly within host cells has implications for the study of the origin of life, as it may support the hypothesis that life could have started as self-assembling organic molecules.\n\nTo reflect the minimum phenomena required, other biological definitions of life have been proposed, with many of these being based upon chemical systems. Biophysicists have commented that living things function on negative entropy. In other words, living processes can be viewed as a delay of the spontaneous diffusion or dispersion of the internal energy of biological molecules towards more potential microstates. In more detail, according to physicists such as John Bernal, Erwin Schrödinger, Eugene Wigner, and John Avery, life is a member of the class of phenomena that are open or continuous systems able to decrease their internal entropy at the expense of substances or free energy taken in from the environment and subsequently rejected in a degraded form.\n\nLiving systems are open self-organizing living things that interact with their environment. These systems are maintained by flows of information, energy, and matter.\n\nSome scientists have proposed in the last few decades that a general living systems theory is required to explain the nature of life. Such a general theory would arise out of the ecological and biological sciences and attempt to map general principles for how all living systems work. Instead of examining phenomena by attempting to break things down into components, a general living systems theory explores phenomena in terms of dynamic patterns of the relationships of organisms with their environment.\n\nThe idea that the Earth is alive is found in philosophy and religion, but the first scientific discussion of it was by the Scottish scientist James Hutton. In 1785, he stated that the Earth was a superorganism and that its proper study should be physiology. Hutton is considered the father of geology, but his idea of a living Earth was forgotten in the intense reductionism of the 19th century. The Gaia hypothesis, proposed in the 1960s by scientist James Lovelock, suggests that life on Earth functions as a single organism that defines and maintains environmental conditions necessary for its survival. This hypothesis served as one of the foundations of the modern Earth system science.\n\nThe first attempt at a general living systems theory for explaining the nature of life was in 1978, by American biologist James Grier Miller. Robert Rosen (1991) built on this by defining a system component as \"a unit of organization; a part with a function, i.e., a definite relation between part and whole.\" From this and other starting concepts, he developed a \"relational theory of systems\" that attempts to explain the special properties of life. Specifically, he identified the \"nonfractionability of components in an organism\" as the fundamental difference between living systems and \"biological machines.\"\n\nA systems view of life treats environmental fluxes and biological fluxes together as a \"reciprocity of influence,\" and a reciprocal relation with environment is arguably as important for understanding life as it is for understanding ecosystems. As Harold J. Morowitz (1992) explains it, life is a property of an ecological system rather than a single organism or species. He argues that an ecosystemic definition of life is preferable to a strictly biochemical or physical one. Robert Ulanowicz (2009) highlights mutualism as the key to understand the systemic, order-generating behavior of life and ecosystems.\n\nComplex systems biology (CSB) is a field of science that studies the emergence of complexity in functional organisms from the viewpoint of dynamic systems theory. The latter is also often called systems biology and aims to understand the most fundamental aspects of life. A closely related approach to CSB and systems biology called relational biology is concerned mainly with understanding life processes in terms of the most important relations, and categories of such relations among the essential functional components of organisms; for multicellular organisms, this has been defined as \"categorical biology\", or a model representation of organisms as a category theory of biological relations, as well as an algebraic topology of the functional organization of living organisms in terms of their dynamic, complex networks of metabolic, genetic, and epigenetic processes and signaling pathways. Alternative but closely related approaches focus on the interdependance of constraints, where constraints can be either molecular, such as enzymes, or macroscopic, such as the geometry of a bone or of the vascular system.\n\nIt has also been argued that the evolution of order in living systems and certain physical systems obeys a common fundamental principle termed the Darwinian dynamic. The Darwinian dynamic was formulated by first considering how macroscopic order is generated in a simple non-biological system far from thermodynamic equilibrium, and then extending consideration to short, replicating RNA molecules. The underlying order-generating process was concluded to be basically similar for both types of systems.\n\nAnother systemic definition called the operator theory proposes that \"life is a general term for the presence of the typical closures found in organisms; the typical closures are a membrane and an autocatalytic set in the cell\" and that an organism is any system with an organisation that complies with an operator type that is at least as complex as the cell. Life can also be modeled as a network of inferior negative feedbacks of regulatory mechanisms subordinated to a superior positive feedback formed by the potential of expansion and reproduction.\n\nSome of the earliest theories of life were materialist, holding that all that exists is matter, and that life is merely a complex form or arrangement of matter. Empedocles (430 BC) argued that everything in the universe is made up of a combination of four eternal \"elements\" or \"roots of all\": earth, water, air, and fire. All change is explained by the arrangement and rearrangement of these four elements. The various forms of life are caused by an appropriate mixture of elements.\n\nDemocritus (460 BC) thought that the essential characteristic of life is having a soul (\"psyche\"). Like other ancient writers, he was attempting to explain what makes something a \"living\" thing. His explanation was that fiery atoms make a soul in exactly the same way atoms and void account for any other thing. He elaborates on fire because of the apparent connection between life and heat, and because fire moves.\n\nThe mechanistic materialism that originated in ancient Greece was revived and revised by the French philosopher René Descartes, who held that animals and humans were assemblages of parts that together functioned as a machine. In the 19th century, the advances in cell theory in biological science encouraged this view. The evolutionary theory of Charles Darwin (1859) is a mechanistic explanation for the origin of species by means of natural selection.\n\nHylomorphism is a theory first expressed by the Greek philosopher Aristotle (322 BC). The application of hylomorphism to biology was important to Aristotle, and biology is extensively covered in his extant writings. In this view, everything in the material universe has both matter and form, and the form of a living thing is its soul (Greek \"psyche\", Latin \"anima\"). There are three kinds of souls: the \"vegetative soul\" of plants, which causes them to grow and decay and nourish themselves, but does not cause motion and sensation; the \"animal soul\", which causes animals to move and feel; and the \"rational soul\", which is the source of consciousness and reasoning, which (Aristotle believed) is found only in man. Each higher soul has all of the attributes of the lower ones. Aristotle believed that while matter can exist without form, form cannot exist without matter, and that therefore the soul cannot exist without the body.\n\nThis account is consistent with teleological explanations of life, which account for phenomena in terms of purpose or goal-directedness. Thus, the whiteness of the polar bear's coat is explained by its purpose of camouflage. The direction of causality (from the future to the past) is in contradiction with the scientific evidence for natural selection, which explains the consequence in terms of a prior cause. Biological features are explained not by looking at future optimal results, but by looking at the past evolutionary history of a species, which led to the natural selection of the features in question.\n\nSpontaneous generation was the belief on the ordinary formation of living organisms without descent from similar organisms. Typically, the idea was that certain forms such as fleas could arise from inanimate matter such as dust or the supposed seasonal generation of mice and insects from mud or garbage.\n\nThe theory of spontaneous generation was proposed by Aristotle, who compiled and expanded the work of prior natural philosophers and the various ancient explanations of the appearance of organisms; it held sway for two millennia. It was decisively dispelled by the experiments of Louis Pasteur in 1859, who expanded upon the investigations of predecessors such as Francesco Redi. Disproof of the traditional ideas of spontaneous generation is no longer controversial among biologists.\n\nVitalism is the belief that the life-principle is non-material. This originated with Georg Ernst Stahl (17th century), and remained popular until the middle of the 19th century. It appealed to philosophers such as Henri Bergson, Friedrich Nietzsche, and Wilhelm Dilthey, anatomists like Marie François Xavier Bichat, and chemists like Justus von Liebig. Vitalism included the idea that there was a fundamental difference between organic and inorganic material, and the belief that organic material can only be derived from living things. This was disproved in 1828, when Friedrich Wöhler prepared urea from inorganic materials. This Wöhler synthesis is considered the starting point of modern organic chemistry. It is of historical significance because for the first time an organic compound was produced in inorganic reactions.\n\nDuring the 1850s, Hermann von Helmholtz, anticipated by Julius Robert von Mayer, demonstrated that no energy is lost in muscle movement, suggesting that there were no \"vital forces\" necessary to move a muscle. These results led to the abandonment of scientific interest in vitalistic theories, although the belief lingered on in pseudoscientific theories such as homeopathy, which interprets diseases and sickness as caused by disturbances in a hypothetical vital force or life force.\n\nThe age of the Earth is about 4.54 billion years. Evidence suggests that life on Earth has existed for at least 3.5 billion years, with the oldest physical traces of life dating back 3.7 billion years; however, some theories, such as the Late Heavy Bombardment theory, suggest that life on Earth may have started even earlier, as early as 4.1–4.4 billion years ago, and the chemistry leading to life may have begun shortly after the Big Bang, 13.8 billion years ago, during an epoch when the universe was only 10–17 million years old.\n\nMore than 99% of all species of life forms, amounting to over five billion species, that ever lived on Earth are estimated to be extinct.\n\nAlthough the number of Earth's catalogued species of lifeforms is between 1.2 million and 2 million, the total number of species in the planet is uncertain. Estimates range from 8 million to 100 million, with a more narrow range between 10 and 14 million, but it may be as high as 1 trillion (with only one-thousandth of one percent of the species described) according to studies realized in May 2016. The total amount of related DNA base pairs on Earth is estimated at 5.0 x 10 and weighs 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon). In July 2016, scientists reported identifying a set of 355 genes from the Last Universal Common Ancestor (LUCA) of all organisms living on Earth.\n\nAll known life forms share fundamental molecular mechanisms, reflecting their common descent; based on these observations, hypotheses on the origin of life attempt to find a mechanism explaining the formation of a universal common ancestor, from simple organic molecules via pre-cellular life to protocells and metabolism. Models have been divided into \"genes-first\" and \"metabolism-first\" categories, but a recent trend is the emergence of hybrid models that combine both categories.\n\nThere is no current scientific consensus as to how life originated. However, most accepted scientific models build on the Miller–Urey experiment and the work of Sidney Fox, which show that conditions on the primitive Earth favored chemical reactions that synthesize amino acids and other organic compounds from inorganic precursors, and phospholipids spontaneously form lipid bilayers, the basic structure of a cell membrane.\n\nLiving organisms synthesize proteins, which are polymers of amino acids using instructions encoded by deoxyribonucleic acid (DNA). Protein synthesis entails intermediary ribonucleic acid (RNA) polymers. One possibility for how life began is that genes originated first, followed by proteins; the alternative being that proteins came first and then genes.\n\nHowever, because genes and proteins are both required to produce the other, the problem of considering which came first is like that of the chicken or the egg. Most scientists have adopted the hypothesis that because of this, it is unlikely that genes and proteins arose independently.\n\nTherefore, a possibility, first suggested by Francis Crick, is that the first life was based on RNA, which has the DNA-like properties of information storage and the catalytic properties of some proteins. This is called the RNA world hypothesis, and it is supported by the observation that many of the most critical components of cells (those that evolve the slowest) are composed mostly or entirely of RNA. Also, many critical cofactors (ATP, Acetyl-CoA, NADH, etc.) are either nucleotides or substances clearly related to them. The catalytic properties of RNA had not yet been demonstrated when the hypothesis was first proposed, but they were confirmed by Thomas Cech in 1986.\n\nOne issue with the RNA world hypothesis is that synthesis of RNA from simple inorganic precursors is more difficult than for other organic molecules. One reason for this is that RNA precursors are very stable and react with each other very slowly under ambient conditions, and it has also been proposed that living organisms consisted of other molecules before RNA. However, the successful synthesis of certain RNA molecules under the conditions that existed prior to life on Earth has been achieved by adding alternative precursors in a specified order with the precursor phosphate present throughout the reaction. This study makes the RNA world hypothesis more plausible.\n\nGeological findings in 2013 showed that reactive phosphorus species (like phosphite) were in abundance in the ocean before 3.5 Ga, and that Schreibersite easily reacts with aqueous glycerol to generate phosphite and glycerol 3-phosphate. It is hypothesized that Schreibersite-containing meteorites from the Late Heavy Bombardment could have provided early reduced phosphorus, which could react with prebiotic organic molecules to form phosphorylated biomolecules, like RNA.\n\nIn 2009, experiments demonstrated Darwinian evolution of a two-component system of RNA enzymes (ribozymes) \"in vitro\". The work was performed in the laboratory of Gerald Joyce, who stated \"This is the first example, outside of biology, of evolutionary adaptation in a molecular genetic system.\"\n\nPrebiotic compounds may have originated extraterrestrially. NASA findings in 2011, based on studies with meteorites found on Earth, suggest DNA and RNA components (adenine, guanine and related organic molecules) may be formed in outer space.\n\nIn March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.\n\nAccording to the panspermia hypothesis, microscopic life—distributed by meteoroids, asteroids and other small Solar System bodies—may exist throughout the universe.\n\nThe diversity of life on Earth is a result of the dynamic interplay between genetic opportunity, metabolic capability, environmental challenges, and symbiosis. For most of its existence, Earth's habitable environment has been dominated by microorganisms and subjected to their metabolism and evolution. As a consequence of these microbial activities, the physical-chemical environment on Earth has been changing on a geologic time scale, thereby affecting the path of evolution of subsequent life. For example, the release of molecular oxygen by cyanobacteria as a by-product of photosynthesis induced global changes in the Earth's environment. Because oxygen was toxic to most life on Earth at the time, this posed novel evolutionary challenges, and ultimately resulted in the formation of Earth's major animal and plant species. This interplay between organisms and their environment is an inherent feature of living systems.\n\nThe biosphere is the global sum of all ecosystems. It can also be termed as the zone of life on Earth, a closed system (apart from solar and cosmic radiation and heat from the interior of the Earth), and largely self-regulating. By the most general biophysiological definition, the biosphere is the global ecological system integrating all living beings and their relationships, including their interaction with the elements of the lithosphere, geosphere, hydrosphere, and atmosphere.\n\nLife forms live in every part of the Earth's biosphere, including soil, hot springs, inside rocks at least deep underground, the deepest parts of the ocean, and at least high in the atmosphere. Under certain test conditions, life forms have been observed to thrive in the near-weightlessness of space and to survive in the vacuum of outer space. Life forms appear to thrive in the Mariana Trench, the deepest spot in the Earth's oceans. Other researchers reported related studies that life forms thrive inside rocks up to below the sea floor under of ocean off the coast of the northwestern United States, as well as beneath the seabed off Japan. In August 2014, scientists confirmed the existence of life forms living below the ice of Antarctica. According to one researcher, \"You can find microbes everywhere — they're extremely adaptable to conditions, and survive wherever they are.\"\n\nThe biosphere is postulated to have evolved, beginning with a process of biopoesis (life created naturally from non-living matter, such as simple organic compounds) or biogenesis (life created from living matter), at least some 3.5 billion years ago. The earliest evidence for life on Earth includes biogenic graphite found in 3.7 billion-year-old metasedimentary rocks from Western Greenland and microbial mat fossils found in 3.48 billion-year-old sandstone from Western Australia. More recently, in 2015, \"remains of biotic life\" were found in 4.1 billion-year-old rocks in Western Australia. In 2017, putative fossilized microorganisms (or microfossils) were announced to have been discovered in hydrothermal vent precipitates in the Nuvvuagittuq Belt of Quebec, Canada that were as old as 4.28 billion years, the oldest record of life on earth, suggesting \"an almost instantaneous emergence of life\" after ocean formation 4.4 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. According to biologist Stephen Blair Hedges, \"If life arose relatively quickly on Earth ... then it could be common in the universe.\"\n\nIn a general sense, biospheres are any closed, self-regulating systems containing ecosystems. This includes artificial biospheres such as Biosphere 2 and BIOS-3, and potentially ones on other planets or moons.\n\nThe inert components of an ecosystem are the physical and chemical factors necessary for life—energy (sunlight or chemical energy), water, temperature, atmosphere, gravity, nutrients, and ultraviolet solar radiation protection. In most ecosystems, the conditions vary during the day and from one season to the next. To live in most ecosystems, then, organisms must be able to survive a range of conditions, called the \"range of tolerance.\" Outside that are the \"zones of physiological stress,\" where the survival and reproduction are possible but not optimal. Beyond these zones are the \"zones of intolerance,\" where survival and reproduction of that organism is unlikely or impossible. Organisms that have a wide range of tolerance are more widely distributed than organisms with a narrow range of tolerance.\n\nTo survive, selected microorganisms can assume forms that enable them to withstand freezing, complete desiccation, starvation, high levels of radiation exposure, and other physical or chemical challenges. These microorganisms may survive exposure to such conditions for weeks, months, years, or even centuries. Extremophiles are microbial life forms that thrive outside the ranges where life is commonly found. They excel at exploiting uncommon sources of energy. While all organisms are composed of nearly identical molecules, evolution has enabled such microbes to cope with this wide range of physical and chemical conditions. Characterization of the structure and metabolic diversity of microbial communities in such extreme environments is ongoing.\n\nMicrobial life forms thrive even in the Mariana Trench, the deepest spot on the Earth. Microbes also thrive inside rocks up to below the sea floor under of ocean.\n\nInvestigation of the tenacity and versatility of life on Earth, as well as an understanding of the molecular systems that some organisms utilize to survive such extremes, is important for the search for life beyond Earth. For example, lichen could survive for a month in a simulated Martian environment.\n\nAll life forms require certain core chemical elements needed for biochemical functioning. These include carbon, hydrogen, nitrogen, oxygen, phosphorus, and sulfur—the elemental macronutrients for all organisms—often represented by the acronym CHNOPS. Together these make up nucleic acids, proteins and lipids, the bulk of living matter. Five of these six elements comprise the chemical components of DNA, the exception being sulfur. The latter is a component of the amino acids cysteine and methionine. The most biologically abundant of these elements is carbon, which has the desirable attribute of forming multiple, stable covalent bonds. This allows carbon-based (organic) molecules to form an immense variety of chemical arrangements. Alternative hypothetical types of biochemistry have been proposed that eliminate one or more of these elements, swap out an element for one not on the list, or change required chiralities or other chemical properties.\n\nDeoxyribonucleic acid is a molecule that carries most of the genetic instructions used in the growth, development, functioning and reproduction of all known living organisms and many viruses. DNA and RNA are nucleic acids; alongside proteins and complex carbohydrates, they are one of the three major types of macromolecule that are essential for all known forms of life. Most DNA molecules consist of two biopolymer strands coiled around each other to form a double helix. The two DNA strands are known as polynucleotides since they are composed of simpler units called nucleotides. Each nucleotide is composed of a nitrogen-containing nucleobase—either cytosine (C), guanine (G), adenine (A), or thymine (T)—as well as a sugar called deoxyribose and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. According to base pairing rules (A with T, and C with G), hydrogen bonds bind the nitrogenous bases of the two separate polynucleotide strands to make double-stranded DNA. The total amount of related DNA base pairs on Earth is estimated at 5.0 x 10, and weighs 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon).\n\nDNA stores biological information. The DNA backbone is resistant to cleavage, and both strands of the double-stranded structure store the same biological information. Biological information is replicated as the two strands are separated. A significant portion of DNA (more than 98% for humans) is non-coding, meaning that these sections do not serve as patterns for protein sequences.\n\nThe two strands of DNA run in opposite directions to each other and are therefore anti-parallel. Attached to each sugar is one of four types of nucleobases (informally, \"bases\"). It is the sequence of these four nucleobases along the backbone that encodes biological information. Under the genetic code, RNA strands are translated to specify the sequence of amino acids within proteins. These RNA strands are initially created using DNA strands as a template in a process called transcription.\n\nWithin cells, DNA is organized into long structures called chromosomes. During cell division these chromosomes are duplicated in the process of DNA replication, providing each cell its own complete set of chromosomes. Eukaryotic organisms (animals, plants, fungi, and protists) store most of their DNA inside the cell nucleus and some of their DNA in organelles, such as mitochondria or chloroplasts. In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm. Within the chromosomes, chromatin proteins such as histones compact and organize DNA. These compact structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.\n\nDNA was first isolated by Friedrich Miescher in 1869. Its molecular structure was identified by James Watson and Francis Crick in 1953, whose model-building efforts were guided by X-ray diffraction data acquired by Rosalind Franklin.\n\nLife is usually classified by eight levels of taxa—domains, kingdoms, phyla, class, order, family, genus, and species. In May 2016, scientists reported that 1 trillion species are estimated to be on Earth currently with only one-thousandth of one percent described.\n\nThe first known attempt to classify organisms was conducted by the Greek philosopher Aristotle (384–322 BC), who classified all living organisms known at that time as either a plant or an animal, based mainly on their ability to move. He also distinguished animals with blood from animals without blood (or at least without red blood), which can be compared with the concepts of vertebrates and invertebrates respectively, and divided the blooded animals into five groups: viviparous quadrupeds (mammals), oviparous quadrupeds (reptiles and amphibians), birds, fishes and whales. The bloodless animals were also divided into five groups: cephalopods, crustaceans, insects (which included the spiders, scorpions, and centipedes, in addition to what we define as insects today), shelled animals (such as most molluscs and echinoderms), and \"zoophytes\" (animals that resemble plants). Though Aristotle's work in zoology was not without errors, it was the grandest biological synthesis of the time and remained the ultimate authority for many centuries after his death.\n\nThe exploration of the Americas revealed large numbers of new plants and animals that needed descriptions and classification. In the latter part of the 16th century and the beginning of the 17th, careful study of animals commenced and was gradually extended until it formed a sufficient body of knowledge to serve as an anatomical basis for classification. In the late 1740s, Carl Linnaeus introduced his system of binomial nomenclature for the classification of species. Linnaeus attempted to improve the composition and reduce the length of the previously used many-worded names by abolishing unnecessary rhetoric, introducing new descriptive terms and precisely defining their meaning.\n\nThe fungi were originally treated as plants. For a short period Linnaeus had classified them in the taxon Vermes in Animalia, but later placed them back in Plantae. Copeland classified the Fungi in his Protoctista, thus partially avoiding the problem but acknowledging their special status. The problem was eventually solved by Whittaker, when he gave them their own kingdom in his five-kingdom system. Evolutionary history shows that the fungi are more closely related to animals than to plants.\n\nAs new discoveries enabled detailed study of cells and microorganisms, new groups of life were revealed, and the fields of cell biology and microbiology were created. These new organisms were originally described separately in protozoa as animals and protophyta/thallophyta as plants, but were united by Haeckel in the kingdom Protista; later, the prokaryotes were split off in the kingdom Monera, which would eventually be divided into two separate groups, the Bacteria and the Archaea. This led to the six-kingdom system and eventually to the current three-domain system, which is based on evolutionary relationships. However, the classification of eukaryotes, especially of protists, is still controversial.\n\nAs microbiology, molecular biology and virology developed, non-cellular reproducing agents were discovered, such as viruses and viroids. Whether these are considered alive has been a matter of debate; viruses lack characteristics of life such as cell membranes, metabolism and the ability to grow or respond to their environments. Viruses can still be classed into \"species\" based on their biology and genetics, but many aspects of such a classification remain controversial.\n\nIn the 1960s a trend called cladistics emerged, arranging taxa based on clades in an evolutionary or phylogenetic tree.\n\nIn systems of scientific classification, Biota is the superdomain that classifies all life.\n\nCells are the basic unit of structure in every living thing, and all cells arise from pre-existing cells by division. Cell theory was formulated by Henri Dutrochet, Theodor Schwann, Rudolf Virchow and others during the early nineteenth century, and subsequently became widely accepted. The activity of an organism depends on the total activity of its cells, with energy flow occurring within and between them. Cells contain hereditary information that is carried forward as a genetic code during cell division.\n\nThere are two primary types of cells. Prokaryotes lack a nucleus and other membrane-bound organelles, although they have circular DNA and ribosomes. Bacteria and Archaea are two domains of prokaryotes. The other primary type of cells are the eukaryotes, which have distinct nuclei bound by a nuclear membrane and membrane-bound organelles, including mitochondria, chloroplasts, lysosomes, rough and smooth endoplasmic reticulum, and vacuoles. In addition, they possess organized chromosomes that store genetic material. All species of large complex organisms are eukaryotes, including animals, plants and fungi, though most species of eukaryote are protist microorganisms. The conventional model is that eukaryotes evolved from prokaryotes, with the main organelles of the eukaryotes forming through endosymbiosis between bacteria and the progenitor eukaryotic cell.\n\nThe molecular mechanisms of cell biology are based on proteins. Most of these are synthesized by the ribosomes through an enzyme-catalyzed process called protein biosynthesis. A sequence of amino acids is assembled and joined together based upon gene expression of the cell's nucleic acid. In eukaryotic cells, these proteins may then be transported and processed through the Golgi apparatus in preparation for dispatch to their destination.\n\nCells reproduce through a process of cell division in which the parent cell divides into two or more daughter cells. For prokaryotes, cell division occurs through a process of fission in which the DNA is replicated, then the two copies are attached to parts of the cell membrane. In eukaryotes, a more complex process of mitosis is followed. However, the end result is the same; the resulting cell copies are identical to each other and to the original cell (except for mutations), and both are capable of further division following an interphase period.\n\nMulticellular organisms may have first evolved through the formation of colonies like cells. These cells can form group organisms through cell adhesion. The individual members of a colony are capable of surviving on their own, whereas the members of a true multi-cellular organism have developed specializations, making them dependent on the remainder of the organism for survival. Such organisms are formed clonally or from a single germ cell that is capable of forming the various specialized cells that form the adult organism. This specialization allows multicellular organisms to exploit resources more efficiently than single cells. In January 2016, scientists reported that, about 800 million years ago, a minor genetic change in a single molecule, called GK-PID, may have allowed organisms to go from a single cell organism to one of many cells.\n\nCells have evolved methods to perceive and respond to their microenvironment, thereby enhancing their adaptability. Cell signaling coordinates cellular activities, and hence governs the basic functions of multicellular organisms. Signaling between cells can occur through direct cell contact using juxtacrine signalling, or indirectly through the exchange of agents as in the endocrine system. In more complex organisms, coordination of activities can occur through a dedicated nervous system.\n\nThough life is confirmed only on Earth, many think that extraterrestrial life is not only plausible, but probable or inevitable. Other planets and moons in the Solar System and other planetary systems are being examined for evidence of having once supported simple life, and projects such as SETI are trying to detect radio transmissions from possible alien civilizations. Other locations within the Solar System that may host microbial life include the subsurface of Mars, the upper atmosphere of Venus, and subsurface oceans on some of the moons of the giant planets.\nBeyond the Solar System, the region around another main-sequence star that could support Earth-like life on an Earth-like planet is known as the habitable zone. The inner and outer radii of this zone vary with the luminosity of the star, as does the time interval during which the zone survives. Stars more massive than the Sun have a larger habitable zone, but remain on the main sequence for a shorter time interval. Small red dwarfs have the opposite problem, with a smaller habitable zone that is subject to higher levels of magnetic activity and the effects of tidal locking from close orbits. Hence, stars in the intermediate mass range such as the Sun may have a greater likelihood for Earth-like life to develop. The location of the star within a galaxy may also affect the likelihood of life forming. Stars in regions with a greater abundance of heavier elements that can form planets, in combination with a low rate of potentially habitat-damaging supernova events, are predicted to have a higher probability of hosting planets with complex life. The variables of the Drake equation are used to discuss the conditions in planetary systems where civilization is most likely to exist. Use of the equation to predict the amount of extraterrestrial life, however, is difficult; because many of the variables are unknown, the equation functions as more of a mirror to what its user already thinks. As a result, the number of civilizations in the galaxy can be estimated as low as 9.1 x 10^-11 or as high as 156 million; for the calculations, see Drake equation.\n\nArtificial life is the simulation of any aspect of life, as through computers, robotics, or biochemistry. The study of artificial life imitates traditional biology by recreating some aspects of biological phenomena. Scientists study the logic of living systems by creating artificial environments—seeking to understand the complex information processing that defines such systems. While life is, by definition, alive, artificial life is generally referred to as data confined to a digital environment and existence.\n\nSynthetic biology is a new area of biotechnology that combines science and biological engineering. The common goal is the design and construction of new biological functions and systems not found in nature. Synthetic biology includes the broad redefinition and expansion of biotechnology, with the ultimate goals of being able to design and build engineered biological systems that process information, manipulate chemicals, fabricate materials and structures, produce energy, provide food, and maintain and enhance human health and the environment.\n\nDeath is the permanent termination of all vital functions or life processes in an organism or cell. It can occur as a result of an accident, medical conditions, biological interaction, malnutrition, poisoning, senescence, or suicide. After death, the remains of an organism re-enter the biogeochemical cycle. Organisms may be consumed by a predator or a scavenger and leftover organic material may then be further decomposed by detritivores, organisms that recycle detritus, returning it to the environment for reuse in the food chain.\n\nOne of the challenges in defining death is in distinguishing it from life. Death would seem to refer to either the moment life ends, or when the state that follows life begins. However, determining when death has occurred is difficult, as cessation of life functions is often not simultaneous across organ systems. Such determination therefore requires drawing conceptual lines between life and death. This is problematic, however, because there is little consensus over how to define life. The nature of death has for millennia been a central concern of the world's religious traditions and of philosophical inquiry. Many religions maintain faith in either a kind of afterlife or reincarnation for the soul, or resurrection of the body at a later date.\n\nExtinction is the process by which a group of taxa or species dies out, reducing biodiversity. The moment of extinction is generally considered the death of the last individual of that species. Because a species' potential range may be very large, determining this moment is difficult, and is usually done retrospectively after a period of apparent absence. Species become extinct when they are no longer able to survive in changing habitat or against superior competition. In Earth's history, over 99% of all the species that have ever lived are extinct; however, mass extinctions may have accelerated evolution by providing opportunities for new groups of organisms to diversify.\n\nFossils are the preserved remains or traces of animals, plants, and other organisms from the remote past. The totality of fossils, both discovered and undiscovered, and their placement in fossil-containing rock formations and sedimentary layers (strata) is known as the \"fossil record\". A preserved specimen is called a fossil if it is older than the arbitrary date of 10,000 years ago. Hence, fossils range in age from the youngest at the start of the Holocene Epoch to the oldest from the Archaean Eon, up to 3.4 billion years old.\n\n\n", "id": "18393", "title": "Life"}
{"url": "https://en.wikipedia.org/wiki?curid=2169038", "text": "Homochirality\n\nHomochirality describes a geometric property of some materials that are composed of chiral units. Chiral bodies are objects which are non-superposable on their mirror images. For example, left and right hands are chiral. A substance is said to be \"homochiral\" if all the constituent units have the same chiral form (enantiomer).\n\nIn biology, homochirality is a common property of amino acids and sugars; almost all biologically produced chiral amino acids are -chiral, while sugars are -chiral. While the origin of this phenomenon is not clearly understood, many different mechanism for the emergence of homochirality are proposed. One of the difficulties in determining the correct mechanism for the origin of homochirality stems from the ambiguity in its time line, and it is not clear when homochirality occurred in comparison to the origin of life.\n\nIt is unclear if homochirality has a purpose; however it appears to be a form of information storage. One suggestion is that it reduces entropy barriers in the formation of large organized molecules. It has been experimentally verified that amino acids form large aggregates in larger abundance from enantiopure substrates than from racemic ones.\n\nHomochirality is said to evolve in three distinct steps: mirror-symmetry breaking creates a minute enantiomeric imbalance and is key to homochirality, chiral amplification is a process of enantiomeric enrichment and chiral transmission allows the transfer of chirality of one set of molecules to another.\n\nIt is also entirely possible that homochirality is simply a result of the natural autoamplification process of life —that either the formation of life as preferring one chirality or the other was a chance rare event which happened to occur with the chiralities we observe, or that all chiralities of life emerged rapidly but due to catastrophic events and strong competition, the other unobserved chiral preferences were wiped out by the preponderance and metabolic, enantiomeric enrichment from the 'winning' chirality choices. The emergence of chirality consensus as a natural autoamplification process has been associated with the 2nd law of thermodynamics.\n\nKnown mechanisms for the production of non-racemic mixtures from racemic starting materials include: asymmetric physical laws, such as the electroweak interaction; asymmetric environments, such as those caused by circularly polarized light, quartz crystals, or the Earth's rotation; and statistical fluctuations during racemic synthesis. Once established, chirality would be selected for. A small enantiomeric excess can be amplified into a large one by asymmetric autocatalysis, such as in the Soai reaction. In asymmetric autocatalysis, the catalyst is a chiral molecule, which means that a chiral molecule is catalysing its own production. An initial enantiomeric excess, such as can be produced by polarized light, then allows the more abundant enantiomer to outcompete the other.\n\nOne supposition is that the discovery of an enantiomeric imbalance in molecules in the Murchison meteorite supports an extraterrestrial origin of homochirality: there is evidence for the existence of circularly polarized light originating from Mie scattering on aligned interstellar dust particles which may trigger the formation of an enantiomeric excess within chiral material in space. Another speculation (the Vester-Ulbricht hypothesis) suggests that fundamental chirality of physical processes such as that of the beta decay (see Parity violation) leads to slightly different half-lives of biologically relevant molecules. Homochirality may also result from spontaneous absolute asymmetric synthesis.\n\nIn 1953, Charles Frank proposed a model to demonstrate that homochirality is a consequence of autocatalysis. In his model the and enantiomers of a chiral molecule are autocatalytically produced from an achiral molecule A\n\nwhile suppressing each other through a reaction that he called \"mutual antagonism\"\nIn this model the racemic state is unstable in the sense that the slightest enantiomeric excess will be amplified to a completely homochiral state. This can be shown by computing the reaction rates from the law of mass action:\nwhere formula_2 is the rate constant for the autocatalytic reactions, formula_3 is the rate constant for mutual antagonism reaction, and the concentration of A is kept constant for simplicity. By defining the enantiomeric excess formula_4 as\nwe can compute the rate of change of enatiomeric excess using chain rule from the rate of change of the concentrations of enantiomeres and .\nLinear stability analysis of this equation shows that the racemic state formula_7 is unstable. Starting from almost everywhere in the concentration space, the system evolves to a homochiral state.\n\nIt is generally understood that autocatalysis alone does not yield to homochirality, and the presence of the mutually antagonistic relationship between the two enantiomers is necessary for the instability of the racemic mixture. However, recent studies show that homochirality could be achieved from autocatalysis in the absence of the mutually antagonistic relationship, but the underlying mechanism for symmetry-breaking is different.\n\nLaboratory experiments exist demonstrating how in certain autocatalytic reaction systems the presence of a small amount of reaction product with enantiomeric excess at the start of the reaction can result in a much larger enantiomeric excess at the end of the reaction. In the Soai reaction, pyrimidine-5-carbaldehyde (\"Scheme 1\") is alkylated by diisopropylzinc to the corresponding pyrimidyl alcohol. Because the initial reaction product is also an effective catalyst the reaction is autocatalytic. The presence of just 0.2 equivalent of the alcohol S-enantiomer at the start of the reaction is sufficient to amplify the enantiomeric excess to 93%.\n\nAnother study concerns the proline catalyzed aminoxylation of propionaldehyde by nitrosobenzene (\"scheme 2\"). In this system too the presence of enantioenriched catalyst drives the reaction towards one of the two possible optical isomers.\n\nSerine octamer clusters are also contenders. These clusters of 8 serine molecules appear in mass spectrometry with an unusual homochiral preference, however there is no evidence that such clusters exist under non-ionizing conditions and amino acid phase behavior is far more prebiotically relevant. The recent observation that partial sublimation of a 10% enantioenriched sample of leucine results in up to 82% enrichment in the sublimate shows that enantioenrichment of amino acids could occur in space. Partial sublimation processes can take place on the surface of meteors where large variations in temperature exist. This finding may have consequences for the development of the Mars Organic Detector scheduled for launch in 2013 which aims to recover trace amounts of amino acids from the Mars surface exactly by a sublimation technique.\n\nA high asymmetric amplification of the enantiomeric excess of sugars are also present in the amino acid catalyzed asymmetric formation of carbohydrates\n\nOne classic study involves an experiment that takes place in the laboratory. When sodium chlorate is allowed to crystallize from water and the collected crystals examined in a polarimeter, each crystal turns out to be chiral and either the form or the form. In an ordinary experiment the amount of crystals collected equals the amount of crystals (corrected for statistical effects). However, when the sodium chlorate solution is stirred during the crystallization process the crystals are either exclusively or exclusively . In 32 consecutive crystallization experiments 14 experiments deliver -crystals and 18 others -crystals. The explanation for this symmetry breaking is unclear but is related to autocatalysis taking place in the nucleation process.\n\nIn a related experiment, a crystal suspension of a racemic amino acid derivative continuously stirred, results in a 100% crystal phase of one of the enantiomers because the enantiomeric pair is able to equilibrate in solution (compare with dynamic kinetic resolution).\n\nMany strategies in asymmetric synthesis are built on chiral transmission. Especially important is the so-called organocatalysis of organic reactions by proline for example in Mannich reactions.\n\nThere exists no theory elucidating correlations among -amino acids. If one takes, for example, alanine, which has a small methyl group, and phenylalanine, which has a larger benzyl group, a simple question is in what aspect, -alanine resembles -phenylalanine more than -phenylalanine, and what kind of mechanism causes the selection of all -amino acids. Because it might be possible that alanine was and phenylalanine was .\n\nIt was reported in 2004 that excess racemic ,-asparagine (Asn), which spontaneously forms crystals of either isomer during recrystallization, induces asymmetric resolution of a co-existing racemic amino acid such as arginine (Arg), aspartic acid (Asp), glutamine (Gln), histidine (His), leucine (Leu), methionine (Met), phenylalanine (Phe), serine (Ser), valine (Val), tyrosine (Tyr), and tryptophan (Trp). The enantiomeric excess {ee=100x(-)/(+)} of these amino acids was correlated almost linearly with that of the inducer, i.e., Asn. When recrystallizations from a mixture of 12 ,-amino acids (Ala, Asp, Arg, Glu, Gln, His, Leu, Met, Ser, Val, Phe, and Tyr) and excess ,-Asn were made, all amino acids with the same configuration with Asn were preferentially co-crystallized. It was incidental whether the enrichment took place in - or -Asn, however, once the selection was made, the co-existing amino acid with the same configuration at the α-carbon was preferentially involved because of thermodynamic stability in the crystal formation. The maximal ee was reported to be 100%. Based on these results, it is proposed that a mixture of racemic amino acids causes spontaneous and effective optical resolution, even if asymmetric synthesis of a single amino acid does not occur without an aid of an optically active molecule.\n\nThis is the first study elucidating reasonably the formation of chirality from racemic amino acids with experimental evidences.\n\nThis term was introduced by Kelvin in 1904, the year that he published his Baltimore Lecture of 1884. Kelvin used the term homochirality as a relationship between two molecules, i.e. two molecule are homochiral if they have the same chirality. Recently, however, homochiral has been used in the same sense as enantiomerically pure. This is permitted in some journals (but not encouraged), its meaning changing into the preference of a process or system for a single optical isomer in a pair of isomers in these journals.\n\n\n", "id": "2169038", "title": "Homochirality"}
{"url": "https://en.wikipedia.org/wiki?curid=32703814", "text": "Chirality\n\nChirality is a property of asymmetry important in several branches of science. The word \"chirality\" is derived from the Greek , \"hand,\" a familiar chiral object.\n\nAn object or a system is chiral if it is distinguishable from its mirror image; that is, it cannot be superposed onto it. Conversely, a mirror image of an \"achiral\" object, such as a sphere, cannot be distinguished from the object. A chiral object and its mirror image are called enantiomorphs (Greek, \"opposite forms\") or, when referring to molecules, enantiomers. A non-chiral object is called achiral (sometimes also amphichiral) and can be superposed on its mirror image. If the object is non-chiral and is imagined as being colored blue and its mirror image is imagined as colored yellow, then by a series of rotations and translations the two can be superposed, producing green, with none of the original colors remaining.\n\nThe term was first used by Lord Kelvin in 1893 in the second Robert Boyle Lecture at the Oxford University Junior Scientific Club which was published in 1894:\n\nHuman hands are perhaps the most universally recognized example of chirality. The left hand is a non-superimposable mirror image of the right hand; no matter how the two hands are oriented, it is impossible for all the major features of both hands to coincide across all axes. This difference in symmetry becomes obvious if someone attempts to shake the right hand of a person using their left hand, or if a left-handed glove is placed on a right hand. In mathematics, \"chirality\" is the property of a figure that is not identical to its mirror image.\n\nIn mathematics, a figure is chiral (and said to have chirality) if it cannot be mapped to its mirror image by rotations and translations alone. For example, a right shoe is different from a left shoe, and clockwise is different from anticlockwise. See for a full mathematical definition.\n\nA chiral object and its mirror image are said to be enantiomorphs. The word \"enantiomorph\" stems from the Greek (enantios) 'opposite' + (morphe) 'form'. A non-chiral figure is called achiral or amphichiral.\n\nThe helix (and by extension a spun string, a screw, a propeller, etc.) and Möbius strip are chiral two-dimensional objects in three-dimensional ambient space. The J, L, S and Z-shaped \"tetrominoes\" of the popular video game Tetris also exhibit chirality, but only in a two-dimensional space.\n\nMany other familiar objects exhibit the same chiral symmetry of the human body, such as gloves, glasses (where two lenses differ in prescription), and shoes. A similar notion of chirality is considered in knot theory, as explained below.\n\nSome chiral three-dimensional objects, such as the helix, can be assigned a right or left handedness, according to the right-hand rule.\n\nIn geometry a figure is achiral if and only if its symmetry group contains at least one \"orientation-reversing\" isometry.\nIn two dimensions, every figure that possesses an axis of symmetry is achiral, and it can be shown that every \"bounded\" achiral figure must have an axis of symmetry.\nIn three dimensions, every figure that possesses a plane of symmetry or a center of symmetry is achiral. There are, however, achiral figures lacking both plane and center of symmetry.\nIn terms of point groups, all chiral figures lack an improper axis of rotation (S). This means that they cannot contain a center of inversion (i) or a mirror plane (σ). Only figures with a point group designation of C, C, D, T, O, or I can be chiral.\n\nA knot is called achiral if it can be continuously deformed into its mirror image, otherwise it is called chiral. For example, the unknot and the figure-eight knot are achiral, whereas the trefoil knot is chiral.\n\nIn physics, chirality may be found in the spin of a particle, where the handedness of the object is determined by the direction in which the particle spins. Not to be confused with helicity, which is the projection of the spin along the linear momentum of a subatomic particle, chirality is a purely quantum mechanical phenomenon like spin. Although both can have left-handed or right-handed properties, only in the massless case do they have a simple relation. In particular for a massless particle the helicity is the same as the chirality while for an antiparticle they have opposite sign.\n\nThe handedness in both chirality and helicity relate to the rotation of a particle while it proceeds in linear motion with reference to the human hands. The thumb of the hand points towards the direction of linear motion whilst the fingers curl into the palm, representing the direction of rotation of the particle (i.e. clockwise and counterclockwise). Depending on the linear and rotational motion, the particle can either be defined by left-handedness (ex. translating leftwards and rotating counterclockwise) or right-handedness (ex. translating in the right direction and rotating clockwise). A symmetry transformation between the two is called parity. Invariance under parity by a Dirac fermion is called chiral symmetry.\n\nElectromagnetic wave propagation as handedness is wave polarization and described in terms of helicity (occurs as a helix). Polarization of an electromagnetic wave is the property that describes the orientation, i.e., the time-varying, direction (vector), and amplitude of the electric field vector. For a depiction, see the adjacent image.\n\nA chiral molecule is a type of molecule that has a non-superposable mirror image. The feature that is most often the cause of chirality in molecules is the presence of an asymmetric carbon atom.\n\nThe term chiral in general is used to describe the object that is non-superposable on its mirror image.\n\nIn chemistry, chirality usually refers to molecules. Two mirror images of a chiral molecule are called enantiomers or optical isomers. Pairs of enantiomers are often designated as \"right-\", \"left-handed\" or if it has no bias achiral. As polarized light passes through a chiral molecule, the plane of polarization, when viewed along the axis toward the source, will be rotated in a clockwise (to the right) or anticlockwise (to the left). A right handed rotation is dextrorotary (d); that to the left is levorotary (l). The d- and l-isomers are the same compound but are called enantiomers. An equimolar mixture of the two optical isomers will produce no net rotation of polarized light as it passes through. Left handed molecules have l- prefixed to their names; d- is prefixed to right handed molecules.\n\nMolecular chirality is of interest because of its application to stereochemistry in inorganic chemistry, organic chemistry, physical chemistry, biochemistry, and supramolecular chemistry.\n\nMore recent developments in chiral chemistry include the development of chiral inorganic nanoparticles that may have the similar tetrahedral geometry as chiral centers associated with sp3 carbon atoms traditionally associated with chiral compounds, but at larger scale. Helical and other symmetries of chiral nanomaterials were also obtained.\n\nAll of the known life-forms show specific chiral properties in chemical structures as well as macroscopic anatomy, development and behavior. In any specific organism or evolutionarily related set thereof, individual compounds, organs, or behavior are found in the same single enantiomorphic form. Deviation (having the opposite form) could be found in a small number of chemical compounds, or certain organ or behavior but that variation strictly depends upon the genetic make up of the organism. From chemical level (molecular scale), biological systems show extreme stereospecificity in synthesis, uptake, sensing, metabolic processing. A living system usually deals with two enantiomers of same compound in a drastically different way.\n\nIn biology, homochirality is a common property of amino acids and carbohydrates. The chiral protein-making amino acids, which are translated through the ribosome from genetic coding, occur in the form. However, -amino acids are also found in nature. The monosaccharides (carbohydrate-units) are commonly found in -configuration. DNA double helix is chiral (as any kind of helix is chiral), and B-form of DNA shows a right-handed turn.\n\nSometimes, when two enantiomers of a compound found in organisms, they significantly differ in their taste, smell and other biological actions. For example, (+)-Limonene found in orange (causing its smell), and (–)-Limonene found in Lemons (causing its smell), show different smells due to different biochemical interactions at human nose. (+)-Carvone is responsible for the smell of Caraway seed oil whereas (–)-carvone is responsible for smell of Spearmint oil.\n\nAlso, for artificial compounds, including medicines, in case of chiral drugs, the two enantiomers show remarkable difference in effect of their biological actions. Darvon (Dextropropoxyphene) is a painkiller, whereas its enantiomer, Novrad (Levopropoxyphene) is an anti-cough agent. In case of Penicillamine, the S-isomer used in treatment of primary chronic arthritis, Whereas the R-isomer has no therapeutic effect as well as being highly toxic.\nMacroscopic example of Chirality is found in plant kingdom, animal kingdom and all other groups of organism. A simple example is the coiling direction of any climber plants. It may be one of two possible type of helix.\nIn anatomy, chirality is found in the imperfect mirror image symmetry of many kinds of animal bodies. Organisms such as gastropods exhibit chirality in their coiled shells, resulting in an asymmetrical appearance. Over 90% of gastropod species have dextral (right-handed) shells in their coiling, but a small minority of species and genera are virtually always sinistral (left-handed). A very few species (for example \"Amphidromus perversus\") show an equal mixture of dextral and sinistral individuals.\n\nIn humans, chirality (also referred to as handedness or laterality) is an attribute of humans defined by their unequal distribution of fine motor skill between the left and right hands. An individual who is more dexterous with the right hand is called \"right-handed\", and one who is more skilled with the left is said to be \"left-handed\". Chirality is also seen in the study of facial asymmetry.\n\nIn flatfish, the Summer flounder or fluke are left-eyed, while halibut are right-eyed.\n\n\n", "id": "32703814", "title": "Chirality"}
{"url": "https://en.wikipedia.org/wiki?curid=1170166", "text": "Chirality (chemistry)\n\nChirality is a geometric property of some molecules and ions. A chiral molecule/ion is non-superimposable on its mirror image. The presence of an asymmetric carbon center is one of several structural features that induce chirality in organic and inorganic molecules. The term \"chirality\" is derived from the Greek word for hand, χειρ (kheir).\n\nThe mirror images of a chiral molecule/ion are called enantiomers or optical isomers. Individual enantiomers are often designated as either right-handed or left-handed. Chirality is an essential consideration when discussing the stereochemistry in organic and inorganic chemistry. The concept is of great practical importance because most biomolecules and pharmaceuticals are chiral.\n\nChiral molecules and ions are described by various ways of designating their absolute configuration, which codify either the entity's geometry or its ability to rotate plane-polarized light, a common technique in studying chirality.\n\nChirality is based on molecular symmetry elements. Specifically, a chiral compound can contain no improper axis of rotation (S), which includes planes of symmetry and inversion center. Chiral molecules are always dissymmetric (lacking S) but not always asymmetric (lacking all symmetry elements except the trivial identity). Asymmetric molecules are always chiral.\n\nIn general, chiral molecules have point chirality at a single \"stereogenic\" atom, which has four different substituents. The two enantiomers of such compounds are said to have different absolute configurations at this center. This center is thus stereogenic (i.e., a grouping within a molecular entity that may be considered a focus of stereoisomerism). The stereogenic atom is usually carbon, as in many biological molecules. However chirality can exist in any atom, including metals (as in many chiral coordination compounds), phosphorus, or sulfur. Chiral nitrogen is equally possible, although the effects of nitrogen inversion can make many of these compounds impossible to isolate.\n\nWhile the presence of a stereogenic atom describes the great majority of cases, many variations and exceptions exist. For instance it is not necessary for the chiral substance to have a stereogenic atom. Examples include 1-bromo-3-chloro-5-fluoroadamantane, methylethylphenyltetrahedrane, certain calixarenes and fullerenes, which have inherent chirality. The C-symmetric species 1,1'-bi-2-naphthol (BINOL), 1,3-dichloro-allene have axial chirality. (\"E\")-cyclooctene and many ferrocenes have planar chirality.\n\nWhen the optical rotation for an enantiomer is too low for practical measurement, the species is said to exhibit cryptochirality.\n\nEven isotopic differences must be considered when examining chirality. Illustrative is the derivative of benzyl alcohol PhCHDOH, which is chiral. The \"S\" enantiomer has [α] = +0.715°.\n\nMany biologically active molecules are chiral, including the naturally occurring amino acids (the building blocks of proteins) and sugars. In biological systems, most of these compounds are of the same chirality: most amino acids are levorotatory () and sugars are dextrorotatory (). Typical naturally occurring proteins, made of amino acids, are known as \"left-handed proteins\", whereas amino acids produce \"right-handed proteins\". -amino acids are very rare in nature and have only been found in small peptides attached to bacteria cell walls.\n\nThe origin of this homochirality in biology is the subject of much debate. Most scientists believe that Earth life's \"choice\" of chirality was purely random, and that if carbon-based life forms exist elsewhere in the universe, their chemistry could theoretically have opposite chirality. However, there is some suggestion that early amino acids could have formed in comet dust. In this case, circularly polarised radiation (which makes up 17% of stellar radiation) could have caused the selective destruction of one chirality of amino acids, leading to a selection bias which ultimately resulted in all life on Earth being homochiral.\n\nEnzymes, which are chiral, often distinguish between the two enantiomers of a chiral substrate. One could imagine an enzyme as having a glove-like cavity that binds a substrate. If this glove is right-handed, then one enantiomer will fit inside and be bound, whereas the other enantiomer will have a poor fit and is unlikely to bind.\n\n-forms of amino acids tend to be tasteless, whereas -forms tend to taste sweet. Spearmint leaves contain the -enantiomer of the chemical carvone or \"R\"-(–)-carvone and caraway seeds contain the -enantiomer or \"S\"-(+)-carvone. These smell different to most people because our olfactory receptors are chiral.\n\nChirality is important in context of ordered phases as well, for example the addition of a small amount of an optically active molecule to a nematic phase (a phase that has long range orientational order of molecules) transforms that phase to a chiral nematic phase (or cholesteric phase). Chirality in context of such phases in polymeric fluids has also been studied in this context.\n\nChirality is a symmetry property, not a characteristic of any part of the periodic table. Thus many inorganic materials, molecules, and ions are chiral. Quartz is an example from the mineral kingdom. Such noncentric materials are of interest for applications in nonlinear optics.\n\nIn the areas of coordination chemistry and organometallic chemistry, chirality is pervasive and of practical importance. A famous example is tris(bipyridine)ruthenium(II) complex in which the three bipyridine ligands adopt a chiral propeller-like arrangement. The two enantiomers of complexes such as [Ru(2,2′-bipyridine)] may be designated as Λ (capital lambda, the Greek version of \"L\") for a left-handed twist of the propeller described by the ligands, and Δ (capital delta, Greek \"D\") for a right-handed twist (pictured).\n\nChiral ligands confer chirality to a metal complex, as illustrated by metal-amino acid complexes. If the metal exhibits catalytic properties, its combination with a chiral ligand is the basis of asymmetric catalysis.\n\nThe term \"optical activity\" is derived from the interaction of chiral materials with polarized light. In a solution, the (−)-form, or levorotatory form, of an optical isomer rotates the plane of a beam of linearly polarized light counterclockwise. The (+)-form, or dextrorotatory form, of an optical isomer does the opposite. The rotation of light is measured using a polarimeter and is expressed as the optical rotation.\n\n\nThe rotation of plane polarized light by chiral substances was first observed by Jean-Baptiste Biot in 1815, and gained considerable importance in the sugar industry, analytical chemistry, and pharmaceuticals. Louis Pasteur deduced in 1848 that this phenomenon has a molecular basis. The term \"chirality\" itself was coined by Lord Kelvin in 1894. Different enantiomers or diastereomers of a compound were formerly called optical isomers due to their different optical properties. At one time, chirality was thought to be associated with organic chemistry, but this misconception was overthrown by the resolution of a purely inorganic compound, hexol, by Alfred Werner.\n\n\n", "id": "1170166", "title": "Chirality (chemistry)"}
{"url": "https://en.wikipedia.org/wiki?curid=177052", "text": "Immortality\n\nImmortality is eternal life, being exempt from death, unending existence. Some modern species may possess biological immortality.\n\nCertain scientists, futurists, and philosophers have theorized about the immortality of the human body, with some suggesting that human immortality may be achievable in the first few decades of the 21st century. Other advocates believe that life extension is a more achievable goal in the short term, with immortality awaiting further research breakthroughs. The absence of aging would provide humans with biological immortality, but not invulnerability to death by disease or physical trauma; although mind uploading could solve that issue if it proved possible. Whether the process of internal endoimmortality is delivered within the upcoming years depends chiefly on research (and in neuron research in the case of endoimmortality through an immortalized cell line) in the former view and perhaps is an awaited goal in the latter case.\n\nIn religious contexts, immortality is often stated to be one of the promises of God (or other deities) to human beings who show goodness or else follow divine law. What form an unending human life would take, or whether an immaterial soul exists and possesses immortality, has been a major point of focus of religion, as well as the subject of speculation, fantasy, and debate.\n\nLife extension technologies promise a path to complete rejuvenation. Cryonics holds out the hope that the dead can be revived in the future, following sufficient medical advancements. While, as shown with creatures such as hydra and planarian worms, it is indeed possible for a creature to be biologically immortal, it is not known if it is possible for humans.\n\nMind uploading is the transference of brain states from a human brain to an alternative medium providing similar functionality. Assuming the process to be possible and repeatable, this would provide immortality to the computation of the original brain, as predicted by futurists such as Ray Kurzweil.\n\nThe belief in an \"afterlife\" is a fundamental tenet of most religions, including Hinduism, Buddhism, Jainism, Sikhism, Christianity, Zoroastrianism, Islam, Judaism, and the Bahá'í Faith; however, the concept of an immortal soul is not. The \"soul\" itself has different meanings and is not used in the same way in different religions and different denominations of a religion. For example, various branches of Christianity have disagreeing views on the soul's immortality and its relation to the body.\n\nPhysical immortality is a state of life that allows a person to avoid death and maintain conscious thought. It can mean the unending existence of a person from a physical source other than organic life, such as a computer. Active pursuit of physical immortality can either be based on scientific trends, such as cryonics, digital immortality, breakthroughs in rejuvenation or predictions of an impending technological singularity, or because of a spiritual belief, such as those held by Rastafarians or Rebirthers.\n\nThere are three main causes of death: aging, disease and physical trauma. Such issues can be resolved with the solutions provided in research to any end providing such alternate theories at present that require unification.\n\nAubrey de Grey, a leading researcher in the field, defines aging as \"a collection of cumulative changes to the molecular and cellular structure of an adult organism, which result in essential metabolic processes, but which also, once they progress far enough, increasingly disrupt metabolism, resulting in pathology and death.\" The current causes of aging in humans are cell loss (without replacement), DNA damage, oncogenic nuclear mutations and epimutations, cell senescence, mitochondrial mutations, lysosomal aggregates, extracellular aggregates, random extracellular cross-linking, immune system decline, and endocrine changes. Eliminating aging would require finding a solution to each of these causes, a program de Grey calls engineered negligible senescence. There is also a huge body of knowledge indicating that change is characterized by the loss of molecular fidelity.\n\nDisease is theoretically surmountable via technology. In short, it is an abnormal condition affecting the body of an organism, something the body shouldn't typically have to deal with its natural make up. Human understanding of genetics is leading to cures and treatments for a myriad of previously incurable diseases. The mechanisms by which other diseases do their damage are becoming better understood. Sophisticated methods of detecting diseases early are being developed. Preventative medicine is becoming better understood. Neurodegenerative diseases like Parkinson's and Alzheimer's may soon be curable with the use of stem cells. Breakthroughs in cell biology and telomere research are leading to treatments for cancer. Vaccines are being researched for AIDS and tuberculosis. Genes associated with type 1 diabetes and certain types of cancer have been discovered, allowing for new therapies to be developed. Artificial devices attached directly to the nervous system may restore sight to the blind. Drugs are being developed to treat a myriad of other diseases and ailments.\n\nPhysical trauma would remain as a threat to perpetual physical life, as an otherwise immortal person would still be subject to unforeseen accidents or catastrophes. The speed and quality of paramedic response remains a determining factor in surviving severe trauma. A body that could automatically repair itself from severe trauma, such as speculated uses for nanotechnology, would mitigate this factor.\nBeing the seat of consciousness, the brain cannot be risked to trauma if a continuous physical life is to be maintained. This aversion to trauma risk to the brain would naturally result in significant behavioral changes that would render physical immortality undesirable for some people.\n\nOrganisms otherwise unaffected by these causes of death would still face the problem of obtaining sustenance (whether from currently available agricultural processes or from hypothetical future technological processes) in the face of changing availability of suitable resources as environmental conditions change. After avoiding aging, disease, and trauma, you could still starve to death.\n\nIf there is no limitation on the degree of gradual mitigation of risk then it is possible that the cumulative probability of death over an infinite horizon is less than certainty, even when the risk of fatal trauma in any finite period is greater than zero. Mathematically, this is an aspect of achieving \"actuarial escape velocity\"\n\nBiological immortality is an absence of aging, specifically the absence of a sustained increase in rate of mortality as a function of chronological age. A cell or organism that does not experience aging, or ceases to age at some point, is biologically immortal.\n\nBiologists have chosen the word immortal to designate cells that are not limited by the Hayflick limit, where cells no longer divide because of DNA damage or shortened telomeres. The first and still most widely used immortal cell line is HeLa, developed from cells taken from the malignant cervical tumor of Henrietta Lacks without her consent in 1951. Prior to the 1961 work of Leonard Hayflick, there was the erroneous belief fostered by Alexis Carrel that all normal somatic cells are immortal. By preventing cells from reaching senescence one can achieve biological immortality; telomeres, a \"cap\" at the end of DNA, are thought to be the cause of cell aging. Every time a cell divides the telomere becomes a bit shorter; when it is finally worn down, the cell is unable to split and dies. Telomerase is an enzyme which rebuilds the telomeres in stem cells and cancer cells, allowing them to replicate an infinite number of times. No definitive work has yet demonstrated that telomerase can be used in human somatic cells to prevent healthy tissues from aging. On the other hand, scientists hope to be able to grow organs with the help of stem cells, allowing organ transplants without the risk of rejection, another step in extending human life expectancy. These technologies are the subject of ongoing research, and are not yet realized.\n\nLife defined as biologically immortal is still susceptible to causes of death besides aging, including disease and trauma, as defined above. Notable immortal species include:\n\nAs the existence of biologically immortal species demonstrates, there is no thermodynamic necessity for senescence: a defining feature of life is that it takes in free energy from the environment and unloads its entropy as waste. Living systems can even build themselves up from seed, and routinely repair themselves. Aging is therefore presumed to be a byproduct of evolution, but why mortality should be selected for remains a subject of research and debate. Programmed cell death and the telomere \"end replication problem\" are found even in the earliest and simplest of organisms. This may be a tradeoff between selecting for cancer and selecting for aging.\n\nModern theories on the evolution of aging include the following:\n\nThere are some known naturally occurring and artificially produced chemicals that may increase the lifetime or life-expectancy of a person or organism, such as resveratrol.\n\nSome scientists believe that boosting the amount or proportion of telomerase in the body, a naturally forming enzyme that helps maintain the protective caps at the ends of chromosomes, could prevent cells from dying and so may ultimately lead to extended, healthier lifespans. A team of researchers at the Spanish National Cancer Centre (Madrid) tested the hypothesis on mice. It was found that those mice which were genetically engineered to produce 10 times the normal levels of telomerase lived 50% longer than normal mice.\n\nIn normal circumstances, without the presence of telomerase, if a cell divides repeatedly, at some point all the progeny will reach their Hayflick limit. With the presence of telomerase, each dividing cell can replace the lost bit of DNA, and any single cell can then divide unbounded. While this unbounded growth property has excited many researchers, caution is warranted in exploiting this property, as exactly this same unbounded growth is a crucial step in enabling cancerous growth. If an organism can replicate its body cells faster, then it would theoretically stop aging.\n\nEmbryonic stem cells express telomerase, which allows them to divide repeatedly and form the individual. In adults, telomerase is highly expressed in cells that need to divide regularly (e.g., in the immune system), whereas most somatic cells express it only at very low levels in a cell-cycle dependent manner.\n\nTechnological immortality is the prospect for much longer life spans made possible by scientific advances in a variety of fields: nanotechnology, emergency room procedures, genetics, biological engineering, regenerative medicine, microbiology, and others. Contemporary life spans in the advanced industrial societies are already markedly longer than those of the past because of better nutrition, availability of health care, standard of living and bio-medical scientific advances. Technological immortality predicts further progress for the same reasons over the near term. An important aspect of current scientific thinking about immortality is that some combination of human cloning, cryonics or nanotechnology will play an essential role in extreme life extension. Robert Freitas, a nanorobotics theorist, suggests tiny medical nanorobots could be created to go through human bloodstreams, find dangerous things like cancer cells and bacteria, and destroy them. Freitas anticipates that gene-therapies and nanotechnology will eventually make the human body effectively self-sustainable and capable of living indefinitely in empty space, short of severe brain trauma. This supports the theory that we will be able to continually create biological or synthetic replacement parts to replace damaged or dying ones. Future advances in nanomedicine could give rise to life extension through the repair of many processes thought to be responsible for aging. K. Eric Drexler, one of the founders of nanotechnology, postulated cell repair devices, including ones operating within cells and utilizing as yet hypothetical biological machines, in his 1986 book Engines of Creation. Raymond Kurzweil, a futurist and transhumanist, stated in his book \"The Singularity Is Near\" that he believes that advanced medical nanorobotics could completely remedy the effects of aging by 2030. According to Richard Feynman, it was his former graduate student and collaborator Albert Hibbs who originally suggested to him (circa 1959) the idea of a \"medical\" use for Feynman's theoretical micromachines (see nanobiotechnology). Hibbs suggested that certain repair machines might one day be reduced in size to the point that it would, in theory, be possible to (as Feynman put it) \"swallow the doctor\". The idea was incorporated into Feynman's 1959 essay \"There's Plenty of Room at the Bottom.\"\n\nCryonics, the practice of preserving organisms (either intact specimens or only their brains) for possible future revival by storing them at cryogenic temperatures where metabolism and decay are almost completely stopped, can be used to 'pause' for those who believe that life extension technologies will not develop sufficiently within their lifetime. Ideally, cryonics would allow clinically dead people to be brought back in the future after cures to the patients' diseases have been discovered and aging is reversible. Modern cryonics procedures use a process called vitrification which creates a glass-like state rather than freezing as the body is brought to low temperatures. This process reduces the risk of ice crystals damaging the cell-structure, which would be especially detrimental to cell structures in the brain, as their minute adjustment evokes the individual's mind.\n\nOne idea that has been advanced involves uploading an individual's habits and memories via direct mind-computer interface. The individual's memory may be loaded to a computer or to a new organic body. Extropian futurists like Moravec and Kurzweil have proposed that, thanks to exponentially growing computing power, it will someday be possible to upload human consciousness onto a computer system, and exist indefinitely in a virtual environment. This could be accomplished via advanced cybernetics, where computer hardware would initially be installed in the brain to help sort memory or accelerate thought processes. Components would be added gradually until the person's entire brain functions were handled by artificial devices, avoiding sharp transitions that would lead to issues of identity, thus running the risk of the person to be declared dead and thus not be a legitimate owner of his or her property. After this point, the human body could be treated as an optional accessory and the program implementing the person could be transferred to any sufficiently powerful computer. Another possible mechanism for mind upload is to perform a detailed scan of an individual's original, organic brain and simulate the entire structure in a computer. What level of detail such scans and simulations would need to achieve to emulate awareness, and whether the scanning process would destroy the brain, is still to be determined. Whatever the route to mind upload, persons in this state could then be considered essentially immortal, short of loss or traumatic destruction of the machines that maintained them.\n\nTransforming a human into a cyborg can include brain implants or extracting a human processing unit and placing it in a robotic life-support system. Even replacing biological organs with robotic ones could increase life span (e.g. pace makers) and depending on the definition, many technological upgrades to the body, like genetic modifications or the addition of nanobots would qualify an individual as a cyborg. Some people believe that such modifications would make one impervious to aging and disease and theoretically immortal unless killed or destroyed.\n\nAnother approach, developed by biogerontologist Marios Kyriazis, holds that human biological immortality is an inevitable consequence of evolution. As the natural tendency is to create progressively more complex structures, there will be a time (Kyriazis claims this time is now), when evolution of a more complex human brain will be faster via a process of developmental singularity rather than through Darwinian evolution. In other words, the evolution of the human brain as we know it will cease and there will be no need for individuals to procreate and then die. Instead, a new type of development will take over, in the same individual who will have to live for many centuries in order for the development to take place. This intellectual development will be facilitated by technology such as synthetic biology, artificial intelligence and a technological singularity process.\n\nAs late as 1952, the editorial staff of the \"\" found in their compilation of the Great Books of the Western World, that \"The philosophical issue concerning immortality cannot be separated from issues concerning the existence and nature of man's soul.\" Thus, the vast majority of speculation regarding immortality before the 21st century was regarding the nature of the afterlife.\n\nImmortality in ancient Greek religion originally always included an eternal union of body and soul as can be seen in Homer, Hesiod, and various other ancient texts. The soul was considered to have an eternal existence in Hades, but without the body the soul was considered dead. Although almost everybody had nothing to look forward to but an eternal existence as a disembodied dead soul, a number of men and women were considered to have gained physical immortality and been brought to live forever in either Elysium, the Islands of the Blessed, heaven, the ocean or literally right under the ground. Among these were Amphiaraus, Ganymede, Ino, Iphigenia, Menelaus, Peleus, and a great part of those who fought in the Trojan and Theban wars. Some were considered to have died and been resurrected before they achieved physical immortality. Asclepius was killed by Zeus only to be resurrected and transformed into a major deity. In some versions of the Trojan War myth, Achilles, after being killed, was snatched from his funeral pyre by his divine mother Thetis, resurrected, and brought to an immortal existence in either Leuce, the Elysian plains, or the Islands of the Blessed. Memnon, who was killed by Achilles, seems to have received a similar fate. Alcmene, Castor, Heracles, and Melicertes were also among the figures sometimes considered to have been resurrected to physical immortality. According to Herodotus' Histories, the 7th century BC sage Aristeas of Proconnesus was first found dead, after which his body disappeared from a locked room. Later he was found not only to have been resurrected but to have gained immortality.\n\nThe philosophical idea of an immortal soul was a belief first appearing with either Pherecydes or the Orphics, and most importantly advocated by Plato and his followers. This, however, never became the general norm in Hellenistic thought. As may be witnessed even into the Christian era, not least by the complaints of various philosophers over popular beliefs, many or perhaps most traditional Greeks maintained the conviction that certain individuals were resurrected from the dead and made physically immortal and that others could only look forward to an existence as disembodied and dead, though everlasting, souls. The parallel between these traditional beliefs and the later resurrection of Jesus was not lost on the early Christians, as Justin Martyr argued: \"when we say ... Jesus Christ, our teacher, was crucified and died, and rose again, and ascended into heaven, we propose nothing different from what you believe regarding those whom you consider sons of Zeus.\" (\"1 Apol.\" 21).\n\nThe goal of Hinayana is Arhatship and Nirvana. By contrast, the goal of Mahayana is Buddhahood.\n\nAccording to one Tibetan Buddhist teaching, Dzogchen, individuals can transform the physical body into an immortal body of light called the rainbow body.\n\nChristian theology holds that Adam and Eve lost physical immortality for themselves and all their descendants in the Fall of Man, although this initial \"imperishability of the bodily frame of man\" was \"a preternatural condition\".\nChristians who profess the Nicene Creed believe that every dead person (whether they believed in Christ or not) will be resurrected from the dead at the Second Coming, and this belief is known as Universal resurrection.\n\nN.T. Wright, a theologian and former Bishop of Durham, has said many people forget the physical aspect of what Jesus promised. He told Time: \"Jesus' resurrection marks the beginning of a restoration that he will complete upon his return. Part of this will be the resurrection of all the dead, who will 'awake', be embodied and participate in the renewal. Wright says John Polkinghorne, a physicist and a priest, has put it this way: 'God will download our software onto his hardware until the time he gives us new hardware to run the software again for ourselves.' That gets to two things nicely: that the period after death (the Intermediate state) is a period when we are in God's presence but not active in our own bodies, and also that the more important transformation will be when we are again embodied and administering Christ's kingdom.\" This kingdom will consist of Heaven and Earth \"joined together in a new creation\", he said.\n\nHindus believe in an immortal soul which is reincarnated after death. According to Hinduism, people repeat a process of life, death, and rebirth in a cycle called \"samsara\". If they live their life well, their \"karma\" improves and their station in the next life will be higher, and conversely lower if they live their life poorly. After many life times of perfecting its karma, the soul is freed from the cycle and lives in perpetual bliss. There is no place of eternal torment in Hinduism, although if a soul consistently lives very evil lives, it could work its way down to the very bottom of the cycle.\n\nThere are explicit renderings in the Upanishads alluding to a physically immortal state brought about by purification, and sublimation of the 5 elements that make up the body. For example, in the Shvetashvatara Upanishad (Chapter 2, Verse 12), it is stated \"When earth, water fire, air and akasa arise, that is to say, when the five attributes of the elements, mentioned in the books on yoga, become manifest then the yogi's body becomes purified by the fire of yoga and he is free from illness, old age and death.\" This phenomenon is possible when the soul reaches enlightenment while the body and mind are still intact, an extreme rarity, and can only be achieved upon the highest most dedication, meditation and consciousness.\n\nAnother view of immortality is traced to the Vedic tradition by the interpretation of Maharishi Mahesh Yogi:\n\"That man indeed whom these (contacts)<br>do not disturb, who is even-minded in<br>pleasure and pain, steadfast, he is fit<br>for immortality, O best of men\".\n\nTo Maharishi Mahesh Yogi, the verse means, \"Once a man has become established in the understanding of the permanent reality of life, his mind rises above the influence of pleasure and pain. Such an unshakable man passes beyond the influence of death and in the permanent phase of life: he attains eternal life ... A man established in the understanding of the unlimited abundance of absolute existence is naturally free from existence of the relative order. This is what gives him the status of immortal life.\"\n\nAn Indian Tamil saint known as Vallalar claimed to have achieved immortality before disappearing forever from a locked room in 1874.\n\nMany Indian fables and tales include instances of metempsychosis—the ability to jump into another body—performed by advanced Yogis in order to live a longer life.\n\nThe traditional concept of an immaterial and immortal soul distinct from the body was not found in Judaism before the Babylonian Exile, but developed as a result of interaction with Persian and Hellenistic philosophies. Accordingly, the Hebrew word \"nephesh\", although translated as \"soul\" in some older English Bibles, actually has a meaning closer to \"living being\". \"Nephesh\" was rendered in the Septuagint as \"\" (\"psūchê\"), the Greek word for soul.\n\nThe only Hebrew word traditionally translated \"soul\" (\"nephesh\") in English language Bibles refers to a living, breathing conscious body, rather than to an immortal soul. In the New Testament, the Greek word traditionally translated \"soul\" () has substantially the same meaning as the Hebrew, without reference to an immortal soul. ‘Soul’ may refer to the whole person, the self: ‘three thousand souls’ were converted in Acts 2:41 (see Acts 3:23).\n\nThe Hebrew Bible speaks about \"Sheol\" (שאול), originally a synonym of the grave-the repository of the dead or the cessation of existence until the Resurrection. This doctrine of resurrection is mentioned explicitly only in although it may be implied in several other texts. New theories arose concerning Sheol during the intertestamental literature.\n\nThe views about immortality in Judaism is perhaps best exemplified by the various references to this in Second Temple Period. The concept of resurrection of the physical body is found in 2 Maccabees, according to which it will happen through recreation of the flesh. Resurrection of the dead also appears in detail in the extra-canonical books of Enoch, and in Apocalypse of Baruch. According to the British scholar in ancient Judaism Philip R. Davies, there is “little or no clear reference … either to immortality or to resurrection from the dead” in the Dead Sea scrolls texts. Both Josephus and the New Testament record that the Sadducees did not believe in an afterlife, but the sources vary on the beliefs of the Pharisees. The New Testament claims that the Pharisees believed in the resurrection, but does not specify whether this included the flesh or not. According to Josephus, who himself was a Pharisee, the Pharisees held that only the soul was immortal and the souls of good people will be reincarnated and “pass into other bodies,” while “the souls of the wicked will suffer eternal punishment.” Jubilees seems to refer to the resurrection of the soul only, or to a more general idea of an immortal soul.\n\nRabbinic Judaism claims that the righteous dead will be resurrected in the Messianic age with the coming of the messiah. They will then be granted immortality in a perfect world. The wicked dead, on the other hand, will not be resurrected at all. This is not the only Jewish belief about the afterlife. The Tanakh is not specific about the afterlife, so there are wide differences in views and explanations among believers.\n\nIt is repeatedly stated in Lüshi Chunqiu that death is unavoidable. Henri Maspero noted that many scholarly works frame Taoism as a school of thought focused on the quest for immortality. Isabelle Robinet asserts that Taoism is better understood as a \"way of life\" than as a religion, and that its adherents do not approach or view Taoism the way non-Taoist historians have done. In the Tractate of Actions and their Retributions, a traditional teaching, spiritual immortality can be rewarded to people who do a certain amount of good deeds and live a simple, pure life. A list of good deeds and sins are tallied to determine whether or not a mortal is worthy. Spiritual immortality in this definition allows the soul to leave the earthly realms of afterlife and go to pure realms in the Taoist cosmology.\n\nZoroastrians believe that on the fourth day after death, the human soul leaves the body and the body remains as an empty shell. Souls would go to either heaven or hell; these concepts of the afterlife in Zoroastrianism may have influenced Abrahamic religions. The Persian word for \"immortal\" is associated with the month \"Amurdad\", meaning \"deathless\" in Persian, in the Iranian calendar (near the end of July). The month of Amurdad or Ameretat is celebrated in Persian culture as ancient Persians believed the \"Angel of Immortality\" won over the \"Angel of Death\" in this month.\n\nAlcmaeon of Croton argued that the soul is continuously and ceaselessly in motion. The exact form of his argument is unclear, but it appears to have influenced Plato, Aristotle, and other later writers.\n\nPlato's Phaedo advances four arguments for the soul's immortality:\nThe Cyclical Argument, or Opposites Argument explains that Forms are eternal and unchanging, and as the soul always brings life, then it must not die, and is necessarily \"imperishable\". As the body is mortal and is subject to physical death, the soul must be its indestructible opposite. Plato then suggests the analogy of fire and cold. If the form of cold is imperishable, and fire, its opposite, was within close proximity, it would have to withdraw intact as does the soul during death. This could be likened to the idea of the opposite charges of magnets.\n\nThe Theory of Recollection explains that we possess some non-empirical knowledge (e.g. The Form of Equality) at birth, implying the soul existed before birth to carry that knowledge. Another account of the theory is found in Plato's Meno, although in that case Socrates implies anamnesis (previous knowledge of everything) whereas he is not so bold in Phaedo.\n\nThe Affinity Argument, explains that invisible, immortal, and incorporeal things are different from visible, mortal, and corporeal things. Our soul is of the former, while our body is of the latter, so when our bodies die and decay, our soul will continue to live.\n\nThe Argument from Form of Life, or The Final Argument explains that the Forms, incorporeal and static entities, are the cause of all things in the world, and all things participate in Forms. For example, beautiful things participate in the Form of Beauty; the number four participates in the Form of the Even, etc. The soul, by its very nature, participates in the Form of Life, which means the soul can never die.\n\nPlotinus offers a version of the argument that Kant calls \"The Achilles of Rationalist Psychology\". Plotinus first argues that the soul is simple, then notes that a simple being cannot decompose. Many subsequent philosophers have argued both that the soul is simple and that it must be immortal. The tradition arguably culminates with Moses Mendelssohn's Phaedon.\n\nMetochites argues that part of the soul's nature is to move itself, but that a given movement will cease only if what causes the movement is separated from the thing moved – an impossibility if they are one and the same.\n\nAvicenna argued for the distinctness of the soul and the body, and the incorruptibility of the former.\n\nThe full argument for the immortality of the soul and Aquinas' elaboration of Aristotelian theory is found in Question 75 of the First Part of the Summa Theologica.\n\nDescartes endorses the claim that the soul is simple, and also that this entails that it cannot decompose. Descartes does not address the possibility that the soul might suddenly disappear.\n\nIn early work, Leibniz endorses a version of the argument from the simplicity of the soul to its immortality, but like his predecessors, he does not address the possibility that the soul might suddenly disappear. In his monadology he advances a sophisticated novel argument for the immortality of monads.\n\nMoses Mendelssohn's Phaedon is a defense of the simplicity and immortality of the soul. It is a series of three dialogues, revisiting the Platonic dialogue Phaedo, in which Socrates argues for the immortality of the soul, in preparation for his own death. Many philosophers, including Plotinus, Descartes, and Leibniz, argue that the soul is simple, and that because simples cannot decompose they must be immortal. In the Phaedon, Mendelssohn addresses gaps in earlier versions of this argument (an argument that Kant calls the Achilles of Rationalist Psychology). The Phaedon contains an original argument for the simplicity of the soul, and also an original argument that simples cannot suddenly disappear. It contains further original arguments that the soul must retain its rational capacities as long as it exists.\n\nThe possibility of clinical immortality raises a host of medical, philosophical, and religious issues and ethical questions. These include persistent vegetative states, the nature of personality over time, technology to mimic or copy the mind or its processes, social and economic disparities created by longevity, and survival of the heat death of the universe.\n\nThe \"Epic of Gilgamesh\", one of the first literary works, is primarily a quest of a hero seeking to become immortal.\n\nPhysical immortality has also been imagined as a form of eternal torment, as in Mary Shelley's short story \"The Mortal Immortal\", the protagonist of which witnesses everyone he cares about dying around him. Jorge Luis Borges explored the idea that life gets its meaning from death in the short story \"The Immortal\"; an entire society having achieved immortality, they found time becoming infinite, and so found no motivation for any action. In his book \"Thursday's Fictions\", and the stage and film adaptations of it, Richard James Allen tells the story of a woman named Thursday who tries to cheat the cycle of reincarnation to get a form of eternal life. At the end of this fantastical tale, her son, Wednesday, who has witnessed the havoc his mother's quest has caused, forgoes the opportunity for immortality when it is offered to him. Likewise, the novel Tuck Everlasting depicts immortality as \"falling off the wheel of life\" and is viewed as a curse as opposed to a blessing. In the anime Casshern Sins humanity achieves immortality due to advances in medical technology, however the inability of the human race to die causes Luna, a Messianic figure, to come forth and offer normal lifespans because she had believed that without death, humans could not live. Ultimately, Casshern takes up the cause of death for humanity when Luna begins to restore humanity's immortality. In Anne Rice's book series \"The Vampire Chronicles\", vampires are portrayed as immortal and ageless, but their inability to cope with the changes in the world around them means that few vampires live for much more than a century, and those who do often view their changeless form as a curse.\n\nAlthough some scientists state that radical life extension, delaying and stopping aging are achievable, there are no international or national programs focused on stopping aging or on radical life extension. In 2012 in Russia, and then in the United States, Israel and the Netherlands, pro-immortality political parties were launched. They aimed to provide political support to anti-aging and radical life extension research and technologies and at the same time transition to the next step, radical life extension, life without aging, and finally, immortality and aim to make possible access to such technologies to most currently living people.\n\nThere are numerous symbols representing immortality. The ankh is an Egyptian symbol of life that holds connotations of immortality when depicted in the hands of the and pharaohs, who were seen as having control over the journey of life. The Möbius strip in the shape of a trefoil knot is another symbol of immortality. Most symbolic representations of infinity or the life cycle are often used to represent immortality depending on the context they are placed in. Other examples include the Ouroboros, the Chinese fungus of longevity, the \"ten\" kanji, the phoenix, the peacock in Christianity, and the colors amaranth (in Western culture) and peach (in Chinese culture).\n\nImmortality is a popular subject in fiction, as it explores humanity's deep-seated fears and comprehension of its own mortality. Immortal beings and species abound in fiction, especially fantasy fiction, and the meaning of \"immortal\" tends to vary.\n\nSome fictional beings are completely immortal (or very nearly so) in that they are immune to death by injury, disease and age. Sometimes such powerful immortals can only be killed by each other, as is the case with the Q from the \"Star Trek\" series. Even if something can't be killed, a common plot device involves putting an immortal being into a slumber or limbo, as is done with Morgoth in J. R. R. Tolkien's \"The Silmarillion\" and the Dreaming God of \"Pathways Into Darkness\". Storytellers often make it a point to give weaknesses to even the most indestructible of beings. For instance, Superman is supposed to be invulnerable, yet his enemies were able to exploit his now-infamous weakness: Kryptonite. (See also Achilles' heel.)\n\nMany fictitious species are said to be immortal if they cannot die of old age, even though they can be killed through other means, such as injury. Modern fantasy elves often exhibit this form of immortality. Other creatures, such as vampires and the immortals in the film \"Highlander\", can only die from beheading. The classic and stereotypical vampire is typically slain by one of several very specific means, including a silver bullet (or piercing with other silver weapons), a stake through the heart (perhaps made of consecrated wood), or by exposing them to sunlight.\n\n\n\n\n", "id": "177052", "title": "Immortality"}
{"url": "https://en.wikipedia.org/wiki?curid=183290", "text": "Life extension\n\nLife extension science, also known as anti-aging medicine, indefinite life extension, experimental gerontology, and biomedical gerontology, is the study of slowing down or reversing the processes of aging to extend both the maximum and average lifespan. The ability to achieve this, however, does not currently exist.\n\nSome researchers in this area, and \"life extensionists\", \"immortalists\" or \"longevists\" (those who wish to achieve longer lives themselves), believe that future breakthroughs in tissue rejuvenation, stem cells, regenerative medicine, molecular repair, gene therapy, pharmaceuticals, and organ replacement (such as with artificial organs or xenotransplantations) will eventually enable humans to have indefinite lifespans (agerasia) through complete rejuvenation to a healthy youthful condition. The ethical ramifications, if life extension becomes a possibility, are debated by bioethicists.\n\nThe sale of purported anti-aging products such as supplements and hormone replacement is a lucrative global industry. For example, the industry that promotes the use of hormones as a treatment for consumers to slow or reverse the aging process in the US market generated about $50 billion of revenue a year in 2009. The use of such products has not been proven to be effective or safe.\n\nDuring the process of aging, an organism accumulates damage to its macromolecules, cells, tissues, and organs. Specifically, aging is characterized as and thought to be caused by \"genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication.\" Oxidation damage to cellular contents caused by free radicals is believed to contribute to aging as well.\n\nThe longest a human has ever been proven to live is 122 years, the case of Jeanne Calment who was born in 1875 and died in 1997, whereas the maximum lifespan of a wildtype mouse, commonly used as a model in research on aging, is about three years. Genetic differences between humans and mice that may account for these different aging rates include differences in efficiency of DNA repair, antioxidant defenses, energy metabolism, proteostasis maintenance, and recycling mechanisms such as autophagy.\n\nAverage lifespan in a population is lowered by infant and child mortality, which are frequently linked to infectious diseases or nutrition problems. Later in life, vulnerability to accidents and age-related chronic disease such as cancer or cardiovascular disease play an increasing role in mortality. Extension of expected lifespan can often be achieved by access to improved medical care, vaccinations, good diet, exercise and avoidance of hazards such as smoking.\n\nMaximum lifespan is determined by the rate of aging for a species inherent in its genes and by environmental factors. Widely recognized methods of extending maximum lifespan in model organisms such as nematodes, fruit flies, and mice include caloric restriction, gene manipulation, and administration of pharmaceuticals. Another technique uses evolutionary pressures such as breeding from only older members or altering levels of extrinsic mortality.\nSome animals such as hydra, planarian flatworms, and certain sponges, corals, and jellyfish do not die of old age and exhibit potential immortality.\n\nMuch life extension research focuses on nutrition—diets or supplements— although there is little evidence that they have an effect. The many diets promoted by anti-aging advocates are often contradictory. \n\nIn some studies calorie restriction has been shown to extend the life of mice, yeast, and rhesus monkeys. However, a more recent study did not find calorie restriction to improve survival in rhesus monkeys. In humans the long-term health effects of moderate caloric restriction with sufficient nutrients are unknown.\n\nThe free-radical theory of aging suggests that antioxidant supplements might extend human life. However, evidence suggest that β-carotene supplements and high doses of vitamin E increase mortality rates. Resveratrol is a sirtuin stimulant that has been shown to extend life in animal models, but the effect of resveratrol on lifespan in humans is unclear as of 2011.\n\nThe anti-aging industry offers several hormone therapies. Some of these have been criticized for possible dangers and a lack of proven effect. For example, the American Medical Association has been critical of some anti-aging hormone therapies.\n\nWhile growth hormone (GH) decreases with age, the evidence for use of growth hormone as an anti-aging therapy is mixed and based mostly on animal studies. There are mixed reports that GH or IGF-1 modulates the aging process in humans and about whether the direction of its effect is positive or negative.\n\nThe extension of life has been a desire of humanity and a mainstay motif in the history of scientific pursuits and ideas throughout history, from the Sumerian Epic of Gilgamesh and the Egyptian Smith medical papyrus, all the way through the Taoists, Ayurveda practitioners, alchemists, hygienists such as Luigi Cornaro, Johann Cohausen and Christoph Wilhelm Hufeland, and philosophers such as Francis Bacon, René Descartes, Benjamin Franklin and Nicolas Condorcet. However, the beginning of the modern period in this endeavor can be traced to the end of the 19th – beginning of the 20th century, to the so-called \"fin-de-siècle\" (end of the century) period, denoted as an \"end of an epoch\" and characterized by the rise of scientific optimism and therapeutic activism, entailing the pursuit of life extension (or life-extensionism). Among the foremost researchers of life extension at this period were the Nobel Prize winning biologist Elie Metchnikoff (1845-1916) -- the author of the cell theory of immunity and vice director of Institut Pasteur in Paris, and Charles-Édouard Brown-Séquard (1817-1894) -- the president of the French Biological Society and one of the founders of modern endocrinology.\n\nSociologist James Hughes claims that science has been tied to a cultural narrative of conquering death since the Age of Enlightenment. He cites Francis Bacon (1561–1626) as an advocate of using science and reason to extend human life, noting Bacon's novel \"New Atlantis\", wherein scientists worked toward delaying aging and prolonging life. Robert Boyle (1627–1691), founding member of the Royal Society, also hoped that science would make substantial progress with life extension, according to Hughes, and proposed such experiments as \"to replace the blood of the old with the blood of the young\". Biologist Alexis Carrel (1873–1944) was inspired by a belief in indefinite human lifespan that he developed after experimenting with cells, says Hughes.\n\nIn 1970, the American Aging Association was formed under the impetus of Denham Harman, originator of the free radical theory of aging. Harman wanted an organization of biogerontologists that was devoted to research and to the sharing of information among scientists interested in extending human lifespan.\n\nIn 1976, futurists Joel Kurtzman and Philip Gordon wrote \"No More Dying. The Conquest Of Aging And The Extension Of Human Life\", () the first popular book on research to extend human lifespan. Subsequently, Kurtzman was invited to testify before the House Select Committee on Aging, chaired by Claude Pepper of Florida, to discuss the impact of life extension on the Social Security system.\n\nSaul Kent published \"The Life Extension Revolution\" () in 1980 and created a nutraceutical firm called the Life Extension Foundation, a non-profit organization that promotes dietary supplements. The Life Extension Foundation publishes a periodical called \"Life Extension Magazine\". The 1982 bestselling book \"\" () by Durk Pearson and Sandy Shaw further popularized the phrase \"life extension\".\n\nIn 1983, Roy Walford, a life-extensionist and gerontologist, published a popular book called \"Maximum Lifespan\". In 1988, Walford and his student Richard Weindruch summarized their research into the ability of calorie restriction to extend the lifespan of rodents in \"The Retardation of Aging and Disease by Dietary Restriction\" (). It had been known since the work of Clive McCay in the 1930s that calorie restriction can extend the maximum lifespan of rodents. But it was the work of Walford and Weindruch that gave detailed scientific grounding to that knowledge. Walford's personal interest in life extension motivated his scientific work and he practiced calorie restriction himself. Walford died at the age of 80 from complications caused by amyotrophic lateral sclerosis.\n\nMoney generated by the non-profit Life Extension Foundation allowed Saul Kent to finance the Alcor Life Extension Foundation, the world's largest cryonics organization. The cryonics movement had been launched in 1962 by Robert Ettinger's book, \"The Prospect of Immortality\". In the 1960s, Saul Kent had been a co-founder of the Cryonics Society of New York. Alcor gained national prominence when baseball star Ted Williams was cryonically preserved by Alcor in 2002 and a family dispute arose as to whether Williams had really wanted to be cryopreserved.\n\nRegulatory and legal struggles between the Food and Drug Administration (FDA) and the Life Extension Foundation included seizure of merchandise and court action. In 1991, Saul Kent and Bill Faloon, the principals of the Foundation, were jailed. The LEF accused the FDA of perpetrating a \"Holocaust\" and \"seeking gestapo-like power\" through its regulation of drugs and marketing claims.\n\nIn 2003, Doubleday published \"The Immortal Cell: One Scientist's Quest to Solve the Mystery of Human Aging,\" by Michael D. West. West emphasised the potential role of embryonic stem cells in life extension.\n\nOther modern life extensionists include writer Gennady Stolyarov, who insists that death is \"the enemy of us all, to be fought with medicine, science, and technology\"; transhumanist philosopher Zoltan Istvan, who proposes that the \"transhumanist must safeguard one's own existence above all else\"; futurist George Dvorsky, who considers aging to be a problem that desperately needs to be solved; and recording artist Steve Aoki, who has been called \"one of the most prolific campaigners for life extension\".\n\nIn 1991, the American Academy of Anti-Aging Medicine (A4M) was formed as a non-profit organization to create what it considered an anti-aging medical specialty distinct from geriatrics, and to hold trade shows for physicians interested in anti-aging medicine. The A4M trains doctors in anti-aging medicine and publicly promotes the field of anti-aging research. It has about 26,000 members, of whom about 97% are doctors and scientists. The American Board of Medical Specialties recognizes neither anti-aging medicine nor the A4M's professional standing.\n\nIn 2003, Aubrey de Grey and David Gobel formed the Methuselah Foundation, which gives financial grants to anti-aging research projects. In 2009, de Grey and several others founded the SENS Research Foundation, a California-based scientific research organization which conducts research into aging and funds other anti-aging research projects at various universities. In 2013, Google announced Calico, a new company based in San Francisco that will harness new technologies to increase scientific understanding of the biology of aging. It is led by Arthur D. Levinson, and its research team includes scientists such as Hal V. Barron, David Botstein, and Cynthia Kenyon. In 2014, biologist Craig Venter founded Human Longevity Inc., a company dedicated to scientific research to end aging through genomics and cell therapy. They received funding with the goal of compiling a comprehensive human genotype, microbiome, and phenotype database.\n\nAside from private initiatives, aging research is being conducted in university laboratories, and includes universities such as Harvard and UCLA. University researchers have made a number of breakthroughs in extending the lives of mice and insects by reversing certain aspects of aging.\n\nPolitics relevant to the substances of life extension pertain mostly to communications and availability.\n\nIn the United States, product claims on food and drug labels are strictly regulated. The First Amendment (freedom of speech) protects third-party publishers' rights to distribute fact, opinion and speculation on life extension practices. Manufacturers and suppliers also provide informational publications, but because they market the substances, they are subject to monitoring and enforcement by the Federal Trade Commission (FTC), which polices claims by marketers. What constitutes the difference between truthful and false claims is hotly debated and is a central controversy in this arena.\n\nSome critics dispute the portrayal of aging as a disease. For example, Leonard Hayflick, who determined that fibroblasts are limited to around 50 cell divisions, reasons that aging is an unavoidable consequence of entropy. Hayflick and fellow biogerontologists Jay Olshansky and Bruce Carnes have strongly criticized the anti-aging industry in response to what they see as unscrupulous profiteering from the sale of unproven anti-aging supplements.\n\nResearch by Sobh and Martin (2011) suggests that people buy anti-aging products to obtain a hoped-for self (e.g., keeping a youthful skin) or to avoid a feared-self (e.g., looking old). The research shows that when consumers pursue a hoped-for self, it is expectations of success that most strongly drive their motivation to use the product. The research also shows why doing badly when trying to avoid a feared self is more motivating than doing well. Interestingly, when product use is seen to fail it is more motivating than success when consumers seek to avoid a feared-self.\n\nThough many scientists state that life extension and radical life extension are possible, there are still no international or national programs focused on radical life extension. There are political forces staying for and against life extension. By 2012, in Russia, the United States, Israel, and the Netherlands, the Longevity political parties started. They aimed to provide political support to radical life extension research and technologies, and ensure the fastest possible and at the same time soft transition of society to the next step – life without aging and with radical life extension, and to provide access to such technologies to most currently living people.\n\nSome tech innovators and Silicon Valley entrepreneurs have invested heavily into anti-aging research. This includes Larry Ellison (founder of Oracle), Peter Thiel (former Paypal CEO), Larry Page (co-founder of Google), and Peter Diamandis.\n\nLeon Kass (chairman of the US President's Council on Bioethics from 2001 to 2005) has questioned whether potential exacerbation of overpopulation problems would make life extension unethical. He states his opposition to life extension with the words:\nJohn Harris, former editor-in-chief of the Journal of Medical Ethics, argues that as long as life is worth living, according to the person himself, we have a powerful moral imperative to save the life and thus to develop and offer life extension therapies to those who want them.\n\nTranshumanist philosopher Nick Bostrom has argued that any technological advances in life extension must be equitably distributed and not restricted to a privileged few. In an extended metaphor entitled \"The Fable of the Dragon-Tyrant\", Bostrom envisions death as a monstrous dragon who demands human sacrifices. In the fable, after a lengthy debate between those who believe the dragon is a fact of life and those who believe the dragon can and should be destroyed, the dragon is finally killed. Bostrom argues that political inaction allowed many preventable human deaths to occur.\n\nControversy about life extension is due to fear of overpopulation and possible effects on society. Biogerontologist Aubrey De Grey counters the overpopulation critique by pointing out that the therapy could postpone or eliminate menopause, allowing women to space out their pregnancies over more years and thus \"decreasing\" the yearly population growth rate. Moreover, the philosopher and futurist Max More argues that, given the fact the worldwide population growth rate is slowing down and is projected to eventually stabilize and begin falling, superlongevity would be unlikely to contribute to overpopulation.\n\nA Spring 2013 Pew Research poll in the United States found that 38% of Americans would want life extension treatments, and 56% would reject it. However, it also found that 68% believed most people would want it and that only 4% consider an \"ideal lifespan\" to be more than 120 years. The median \"ideal lifespan\" was 91 years of age and the majority of the public (63%) viewed medical advances aimed at prolonging life as generally good. 41% of Americans believed that radical life extension (RLE) would be good for society, while 51% said they believed it would be bad for society. One possibility for why 56% of Americans claim they would reject life extension treatments may be due to the cultural perception that living longer would result in a longer period of decrepitude, and that the elderly in our current society are unhealthy.\n\nReligious people are no more likely to oppose life extension than the unaffiliated, though some variation exists between religious denominations.\n\nMost mainstream medical organizations and practitioners do not consider aging to be a disease. David Sinclair says: \"I don't see aging as a disease, but as a collection of quite predictable diseases caused by the deterioration of the body\". The two main arguments used are that aging is both inevitable and universal while diseases are not. However, not everyone agrees. Harry R. Moody, director of academic affairs for AARP, notes that what is normal and what is disease strongly depend on a historical context. David Gems, assistant director of the Institute of Healthy Ageing, strongly argues that aging should be viewed as a disease. In response to the universality of aging, David Gems notes that it is as misleading as arguing that Basenji are not dogs because they do not bark. Because of the universality of aging he calls it a \"special sort of disease\". Robert M. Perlman, coined the terms \"aging syndrome\" and \"disease complex\" in 1954 to describe aging.\n\nThe discussion whether aging should be viewed as a disease or not has important implications. It would stimulate pharmaceutical companies to develop life extension therapies and in the United States of America, it would also increase the regulation of the anti-aging market by the FDA. Anti-aging now falls under the regulations for cosmetic medicine which are less tight than those for drugs.\n\nTheoretically, extension of maximum lifespan in humans could be achieved by reducing the rate of aging damage by periodic replacement of damaged tissues, molecular repair or rejuvenation of deteriorated cells and tissues, reversal of harmful epigenetic changes, or the enhancement of telomerase enzyme activity.\n\nResearch geared towards life extension strategies in various organisms is currently under way at a number of academic and private institutions. Since 2009, investigators have found ways to increase the lifespan of nematode worms and yeast by 10-fold; the record in nematodes was achieved through genetic engineering and the extension in yeast by a combination of genetic engineering and caloric restriction. A 2009 review of longevity research noted: \"Extrapolation from worms to mammals is risky at best, and it cannot be assumed that interventions will result in comparable life extension factors. Longevity gains from dietary restriction, or from mutations studied previously, yield smaller benefits to Drosophila than to nematodes, and smaller still to mammals. This is not unexpected, since mammals have evolved to live many times the worm's lifespan, and humans live nearly twice as long as the next longest-lived primate. From an evolutionary perspective, mammals and their ancestors have already undergone several hundred million years of natural selection favoring traits that could directly or indirectly favor increased longevity, and may thus have already settled on gene sequences that promote lifespan. Moreover, the very notion of a \"life-extension factor\" that could apply across taxa presumes a linear response rarely seen in biology.\"\n\nThere are a number of chemicals intended to slow the aging process currently being studied in animal models. One type of research is related to the observed effects of a calorie restriction (CR) diet, which has been shown to extend lifespan in some animals Based on that research, there have been attempts to develop drugs that will have the same effect on the aging process as a caloric restriction diet, which are known as Caloric restriction mimetic drugs. Some drugs that are already approved for other uses have been studied for possible longevity effects on laboratory animals because of a possible CR-mimic effect; they include rapamycin, metformin and other geroprotectors. MitoQ, resveratrol and pterostilbene are dietary supplements that have also been studied in this context.\n\nOther attempts to create anti-aging drugs have taken different research paths. One notable direction of research has been research into the possibility of using the enzyme telomerase in order to counter the process of telomere shortening. However, there are potential dangers in this, since some research has also linked telomerase to cancer and to tumor growth and formation.\n\nFuture advances in nanomedicine could give rise to life extension through the repair of many processes thought to be responsible for aging. K. Eric Drexler, one of the founders of nanotechnology, postulated cell repair machines, including ones operating within cells and utilizing as yet hypothetical molecular computers, in his 1986 book Engines of Creation. Raymond Kurzweil, a futurist and transhumanist, stated in his book \"The Singularity Is Near\" that he believes that advanced medical nanorobotics could completely remedy the effects of aging by 2030. According to Richard Feynman, it was his former graduate student and collaborator Albert Hibbs who originally suggested to him (circa 1959) the idea of a \"medical\" use for Feynman's theoretical nanomachines (see biological machine). Hibbs suggested that certain repair machines might one day be reduced in size to the point that it would, in theory, be possible to (as Feynman put it) \"swallow the doctor\". The idea was incorporated into Feynman's 1959 essay \"There's Plenty of Room at the Bottom.\"\n\nSome life extensionists suggest that therapeutic cloning and stem cell research could one day provide a way to generate cells, body parts, or even entire bodies (generally referred to as reproductive cloning) that would be genetically identical to a prospective patient. Recently, the US Department of Defense initiated a program to research the possibility of growing human body parts on mice. Complex biological structures, such as mammalian joints and limbs, have not yet been replicated. Dog and primate brain transplantation experiments were conducted in the mid-20th century but failed due to rejection and the inability to restore nerve connections. As of 2006, the implantation of bio-engineered bladders grown from patients' own cells has proven to be a viable treatment for bladder disease. Proponents of body part replacement and cloning contend that the required biotechnologies are likely to appear earlier than other life-extension technologies.\n\nThe use of human stem cells, particularly embryonic stem cells, is controversial. Opponents' objections generally are based on interpretations of religious teachings or ethical considerations. Proponents of stem cell research point out that cells are routinely formed and destroyed in a variety of contexts. Use of stem cells taken from the umbilical cord or parts of the adult body may not provoke controversy.\n\nThe controversies over cloning are similar, except general public opinion in most countries stands in opposition to reproductive cloning. Some proponents of therapeutic cloning predict the production of whole bodies, lacking consciousness, for eventual brain transplantation.\n\nReplacement of biological (susceptible to diseases) organs with mechanical ones could extend life. This is the goal of the 2045 Initiative.\n\nFor cryonicists (advocates of cryopreservation), storing the body at low temperatures after death may provide an \"ambulance\" into a future in which advanced medical technologies may allow resuscitation and repair. They speculate cryogenic temperatures will minimize changes in biological tissue for many years, giving the medical community ample time to cure all disease, rejuvenate the aged and repair any damage that is caused by the cryopreservation process.\n\nMany cryonicists do not believe that legal death is \"real death\" because stoppage of heartbeat and breathing—the usual medical criteria for legal death—occur before biological death of cells and tissues of the body. Even at room temperature, cells may take hours to die and days to decompose. Although neurological damage occurs within 4–6 minutes of cardiac arrest, the irreversible neurodegenerative processes do not manifest for hours. Cryonicists state that rapid cooling and cardio-pulmonary support applied immediately after certification of death can preserve cells and tissues for long-term preservation at cryogenic temperatures. People, particularly children, have survived up to an hour without heartbeat after submersion in ice water. In one case, full recovery was reported after 45 minutes underwater. To facilitate rapid preservation of cells and tissue, cryonics \"standby teams\" are available to wait by the bedside of patients who are to be cryopreserved to apply cooling and cardio-pulmonary support as soon as possible after declaration of death.\n\nNo mammal has been successfully cryopreserved and brought back to life, with the exception of frozen human embryos. Resuscitation of a postembryonic human from cryonics is not possible with current science. Some scientists still support the idea based on their expectations of the capabilities of future science.\n\nAnother proposed life extension technology would combine existing and predicted future biochemical and genetic techniques. SENS proposes that rejuvenation may be obtained by removing aging damage via the use of stem cells and tissue engineering, telomere-lengthening machinery, allotopic expression of mitochondrial proteins, targeted ablation of cells, immunotherapeutic clearance, and novel lysosomal hydrolases.\n\nWhile many biogerontologists find these ideas \"worthy of discussion\" and SENS conferences feature important research in the field, some contend that the alleged benefits are too speculative given the current state of technology, referring to it as \"fantasy rather than science\".\n\nGene therapy, in which nucleic acid polymers are delivered as a drug and are either expressed as proteins, interfere with the expression of proteins, or correct genetic mutations, has been proposed as a future strategy to prevent aging.\n\nA large array of genetic modifications have been found to increase lifespan in model organisms such as yeast, nematode worms, fruit flies, and mice. As of 2013, the longest extension of life caused by a single gene manipulation was roughly 150% in mice and 10-fold in nematode worms.\n\nIn \"The Selfish Gene\", Richard Dawkins describes an approach to life-extension that involves \"fooling genes\" into thinking the body is young. Dawkins attributes inspiration for this idea to Peter Medawar. The basic idea is that our bodies are composed of genes that activate throughout our lifetimes, some when we are young and others when we are older. Presumably, these genes are activated by environmental factors, and the changes caused by these genes activating can be lethal. It is a statistical certainty that we possess more lethal genes that activate in later life than in early life. Therefore, to extend life, we should be able to prevent these genes from switching on, and we should be able to do so by \"identifying changes in the internal chemical environment of a body that take place during aging... and by simulating the superficial chemical properties of a young body\".\n\nAccording to some lines of thinking, the ageing process is routed into a basic reduction of biological complexity, and thus loss of information. In order to reverse this loss, gerontologist Marios Kyriazis suggested that it is necessary to increase input of actionable and meaningful information both individually (into individual brains), and collectively (into societal systems). This technique enhances overall biological function through up-regulation of immune, hormonal, antioxidant and other parameters, resulting in improved age-repair mechanisms. Working in parallel with natural evolutionary mechanisms that can facilitate survival through increased fitness, Kyriazis claims that the technique may lead to a reduction of the rate of death as a function of age, i.e. indefinite lifespan.\n\nOne hypothetical future strategy that, as some suggest, \"eliminates\" the complications related to a physical body, involves the copying or transferring (e.g. by progressively replacing neurons with transistors) of a conscious mind from a biological brain to a non-biological computer system or computational device. The basic idea is to scan the structure of a particular brain in detail, and then construct a software model of it that is so faithful to the original that, when run on appropriate hardware, it will behave in essentially the same way as the original brain. Whether or not an exact copy of one's mind constitutes actual life extension is matter of debate.\n\nSome scientists believe that the dead may one day be \"resurrected\" through simulation technology.\n\nSome clinics currently offer injection of blood products from young donors. The alleged benefits of the treatment, none of which have been demonstrated in a proper study, include a longer life, darker hair, better memory, better sleep, curing heart diseases, diabetes and Alzheimer. The approach is based on parabiosis studies neurologists such as Irina Conboy do on mice, but Conboy says young blood does not reverse aging (even in mice) and that those who offer those treatments have misunderstood her research. Neuroscientist Tony Wyss-Coray, who also studied blood exchanges on mice as recently as 2014, said people offering those treatments are \"basically abusing people‘s trust\" and that young blood treatments are \"the scientific equivalent of fake news\". The treatment appeared in HBO's Silicon Valley fiction series.\n\nTwo clinics in California, run by Jesse Karmazin and David C. Wright, offer $8,000 injections of plasma extracted from the blood of young people. Karmazin has not published in any peer-reviewed journal and his current study does not use a control group.\n\n\n", "id": "183290", "title": "Life extension"}
{"url": "https://en.wikipedia.org/wiki?curid=55842160", "text": "Life Extension Advocacy Foundation\n\nThe Life Extension Advocacy Foundation (LEAF) is a 501(c)(3) non-profit organization based in New York, New York, United States. Founded in 2014, LEAF is directly supporting fundamental research on the main mechanisms of aging and age-related diseases and disseminating knowledge about the possibility to bring aging under medical control in order to prevent, postpone and cure age-related diseases . \n\nThe flagship project of LEAF is Lifespan.io, a nonprofit crowdfunding platform focused on the biomedical research of aging. As of 2017, the platform has hosted 6 successful projects, including two projects by the SENS Research Foundation: an OncoSENS ALT-related project in 2016 and a MitoSENS mitochondrial repair-focused project in 2015.\nLEAF maintains a variety of educational activities:\n\nIn 2017, as a part of its outreach activities, LEAF provided scientific advice to help Kurzgesagt create two educational popular science videos about biological aging and the potential of regenerative medicine to address its underlying mechanisms to extend the healthy period of life..\n\nTo date, Lifespan.io has hosted the following projects:\n\nLEAF and Lifespan.io have received support from the following public figures: Jim Mellon (british entrepreneur and investor, author of Juvenescence), Aubrey de Grey (chief science officer of SENS Research Foundation, Michael Greve (German entrepreneur and founder of Forever Healthy Foundation), Bill Gelpi (entrepreneur), Zoltan Istvan (futurist, US President candidate), Didier Coeurnelle (President of HEALES), Nell Watson (Alumni of Singularity University, engineer, futurist, researcher of the Artificial Intelligence), Alex Zhavoronkov (CEO at Insilico Medicine). \n", "id": "55842160", "title": "Life Extension Advocacy Foundation"}
{"url": "https://en.wikipedia.org/wiki?curid=558685", "text": "Natural environment\n\nThe natural environment encompasses all living and non-living things occurring naturally, meaning in this case not artificial. The term is most often applied to the Earth or some parts of Earth. This environment encompasses the interaction of all living species, climate, weather, and natural resources that affect human survival and economic activity.\n\nThe concept of the \"natural environment\" can be distinguished as components:\n\nIn contrast to the natural environment is the built environment. In such areas where man has fundamentally transformed landscapes such as urban settings and agricultural land conversion, the natural environment is greatly modified into a simplified human environment. Even acts which seem less extreme, such as building a mud hut or a photovoltaic system in the desert, modify the natural environment into an artificial one. Though many animals build things to provide a better environment for themselves, they are not human, hence beaver dams and the works of Mound-building termites are thought of as natural.\n\nPeople seldom find \"absolutely natural\" environments on Earth, and naturalness usually varies in a continuum, from 100% natural in one extreme to 0% natural in the other. More precisely, we can consider the different aspects or components of an environment, and see that their degree of naturalness is not uniform. If, for instance, in an agricultural field, the mineralogic composition and the structure of its soil are similar to those of an undisturbed forest soil, but the structure is quite different.\n\n\"Natural environment\" is often used as a synonym for habitat. For instance, when we say that the natural environment of giraffes is the savanna.\n\nEarth science generally recognizes 4 spheres, the lithosphere, the hydrosphere, the atmosphere, and the biosphere as correspondent to rocks, water, air, and life respectively. Some scientists include, as part of the spheres of the Earth, the cryosphere (corresponding to ice) as a distinct portion of the hydrosphere, as well as the pedosphere (corresponding to soil) as an active and intermixed sphere. Earth science (also known as geoscience, the geosciences or the Earth Sciences), is an all-embracing term for the sciences related to the planet Earth. There are four major disciplines in earth sciences, namely geography, geology, geophysics and geodesy. These major disciplines use physics, chemistry, biology, chronology and mathematics to build a qualitative and quantitative understanding of the principal areas or \"spheres\" of Earth.\n\nThe Earth's crust, or lithosphere, is the outermost solid surface of the planet and is chemically and mechanically different from underlying mantle. It has been generated greatly by igneous processes in which magma cools and solidifies to form solid rock. Beneath the lithosphere lies the mantle which is heated by the decay of radioactive elements. The mantle though solid is in a state of rheic convection. This convection process causes the lithospheric plates to move, albeit slowly. The resulting process is known as plate tectonics. Volcanoes result primarily from the melting of subducted crust material or of rising mantle at mid-ocean ridges and mantle plumes.\n\nMost water is found in one or another natural kind of body of water.\n\nAn ocean is a major body of saline water, and a component of the hydrosphere. Approximately 71% of the Earth's surface (an area of some 362 million square kilometers) is covered by ocean, a continuous body of water that is customarily divided into several principal oceans and smaller seas. More than half of this area is over 3,000 meters (9,800 ft) deep. Average oceanic salinity is around 35 parts per thousand (ppt) (3.5%), and nearly all seawater has a salinity in the range of 30 to 38 ppt. Though generally recognized as several 'separate' oceans, these waters comprise one global, interconnected body of salt water often referred to as the World Ocean or global ocean. The deep seabeds are more than half the Earth's surface, and are among the least-modified natural environments. The major oceanic divisions are defined in part by the continents, various archipelagos, and other criteria: these divisions are (in descending order of size) the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Southern Ocean and the Arctic Ocean.\nA river is a natural watercourse, usually freshwater, flowing toward an ocean, a lake, a sea or another river. A few rivers simply flow into the ground and dry up completely before reaching another body of water. \nThe water in a river is usually in a channel, made up of a stream bed between banks. In larger rivers there is also a wider floodplain shaped by waters over-topping the channel. Flood plains may be very wide in relation to the size of the river channel. Rivers are a part of the hydrological cycle. Water within a river is generally collected from precipitation through surface runoff, groundwater recharge, springs, and the release of water stored in glaciers and snowpacks.\n\nSmall rivers may also be termed by several other names, including stream, creek and brook. Their current is confined within a bed and stream banks. Streams play an important corridor role in connecting fragmented habitats and thus in conserving biodiversity. The study of streams and waterways in general is known as \"surface hydrology.\" \n\nA lake (from Latin \"lacus\") is a terrain feature, a body of water that is localized to the bottom of basin. A body of water is considered a lake when it is inland, is not part of an ocean, and is larger and deeper than a pond.\nNatural lakes on Earth are generally found in mountainous areas, rift zones, and areas with ongoing or recent glaciation. Other lakes are found in endorheic basins or along the courses of mature rivers. In some parts of the world, there are many lakes because of chaotic drainage patterns left over from the last Ice Age. All lakes are temporary over geologic time scales, as they will slowly fill in with sediments or spill out of the basin containing them.\n\nA pond is a body of standing water, either natural or man-made, that is usually smaller than a lake. A wide variety of man-made bodies of water are classified as ponds, including water gardens designed for aesthetic ornamentation, fish ponds designed for commercial fish breeding, and solar ponds designed to store thermal energy. Ponds and lakes are distinguished from streams by their current speed. While currents in streams are easily observed, ponds and lakes possess thermally driven micro-currents and moderate wind driven currents. These features distinguish a pond from many other aquatic terrain features, such as stream pools and tide pools.\n\nThe atmosphere of the Earth serves as a key factor in sustaining the planetary ecosystem. The thin layer of gases that envelops the Earth is held in place by the planet's gravity. Dry air consists of 78% nitrogen, 21% oxygen, 1% argon and other inert gases, such as carbon dioxide. The remaining gases are often referred to as trace gases, among which are the greenhouse gases such as water vapor, carbon dioxide, methane, nitrous oxide, and ozone. Filtered air includes trace amounts of many other chemical compounds. Air also contains a variable amount of water vapor and suspensions of water droplets and ice crystals seen as clouds. Many natural substances may be present in tiny amounts in an unfiltered air sample, including dust, pollen and spores, sea spray, volcanic ash, and meteoroids. Various industrial pollutants also may be present, such as chlorine (elementary or in compounds), fluorine compounds, elemental mercury, and sulphur compounds such as sulphur dioxide [SO].\n\nThe ozone layer of the Earth's atmosphere plays an important role in depleting the amount of ultraviolet (UV) radiation that reaches the surface. As DNA is readily damaged by UV light, this serves to protect life at the surface. The atmosphere also retains heat during the night, thereby reducing the daily temperature extremes.\n\nEarth's atmosphere can be divided into five main layers. These layers are mainly determined by whether temperature increases or decreases with altitude. From highest to lowest, these layers are:\n\nWithin the five principal layers determined by temperature are several layers determined by other properties.\n\nThe potential dangers of global warming are being increasingly studied by a wide global consortium of scientists. These scientists are increasingly concerned about the potential long-term effects of global warming on our natural environment and on the planet. Of particular concern is how climate change and global warming caused by anthropogenic, or human-made releases of greenhouse gases, most notably carbon dioxide, can act interactively, and have adverse effects upon the planet, its natural environment and humans' existence. It is clear the planet is warming, and warming rapidly.–This warming is also responsible for the extinction of natural habitats, which in turn leads to a reduction in wildlife population.The most recent report from the Intergovernmental Panel on Climate Change (the group of the leading climate scientists in the world) concluded that the earth will warm anywhere from 2.7 to almost 11 degrees Fahrenheit (1.5 to 6 degrees Celsius) between 1990 and 2100.\nEfforts have been increasingly focused on the mitigation of greenhouse gases that are causing climatic changes, on developing adaptative strategies to global warming, to assist humans, other animal, and plant species, ecosystems, regions and nations in adjusting to the effects of global warming. Some examples of recent collaboration to address climate change and global warming include:\n\nA significantly profound challenge is to identify the natural environmental dynamics in contrast to environmental changes not within natural variances. A common solution is to adapt a static view neglecting natural variances to exist. Methodologically, this view could be defended when looking at processes which change slowly and short time series, while the problem arrives when fast processes turns essential in the object of the study.\n\nClimate encompasses the statistics of temperature, humidity, atmospheric pressure, wind, rainfall, atmospheric particle count and numerous other meteorological elements in a given region over long periods of time. Climate can be contrasted to weather, which is the present condition of these same elements over periods up to two weeks.\n\nClimates can be classified according to the average and typical ranges of different variables, most commonly temperature and precipitation. The most commonly used classification scheme is the one originally developed by Wladimir Köppen. The Thornthwaite system, in use since 1948, incorporates evapotranspiration in addition to temperature and precipitation information and is used in studying animal species diversity and potential impacts of climate changes.\n\nWeather is a set of all the phenomena occurring in a given atmospheric area at a given time. Most weather phenomena occur in the troposphere, just below the stratosphere. Weather refers, generally, to day-to-day temperature and precipitation activity, whereas climate is the term for the average atmospheric conditions over longer periods of time. When used without qualification, \"weather\" is understood to be the weather of Earth.\n\nWeather occurs due to density (temperature and moisture) differences between one place and another. These differences can occur due to the sun angle at any particular spot, which varies by latitude from the tropics. The strong temperature contrast between polar and tropical air gives rise to the jet stream. Weather systems in the mid-latitudes, such as extratropical cyclones, are caused by instabilities of the jet stream flow. Because the Earth's axis is tilted relative to its orbital plane, sunlight is incident at different angles at different times of the year. On the Earth's surface, temperatures usually range ±40 °C (100 °F to −40 °F) annually. Over thousands of years, changes in the Earth's orbit have affected the amount and distribution of solar energy received by the Earth and influence long-term climate\n\nSurface temperature differences in turn cause pressure differences. Higher altitudes are cooler than lower altitudes due to differences in compressional heating. Weather forecasting is the application of science and technology to predict the state of the atmosphere for a future time and a given location. The atmosphere is a chaotic system, and small changes to one part of the system can grow to have large effects on the system as a whole. Human attempts to control the weather have occurred throughout human history, and there is evidence that civilized human activity such as agriculture and industry has inadvertently modified weather patterns.\n\nEvidence suggests that life on Earth has existed for about 3.7 billion years. All known life forms share fundamental molecular mechanisms, and based on these observations, theories on the origin of life attempt to find a mechanism explaining the formation of a primordial single cell organism from which all life originates. There are many different hypotheses regarding the path that might have been taken from simple organic molecules via pre-cellular life to protocells and metabolism.\n\nAlthough there is no universal agreement on the definition of life, scientists generally accept that the biological manifestation of life is characterized by organization, metabolism, growth, adaptation, response to stimuli and reproduction. Life may also be said to be simply the characteristic state of organisms. In biology, the science of living organisms, \"life\" is the condition which distinguishes active organisms from inorganic matter, including the capacity for growth, functional activity and the continual change preceding death.\n\nA diverse variety of living organisms (life forms) can be found in the biosphere on Earth, and properties common to these organisms—plants, animals, fungi, protists, archaea, and bacteria—are a carbon- and water-based cellular form with complex organization and heritable genetic information. Living organisms undergo metabolism, maintain homeostasis, possess a capacity to grow, respond to stimuli, reproduce and, through natural selection, adapt to their environment in successive generations. More complex living organisms can communicate through various means.\n\nAn ecosystem (also called as environment) is a natural unit consisting of all plants, animals and micro-organisms (biotic factors) in an area functioning together with all of the non-living physical (abiotic) factors of the environment.\n\nCentral to the ecosystem concept is the idea that living organisms are continually engaged in a highly interrelated set of relationships with every other element constituting the environment in which they exist. Eugene Odum, one of the founders of the science of ecology, stated: \"Any unit that includes all of the organisms (ie: the \"community\") in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e.: exchange of materials between living and nonliving parts) within the system is an ecosystem.\"\n\nA greater number or variety of species or biological diversity of an ecosystem may contribute to greater resilience of an ecosystem, because there are more species present at a location to respond to change and thus \"absorb\" or reduce its effects. This reduces the effect before the ecosystem's structure is fundamentally changed to a different state. This is not universally the case and there is no proven relationship between the species diversity of an ecosystem and its ability to provide goods and services on a sustainable level.\n\nThe term ecosystem can also pertain to human-made environments, such as human ecosystems and human-influenced ecosystems, and can describe any situation where there is relationship between living organisms and their environment. Fewer areas on the surface of the earth today exist free from human contact, although some genuine wilderness areas continue to exist without any forms of human intervention.\n\nBiomes are terminologically similar to the concept of ecosystems, and are climatically and geographically defined areas of ecologically similar climatic conditions on the Earth, such as communities of plants, animals, and soil organisms, often referred to \"as\" ecosystems. Biomes are defined on the basis of factors such as plant structures (such as trees, shrubs, and grasses), leaf types (such as broadleaf and needleleaf), plant spacing (forest, woodland, savanna), and climate. Unlike ecozones, biomes are not defined by genetic, taxonomic, or historical similarities. Biomes are often identified with particular patterns of ecological succession and climax vegetation.\n\nGlobal biogeochemical cycles are critical to life, most notably those of water, oxygen, carbon, nitrogen and phosphorus.\n\nWilderness is generally defined as a natural environment on Earth that has not been significantly modified by human activity. The WILD Foundation goes into more detail, defining wilderness as: \"The most intact, undisturbed wild natural areas left on our planet - those last truly wild places that humans do not control and have not developed with roads, pipelines or other industrial infrastructure.\" Wilderness areas and protected parks are considered important for the survival of certain species, ecological studies, conservation, solitude, and recreation. Wilderness is deeply valued for cultural, spiritual, moral, and aesthetic reasons. Some nature writers believe wilderness areas are vital for the human spirit and creativity.\n\nThe word, \"wilderness\", derives from the notion of wildness; in other words that which is not controllable by humans. The word's etymology is from the Old English \"wildeornes\", which in turn derives from \"wildeor\" meaning \"wild beast\" (wild + deor = beast, deer). From this point of view, it is the wildness of a place that makes it a wilderness. The mere presence or activity of people does not disqualify an area from being \"wilderness.\" Many ecosystems that are, or have been, inhabited or influenced by activities of people may still be considered \"wild.\" This way of looking at wilderness includes areas within which natural processes operate without very noticeable human interference.\n\nWildlife includes all non-domesticated plants, animals and other organisms. Domesticating wild plant and animal species for human benefit has occurred many times all over the planet, and has a major impact on the environment, both positive and negative. Wildlife can be found in all ecosystems. Deserts, rain forests, plains, and other areas—including the most developed urban sites—all have distinct forms of wildlife. While the term in popular culture usually refers to animals that are untouched by civilized human factors, most scientists agree that wildlife around the world is (now) impacted by human activities.\n\nIt is the common understanding of \"natural environment\" that underlies environmentalism — a broad political, social, and philosophical movement that advocates various actions and policies in the interest of protecting what nature remains in the natural environment, or restoring or expanding the role of nature in this environment. While true wilderness is increasingly rare, \"wild\" nature (e.g., unmanaged forests, uncultivated grasslands, wildlife, wildflowers) can be found in many locations previously inhabited by humans.\n\nGoals for the benefit of people and natural systems, commonly expressed by environmental scientists and environmentalists include:\n\nIn some cultures the term environment is meaningless because there is no separation between people and what they view as the natural world, or their surroundings. Specifically in the United States, many native cultures do not recognize the \"environment\", or see themselves as environmentalists.\n\n", "id": "558685", "title": "Natural environment"}
{"url": "https://en.wikipedia.org/wiki?curid=44314964", "text": "Protected area downgrading, downsizing, and degazettement\n\nProtected area downgrading, downsizing, and degazettement (PADDD) are processes that change the legal status of national parks and other protected areas. \"Downgrading\" is \"a decrease in legal restrictions on the number, magnitude, or extent of human activities within a protected area (i.e., legal authorization for increased human use).\" \"Downsizing\" refers to a \"decrease in size of a protected area as a result of excision of land or sea area through a legal boundary change.\" \"Degazettement\" is defined as a loss of legal protection for an entire national park or other protected area. Collectively, PADDD represents legal processes that temper regulations, shrink boundaries, or eliminate all legal protections originally associated with establishment of a protected area.\n\nPADDD is a phenomenon that has recently gained attention among scientists and policymakers. Scientific publications have identified more than 600 enacted PADDD events in 57 countries, encompassing a total of more than 550,000 km2 of protected lands.\n\nPADDD was a topic of discussion at the World Parks Congress in Sydney, Australia, in November, 2014.\n", "id": "44314964", "title": "Protected area downgrading, downsizing, and degazettement"}
{"url": "https://en.wikipedia.org/wiki?curid=54096894", "text": "City Environmental Quality Review\n\nCity Environmental Quality Review (CEQR) is a process New York City agencies must undergo to determine if any discretionary action they approved has any deteriorating impact on the environment. Projects that have to be reviewed by CEQR are either in need of permits or approval from a city agency, need city funding, or are directly undertaken by a city agency.\n\nIn 1973, New York City put into place Executive Order No. 87, meaning that any city project being implemented must be assessed for the impact it could have on the environment. This initiative was in response to the country's National Environmental Policy Act (NEPA), which was enacted in 1970 and was one of the first national laws dedicated to protecting the environment. Continuing after that, in 1975 the State of New York put into place the State Environmental Quality Review Act (SEQR). SEQR required that state and local government agencies must review the environmental impact of any discretionary action before funding or approving that action.\n\nFrom the SEQR came the CEQR in 1977. SEQR regulations allowed local governments to create their own environmental review procedures. Through the Executive Order No. 91, CEQR was established in New York City and so was the creation of a two co-lead agency system that would handle most of the environmental review functions. However, in 1991 the two co-lead agency system was replaced by a new system of lead agencies, where City agencies would act as the lead agency to any projects they wished to fund or implement.\n\nAn environmental review process starts when an agency proposes a project. In addition to the proposal, the agency must have a completed list of any permits, approvals, or funding needed to complete the project. The list will help guide the CEQR process; determining how intensely the CEQR process must be, or if a CEQR is even needed for a project.\n\nOnce the CEQR process begins, it is separated into two stages. The first stage is an initial assessment, known as the Environmental Assessment Statement (EAS), which discloses the description of the project and any potential impacts it could have. The EAS is then reviewed for any significant impacts that could arise and if any are found, the lead agency declares an Environmental Impact Statement (EIS) to be completed, which analyzes the impacts described in the EAS.\n\nDuring the review, projects can be categorized into several defining labels. One label is Project Development, where the project is still being defined and the data is being gathered before the CEQR process. Another label is Active, where the CEQR process is in progress. On Hold, where the review has been suspended. Complete, when the CEQR process is completed. Withdrawn, where an applicant withdraws the review. Terminated, when the review, after 6 months of inactivity, becomes canceled. And Monitoring, when a project, which has completed the CEQR process, is tracked, making sure guidelines are being followed.\n\nWhen conducting the review, CEQR takes the following areas into account for their assessment:\n\n", "id": "54096894", "title": "City Environmental Quality Review"}
{"url": "https://en.wikipedia.org/wiki?curid=21469177", "text": "Suctorial\n\nSuctorial pertains to the adaptation for sucking or suction, as possessed by marine parasites such as the Cookiecutter shark, specifically in a specialised lip organ enabling attachment to the host.\n\nSuctorial organs of a different form are possessed by the Solifugae arachnids, enabling the climbing of smooth, vertical surfaces.\n\nAnother variation on the suctorial organ can be found as part of the glossa proboscis of Masarinae (pollen wasps), enabling nectar feeding from the deep and narrow corolla of flowers.\n", "id": "21469177", "title": "Suctorial"}
{"url": "https://en.wikipedia.org/wiki?curid=24630261", "text": "Stenochoria\n\nStenochoria from (Greek στενοχωρία, \"stenos\" narrow + \"chôra\" space) may refer to:\n\n\n\n", "id": "24630261", "title": "Stenochoria"}
{"url": "https://en.wikipedia.org/wiki?curid=1466225", "text": "Allee effect\n\nThe Allee effect is a phenomenon in biology characterized by a correlation between population size or density and the mean individual fitness (often measured as \"per capita\" population growth rate) of a population or species.\n\nAlthough the concept of Allee effect had no title at the time, it was first described in the 1930s by its namesake, Warder Clyde Allee. Through experimental studies, Allee was able to demonstrate that goldfish grow more rapidly when there are more individuals within the tank. This led him to conclude that aggregation can improve the survival rate of individuals, and that cooperation may be crucial in the overall evolution of social structure. The term \"Allee principle\" was introduced in the 1950s, a time when the field of ecology was heavily focused on the role of competition among and within species. The classical view of population dynamics stated that due to competition for resources, a population will experience a reduced overall growth rate at higher density and increased growth rate at lower density. In other words, individuals in a population would be better off when there are fewer individuals around due to a limited amount of resources (see ). However, the concept of the Allee effect introduced the idea that the reverse holds true when the population density is low. Individuals within a species often require the assistance of another individual for more than simple reproductive reasons in order to persist. The most obvious example of this is observed in animals that hunt for prey or defend against predators as a group.\n\nThe generally accepted definition of Allee effect is positive density dependence, or the positive correlation between population density and individual fitness. It is sometimes referred to as \"undercrowding\" and it is analogous (or even considered synonymous by some) to \"depensation\" in the field of fishery sciences. Listed below are a few significant subcategories of the Allee effect used in the ecology literature.\n\nThe component Allee effect is the positive relationship between any measurable component of individual fitness and population density. The demographic Allee effect is the positive relationship between the overall individual fitness and population density.\n\nThe distinction between the two terms lies on the scale of the Allee effect: the presence of a demographic Allee effect suggests the presence of at least one component Allee effect, while the presence of a component Allee effect does not necessarily result in a demographic Allee effect. For example, cooperative hunting and the ability to more easily find mates, both influenced by population density, are component Allee effects, as they influence individual fitness of the population. At low population density, these component Allee effects would add up to produce an overall demographic Allee effect (increased fitness with higher population density). When population density reaches a high number, negative density dependence often offsets the component Allee effects through resource competition, thus erasing the demographic Allee effect. It is important to note that Allee effects might occur even at high population density for some species.\n\nThe strong Allee effect is a demographic Allee effect with a critical population size or density. The weak Allee effect is a demographic Allee effect without a critical population size or density.\n\nThe distinction between the two terms is based on whether or not the population in question exhibits a critical population size or density. A population exhibiting a weak Allee effect will possess a reduced per capita growth rate (directly related to individual fitness of the population) at lower population density or size. However, even at this low population size or density, the population will always exhibit a positive per capita growth rate. Meanwhile, a population exhibiting a strong Allee effect will have a critical population size or density under which the population growth rate becomes negative. Therefore, when the population density or size hits a number below this threshold, the population will be destined for extinction without any further aid. A strong Allee effect is often easier to demonstrate empirically using time series data, as one can pinpoint the population size or density at which per capita growth rate becomes negative.\n\nDue to its definition as the positive correlation between population density and average fitness, the mechanisms for which an Allee effect arises are therefore inherently tied to survival and reproduction. In general, these Allee effect mechanisms arise from cooperation or facilitation among individuals in the species. Examples of such cooperative behaviors include better mate finding, environmental conditioning, and group defense against predators. As these mechanisms are more easily observable in the field, they tend to be more commonly associated with the Allee effect concept. Nevertheless, mechanisms of Allee effect that are less conspicuous such as inbreeding depression and sex ratio bias should be considered as well.\n\nAlthough numerous ecological mechanisms for Allee effects exist, the list of most commonly cited facilitative behaviors that contribute to Allee effects in the literature include: mate limitation, cooperative defense, cooperative feeding, and environmental conditioning. While these behaviors are classified in separate categories, note that they can overlap and tend to be context dependent (will operate only under certain conditions – for example, cooperative defense will only be useful when there are predators or competitors present).\n\n\n\n\n\nClassic economic theory predicts that human exploitation of a population is unlikely to result in species extinction because the escalating costs to find the last few individuals will exceed the fixed price one achieves by selling the individuals on the market. However, when rare species are more desirable than common species, prices for rare species can exceed high harvest costs. This phenomena can create an \"anthropogenic\" Allee effect where rare species go extinct but common species are sustainably harvested. The anthropogenic Allee effect has become a standard approach for conceptualizing the threat of economic markets on endangered species. However, the original theory was posited using a one dimensional analysis of a two dimensional model. It turns out that a two dimensional analysis yields an Allee curve in human exploiter and biological population space and that this curve separating species destined to extinction vs persistence can be complicated. Even very high population sizes can potentially pass through the originally proposed Allee thresholds on predestined paths to extinction. \n\nDeclines in population size can result in a loss of genetic diversity, and owing to genetic variation's role in the evolutionary potential of a species, this could in turn result in an observable Allee effect. As a species' population becomes smaller, its gene pool will be reduced in size as well. One possible outcome from this genetic bottleneck is a reduction in fitness of the species through the process of genetic drift, as well as inbreeding depression. This overall fitness decrease of a species is caused by an accumulation of deleterious mutations throughout the population. Genetic variation within a species could range from beneficial to detrimental. Nevertheless, in a smaller sized gene pool, there is a higher chance of a stochastic event in which deleterious alleles become fixed (genetic drift). While evolutionary theory states that expressed deleterious alleles should be purged through natural selection, purging would be most efficient only at eliminating alleles that are highly detrimental or harmful. Mildly deleterious alleles such as those that act later in life would be less likely to be removed by natural selection, and conversely, newly acquired beneficial mutations are more likely to be lost by random chance in smaller genetic pools than larger ones.\n\nAlthough the long-term population persistence of several species with low genetic variation has recently prompted debate on the generality of inbreeding depression, there are various empirical evidences for genetic Allee effects. One such case was observed in the endangered Florida panther (\"Puma concolor coryi\"). The Florida panther experienced a genetic bottleneck in the early 1990s where the population was reduced to ~ 25 adult individuals. This reduction in genetic diversity was correlated with defects that include lower sperm quality, abnormal testosterone levels, cowlicks, and kinked tails. In response, a genetic rescue plan was put in motion and several female pumas from Texas were introduced into the Florida population. This action quickly led to the reduction in the prevalence of the defects previously associated with inbreeding depression. Although the timescale for this inbreeding depression is larger than of those more immediate Allee effects, it has significant implications on the long-term persistence of a species.\n\nDemographic stochasticity refers to variability in population growth arising from sampling random births and deaths in a population of finite size. In small populations, demographic stochasticity will decrease the population growth rate, causing an effect similar to the Allee effect, which will increase the risk of population extinction. Whether or not demographic stochasticity can be considered a part of Allee effect is somewhat contentious however. The most current definition of Allee effect considers the correlation between population density and mean individual fitness. Therefore, random variation resulting from birth and death events would not be considered part of Allee effect as the increased risk of extinction is not a consequence of the changing fates of individuals within the population.\nMeanwhile, when demographic stochasticity results in fluctuations of sex ratios, it arguably reduces the mean individual fitness as population declines. For example, a fluctuation in small population that causes a scarcity in one sex would in turn limit the access of mates for the opposite sex, decreasing the fitness of the individuals within the population. This type of Allee effect will likely be more prevalent in monogamous species than polygynous species.\n\nDemographic and mathematical studies demonstrate that the existence of an Allee effect can reduce the speed of range expansion of a population and can even prevent biological invasions.\n\nRecent results based on spatio-temporal models show that the Allee effect can also promote genetic diversity in expanding populations. These results counteract commonly held notions that the Allee effect possesses net adverse consequences. Reducing the growth rate of the individuals ahead of the colonization front simultaneously reduces the speed of colonization and enables a diversity of genes coming from the core of the population to remain on the front. The Allee effect also affects the spatial distribution of diversity. Whereas spatio-temporal models which do not include an Allee effect lead to a vertical pattern of genetic diversity (i.e., a strongly structured spatial distribution of genetic fractions), those including an Allee effect lead to a \"horizontal pattern\" of genetic diversity (i.e., an absence of genetic differentiation in space).\n\nA simple mathematical example of an Allee effect is given by the cubic growth model\nwhere the population has a negative growth rate for formula_2, and \na positive growth rate for formula_3 (assuming formula_4).\nThis is a departure from the logistic growth equation\nwhere\n\nAfter dividing both sides of the equation by the population size N, in the logistic growth the left hand side of the equation represents the per capita population growth rate, which is dependent on the population size N, and decreases with increasing N throughout the entire range of population sizes. In contrast, when there is an Allee effect the per-capita growth rate increases with increasing N over some range of population sizes [0, N].\n\nSpatio-temporal models can take Allee effect into account as well. A simple example is given by the reaction-diffusion model\nwhere\n\nWhen a population is made up of small sub-populations additional factors to the Allee effect arise.\n\nIf the sub-populations are subject to different environmental variations (i.e. separated enough that a disaster could occur at one sub-population site without affecting the other sub-populations) but still allow individuals to travel between sub-populations, then the individual sub-populations are more likely to go extinct than the total population. In the case of a catastrophic event decreasing numbers at a sub-population, individuals from another sub-population site may be able to repopulate the area.\n\nIf all sub-populations are subject to the same environmental variations (i.e. if a disaster affected one, it would affect them all) then fragmentation of the population is detrimental to the population and increases extinction risk for the total population. In this case, the species receives none of the benefits of a small sub-population (loss of the sub-population is not catastrophic to the species as a whole) and all of the disadvantages (inbreeding depression, loss of genetic diversity and increased vulnerability to environmental instability) and the population would survive better unfragmented.\n\nClumping results due to individuals aggregating in response to: 1) local habitat or landscape differences, 2) daily and seasonal weather changes, 3) reproductive processes, or 4) as the result of social attractions.\n\n\n", "id": "1466225", "title": "Allee effect"}
{"url": "https://en.wikipedia.org/wiki?curid=2468049", "text": "Autotoxicity\n\nAutotoxicity is a biological phenomenon whereby a species inhibits growth or reproduction of members of that same species through the production of chemicals that are released into the environment. It is related to allelopathy but is technically different: autotoxicity means self-toxicity, and allelopathic effects refer to the effects of chemical compounds from one species on another after release into the environment; also, autotoxic effects are always inhibitory, whereas allelopathic effects are not necessarily inhibitory - they may stimulate other organisms.\n\nThis mechanism will result in reduced competition between members of the same species.\nInhibition on the growth of other plants will increase the availability of nutrients.\n\nIn cultivation, autotoxicity can make it difficult or impossible to grow the same species after harvest of a crop. For example, this is known in alfalfa and the tree \"Cunninghamia lanceolata\" Other species displaying autotoxicity include the rush \"Juncus effusus\" and the grass \"Lolium rigidum\".\n\nAutotoxicity in alfalfa is produced from the first seeding of the plant. The plant emits a chemical or chemicals into the soil that reduce the effectiveness of further alfalfa seedings. Studies show that the chemical is extractable from fresh alfalfa, is water-soluble, reduces germination, and prevents root growth. Some believe that a chemical called medicarpin is responsible for autotoxicity. Roots of affected plants can be swollen, curled, discolored, and lack root hairs. Lack of root hairs reduces the plants ability to gather nutrients and absorb water.\nCrop rotation is used to counteract autotoxicity in alfalfa.\n", "id": "2468049", "title": "Autotoxicity"}
{"url": "https://en.wikipedia.org/wiki?curid=17970483", "text": "Biofact (biology)\n\nIn biology, a biofact is dead material of a once-living organism.\n\nIn 1943, the protozoologist Bruno M. Klein of Vienna (1891–1968) coined the term in his article \"Biofakt und Artefakt\" in the microscopy journal \"Mikrokosmos\", though at that time it was not adopted by the scientific community. Klein's concept of biofact stressed the dead materials produced by living organisms as sheaths, such as shells.\n\nThe word \"biofact\" is now widely used in the zoo/aquarium world, but was first used in 1993 in the Education Department at the New England Aquarium, Boston, to refer to preserved items such as animal bones, skins, molts and eggs. The Accreditation Standards and Related Policies of the Association of Zoos and Aquariums states that biofacts can be useful education tools, and are preferable to live animals because of potential ethical considerations.\n\n", "id": "17970483", "title": "Biofact (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=7675689", "text": "Catabiosis\n\nCatabiosis is the process of growing older, aging and physical degradation.\n\nThe word comes from Greek \"kata\"—down, against, reverse and \"biosis\"—way of life and is generally used to describe senescence and degeneration in living organisms and biophysics of aging in general.\n\nOne of the popular catabiotic theories is the entropy theory of aging, where aging is characterized by thermodynamically favourable increase in structural disorder. Living organisms are open systems that take free energy from the environment and offload their entropy as waste. However, basic components of living systems—DNA, proteins, lipids and sugars—tend towards the state of maximum entropy while continuously accumulating damages causing catabiosis of the living structure. \n\nCatabiotic force on the contrary is the influence exerted by living structures on adjoining cells, by which the latter are developed in harmony with the primary structures. \n\n", "id": "7675689", "title": "Catabiosis"}
{"url": "https://en.wikipedia.org/wiki?curid=5387903", "text": "Commissure\n\nA commissure is the place where two things are joined. The term is used especially in the fields of anatomy and biology.\n\nIn anatomy, a \"commissure\" refers to a bundle of commissural fibers as a tract that crosses the midline at its level of origin or entry (as opposed to a decussation of fibers that cross obliquely). \n\nIn biology, the meeting of the two valves of a brachiopod or clam is a commissure; in botany, the term is used to denote the place where a fern's laterally expanded vein endings come together in a continuous marginal sorus.\n\n", "id": "5387903", "title": "Commissure"}
{"url": "https://en.wikipedia.org/wiki?curid=4122426", "text": "Modularity (biology)\n\nModularity refers to the ability of a system to organize discrete, individual units that can overall increase the efficiency of network activity and, in a biological sense, facilitates selective forces upon the network. Modularity is observed in all model systems, and can be studied at nearly every scale of biological organization, from molecular interactions all the way up to the whole organism.\n\nThe exact evolutionary origins of biological modularity has been debated since the 1990s. In the mid 1990s, Günter Wagner argued that modularity could have arisen and been maintained through the interaction of four evolutionary modes of action:\n\n[1] Selection for the rate of adaptation: If different complexes evolve at different rates, then those evolving more quickly reach fixation in a population faster than other complexes. Thus, common evolutionary rates could be forcing the genes for certain proteins to evolve together while preventing other genes from being co-opted unless there is a shift in evolutionary rate.\n\n[2] Constructional selection: When a gene exists in many duplicated copies, it may be maintained because of the many connections it has (also termed pleiotropy). There is evidence that this is so following whole genome duplication, or duplication at a single locus. However, the direct relationship that duplication processes have with modularity has yet to be directly examined.\n\n[3] Stabilizing selection: While seeming antithetical to forming novel modules, Wagner maintains that it is important to consider the effects of stabilizing selection as it may be \"an important counter force against the evolution of modularity\". Stabilizing selection, if ubiquitously spread across the network, could then be a \"wall\" that makes the formation of novel interactions more difficult and maintains previously established interactions. Against such strong positive selection, other evolutionary forces acting on the network must exist, with gaps of relaxed selection, to allow focused reorganization to occur.\n\n[4] Compounded effect of stabilizing and directional selection: This is the explanation seemingly favored by Wagner and his contemporaries as it provides a model through which modularity is constricted, but still able to unidirectionally explore different evolutionary outcomes. The semi-antagonistic relationship is best illustrated using the corridor model, whereby stabilizing selection forms barriers in \"phenotype space\" that only allow the system to move towards the \"optimum\" along a single path. This allows directional selection to act and inch the system closer to optimum through this evolutionary corridor.\n\nFor over a decade, researchers examined the dynamics of selection on network modularity. However, in 2013 Clune and colleagues challenged the sole focus on selective forces, and instead provided evidence that there are inherent \"connectivity costs\" that limit the number of connections between nodes to maximize efficiency of transmission. This hypothesis originated from neurological studies that found that there is an inverse relationship between the number of neural connections and the overall efficiency (more connections seemed to limit the overall performance speed/precision of the network). This connectivity cost had yet to be applied to evolutionary analyses. Clune et al. created a series of models that compared the efficiency of various \"evolved\" network topologies in an environment where performance, their only metric for selection, was taken into account, and another treatment where performance as well as the connectivity cost were factored together. The results show not only that modularity formed ubiquitously in the models that factored in connection cost, but that these models also outperformed the performance-only based counterparts in every task. This suggests a potential model for module evolution whereby modules form from a system’s tendency to resist maximizing connections to create more efficient and compartmentalized network topologies.\n\n\n", "id": "4122426", "title": "Modularity (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=8474825", "text": "Cranial vault\n\nThe cranial vault is the space in the skull within the neurocranium, occupied by the brain. In humans, the size and shape of the brain, may be affected by the size of the vault as shown in craniometry, but studies relating it to intelligence have found no conclusive evidence. The vault is alternatively called \"skullcap\" or even calvaria, though these properly refer to the upper portion of the skull only.\n\nIn humans, the cranial vault is imperfectly composed in newborns, to allow the large human head to pass through the birth canal. During birth, the various bones connected by cartilage and ligaments only will move relatively to each other. The open portion between the major bones of the upper part of the vault, called fontanelles, normally remain soft up to two years after birth. \n\nAs the fontanelles close, the vault loses some of its plasticity. The sutures between the bones remain until 30 to 40 years of age, allowing for growth of the brain. Cranial vault size is directly proportional to skull size and is developed early.\n\nThe size and shape of the brain and the surrounding vault remain quite plastic as the brain grows in childhood. In several ancient societies, head shape was altered for aesthetic or religious reasons by binding cloth or boards tightly around the head during infancy. It is not known whether such artificial cranial deformation has an effect in mental capacity.\n\nThe cranial vault is composed of the endocranium forming the basal parts, topped by the skull roof in land vertebrates.\n\nIn fishes no distinct cranial vault as such exists, the skull being composed of loosely jointed bones. The cranial vault as a distinct unit arose with the fusion of the skull roof and the endocranium on the early Labyrinthodonts. In amphibians and reptiles the vault is rather small and inconspicuous, only forming proper vaults in mammals and birds.\n\n", "id": "8474825", "title": "Cranial vault"}
{"url": "https://en.wikipedia.org/wiki?curid=1418389", "text": "Phototroph\n\nPhototrophs (\"Gr\": φῶς, φωτός = light, τροϕή = nourishment) are the organisms that carry out photon capture to acquire energy. They use the energy from light to carry out various cellular metabolic processes. It is a common misconception that phototrophs are obligatorily photosynthetic. Many, but not all, phototrophs often photosynthesize: they anabolically convert carbon dioxide into organic material to be utilized structurally, functionally, or as a source for later catabolic processes (e.g. in the form of starches, sugars and fats). All phototrophs either use electron transport chains or direct proton pumping to establish an electro-chemical gradient which is utilized by ATP synthase, to provide the molecular energy currency for the cell. \nPhototrophs can be either autotrophs or heterotrophs. As their electron and hydrogen donors are inorganic compounds [Na2S2O3 (PSB) and H2S (GSB)] they can be also called as lithotrophs, and so, some photoautotrophs are also called photolithoautotrophs. Examples of phototroph organims: \"Rhodobacter capsulatus\", \"Chromatium\", \"Chlorobium\" etc.\n\nOriginally used with a different meaning, the term took its current definition after Lwoff and collaborators (1946).\n\nMost of the well-recognized phototrophs are autotrophic, also known as photoautotrophs, and can fix carbon. They can be contrasted with chemotrophs that obtain their energy by the oxidation of electron donors in their environments. Photoautotrophs are capable of synthesizing their own food from inorganic substances using light as an energy source. Green plants and photosynthetic bacteria are photoautotrophs. Photoautotrophic organisms are sometimes referred to as holophytic. Such organisms derive their energy for food synthesis from light and are capable of using carbon dioxide as their principal source of carbon.\n\nOxygenic photosynthetic organisms use chlorophyll for light-energy capture and oxidize water, \"splitting\" it into molecular oxygen. In contrast, anoxygenic photosynthetic bacteria have a substance called bacteriochlorophyll - which absorbs predominantly at non-optical wavelengths - for light-energy capture, live in aquatic environments, and will, using light, oxidize chemical substances such as hydrogen sulfide rather than water.\n\nIn an ecological context, phototrophs are often the food source for neighboring heterotrophic life. In terrestrial environments, plants are the predominant variety, while aquatic environments include a range of phototrophic organisms such as algae (e.g., kelp), other protists (such as euglena), phytoplankton, and bacteria (such as cyanobacteria). The depth to which sunlight or artificial light can penetrate into water, so that photosynthesis may occur, is known as the photic zone.\n\nCyanobacteria, which are prokaryotic organisms which carry out oxygenic photosynthesis, occupy many environmental conditions, including fresh water, seas, soil, and lichen. Cyanobacteria carry out plant-like photosynthesis because the organelle in plants that carries out photosynthesis is derived from an endosymbiosis cyanobacterium. This bacterium can use water as a source of electrons in order to perform CO reduction reactions. Evolutionarily, cyanobacteria's ability to survive in oxygenic conditions, which are considered toxic to most anaerobic bacteria, might have given the bacteria an adaptive advantage which could have allowed the cyanobacteria to populate more efficiently.\n\nA \"photolithoautotroph\" is an autotrophic organism that uses light energy, and an inorganic electron donor (e.g., HO, H, HS), and CO as its carbon source. Examples include plants.\n\nIn contrast to photoautotrophs, photoheterotrophs are organisms that depend solely on light for their energy and principally on organic compounds for their carbon. Photoheterotrophs produce ATP through photophosphorylation but use environmentally obtained organic compounds to build structures and other bio-molecules.\n\n\n", "id": "1418389", "title": "Phototroph"}
{"url": "https://en.wikipedia.org/wiki?curid=1022111", "text": "Monoclonal\n\nMonoclonal cells are defined as a group of cells produced from a single ancestral cell by repeated cellular replication. Thus they can be said to form a single \"clone\". The term \"monoclonal\" comes from the Ancient Greek \"monos\", meaning \"alone\" or \"single\", and \"klon\", denoting \"twig\".\n\nThe process of replication can occur \"in vivo\", or may be stimulated \"in vitro\" for laboratory manipulations. The use of the term typically implies that there is some method to distinguish between the cells of the original population from which the single ancestral cell is derived, such as a random genetic alteration, which is inherited by the progeny.\n\nThe most common usages of this term are:\n", "id": "1022111", "title": "Monoclonal"}
{"url": "https://en.wikipedia.org/wiki?curid=8271812", "text": "Septate\n\nSeptate is a morphological term defined in biology in two different instances:\n\n\n", "id": "8271812", "title": "Septate"}
{"url": "https://en.wikipedia.org/wiki?curid=162797", "text": "Individuation\n\nThe principle of individuation, or , describes the manner in which a thing is identified as distinguished from other things.\n\nThe concept appears in numerous fields and is encountered in works of Carl Gustav Jung, Gilbert Simondon, Alan Watts, Bernard Stiegler, Friedrich Nietzsche, Arthur Schopenhauer, David Bohm, Henri Bergson, Gilles Deleuze, and Manuel De Landa.\n\nThe word \"individuation\" occurs with different meanings and connotations in different fields.\n\nPhilosophically, \"individuation\" expresses the general idea of how a thing is identified as an individual thing that \"is not something else\". This includes how an individual person is held to be distinct from other elements in the world and how a person is distinct from other persons.\n\nIn Jungian psychology, also called analytical psychology, individuation is the process in which the individual self develops out of an undifferentiated unconscious – seen as a developmental psychic process during which innate elements of personality, the components of the immature psyche, and the experiences of the person's life become, if the process is more or less successful, integrated over time into a well-functioning whole.\nThe media industry has begun using the term \"individuation\" to denote new printing and online technologies that permit mass customization of the contents of a newspaper, a magazine, a broadcast program, or a website so that its contents match each individual user's unique interests. This differs from the traditional mass-media practice of producing the same contents for all readers, viewers, listeners, or online users.\n\nCommunications theorist Marshall McLuhan alluded to this trend when discussing the future of printed books in an electronically interconnected world.\n\nTwo quantum entangled particles cannot be understood independently. Two or more states in quantum superposition, e.g., as in Schrödinger's cat being simultaneously in a half dead and half alive state, is mathematically not the same as assuming the cat is in an individual alive state with 50% probability. The Heisenberg's uncertainty principle says that complementary variables, such as position and momentum, cannot both be precisely known - in some sense, they are not individual variables. A \"natural criterion of individuality\" has been suggested.\n\nAccording to Jungian psychology, individuation () is a process of psychological integration. \"In general, it is the process by which individual beings are formed and differentiated [from other human beings]; in particular, it is the development of the psychological individual as a being distinct from the general, collective psychology.\"\n\nIndividuation is a process of transformation whereby the personal and collective unconscious are brought into consciousness (e.g., by means of dreams, active imagination, or free association) to be assimilated into the whole personality. It is a completely natural process necessary for the integration of the psyche. Individuation has a holistic healing effect on the person, both mentally and physically.\n\nIn addition to Jung's theory of complexes, his theory of the individuation process forms conceptions of an unconscious filled with mythic images, a non-sexual libido, the general types of extraversion and introversion, the compensatory and prospective functions of dreams, and the synthetic and constructive approaches to fantasy formation and utilization.\n\n\"The symbols of the individuation process . . . mark its stages like milestones, prominent among them for Jungians being the shadow, the wise old man . . . and lastly the anima in man and the animus in woman.\" Thus, \"There is often a movement from dealing with the persona at the start . . . to the ego at the second stage, to the shadow as the third stage, to the anima or animus, to the Self as the final stage. Some would interpose the Wise Old Man and the Wise Old Woman as spiritual archetypes coming before the final step of the Self.\"\n\nIn \"L'individuation psychique et collective\", Gilbert Simondon developed a theory of individual and collective individuation in which the individual subject is considered as an effect of individuation rather than a cause. Thus, the individual atom is replaced by a never-ending ontological process of individuation.\n\nSimondon also conceived of \"pre-individual fields\" which make individuation possible. Individuation is an ever-incomplete process, always leaving a \"pre-individual\" left over, which makes possible future individuations. Furthermore, individuation always creates both an individual subject and a collective subject, which individuate themselves concurrently.\n\nThe philosophy of Bernard Stiegler draws upon and modifies the work of Gilbert Simondon on individuation and also upon similar ideas in Friedrich Nietzsche and Sigmund Freud. During a talk given at the Tate Modern art gallery in 2004, Stiegler summarized his understanding of individuation. The essential points are the following:\n\n\n\n", "id": "162797", "title": "Individuation"}
{"url": "https://en.wikipedia.org/wiki?curid=25381180", "text": "Pyrrolizidine alkaloid sequestration\n\nPyrrolizidine alkaloid sequestration by insects is done for defense and mating purposes. Various species of insects have been known to utilize molecular compounds from plants for their own defense and even as their pheromones or precursors to their pheromones. A few Lepidoptera have been found to sequester chemicals from plants which they retain throughout their life and Arctiidae is no exception to this strategy. Starting in the mid-twentieth century researchers investigated various members of Arctiidae, and how these insects sequester pyrrolizidine alkaloids (PAs) during their life stages, and utilize these chemicals as adults for pheromones or pheromone precursors. PAs are also used by members of the Arctiidae for defense against predators throughout the life of the insect.\n\nPyrrolizidine alkaloids are a group of chemicals produced by plants as secondary metabolites, all of which contain a pyrrolizidine nucleus. This nucleus is made up of two pyrrole rings bonded by one carbon and one nitrogen. There are two forms in which PAs can exist and will readily interchange between: a pro-toxic free base form, also called a tertiary amine, or in a non-toxic form of N-oxide.\n\nResearchers have collected data that strongly suggests that PAs can be registered by taste receptors of predators, acting as a deterrent from being ingested. Taste receptors are also used by the various moth species that sequester PAs, which often stimulates them to feed. As of 2005, all of the PA sequestering insects that have been studied have all evolved a system to keep concentrations of the PA pro-toxic form low within the insect’s tissues.\n\nResearchers have found a number of Arctiidae that utilize PAs for protection and for male pheromones or precursors of the male pheromones, and some studies have found evidence suggesting PAs have behavioral and developmental effects. \"Estigmene acrea\", \"Cosmosoma myrodora\", \"Utetheisa ornatrix\", \"Creatonotos gangis\" and \"Creatonotos transiens\" are all members of the family Arctiidae and found to use PAs for their defense and/or male pheromones. Parsimony suggests that the sequestering of PAs in the larval stage evolved in the subfamily Arctiinae common ancestor. The loss of ability to sequester and utilize PAs has occurred in a number of species, along with the switch from larval uptake to adult uptake of PAs occurring multiple times within the Arctiinae taxon.\nMembers of Arctiidae typically sequester PAs from their diets, but sometimes must specifically ingest fluids excreted by plants that are not a part of their diets. Sequestered PAs are kept in various tissues and varying concentration which is dependent upon the species. PAs are found in the cuticle of all studied Arctiidae mentioned here, but some also package these chemicals into their spermatophores as seen in \"Creatonotos gangis\" and \"Creatonotos transiens\". The display of PAs on the exoskeleton is believed to cue predators to the unpalatability of the prey.\n\nEisner and Eisner looked at the palatability of PA positive and negative \"U. ornatrix\" to wolf spiders, \"Lycosa ceratiola\", in both the larval form and adult form. They found that the pyrrolizidine-positive organisms were typically released unharmed by spiders except in two field circumstances where the larvae were probably envenomated prior to the spider’s release and died two days after the attack. All of the PA-negative organisms were eaten by spiders. These findings were in line with prior studies done by Eisner and Meinwald which looked at orb weavers and \"U. ornatrix\", along with spiders being fed beetle larva covered in PAs, which they rejected. All of these findings support PAs being utilized for defense against predation.\n\nStudies have further elucidated the defenses and uses of PAs in Arctiidae. One study researched \"C. myrodora\" and how PAs protect this species from spider predation among other things. It found that PAs ingested from fluids excreted by plants aided in defense from predation. All organisms permitted access to PA-containing diets that were fed to spiders were cut loose from the webs. Females that had PA-deprived diets, but were allowed to mate with PA-positive males, were also released from the spider’s webs. Further observations showed that male \"C. myrodora\" have a pair of pouches where they produce PA-laden filaments, which are typically released over the female prior to copulation as a nuptial gift. Experiments show that the filaments give the females more PAs, explaining why spiders released mated PA-negative females from their webs. Most of the PAs from the males were subsequently transferred to the eggs when deposited. Three clusters of eggs that were laid after copulation with a PA-positive male all tested positive for alkaloids and the one cluster that resulted from a PA-negative male copulation tested negative. By the eggs getting a dose of PAs, the authors suggest that the eggs are being protected from predators such as Coccinellidae beetles.\n\nJordan and others’ study found a very interesting effect of the larval ingestion of PAs. Male \"Estigmene acrea\" moths that consumed PAs in their diet as larvae produced hydroxydanaidal, a volatile PA compound, and displayed their coremata: a bifid, inflatable male-specific organ, utilized in dispersing pheromones in the adult stage. Larvae that were fed diets without PAs rarely displayed their coremata and did not produce hydroxydanaidal. E. acrea have been observed in the wild displaying their coremata, an activity which attracts both males and females and is known as \"lekking\". Lekking was described by Willis and Birch in 1982, but larvae raised in the laboratory prior to this study rarely engaged in lekking or corematal displays. Scientists were unsure of why this phenomenon didn’t occur in the lab, but laboratory raised larvae were usually reared on commercially available food which lacks PAs. The authors suggest that the PAs are used by the males to attract other moths by releasing the volatile PA hydroxydanaidal into the air. It is suggested in this study that this strategy of mate attraction came about by tapping into the PA affinity already programmed into the moths for feeding, which is further supported by the observation that E. acrea females release their pheromones a little bit later in the evening than the males.\nSimilar uses of coremata to attract other moths have been observed in \"C. gangis\" and \"C. transiens\" along with altered development of coremata when larvae are reared without PAs. Boppre and Schneider observed adult males of these two species that were not permitted to eat PAs. Their coremata only developed into two, stalk-like projections with very few hairs arising from these stalks. Males that were given plants that produced PAs to feed upon, developed long coremata with four tubes, each longer than the males body, and each tube was highly pubescent. The authors suggest from this observation that there is a basic corematal phenotype, the two stalked coremata, and that PAs are required to form full coremata which is much larger and more elaborate than the basic corematal expression. These observations were further investigated by feeding larvae different amounts of PAs which had a direct correlation to the development of the coremata, which reached a maximum plateau around 2 mg of PAs ingested while in larval form. Similar to Jordan and others’ findings, the males raised on a diet devoid of PAs did not produce hydroxydanaidal.\n", "id": "25381180", "title": "Pyrrolizidine alkaloid sequestration"}
{"url": "https://en.wikipedia.org/wiki?curid=973095", "text": "Auxology\n\nAuxology, sometimes called auxanology (from Greek , \"auxō\", or , \"auxanō\", \"grow\"; and , \"-logia\"), is a meta-term covering the study of all aspects of human physical growth. (Although, it is also fundamental of biology.) Auxology is a multi-disciplinary science involving health sciences/medicine (pediatrics, general practice, endocrinology, neuroendocrinology, physiology, epidemiology), and to a lesser extent: nutrition, genetics, anthropology, anthropometry, ergonomics, history, economic history, economics, socioeconomics, sociology, public health, and psychology, among others.\n\n", "id": "973095", "title": "Auxology"}
{"url": "https://en.wikipedia.org/wiki?curid=24100660", "text": "Facultative\n\nFacultative means \"optional\" or \"discretionary\" (antonym \"obligate\"), used mainly in biology in phrases such as:\n\n", "id": "24100660", "title": "Facultative"}
{"url": "https://en.wikipedia.org/wiki?curid=7286489", "text": "Obligate\n\nAs an adjective, obligate means \"by necessity\" (antonym \"facultative\") and is used mainly in biology in phrases such as:\n\n", "id": "7286489", "title": "Obligate"}
{"url": "https://en.wikipedia.org/wiki?curid=3353280", "text": "Glossary of invasion biology terms\n\nThe need for a clearly defined and consistent invasion biology terminology has been acknowledged by many sources. \"Invasive species\", or \"invasive exotics\", is a nomenclature term and categorization phrase used for flora and fauna, and for specific restoration-preservation processes in native habitats. \"Invasion biology\" is the study of these organisms and the processes of species invasion. \n\nThe terminology in this page contains definitions for invasion biology terms in common usage today, taken from accessible publications. References for each definition are included. Terminology relates primarily to invasion biology terms with some ecology terms included to clarify language and phrases on linked pages.\n\nDefinitions of \"invasive non-indigenous species have been inconsistent\", which has led to confusion both in literature and in popular publications (Williams and Meffe 2005). Also, many scientists and managers feel that there is no firm definition of non-indigenous species, native species, exotic species, \"and so on, and ecologists do not use the terms consistently.\" (Shrader-Frechette 2001) Another question asked is whether current language is likely to promote \"effective and appropriate action\" towards invasive species through cohesive language (Larson 2005). Biologists today spend more time and effort on invasive species work because of the rapid spread, economic cost, and effects on ecological systems, so the importance of effective communication about invasive species is clear. (Larson 2005)\n\nControversy in invasion biology terms exists because of past usage and because of preferences for certain terms. Even for biologists, defining a species as native may be far from being a straightforward matter of biological classification based on the location or the discipline a biologist is working in (Helmreich 2005). Questions often arise as to what exactly makes a species native as opposed to non-native, because some non-native species have no known negative effects (Woods and Moriarty 2001). Natural biological invasions, generally considered range expansions, and introductions involving human activities are important and could be considered a normal ecological process (Vermeij 2005). Non-native and native species may be sometimes considered invasive, and these invasions often follow human-induced landscape changes, with subsequent damage to existing landscapes a value judgment (Foster and Sandberg 2004). As a result, many important terms relevant to invasion biology, such as invasive, weed, or transient, include qualities that are \"open to subjective interpretation\" (Colautti and MacIsaac 2004). Sometimes one species can have both beneficial and detrimental effects, such as the Mosquito fish (\"Gambusia affinis\"), which has been widely introduced because of its suppression of larval mosquitoes, although it also has negative impacts on native species of insects, fish and amphibians (Colautti and MacIsaac 2004).\n\nThe large number and current complexity of terms makes interpretation of some of the invasion biology literature challenging and intimidating. Exotic, alien, transplanted, introduced, non-indigenous, and invasive are all words that have been used to describe plants and animals that have been moved beyond their native ranges by humans (Williams and Meffe 2005), along with other terms such as foreign, injurious, aquatic nuisance, pest, non-native, all with a particular implication. Even the use of what seem to be simple, basic terms to articulate ecological concepts \"can confuse ideological debates and undermine management efforts\" (Colautti and MacIsaac 2004). Attempts to redefine commonly used terms in invasion biology have been difficult because many authors and biologists are particular to a favorite definition (Colautti and MacIsaac 2004). Also, the status and identification of any species as an invader, a weed, or an exotic are \"conditioned by cultural and political circumstances.\" (Robbins 2004)\n\nWhere words in a sentence are also defined elsewhere in this article, they appear in italics.\n\n\n\n\n\n\n\n", "id": "3353280", "title": "Glossary of invasion biology terms"}
{"url": "https://en.wikipedia.org/wiki?curid=26137572", "text": "Plant litter\n\nLitterfall, plant litter, leaf litter, tree litter, soil litter, or duff, is dead plant material, such as leaves, bark, needles, twigs, and cladodes; that have fallen to the ground. This detritus or dead organic material and its constituent nutrients are added to the top layer of soil, commonly known as the litter layer or O horizon (\"O\" for \"organic\"). Litter has occupied the attention of ecologists at length for the reasons that it is an instrumental factor in ecosystem dynamics, it is indicative of ecological productivity, and may be useful in predicting regional nutrient cycling and soil fertility.\n\nLitterfall is characterized as fresh, undecomposed, and easily recognizable (by species and type) plant debris. This can be anything from leaves, cones, needles, twigs, bark, seeds/nuts, logs, or reproductive organs (e.g. the stamen of flowering plants). Items larger than 2 cm diameter are referred to as coarse litter, while anything smaller is referred to as fine litter or litter. The type of litterfall is most directly affected by ecosystem type. \nFor example, leaf tissues account for about 70 percent of litterfall in forests, but woody litter tends to increase with forest age. In grasslands, there is very little aboveground perennial tissue so the annual litterfall is very low and quite nearly equal to the net primary production.\n\nIn soil science, soil litter is classified in three layers, which form on the surface of the O Horizon. These are the L, F, and H layers:\nThe litter layer is quite variable in its thickness, decomposition rate and nutrient content and is affected in part by seasonality, plant species, climate, soil fertility, elevation, and latitude. The most extreme variability of litterfall is seen as a function of seasonality; each individual species of plant has seasonal losses of certain parts of its body, which can be determined by the collection and classification of plant litterfall throughout the year, and in turn affects the thickness of the litter layer. In tropical environments, the largest amount of debris falls in the latter part of dry seasons and early during wet season. As a result of this variability due to seasons, the decomposition rate for any given area will also be variable.\nLatitude also has a strong effect on litterfall rates and thickness. Specifically, litterfall declines with increasing latitude. In tropical rainforests, there is a thin litter layer due to the rapid decomposition, while in boreal forests, the rate of decomposition is slower and leads to the accumulation of a thick litter layer, also known as a mor. Net primary production works inversely to this trend, suggesting that the accumulation of organic matter is mainly a result of decomposition rate.\n\nSurface detritus facilitates the capture and infiltration of rainwater into lower soil layers. Soil litter protects soil aggregates from raindrop impact, preventing the release of clay and silt particles from plugging soil pores. Releasing clay and silt particles reduces the capacity for soil to absorb water and increases cross surface flow, accelerating soil erosion. In addition soil litter reduces wind erosion by preventing soil from losing moisture and providing cover preventing soil transportation.\n\nOrganic matter accumulation also helps protect soils from wildfire damage. Soil litter can be completely removed depending on intensity and severity of wildfires and season. Regions with high frequency wildfires have reduced vegetation density and reduced soil litter accumulation. Climate also influences the depth of plant litter. Typically humid tropical and sub-tropical climates have reduced organic matter layers and horizons due to year round decomposition and high vegetation density and growth. In temperate and cold climates, litter tends to accuculate and decompose slower due to a shorter growing season.\n\nNet primary production and litterfall are intimately connected. In every terrestrial ecosystem, the largest fraction of all net primary production is lost to herbivores and litterfall. Due to their interconnectedness, global patterns of litterfall are similar to global patterns of net primary productivity.\n\nLitter provides habitat for a variety of organisms.\n\nCertain plants are specially adapted for germinating and thriving in the litter layers. For example, bluebell (\"Hyacinthoides non-scripta\") shoots puncture the layer to emerge in spring. Some plants with rhizomes, such as common wood sorrel (\"Oxalis acetosella\") do well in this habitat.\n\nMany organisms that live on the forest floor are decomposers, such as fungi. Organisms whose diet consists of plant detritus, such as earthworms, are termed detritivores. The community of decomposers in the litter layer also includes bacteria, amoeba, nematodes, rotifer, springtails, cryptostigmata, potworms, insect larvae, mollusks, oribatid mites, woodlice, and millipedes. Their consumption of the litterfall results in the breakdown of simple carbon compounds into carbon dioxide (CO) and water (HO), and releases inorganic ions (like nitrogen and phosphorus) into the soil where the surrounding plants can then reabsorb the nutrients that were shed as litterfall. In this way, litterfall becomes an important part of the nutrient cycle that sustains forest environments.\n\nAs litter decomposes, nutrients are released into the environment. The portion of the litter that is not readily decomposable is known as humus. Litter aids in soil moisture retention by cooling the ground surface and holding moisture in decaying organic matter. The flora and fauna working to decompose soil litter also aid in soil respiration. A litter layer of decomposing biomass provides a continuous energy source for macro- and micro-organisms.\n\nNumerous reptiles, amphibians, birds, and even some mammals rely on litter for shelter and forage. For example, amphibians such as salamanders and caecilians inhabit the damp microclimate underneath fallen leaves for part or all of their life cycle. This makes them difficult to observe. A BBC film crew captured footage of a female caecilian with young for the first time in a documentary that aired in 2008.\nSome species of birds require leaf litter both for foraging and as material used to construct nests, such as the ovenbird of eastern North America. Sometimes litterfall even provides energy to much larger mammals, such as in boreal forests where lichen litterfall is one of the main constituents of wintering deer and elk diets.\n\nDuring leaf senescence, a portion of the plant’s nutrients are reabsorbed into the leaves. The nutrient concentrations in litterfall differ from the nutrient concentrations in the mature foliage by the reabsorption of constituents during leaf senescence. Plants that grow in areas with low nutrient availability tend to produce litter with low nutrient concentrations, as a larger proportion of the available nutrients is reabsorbed. After senescence, the nutrient-enriched leaves become litterfall and settle on the soil below.\n\nLitterfall is the dominant pathway for nutrient return to the soil, especially for nitrogen (N) and phosphorus (P). The accumulation of these nutrients in the top layer of soil is known as soil immobilization. Once the litterfall has settled, decomposition of the litter layer, accomplished through the leaching of nutrients by rainfall and throughfall and by the efforts of detritivores, releases the breakdown products into the soil below and therefore contributes to the cation exchange capacity of the soil. This holds especially true for highly weathered tropical soils.\n\nLeaching is the process by which cations such as iron (Fe) and aluminum (Al), as well as organic matter are removed from the litterfall and transported downward into the soil below. This process is known as podzolization and is particularly intense in boreal and cool temperate forests that are mainly constituted by coniferous pines whose litterfall is rich in phenolic compounds and fulvic acid.\n\nBy the process of biological decomposition by microfauna, bacteria, and fungi, CO and HO, nutrient elements, and an exceedingly resistant organic compound called humus are released. Humus composes the bulk of organic matter in the lower soil profile.\n\nThe decline of nutrient ratios is also a function of decomposition of litterfall (i.e. as litterfall decomposes, more nutrients enter the soil below and the litter will have a lower nutrient ratio). Litterfall containing high nutrient concentrations will decompose more rapidly and asymptote as those nutrients decrease. Knowing this, ecologists have been able to use nutrient concentrations as measured by remote sensing as an index of a potential rate of decomposition for any given area. Globally, data from various forest ecosystems shows an inverse relationship in the decline in nutrient ratios to the apparent nutrition availability of the forest.\n\nOnce nutrients have re-entered the soil, the plants can then reabsorb them through their roots. Therefore, nutrient reabsorption during senescence presents an opportunity for a plant’s future net primary production use. A relationship between nutrient stores can also be defined as:\n\nThe main objectives of litterfall sampling and analysis are to quantify litterfall production and chemical composition over time in order to assess the variation in litterfall quantities, and hence its role in nutrient cycling across an environmental gradient of climate (moisture and temperature) and soil conditions.\n\nEcologists employ a simple approach to the collection of litterfall, most of which centers around once piece of equipment, known as a litterbag. A litterbag is simply any type of container that can be set out in any given area for a specified amount of time to collect the plant litter that falls from the canopy above.\n\nLitterbags are generally set in random locations within a given area and marked with GPS or local coordinates, and then monitored on a specific time interval. Once the samples have been collected, they are usually classified on type, size and species (if possible) and recorded on a spreadsheet. When measuring bulk litterfall for an area, ecologists will weigh the dry contents of the litterbag. By this method litterfall flux can be defined as:\n\nThe litterbag may also be used to study decomposition of the litter layer. By confining fresh litter in the mesh bags and placing them on the ground, an ecologist can monitor and collect the decay measurements of that litter. An exponential decay pattern has been produced by this type of experiment: formula_1, where formula_2 is the initial leaf litter and formula_3 is a constant fraction of detrital mass.\n\nThe mass-balance approach is also utilized in these experiments and suggests that the decomposition for a given amount of time should equal the input of litterfall for that same amount of time.\n\nFor study various groups from edaphic fauna you need a different mesh sizes in the litterbags \n\nIn some regions of Australia and North America, earthworms have been introduced where they are not native. Non-native earthworms have led to environmental changes by accelerating the rate of decomposition of litter. These changes are being studied, but may have negative impacts on some inhabitants such as salamanders.\n\nThis thin, delicate layer of organic material can be easily affected by humans. For instance, forest litter raking as a replacement for straw in husbandry is an old non-timber practice in forest management that has been widespread in Europe since the seventeenth century. At its peak in 1853, an estimated 50 Kg dry litter per year was raked at the European level. This human disturbance, if not combined with other degradation factors, could promote podzolisation; if managed properly (for example, by burying litter removed after its use in animal husbandry), even the repeated removal of forest biomass may not have negative effects on pedogenesis.\n\n\n", "id": "26137572", "title": "Plant litter"}
{"url": "https://en.wikipedia.org/wiki?curid=25652596", "text": "Climax species\n\nClimax species, also called late seral, late-successional, K-selected or equilibrium species, are plant species that will remain essentially unchanged in terms of species composition for as long as a site remains undisturbed. They are the most shade-tolerant species of tree to establish in the process of forest succession. The seedlings of climax species can grow in the shade of the parent trees, ensuring their dominance indefinitely. A disturbance, such as fire, may kill the climax species, allowing pioneer or earlier successional species to re-establish for a time. They are the opposite of pioneer species, also known as ruderal, fugitive, opportunistic or R-selected species, in the sense that climax species are good competitors but poor colonizers, whereas pioneer species are good colonizers but poor competitors. Climax species dominate the climax community, when the pace of succession slows down, the result of ecological homeostasis, which features maximum permitted biodiversity, given the prevailing ecological conditions. Their reproductive strategies and other adaptive characteristics can be considered more sophisticated than those of opportunistic species. Through negative feedback, they adapt themselves to specific environmental conditions. Climax species are mostly found in forests. Climax species, closely controlled by carrying capacity, follow K strategies, wherein species produce fewer numbers of potential offspring, but invest more heavily in securing the reproductive success of each one to the micro-environmental conditions of its specific ecological niche. Climax species might be iteroparous, energy consumption efficient and nutrient cycling.\n\nThe idea of a climax species has been criticized in recent ecological literature. Any assessment of successional states depends on assumptions about the natural fire regime. But the idea of a dominant species is still widely used in silvicultural programs and California Department of Forestry literature.\n\nWhite spruce (\"Picea glauca\") is an example of a climax species in the northern forests of North America.\n\nOther examples:\n\n\n", "id": "25652596", "title": "Climax species"}
{"url": "https://en.wikipedia.org/wiki?curid=17344084", "text": "Durophagy\n\nDurophagy is the eating behavior of animals that consume hard-shelled or exoskeleton bearing organisms, such as corals, shelled mollusks, or crabs. It is mostly used describing fish, but is also used when describing reptiles, including fossil turtles, placodonts and invertebrates, as well as \"bone-crushing\" mammalian carnivores such as hyenas. Durophagy requires special adaptions, such as blunt, strong teeth and a heavy jaw.\n\nIn the order Carnivora there are two dietary categories of durophagy, bonecrackers and bamboo eaters. Bonecrackers are exemplified by hyenas and saber-toothed cats, while bamboo eaters are primarily the giant panda and the red panda. Both have developed similar cranial morphology. However, the mandible morphology reveals more about their dietary resources. Both have a raised and dome-like anterior cranial, enlarged areas for the attachment of masticatory muscles, enlarged premolars, and reinforced tooth enamel. Bamboo eaters tend to larger mandibles, while bonecrackers have more sophisticated premolars.\n\n", "id": "17344084", "title": "Durophagy"}
{"url": "https://en.wikipedia.org/wiki?curid=23291939", "text": "Plant community\n\nA plant community (sometimes \"phytocoenosis\" or \"phytocenosis\") is a collection or association of plant species within a designated geographical unit, which forms a relatively uniform patch, distinguishable from neighboring patches of different vegetation types. The components of each plant community are influenced by soil type, topography, climate and human disturbance. In many cases there are several soil types within a given phytocoenosis. \n\nA plant community can be described \"floristically\" (the species it contains) and/or \"physiognomically\" (its physical structure). For example, a forest (a community of trees) includes the overstory, or upper tree layer of the canopy, as well as the understory, further subdivided into the shrub layer, herbaceous layer, and sometimes also moss layer. In some cases of complex forests there is also a well-defined lower tree layer. A plant community is similar in concept to a vegetation type, with the former having more of an emphasis on the ecological association of species within it, and the latter on overall appearance by which it is readily recognized by a layperson.\n\nA plant community can be rare even if none of the major species defining it are rare. This is because it is the association of species and relationship to their environment that may be rare. An example is the Sycamore Alluvial Woodland in California dominated by the California sycamore \"Platanus racemosa\". The community is rare, being localized to a small area of California and existing nowhere else, yet the California sycamore is not a rare tree in California.\n\nAn example is a grassland on the northern Caucasus Steppes, where common grass species found are \"Festuca sulcata\" and \"Poa bulbosa\". A common sedge in this grassland phytocoenosis is \"Carex shreberi\". Other representative forbs occurring in these steppe grasslands are \"Artemisia austriaca\" and \"Polygonum aviculare.\n\nAn example of a three tiered plant community is in Central Westland of South Island, New Zealand. These forests are the most extensive continuous reaches of podocarp/broadleaf forests in that country. The overstory includes miro, rimu and mountain totara. The mid-story includes tree ferns such as \"Cyathea smithii\" and \"Dicksonia squarrosa\", whilst the lowest tier and epiphytic associates include \"Asplenium polyodon\", \"Tmesipteris tannensis\", \"Astelia solandri\" and \"Blechnum discolor\".\n\n\n", "id": "23291939", "title": "Plant community"}
{"url": "https://en.wikipedia.org/wiki?curid=23617809", "text": "Permanent vegetative cover\n\nPermanent vegetative cover refers to trees, perennial bunchgrasses and grasslands, legumes, and shrubs with an\nexpected life span of at least 5 years.\n\nIn the United States, permanent cover is required on cropland entered into the Conservation Reserve Program.\n", "id": "23617809", "title": "Permanent vegetative cover"}
{"url": "https://en.wikipedia.org/wiki?curid=4190476", "text": "Phytogeography\n\nPhytogeography (from Greek φυτό, \"phyto\" = plant and γεωγραφία, \"geography\" meaning also distribution) or botanical geography is the branch of biogeography that is concerned with the geographic distribution of plant species and their influence on the earth's surface. Phytogeography is concerned with all aspects of plant distribution, from the controls on the distribution of individual species ranges (at both large and small scales, see species distribution) to the factors that govern the composition of entire communities and floras. Geobotany, by contrast, focuses on the geographic space's influence on plants.\n\nPhytogeography is part of a more general science known as biogeography. Phytogeographers are concerned with patterns and process in plant distribution. Most of the major questions and kinds of approaches taken to answer such questions are held in common between phyto- and zoogeographers.\n\nPhytogeography in wider sense (or geobotany, in German literature) encompasses four fields, according with the focused aspect, environment, flora (taxa), vegetation (plant community) and origin, respectively:\n\nPhytogeography is often divided into two main branches: ecological phytogeography and historical phytogeography. The former investigates the role of current day biotic and abiotic interactions in influencing plant distributions; the latter are concerned with historical reconstruction of the origin, dispersal, and extinction of taxa.\n\nThe basic data elements of phytogeography are occurrence records (presence or absence of a species) with operational geographic units such as political units or geographical coordinates. These data are often used to construct phytogeographic provinces (floristic provinces) and elements.\n\nThe questions and approaches in phytogeography are largely shared with zoogeography, except zoogeography is concerned with animal distribution rather than plant distribution. The term phytogeography itself suggests a broad meaning. How the term is actually applied by practicing scientists is apparent in the way periodicals use the term. The \"American Journal of Botany\", a monthly primary research journal, frequently publishes a section titled \"Systematics, Phytogeography, and Evolution.\" Topics covered in the \"American Journal of Botany\"'s \"Systematics and Phytogeography\" section include phylogeography, distribution of genetic variation and, historical biogeography, and general plant species distribution patterns. Biodiversity patterns are not heavily covered.\n\nPhytogeography has a long history. One of the subjects earliest proponents was Prussian naturalist Alexander von Humboldt, who is often referred to as the \"father of phytogeography\". Von Humboldt advocated a quantitative approach to phytogeography that has characterized modern plant geography.\n\nGross patterns of the distribution of plants became apparent early on in the study of plant geography. For example, Alfred Russel Wallace, co-discoverer of the principle of natural selection, discussed the Latitudinal gradients in species diversity, a pattern observed in other organisms as well. Much research effort in plant geography has since then been devoted to understanding this pattern and describing it in more detail.\n\nIn 1890, the United States Congress passed an act that appropriated funds to send expeditions to discover the geographic distributions of plants (and animals) in the United States. The first of these was The Death Valley Expedition, including Frederick Vernon Coville, Frederick Funston, Clinton Hart Merriam, and others.\nResearch in plant geography has also been directed to understanding the patterns of adaptation of species to the environment. This is done chiefly by describing geographical patterns of trait/environment relationships. These patterns termed ecogeographical rules when applied to plants represent another area of phytogeography. Recently, a new field termed macroecology has developed, which focuses on broad-scale (in both time and space) patterns and phenomena in ecology. Macroecology focuses as much on other organisms as plants.\n\nFloristics is a study of the flora of some territory or area. Traditional phytogeography concerns itself largely with floristics and floristic classification, see floristic province.\n\n\n\n", "id": "4190476", "title": "Phytogeography"}
{"url": "https://en.wikipedia.org/wiki?curid=460983", "text": "Ecotone\n\nAn ecotone is a transition area between two biomes. It is where two communities meet and integrate. It may be narrow or wide, and it may be local (the zone between a field and forest) or regional (the transition between forest and grassland ecosystems). An ecotone may appear on the ground as a gradual blending of the two communities across a broad area, or it may manifest itself as a sharp boundary line.\n\nThe word ecotone was coined from a combination of \"eco\"(logy) plus \"-tone\", from the Greek \"tonos\" or tension – in other words, a place where ecologies are in tension.\n\nThere are several distinguishing features of an ecotone. First, an ecotone can have a sharp vegetation transition, with a distinct line between two communities. For example,\na change in colors of grasses or plant life can indicate an ecotone. Second, a change in physiognomy (physical appearance of a plant species) can be a key indicator. Water bodies, such as estuaries, can also have a region of transition, and the boundary is characterized by the differences in heights of the macrophytes or plant species present in the areas because this distinguishes the two areas' accessibility to light. Scientists look at color variations and changes in plant height. Third, a change of species can signal an ecotone. There will be specific organisms on one side of an ecotone or the other.\n\nOther factors can illustrate or obscure an ecotone, for example, migration and the establishment of new plants. These are known as spatial mass effects, which are noticeable because some organisms will not be able to form self-sustaining populations if they cross the ecotone. If different species can survive in both communities of the two biomes, then the ecotone is considered to have species richness; ecologists measure this when studying the food chain and success of organisms. Lastly, the abundance of exotic species in an ecotone can reveal the type of biome or efficiency of the two communities sharing space. Because an ecotone is the zone in which two communities integrate, many different forms of life have to live together and compete for space. Therefore, an ecotone can create a diverse ecosystem.\n\nChanges in the physical environment may produce a sharp boundary, as in the example of the interface between areas of forest and cleared land. Elsewhere, a more gradually blended interface area will be found, where species from each community will be found together as well as unique local species. Mountain ranges often create such ecotones, due to the wide variety of climatic conditions experienced on their slopes. They may also provide a boundary between species due to the obstructive nature of their terrain. Mont Ventoux in France is a good example, marking the boundary between the flora and fauna of northern and southern France. Most wetlands are ecotones. The spatial variation of ecotones often form due to disturbances, creating patches that separate patches of vegetation. Different intensity of disturbances can cause landslides, land shifts, or movement of sediment that can create these vegetation patches and ecotones.\n\nPlants in competition extend themselves on one side of the ecotone as far as their ability to maintain themselves allows. Beyond this competitors of the adjacent community take over. As a result, the ecotone represents a shift in dominance. Ecotones are particularly significant for mobile animals, as they can exploit more than one set of habitats within a short distance. The ecotone contains not only species common to the communities on both sides; it may also include a number of highly adaptable species that tend to colonize such transitional areas. The phenomenon of increased variety of plants as well as animals at the community junction is called the edge effect and is essentially due to a locally broader range of suitable environmental conditions or ecological niches.\n\nAn ecotone is often associated with an ecocline: a \"physical transition zone\" between two systems. The ecotone and ecocline concepts are sometimes confused: an ecocline can signal an ecotone chemically (ex: pH or salinity gradient), or microclimatically (hydrothermal gradient) between two ecosystems.\n\nIn contrast:\n\n\n\n", "id": "460983", "title": "Ecotone"}
{"url": "https://en.wikipedia.org/wiki?curid=23515723", "text": "Landscape limnology\n\nLandscape limnology is the spatially explicit study of lakes, streams, and wetlands as they interact with freshwater, terrestrial, and human landscapes to determine the effects of pattern on ecosystem processes across temporal and spatial scales. Limnology is the study of inland water bodies inclusive of rivers, lakes, and wetlands; landscape limnology seeks to integrate all of these ecosystem types.\n\nThe terrestrial component represents spatial hierarchies of landscape features that influence which materials, whether solutes or organisms, are transported to aquatic systems; aquatic connections represent how these materials are transported; and human activities reflect features that influence how these materials are transported as well as their quantity and temporal dynamics.\n\nThe core principles or themes of landscape ecology provide the foundation for landscape limnology. These ideas can be synthesized into a set of four landscape ecology themes that are broadly applicable to any aquatic ecosystem type, and that consider the unique features of such ecosystems. \n\nA landscape limnology framework begins with the premise of Thienemann (1925). Wiens (2002): freshwater ecosystems can be considered patches. As such, the location of these patches and their placement relative to other elements of the landscape is important to the ecosystems and their processes. Therefore, the four main themes of landscape limnology are:\n\n(1) Patch characteristics: The characteristics of a freshwater ecosystem include its physical morphometry, chemical, and biological features, as well as its boundaries. These boundaries are often more easily defined for aquatic ecosystems than for terrestrial ecosystems (e.g., shoreline, riparian zones, and emergent vegetation zone) and are often a focal-point for important ecosystem processes linking terrestrial and aquatic components.\n\n(2) Patch context: The freshwater ecosystem is embedded in a complex terrestrial mosaic (e.g., soils, geology, and land use/cover) that has been shown to drive many within-ecosystem features and processes such as water chemistry, species richness, and primary and secondary productivity. \n\n(3) Patch connectivity and directionality: The complex freshwater mosaic is connected to the particular patch of interest and defines the degree to which materials and organisms move across the landscape through freshwater connections. For freshwater ecosystems, these connections often display a strong directionality component that must be explicitly considered. For example, a specific wetland can be connected through groundwater to other wetlands or lakes, or through surface water connections directly to lakes and rivers, or both, and the directionality of those connections will strongly impact the movement of nutrients and biota.\n\n(4) Spatial scale and hierarchy: Interactions among terrestrial and freshwater elements occur at multiple spatial scales that must be considered hierarchically. The explicit integration of hierarchy into landscape limnology is important because (a) many freshwater ecosystems are hierarchically organized and controlled by processes that are hierarchically organized, (b) most freshwater ecosystems are managed at multiple spatial scales, from policy set at the national level, to land management conducted at local scales, and (c) the degree of homogeneity among freshwater ecosystems can change in relation to the scale of observation.\n\nFindings from landscape limnology research are contributing to many facets of aquatic ecosystem research, management, and conservation. Landscape limnology is especially relevant for geographical areas with thousands of ecosystems (i.e. lake-rich regions of the world), in situations with a range of human disturbances, or when considering lakes, streams, and wetlands that are connected to other such ecosystems. For example, landscape limnology perspectives have contributed to the development of nutrient criteria for lakes, formation of classification systems that can be used to monitor the health of aquatic ecosystems, understanding ecosystem responses to environmental stressors, or explaining biogeographic patterns of community composition.\n\n", "id": "23515723", "title": "Landscape limnology"}
{"url": "https://en.wikipedia.org/wiki?curid=11589621", "text": "Patch dynamics\n\nPatch dynamics is an ecological perspective that the structure, function, and dynamics of ecological systems can be understood through studying their interactive patches. Patch dynamics, as a term, may also refer to the spatiotemporal changes within and among patches that make up a landscape. Patch dynamics is ubiquitous in terrestrial and aquatic systems across organizational levels and spatial scales. From a patch dynamics perspective, populations, communities, ecosystems, and landscapes may all be studied effectively as mosaics of patches that differ in size, shape, composition, history, and boundary characteristics. \n\nThe idea of patch dynamics dates back to the 1940s when plant ecologists studied the structure and dynamics of vegetation in terms of the interactive patches that it comprises. A mathematical theory of patch dynamics was developed by Simon Levin and Robert Paine in the 1970s, originally to describe the pattern and dynamics of an intertidal community as a patch mosaic created and maintained by tidal disturbances. Patch dynamics became a dominant theme in ecology between the late 1970s and the 1990s. \n\nPatch dynamics is a conceptual approach to ecosystem and habitat analysis that emphasizes dynamics of heterogeneity within a system (i.e. that each area of an ecosystem is made up of a mosaic of small 'sub-ecosystems').\n\nDiverse patches of habitat created by natural disturbance regimes are seen as critical to the maintenance of this diversity (ecology). A \"habitat patch\" is any discrete area with a definite shape, spatial and configuration used by a species for breeding or obtaining other resources. \"Mosaics\" are the patterns within landscapes that are composed of smaller elements, such as individual forest stands, shrubland patches, highways, farms, or towns.\n\nHistorically, due to the short time scale of human observation, mosaic landscapes were perceived to be static patterns of human population mosaics. This focus centered on the idea that the status of a particular population, community, or ecosystem could be understood by studying a particular patch within a mosaic. However, this perception ignored the conditions that interact with, and connect patches. In 1979, Bormann and Likens coined the phrase \"shifting mosaic\" to describe the theory that landscapes change and fluctuate, and are in fact dynamic. This is related to the battle of cells that occurs in a Petri dish.\n\n\"Patch dynamics\" refers to the concept that landscapes are dynamic. There are three states that a patch can exist in: \"potential\", \"active\", and \"degraded\". Patches in the \"potential\" state are transformed into active patches through colonization of the patch by dispersing species arriving from other \"active\" or \"degrading\" patches. Patches are transformed from the \"active\" state to the \"degraded\" state when the patch is abandoned, and patches change from \"degraded\" to \"potential\" through a process of recovery.\n\nLogging, fire, farming, and reforestation can all contribute to the process of colonization, and can effectively change the shape of the patch. \"Patch dynamics\" also refers to changes in the structure, function, and composition of individual patches that can, for example, effect the rate of nutrient cycling.\n\nPatches are also linked, although separated from other patches, migration occurs from one patch to another. This migration maintains the population of some patches, and can be the mechanism by which some plant species spread. This implies that ecological systems within landscapes are open, rather than closed and isolated. (Pickett, 2006)\n\nRecognizing the patch dynamics within a system is needed for conservation (ecology) efforts to succeed. Successful conservation includes understanding how a patch changes and predicting how they will be affected by external forces. These externalities include natural effects, such as land use, disturbance, restoration, and succession, and the effects of human activities. In a sense, conservation is the active maintenance of patch dynamics (Pickett, 2006). The analysis of patch dynamics could be used to predict changes in biodiversity of an ecosystem. When patches of species can be tracked, it has been shown that fluctuations on the biggest patch (the most dominant species) can be used as an early warning of a Biodiversity collapse . That means that if external conditions, like climate change and habitat fragmentation, change the internal dynamics of patches a sharp reduction in biodiversity can be detected before it is produced . \n\n\n", "id": "11589621", "title": "Patch dynamics"}
{"url": "https://en.wikipedia.org/wiki?curid=196971", "text": "Ecological land classification\n\nEcological land classification is a cartographical delineation or regionalisation of distinct ecological areas, identified by their geology, topography, soils, vegetation, climate conditions, living species, habitats, water resources, and sometimes also anthropic factors. These factors control and influence biotic composition and ecological processes.\n\nThe expression \"ecological land classification\" as understood in this article, is approximate with the biogeographical and ecological regionalisations in a scientific context (see biogeographic units).\n\nHowever, its actual usage is more approximate with a tool used for land management, in the context of environmental resource management.\n\nIn Canada ecological land classification schemes are commonly used. Provincial authorities have adopted methods to classify ecosystems within various ecoregions of the province. Ontario is one such province that uses an extensive method to define ecological units. Improvements in hand held technology have allowed for more efficient collection of vegetation and physiological data in the field, such as with the ELC eTool.\n\nMany different lists and ecological land classification schemes have been developed.\n\nFollowing, a comparison of classification schemes and terms used in the study of the biotic and abiotic components of ecosystems and the Earth in ecology and other fields.\n\nIn ecology:\n\nIn biogeography:\n\nIn zoogeography:\n\nIn phytogeography:\n\n\nFor the physiognomic approach, see Vegetation#Classifications.\n\nFor the association (phytosociological) approach, see Phytosociology#Classificatory traditions.\n\nIn physiography:\n\nIn Geology:\n\nIn pedology (soil study):\n\n\n\n\n\n", "id": "196971", "title": "Ecological land classification"}
{"url": "https://en.wikipedia.org/wiki?curid=14812859", "text": "Biomics\n\nBiomics is the biological study of biomes, and the processing of obtained information, such as ecological communities of plants, animals, and soil organisms.\n\nBiomics is part of biogeography, ecosystems, and habitats research. Ecoregions are grouped into both biomes and ecozones.\n\nBiomics, in molecular biology, uses bioinformatics to collectively analyze diverse biome data. A biome may contain very large scale omics information, such as metagenome and pangenome where genomic sequences are mass-produced.\n\n\n", "id": "14812859", "title": "Biomics"}
{"url": "https://en.wikipedia.org/wiki?curid=3398718", "text": "Z-value (temperature)\n\n\"F\" is defined as the number of equivalent minutes of steam sterilization at temperature 121.1°C delivered to a container or unit of product calculated using a z-value of 10°C. The term F-value or \"F\" is defined as the equivalent number of minutes to a certain reference temperature (T) for a certain control microorganism with an established Z-value. \n\nZ-value is a term used in microbial thermal death time calculations. It is the number of degrees the temperature has to be increased to achieve a tenfold (i.e. 1 log) reduction in the D-value. The D-value of an organism is the time required in a given medium, at a given temperature, for a ten-fold reduction in the number of organisms. It is useful when examining the effectiveness of thermal inactivations under different conditions, for example in food cooking and preservation. The z-value is a measure of the change of the D-value with varying temperature, and is a simplified version of an Arrhenius equation and it is equivalent to z=2.303 RT T/E .\n\nThe z-value of an organism in a particular medium is the temperature change required for the D-value to change by a factor of ten, or put another way, the temperature required for the thermal destruction curve to move one log cycle. It is the reciprocal of the slope resulting from the plot of the logarithm of the D-value versus the temperature at which the D-value was obtained. While the D-value gives the time needed at a certain temperature to kill 90% of the organisms, the z-value relates the resistance of an organism to differing temperatures. The z-value allows calculation of the equivalency of two thermal processes, if the D-value and the z-value are known.\n\nExample: if it takes an increase of 10°F to move the curve one log, then our z-value is 10. Given a D-value of 4.5 minutes at 150°F, the D-value can be calculated for 160°F by reducing the time by 1 log. The new D-value for 160°F given the z-value is 0.45 minutes. This means that each 10°F increase in temperature will reduce our D-value by 1 log. Conversely, a 10°F decrease in temperature will increase our D-value by 1 log. So, the D-value for a temperature of 140°F would be 45 minutes.\n\n", "id": "3398718", "title": "Z-value (temperature)"}
{"url": "https://en.wikipedia.org/wiki?curid=4173609", "text": "Tiller (botany)\n\nA tiller is a stem produced by grass plants, and refers to all shoots that grow after the initial parent shoot grows from a seed. Tillers are segmented, each segment possessing its own two-part leaf. They are involved in vegetative propagation and, in some cases, also seed production. \n\n\"Tillering\" refers to the production of side shoots and is a property possessed by many species in the family Poaceae. This enables them to produce multiple stems (tillers) starting from the initial single seedling. This ensures the formation of dense tufts and multiple seed heads. Tillering rates are heavily influenced by soil water status. When soil moisture is low, grasses tend to develop more sparse and deep root systems (as opposed to dense, lateral systems). Thus tillering is inhibited; the lateral nature of tillering is not supported by lateral root growth in dry soils.\n", "id": "4173609", "title": "Tiller (botany)"}
{"url": "https://en.wikipedia.org/wiki?curid=2447975", "text": "Colonisation (biology)\n\nColonisation or colonization is the process in biology by which a species spreads to new areas. Colonisation often refers to \"successful\" immigration where a population becomes integrated into a community, having resisted initial local extinction.\n\nOne classic model in biogeography posits that species must continue to colonize new areas through its life cycle (called a \"taxon cycle\") in order to achieve longevity. Accordingly, colonisation and extinction are key components of island biogeography, a theory that has many applications in ecology, such as metapopulations.\n\nColonisation occurs on several scales:\n\nThe term is generally only used to refer to the spread into new areas by \"natural\" means, as opposed to introduction or translocation by humans, which are called introduced species and sometimes becoming invasive species.\n\nSome large-scale notable colonisation events in the 20th Century are:\n\n\n\n\n", "id": "2447975", "title": "Colonisation (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=6743196", "text": "Live crown\n\nThe live crown is the top part of a tree, the part that has green leaves (as opposed to the bare trunk, bare branches, and dead leaves). The ratio of the size of a tree's live crown to its total height is used in estimating its health and its level of competition with neighboring trees.\n", "id": "6743196", "title": "Live crown"}
{"url": "https://en.wikipedia.org/wiki?curid=827837", "text": "Ecosystem engineer\n\nAn ecosystem engineer is any organism that creates, significantly modifies, maintains or destroys a habitat. These organisms can have a large impact on the species richness and landscape-level heterogeneity of an area. As a result, ecosystem engineers are important for maintaining the health and stability of the environment they are living in. Since all organisms impact the environment they live in in one way or another, it has been proposed that the term \"ecosystem engineers\" be used only for keystone species whose behavior very strongly affects other organisms.\n\nJones et al. identified two different types of ecosystem engineers:\n\nAllogenic engineers modify the environment (biophysical) by mechanically changing living or nonliving materials from one form to another. Beavers are the original model for ecosystem engineers; in the process of clearcutting and damming, beavers alter their ecosystem extensively. The addition of a dam will change both the distribution and the abundance of many organisms in the area. Caterpillars are another example in that by creating shelters from leaves, they are also creating shelters for other organisms which may occupy them either simultaneously or subsequently. An additional example may be that of woodpeckers or other birds who create holes in trees for them to nest in. Once these birds are through with them, the holes are used by other species of birds or mammals for housing.\n\nAutogenic engineers modify the environment by modifying themselves. Trees are a good example, because as they grow, their trunks and branches create habitats for other living things; these may include squirrels, birds or insects among others. In the tropics, lianas connect trees, which allow many animals to travel exclusively through the forest canopy.\n\nBeing able to identify ecosystem engineers in an environment can be important when looking at the influence these individuals may have over other organisms living in the same environment – especially in terms of resource availability.\n\nThe presence of some ecosystem engineers has been linked to higher species richness at the landscape level. By modifying the habitat, organisms like the beaver create more habitat heterogeneity and so can support species not found elsewhere. Thoughts may be that similar to other umbrella species by conserving an ecosystem engineer you may be able to protect the overall diversity of a landscape. Beavers have also been shown to maintain habitats in such a way as to protect the rare Saint Francis' satyr butterfly and increase plant diversity.\n\nBiodiversity may also be affected by ecosystem engineer's ability to increase the complexity of processes within an ecosystem, potentially allowing greater species richness and diversity in the local environments. As an example, beavers have the capacity to modify riparian forest and expand wetland habitats, which results in an increase of the diversity of the habitats by allowing a greater number of species to inhabit the landscape. Coral-reef habitats, created by the ecosystem engineer coral species, hold some of the highest abundances of aquatic species in the world.\n\nSpecies are able to be transported across all parts of the world by humans or human-made vessels at boundless rates resulting in foreign ecosystem engineers changing the dynamics of species interactions and the possibility for engineering to occur in locations that would not have been accessible by engineers without the mediation by humans.\n\nIntroduced species, which may be invasive species, are often ecosystem engineers. Kudzu, a leguminous plant introduced to the southeast U.S., changes the distribution and number of animal and bird species in the areas it invades. It also crowds out native plant species. The zebra mussel is an ecosystem engineer in North America. By providing refuge from predators, it encourages the growth of freshwater invertebrates through increasing microhabitats. Light penetration into infected lakes also improves the ecosystem, resulting in an increase in algae. In contrast to the benefits some ecosystem engineers can cause, invasive species often have the reverse effect.\n\nHumans are thought to be one of the most dramatic ecosystem engineers. Niche construction has been prevalent since the earliest days of human activity. Through urban development, agricultural practices, logging, damming and mining, humans have changed the way they interact with the environment. \nThis interaction is more studied in the field of human ecology.\n\nDue to the complexity of many communities and ecosystems, restoration projects are often difficult. Ecosystem engineers have been proposed as a means to restore a given area to its previous state. While ideally these would all be natural agents, with today's level of development some form of human intervention may be necessary as well. In addition to being able to assist in restoration ecology, ecosystem engineers may be a helpful agent in invasive species management. New fields are developing which focus on restoring those ecosystems which have been disrupted or destroyed by human activities as well as developing ecosystems that are sustainable with both human and ecological values.\n\nBesides the previously mentioned beaver acting as an ecosystem engineer, other terrestrial animals do the same. This may be through feeding habits, migration patterns or other behaviors that result in more permanent changes.\n\nResearch has suggested primates as ecosystem engineers as a result of their feeding strategies – frugivory and folivory – making them act as seed dispersers. As a whole primates are very abundant and feed on a large quantity of fruit that is then distributed around their territory. Elephants have also been designated ecosystem engineers as they cause very large changes to their environment whether it be through feeding, digging or migratory behavior.\n\nNot only animals are ecosystem engineers. Fungi are able to connect regions that are distant from one another and translocate nutrients between them. Doing so they create nutritional niches for xylophagous invertebrates, supply trees with N translocated from previously predated animals or even form an \"underground pipeline\" that redistributes carbon between trees. Thus fungi are engineers controlling nutrient cycles in ecosystems.\n\nPrairie dogs are another terrestrial form of allogenic ecosystem engineers due to the fact that the species has the ability to perform substantial modifications by burrowing and turning soil. They are able to influence soils and vegetation of the landscape while providing underground corridors for arthropods, avians, other small mammals, and reptiles. This has a positive effect on species richness and diversity of their habitats which results in the prairie dogs being labelled as keystone species.\n\nIn marine environments, filter feeders and plankton are ecosystem engineers because they alter turbidity and light penetration, controlling the depth at which photosynthesis can occur. This in turn limits the primary productivity of benthic and pelagic habitats and influences consumption patterns between trophic groups.\n\nAnother example of ecosystem engineers in marine environments would be scleractinian corals as they create the framework for the habitat most coral-reef organisms depend on. Some ecosystem engineers such as coral have help maintaining their environment. Parrotfish often help maintain coral reefs as they feed on macroalgae that competes with the coral. As this relationship is mutually beneficial, a positive feedback cycle is formed between the two organisms, making them both responsible for creating and maintaining coral reef ecosystems.\n\n\n\n", "id": "827837", "title": "Ecosystem engineer"}
{"url": "https://en.wikipedia.org/wiki?curid=1414572", "text": "Storage organ\n\nA storage organ is a part of a plant specifically modified for storage of energy \n(generally in the form of carbohydrates) or water. Storage organs often grow underground, where they are better protected from attack by herbivores. Plants that have an underground storage organ are called geophytes in the Raunkiær plant life-form classification system. Storage organs often, but not always, act as perennating organs which enable plants to survive adverse conditions (such as cold, excessive heat, lack of light or drought).\n\nStorage organs may act as perennating organs ('perennating' as in perennial, meaning \"through the year\", used in the sense of continuing beyond the year and in due course lasting for multiple years). These are used by plants to survive adverse periods in the plant's life-cycle (e.g. caused by cold, excessive heat, lack of light or drought). During these periods, parts of the plant die and then when conditions become favourable again, re-growth occurs from buds in the perennating organs. For example, geophytes growing in woodland under deciduous trees (e.g. bluebells, trilliums) die back to underground storage organs during summer when tree leaf cover restricts light and water is less available.\n\nHowever, perennating organs need not be storage organs. After losing their leaves, deciduous trees grow them again from 'resting buds', which are the perennating organs of phanerophytes in the Raunkiær classification, but which do not specifically act as storage organs. Equally, storage organs need not be perennating organs. Many succulents have leaves adapted for water storage, which they retain in adverse conditions.\n\nIn common parlance, underground storage organs may be generically called roots, tubers, or bulbs, but to the botanist there is more specific technical nomenclature:\n\n\nSome of the above, particularly pseudobulbs and caudices, may occur wholly or partially above ground. Intermediates and combinations of the above are also found, making classification difficult. As an example of an intermediate, the tuber of \"Cyclamen\" arises from the stem of the seedling, which forms the junction of the roots and stem of the mature plant. In some species (e.g. \"Cyclamen coum\") roots come from the bottom of the tuber, suggesting that it is a stem tuber; in others (e.g. \"Cyclamen hederifolium\") roots come largely from the top of the tuber, suggesting that it is a root tuber. As an example of a combination, juno irises have both bulbs and storage roots.\n\nUnderground storage organs used for food may be generically called root vegetables, although this phrase should not be taken to imply that the class only includes true roots.\n\nSucculents are plants which are adapted to withstand periods of drought by their ability to store moisture in specialized storage organs.\n\n", "id": "1414572", "title": "Storage organ"}
{"url": "https://en.wikipedia.org/wiki?curid=18952287", "text": "Woody plant\n\nA woody plant is a plant that produces wood as its structural tissue. Woody plants are usually either trees, shrubs, or lianas. These are usually perennial plants whose stems and larger roots are reinforced with wood produced from secondary xylem. The main stem, larger branches, and roots of these plants are usually covered by a layer of bark. Wood is a structural cellular adaptation that allows woody plants to grow from above ground stems year after year, thus making some woody plants the largest and tallest terrestrial plants.\n\nWood is primarily composed of xylem cells with cell walls made of cellulose and lignin. Xylem is a vascular tissue which moves water and nutrients from the roots to the leaves. Most woody plants form new layers of woody tissue each year, and so increase their stem diameter from year to year, with new wood deposited on the inner side of a vascular cambium layer located immediately beneath the bark. However, in some monocotyledons such as palms and dracaenas, the wood is formed in bundles scattered through the interior of the trunk.\n\nWoody herbs are herbaceous plants that develop hard woody stems. They include such plants as \"Uraria picta\" and certain species in family Polygonaceae. These herbs are not truly woody but have hard densely packed stem tissue. Other herbaceous plants have woody stems called a caudex, which is a thickened stem base often found in plants that grow in alpine or dry environments.\n\nUnder specific conditions, woody plants may decay or may in time become petrified wood.\n\nThe symbol for a woody plant, based on \"Species Plantarum\" by Linnaeus is , which is also the astronomical symbol for the planet Saturn.\n\n", "id": "18952287", "title": "Woody plant"}
{"url": "https://en.wikipedia.org/wiki?curid=551448", "text": "Liana\n\nA liana is any of various long-stemmed, woody vines that are rooted in the soil at ground level and use trees, as well as other means of vertical support, to climb up to the canopy to get access to well-lit areas of the forest. Lianas are characteristic of tropical moist deciduous forests (especially seasonal forests), but may be found in temperate rainforests. There are also temperate lianas, for example the members of the \"Clematis\" or \"Vitis\" (wild grape) genera. Lianas can form bridges amidst the forest canopy, providing arboreal animals with paths across the forest. These bridges can protect weaker trees from strong winds. Lianas compete with forest trees for sunlight, water and nutrients from the soil. Forests without lianas grow 150% more fruit; trees with lianas have twice the probability of dying.\n\nThe term \"liana\" is not a taxonomic grouping, but rather a description of the way the plant grows – much like \"tree\" or \"shrub\". Lianas may be found in many different plant families. One way of distinguishing lianas from trees and shrubs is based on the stiffness, specifically, the Young's modulus of various parts of the stem. Trees and shrubs have young twigs and smaller branches which are quite flexible and older growth such as trunks and large branches which are stiffer. A liana often has stiff young growths and older, more flexible growth at the base of the stem.\n\nDescribed genera containing liana species include:\n\nGnetophyta\n\nAcanthaceae\nAncistrocladaceae\nAnnonaceae\nApocynaceae\nArecaceae (Palmae)\nAraceae\nAristolochiaceae\nBignoniaceae\nCapparaceae\nConnaraceae\nDilleniaceae\nDioscoreaceae\n\nFabaceae: perhaps not surprisingly, leguminous vines are well represented:<br>\n- Caesalpinioideae\n- Faboideae (Papilionoideae)\n- Mimosoideae\n\nFlagellariaceae (order Poales)\nLoganiaceae\nNepenthaceae\nOleaceae\nPolygalaceae\nSapindaceae\nRhamnaceae\nRubiaceae\nRutaceae\nSchlegeliaceae\nSmilacaceae\nVitaceae\n\nLianas compete intensely with trees, greatly reducing tree growth and tree reproduction, greatly increasing tree mortality, preventing tree seedlings from establishing, and altering the course of regeneration in forests. Lianas also provide access routes in the forest canopy for many arboreal animals, including ants and many other invertebrates, lizards, rodents, sloths, monkeys, and lemurs. For example, in the Eastern tropical forests of Madagascar, many lemurs achieve higher mobility from the web of lianas draped amongst the vertical tree species. Many lemurs prefer trees with lianas for their roost sites. Lianas also provide support for trees when strong winds blow. However, they may be destructive in that when one tree falls, the connections made by the lianas may cause many other trees to fall.\n\nAs noted by Charles Darwin, because lianas are supported by other plants, they may conserve resources that other plants must allocate to the development of structure and use them instead for growth and reproduction. In general, lianas are detrimental to the trees that support them: growth rates are lower for trees with lianas, they directly damage hosts by mechanical abrasion and strangulation, render hosts more susceptible to ice and wind damage, and increase the probability that the host tree falls. Lianas also make the canopy of trees more accessible to animals which eat leaves. Because of these negative effects, trees which remain free of lianas are at an advantage; some species have evolved characteristics which help them avoid or shed lianas.\n\n", "id": "551448", "title": "Liana"}
{"url": "https://en.wikipedia.org/wiki?curid=4695949", "text": "Soil seed bank\n\nThe soil seed bank is the natural storage of seeds, often dormant, within the soil of most ecosystems. The study of soil seed banks started in 1859 when Charles Darwin observed the emergence of seedlings using soil samples from the bottom of a lake. The first scientific paper on the subject was published in 1882 and reported on the occurrence of seeds at different soil depths. Weed seed banks have been studied intensely in agricultural science because of their important economic impacts; other fields interested in soil seed banks include forest regeneration and restoration ecology.\n\nMany taxa have been classified according to the longevity of their seeds in the soil seed bank. Seeds of \"transient\" species remain viable in the soil seed bank only to the next opportunity to germinate, while seeds of \"persistent\" species can survive longer than the next opportunity—often much longer than one year. Species with seeds that remain viable in the soil longer than five years form the \"long-term\" persistent seed bank, while species whose seeds generally germinate or die within one to five years are called \"short-term\" persistent. A typical long-term persistent species is \"Chenopodium album\" (Lambsquarters); its seeds commonly remain viable in the soil for up to 40 years and in rare situations perhaps as long as 1,600 years. A species forming no soil seed bank at all (except the dry season between ripening and the first autumnal rains) is \"Agrostemma githago\" (Corncockle), which is a formerly widespread cereal weed.\n\nLongevity of seeds is very variable and depends on many factors; few species exceed 100 years. In typical soils the longevity of seeds can range from nearly zero (germinating immediately when reaching the soil or even before) to several hundred years. Some of the oldest still-viable seeds were those of Lotus (\"Nelumbo nucifera\") found buried in the soil of a pond; these seeds were estimated by carbon dating to be around 1,200 years old.\n\nOne of the longest-running soil seed viability trials was started in Michigan in 1879 by James Beal. The experiment involved the burying of 20 bottles holding 50 seeds from 21 species. Every five years, a bottle from every species was retrieved and germinated on a tray of sterilized soil which was kept in a growth chamber. Later, after responsibility for managing the experiment was delegated to caretakers, the period between retrievals became longer. In 1980, more than 100 years after the trial was started, seeds of only three species were observed to germinate: moth mullein (\"Verbascum blattaria\"), common mullein (\"Verbascum thapsus\") and common mallow (\"Malva neglecta\").\n\nSoil seed banks play an important role in the natural environment of many ecosystems. For example, the rapid re-vegetation of sites disturbed by wildfire, catastrophic weather, agricultural operations, and timber harvesting is largely due to the soil seed bank. Forest ecosystems and wetlands contain a number of specialized plant species forming persistent soil seed banks.\n\nBefore the advent of herbicides a good example of a persistent seed bank species, Papaver rhoeas sometimes was so abundant in agricultural fields in Europe that it could be mistaken for a crop.\n\nThe absence of a soil seed bank impedes the establishment of vegetation during primary succession, while presence of a well-stocked soil seed bank permits rapid development of species-rich ecosystems during secondary succession.\n\nThe mortality of seeds in the soil is one of the key factors for the persistence and density fluctuations of plant populations, especially for annual plants. Studies on the genetic structure of \"Androsace septentrionalis\" populations in the seed bank compared to those of established plants showed that diversity within populations is higher below ground than above ground.\n\nThere are indications that mutations are more important for species forming a persistent seed bank compared to those with only transient seeds. The increase of species richness in a plant community due to a species-rich and abundant soil seed bank is known as the \"storage effect\".\n\nSpecies of \"Striga\" (witchweed) are known to leave some of the highest seed densities in the soil compared to other plant genera; this is a major factor that aids their invasive potential. Each plant has the capability to produce between 90,000 and 450,000 seeds, although a majority of these seeds are not viable. It has been estimated that only two witchweeds would produce enough seeds required to refill a seed bank after seasonal losses.\n\nThe term soil diaspore bank can be used to include non-flowering plants such as ferns and bryophytes.\n\nIn addition to seeds, perennial plants have vegetative propagules to facilitate forming new plants, migration into new ground, or reestablishment after being top-killed. These propagules are collectively called the 'soil bud bank', and include dormant and adventitious buds on stolons, rhizomes, and bulbs.\n", "id": "4695949", "title": "Soil seed bank"}
{"url": "https://en.wikipedia.org/wiki?curid=3686698", "text": "Disjunct distribution\n\nIn biology, a taxon with a disjunct distribution is one that has two or more groups that are related but widely separated from each other geographically. The causes are varied and might demonstrate either the expansion or contraction of a species range.\n\nAlso called range fragmentation, disjunct distributions may be caused by changes in the environment, such as mountain building and continental drift or rising sea levels; it may also be due to an organism expanding its range into new areas, by such means as rafting, or other animals transporting an organism to a new location (plant seeds consumed by birds and animals, can be moved to new locations during bird or animals migrations, and those seeds can be deposited in new locations in fecal matter). Other conditions that can produce disjunct distributions include: flooding, or changes in wind, stream, and current flows, plus others such as anthropogenic introduction of alien introduced species either accidentally or deliberately (agriculture and horticulture).\n\nDisjunct distributions can occur when suitable habitat is fragmented, which produces fragmented populations, and when that fragmentation becomes so divergent that species movement between one suitable habitat to the next is disrupted, isolated population can be produced. Extinctions can cause disjunct distribution, especially in areas where only scattered areas are habitable by a species; for instance, island chains or specific elevations along a mountain range or areas along a coast or between bodies of water like streams, lakes and ponds.\n\nThere are many patterns of disjunct distributions at many scales: Irano-Turanian disjunction, Europe - East Asia, Europe-South Africa (e.g. genus \"Erica\"), Mediterranean-Hoggart disjunction (genus \"Olea\"), etc.\n\nThis kind of disjunct distribution of a species, such that it occurs in Iberia and in Ireland, without any intermediate localities, is usually called \"Lusitanian\" (named after the Roman Province Lusitania, corresponding to modern day Portugal).\n\nExamples of animal species with a Lusitanian distribution are: the Kerry slug \"Geomalacus maculosus\" and the Pyrenean glass snail \"Semilimax pyrenaicus\". Plant species with this kind of distribution include several heather species (\"Calluna\" spp.) and the strawberry tree (\"Arbutus unedo\").\n\nThe theory behind the name \"Lusitanian\" is now discredited; it posited that there was an ice-free land mass that served as a refugium off of the south-west of Ireland during the Quaternary (last) glaciation. In this refugium, relic fauna and flora from a previous ice-free period survived until the present warmer interstadial period. Although the theory is no longer accepted, the term Lusitanian is still used as a descriptive term for faunal elements such as the Kerry slug.\nRecently a better explanation of the occurrence of the Kerry slug and similar faunal elements in southwestern Ireland has been developed. This new theory is supported by two recent discoveries: the genetic similarity of much of Ireland’s fauna to that of northern Spain, and the genetic similarity of much of Ireland’s human population to that of northern Spain.\n\nMascheretti et al. (2003) examined the genotypes of Eurasian pygmy shrew, a small mammal, across its range in Europe. The Irish population showed close genetic affinity to a population from Andorra but not to that of Britain or other places in Europe. The genetic structure of the population further showed that the entire Irish population of the Eurasian pygmy shrew had originated from a single founder event. The authors concluded that it had been introduced in the early (Palaeolithic) or middle (Mesolithic) Stone Age, by boat, probably from south-west Europe. This coincides with work on human populations, which found a strong genetic similarity in make-up between populations in western Ireland and in northern Spain. This would be explained by a human migration from Spain to Ireland in the late Paleolithic or early Mesolithic.\n\nIt seems increasingly likely that much of Ireland’s Lusitanian fauna is in reality an artefact of this era of human expansion in the early part of the Postglacial era. In other words, it seems likely that these species were introduced accidentally with trade items or goods brought by boat from Iberia.\n\n", "id": "3686698", "title": "Disjunct distribution"}
{"url": "https://en.wikipedia.org/wiki?curid=29552335", "text": "Thanatocoenosis\n\nThanatocoensis (from Greek language \"thanatos\" - death and \"koinos\" - common) are all the embedded fossils at a single discovery site. This site may be referred to as a death assemblage. Such groupings are composed of fossils of organisms which may not have been associated during life, often originating from different habitats. Examples include marine fossils having been brought together by a water current or animal bones having been deposited by a predator.\n\n", "id": "29552335", "title": "Thanatocoenosis"}
{"url": "https://en.wikipedia.org/wiki?curid=1928089", "text": "Conservation status\n\nThe conservation status of a group of organisms (for instance, a species) indicates whether the group still exists and how likely the group is to become extinct in the near future. Many factors are taken into account when assessing conservation status: not simply the number of individuals remaining, but the overall increase or decrease in the population over time, breeding success rates, and known threats. Various systems of conservation status exist and are in use at international, multi-country, national and local levels as well as for consumer use.\n\nThe IUCN Red List of Threatened Species is the best known worldwide conservation status listing and ranking system. Species are classified by the IUCN Red List into nine groups set through criteria such as rate of decline, population size, area of geographic distribution, and degree of population and distribution fragmentation.\n\nAlso included are species that have gone extinct since 500 AD. When discussing the IUCN Red List, the official term \"threatened\" is a grouping of three categories: critically endangered, endangered, and vulnerable.\n\n\nThe Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES) aims to ensure that international trade in specimens of wild animals and plants does not threaten their survival. Many countries require CITES permits when importing plants and animals listed on CITES.\n\nIn the European Union (EU), the Birds and Habitats Directives are the legal instruments that evaluate the conservation status within the EU of species and habitats.\n\nNatureServe conservation status focuses on Latin America, United States, Canada, and the Caribbean. It has been developed by scientists from NatureServe, The Nature Conservancy, and the network of natural heritage programs and data centers. It is increasingly integrated with the IUCN Red List system. Its categories for species include: \"presumed extinct\" (GX), \"possibly extinct\" (GH), \"critically imperiled\" (G1), \"imperiled\" (G2), \"vulnerable\" (G3), \"apparently secure\" (G4), and \"secure\" (G5). The system also allows ambiguous or uncertain ranks including \"inexact numeric ranks\" (e.g. G2?), and \"range ranks\" (e.g. G2G3) for when the exact rank is uncertain. NatureServe adds a qualifier for \"captive or cultivated only\" (C), which has a similar meaning to the IUCN Red List \"extinct in the wild\" (EW) status.\n\nThe Red Data Book of the Russian Federation is used within the Russian Federation, and also accepted in parts of Africa.\n\nIn Australia, the \"Environment Protection and Biodiversity Conservation Act 1999\" (EPBC Act) describes lists of threatened species, ecological communities and threatening processes. The categories resemble those of the 1994 IUCN Red List Categories & Criteria (version 2.3). Prior to the EPBC Act, a simpler classification system was used by the \"Endangered Species Protection Act 1992\". Some state and territory governments also have their own systems for conservation status.\n\nIn Belgium, the Flemish Research Institute for Nature and Forest publishes an online set of more than 150 nature indicators in Dutch.\n\nIn Canada, the Committee on the Status of Endangered Wildlife in Canada (COSEWIC) is a group of experts that assesses and designates which wild species are in some danger of disappearing from Canada. Under the Species at Risk Act (SARA), it is up to the federal government, which is politically accountable, to legally protect species assessed by COSEWIC.\n\nIn China, the State, provinces and some counties have determined their key protected wildlife species. There is the China red data book.\n\nIn Finland, a large number of species are protected under the Nature Conservation Act, and through the EU Habitats Directive and EU Birds Directive.\n\nIn Germany, the Federal Agency for Nature Conservation publishes \"red lists of endangered species\".\n\nIndia has the Wild Life Protection Act, 1972, Amended 2003 and the Biological Diversity Act, 2002.\n\nIn Japan, the Ministry of Environment publishes a Threatened Wildlife of Japan Red Data Book.\n\nIn the Netherlands, the Dutch Ministry of Agriculture, Nature and Food Quality publishes a list of threatened species, and conservation is enforced by the Nature Conservation Act 1998. Species are also protected through the Wild Birds and Habitats Directives.\n\nIn New Zealand, the Department of Conservation publishes the New Zealand Threat Classification System lists. Under this system threatened species or subspecies are assigned one of seven categories: Nationally Critical, Nationally Endangered, Nationally Vulnerable, Serious Decline, Gradual Decline, Sparse, or Range Restricted. While the classification looks only at a national level, many species are unique to New Zealand, and species which are secure overseas are noted as such.\n\nIn Russia, the Red Book of Russian Federation came out in 2001, it contains categories defining preservation status for different species. In it there are 8 taxa of amphibians, 21 taxa of reptiles, 128 taxa of birds, and 74 taxa of mammals, in total 231. There are also more than 30 regional red books, for example the red book of the Altaic region which came out in 1994.\n\nIn South Africa, The South African National Biodiversity Institute, established under the National Environmental Management: Biodiversity Act, 2004, is responsible for drawing up lists of affected species, and monitoring compliance with CITES decisions. It is envisaged that previously diverse Red lists would be more easily kept current, both technically and financially.\n\nIn Thailand, the Wild Animal Reservation and Protection Act of BE 2535 defines fifteen reserved animal species and two classes of protected species, of which hunting, breeding, possession, and trade are prohibited or restricted by law. The National Park, Wildlife and Plant Conservation Department of the Ministry of Natural Resources and Environment is responsible for the regulation of these activities.\n\nIn Ukraine, the Ministry of Environment Protection maintains list of endangered species (divided into seven categories from \"0\" - extinct to \"VI\" - rehabilitated) and publishes it in the Red Book of Ukraine.\n\nIn the United States of America, the Endangered Species Act created the Endangered Species List.\n\nSome consumer guides for seafood, such as Seafood Watch, divide fish and other sea creatures into three categories, analogous to conservation status categories:\n\nThe categories do not simply reflect the imperilment of individual species, but also consider the environmental impacts of how and where they are fished, such as through bycatch or ocean bottom trawlers. Often groups of species are assessed rather than individual species (e.g. squid, prawns).\n\nThe Marine Conservation Society has five levels of ratings for seafood species, as displayed on their \"FishOnline\" website.\n\n\n", "id": "1928089", "title": "Conservation status"}
{"url": "https://en.wikipedia.org/wiki?curid=13377974", "text": "Ecological trap\n\nEcological traps are scenarios in which rapid environmental change leads organisms to prefer to settle in poor-quality habitats.\nThe concept stems from the idea that organisms that are actively selecting habitat must rely on environmental cues to help them identify high-quality habitat. If either the habitat quality or the cue changes so that one does not reliably indicate the other, organisms may be lured into poor-quality habitat.\n\nEcological traps are thought to occur when the attractiveness of a habitat increases disproportionately in relation to its value for survival and reproduction. The result is preference of falsely attractive habitat and a general avoidance of high-quality but less-attractive habitats. For example, Indigo buntings typically nest in shrubby habitat or broken forest transitions between closed canopy forest and open field. Human activity can create 'sharper', more abrupt forest edges and buntings prefer to nest along these edges. However, these artificial sharp forest edges also concentrate the movement of predators which predate their nests. In this way, Buntings prefer to nest in highly altered habitats where their nest success is lowest.\n\nWhile the demographic consequences of this type of maladaptive habitat selection behavior have been explored in the context of the sources and sinks, ecological traps are an inherently behavioral phenomenon of individuals. Despite being a behavioural mechanism, ecological traps can have far-reaching population consequences for species with large dispersal capabilities, such as the grizzly bear (Ursus arctos) The ecological trap concept was introduced in 1972 by Dwernychuk and Boag and the many studies that followed suggested that this trap phenomenon may be widespread because of anthropogenic habitat change.\n\nAs a corollary, novel environments may represent fitness opportunities that are unrecognized by native species if high-quality habitats lack the appropriate cues to encourage settlement; these are known as perceptual traps. Theoretical and empirical studies have shown that errors made in judging habitat quality can lead to population declines or extinction. Such mismatches are not limited to habitat selection, but may occur in any behavioral context (e.g. predator avoidance, mate selection, navigation, foraging site selection, etc.). Ecological traps are thus a subset of the broader phenomena of evolutionary traps.\n\nAs ecological trap theory developed, researchers have recognized that traps may operate on a variety of spatial and temporal scales which might also hinder their detection. For example, because a bird must select habitat on several scales (a habitat patch, an individual territory within that patch, as well as a nest site within the territory), traps may operate on any one of these scales. Similarly, traps may operate on a temporal scale so that an altered environment may appear to cause a trap in one stage of an organism’s life, yet have positive effects on later life stages. As a result, there has been a great deal of uncertainty as to how common traps may be, despite widespread acceptance as a theoretical possibility. However, given the accelerated rate of ecological change driven by human land-use change, global warming, exotic species invasions, and changes in ecological communities resulting from species loss, ecological traps may be an increasing and highly underappreciated threat to biodiversity.\n\nA 2006 review of the literature on ecological traps provides guidelines for demonstrating the existence of an ecological trap. A study must show a preference for one habitat over another (or equal preference) and that individuals selecting the preferred habitat (or equally preferred habitat) have lower fitness (i.e., experience lower survival or reproductive success). Since the publication of that paper which found only a few well-documented examples of ecological traps, interest in ecological and evolutionary traps has grown very rapidly and new empirical examples are being published at an accelerating rate. There are now roughly 30 examples of ecological traps affecting a broad diversity of taxa including birds, mammals, arthropods, fish and reptiles.\n\nBecause ecological and evolutionary traps are still very poorly understood phenomena, many questions about their proximate and ultimate causes as well as their ecological consequences remain unanswered. Are traps simply an inevitable consequence of the inability of evolution to anticipate novelty or react quickly to rapid environmental change? How common are traps? Do ecological traps necessarily lead to population declines or extinctions or is it possible that they may persist indefinitely? Under what ecological and evolutionary conditions should this occur? Are organisms with certain characteristics predisposed to being \"trapped\"? Is rapid environmental change necessary to trigger traps? Can global warming, pollution or exotic invasive species create traps? Embracing genetic and phylogenetic approaches may provide more robust answers to the above questions as well as providing deeper insight into the proximate and ultimate basis for maladaptation in general. Because ecological and evolutionary traps are predicted to add in concert with other sources of population decline, traps are an important research priority for conservation scientists. Given the rapid current rate of global environmental change, traps may be far more common than it is realized and it will be important to examine the proximate and ultimate causes of traps if management is to prevent or eliminate traps in the future.\n\nPolarized light pollution is perhaps the most compelling and well-documented cue triggering ecological traps. Orientation to polarized sources of light is the most important mechanism that guides at least 300 species of dragonflies, mayflies, caddisflies, tabanid flies, diving beetles, water bugs, and other aquatic insects in their search for the water bodies they require for suitable feeding/breeding habitat and oviposition sites (Schwind 1991; Horváth and Kriska 2008). Because of their strong linear polarization signature, artificial polarizing surfaces (e.g., asphalt, gravestones, cars, plastic sheeting, oil pools, windows) are commonly mistaken for bodies of water (Horváth and Zeil 1996; Kriska et al. 1998, 2006a, 2007, 2008; Horváth et al. 2007, 2008). Light reflected by these surfaces is often more highly polarized than that of light reflected by water, and artificial polarizers can be even more attractive to polarotactic aquatic insects than a water body (Horváth and Zeil 1996; Horváth et al. 1998; Kriska et al. 1998) and appear as exaggerated water surfaces acting as supernormal optical stimuli. Consequently, dragonflies, mayflies, caddisflies, and other water-seeking species actually prefer to mate, settle, swarm, and oviposit upon these surfaces than available water bodies.\n\n\n\n", "id": "13377974", "title": "Ecological trap"}
{"url": "https://en.wikipedia.org/wiki?curid=30139689", "text": "Biological pollution\n\nBiological pollution (impacts or biopollution) is a term that describes the impacts of humanity's actions on the quality of aquatic and terrestrial environment.\nBiopollution may cause adverse effects at several levels of biological organization: \nBiopollution may also cause decline in naturalness of nature conservation areas, adverse economic consequences and impacts on human health. The notion of \"biological pollution\" and \"biological pollutants\" described by Elliott (2003) is generally accepted in invasion biology; it was used to develop the concept of biopollution level assessment (Olenin et al., 2007) and criteria for a Good Ecological Status descriptor in the European Marine Strategy Framework Directive (Olenin et al., 2010)\nThe magnitude of the bioinvasion impact or biopollution level (Olenin et al., 2007) may be quantified using a free online service BINPAS.\n\n\"Biopollution Level (BPL)\" is a quantitative measure of the magnitude of the biological invasion impact, ranging from \"no impact\" (BPL=0) through \"weak\" (BPL=1), \"moderate\" (BPL=2), \"strong\" (BPL=3) and \"massive\" (BPL=4) impact.\nInitially the method of calculation involves assessing the abundance and distribution range of a non-indigenous species (NIS) for a specific area (this can be, for example, an entire regional sea, bay, inlet, lagoon, pond, lake, marina, a sand bank, an aquaculture site etc.). Abundance of a NIS may be ranked as \"low\", \"moderate\" or \"high\"; and the distribution may be scored as \"one locality\" (when a NIS was found only at one locality within the assessment area), \"several localities\", \"many localities\" or \"all localities\" (found at all localities). Combination of the abundance and distribution scores gives five classes of the abundance and distribution range. Once obtained this value aids in calculating an impact on 1) native communities, 2) habitats and, 3) ecosystem functioning. The calculation is based on ecological concepts, e.g. \"key species\", \"type specific communities\", \"habitat alteration, fragmentation and loss\", \"functional groups\", \"food web shift\", etc. Calculations are for a stated time period to enable assessment of temporal changes.\nThe method can be used for a single species or for several species for a specific (assessment) area. The method was designed for species in aquatic ecosystems (Olenin et al., 2007) but is currently being tested for terrestrial environments and there is a free on-line service BINPAS.\nThe biopollution level enables quantification of an impact in a robust manner in a standard and repeatable way. It makes it possible for comparison between different regions and taxonomic groups at different time intervals. The most impacting biota can be readily distinguished for a given region. It does not evaluate whether an impact effect is either good or bad, it states the change in an ecosystem due to an alien species invasion and measures the magnitude of this change. However, the method requires adequate information in order to obtain the magnitude of the impact, assessed at three levels of confidence (low, medium and high) according to the quality of the data available.\nThe method is simple to undertake and provide a means of quantifying impacts within any world region. Some assessments have been published (Olenina et al., 2010).\n\nThis is a free online system that calculates the magnitude of the biological invasion impact or biopollution level (Narščius et al., 2012).\nBINPAS translates the existing data on miscellaneous invasive alien species impacts on population, community, habitat or ecosystem into uniform biopollution measurement units. The service is free of charge and available at for anyone interested in biological invasions. Experts willing to perform the assessment for their studied regions are welcome to register and compile the information as contributors.\n\n", "id": "30139689", "title": "Biological pollution"}
{"url": "https://en.wikipedia.org/wiki?curid=10389193", "text": "Temperature-sensitive mutant\n\nTemperature sensitive mutants are variants of genes that allow normal function of the organism at low temperatures, but altered function at higher temperatures.\n\nMost temperature sensitive mutations affect proteins, and cause loss of protein function at the non-permissive temperature. The permissive temperature is one at which the protein typically can fold properly, or remain properly folded. At higher temperatures, the protein is unstable and ceases to function properly. These mutations are usually recessive in diploid organisms.\n\nThe permissive temperature is the temperature at which a temperature sensitive mutant gene product takes on a normal, functional phenotype.\nWhen a temperature sensitive mutant is grown in a permissive condition, the mutated gene product behaves normally (meaning that the phenotype isn't observed), even if there is a mutant allele present. This results in the survival of the cell or organism, as if it were a wild type strain. In contrast, the nonpermissive temperature or restrictive temperature is the temperature at which the mutant phenotype is observed.\n\nTemperature sensitive mutants are useful in biological research. They allow to study essential processes required for the survival of the cell or organism. Mutations to essential genes are generally lethal and hence temperature sensitive mutants enable researchers to induce the phenotype at the restrictive temperatures and study the effects. The temperature sensitive phenotype could be expressed during a specific developmental stage to study the effects. \n\nTemperature sensitive mutants were used by Randy Schekman's group to isolate and identify mutants having impaired secretory pathway. In yeast, secretory vesicles deliver raw materials for the growth of the new bud. Any mutations in the pathway will render the cell dead. So there is a need to selectively induce the phenotype at restrictive temperatures. By using temperature sensitive mutants, Schekman identified 23 genes required for the secretory pathway . \n", "id": "10389193", "title": "Temperature-sensitive mutant"}
{"url": "https://en.wikipedia.org/wiki?curid=4584079", "text": "Biomarker\n\nA biomarker, or biological marker, generally refers to a measurable indicator of some biological state or condition. The term is also occasionally used to refer to a substance whose detection indicates the presence of a living organism.\n\nBiomarkers are often measured and evaluated to examine normal biological processes, pathogenic processes, or pharmacologic responses to a therapeutic intervention. Biomarkers are used in many scientific fields.\n\nThe widespread use of the term \"biomarker\" dates back to as early as 1980. The term \"biological marker\" was introduced in 1950s. In 1998, the National Institutes of Health Biomarkers Definitions Working Group defined a biomarker as \"a characteristic that is objectively measured and evaluated as an indicator of normal biological processes, pathogenic processes, or pharmacologic responses to a therapeutic intervention.\"\n\nIn medicine, a biomarker can be a traceable substance that is introduced into an organism as a means to examine organ function or other aspects of health. For example, rubidium chloride is used as a radioactive isotope to evaluate perfusion of heart muscle.\n\nIt can also be a substance whose detection indicates a particular disease state, for example, the presence of an antibody may indicate an infection. More specifically, a biomarker indicates a change in expression or state of a protein that correlates with the risk or progression of a disease, or with the susceptibility of the disease to a given treatment.\n\nOther biomarkers can be based on measures of the electrical activity of the brain (using Electroencephalography (so-called Quantitative electroencephalography (qEEG)) or Magnetoencephalography), or volumetric measures of certain brain regions (using Magnetic resonance imaging) or saliva testing of natural metabolites, such as saliva nitrite, a surrogate marker for nitric oxide. One example of a commonly used biomarker in medicine is prostate-specific antigen (PSA). This marker can be measured as a proxy of prostate size with rapid changes potentially indicating cancer. The most extreme case would be to detect mutant proteins as cancer specific biomarkers through Selected Reaction Monitoring (SRM), since mutant proteins can only come from an existing tumor, thus providing ultimately the best specificity for medical purposes.\n\nBiomarkers used for personalized medicine are typically categorized as either prognostic or predictive. An example is KRAS, an oncogene that encodes a GTPase involved in several signal transduction pathways. Prognostic biomarkers indicate the likelihood of patient outcome regardless of a specific treatment. Predictive biomarkers are used to help optimize ideal treatments, and indicates the likelihood of benefiting from a specific therapy. Biomarkers for precision oncology are typically utilized in the molecular diagnostics of chronic myeloid leukemia, colon, breast, and lung cancer, and in melanoma.\n\nProof of concept\n\nPreviously used to identify the specific characteristics of the biomarker, this step is essential for doing an \"in situ\" validation of these benefits. A large number of candidates must be tested to select the most relevant ones.\n\nExperimental validation\n\nThis step allows the development of the most adapted protocol for routine use of the biomarker. Simultaneously, it is possible to confirm the relevance of the protocol with various methods (histology, PCR, ELISA, ...) and to define strata based on the results.\n\nAnalytical performances validation\n\nOne of the most important steps, it serves to identify specific characteristics of the candidate biomarker before developing a routine test. Several parameters are considered including:\n\n\nProtocol standardization\n\nThis optimizes the validated protocol for routine use, including analysis of the critical points by scanning the entire procedure to identify and control the potential risks.\n\nIn cell biology, a biomarker is a molecule that allows the detection and isolation of a particular cell type (for example, the protein Oct-4 is used as a biomarker to identify embryonic stem cells).\n\nIn genetics, a biomarker (identified as genetic marker) is a DNA sequence that causes disease or is associated with susceptibility to disease. They can be used to create genetic maps of whatever organism is being studied.\n\nA biomarker can be any kind of molecule indicating the existence, past or present, of living organisms. In the fields of geology and astrobiology, biomarkers, versus geomarkers, are also known as biosignatures. The term biomarker is also used to describe biological involvement in the generation of petroleum.\n\nBiomarkers are used to indicate an exposure to or the effect of xenobiotics which are present in the environment and in organisms. The biomarker may be an external substance itself (e.g. asbestos particles or NNK from tobacco), or a variant of the external substance processed by the body (a metabolite) that usually can be quantified.\n\n", "id": "4584079", "title": "Biomarker"}
{"url": "https://en.wikipedia.org/wiki?curid=1072857", "text": "Biosignature\n\nA biosignature (sometimes called chemical fossil or molecular fossil) is any substance – such as an element, isotope, molecule, or phenomenon – that provides scientific evidence of past or present life. Measurable attributes of life include its complex physical and chemical structures and also its utilization of free energy and the production of biomass and wastes. Due to its unique characteristics, a biosignature can be interpreted as having been produced by living organisms; however, it is important that they not be considered definitive because there is no way of knowing in advance which ones are universal to life and which ones are unique to the peculiar circumstances of life on Earth. Nonetheless, life forms are known to shed unique chemicals, including DNA, into the environment as evidence of their presence in a particular location.\n\nThe ancient record on Earth provides an opportunity to see what geochemical signatures are produced by microbial life and how these signatures are preserved over geologic time. Some related disciplines such as geochemistry, geobiology, and geomicrobiology often use biosignatures to determine if living organisms are or were present in a sample. These possible biosignatures include: (a) microfossils and stromatolites; (b) molecular structures (biomarkers) and isotopic compositions of carbon, nitrogen and hydrogen in organic matter; (c) multiple sulfur and oxygen isotope ratios of minerals; and (d) abundance relationships and isotopic compositions of redox sensitive metals (e.g., Fe, Mo, Cr, and rare earth elements).\n\nFor example, the particular fatty acids measured in a sample can indicate which types of bacteria and archaea live in that environment. Another example are the long-chain fatty alcohols with more than 23 atoms that are produced by planktonic bacteria. When used in this sense, geochemists often prefer the term biomarker. Another example is the presence of straight-chain lipids in the form of alkanes, alcohols an fatty acids with 20-36 carbon atoms in soils or sediments. Peat deposits are an indication of originating from the epicuticular wax of higher plants.\n\nLife processes may produce a range of biosignatures such as nucleic acids, lipids, proteins, amino acids, kerogen-like material and various morphological features that are detectable in rocks and sediments.\nMicrobes often interact with geochemical processes, leaving features in the rock record indicative of biosignatures. For example, bacterial micrometer-sized pores in carbonate rocks resemble inclusions under transmitted light, but have distinct size, shapes and patterns (swirling or dendritic) and are distributed differently from common fluid inclusions. A potential biosignature is a phenomenon that \"may\" have been produced by life, but for which alternate abiotic origins may also be possible.\n\nAstrobiological exploration is founded upon the premise that biosignatures encountered in space will be recognizable as extraterrestrial life. The usefulness of a biosignature is determined, not only by the probability of life creating it, but also by the improbability of nonbiological (abiotic) processes producing it. Concluding that evidence of an extraterrestrial life form (past or present) has been discovered, requires proving that a possible biosignature was produced by the activities or remains of life. As with most scientific discoveries, discovery of a biosignature will require of evidence building up until no other explanation exists. \n\nPossible examples of a biosignature might be complex organic molecules and/or structures whose formation is virtually unachievable in the absence of life. For example, cellular and extracellular morphologies, biomolecules in rocks, bio-organic molecular structures, chirality, biogenic minerals, biogenic stable isotope patterns in minerals and organic compounds, atmospheric gases, and remotely detectable features on planetary surfaces, such as photosynthetic pigments, etc.\n\nIn general, biosignatures and habitable environment signatures can be grouped into ten broad categories: \n\nNo single compound will prove life once existed. Rather, it will be distinctive patterns present in any organic compounds showing a process of selection. For example, membrane lipids left behind by degraded cells will be concentrated, have a limited size range, and comprise an even number of carbons. Similarly, life only uses left-handed amino acids. Biosignatures need not be chemical, however, and can also be suggested by a distinctive magnetic biosignature.\n\nOn Mars, surface oxidants and UV radiation will have altered or destroyed organic molecules at or near the surface. One issue that may add ambiguity in such a search is the fact that, throughout Martian history, abiogenic organic-rich chondritic meteorites have undoubtedly rained upon the Martian surface. At the same time, strong oxidants in Martian soil along with exposure to ionizing radiation might alter or destroy molecular signatures from meteorites or organisms. An alternative approach would be to seek concentrations of buried crystalline minerals, such as clays and evaporites, which may protect organic matter from the destructive effects of ionizing radiation and strong oxidants. The search for Martian biosignatures has become\nmore promising due to the discovery that surface and near-surface aqueous environments existed on Mars at the same time when biological organic matter was being preserved in ancient aqueous sediments on Earth.\n\nAnother possible biosignature might be morphology since the shape and size of certain objects may potentially indicate the presence of past or present life. For example, microscopic magnetite crystals in the Martian meteorite ALH84001 were the longest-debated of several potential biosignatures in that specimen because it was believed until recently that only bacteria could create crystals of their specific shape. For example, the possible biomineral studied in the Martian ALH84001 meteorite includes putative microbial fossils, tiny rock-like structures whose shape was a potential biosignature because it resembled known bacteria. Most scientists ultimately concluded that these were far too small to be fossilized cells. A consensus that has emerged from these discussions, and is now seen as a critical requirement, is the demand for further lines of evidence in addition to any morphological data that supports such extraordinary claims. Currently, the scientific consensus is that \"morphology alone cannot be used unambiguously as a tool for primitive life detection.\" Interpretation of morphology is notoriously subjective, and its use alone has led to numerous errors of interpretation. \n\nThe atmospheric properties of exoplanets are of particular importance, as atmospheres provide the most likely observables for the near future, including habitability indicators and biosignatures. Over billions of years, the processes of life on a planet would result in a mixture of chemicals unlike anything that could form in an ordinary chemical equilibrium. For example, large amounts of oxygen and small amounts of methane are generated by life on Earth.\n\nAlso, an exoplanet's color —or reflectance spectrum— might give away the presence of vast colonies of life forms at its surface.\n\nThe presence of methane in the atmosphere of Mars indicates that there must be an active source on the planet, as it is an unstable gas. Furthermore, current photochemical models cannot explain the presence of methane in the atmosphere of Mars and its reported rapid variations in space and time. Neither its fast appearance nor disappearance can be explained yet. To rule out a biogenic origin for the methane, a future probe or lander hosting a mass spectrometer will be needed, as the isotopic proportions of carbon-12 to carbon-14 in methane could distinguish between a biogenic and non-biogenic origin, similarly to the use of the δ13C standard for recognizing biogenic methane on Earth. In June, 2012, scientists reported that measuring the ratio of hydrogen and methane levels on Mars may help determine the likelihood of life on Mars. According to the scientists, \"...low H/CH ratios (less than approximately 40) indicate that life is likely present and active.\" The planned ExoMars Trace Gas Orbiter, launched in March 2016 to Mars, will study atmospheric trace gases and will attempt to characterize potential biochemical and geochemical processes at work.\n\nOther scientists have recently reported methods of detecting hydrogen and methane in extraterrestrial atmospheres. Habitability indicators and biosignatures must be interpreted within a planetary and environmental context. For example, the presence of oxygen and methane together could indicate the kind of extreme thermochemical disequilibrium generated by life. Two of the top 14,000 proposed atmospheric biosignatures are dimethyl sulfide and chloromethane ().\n\nScientific observations include the possible identification of biosignatures through indirect observation. For example, electromagnetic information through infrared radiation telescopes, radio-telescopes, space telescopes, etc. From this discipline, the hypothetical electromagnetic radio signatures that SETI scans for would be a biosignature, since a message from intelligent aliens would certainly demonstrate the existence of extraterrestrial life.\n\nThe \"Viking\" missions to Mars in the 1970s conducted the first experiments which were explicitly designed to look for biosignatures on another planet. Each of the two \"Viking\" landers carried three life-detection experiments which looked for signs of metabolism; however, the results were declared inconclusive.\n\n\nThe \"Curiosity\" rover from the Mars Science Laboratory mission, with its \"Curiosity\" rover is currently assessing the potential past and present habitability of the Martian environment and is attempting to detect biosignatures on the surface of Mars. Considering the MSL instrument payload package, the following classes of biosignatures are within the MSL detection window: organism morphologies (cells, body fossils, casts), biofabrics (including microbial mats), diagnostic organic molecules, isotopic signatures, evidence of biomineralization and bioalteration, spatial patterns in chemistry, and biogenic gases. The \"Curiosity\" rover targets outcrops to maximize the probability of detecting 'fossilized' organic matter preserved in sedimentary deposits.\n\nThe 2016 ExoMars Trace Gas Orbiter (TGO) is a Mars telecommunications orbiter and atmospheric gas analyzer mission. It delivered the \"Schiaparelli\" EDM lander and then began to settle into its science orbit to map the sources of methane on Mars and other gases, and in doing so, will help select the landing site for the ExoMars rover to be launched in 2020. The primary objective of the ExoMars rover mission is the search for biosignatures on the surface and subsurface by using a drill able to collect samples down to a depth of , away from the destructive radiation that bathes the surface.\n\n\nThe Mars 2020 rover, planned to launch in 2020, is intended to investigate an astrobiologically relevant ancient environment on Mars, investigate its surface geological processes and history, including the assessment of its past habitability, the possibility of past life on Mars, and potential for preservation of biosignatures within accessible geological materials. In addition, it will cache the most interesting samples for possible future transport to Earth.\n\nThe planned Dragonfly lander/aircraft to launch in 2025, would seek evidence of biosignatures on the organic-rich surface and atmosphere of Titan, as well as study its possible prebiotic primordial soup.\n", "id": "1072857", "title": "Biosignature"}
{"url": "https://en.wikipedia.org/wiki?curid=1098086", "text": "Relict\n\nA relict is a surviving remnant of a natural phenomenon.\n\n\nOther uses:\n\nIn various places around the world, minority ethnic groups represent lineages of ancient human migrations in places now occupied by more populous ethnic groups, whose ancestors arrived later. For example, the first human groups to inhabit the Caribbean islands were hunter-gatherer tribes from South and Central America. Genetic testing of natives of Cuba show that, in late pre-Columbian times, the island was home to agriculturalists of Taino ethnicity. In addition, a relict population of the original hunter-gatherers remained in western Cuba as the Ciboney people.\n\n", "id": "1098086", "title": "Relict"}
{"url": "https://en.wikipedia.org/wiki?curid=1207755", "text": "Monogastric\n\nA monogastric organism has a simple single-chambered stomach, compared with a ruminant organism, like a cow, goat, or sheep, which has a four-chambered complex stomach. Examples of monogastric animals include omnivores such as humans, rats, dogs and pigs, carnivores such as cats, and herbivores such as horses and rabbits. Herbivores with monogastric digestion can digest cellulose in their diets by way of symbiotic gut bacteria. However, their ability to extract energy from cellulose digestion is less efficient than in ruminants. \n\nHerbivores digest cellulose by microbial fermentation. Monogastric herbivores which can digest cellulose nearly as well as ruminants are called hindgut fermenters, while ruminants are called foregut fermenters. These are subdivided into two groups based on the relative size of various digestive organs in relationship to the rest of the system: colonic fermenters tend to be larger species such as horses and rhinos, and cecal fermenters are smaller animals such as rabbits and rodents. Great apes (other than humans) derive significant amounts of phytanic acid from the hindgut fermentation of plant materials.\n\nMonogastrics cannot digest the fiber molecule cellulose as efficiently as ruminants, though the ability to digest cellulose varies amongst species.\n\nA monogastric digestive system works as soon as the food enters the mouth. Saliva moistens the food and begins the digestive process. (Note that horses have no (or negligible amounts of) amylase in their saliva. After being swallowed, the food passes from the esophagus into the stomach, where stomach acid and enzymes help to break down the food. Bile salts are stored in the gall bladder (note that horses do not have a gall bladder and gall is directly secreted into the small intestine) and secreted once the contents of the stomach have reached the small intestines where most fats are broken down. The pancreas secretes enzymes and alkali to neutralize the stomach acid.\n", "id": "1207755", "title": "Monogastric"}
{"url": "https://en.wikipedia.org/wiki?curid=10013669", "text": "Function (biology)\n\nA biological function is the reason some object or process occurred in a system that evolved through natural selection. That reason is typically that it achieves some result, such as that chlorophyll helps to capture the energy of sunlight in photosynthesis. Hence, the organism that contains it is more likely to survive and reproduce, in other words the function increases the organism's fitness. A characteristic that assists in evolution is called an adaptation; other characteristics may be non-functional spandrels, though these in turn may later be co-opted by evolution to serve new functions.\n\nIn the philosophy of biology, talk of function inevitably suggests some kind of teleological purpose, even though natural selection operates without any goal for the future. All the same, biologists often use teleological language as a shorthand for function.\n\nIn physiology, a function is an activity or process carried out by a system in an organism, such as sensation or locomotion in an animal. This concept of function as opposed to form (respectively Aristotle's \"ergon\" and \"morphê\") was central in biological explanations in classical antiquity, and in more modern times formed part of the Cuvier–Geoffroy debate.\n\nA functional characteristic is known in evolutionary biology as an adaptation, and the research strategy for investigating whether a character is adaptive is known as adaptationism. Although an assumption that a character is functional may be fruitful as a research method, some characteristics of organisms are non-functional, and may simply be spandrels, side effects of functional systems.\n\nFrom the point of view of natural selection, biological functions exist to contribute to fitness, increasing the chance that an organism will survive to reproduce. For example, the function of chlorophyll in a plant is to capture the energy of sunlight for photosynthesis, which contributes to evolutionary success.\n\nFunction is not the same as purpose in the teleological sense. In the philosophy of biology, evolution is a blind process which has no 'goal' for the future. For example, a tree does not grow flowers for any purpose, but does so simply because it has evolved to do so. To say 'a tree grows flowers to attract pollinators' would be incorrect if the 'to' implies purpose. A function describes what something \"does\", not what its 'purpose' is. However, teleological language is often used by biologists as a shorthand way of describing function, even though its applicability is disputed.\n\nThe ethologist Niko Tinbergen named four questions, based on Aristotle's Four Causes, that a biologist could ask to help explain a behaviour, though they have been generalised to a wider scope. 1) Mechanism: What mechanisms cause the animal to behave as it does? 2) Ontogeny: What developmental mechanisms in the animal's embryology (and its youth, if it learns) created the structures that cause the behaviour? 3) Function/adaptation: What is the evolutionary function of the behaviour? 4) Evolution: What is the phylogeny of the behaviour, or in other words, when did it first appear in the evolutionary history of the animal? The questions are interdependent, so that, for example, adaptive function is constrained by embryonic development.\n\n", "id": "10013669", "title": "Function (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=11135347", "text": "Exopheromone\n\nExopheromone is a term coined by Terence McKenna, proposed in his book \"Food of the Gods\" for the controversial idea of chemical signals between members of different classes of living things, as opposed to among conspecifics. He suggested that certain chemicals produced in abundance in various hallucinogenic plants and fungi, such as dimethyltryptamine and psilocybin may act as pheromones produced by one kingdom (the vegetal) waiting for absorption by various others (for example, early primates or hominids). In this way a kind of ecological pheromonal system may be at work among biological kingdoms and ecosystems that have coevolved closely for long stretches of time. The term is not scientifically accepted.\n", "id": "11135347", "title": "Exopheromone"}
{"url": "https://en.wikipedia.org/wiki?curid=14949505", "text": "Entrainment (biomusicology)\n\nEntrainment in the biomusicological sense refers to the synchronization of organisms (only humans as a whole, with some particular instances of a particular animal) to an external perceived rhythm, such as human music and dance such as foot tapping.\n\nBeat induction is the process in which a regular isochronous pulse is activated while one listens to music (i.e. the beat to which one would tap one's foot). It was thought that the cognitive mechanism that allows us to infer a beat from a sound pattern, and to synchronize or dance to it, was uniquely human. No primate tested so far—with exception of the human species—can dance or collaboratively clap to the beat of the music. Humans know when to start, when to stop, when to speed up or to slow down, in synchronizing with their fellow dancers or musicians. Although primates do not appear to display beat induction, some parrots do. The most famous example, Snowball was shown to display genuine dance, including changing his movements to a change in tempo (Patel et al., 2009)\n\nBeat induction can be seen as a fundamental cognitive skill that allows for music (e.g., Patel, 2008; Honing, 2007; 2012). We can hear a pulse in a rhythmic pattern while it might not even be explicitly in there: The pulse is being induced (hence the name) while listening—like a perspective can be induced by looking at an arrangement of objects in a picture.\n\nNeuroscientist Ani Patel proposes beat induction—referring to it as \"beat-based rhythm processing\"—as a key area in music-language research, suggesting beat induction \"a fundamental aspect of music cognition that is not a byproduct of cognitive mechanisms that also serve other, more clearly adaptive, domains (e.g. auditory scene analysis or language)\" (Patel, 2008).\n\nJoseph Jordania recently suggested that the human ability to be entrained was developed by the forces of natural selection as an important part of achieving the specific altered state of consciousness, battle trance. Achieving this state, in which humans lose their individuality, do not feel fear and pain, are united in a shared collective identity, and act in the best interests of the group, was crucial for the physical survival of our ancestors against the big African predators, after hominids descended from the safer trees to the dangerous ground and became terrestrial.\n\n\n\n", "id": "14949505", "title": "Entrainment (biomusicology)"}
{"url": "https://en.wikipedia.org/wiki?curid=29102236", "text": "Biodilution\n\nBiodilution is the decrease in concentration of an element or pollutant with an increase in trophic level. This effect is primarily caused by the observed trend that an increase in algal biomass will reduce the overall concentration of a pollutant per cell, which ultimately contributes to a lower dietary input to grazers (and higher-level aquatic organisms).\n\nThe primary elements and pollutants of concern are heavy metals such as mercury, cadmium, and lead. These toxins have been shown to bioaccumulate up a food web. In some cases, metals, such as mercury, can biomagnify. This is a major concern since methylmercury, the most toxic mercury species, can be found in high concentrations in human-consumed fish and other aquatic organisms. Persistent organic pollutants, such as carcinogenic polycyclic aromatic hydrocarbons and alkylphenols, have also shown to biodilute in the marine environment.\n\nNumerous studies have linked lower mercury concentrations in zooplankton found in eutrophic (nutrient-rich and highly productive) as compared to oligotrophic (low nutrient) aquatic environments. Nutrient enrichment (mainly phosphorus and nitrogen) reduce the input of mercury, and other heavy metals, into aquatic food webs through this biodilution effect. Primary producers, such as phytoplankton, uptake these heavy metals and accumulate them into their cells. The higher the population of phytoplankton, the less concentrated these pollutants will be in their cells. Once consumed by primary consumers, such as zooplankton, these phytoplankton-bound pollutants are incorporated into the consumer’s cells. Higher phytoplankton biomass means a lower concentration of pollutants accumulated by the zooplankton, and so on up the food web. This effect causes an overall dilution of the original concentration up the food web. That is, the concentration of a pollutant will be lower in the zooplankton than the phytoplankton in a high bloom condition.\n\nAlthough most biodilution studies have been on freshwater environments, biodilution has been shown to occur in the marine environment as well. The Northwater Polynya, located in Baffin Bay, was found to have a negative correlation of cadmium, lead, and nickel with an increase in trophic level Cadmium and lead are both non-essential metals that will compete for calcium within an organism, which is detrimental for organism growth.\n\nMost studies measure bioaccumulation and biodilution using the δ15N isotope of nitrogen. The δ15N isotopic signature is enriched up the food web. A predator will have a higher δ15N as compared to its prey. This trend allows the tropic position of an organism to be derived. Coupled to the concentration of a specific pollutant, such as mercury, the concentration verses trophic position can be accessed.\n\nWhile most heavy metals bioaccumulate, under certain conditions, heavy metals and organic pollutants have the potential to biodilute, making a higher organism less exposed to the toxin.\n", "id": "29102236", "title": "Biodilution"}
{"url": "https://en.wikipedia.org/wiki?curid=407292", "text": "Biophotonics\n\nThe term biophotonics denotes a combination of biology and photonics, with photonics being the science and technology of generation, manipulation, and detection of photons, quantum units of light. Photonics is related to electronics and photons. Photons play a central role in information technologies such as fiber optics the way electrons do in electronics.\n\nBiophotonics can also be described as the \"development and application of optical techniques, particularly imaging, to the study of biological molecules, cells and tissue\". One of the main benefits of using optical techniques which make up biophotonics is that they preserve the integrity of the biological cells being examined.\n\nBiophotonics has therefore become the established general term for all techniques that deal with the interaction between biological items and photons. This refers to emission, detection, absorption, reflection, modification, and creation of radiation from biomolecular, cells, tissues, organisms and biomaterials. Areas of application are life science, medicine, agriculture, and environmental science.\nSimilar to the differentiation between \"electric\" and \"electronics\" a difference can be made between applications such as Therapy and surgery, which use light mainly to transfer energy, and applications such as diagnostics, which use light to excite matter and to transfer information back to the operator. In most cases the term biophotonics refers to the latter type of application.\n\nApplications\n\nBiophotonics is an interdisciplinary field involving the interaction between electromagnetic radiation and biological materials including: tissues, cells, sub-cellular structures, and molecules in living organisms.\n\nRecent biophotonics research has created new applications for clinical diagnostics and therapies involving fluids, cells and tissues. These advances are allowing scientists and physicians opportunities for superior, non-invasive diagnostics for vascular and blood flow, as well as tools for better examination of skin lesions.  In addition to new diagnostic tools, the advancements in biophotonics research has provided new photothermal, photodynamic, and tissue therapies.\n\n\"Dermatology\" By observing the numerous and complex interactions between light and biological materials, the field of biophotonics presents a unique set of diagnostic techniques that medical practitioners can utilize. Biophotonic imaging provides the field of dermatology with the only non-invasive technique available for diagnosing skin cancers. Traditional diagnostic procedures for skin cancers involve visual assessment and biopsy, but a new Laser Induced Fluorescence spectroscopy technique allow dermatologists to compare spectrographs of a patient's skin with spectrographs known to correspond with malignant tissue. This provides doctors with earlier diagnosis and treatment options.\n\n“Among optical techniques, an emerging imaging technology based on laser scanning, the optical coherence tomography or OCT imaging is considered to be a useful tool to differentiate healthy from malignant skin tissue.” The information is immediately accessible and eliminates the need for skin excision. This also eliminates the need for the skin samples to be processed in a lab which reduces labor costs and processing time.\n\nFurthermore, these optical imaging technologies can be used during traditional surgical procedures to determine the boundaries of lesions to ensure that the entirety of the diseased tissue is removed.  This is accomplished by exposing nanoparticles that have been dyed with a fluorescing substance to the acceptable light photons. Nanoparticles that are functionalized with fluorescent dyes and marker proteins will congregate in a chosen tissue type. When the particles are exposed to wavelengths of light that correspond to the fluorescent dye, the unhealthy tissue glows. This allows for the attending surgeon to quickly visually identify boundaries between healthy and unhealthy tissue, resulting in less time on the operating table and higher patient recovery.  “Using dielectrophoretic microarray devices, nanoparticles and DNA biomarkers were rapidly isolated and concentrated onto specific microscopic locations where they were easily detected by epifluorescent microscopy”\n\n\"Optical Tweezers\"\n\nOptical tweezers (or traps) are scientific tools employed to maneuver microscopic particles such as atoms, DNA, bacteria, viruses, and other types of nanoparticles.  It uses the lights momentum to exert small forces on a sample. This technique allows for the organizing and sorting of cells, the tracking of the movement of bacteria, and the changing of cell structure\n\n\"Laser Micro-scalpels\" are a combination of fluorescence  microscopy and a femtosecond laser “can penetrate up to 250 micrometers into tissue and target single cells in 3-D space.” The technology, which was patented by researchers at the University of Texas at Austin, means that surgeons can excise diseased or damaged cells without disturbing or damaging healthy surrounding cells in delicate surgeries involving areas such as the eyes and vocal chords.\n\n\"Photoacoustic microscopy (PAM)\" is an imaging technology that utilizes both laser technology and ultrasound technology.  This dual imaging modality is far superior at imaging deep tissue and vascular tissues than previous imaging technologies. The improvement in resolution provides higher quality images of deep tissues and vascular systems, allowing non-invasive differentiation of cancerous tissues vs healthy tissue by observing such things as “water content, oxygen saturation level, and hemoglobin concentration.” Researchers have also been able to use PAM to diagnose endometriosis in rats.\n\n\"Low Level Laser Therapy (LLLT)\", although somewhat controversial as to it efficacy, can be used  to treat wounds by repairing tissue and preventing tissue death.  However, more recent studies indicate that LLLT is more useful for  reducing inflammation and assuaging chronic joint pain.  In addition, it is believed that LLLT could possibly prove to be useful in the treatment of severe brain injury or trauma, stroke, and degenerative neurological diseases.\n\n\"Photodyanamic Therapy (PT)\" uses photosynthesizing chemicals and oxygen to induce a cellular reaction to light.  It can be used to kill cancer cells, treat acne, and reduce scarring. PT can also kill bacteria, viruses, and fungi. The technology provides treatment with little to no long-term side effects, is less invasive than surgery and can be repeated more often than radiation. Treatment is limited, however, to surfaces and organs that can be exposed to light, which eliminates deep tissue cancer treatments.\n\n\"Photothermal therapy\" most commonly uses nanoparticles made of a noble metal to convert light into heat. The nanoparticles are engineered to absorb light in the 700-1000nm range, where the human body is optically transparent. When the particles are hit by light they heat up, disrupting or destroying the surrounding cells via hyperthermia. Because the light used does not interact with tissue directly, photothermal therapy has few long term side effects and it can be used to treat cancers deep within the body.\nFluorescence Resonance Energy Transfer, also known as Foerster Resonance Energy Transfer (FRET in both cases) is the term given to the process where two excited \"fluorophores\" pass energy one to the other non-radiatively (i.e., without exchanging a photon). By carefully selecting the excitation of these flurophores and detecting the emission, FRET has become one of the most widely used techniques in the field of Biophotonics, giving scientists the chance to investigate sub-cellular environments. See the full article on FRET\n\nBiofluorescence describes the absorption of ultraviolet or visible light and the sub sequential emission of photons at a lower energy level (S_1 excited state relaxes to S_0 ground state) by intrinsically fluorescent proteins or by synthetic fluorescent molecules covalently attached to a biomarker of interest. Biomarkers are molecules indicative or disease or distress and are a typically monitored systemically in a living organism, or by using an \"ex vivo\" tissue sample for microscopy, or \"in vitro\": in the blood, urine, sweat, saliva, interstitial fluid, aqueous humor, or sputum. Stimulating light excites an electron, raising energy to an unstable level. This instability is unfavorable, so the energized electron is returned to a stable state almost as immediately as it becomes unstable.The time delay between excitation and re-emission that occurs when returning to the stable ground state causes the photon that is re-emitted to be a different color (i.e. it relaxes to a lower energy and thus the photon emitted is at a shorter wavelength, as governed by the Plank-Einstein relation<nowiki> E={\\frac {hc}{\\lambda }}) than the excitation light that was absorbed. This return to stability corresponds with the release of excess energy in the form of fluorescent light. This emission of light is only observable whilst the exciation light is still providing photons to the fluorescent molecule and is typically excited by blue or green light and emits purple, yellow, orange, green, cyan, or red. Biofluorescence is often confused with the following forms of biotic light: bioluminescence and biophosphorescence.</nowiki>\n\nBioluminescence differs from biofluorescence in that it is the natural production of light by chemical reactions within an organism, whereas biofluorescence and biophosphorescence are the absorption and reemission of light from the natural environment.\n\nBiophosphorescence is similar to biofluorescence in its requirement of light at specified wavelengths as a provider of excitation energy. The difference here lies in the relative stability of the energized electron. Unlike with biofluorescence, here the electron retains stability in the forbidden triplet state (unpaired spins), with a longer delay in emitting light resulting in the effect that it continues to “glow-in-the-dark” even long after the stimulating light source has been removed.\n\nThe predominantly used light sources are beam lights. LEDs and superluminescent diodes also play an important role. Typical wavelengths used in biophotonics are between 600 nm (Visible) and 3000 nm (near IR).\n\nLasers play an increasingly important role in biophotonics. Their unique intrinsic properties like precise wavelength selection, widest wavelength coverage, highest focusability and thus best spectral resolution, strong power densities and broad spectrum of excitation periods make them the most universal light tool for a wide spectrum of applications. As a consequence a variety of different laser technologies from a broad number of suppliers can be found in the market today.\n\nMajor gas lasers used for biophotonics applications, and their most important wavelengths, are:\n\n- Argon Ion laser: 457.8 nm, 476.5 nm, 488.0 nm, 496.5 nm, 501.7 nm, 514.5 nm (multi-line operation possible)\n\n- Krypton Ion laser: 350.7 nm, 356.4 nm, 476.2 nm, 482.5 nm, 520.6 nm, 530.9 nm, 568.2 nm, 647.1 nm, 676.4 nm, 752.5 nm, 799.3 nm\n\n- Helium–neon laser: 632.8 nm (543.5 nm, 594.1 nm, 611.9 nm)\n\n- HeCd lasers: 325 nm, 442 nm\n\nOther commercial gas lasers like carbon dioxide (CO2), carbon monoxide, nitrogen, oxygen, xenon-ion, excimer or metal vapor lasers have no or only very minor importance in biophotonics.\nMajor advantage of gas lasers in biophotonics is their fixed wavelength, their perfect beam quality and their low linewidth/high coherence. Argon ion lasers can also operate in multi-line mode. Major disadvantage are high power consumption, generation of mechanical noise due to fan cooling and limited laser powers. Key suppliers are Coherent, CVI/Melles Griot, JDSU, Lasos, LTB and Newport/Spectra Physics.\n\nThe most commonly integrated laser diodes, which are used for diode lasers in biophotonics are based either on GaN or GaAs semiconductor material. GaN covers a wavelength spectrum from 375 to 488 nm (commercial products at 515 have been announced recently) whereas GaAs covers a wavelength spectrum starting from 635 nm.\n\nMost commonly used wavelengths from diode lasers in biophotonics are: 375, 405, 445, 473, 488, 515, 640, 643, 660, 675, 785 nm.\n\nLaser Diodes are available in 4 classes:\n\n- Single edge emitter/broad stripe/broad area\n\n- Surface emitter/VCSEL\n\n- Edge emitter/Ridge waveguide\n\n- Grating stabilized (FDB, DBR, ECDL)\n\nFor biophotonic applications, the most commonly used laser diodes are edge emitting/ridge waveguide diodes, which are single transverse mode and can be optimized to an almost perfect TEM00 beam quality. Due to the small size of the resonator, digital modulation can be very fast (up to 500 MHz). Coherence length is low (typically < 1 mm) and the typical linewidth is in the nm-range. Typical power levels are around 100 mW (depending on wavelength and supplier).\nKey suppliers are: Coherent, Melles Griot, Omicron, Toptica, JDSU, Newport, Oxxius, Power Technology.\nGrating stabilized diode lasers either have an lithographical incorporated grating (DFB, DBR) or an external grating (ECDL). As a result, the coherence length will raise into the range of several meters, whereas the linewidth will drop well below picometers (pm). Biophotonic applications, which make use of this characteristics are Raman spectroscopy (requires linewidth below cm-1) and spectroscopic gas sensing.\n\nSolid-state lasers are lasers based on solid-state gain media such as crystals or glasses doped with rare earth or transition metal ions, or semiconductor lasers. (Although semiconductor lasers are of course also solid-state devices, they are often not included in the term solid-state lasers.) Ion-doped solid-state lasers (also sometimes called doped insulator lasers) can be made in the form of bulk lasers, fiber lasers, or other types of waveguide lasers. Solid-state lasers may generate output powers between a few milliwatts and (in high-power versions) many kilowatts.\n\nMany advanced applications in biophotonics require individually selectable light at multiple wavelengths. As a consequence a series of new laser technologies has been introduced, which currently looks for precise wording.\n\nThe most commonly used terminology are supercontinuum lasers, which emit visible light over a wide spectrum simultaneously. This light is then filtered e.g. via acousto-optic modulators (AOM, AOTF) into 1 or up to 8 different wavelengths. Typical suppliers for this technology was NKT Photonics or Fianium. Recently NKT Photonics bought Fianium, remaining the major supplier of the supercontinuum technology on the market.\n\nIn another approach (Toptica/iChrome) the supercontinuum is generated in the infra-red and then converted at a single selectable wavelength into the visible regime. This approach does not require AOTF's and has a background-free spectral purity.\n\nSince both concepts have major importance for biophotonics the umbrella term \"ultrachrome lasers\" is often used.\n\nSwept sources are designed to continuously change ('sweep') emitted light frequency in time. They typically continuously circle through a pre-defined range of frequencies (e.g., 800 +/- 50 nm). Swept sources in the terahertz regime have been demonstrated. A typical application of swept sources in biophotonics is Optical Coherence Tomography (OCT) Imaging.\n\nSingle photon sources are novel types of light sources distinct from coherent light sources (lasers) and thermal light sources (such as incandescent light bulbs and mercury-vapor lamps) that emit light as single particles or photons.\n", "id": "407292", "title": "Biophotonics"}
{"url": "https://en.wikipedia.org/wiki?curid=27522256", "text": "Dexiothetism\n\nDexiothetism refers to a reorganisation of a clade's bauplan, with right becoming ventral and left becoming dorsal. The organism would then recruit a new left hand side.\n\nIf a bilaterially symmetrical ancestor were to become affixed by its right hand side, it would occlude all features on that side. When that organism wanted to become secondarily bilaterally symmetrical again, it would be forced to resculpt its new left and right hand sides from the old left hand side. The end result is a bilaterially symmetrical animal, but with its dorsoventral axis rotated a quarter of a turn.\n\nDexiothetism has been implicated in the origin of the unusual embryology of the cephalochordate amphioxus, whereby its gill slits originate on the left hand side and the migrate to the right hand side.\n\nIn Jefferies' Calcichordate Theory, he supposes that all chordates and their mitrate ancestors are dexiothetic.\n", "id": "27522256", "title": "Dexiothetism"}
{"url": "https://en.wikipedia.org/wiki?curid=7381751", "text": "Permissiveness (biology)\n\nIn endocrinology, permissiveness is a biochemical phenomenon in which the presence of one hormone is required in order for another hormone to exert its full effects on a target cell. Hormones can interact in permissive, synergistic, or antagonistic ways. The chemical classes of hormones include amines, polypeptides, glycoproteins and steroids. Permissive hormones act as precursors to active hormones and may be classified as either prohormones or prehormones.\n\nThyroid hormone increases the number of receptors available for epinephrine at the latter's target cell, thereby increasing epinephrine's effect on that cell. Without the thyroid hormone, epinephrine would have only a weak effect. \n\nCortisol exerts a permissive effect on growth hormone.\n\nThe effects of a hormone in the body depend on its concentration. Permissive actions of glucocorticoids like cortisol generally occur at low concentrations. Abnormally high amounts of a hormone can result in atypical effects. Glucocorticoids function by attaching to cytoplasmic receptors to either enhance or suppress changes in the transcription of DNA and thus the synthesis of proteins. Glucocorticoids also inhibit the secretion of cytokines via post-translational modification effects.\n", "id": "7381751", "title": "Permissiveness (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=29004985", "text": "Whorl (biology)\n\nIn biology, a whorl is a cluster of cells or tissue that surrounds another and wraps around another in an expanding circular pattern. Whorls occur at the ends of different structures or in the middle of structures. Structures of some organs are often described as whorls and used in the aid of identification.\n\nThe Hassall's corpuscle, formed from type VI epithelial reticular cells in the thymus, is an example of a whorl-shaped structure.\n", "id": "29004985", "title": "Whorl (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=31536987", "text": "Perceptual trap\n\nA perceptual trap is an ecological scenario in which environmental change, typically anthropogenic, leads an organism to avoid an otherwise high-quality habitat. The concept is related to that of an ecological trap, in which environmental change causes preference towards a low-quality habitat.\n\nIn a 2004 article discussing source-sink dynamics, James Battin did not distinguish between high-quality habitats that are preferred or avoided, labelling both \"sources.\" The latter scenario, in which a high-quality habitat is avoided, was first recognised as an important phenonmenon in 2007 by Gilroy and Sutherland, who described them as \"undervalued resources.\" The term \"perceptual trap\" was first proposed by Michael Patten and Jeffrey Kelly in a 2010 article. Hans Van Dyck argues that the term is misleading because perception is also a major component in other cases of trapping.\n\nAnimals use discrete environmental cues to select habitat. A perceptual trap occurs if change in an environmental cue leads an organism to avoid a high-quality habitat. It differs, therefore, from simple habitat avoidance, which may be a correct decision given the habitat's quality. The concept of a perceptual trap is related to that of an ecological trap, in which environmental change causes preference towards a low-quality habitat. There is expected to be strong natural selection against ecological traps, but not necessarily against perceptual traps, as Allee effects may restrict a population’s ability to establish itself.\n\nTo support the concept of a perceptual trap, Patten and Kelly cited a study of the lesser prairie chicken (\"Tympanuchus pallidicinctus\"). The species' natural environment, shinnery oak grassland, is often treated with the herbicide tebuthiuron to increase grass cover for cattle grazing. Herbicide treatment resulted in less shrub cover, a habitat cue that caused female lesser prairie-chickens to avoid the habitat in favour of untreated areas. However, females who nested in herbicide-treated areas achieved comparable nesting successes and clutch sizes to those in untreated areas. Patten and Kelly suggest that the adverse effects of tebuthiuron treatment on nesting success are countered by various effects, such as greater nest concealment through increased grass cover. Therefore, female birds are erroneously avoiding a high-quality habitat. Patten and Kelly also cited as a possible perceptual trap the cases of the spotted towhee (\"Pipilo maculatus\") and rufous-crowned sparrow (\"Aimophila ruficeps\"), which tend to avoid habitat fragments, even though birds nesting in habitat fragments achieve increased nesting success due to a reduction in snake predation.\n\n", "id": "31536987", "title": "Perceptual trap"}
{"url": "https://en.wikipedia.org/wiki?curid=25086118", "text": "Autotroph\n\nAn autotroph (\"self-feeding\", from the Greek \"autos\" \"self\" and \"trophe\" \"nourishing\") or producer, is an organism that produces complex organic compounds (such as carbohydrates, fats, and proteins) from simple substances present in its surroundings, generally using energy from light (photosynthesis) or inorganic chemical reactions (chemosynthesis). They are the producers in a food chain, such as plants on land or algae in water (in contrast to heterotrophs as consumers of autotrophs). They do not need a living source of energy or organic carbon. Autotrophs can reduce carbon dioxide to make organic compounds for biosynthesis and also create a store of chemical energy. Most autotrophs use water as the reducing agent, but some can use other hydrogen compounds such as hydrogen sulfide. Some autotrophs, such as green plants and algae, are phototrophs, meaning that they convert electromagnetic energy from sunlight into chemical energy in the form of reduced carbon.\n\nAutotrophs can be photoautotrophs or chemoautotrophs. Phototrophs use light as an energy source, while chemotrophs use electron donors as a source of energy, whether from organic or inorganic sources; however in the case of autotrophs, these electron donors come from inorganic chemical sources. Such chemotrophs are lithotrophs. Lithotrophs use inorganic compounds, such as hydrogen sulfide, elemental sulfur, ammonium and ferrous iron, as reducing agents for biosynthesis and chemical energy storage. Photoautotrophs and lithoautotrophs use a portion of the ATP produced during photosynthesis or the oxidation of inorganic compounds to reduce NADP to NADPH to form organic compounds.\n\nThe Greek term \"autotroph\" was coined by the German botanist Albert Bernhard Frank in 1892.\n\nSome organisms rely on organic compounds as a source of carbon, but are able to use light or inorganic compounds as a source of energy. Such organisms are not defined as autotrophic, but rather as heterotrophic. An organism that obtains carbon from organic compounds but obtains energy from light is called a photoheterotroph, while an organism that obtains carbon from organic compounds but obtains energy from the oxidation of inorganic compounds is termed a chemoheterotroph, chemolithoheterotroph, or lithoheterotroph.\n\nEvidence suggests that some fungi may also obtain energy from radiation. Such radiotrophic fungi were found growing inside a reactor of the Chernobyl nuclear power plant.\n\nAutotrophs are fundamental to the food chains of all ecosystems in the world. They take energy from the environment in the form of sunlight or inorganic chemicals and use it to create energy-rich molecules such as carbohydrates. This mechanism is called primary production. Other organisms, called heterotrophs, take in autotrophs as food to carry out functions necessary for their life. Thus, heterotrophs — all animals, almost all fungi, as well as most bacteria and protozoa — depend on autotrophs, or primary producers, for the energy and raw materials they need. Heterotrophs obtain energy by breaking down organic molecules (carbohydrates, fats, and proteins) obtained in food. Carnivorous organisms rely on autotrophs indirectly, as the nutrients obtained from their heterotroph prey come from autotrophs they have consumed.\n\nMost ecosystems are supported by the autotrophic primary production of plants that capture photons initially released by the sun. Plants can only use a fraction of this energy for photosynthesis, approximately 1% is used by autotrophs. The process of photosynthesis splits a water molecule (HO), releasing oxygen (O) into the atmosphere, and reducing carbon dioxide (CO) to release the hydrogen atoms that fuel the metabolic process of primary production. Plants convert and store the energy of the photon into the chemical bonds of simple sugars during photosynthesis. These plant sugars are polymerized for storage as long-chain carbohydrates, including other sugars, starch, and cellulose; glucose is also used to make fats and proteins. When autotrophs are eaten by heterotrophs, i.e., consumers such as animals, the carbohydrates, fats, and proteins contained in them become energy sources for the heterotrophs. Proteins can be made using nitrates, sulfates, and phosphates in the soil.\n\n", "id": "25086118", "title": "Autotroph"}
{"url": "https://en.wikipedia.org/wiki?curid=33807629", "text": "Hindgut fermentation\n\nHindgut fermentation is a digestive process seen in monogastric herbivores, animals with a simple, single-chambered stomach. Cellulose is digested with the aid of symbiotic bacteria. The microbial fermentation occurs in the digestive organs that follow the small intestine: the large intestine and cecum. Examples of hindgut fermenters include proboscideans and large odd-toed ungulates such as horses and rhinos, as well as small animals such as rodents, rabbits and koalas. In contrast, foregut fermentation is the form of cellulose digestion seen in ruminants such as cattle which have a four-chambered stomach, as well as in sloths, macropodids, some monkeys, and one bird, the hoatzin.\n\nHindgut fermenters generally have a cecum and large intestine that are much larger and more complex than those of a foregut or midgut fermenter. Research on small cecum fermenters such as flying squirrels, rabbits and lemurs has revealed these mammals to have a GI tract about 10-13 times the length of their body. This is due to the high intake of fiber and other hard to digest compounds that are characteristic to the diet of monogastric herbivores. Unlike in foregut fermenters, the cecum is located after the stomach and small intestine in monogastric animals, which limits the amount of further digestion or absorption that can occur after the food is fermented.\n\nIn smaller hindgut fermenters of the order Lagomorpha (rabbits, hares, and pikas), cecotropes formed in the cecum are passed through the large intestine and subsequently reingested to allow another opportunity to absorb nutrients. Cecotropes are surrounded by a layer of mucus which protects them from stomach acid but which does not inhibit nutrient absorption in the small intestine. Coprophagy is also practiced by some rodents, such as the capybara, guinea pig and related species, and by the marsupial common ringtail possum. This process is also beneficial in allowing for restoration of the microflora population, or gut flora. These microbes are found in the digestive organs of living creatures and can act as protective agents that strengthen the immune system. Hindgut fermenters do have the ability to expel their microflora, which is useful during the acts of hibernation, estivation and torpor.\n\nWhile foregut fermentation is generally considered more efficient, and monogastric animals cannot digest cellulose as efficiently as ruminants, hindgut fermentation allows animals to consume small amounts of low-quality forage all day long and thus survive in conditions where ruminants might not be able to obtain nutrition adequate for their needs. Hindgut fermenters are able to extract more nutrition out of small quantities of feed. The large hind-gut fermenters are bulk feeders: they ingest large quantities of low-nutrient food, which they process more rapidly than would be possible for a similarly sized foregut fermenter. The main food in that category is grass, and grassland grazers move over long distances to take advantage of the growth phases of grass in different regions.\n\nThe ability to process food more rapidly than foregut fermenters gives hindgut fermenters an advantage at very large body size, as they are able to accommodate significantly larger food intakes. The largest extant and prehistoric megaherbivores, elephants and indricotheres (a type of rhino), respectively, have been hindgut fermenters. Study of the rates of evolution of larger maximum body mass in different terrestrial mammalian groups has shown that the fastest growth in body mass over time occurred in hindgut fermenters (perissodactyls, rodents and proboscids).\n\nHindgut fermenters are subdivided into two groups based on the relative size of various digestive organs in relationship to the rest of the system: colonic fermenters tend to be larger species such as horses, and cecal fermenters are smaller animals such as rabbits and rodents. However, in spite of the terminology, colonic fermenters such as horses make extensive use of the cecum to break down cellulose. Also, colonic fermenters typically have a proportionally longer large intestine than small intestine, whereas cecal fermenters have a considerably enlarged cecum compared to the rest of the digestive tract.\n\nIn addition to mammals, several insects are also hindgut fermenters, the best studied of which are the termites, which are characterised by an enlarged \"paunch\" of the hindgut that also houses the bulk of the gut microbiota. Digestion of wood particles in lower termites is accomplished inside the phagosomes of gut flagellates, but in the flagellate-free higher termites, this appears to be accomplished by fibre-associated bacteria.\n\n", "id": "33807629", "title": "Hindgut fermentation"}
{"url": "https://en.wikipedia.org/wiki?curid=61762", "text": "Class (biology)\n\nIn biological classification, class () is:\n\nThe composition of each class is determined by a taxonomist. Often there is no exact agreement, with different taxonomists taking different positions. There are no hard rules that a taxonomist needs to follow in describing a class, but for well-known animals there is likely to be consensus. \n\nIn botany, classes are now rarely discussed. Since the first publication of the APG system in 1998, which proposed a taxonomy of the flowering plants up to the level of orders, many sources have preferred to treat ranks higher than orders as informal clades. Where formal ranks have been assigned, the ranks have been reduced to a very much lower level, e.g. class Equisitopsida for the land plants, with the major divisions within the class assigned to subclasses and superorders.\n\nFor some clades, a number of alternative classifications are used.\n\nThe class as a distinct rank of biological classification having its own distinctive name (and not just called a \"top-level genus\" \"(genus summum)\" was first introduced by the French botanist Joseph Pitton de Tournefort in his classification of plants that appeared in his \"Eléments de botanique\", 1694.\n\nIn the first edition of his \"Systema Naturae\" (1735). Carl Linnaeus divided all three of his kingdoms of Nature (minerals, plants, and animals) into classes. Only in the animal kingdom are Linnaeus's classes similar to the classes used today; his classes and orders of plants were never intended to represent natural groups, but rather to provide a convenient \"artificial key\" according to his \"Systema Sexuale\", largely based on the arrangement of flowers.\n\nThe class was considered the highest level of the taxonomic hierarchy until George Cuvier's \"embranchements\", first called Phyla by Ernst Haeckel, were introduced in the early nineteenth century.\n\n", "id": "61762", "title": "Class (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=393574", "text": "Subvariety\n\nA subvariety (Latin: subvarietas) in botanical nomenclature is a taxonomic rank. They are rarely used to classify organisms.\n\nSubvariety is ranked: \n\nSubvariety is an infraspecific taxon. \nIts name consists of three parts: \n\nTo indicate the subvariety rank, the abbreviation \"subvar.\" is put before the infraspecific epithet.\n", "id": "393574", "title": "Subvariety"}
{"url": "https://en.wikipedia.org/wiki?curid=185894", "text": "Subphylum\n\nIn zoological nomenclature, a subphylum is a taxonomic rank below the rank of phylum. \n\nThe taxonomic rank of \"subdivision\" in fungi and plant taxonomy is equivalent to \"subphylum\" in zoological taxonomy. \n\nSubphylum is:\n\nWhere convenient, subphyla in turn may be divided into infraphyla; in turn such an infraphylum also would be superordinate to any classes or superclasses in the hierarchy.\n\nNot all fauna phyla are divided into subphyla. Those that are include: \n\nExamples of infraphyla include the Mycetozoa and the Gnathostomata.\n", "id": "185894", "title": "Subphylum"}
{"url": "https://en.wikipedia.org/wiki?curid=6867017", "text": "Paratype\n\nIn zoology and botany, a paratype is a specimen of an organism that helps define what the scientific name of a species and other taxon actually represents, but it is not the holotype (and in botany is also neither an isotype nor a syntype). Often there is more than one paratype. Paratypes are usually held in museum research collections.\n\nThe exact meaning of the term \"paratype\" when it is used in zoology is not the same as the meaning when it is used in botany. In both cases however, this term is used in conjunction with \"holotype\".\n\nIn zoological nomenclature, a paratype is officially defined as \"Each specimen of a type series other than the holotype.\"\n\nIn turn, this definition relies on the definition of a \"type series.\" A type series is the material (specimens of organisms) that was cited in the original publication of the new species or subspecies, and was not excluded from being type material by the author (this exclusion can be implicit, e.g., if an author mentions \"paratypes\" and then subsequently mentions \"other material examined\", the latter are not included in the type series), nor referred to as a variant, or only dubiously included in the taxon (e.g., a statement such as \"I have before me a specimen which agrees in most respects with the remainder of the type series, though it may yet prove to be distinct\" would exclude this specimen from the type series).\n\nThus, in a type series of five specimens, if one is the holotype, the other four will be paratypes.\n\nA paratype may originate from a different locality than the holotype. A paratype cannot become a lectotype, though it is eligible (and often desirable) for designation as a neotype.\n\nThe International Code of Zoological Nomenclature (ICZN) has not always required a type specimen, but any species or subspecies newly described after the end of 1999 must have a designated holotype or syntypes.\n\nA related term is allotype, a term that indicates a specimen that exemplifies the opposite sex of the holotype, and is almost without exception designated in the original description, and, accordingly, part of the type series, and thus a paratype; in such cases, it is functionally no different from any other paratype. It has no nomenclatural standing whatsoever, and although the practice of designating an allotype is recognized by the Code, it is not a \"name-bearing type\" and there are no formal rules controlling how one is designated. Apart from species exhibiting strong sexual dimorphism, relatively few authors take the trouble to designate such a specimen. It is not uncommon for an allotype to be a member of an entirely different species from the holotype, because of an incorrect association by the original author.\n\nIn botanical nomenclature, a paratype is a specimen cited in the original description that may not have been said to be a type. It is not the holotype nor an isotype (duplicate of the holotype).\n\n\nLike other types, a paratype may be specified for taxa at the rank of family or below (Article 7).\n\nA paratype may be designated as a lectotype if no holotype, isotype, syntype, or isosyntype (duplicate of a syntype) is extant (Article 9.12).\n\n", "id": "6867017", "title": "Paratype"}
{"url": "https://en.wikipedia.org/wiki?curid=1045127", "text": "Landrace\n\nA landrace is a domesticated, locally adapted, traditional variety of a species of animal or plant that has developed over time, through adaptation to its natural and cultural environment of agriculture and pastoralism, and due to isolation from other populations of the species. Landraces are generally distinguished from cultivars, and from breeds in the standardized sense, although the term landrace breed is sometimes used as distinguished from the term \"standardized breed\" when referring to cattle. The \"-race\" in this word refers to the taxonomic definition of \"race\" in biology, not the ethnographic sense of the word.\n\nSpecimens of a landrace tend to be relatively genetically uniform, but are more diverse than members of a standardized or formal breed. Some standardized animal breeds originate from attempts to make landraces more consistent through selective breeding and a landrace may become a more formal breed with the creation of a breed registry and/or publication of a breed standard. In such a case, the landrace may be thought of as a \"stage\" in breed development. However, in other cases, formalizing a landrace may result in the genetic resource of a landrace being lost through crossbreeding. Landraces are distinct from ancestral wild species of modern stock, and from separate species or subspecies derived from the same ancestor as modern domestic stock. Landraces are not all derived from ancient stock largely unmodified by human breeding interests. In a number of cases, most commonly dogs and horses, domestic animals have escaped in sufficient numbers in an area to breed feral populations that, through evolutionary pressure, can form new landraces in only a few centuries. In other cases, simple failure to maintain breeding regimens can do the same. For example, selectively bred cultivars can become new landraces when loosely selective reproduction is applied.\n\nIncreasing adoption of and reliance upon modern, purposefully selected plant strains, considered \"improved\" – \"scientifically bred to be uniform and stable\" – has led to a reduction in biodiversity. The majority of the genetic diversity of domesticated species lies in landraces and other traditionally used varieties, a \"reservoir of genetic resources\".\n\nGeneral features that characterize a landrace may include:\n\n\nNot every source on the topic enumerates each of these criteria, and they may be weighted differently depending on a given source's focus (e.g., governmental regulation, biological sciences, agribusiness, anthropology and culture, environmental conservation, pet keeping and breeding, etc.). Additionally, not all cultivars agreed to be landraces exhibit all possible landrace characteristics. Plant landraces have been the subject of more intensive study, and the majority of the academic literature about landraces is focused on agricultural botany, not animal husbandry. Most plant landraces are associated with traditional agricultural systems.\n\nWhile many landrace animals are associated with farming, other domestic animals have been put to use as modes of transportation, as companion animals, for sporting purposes, and for other non-farming uses, so their geographic distribution may differ. For example, horse landraces are less common because human use of them for transport has meant that they have moved with people more commonly and constantly than most other domestic animals, reducing the incidence of populations locally genetically isolated for extensive periods of time.\n\nThe word \"landrace\" literally means 'country-breed' (German: \"Landrasse\") and close cognates of it are found in various Germanic languages. The term was first defined (in German) by Kurt von Rümker in 1908, and more clearly described (in Dutch) in 1909 by U. J. Mansholt, who wrote that landraces have better \"stability of their characteristics\" and \"resistance capacity to tolerate adverse influences\" but lower production capacity than cultivars, and are apt to change genetically when moved to another environment. H. Kiessling added in 1912 that a landrace is a mixture of phenotypic forms despite relative outward uniformity, and a great adaptability to its natural and human environment. The word entered non-academic English in the early 1930s, by way of the Danish Landrace pig, a particular breed of lop-eared swine.\n\nAside from some standardized breeds having \"Landrace\" in their names, actual landraces and standardized breeds are sometimes further confused when the word \"breed\" is used very broadly. As one example, a glossary in a Food and Agriculture Organization of the United Nations (FAO) guideline defines \"landrace\" or \"landrace breed\" (treated synonymously) as \"a breed that has largely developed through adaptation to the natural environment and traditional production system in which it has been raised\". It also defines \"breed\" expansively and in multiple ways, with a focus on treating differing senses, landrace breed and standardized breed, as equivalent for \"genetic management\" purposes, the focus of the FAO guideline. It does clearly distinguish between the two concepts, however, both with a distinct definition of \"standardized breed\" and in the main body of the guideline, referring to the \"interaction between landraces and standardized breeds\"), and that FAO document uses \"breed\" to mean \"the unit of conservation, i.e. the specific population of animals that is to be conserved\". Similarly, the \"Oxford English Dictionary\" defines \"landrace\" as a \"local cultivar or animal breed that has been improved by traditional agricultural methods\", without specifying which definition of \"breed\" is cross-referenced. (The definition is also at odds with some peer-reviewed material, in which lack of formal, scientific breeding for genetic improvement (e.g. uniformity and stability) is characteristic of landraces; such sources clearly distinguish landraces from cultivars.)\n\nA landrace native to, or produced for a long time (e.g. 100 years or longer) within the agricultural system in which it is found is referred to as an \"autochthonous landrace\", while an introduced one is termed an allochthonous landrace. \"Within academic agronomy, the term \"autochthonous landrace\" is sometimes used with a more specific, productivity-related definition, synthesized by A. C. Zeven from previous definitions beginning with Mansholt's; it is not often encountered outside that field. These terms are most often applied to plants, with animals more often being referred to as indigenous or native.\n\nMany languages do not use separate terms, like \"landrace\" and \"breed\" in English, but instead rely on extended description to convey such distinctions. The FAO notes: \"The distinction between breeds and ecotypes within breeds is not very objective, and generally involves cultural rather than genetic factors.\"\n\nThe term landrace breed is sometimes encountered. In various domestic species (including pigs, goats, sheep and geese) some standardized breeds include \"Landrace\" in their names, and \"Landrace breeds\" (with capital \"L\") is sometimes used to refer to them collectively. but may be used more ambiguously to include actual landraces.\n\nSimilar ambiguity may be encountered in the use of terms such as ancient breed, native breed (not to be confused with native species), old breed, and indigenous breed. Farmers' variety, usually applied to local cultivars, or seen as intermediate between a landrace and a cultivar, may also include landraces when referring to plant varieties not subjected to formal breeding programs.\n\nThe term \"breed\" itself has multiple definitions and uses, some of which may encompass the concept of landraces. For example, the FAO Commission on Genetic Resources for Food and Agriculture (CGRFA) guideline provides a definition of \"breed\", for \"genetic management\" purposes, that overlaps with many definitions of \"landrace\", and defines \"landrace (or landrace breed)\" as a type of \"breed\".\n\nDue to their adaptation to the local environment, some farmers using scientifically \"improved\" domesticates also continue to raise landraces, because the latter often exhibit benefits, ranging from lower cost, and cultural (e.g. culinary) preference, to superior hardiness in a less-than-ideal climate, and better disease resistance. There may be more variety-specific pluses; a plant landrace may have, e.g., lower fertilizer requirements, or something about a plant or animal product's texture, color or ease of use might be a major factor.\n\nLandraces are often free from many intellectual property and other regulatory encumbrances. However, in some jurisdictions, a focus on their production may result in missing out on some benefits afforded to producers of genetically selected and homogenous organisms, including breeders' rights legislation, easier availability of loans and other business services, even the right to share seed or stock with others, depending on how favorable the laws in the area are to high-yield agribusiness interests. As Regine Andersen of the Fridtjof Nansen Institute (Norway) and the Farmers' Rights Project puts it, \"Agricultural biodiversity is being eroded. This trend is putting at risk the ability of future generations to feed themselves. In order to reverse the trend, new policies must be implemented worldwide. The irony of the matter is that the poorest farmers are the stewards of genetic diversity.\" Protecting farmer interests and protecting biodiversity is at the heart of the International Treaty on Plant Genetic Resources for Food and Agriculture (the \"Plant Treaty\" for short), under the Food and Agriculture Organization of the United Nations (FAO), though its concerns are not exclusively limited to landraces.\n\nIn 2005, a \"working definition\" of plant landraces was proposed: \"a dynamic population(s) of a cultivated plant that has historical origin, distinct identity and lacks formal crop improvement, as well as often being genetically diverse, locally adapted and associated with traditional farming systems\". Another definition, dating to 1975, of the term \"landrace\" as used in botany (and by extension in agriculture, horticulture, anthropology, etc.) was provided by J. R. Harlan: \n\nLandrace plants are grown from seeds which have not been systematically selected and marketed by seed companies, nor developed by plant breeders. The label \"landraces\" includes all those regional cultigens that are highly heterogeneous, but with enough characteristics in common to permit their recognition as a group.\n\nThis includes all cultigens cultivated without any specific nomenclature and value. A landrace identified with a unique feature, and selected for uniformity over a period of time for maintenance of the characteristic features of the population, can evolve into a \"farmers' variety\", or even a modern cultivar as in many crops (for example, \"Cajanus cajan\" 'Maruti' in the case of pigeon peas).\n\nConversely, a modern cultivar grown over time can \"evolve\" into a landrace, especially when self-seeded and some human selection is applied.\n\nA \"significant proportion\" of farmers around the world continue to grow landrace crops. However, as industrialized agriculture spreads, cultivars, which are selectively bred for high yield, rapid growth, disease and drought resistance, and other commercial production values, are supplanting many landraces, putting more and more of them at risk of extinction.\n\nUsing Europe as an example, data collected for an agricultural study published in 2008, showed that landrace cereal crops began to decline in Europe in the 19th century with selective seed improvements, and continued with varietal improvement in the 20th century, such that cereal landraces \"have largely fallen out of use\" in Europe. Landrace cultivation in central and northwest Europe was almost eradicated by the early 20th century, due to economic pressure to grow improved, modern cultivars. While many in the region are already extinct, some have survived in commercial European farming by being passed from generation to generation of farmers, and have also been revived by enthusiasts outside Europe to preserve European \"agricultural and food heritage\" elsewhere. These survivals are usually for specific uses, such as thatch, and traditional European cuisine and craft beer brewing. Systematic preservation efforts for these cereal strains are ongoing, \"in situ\" and in online-searchable germplasm collections (seed banks), coordinated by Biodiversity International and the National Institute of Agricultural Botany (UK). However, more may need to be done, because plant genetic variety, the source of crop health and seed quality, depends on a diversity of landraces and other traditionally used varieties. Efforts () were mostly focused on Iberia, the Balkans, and European Russia, and dominated by species from mountainous areas. Despite their incompleteness, these efforts have been described as \"crucial in preventing the extinction of many of these local ecotypes\".\n\nOne definition of a landrace applied to both plants and animals is \"which has developed over a long period of time and as a result has adapted to the local natural environment in which it lives.\" Geneticist D. Phillip Sponenberg described animal breeds as \"consistent and predictable genetic entities\" falling into several \"classes\": the landrace, the standardized breed, modern \"type\" breeds, industrial strains and feral populations. He describes landraces as an early stage of breed development, created by a combination of founder effect, isolation and environmental pressures. Isolation prevents the further introduction of genetic material. Human selection for production goals is typical of most landraces.\n\nOne definition of a \"landrace\", as applied to animals, is a biological race of [domestic] animal adapted to thrive in a specific land or locality. Another, applied to both plants and animals, is a variety \"which has developed over a long period of time and as a result has adapted to the local natural environment in which it lives.\"\n\nThere are various distinctive landraces of domestic cat around the world, including the Aegean, Cyprus, domestic long-haired, domestic short-haired, Kellas and Sokoke, among others. The Van cat of modern-day Turkey is a landrace of symbolic (and disputed) cultural value to Turks, Armenians and Kurds.\n\nMany standardized breeds have rather recently (within a century or less) been derived from landraces. Examples, often called \"natural breeds\", include Arabian Mau, Egyptian Mau, Korat, Kurilian Bobtail, Maine Coon, Manx, Norwegian Forest Cat, Siberian, and Thai (which is the landrace ancestor of modern Siamese cats), among many others.\n\nIn some cases, such as the Turkish Angora and Turkish Van breeds and their possible derivation from the Van cat landrace, the relationships are not entirely clear.\n\nOther examples of landrace bovines include Pineywoods, Florida Cracker, Ankole-Watusi and Randall cattle.\n\nDog landraces and the selectively bred dog breeds that follow breed standards vary widely depending on their origins and purpose. Landraces in dogs are defined as \"dog or any livestock animal has been bred without a formal registry, although their breeders may have kept written or informal pedigrees of their animals.\" These are distinguished from dog breeds which have breed standards, breed clubs and registries.\n\nLandrace dogs have more variety in their appearance than do standardized dog breeds. An example of a dog landrace with a related standardized breed with a similar name is the collie. The Scotch Collie is a landrace, while the Rough Collie and the Border Collie are standardized breeds. They can be very different in appearance, though the Rough Collie in particular was developed from the Scotch Collie by inbreeding to fix certain highly desired traits. In contrast to the landrace, in the various standardized Collie breeds, purebred individuals closely match a breed-standard appearance but might have lost other useful characteristics and have developed undesirable traits linked to inbreeding. Similarly, the ancient landrace of the Fertile Crescent that led to the Saluki breed excels in running down game across open tracts of hot desert, but conformation-bred individuals of the breed might not be able to chase and catch desert hares.\n\nThe now extinct St. John's water dog, a landrace native to the island of Newfoundland, Canada, was the foundational stock for a number of purpose-bred dogs, such as the Labrador Retriever, Chesapeake Bay Retriever, Cape Shore Water Dog, and Newfoundland. Another example of a North American landrace, the Carolina Dog or yellow dog, was developed from dogs originally from Asia; it has also been established now as a standardized breed.\n\nThe mountain dog is a group of breeds and landraces, and is the common working dog \"type\" of mountain environs of central Eurasia. One of these is the Armenian Gampr dog.\n\n\nSome standardized, selective breeds that are derived from landraces include the Dutch Landrace, Swedish Landrace and Finnish Landrace goats. The confusingly named Danish Landrace is a modern mix of three different breeds, one of which was a \"Landrace\"-named breed.\n\n\nHorse landraces are \"local types which have become uniform through a combination of founder effect, long isolation from other populations and selection within a local environment.\" Geneticist D. Phillip Sponenberg further refines the definition, noting, \"[t]he relative genetic uniformity of landraces results from both moderate inbreeding and from selection ... a combination of natural factors and human factors, all acting in an agricultural or pastoral setting.\" It is rare for landraces among domestic horses to remain isolated, due to human use of horses for transportation, thus causing horses to move from one local population to another. Examples of horse landraces include isolated island populations such as the Shetland pony and Icelandic horse, insular landraces in Greece and Indonesia, and, on a broader scale, New World populations derived from the founder stock of Colonial Spanish horse. The Yakutian and Mongolian Horses of Asia have \"unimproved\" characteristics. The heavy 'draft' type of domestic horse, developed in Europe, has itself differentiated into many separate landraces or breeds. \n\nThe wild progenitor of the domestic horse is now extinct. The Przewalski's horse, \"Equus ferus przewalskii\", is a wholly separate subspecies with a different number of chromosomes than domesticated horses (\"E. f. caballus\"), and has never been successfully domesticated.\n\nThe Mulefoot pig breed originated as a landrace, but has been a standardized breed since the early 1900s. The standardized swine breeds named \"Landrace\" are not actually landraces, and often not even derived from one, but from other breeds with \"Landrace\" in their names. The Danish Landrace pig breed, pedigreed in 1896 from the actual local landrace, is the principal ancestor of the American Landrace (1930s). The Swedish Landrace is derived from the Danish and from other Scandinavian breeds, as was the British Landrace breed, which was established as late as 1950.\n\nLandrace chicken varieties include:\n\n\nLandrace duck varieties include:\n\n\nLandrace goose varieties include:\n\n\nNote: Many standardized breeds named \"Landrace\", e.g. the Twente Landrace goose, are not actually true landrace breeds, but may be derived from them.\n\n\n", "id": "1045127", "title": "Landrace"}
{"url": "https://en.wikipedia.org/wiki?curid=34058358", "text": "Gracility\n\nGracility is slenderness, the condition of being gracile, which means slender.\n\nIt derives from the Latin adjective \"gracilis\" (masculine or feminine), or \"gracile\" (neuter) which in either form means slender, and when transferred for example to discourse, takes the sense of \"without ornament\", \"simple\", or various similar connotations. \n\nIn his famous \"Glossary of Botanic Terms\", B. D. Jackson speaks dismissively of an entry in earlier dictionary of A. A. Crozier as follows: \"Gracilis (Lat.), slender. Crozier has the needless word \"gracile\"\". However, his objection would be hard to sustain in current usage; apart from the fact that \"gracile\" is a natural and convenient term, it is hardly a neologism; the Shorter Oxford English Dictionary gives the source date for that usage as 1623. \n\nIn the same entry for \"Gracile\", the Shorter Oxford English Dictionary remarks: \"Recently misused (through association with grace) for Gracefully slender.\" This misuse is unfortunate at least, because the terms \"gracile\" and \"grace\" are completely unrelated: the etymological root of \"grace\" is the Latin word \"gratia\" from \"gratus\", meaning pleasing and nothing to do with slenderness or thinness. \n\nIn biology, the term is in common use, whether as English or Latin:\n\nIn biological taxonomy, gracile is the specific name or specific epithet for various species. Where the gender is appropriate, the form is gracilis. Examples include:\n\nThe same root appears in the names of some genera and higher taxa:\n\n", "id": "34058358", "title": "Gracility"}
{"url": "https://en.wikipedia.org/wiki?curid=2248681", "text": "Range state\n\nRange state is a term generally used in zoogeography and conservation biology to refer to any nation that exercises jurisdiction over any part of a range which a particular species, taxon or biotope inhabits, or crosses or overflies at any time on its normal migration route. The term is often expanded to \"also\" include, particularly in international waters, any nation with vessels flying their flag that engage in exploitation (e.g. hunting, fishing, capturing) of that species. Countries in which a species occurs only as a vagrant or ‘accidental’ visitor outside of its normal range or migration route are not usually considered range states. \n\nBecause governmental conservation policy is often formulated on a national scale, and because in most countries, both governmental and private conservation organisations are also organised at the national level, the range state concept is often used by international conservation organizations in formulating their conservation and campaigning policy. \n\nAn example of one such organization is the Convention on the Conservation of Migratory Species of Wild Animals (CMS, or the “Bonn Convention”). It is a multilateral treaty focusing on the conservation of critically endangered and threatened migratory species, their habitats and their migration routes. Because such habitats and/or migration routes may span national boundaries, conservation efforts are less likely to succeed without the cooperation, participation, and coordination of each of the range states. \n\n \n", "id": "2248681", "title": "Range state"}
{"url": "https://en.wikipedia.org/wiki?curid=3630347", "text": "Natural competence\n\nIn microbiology, genetics, cell biology, and molecular biology, competence is the ability of a cell to alter its genetics by taking up extracellular (\"naked\") DNA from its environment in the process called transformation. Competence may be differentiated between \"natural competence\", a genetically specified ability of bacteria which is thought to occur under natural conditions as well as in the laboratory, and \"induced\" or \"artificial competence\", which arises when cells in laboratory cultures are treated to make them transiently permeable to DNA. Competence allows for rapid adaptation and DNA repair of the cell. This article primarily deals with natural competence in bacteria, although information about artificial competence is also provided.\n\nNatural competence was discovered by Frederick Griffith in 1928, when he showed that a preparation of killed cells of a pathogenic bacterium contained something that could transform related non-pathogenic cells into the pathogenic type. In 1944 Oswald Avery, Colin MacLeod, and Maclyn McCarty demonstrated that this 'transforming factor' was pure DNA \n. This was the first compelling evidence that DNA carries the genetic information of the cell.\n\nSince then, natural competence has been studied in a number of different bacteria, particularly \"Bacillus subtilis\", \"Streptococcus pneumoniae\" (Griffith's \"pneumococcus\"), \"Neisseria gonorrhoeae\" and \"Haemophilus influenzae\". Areas of active research include the mechanisms of DNA transport, the regulation of competence in different bacteria, and the evolutionary function of competence.\n\nIn the natural world DNA usually becomes available by death and lysis of other cells, but in the laboratory it is provided by the researcher, often as a genetically engineered fragment or plasmid. During uptake, DNA is transported across the cell membrane(s), and the cell wall if one is present. Once the DNA is inside the cell it may be degraded to nucleotides, which are reused for DNA replication and other metabolic functions. Alternatively it may be recombined into the cell’s genome by its DNA repair enzymes. If this recombination changes the cell’s genotype the cell is said to have been transformed. Artificial competence and transformation are used as research tools in many organisms (\"see Transformation (genetics)\").\n\nIn almost all naturally competent bacteria components of extracellular filaments called type 4 pili (a type of fimbria) bind extracellular double stranded DNA. The DNA is then translocated across the membrane (or membranes for gram negative bacteria) through multi-component protein complexes driven by the degradation of one strand of the DNA. Single stranded DNA in the cell is bound by a well-conserved protein, DprA, which loads the DNA onto RecA, which mediates homologous recombination through the classic DNA repair pathway.\n\nIn laboratory cultures, natural competence is usually tightly regulated and often triggered by nutritional shortages or adverse conditions. However the specific inducing signals and regulatory machinery are much more variable than the uptake machinery, and little is known about the regulation of competence in the natural environments of these bacteria. Transcription factors have been discovered which regulate competence; an example is sxy (also known as tfoX) which has been found to be regulated in turn by a 5' non-coding RNA element. In bacteria capable of forming spores, conditions inducing sporulation often overlap with those inducing competence. Thus cultures or colonies containing sporulating cells often also contain competent cells. Recent research by Süel \"et al.\" has identified an excitable core module of genes which can explain entry into and exit from competence when cellular noise is taken into account.\n\nMost competent bacteria are thought to take up all DNA molecules with roughly equal efficiencies, but bacteria in the families Neisseriaceae and Pasteurellaceae preferentially take up DNA fragments containing short DNA sequences, termed DNA uptake sequence (DUS and USS respectively), that are very frequent in their own genomes. Neisserial genomes contain thousands of copies of the preferred sequence GCCGTCTGAA, and Pasteurellacean genomes contain either AAGTGCGGT or ACAAGCGGT.\n\nMost proposals made for the primary evolutionary function of natural competence as a part of natural bacterial transformation fall into three categories: (1) the selective advantage of genetic diversity; (2) DNA uptake as a source of nucleotides (DNA as “food”); and (3) the selective advantage of a new strand of DNA to promote homologous recombinational repair of damaged DNA (DNA repair). A secondary suggestion has also been made, noting the occasional advantage of horizontal gene transfer.\n\nArguments to support genetic diversity as the primary evolutionary function of sex (including bacterial transformation) are given by Barton and Charleworth . and by Otto and Gerstein. However, the theoretical difficulties associated with the evolution of sex suggest that sex for genetic diversity is problematic. Specifically with respect to bacterial transformation, competence requires the high cost of a global protein synthesis switch, with, for example, more than 16 genes that are switched on only during competence of \"Streptococcus pneumoniae\". However, since bacteria tend to grow in clones, the DNA available for transformation would generally have the same genotype as that of the recipient cells. Thus, there is always a high cost in protein expression without, in general, an increase in diversity. Other differences between competence and sex have been considered in models of the evolution of genes causing competence; these models found that competence's postulated recombinational benefits were even more elusive than those of sex.\n\nThe second hypothesis, DNA as food, relies on the fact that cells that take up DNA inevitably acquire the nucleotides the DNA consists of, and, because nucleotides are needed for DNA and RNA synthesis and are expensive to synthesize, these may make a significant contribution to the cell's energy budget. Some naturally competent bacteria also secrete nucleases into their surroundings, and all bacteria can take up the free nucleotides these nucleases generate from environmental DNA. The energetics of DNA uptake are not understood in any system, so it is difficult to compare the efficiency of nuclease secretion to that of DNA uptake and internal degradation. In principle the cost of nuclease production and the uncertainty of nucleotide recovery must be balanced against the energy needed to synthesize the uptake machinery and to pull DNA in. Other important factors are the likelihoods that nucleases and competent cells will encounter DNA molecules, the relative inefficiencies of nucleotide uptake from the environment and from the periplasm (where one strand is degraded by competent cells), and the advantage of producing ready-to-use nucleotide monophosphates from the other strand in the cytoplasm. Another complicating factor is the self-bias of the DNA uptake systems of species in the family \"Pasteurellaceae\" and the genus \"Neisseria\", which could reflect either selection for recombination or for mechanistically efficient uptake.\n\nIn bacteria, the problem of DNA damage is most pronounced during periods of stress, particularly oxidative stress, that occur during crowding or starvation conditions. Under such conditions there is often only a single chromosome present. The finding that some bacteria induce competence under such stress conditions, supports the third hypothesis, that transformation exists to permit DNA repair. In experimental tests, bacterial cells exposed to agents damaging their DNA, and then undergoing transformation, survived better than cells exposed to DNA damage that did not undergo transformation (Hoelzer and Michod, 1991). In addition, competence to undergo transformation is often inducible by known DNA damaging agents (reviewed by Michod \"et al\"., 2008 and Bernstein \"et al\"., 2012). Thus, a strong short-term selective advantage for natural competence and transformation would be its ability to promote homologous recombinational DNA repair under conditions of stress. Such stress conditions might be incurred during bacterial infection of a susceptible host. Consistent with this idea, Li et al. reported that, among different highly transformable \"S. pneumoniae\" isolates, nasal colonization fitness and virulence (lung infectivity) depends on an intact competence system.\n\nA counter argument was made based on the 1993 report of Redfield who found that single-stranded and double-stranded damage to chromosomal DNA did not induce or enhance competence or transformation in \"B. subtilis\" or \"H. influenzae\", suggesting that selection for repair has played little or no role in the evolution of competence in these species\n\nHowever more recent evidence indicates that competence for transformation is, indeed, specifically induced by DNA damaging conditions. For instance, Claverys \"et al\". in 2006 showed that the DNA damaging agents mitomycin C (a DNA cross-linking agent) and fluoroquinolone (a topoisomerase inhibitor that causes double-strand breaks) induce transformation in \"Streptococcus pneumoniae\". In addition, Engelmoer and Rozen in 2011 demonstrated that in \"S. pneumoniae\" transformation protects against the bactericidal effect of mitomycin C. Induction of competence further protected against the antibiotics kanomycin and streptomycin. Although these aminoglycoside antibiotics were previously regarded as non-DNA damaging, recent studies in 2012 of Foti \"et al\". showed that a substantial portion of their bactericidal activity results from release of the hydroxyl radical and induction of DNA damages, including double-strand breaks.\n\nDorer \"et al\"., in 2010, showed that ciprofloxacin, which interacts with DNA gyrase and causes production of double-strand breaks, induces expression of competence genes in \"Helicobacter pylori\", leading to increased transformation. In 2011 studies of \"Legionella pneumophila\", Charpentier \"et al\". tested 64 toxic molecules to determine which ones induce competence. Only six of these molecules, all DNA damaging agents, strongly induced competence. These molecules were norfloxacin, ofloxacin and nalidixic acid (inhibitors of DNA gyrase that produce double strand breaks), mitomycin C (which produces inter-strand cross-links), bicyclomycin (causes single- and double-strand breaks), and hydroxyurea (causes oxidation of DNA bases). Charpentier \"et al\". also showed that UV irradiation induces competence in \"L. pneumophila\" and further suggested that competence for transformation evolved as a response to DNA damage.\n\nA long-term advantage may occasionally be conferred by occasional instances of horizontal gene transfer also called \"lateral gene transfer\", (which might result from non-homologous recombination after competence is induced), that could provide for antibiotic resistance or other advantages.\n\nRegardless of the nature of selection for competence, the composite nature of bacterial genomes provides abundant evidence that the horizontal gene transfer caused by competence contributes to the genetic diversity that makes evolution possible\n\n", "id": "3630347", "title": "Natural competence"}
{"url": "https://en.wikipedia.org/wiki?curid=7614929", "text": "Decussation\n\nDecussation is used in biological contexts to describe a crossing (Latin: the roman numeral for ten, deca, is an uppercase 'X'). (In Latin anatomical terms the form decussatio is used, e.g. \"decussatio pyramidum\".) Similarly, the anatomical term chiasma is named after the Greek uppercase 'Χ', chi). \n\nExamples include:\n\n\nThe origin of the contralateral organization, the optic chiasm and the major decussations on the nervous system of vertebrates has been a long standing puzzle to scientists. For long the visual map theory of Ramón y Cajal has been the most popular theory., see also for an english summary. More recently, scientists have realized that this theory has some severe flaws. According to the current theory, the decussations are caused by an axial twist which makes that the anterior head, along with the forebrain is turned by 180° with respect to the rest of the body.\n\n\n", "id": "7614929", "title": "Decussation"}
{"url": "https://en.wikipedia.org/wiki?curid=2227366", "text": "Photoheterotroph\n\nPhotoheterotrophs (\"Gk\": photo = light, hetero = (an)other, troph = nourishment) are heterotrophic phototrophs—that is, they are organisms that use light for energy, but cannot use carbon dioxide as their sole carbon source. Consequently, they use organic compounds from the environment to satisfy their carbon requirements; these compounds include carbohydrates, fatty acids, and alcohols. Examples of photoheterotrophic organisms include purple non-sulfur bacteria, green non-sulfur bacteria, and heliobacteria. Recent research has indicated that the oriental hornet and some aphids may be able to use light to supplement their energy supply.\n\nPhotoheterotrophs generate ATP using light, in one of two ways: they use a bacteriochlorophyll-based reaction center, or they use a bacteriorhodopsin. The chlorophyll-based mechanism is similar to that used in photosynthesis, where light excites the molecules in a reaction center and causes a flow of electrons through an electron transport chain (ETS). This flow of electrons through the proteins causes hydrogen ions to be pumped across a membrane. The energy stored in this proton gradient is used to drive ATP synthesis. Unlike in photoautotrophs, the electrons flow only in a cyclic pathway: electrons released from the reaction center flow through the ETS and return to the reaction center. They are not utilized to reduce any organic compounds. Purple non-sulfur bacteria, green non-sulfur bacteria and heliobacteria are examples of bacteria that carry out this scheme of photoheterotrophy.\n\nOther organisms, including halobacteria and flavobacteria and vibrios have purple-rhodopsin-based proton pumps that supplement their energy supply. The archaeal version is called bacteriorhodopsin, while the eubacterial version is called proteorhodopsin. The pump consists of a single protein bound to a Vitamin A derivative, retinal. The pump may have accessory pigments (e.g., carotenoids) associated with the protein. When light is absorbed by the retinal molecule, the molecule isomerises. This drives the protein to change shape and pump a proton across the membrane. The hydrogen ion gradient can then be used to generate ATP, transport solutes across the membrane, or drive a flagellar motor. One particular flavobacterium cannot reduce carbon dioxide using light, but uses the energy from its rhodopsin system to fix carbon dioxide through anaplerotic fixation. The flavobacterium is still a heterotroph as it needs reduced carbon compounds to live and cannot subsist on only light and CO. It cannot carry out reactions in the form of\nwhere HD may be water, HS or another compound/compounds providing the reducing electrons and protons; the 2D + HO pair represents an oxidized form.\n\nHowever, it can fix carbon in reactions like:\nwhere malate or other useful molecules are otherwise obtained by breaking down other compounds by\n\nThis method of carbon fixation is useful when reduced carbon compounds are scarce and cannot be wasted as CO during interconversions, but energy is plentiful in the form of sunlight.\n\n\n\nUniversity of Wisconsin, Madison Microbiology Online Textbook\n", "id": "2227366", "title": "Photoheterotroph"}
{"url": "https://en.wikipedia.org/wiki?curid=14389994", "text": "Natural landscape\n\nA natural landscape is the original landscape that exists before it is acted upon by human culture. The natural landscape and the cultural landscape are separate parts of the landscape. However, in the twenty-first century landscapes that are totally untouched by human activity no longer exist, so that reference is sometimes now made to degrees of naturalness within a landscape.\n\nIn \"Silent Spring\" (1962) Rachel Carson describes a roadside verge as it used to look: \"Along the roads, laurel, viburnum and alder, great ferns and wildflowers delighted the traveler’s eye through much of the year\" and then how it looks now following the use of herbicides: \"The roadsides, once so attractive, were now lined with browned and withered vegetation as though swept by fire\". Even though the landscape before it is sprayed is biologically degraded, and may well contains alien species, the concept of what might constitute a natural landscape can still be deduced from the context.\n\nThe phrase \"natural landscape\" was first used in connection with landscape painting, and landscape gardening, to contrast a formal style with a more natural one, closer to nature. Alexander von Humboldt (1769 – 1859) was to further conceptualize this into the idea of a natural landscape \"separate\" from the cultural landscape. Then in 1908 geographer Otto Schlüter developed the terms original landscape (\"Urlandschaft\") and its opposite cultural landscape (\"Kulturlandschaft\") in an attempt to give the science of geography a subject matter that was different from the other sciences. An early use of the actual phrase \"natural landscape\" by a geographer can be found in Carl O. Sauer's paper \"The Morphology of Landscape\" (1925).\n\nThe concept of a natural landscape was first developed in connection with landscape painting, though the actual term itself was first used in relation to landscape gardening. In both cases it was used to contrast a formal style with a more natural one, that is closer to nature. Chunglin Kwa suggests, \"that a seventeenth-century or early-eighteenth-century person could experience natural scenery ‘just like on a painting,’ and so, with or without the use of the word itself, designate it as a landscape.\" With regard to landscape gardening John Aikin, commented in 1794: \"Whatever, therefore, there be of \"novelty\" in the singular scenery of an artificial garden, it is soon exhausted, whereas the infinite diversity of a natural landscape presents an inexhaustible flore of new forms\". Writing in 1844 the prominent American landscape gardener Andrew Jackson Downing comments: \"straight canals, round or oblong pieces of water, and all the regular forms of the geometric mode ... would evidently be in violent opposition to the whole character and expression of natural landscape\".\n\nIn his extensive travels in South America, Alexander von Humboldt became the first to conceptualize a natural landscape separate from the cultural landscape, though he does not actually use these terms. Andrew Jackson Downing was aware of, and sympathetic to, Humboldt's ideas, which therefore influenced American landscape gardening.\n\nSubsequently, the geographer Otto Schlüter, in 1908, argued that by defining geography as a \"Landschaftskunde\" (landscape science) would give geography a logical subject matter shared by no other discipline. He defined two forms of landscape: the \"Urlandschaft\" (original landscape) or landscape that existed before major human induced changes and the \"Kulturlandschaft\" (cultural landscape) a landscape created by human culture. Schlüter argued that the major task of geography was to trace the changes in these two landscapes.\n\nThe term natural landscape is sometimes used as a synonym for wilderness, but for geographers natural landscape is a scientific term which refers to the biological, geological, climatological and other aspects of a landscape, not the cultural values that are implied by the word wilderness.\n\nMatters are complicated by the fact that the words nature and natural have more than one meaning. On the one hand there is the main dictionary meaning for nature: \"The phenomena of the physical world collectively, including plants, animals, the landscape, and other features and products of the earth, as opposed to humans or human creations\". On the other hand, there is the growing awareness, especially since Charles Darwin, of humanities biological affinity with nature.\n\nThe dualism of the first definition has its roots is an \"ancient concept\", because early people viewed \"nature, or the nonhuman world […] as a divine \"Other\", godlike in its separation from humans\". In the West, Christianity's myth of the fall, that is the expulsion of humankind from the Garden of Eden, where all creation lived in harmony, into an imperfect world, has been the major influence. Cartesian dualism, from the seventeenth century on, further reinforced this dualistic thinking about nature. \nWith this dualism goes value judgement as to the superiority of the natural over the artificial. Modern science, however, is moving towards a holistic view of nature.\n\nWhat is meant by natural, within the American conservation movement, has been changing over the last century and a half.\n\nIn the mid-nineteenth century American began to realize that the land was becoming more and more domesticated and wildlife was disappearing. This led to the creation of American National Parks and other conservation sites. Initially it was believed that all that was needed to do was to separate what was seen as natural landscape and \"avoid disturbances such as logging, grazing, fire and insect outbreaks\". This, and subsequent environmental policy, until recently, was influenced by ideas of the wilderness. However, this policy was not consistently applied, and in Yellowstone Park, to take one example, the existing ecology was altered, firstly by the exclusion of Native Americans and later with the virtual extermination of the wolf population.\n\nA century later, in the mid-twentieth century, it began to be believed that the earlier policy of \"protection from disturbance was inadequate to preserve park values\", and that is that direct human intervention was necessary to restore the landscape of National Parks to its ‘’natural’’ condition. In 1963 the Leopold Report argued that \"A national park should represent a vignette of primitive America\". This policy change eventually led to the restoration of wolves in Yellowstone Park in the 1990s.\n\nHowever, recent research in various disciplines indicates that a pristine natural or \"primitive\" landscape is a myth, and it now realised that people have been changing the natural into a cultural landscape for a long while, and that there are few places untouched in some way from human influence. The earlier conservation policies were now seen as cultural interventions. The idea of what is natural and what artificial or cultural, and how to maintain the natural elements in a landscape, has been further complicated by the discovery of global warming and how it is changing natural landscapes.\n\nAlso important is a reaction recently amongst scholars against dualistic thinking about nature and culture. Maria Kaika comments: \"Nowadays, we are beginning to see nature and culture as intertwined once again – not ontologically separated anymore […].What I used to perceive as a compartmentalized world, consisting of neatly and tightly sealed, autonomous ‘space envelopes’ (the home, the city, and nature) was, in fact, a messy socio-spatial continuum”. And William Cronon argues against the idea of wilderness because it \"involves a dualistic vision in which the human is entirely outside the natural\" and affirms that \"wildness (as opposed to wilderness) can be found anywhere\" even \"in the cracks of a Manhattan sidewalk\". According to Cronon we have to \"abandon the dualism that sees the tree in the garden as artificial […] and the tree in the wilderness as natural […] Both in some ultimate sense are wild.\" Here he bends somewhat the regular dictionary meaning of wild, to emphasise that nothing natural, even in a garden, is fully under human control.\n\nThe landscape of Europe has considerably altered by people and even in an area, like the Cairngorm Mountains of Scotland, with a low population density, only \" the high summits of the Cairngorm Mountains, consist entirely of natural elements. These \"high summits\" are of course only part of the Cairngorms, and there are no longer wolves, bears, wild boar or lynx in Scotland's wilderness. The Scots pine in the form of the Caledonian forest also covered much more of the Scottish landscape than today.\n\nThe Swiss National Park, however, represent a more natural landscape. It was founded in 1914, and is one of the earliest national parks in Europe.\nVisitors are not allowed to leave the motor road, or paths through the park, make fire or camp. The only building within the park is Chamanna Cluozza, mountain hut. It is also forbidden to disturb the animals or the plants, or to take home anything found in the park. Dogs are not allowed. Due to these strict rules, the Swiss National Park is the only park in the Alps who has been categorized by the IUCN as a strict nature reserve, which is the highest protection level.\n\nNo place on the Earth is unaffected by people and their culture. People are part of biodiversity, but human activity affects biodiversity, and this alters the natural landscape. Mankind have altered landscape to such an extent that few places on earth remain pristine, but once free of human influences, the landscape can return to a natural or near natural state.\nEven the remote Yukon and Alaskan wilderness, the bi-national Kluane-Wrangell-St. Elias-Glacier Bay-Tatshenshini-Alsek park system comprising Kluane, Wrangell-St Elias, Glacier Bay and Tatshenshini-Alsek parks, a UNESCO World Heritage Site, is not free from human influence, because the Kluane National Park lies within the traditional territories of the Champagne and Aishihik First Nations and Kluane First Nation who have a long history of living in this region. Through their respective Final Agreements with the Canadian Government, they have made into law their rights to harvest in this region.\n\nCultural forces intentionally or unintentionally, have an influence upon the landscape. Cultural landscapes are places or artifacts created and maintained by people. Examples of cultural intrusions into a landscape are: fences, roads, parking lots, sand pits, buildings, hiking trails, management of plants, including the introduction of invasive species, extraction or removal of plants, management of animals, mining, hunting, natural landscaping, farming and forestry, pollution. Areas that might be confused with a natural landscape include public parks, farms, orchards, artificial lakes and reservoirs, managed forests, golf courses, nature center trails, gardens.\n\n", "id": "14389994", "title": "Natural landscape"}
{"url": "https://en.wikipedia.org/wiki?curid=5044718", "text": "Belt transect\n\nBelt transects are used in biology to estimate the distribution of organisms in relation to a certain area, such as the seashore or a meadow. It records all the species found between two lines and how far they are for a certain place or area and how many of them there are. An interrupted belt transect records all the species found in quadrats (square frames) placed at certain intervals along a line.\n\nThe belt transect method is similar to the line transect method but gives information on abundance as well as presence, or absence of species. It may be considered as a widening of the line transect to form a continuous belt, or series of quadrats.\n\nIn this method, the transect line is laid out across the area to be surveyed and a quadrat is placed on the first marked point on the line. These marked points should be a set amount of space apart. The plants and/or animals inside the quadrat are then identified and their abundance estimated. Animals can be counted within the quadrat, or collected, while it is usual to estimate the percentage cover of plant species. Cover is the area of the quadrat occupied by the above-ground parts of a species when viewed from above. The canopies of the plants inside the quadrat will often overlap each other, so the total percentage cover of plants in a single quadrat will frequently add up to more than 100%. \n\nQuadrats are sampled all the way down the transect line, at each marked point on the line, or at some other predetermined interval (or even randomly) if time is short. It is important that the same person should do the estimations of cover in each quadrat, because the estimation is likely to vary from person to person. If different people estimate percentage cover in different quadrats, then an element of personal variation is introduced which will lead to less accurate results. Sampling should always be as least destructive as possible and trampling the surrounding area should be avoided when carrying out a survey. Not only is there a risk of damaging the area, but there is also a risk of reducing the population or percentage cover of the species being surveyed.\n", "id": "5044718", "title": "Belt transect"}
{"url": "https://en.wikipedia.org/wiki?curid=60359", "text": "Benthos\n\nBenthos is the community of organisms that live on, in, or near the seabed, also known as the benthic zone. This community lives in or near marine sedimentary environments, from tidal pools along the foreshore, out to the continental shelf, and then down to the abyssal depths.\n\nMany organisms adapted to deep-water pressure cannot survive in the upper parts of the water column. The pressure difference can be very significant (approximately one atmosphere for each 10 metres of water depth).\n\nBecause light is absorbed before it can reach deep ocean-water, the energy source for deep benthic ecosystems is often organic matter from higher up in the water column that drifts down to the depths. This dead and decaying matter sustains the benthic food chain; most organisms in the benthic zone are scavengers or detritivores.\n\nThe term \"benthos\", coined by Haeckel in 1891, comes from the Greek noun \"depth of the sea\". \"Benthos\" is also used in freshwater biology to refer to organisms at the bottom of freshwater bodies of water, such as lakes, rivers, and streams. There is also a redundant synonym, \"benthon\".\n\nThe main food sources for the benthos are algae and organic runoff from land. The depth of water, temperature and salinity, and type of local substrate all affect what benthos is present. In coastal waters and other places where light reaches the bottom, benthic photosynthesizing diatoms can proliferate. Filter feeders, such as sponges and bivalves, dominate hard, sandy bottoms. Deposit feeders, such as polychaetes, populate softer bottoms. Fish, such as dragonets, as well as sea stars, snails, cephalopods, and crustaceans are important predators and scavengers.\n\nBenthic organisms, such as sea stars, oysters, clams, sea cucumbers, brittle stars and sea anemones, play an important role as a food source for fish, such as the California sheephead, and humans.\n\nThey are easily visible to the naked eye with the lower range of body size at 0.5 mm but usually larger than 3 mm. In the coastal water ecosystem, they include several species of organisms from different taxa including Porifera, Annelids, Coelenterates, Mollusks, Crustaceans, Arthropods etc.\n\nZoobenthos comprises the animals belonging to the benthos.\n\nPhytobenthos comprises the plants belonging to the benthos, mainly benthic diatoms and macroalgae (seaweed).\n\nEndobenthos lives buried, or burrowing in the sediment, often in the oxygenated top layer, e.g., a sea pen or a sand dollar.\n\nEpibenthos lives on top of the sediments, e.g., like a sea cucumber or a sea snail crawling about.\n\nHyperbenthos lives just above the sediment, e.g., a rock cod.\n\nContrast the terms plankton (the organisms that float or drift within the water), nekton (the organisms that swim (powerfully) in the water), and neuston (the organisms that float on the water).\n\n\n", "id": "60359", "title": "Benthos"}
{"url": "https://en.wikipedia.org/wiki?curid=26537401", "text": "Anthropophilia\n\nIn parasitology, anthropophilia, from the Greek ἅνθρωπος (anthrōpos, \"human being\") and φιλία (philia, \"friendship\" or \"love\"), is a preference of a parasite or dermatophyte for humans over other animals. The related term endophilia refers specifically to a preference for being in human habitats, especially inside dwellings. The term \"zoophilia\", in this context, describes animals which prefer non-human animals for nourishment.\n\nMost usage of the term \"anthropophilia\" refers to hematophagous insects (see \"Anopheles\") that prefer human blood over animal blood (zoophily, but see other meanings of zoophily). Examples other than haematophagy include geckoes that live close to humans, pied crows (\"Corvus albus\"), cockroaches, and many others. In the study of malaria and its disease vectors, researchers make the distinction between anthropophilic mosquitoes and other types as part of disease eradiction efforts.\n\nAnthropic organisms are organisms that show anthropophily, where the adjective \"synanthropic \" refers to organisms that live close to human settlements and houses, and eusynathropic to those that live within human housing.\n", "id": "26537401", "title": "Anthropophilia"}
{"url": "https://en.wikipedia.org/wiki?curid=21168829", "text": "Astroecology\n\nAstroecology concerns the interactions of biota with space environments. It studies resources for life on planets, asteroids and comets, around various stars, in galaxies, and in the universe. The results allow estimating the future prospects for life, from planetary to galactic and cosmological scales.\n\nAvailable energy, and microgravity, radiation, pressure and temperature are physical factors that affect astroecology. The ways by which life can reach space environments, including natural panspermia and directed panspermia are also considered. Further, for human expansion in space and directed panspermia, motivation by life-centered biotic ethics, panbiotic ethics and planetary bioethics are also relevant.\n\nThe term \"astroecology\" was first applied in the context of performing studies in actual meteorites to evaluate their potential resources favorable to sustaining life. Early results showed that meteorite/asteroid materials can support microorganisms, algae and plant cultures under Earth's atmosphere and supplemented with water.\n\nSeveral observations suggest that diverse planetary materials, similar to meteorites collected on Earth, could be used as agricultural soils, as they provide nutrients to support microscopic life when supplemented with water and an atmosphere. Experimental astroecology has been proposed to rate planetary materials as targets for astrobiology exploration and as potential biological in-situ resources. The biological fertilities of planetary materials can be assessed by measuring water-extractable electrolyte nutrients. The results suggest that carbonaceous asteroids and Martian basalts can serve as potential future resources for substantial biological populations in the Solar System.\n\nAnalysis of the essential nutrients (C, N, P, K) in meteorites yielded information for calculating the amount of biomass that can be constructed from asteroid resources. For example, carbonaceous asteroids are estimated to contain about 10 kg potential resource materials, and laboratory results suggest that they could yield a biomass on the order of 6·10 kg, about 100,000 times more than biological matter presently on Earth.\n\nTo quantify the potential amounts of life in biospheres, theoretical astroecology attempts to estimate the amount of biomass over the duration of a biosphere. The resources, and the potential time-integrated biomass were estimated for planetary systems, for habitable zones around stars, and for the galaxy and the universe. Such astroecology calculations suggest that the limiting elements nitrogen and phosphorus in the estimated 10 kg carbonaceous asteroids could support 6·10 kg biomass for the expected five billion future years of the Sun, yielding a future time-integrated \"BIOTA\" (\"BIOTA\", Biomass Integrated Over Times Available, measured in kilogram-years) of 3·10 kg-years in the Solar System, a hundred thousand times more than life on Earth to date. Considering biological requirements of 100 W kg biomass, radiated energy about red giant stars and white and red dwarf stars could support a time-integrated \"BIOTA\" up to 10 kg-years in the galaxy and 10 kg-years in the universe.\n\nSuch astroecology considerations quantify the immense potentials of future life in space, with commensurate biodiversity and possibly, intelligence. Chemical analysis of carbonaceous chondrite meteorites show that they contain extractable bioavailable water, organic carbon, and essential phosphate, nitrate and potassium nutrients. The results allow assessing the soil fertilities of the parent asteroids and planets, and the amounts of biomass that they can sustain.\n\nLaboratory experiments showed that the Murchison meteorite can support a variety of organisms including bacteria (Nocardia asteroides), algae, and plant cultures such as potato and asparagus. The microorganisms used organics in the carbonaceous meteorites as the carbon source. Algae and plant cultures grew well also on Mars meteorites because of their high bio-available phosphate contents. The Martian materials achieved soil fertility ratings comparable to productive agricultural soils. This offers some data relating to terraforming of Mars.\n\nTerrestrial analogues of planetary materials are also used in such experiments for comparison, and to test the effects of space conditions on microorganisms.\n\nThe biomass that can be constructed from resources can be calculated by comparing the concentration of elements in the resource materials and in biomass (Equation 1). A given mass of resource materials (\"m\") can support \"m\" of biomass containing element \"X\" (considering \"X\" as the limiting nutrient), where \"c\" is the concentration (mass per unit mass) of element \"X\" in the resource material and \"c\" is its concentration in the biomass.\n\nformula_1 (1)\nAssuming that 100,000 kg biomass supports one human, the asteroids may then sustain about 6e15 (six million billion) people, equal to a million Earths (a million times the present population). Similar materials in the comets could support biomass and populations about one hundred times larger. Solar energy can sustain these populations for the predicted further five billion years of the Sun. These considerations yield a maximum time-integrated \"BIOTA\" of 3e30 kg-years in the Solar System. After the Sun becomes a white dwarf star, and other white dwarf stars, can provide energy for life much longer, for trillions of eons. (Table 2)\n\nAstroecology also concerns wastage, such as the leakage of biological matter into space. This will cause an exponential decay of space-based biomass as given by Equation (2), where M (biomass 0) is the mass of the original biomass, \"k\" is its rate of decay (the fraction lost in a unit time) and \"biomass t\" is the remaining biomass after time \"t\". \n\nIntegration from time zero to infinity yields Equation (3) for the total time-integrated biomass (\"BIOTA\") contributed by this biomass:\n\nFor example, if 0.01% of the biomass is lost per year, then the time-integrated \"BIOTA\" will be 10,000formula_4. For the 6·10 kg biomass constructed from asteroid resources, this yields 6·10 kg-years of \"BIOTA\" in the Solar System. Even with this small rate of loss, life in the Solar System would disappear in a few hundred thousand years, and the potential total time-integrated \"BIOTA\" of 3·10 kg-years under the main-sequence Sun would decrease by a factor of 5·10, although a still substantial population of 1.2·10 biomass-supported humans could exist through the habitable lifespan of the Sun. \nThe integrated biomass can be maximized by minimizing its rate of dissipation. If this rate can be reduced sufficiently, all the constructed biomass can last for the duration of the habitat and it pays to construct the biomass as fast as possible. However, if the rate of dissipation is significant, the construction rate of the biomass and its steady-state amounts may be reduced allowing a steady-state biomass and population that lasts throughout the lifetime of the habitat.\n\nAn issue that arises is whether we should build immense amounts of life that decays fast, or smaller, but still large, populations that last longer. Life-centered biotic ethics suggests that life should last as long as possible.\n\nIf life reaches galactic proportions, technology should be able to access all of the materials resources, and sustainable life will be defined by the available energy. The maximum amount of biomass about any star is then determined by the energy requirements of the biomass and by the luminosity of the star. For example, if 1 kg biomass needs 100 Watts, we can calculate the steady-state amounts of biomass that can be sustained by stars with various energy outputs. These amounts are multiplied by the life-time of the star to calculate the time-integrated \"BIOTA\" over the life-time of the star. Using similar projections, the potential amounts of future life can then be quantified.\n\nFor the Solar System from its origins to the present, the current 10 kg biomass over the past four billion years gives a time-integrated biomass (\"BIOTA\") of 4·10 kg-years. In comparison, carbon, nitrogen, phosphorus and water in the 10 kg asteroids allows 6·10 kg biomass that can be sustained with energy for the 5 billion future years of the Sun, giving a \"BIOTA\" of 3·10 kg-years in the Solar System and 3·10 kg-years about 10 stars in the galaxy. Materials in comets could give biomass and time-integrated \"BIOTA\" a hundred times larger. \n\nThe Sun will then become a white dwarf star, radiating 10 Watts that sustains 1e13 kg biomass for an immense hundred million trillion (10) years, contributing a time-integrated \"BIOTA\" of 10 years. The 10 white dwarfs that may exist in the galaxy during this time can then contribute a time-integrated \"BIOTA\" of 10 kg-years. Red dwarf stars with luminosities of 10 Watts and life-times of 10 years can contribute 10 kg-years each, and 10 red dwarfs can contribute 10 kg-years, while brown dwarfs can contribute 10 kg-years of time-integrated biomass (\"BIOTA\") in the galaxy. In total, the energy output of stars during 10 years can sustain a time-integrated biomass of about 10 kg-years in the galaxy. This is one billion trillion (10) times more life than has existed on the Earth to date. In the universe, stars in 10 galaxies could then sustain 10 kg-years of life.\n\nThe astroecology results above suggest that humans can expand life in the galaxy through space travel or directed panspermia. The amounts of possible life that can be established in the galaxy, as projected by astroecology, are immense. These projections are based on information about 15 billion past years since the Big Bang, but the habitable future is much longer, spanning trillions of eons. Therefore, physics, astroeclogy resources, and some cosmological scenarios may allow organized life to last, albeit at an ever slowing rate, indefinitely. These prospects may be addressed by the long-term extension of astroecology as cosmoecology.\n\n\n", "id": "21168829", "title": "Astroecology"}
{"url": "https://en.wikipedia.org/wiki?curid=36911458", "text": "Flux (biology)\n\nIn general, \"flux\" in biology relates to movement of a substance between compartments. There are several cases where the concept of flux is important.\n\nFlux is the net movement of particles across a specified area in a specified period of time. The particles may be ions or molecules, or they may be larger, like insects, muskrats or cars. The units of time can be anything from milliseconds to millennia. Flux is not the same as velocity or speed nor is it the same as density or concentration. Movement itself is not enough. \n", "id": "36911458", "title": "Flux (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=7062509", "text": "Biosignal\n\nA biosignal is any signal in living beings that can be continually measured and monitored. The term biosignal is often used to refer to bioelectrical signals, but it may refer to both electrical and non-electrical signals. The usual understanding is to refer only to time-varying signals, although spatial parameter variations (e.g. the nucleotide sequence determining the genetic code) are sometimes subsumed as well.\n\nElectrical biosignals, or bioelectrical time signals, usually refers to the change in electric current produced by the sum of an electrical potential difference across a specialized tissue, organ or cell system like the nervous system. Thus, among the best-known bioelectrical signals are:\n\nEEG, ECG, EOG and EMG are measured with a differential amplifier which registers the difference between two electrodes attached to the skin. However, the galvanic skin response measures electrical resistance and the MEG measures the magnetic field induced by electrical currents (electroencephalogram) of the brain.\n\nWith the development of methods for remote measurement of electric fields using new sensor technology, electric biosignals such as EEG and ECG can be measured without electric contact with the skin. This can be applied for example for remote monitoring of brain waves and heart beat of patients who must not be touched, in particular patients with serious burns.\n\nElectrical currents and changes in electrical resistances across tissues can also be measured from plants.\n\nBiosignals may also refer to any non-electrical signal that is capable of being monitored from biological beings, such as mechanical signals (e.g. the mechanomyogram or MMG), acoustic signals (e.g. phonetic and non-phonetic utterances, breathing), chemical signals (e.g. pH, oxygenation) and optical signals (e.g. movements).\n\nIn recent years, the use of biosignals has gained interest amongst an international artistic community of performers and composers who use biosignals to produce and control sound. Research and practice in the field go back decades in various forms and have lately been enjoying a resurgence, thanks to the increasing availability of more affordable and less cumbersome technologies. An entire issue of \"eContact!\", published by the Canadian Electroacoustic Community in July 2012, was dedicated to this subject, with contributions from the key figures in the domain.\n\n\n\n", "id": "7062509", "title": "Biosignal"}
{"url": "https://en.wikipedia.org/wiki?curid=38797174", "text": "Tropical vegetation\n\nTropical vegetation is any vegetation in tropical latitudes. Plant life that occurs in climates that are warm year-round is in general more biologically diverse that in other latitudes. Some tropical areas may receive abundant rain the whole year round, but others have long dry seasons which last several months and may vary in length and intensity with geographic location. These seasonal droughts have great impact on the vegetation, such as in the Madagascar spiny forests.\n\nPlant species native to the tropics found in tropical ecosystems are known as tropical plants. Some examples of tropical ecosystem are the Guinean Forests of West Africa, the Madagascar dry deciduous forests and the broadleaf forests of the Thai highlands and the El Yunque National Forest in Puerto Rico.\n\nThe term 'Tropical vegetation' is frequently used in the sense of lush and luxuriant, but not all the vegetation of the areas of the Earth under a tropical climate could be adequately defined as such. Despite their lush appearance, often the soils of tropical forests are low in nutrient content making them quite vulnerable to slash-and-burn deforestation techniques, which are sometimes an element of shifting cultivation agricultural systems.\nTropical vegetation may include the following habitat types:\nTropical rainforest ecosystems include significant areas of biodiversity, often coupled with high species endemism. Rainforests are home to half of all the living animal and plant species on the planet and roughly two-thirds of all flowering plants can be found in rainforests. The most representative are the Borneo rainforest, one of the oldest rainforests in the world, the Brazilian and Venezuelan Amazon Rainforest, as well as the eastern Costa Rican rainforests. \nSeasonal tropical forests generally receive high total rainfall, averaging more than 1000 mm per year, but with a distinct dry season. They include: the Congolian forests, a broad belt of highland tropical moist broadleaf forest which extends across the basin of the Congo River; Central American tropical forests in Panama and Nicaragua; the seasonal forests that predominate across much the Indian subcontinent, Indochina and in northern Australia: Queensland.\n\nTropical dry broadleaf forests are territories with a forest cover that is not very dense and has often an unkempt, irregular appearance, especially in the dry season. This type of forests often include bamboo and teak as the dominant large tree species, such as in the Phi Pan Nam Range, part of the Central Indochina dry forests. They are affected by often very long and intense seasonal dry periods and, though less biologically diverse than rainforests, tropical dry forests are home to a wide variety of wildlife.\n\nTropical grasslands, savannas, and shrublands are spread over a large area of the tropics with a vegetation made up mainly of low shrubs and grasses, often including sclerophyll species. Some of the most representative are the Western Zambezian grasslands in Zambia and Angola, as well as the Einasleigh upland savanna in Australia. Tree species such as Acacia and baobab may be present in these ecosystems depending from the region.\n\n\n\n", "id": "38797174", "title": "Tropical vegetation"}
{"url": "https://en.wikipedia.org/wiki?curid=2424150", "text": "Range (biology)\n\nIn biology, the range or distribution of a species is the geographical area within which that species can be found. Within that range, dispersion is variation in local density.\n\nThe term is often qualified:\n\n\nThere are at least five types of distribution patterns:\n\n\nOne common example of bird species' ranges are land mass areas bordering water bodies, such as oceans, rivers, or lakes; they are called a \"coastal strip\". A second example, some species of bird depend on water, usually a river, swamp, etc., or water related forest and live in a \"river corridor\". A separate example of a river corridor would be a river corridor that includes the entire drainage, having the edge of the range delimited by mountains, or higher elevations; the river itself would be a smaller percentage of this entire wildlife corridor, but the corridor is created because of the river.\n\nA further example of a bird wildlife corridor would be a mountain range corridor. In the U.S. of North America, the Sierra Nevada range in the west, and the Appalachian Mountains in the east are two examples of this habitat, used in summer, and winter, by separate species, for different reasons.\n\nBird species in these corridors are connected to a main range for the species (contiguous) or are in an isolated geographic range and be a disjunct range. Birds leaving the area, if they migrate, would leave connected to the main range or have to fly over land not connected to the wildlife corridor; thus, they would be passage migrants over land that they stop on for an intermittent, hit or miss, visit.\n\n", "id": "2424150", "title": "Range (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=2452933", "text": "Vagrancy (biology)\n\nVagrancy is a phenomenon in biology whereby individual animals appear well outside their normal range; individual animals which exhibit vagrancy are known as vagrants. The term accidental is sometimes also used. There are a number of factors which might cause an individual to become a vagrant — genetic factors and weather conditions are two — but the causes are overall poorly understood. Vagrancy can be a precursor to colonisation if individuals survive.\n\nVagrancy is known to occur in a wide range of animals such as birds, insects, mammals and sea turtles.\n\nIn the Northern Hemisphere, adult birds (possibly inexperienced younger adults) of many species are known to continue past their normal breeding range during their spring migration and end up in areas further north (such birds are termed spring overshoots.)\n\nIn autumn, some young birds, instead of heading to their usual wintering grounds, take \"incorrect\" courses and migrate through areas which are not on their normal migration path. For example, Siberian passerines which normally winter in Southeast Asia are commonly found in Northwest Europe, e.g. Arctic warblers in Britain. This is reverse migration, where the birds migrate in the opposite direction to that expected (say, flying north-west instead of south-east). The causes of this are unknown, but genetic mutation or other anomalies relating to the bird's magnetic sensibilities is suspected.\n\nOther birds are sent off course by storms, such as North American birds, blown across the Atlantic Ocean to Europe. Birds can also be blown out to sea, become physically exhausted, land on a ship and end up being carried across the ocean.\n\nVagrancy in insects is recorded from many groups — it is particularly well-studied in butterflies and moths, and dragonflies.\n\nIn mammals, vagrancy has been recorded for bats, pinniped seals, whales and manatees.\n\nVagrancy has been recorded for sea turtles, snakes (e.g. \"Pelamis platura\"), crocodilians, and probably also occurs in lizards. It therefore seems to be a fairly widespread phenomenon in reptiles. Saltwater crocodiles are especially prone to vagrancy, with individuals occasionally being recorded in odd places including Fiji, Iwo Jima, and even the Sea of Japan.\n\nThe term vagrant is also used of plants (e.g. Gleason and Cronquist, 1991), to refer to a plant that is growing far away from its species' usual range (especially north of its range) with the connotation of being a temporary population. In the context of lichens, a vagrant form or species occurs unattached to a substrate (\"loose\"), not necessarily outside its range.\n", "id": "2452933", "title": "Vagrancy (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=6848750", "text": "Molluscivore\n\nA molluscivore is a carnivorous animal that specialises in feeding on molluscs such as snails, bivalves, brachiopods and cephalopods. Known molluscivores include numerous predatory (and often cannibalistic) molluscs, (e.g.octopuses, murexes, decollate snails and oyster drills), arthropods such as crabs and firefly larvae, and, vertebrates such as fish, birds and mammals. Molluscivory is performed in a variety ways with some animals highly adapted to this method of feeding behaviour. A similar behaviour, durophagy, describes the feeding of animals that consume hard-shelled or exoskeleton bearing organisms, such as corals, shelled molluscs, or crabs.\n\nMolluscivory is performed in several ways.\n\nIn some cases, the mollusc prey are simply swallowed entire, including the shell, whereupon the prey is killed through suffocation and or exposure to digestive enzymes. Only cannibalistic sea slugs, snail-eating cone shells of the taxon Coninae, and some sea anemones use this method.\n\nOne method, used especially by vertebrate molluscivores, is to break the shell, either by exerting force on the shell until it breaks, often by biting the shell, like with oyster crackers, mosasaurs, and placodonts, or hammering at the shell, e.g. oystercatchers and crabs, or by simply dashing the mollusc on a rock (e.g. song thrushes, gulls, and sea otters). \nAnother method is to remove the shell from the prey. Molluscs are attached to their shell by strong muscular ligaments, making the shell's removal difficult. Molluscivorous birds, such as oystercatchers and the Everglades snail kite, insert their elongate beak into the shell to sever these attachment ligaments, facilitating removal of the prey. The carnivorous terrestrial pulmonate snail known as the \"decollate snail\" (\"decollate\" being a synonym for \"decapitate\") uses a similar method: it reaches into the opening of the prey's shell and bites through the muscles in the prey's neck, whereupon it immediately begins devouring the fleshy parts of its victim. The walrus sucks meat out of bivalve molluscs by sealing its powerful lips to the organism and withdrawing its piston-like tongue rapidly into its mouth, creating a vacuum.\n\nAnother method is used by many molluscs, themselves. Octopi, nautilii, and most molluscivoruous snails use their radula to drill a hole through the shell, then inject venom and digestive enzymes through the hole, whereupon the digested prey is then sucked out through the hole.\nThe larvae of glowworms and fireflies are simply small enough to enter the shells of terrestrial snails and begin eating immediately.\n\nWhales: Sperm whales, pilot whales, Cuvier's beaked whale, Risso's dolphin and species in the genera \"Mesoplodon\", and \"Hyperoodon\" and the superfamily Physeteroidea are classified as molluscivores, eating mainly squid.\n\nPinnipeds: Elephant seals, Ross seals and South American fur seals are classed as molluscivores. The walrus eats benthic bivalve molluscs, especially clams, for which it forages by grazing along the sea bottom, searching and identifying prey with its sensitive vibrissae. The walrus sucks the meat out by sealing its powerful lips to the organism and withdrawing its piston-like tongue rapidly into its mouth, creating a vacuum. The walrus palate is uniquely vaulted, enabling effective suction.\n\nSeveral species of pufferfish and loaches are molluscivores. As many molluscs are protected by a shell, the feeding techniques applied amongst molluscivore fish are highly specialized and usually divided into two groups: \"crushers\" and \"slurpers.\" Pufferfish tend to be crushers and will use their beak-like teeth to break the shell in order to gain access to the meat inside. Loaches are specialized slurpers, and will make use of their characteristically shaped snout in order to grab hold of the animal living inside the shell.\n\nThe black carp (\"Mylopharyngodon piceus\") commonly feeds by crushing large molluscs with pharyngeal teeth, extracting soft tissue, and spitting out shell fragments. Four-year-old juveniles are capable of consuming approximately 1–2 kg of molluscs per day. This bottom-dwelling molluscivore was purposely imported into the United States in the early 1970s for use as a food fish and also as a biological control agent for snails—an intermediate host for a trematode parasite in fish reared on fish farms. Two snail-eating cichlids, \"Trematocranus placodon\" and \"Maravichromis anaphyrmis\", have been tried as biological control agents of schistosomes in fish ponds in Africa. Redear sunfish (\"Lepomis micropholus\") and bluegill (\"Lepomis macrochirus\") have been used to control quagga mussels (\"Dreissena bugensis\") in the lower Colorado River in the US.\nThe common name of some fish reflects their molluscivorous feeding, for example, the \"snail-crusher hap\" (\"Trematocranus placodon\"), \"\"red rock sheller\" (\"Haplochromis sp.\"), \"Rusinga oral sheller\" (\"Haplochromis sp.\") and \"rainbow sheller\" (\"Haplochromis sp.\"). The redear sunfish is also known as the \"shellcracker\".\n\nGray's monitor (or \"butaan\") is well known for its diet, which consists primarily of ripe fruit; however, several prey items are also consumed, including snails.\n\nThe prehistoric placodont reptiles is an extinct taxon of marine animals that superficially resembled lizards and turtles, most of whose dentition of peg-like incisors and enormous, molar-like teeth allowed them to prey on molluscs and brachiopods by plucking their prey off of the substrate, and crushing the shells.\n\nAmong birds, the eponymous shorebirds known as oystercatchers are renowned for feeding upon bivalves. At least one bird of prey is also primarily a molluscivore—the snail kite, \"Rostrhamus sociabilis\". The limpkin is a small rail-like bird that feeds almost entirely on apple snails. Other birds that will eat molluscs occasionally include mergansers, ducks, coots, dippers and spoonbills.\n\nCone snails: Some cone snails hunt and eat other kinds of snails, such as cowries, olive shells, turbo snails, and conch snails, while others will eat other cone snails. \"Conus marmoreus\" and \"Conus omaria\" are able to kill and swallow prey that are larger than themselves; some Conus species can swallow prey that weigh up to half of their own weight. Snail's bodies are attached to their shell by a columellar muscle that holds onto the columella, the axis of the snail. This muscle also allows the snail to retract back into its shell. If this muscle is broken, the snail will lose its shell and die. It is hard to detach this muscle in a live snail, or even in a dead snail. It is thought that the conotoxins in the venom of cone snails are able to completely relax this muscle so that the body can be pulled out from its shell. The cone snail uses its foot to hold the shell of its prey. Using a strong, steady pulling motion, the body of the snail can be forced out and then swallowed whole. Complete digestion of a snail can take many hours, even days.\n\nStarfish: Primitive starfish, such as \"Astropecten\" and \"Luidia\", swallow their prey whole and start to digest it in their cardiac stomachs. Shell valves and other inedible materials are ejected through their mouths. The semi-digested fluid is passed into their pyloric stomachs and caeca where digestion continues and absorption occurs. The margined sea star (\"Astropecten articulatus\") is a well known molluscivore. It catches prey with its arms which it then takes to the mouth. The prey is then trapped by the long, moving prickles around the mouth cavity and swallowed food.\n\nIn more advanced species of starfish, the cardiac stomach can be everted from the organism's body to engulf and digest food. When the prey is a clam, the starfish pulls with its tube feet to separate the two valves slightly, and inserts a small section of its stomach, which releases enzymes to digest the prey. The stomach and the partially digested prey are later retracted into the disc. Here the food is passed on to the pyloric stomach, which always remains inside the disc. Because of this ability to digest food outside the body, starfish can hunt prey much larger than their mouths.\n\nCrabs: The freshwater crabs \"Syntripsa matannensis\" and \"Syntripsa flavichela\" are classed as molluscivores. Using their massive and powerful claws, adult Florida stone crabs (\"Menippe mercenaria\") feed on acorn barnacles, hard-shelled clams, scallops, and conch.\n", "id": "6848750", "title": "Molluscivore"}
{"url": "https://en.wikipedia.org/wiki?curid=216216", "text": "Conservation biology\n\nConservation biology is the management of nature and of Earth's biodiversity with the aim of protecting species, their habitats, and ecosystems from excessive rates of extinction and the erosion of biotic interactions. It is an interdisciplinary subject drawing on natural and social sciences, and the practice of natural resource management.\n\nThe conservation ethic is based on the findings of conservation biology.\n\nThe term conservation biology and its conception as a new field originated with the convening of \"The First International Conference on Research in Conservation Biology\" held at the University of California, San Diego in La Jolla, California in 1978 led by American biologists Bruce A. Wilcox and Michael E. Soulé with a group of leading university and zoo researchers and conservationists including Kurt Benirschke, Sir Otto Frankel, Thomas Lovejoy, and Jared Diamond. The meeting was prompted by the concern over tropical deforestation, disappearing species, eroding genetic diversity within species. The conference and proceedings that resulted sought to initiate the bridging of a gap between theory in ecology and evolutionary genetics on the one hand and conservation policy and practice on the other. Conservation biology and the concept of biological diversity (biodiversity) emerged together, helping crystallize the modern era of conservation science and policy. The inherent multidisciplinary basis for conservation biology has led to new subdisciplines including conservation social science, conservation behavior and conservation physiology. It stimulated further development of conservation genetics which Otto Frankel had originated first but is now often considered a subdiscipline as well.\n\nThe rapid decline of established biological systems around the world means that conservation biology is often referred to as a \"Discipline with a deadline\". Conservation biology is tied closely to ecology in researching the population ecology (dispersal, migration, demographics, effective population size, inbreeding depression, and minimum population viability) of rare or endangered species. Conservation biology is concerned with phenomena that affect the maintenance, loss, and restoration of biodiversity and the science of sustaining evolutionary processes that engender genetic, population, species, and ecosystem diversity. The concern stems from estimates suggesting that up to 50% of all species on the planet will disappear within the next 50 years, which has contributed to poverty, starvation, and will reset the course of evolution on this planet.\n\nConservation biologists research and educate on the trends and process of biodiversity loss, species extinctions, and the negative effect these are having on our capabilities to sustain the well-being of human society. Conservation biologists work in the field and office, in government, universities, non-profit organizations and industry. The topics of their research are diverse, because this is an interdisciplinary network with professional alliances in the biological as well as social sciences. Those dedicated to the cause and profession advocate for a global response to the current biodiversity crisis based on morals, ethics, and scientific reason. Organizations and citizens are responding to the biodiversity crisis through conservation action plans that direct research, monitoring, and education programs that engage concerns at local through global scales.\n\nConscious efforts to conserve and protect \"global\" biodiversity are a recent phenomenon. Natural resource conservation, however, has a history that extends prior to the age of conservation. Resource ethics grew out of necessity through direct relations with nature. Regulation or communal restraint became necessary to prevent selfish motives from taking more than could be locally sustained, therefore compromising the long-term supply for the rest of the community. This social dilemma with respect to natural resource management is often called the \"Tragedy of the Commons\".\n\nFrom this principle, conservation biologists can trace communal resource based ethics throughout cultures as a solution to communal resource conflict. For example, the Alaskan Tlingit peoples and the Haida of the Pacific Northwest had resource boundaries, rules, and restrictions among clans with respect to the fishing of sockeye salmon. These rules were guided by clan elders who knew lifelong details of each river and stream they managed. There are numerous examples in history where cultures have followed rules, rituals, and organized practice with respect to communal natural resource management.\n\nThe Mauryan emperor Ashoka around 250 B.C. issued edicts restricting the slaughter of animals and certain kinds of birds, as well as opened veterinary clinics.\n\nConservation ethics are also found in early religious and philosophical writings. There are examples in the Tao, Shinto, Hindu, Islamic and Buddhist traditions. In Greek philosophy, Plato lamented about pasture land degradation: \"What is left now is, so to say, the skeleton of a body wasted by disease; the rich, soft soil has been carried off and only the bare framework of the district left.\" In the bible, through Moses, God commanded to let the land rest from cultivation every seventh year. Before the 18th century, however, much of European culture considered it a pagan view to admire nature. Wilderness was denigrated while agricultural development was praised. However, as early as AD 680 a wildlife sanctuary was founded on the Farne Islands by St Cuthbert in response to his religious beliefs.\n\nNatural history was a major preoccupation in the 18th century, with grand expeditions and the opening of popular public displays in Europe and North America. By 1900 there were 150 natural history museums in Germany, 250 in Great Britain, 250 in the United States, and 300 in France. Preservationist or conservationist sentiments are a development of the late 18th to early 20th centuries.\n\nBefore Charles Darwin set sail on the HMS Beagle, most people in the world, including Darwin, believed in special creation and that all species were unchanged. George-Louis Leclerc was one of the first naturalist that questioned this belief. He proposed in his 44 volume natural history book that species evolve due to environmental influences. Erasmus Darwin was also a naturalist who also suggested that species evolved. Erasmus Darwin noted that some species have vestigial structures which are anatomical structures that have no apparent function in the species currently but would have been useful for the species' ancestors. The thinking of these early 18th century naturalist helped to change the mindset and thinking of the early 19th century naturalist.\n\nBy the early 19th century biogeography was ignited through the efforts of Alexander von Humboldt, Charles Lyell and Charles Darwin. The 19th-century fascination with natural history engendered a fervor to be the first to collect rare specimens with the goal of doing so before they became extinct by other such collectors. Although the work of many 18th and 19th century naturalists were to inspire nature enthusiasts and conservation organizations, their writings, by modern standards, showed insensitivity towards conservation as they would kill hundreds of specimens for their collections.\n\nThe modern roots of conservation biology can be found in the late 18th-century Enlightenment period particularly in England and Scotland. A number of thinkers, among them notably Lord Monboddo, described the importance of \"preserving nature\"; much of this early emphasis had its origins in Christian theology.\n\nScientific conservation principles were first practically applied to the forests of British India. The conservation ethic that began to evolve included three core principles: that human activity damaged the environment, that there was a civic duty to maintain the environment for future generations, and that scientific, empirically based methods should be applied to ensure this duty was carried out. Sir James Ranald Martin was prominent in promoting this ideology, publishing many medico-topographical reports that demonstrated the scale of damage wrought through large-scale deforestation and desiccation, and lobbying extensively for the institutionalization of forest conservation activities in British India through the establishment of Forest Departments.\n\nThe Madras Board of Revenue started local conservation efforts in 1842, headed by Alexander Gibson, a professional botanist who systematically adopted a forest conservation program based on scientific principles. This was the first case of state conservation management of forests in the world. Governor-General Lord Dalhousie introduced the first permanent and large-scale forest conservation program in the world in 1855, a model that soon spread to other colonies, as well the United States, where Yellowstone National Park was opened in 1872 as the world's first national park.\n\nThe term \"conservation\" came into widespread use in the late 19th century and referred to the management, mainly for economic reasons, of such natural resources as timber, fish, game, topsoil, pastureland, and minerals. In addition it referred to the preservation of forests (forestry), wildlife (wildlife refuge), parkland, wilderness, and watersheds. This period also saw the passage of the first conservation legislation and the establishment of the first nature conservation societies. The Sea Birds Preservation Act of 1869 was passed in Britain as the first nature protection law in the world after extensive lobbying from the Association for the Protection of Seabirds and the respected ornithologist Alfred Newton. Newton was also instrumental in the passage of the first Game laws from 1872, which protected animals during their breeding season so as to prevent the stock from being brought close to extinction.\n\nOne of the first conservation societies was the Royal Society for the Protection of Birds, founded in 1889 in Manchester as a protest group campaigning against the use of great crested grebe and kittiwake skins and feathers in fur clothing. Originally known as \"the Plumage League\", the group gained popularity and eventually amalgamated with the Fur and Feather League in Croydon, and formed the RSPB. The National Trust formed in 1895 with the manifesto to \"...promote the permanent preservation, for the benefit of the nation, of lands, ...to preserve (so far practicable) their natural aspect.\"\n\nIn the United States, the Forest Reserve Act of 1891 gave the President power to set aside forest reserves from the land in the public domain. John Muir founded the Sierra Club in 1892, and the New York Zoological Society was set up in 1895. A series of national forests and preserves were established by Theodore Roosevelt from 1901 to 1909. The 1916 National Parks Act, included a 'use without impairment' clause, sought by John Muir, which eventually resulted in the removal of a proposal to build a dam in Dinosaur National Monument in 1959.\n\nIn the 20th century, Canadian civil servants, including Charles Gordon Hewitt and James Harkin spearheaded the movement toward wildlife conservation.\n\nIn the mid-20th century, efforts arose to target individual species for conservation, notably efforts in big cat conservation in South America led by the New York Zoological Society. In the early 20th century the New York Zoological Society was instrumental in developing concepts of establishing preserves for particular species and conducting the necessary conservation studies to determine the suitability of locations that are most appropriate as conservation priorities; the work of Henry Fairfield Osborn Jr., Carl E. Akeley, Archie Carr and his son Archie Carr III is notable in this era. Akeley for example, having led expeditions to the Virunga Mountains and observed the mountain gorilla in the wild, became convinced that the species and the area were conservation priorities. He was instrumental in persuading Albert I of Belgium to act in defense of the mountain gorilla and establish Albert National Park (since renamed Virunga National Park) in what is now Democratic Republic of Congo.\n\nBy the 1970s, led primarily by work in the United States under the Endangered Species Act along with the Species at Risk Act (SARA) of Canada, Biodiversity Action Plans developed in Australia, Sweden, the United Kingdom, hundreds of species specific protection plans ensued. Notably the United Nations acted to conserve sites of outstanding cultural or natural importance to the common heritage of mankind. The programme was adopted by the General Conference of UNESCO in 1972. As of 2006, a total of 830 sites are listed: 644 cultural, 162 natural. The first country to pursue aggressive biological conservation through national legislation was the United States, which passed back to back legislation in the Endangered Species Act (1966) and National Environmental Policy Act (1970), which together injected major funding and protection measures to large-scale habitat protection and threatened species research. Other conservation developments, however, have taken hold throughout the world. India, for example, passed the Wildlife Protection Act of 1972.\n\nIn 1980, a significant development was the emergence of the urban conservation movement. A local organization was established in Birmingham, UK, a development followed in rapid succession in cities across the UK, then overseas. Although perceived as a grassroots movement, its early development was driven by academic research into urban wildlife. Initially perceived as radical, the movement's view of conservation being inextricably linked with other human activity has now become mainstream in conservation thought. Considerable research effort is now directed at urban conservation biology. The Society for Conservation Biology originated in 1985.\n\nBy 1992, most of the countries of the world had become committed to the principles of conservation of biological diversity with the Convention on Biological Diversity; subsequently many countries began programmes of Biodiversity Action Plans to identify and conserve threatened species within their borders, as well as protect associated habitats. The late 1990s saw increasing professionalism in the sector, with the maturing of organisations such as the Institute of Ecology and Environmental Management and the Society for the Environment.\n\nSince 2000, the concept of landscape scale conservation has risen to prominence, with less emphasis being given to single-species or even single-habitat focused actions. Instead an ecosystem approach is advocated by most mainstream conservationists, although concerns have been expressed by those working to protect some high-profile species.\n\nEcology has clarified the workings of the biosphere; i.e., the complex interrelationships among humans, other species, and the physical environment. The burgeoning human population and associated agriculture, industry, and the ensuing pollution, have demonstrated how easily ecological relationships can be disrupted.\n\nExtinction rates are measured in a variety of ways. Conservation biologists measure and apply statistical measures of fossil records, rates of habitat loss, and a multitude of other variables such as loss of biodiversity as a function of the rate of habitat loss and site occupancy to obtain such estimates. \"The Theory of Island Biogeography\" is possibly the most significant contribution toward the scientific understanding of both the process and how to measure the rate of species extinction. The current background extinction rate is estimated to be one species every few years.\n\nThe measure of ongoing species loss is made more complex by the fact that most of the Earth's species have not been described or evaluated. Estimates vary greatly on how many species actually exist (estimated range: 3,600,000-111,700,000) to how many have received a species binomial (estimated range: 1.5-8 million). Less than 1% of all species that have been described have been studied beyond simply noting its existence. From these figures, the IUCN reports that 23% of vertebrates, 5% of invertebrates and 70% of plants that have been evaluated are designated as endangered or threatened. Better knowledge is being constructed by The Plant List for actual numbers of species.\n\nSystematic conservation planning is an effective way to seek and identify efficient and effective types of reserve design to capture or sustain the highest priority biodiversity values and to work with communities in support of local ecosystems. Margules and Pressey identify six interlinked stages in the systematic planning approach:\n\n\nConservation biologists regularly prepare detailed conservation plans for grant proposals or to effectively coordinate their plan of action and to identify best management practices (e.g.). Systematic strategies generally employ the services of Geographic Information Systems to assist in the decision making process.\n\nConservation physiology was defined by Steven J. Cooke and colleagues as: 'An integrative scientific discipline applying physiological concepts, tools, and knowledge to characterizing biological diversity and its ecological implications; understanding and predicting how organisms, populations, and ecosystems respond to environmental change and stressors; and solving conservation problems across the broad range of taxa (i.e. including microbes, plants, and animals). Physiology is considered in the broadest possible terms to include functional and mechanistic responses at all scales, and conservation includes the development and refinement of strategies to rebuild populations, restore ecosystems, inform conservation policy, generate decision-support tools, and manage natural resources.' Conservation physiology is particularly relevant to practitioners in that it has the potential to generate cause-and-effect relationships and reveal the factors that contribute to population declines.\n\nThe Society for Conservation Biology is a global community of conservation professionals dedicated to advancing the science and practice of conserving biodiversity. Conservation biology as a discipline reaches beyond biology, into subjects such as philosophy, law, economics, humanities, arts, anthropology, and education. Within biology, conservation genetics and evolution are immense fields unto themselves, but these disciplines are of prime importance to the practice and profession of conservation biology.\n\nIs conservation biology an objective science when biologists advocate for an inherent value in nature? Do conservationists introduce bias when they support policies using qualitative description, such as habitat \"degradation\", or \"healthy\" ecosystems? As all scientists hold values, so do conservation biologists. Conservation biologists advocate for reasoned and sensible management of natural resources and do so with a disclosed combination of science, reason, logic, and values in their conservation management plans. This sort of advocacy is similar to the medical profession advocating for healthy lifestyle options, both are beneficial to human well-being yet remain scientific in their approach.\n\nThere is a movement in conservation biology suggesting a new form of leadership is needed to mobilize conservation biology into a more effective discipline that is able to communicate the full scope of the problem to society at large. The movement proposes an adaptive leadership approach that parallels an adaptive management approach. The concept is based on a new philosophy or leadership theory steering away from historical notions of power, authority, and dominance. Adaptive conservation leadership is reflective and more equitable as it applies to any member of society who can mobilize others toward meaningful change using communication techniques that are inspiring, purposeful, and collegial. Adaptive conservation leadership and mentoring programs are being implemented by conservation biologists through organizations such as the Aldo Leopold Leadership Program.\n\nConservation may be classified as either in-situ conservation, which is protecting an endangered species in its natural habitat, or ex-situ conservation, which occurs outside the natural habitat. In-situ conservation involves protecting or restoring the habitat. Ex-situ conservation, on the other hand, involves protection outside of an organism's natural habitat, such as on reservations or in gene banks, in circumstances where viable populations may not be present in the natural habitat.\n\nAlso, non-interference may be used, which is termed a preservationist method. Preservationists advocate for giving areas of nature and species a protected existence that halts interference from the humans. In this regard, conservationists differ from preservationists in the social dimension, as conservation biology engages society and seeks equitable solutions for both society and ecosystems. Some preservationists emphasize the potential of biodiversity in a world without humans.\n\nConservation biologists are interdisciplinary researchers that practice ethics in the biological and social sciences. Chan states that conservationists must advocate for biodiversity and can do so in a scientifically ethical manner by not promoting simultaneous advocacy against other competing values.\n\nA conservationist may be inspired by the \"resource conservation ethic\", which seeks to identify what measures will deliver \"the greatest good for the greatest number of people for the longest time.\" In contrast, some conservation biologists argue that nature has an intrinsic value that is independent of anthropocentric usefulness or utilitarianism. Intrinsic value advocates that a gene, or species, be valued because they have a utility for the ecosystems they sustain. Aldo Leopold was a classical thinker and writer on such conservation ethics whose philosophy, ethics and writings are still valued and revisited by modern conservation biologists.\n\nThe International Union for the Conservation of Nature (IUCN) has organized a global assortment of scientists and research stations across the planet to monitor the changing state of nature in an effort to tackle the extinction crisis. The IUCN provides annual updates on the status of species conservation through its Red List. The IUCN Red List serves as an international conservation tool to identify those species most in need of conservation attention and by providing a global index on the status of biodiversity. More than the dramatic rates of species loss, however, conservation scientists note that the sixth mass extinction is a biodiversity crisis requiring far more action than a priority focus on rare, endemic or endangered species. Concerns for biodiversity loss covers a broader conservation mandate that looks at ecological processes, such as migration, and a holistic examination of biodiversity at levels beyond the species, including genetic, population and ecosystem diversity. Extensive, systematic, and rapid rates of biodiversity loss threatens the sustained well-being of humanity by limiting supply of ecosystem services that are otherwise regenerated by the complex and evolving holistic network of genetic and ecosystem diversity. While the conservation status of species is employed extensively in conservation management, some scientists highlight that it is the common species that are the primary source of exploitation and habitat alteration by humanity. Moreover, common species are often undervalued despite their role as the primary source of ecosystem services.\n\nWhile most in the community of conservation science \"stress the importance\" of sustaining biodiversity, there is debate on how to prioritize genes, species, or ecosystems, which are all components of biodiversity (e.g. Bowen, 1999). While the predominant approach to date has been to focus efforts on endangered species by conserving \"biodiversity hotspots\", some scientists (e.g) and conservation organizations, such as the Nature Conservancy, argue that it is more cost-effective, logical, and socially relevant to invest in \"biodiversity coldspots\". The costs of discovering, naming, and mapping out the distribution of every species, they argue, is an ill-advised conservation venture. They reason it is better to understand the significance of the ecological roles of species.\n\nBiodiversity hotspots and coldspots are a way of recognizing that the spatial concentration of genes, species, and ecosystems is not uniformly distributed on the Earth's surface. For example, \"[...] 44% of all species of vascular plants and 35% of all species in four vertebrate groups are confined to 25 hotspots comprising only 1.4% of the land surface of the Earth.\"\n\nThose arguing in favor of setting priorities for coldspots point out that there are other measures to consider beyond biodiversity. They point out that emphasizing hotspots downplays the importance of the social and ecological connections to vast areas of the Earth's ecosystems where biomass, not biodiversity, reigns supreme. It is estimated that 36% of the Earth's surface, encompassing 38.9% of the worlds vertebrates, lacks the endemic species to qualify as biodiversity hotspot. Moreover, measures show that maximizing protections for biodiversity does not capture ecosystem services any better than targeting randomly chosen regions. Population level biodiversity (i.e. coldspots) are disappearing at a rate that is ten times that at the species level. The level of importance in addressing biomass versus endemism as a concern for conservation biology is highlighted in literature measuring the level of threat to global ecosystem carbon stocks that do not necessarily reside in areas of endemism. A hotspot priority approach would not invest so heavily in places such as steppes, the Serengeti, the Arctic, or taiga. These areas contribute a great abundance of population (not species) level biodiversity and ecosystem services, including cultural value and planetary nutrient cycling.\n\nThose in favor of the hotspot approach point out that species are irreplaceable components of the global ecosystem, they are concentrated in places that are most threatened, and should therefore receive maximal strategic protections. The IUCN Red List categories, which appear on Wikipedia species articles, is an example of the hotspot conservation approach in action; species that are not rare or endemic are listed the least concern and their Wikipedia articles tend to be ranked low on the importance scale. This is a hotspot approach because the priority is set to target species level concerns over population level or biomass. Species richness and genetic biodiversity contributes to and engenders ecosystem stability, ecosystem processes, evolutionary adaptability, and biomass. Both sides agree, however, that conserving biodiversity is necessary to reduce the extinction rate and identify an inherent value in nature; the debate hinges on how to prioritize limited conservation resources in the most cost-effective way.\n\nConservation biologists have started to collaborate with leading global economists to determine how to measure the wealth and services of nature and to make these values apparent in global market transactions. This system of accounting is called \"natural capital\" and would, for example, register the value of an ecosystem before it is cleared to make way for development. The WWF publishes its \"Living Planet Report\" and provides a global index of biodiversity by monitoring approximately 5,000 populations in 1,686 species of vertebrate (mammals, birds, fish, reptiles, and amphibians) and report on the trends in much the same way that the stock market is tracked.\n\nThis method of measuring the global economic benefit of nature has been endorsed by the G8+5 leaders and the European Commission. Nature sustains many ecosystem services that benefit humanity. Many of the earths ecosystem services are public goods without a market and therefore no price or value. When the \"stock market\" registers a financial crisis, traders on Wall Street are not in the business of trading stocks for much of the planet's living natural capital stored in ecosystems. There is no natural stock market with investment portfolios into sea horses, amphibians, insects, and other creatures that provide a sustainable supply of ecosystem services that are valuable to society. The ecological footprint of society has exceeded the bio-regenerative capacity limits of the planet's ecosystems by about 30 percent, which is the same percentage of vertebrate populations that have registered decline from 1970 through 2005.\n\nThe inherent natural economy plays an essential role in sustaining humanity, including the regulation of global atmospheric chemistry, pollinating crops, pest control, cycling soil nutrients, purifying our water supply, supplying medicines and health benefits, and unquantifiable quality of life improvements. There is a relationship, a correlation, between markets and natural capital, and social income inequity and biodiversity loss. This means that there are greater rates of biodiversity loss in places where the inequity of wealth is greatest\n\nAlthough a direct market comparison of natural capital is likely insufficient in terms of human value, one measure of ecosystem services suggests the contribution amounts to trillions of dollars yearly. For example, one segment of North American forests has been assigned an annual value of 250 billion dollars; as another example, honey-bee pollination is estimated to provide between 10 and 18 billion dollars of value yearly. The value of ecosystem services on one New Zealand island has been imputed to be as great as the GDP of that region. This planetary wealth is being lost at an incredible rate as the demands of human society is exceeding the bio-regenerative capacity of the Earth. While biodiversity and ecosystems are resilient, the danger of losing them is that humans cannot recreate many ecosystem functions through technological innovation.\n\nSome species, called a \"keystone species\" form a central supporting hub unique to their the ecosystem. The loss of such a species results in a collapse in ecosystem function, as well as the loss of coexisting species. Keystone species are usually predators due to their ability to control the population of prey in their ecosystem. The importance of a keystone species was shown by the extinction of the Steller's sea cow (\"Hydrodamalis gigas\") through its interaction with sea otters, sea urchins, and kelp. Kelp beds grow and form nurseries in shallow waters to shelter creatures that support the food chain. Sea urchins feed on kelp, while sea otters feed on sea urchins. With the rapid decline of sea otters due to overhunting, sea urchin populations grazed unrestricted on the kelp beds and the ecosystem collapsed. Left unchecked, the urchins destroyed the shallow water kelp communities that supported the Steller's sea cow's diet and hastened their demise. The sea otter was thought to be a keystone species because the coexistence of many ecological associates in the kelp beds relied upon otters for their survival. However this was later questioned by Turvey and Risley, who showed that hunting alone would have driven the Steller's sea cow extinct.\n\nAn \"indicator species\" has a narrow set of ecological requirements, therefore they become useful targets for observing the health of an ecosystem. Some animals, such as amphibians with their semi-permeable skin and linkages to wetlands, have an acute sensitivity to environmental harm and thus may serve as a \"miner's canary\". Indicator species are monitored in an effort to capture environmental degradation through pollution or some other link to proximate human activities. Monitoring an indicator species is a measure to determine if there is a significant environmental impact that can serve to advise or modify practice, such as through different forest silviculture treatments and management scenarios, or to measure the degree of harm that a pesticide may impart on the health of an ecosystem.\n\nGovernment regulators, consultants, or NGOs regularly monitor indicator species, however, there are limitations coupled with many practical considerations that must be followed for the approach to be effective. It is generally recommended that multiple indicators (genes, populations, species, communities, and landscape) be monitored for effective conservation measurement that prevents harm to the complex, and often unpredictable, response from ecosystem dynamics (Noss, 1997).\n\nAn example of an \"umbrella species\" is the monarch butterfly, because of its lengthy migrations and aesthetic value. The monarch migrates across North America, covering multiple ecosystems and so requires a large area to exist. Any protections afforded to the monarch butterfly will at the same time umbrella many other species and habitats. An umbrella species is often used as \"flagship species\", which are species, such as the giant panda, the blue whale, the tiger, the mountain gorilla and the monarch butterfly, that capture the public's attention and attract support for conservation measures. Paradoxically, however, conservation bias towards flagship species sometimes threatens other species of chief concern. \n\nConservation biologists study trends and process from the paleontological past to the ecological present as they gain an understanding of the context related to species extinction. It is generally accepted that there have been five major global mass extinctions that register in Earth's history. These include: the Ordovician (440 mya), Devonian (370 mya), Permian–Triassic (245 mya), Triassic–Jurassic (200 mya), and Cretaceous–Paleogene extinction event (66 mya) extinction spasms. Within the last 10,000 years, human influence over the Earth's ecosystems has been so extensive that scientists have difficulty estimating the number of species lost; that is to say the rates of deforestation, reef destruction, wetland draining and other human acts are proceeding much faster than human assessment of species. The latest \"Living Planet Report\" by the World Wide Fund for Nature estimates that we have exceeded the bio-regenerative capacity of the planet, requiring 1.6 Earths to support the demands placed on our natural resources.\n\nConservation biologists are dealing with and have published evidence from all corners of the planet indicating that humanity may be causing the sixth and fastest planetary extinction event. It has been suggested that we are living in an era of unprecedented numbers of species extinctions, also known as the Holocene extinction event. The global extinction rate may be approximately 1,000 times higher than the natural background extinction rate. It is estimated that two-thirds of all mammal genera and one-half of all mammal species weighing at least have gone extinct in the last 50,000 years. The Global Amphibian Assessment reports that amphibians are declining on a global scale faster than any other vertebrate group, with over 32% of all surviving species being threatened with extinction. The surviving populations are in continual decline in 43% of those that are threatened. Since the mid-1980s the actual rates of extinction have exceeded 211 times rates measured from the fossil record. However, \"The current amphibian extinction rate may range from 25,039 to 45,474 times the background extinction rate for amphibians.\" The global extinction trend occurs in every major vertebrate group that is being monitored. For example, 23% of all mammals and 12% of all birds are Red Listed by the International Union for Conservation of Nature (IUCN), meaning they too are threatened with extinction. Even though extinction is natural, the decline in species is happening at such an incredible rate that evolution can simply not match, therefore, leading to the greatest continual mass extinction on Earth. Humans have dominated the planet and our high consumption of resources, along with the pollution generated is affecting the environments in which other species live. There are a wide variety of species that humans are working to protect such as the Hawaiian Crow and the Whooping Crane of Texas. People can also take action on preserving species by advocating and voting for global and national policies that improve climate, under the concepts of climate mitigation and climate restoration. The Earth's oceans especially require attention, as climate change has altered pH levels making it inhabitable for organisms with shells, that are dissolving as a result.\n\nGlobal assessments of coral reefs of the world continue to report drastic and rapid rates of decline. By 2000, 27% of the world's coral reef ecosystems had effectively collapsed. The largest period of decline occurred in a dramatic \"bleaching\" event in 1998, where approximately 16% of all the coral reefs in the world disappeared in less than a year. \"Coral bleaching\" is caused by a mixture of environmental stresses, including increases in ocean temperatures and acidity, causing both the release of symbiotic algae and death of corals. Decline and extinction risk in coral reef biodiversity has risen dramatically in the past ten years. The loss of coral reefs, which are predicted to go extinct in the next century, threatens the balance of global biodiversity, will have huge economic impacts, and endangers food security for hundreds of millions of people. Conservation biology plays an important role in international agreements covering the world's oceans (and other issues pertaining to biodiversity).\n\nThe oceans are threatened by acidification due to an increase in CO levels. This is a most serious threat to societies relying heavily upon oceanic natural resources. A concern is that the majority of all marine species will not be able to evolve or acclimate in response to the changes in the ocean chemistry.\n\nThe prospects of averting mass extinction seems unlikely when \"[...] 90% of all of the large (average approximately ≥50 kg), open ocean tuna, billfishes, and sharks in the ocean\" are reportedly gone. Given the scientific review of current trends, the ocean is predicted to have few surviving multi-cellular organisms with only microbes left to dominate marine ecosystems.\n\nSerious concerns also being raised about taxonomic groups that do not receive the same degree of social attention or attract funds as the vertebrates. These include fungal (including lichen-forming species), invertebrate (particularly insect) and plant communities where the vast majority of biodiversity is represented. Conservation of fungi and conservation of insects, in particular, are both of pivotal importance for conservation biology. As mycorrhizal symbionts, and as decomposers and recyclers, fungi are essential for sustainability of forests. The value of insects in the biosphere is enormous because they outnumber all other living groups in measure of species richness. The greatest bulk of biomass on land is found in plants, which is sustained by insect relations. This great ecological value of insects is countered by a society that often reacts negatively toward these aesthetically 'unpleasant' creatures.\n\nOne area of concern in the insect world that has caught the public eye is the mysterious case of missing honey bees (\"Apis mellifera\"). Honey bees provide an indispensable ecological services through their acts of pollination supporting a huge variety of agriculture crops. The use of honey and wax have become vastly used throughout the world. The sudden disappearance of bees leaving empty hives or colony collapse disorder (CCD) is not uncommon. However, in 16-month period from 2006 through 2007, 29% of 577 beekeepers across the United States reported CCD losses in up to 76% of their colonies. This sudden demographic loss in bee numbers is placing a strain on the agricultural sector. The cause behind the massive declines is puzzling scientists. Pests, pesticides, and global warming are all being considered as possible causes.\n\nAnother highlight that links conservation biology to insects, forests, and climate change is the mountain pine beetle (\"Dendroctonus ponderosae\") epidemic of British Columbia, Canada, which has infested of forested land since 1999. An action plan has been prepared by the Government of British Columbia to address this problem.\n\nA large proportion of parasite species are threatened by extinction. A few of them are being eradicated as pests of humans or domestic animals, however, most of them are harmless. Threats include the decline or fragmentation of host populations, or the extinction of host species.\n\nToday, many threats to Biodiversity exist. An acronym that can be used to express the top threats of present-day H.I.P.P.O stands for Habitat Loss, Invasive Species, Pollution, Human Population, and Overharvesting. The primary threats to biodiversity are habitat destruction (such as deforestation, agricultural expansion, urban development), and overexploitation (such as wildlife trade). Habitat fragmentation also poses challenges, because the global network of protected areas only covers 11.5% of the Earth's surface. A significant consequence of fragmentation and lack of linked protected areas is the reduction of animal migration on a global scale. Considering that billions of tonnes of biomass are responsible for nutrient cycling across the earth, the reduction of migration is a serious matter for conservation biology.\n\nHowever, human activities need not necessarily cause irreparable harm to the biosphere. With conservation management and planning for biodiversity at all levels, from genes to ecosystems, there are examples where humans mutually coexist in a sustainable way with nature. Even with the current threats to biodiversity there are ways we can improve the current condition and start anew.\n\nMany of the threats to biodiversity, including disease and climate change, are reaching inside borders of protected areas, leaving them 'not-so protected' (e.g. Yellowstone National Park). Climate change, for example, is often cited as a serious threat in this regard, because there is a feedback loop between species extinction and the release of carbon dioxide into the atmosphere. Ecosystems store and cycle large amounts of carbon which regulates global conditions. In present day, there have been major climate shifts with temperature changes making survival of some species difficult. The effects of global warming add a catastrophic threat toward a mass extinction of global biological diversity. Conservationists have claimed that not all the species can be saved, and they have to decide which their efforts should be used to protect. This concept is known as the Conservation Triage. The extinction threat is estimated to range from 15 to 37 percent of all species by 2050, or 50 percent of all species over the next 50 years. The current extinction rate is 100-100,000 times more rapid today than the last several billion years.\n\n\n\n\n\n\n", "id": "216216", "title": "Conservation biology"}
{"url": "https://en.wikipedia.org/wiki?curid=40470355", "text": "Compilospecies\n\nA compilospecies is a genetically aggressive species that incorporates the heredities of a closely related species and may even completely subsume that species, rendering it extinct. The concept is based on \"Bothriochloa intermedia\", which is directly or indirectly related to the cytogenetic structure of the genera \"Bothriochloa–Dichanthium–Capillipedium\", an apomictic complex. These are all grasses in the tribe Andropogoneae. In this complex, sexual and asexual reproduction are independent and active. Habitats are contiguous, so gene flow is active between species and even genera.\n", "id": "40470355", "title": "Compilospecies"}
{"url": "https://en.wikipedia.org/wiki?curid=23740", "text": "Toxin\n\nA toxin (from ) is a poisonous substance produced within living cells or organisms; synthetic toxicants created by artificial processes are thus excluded. The term was first used by organic chemist Ludwig Brieger (1849–1919).\n\nToxins can be small molecules, peptides, or proteins that are capable of causing disease on contact with or absorption by body tissues interacting with biological macromolecules such as enzymes or cellular receptors. Toxins vary greatly in their toxicity, ranging from usually minor (such as a bee sting) to almost immediately deadly (such as botulinum toxin).\n\nToxins are often distinguished from other chemical agents by their method of production—the word toxin does not specify method of delivery (compare with venom and the broader meaning of poison—all substances that can also cause disturbances to organisms). It simply means it is a biologically produced poison.\nThere was an ongoing terminological dispute between NATO and the Warsaw Pact over whether to call a toxin a biological or chemical agent, in which the NATO opted for biological agent, and the Warsaw Pact, like most other countries in the world, for chemical agent.\n\nAccording to an International Committee of the Red Cross review of the Biological Weapons Convention, \"Toxins are poisonous products of organisms; unlike biological agents, they are inanimate and not capable of reproducing themselves\", and \"Since the signing of the Constitition, there have been no disputes among the parties regarding the definition of biological agents or toxins\".\n\nAccording to Title 18 of the United States Code, \"... the term \"toxin\" means the toxic material or product of plants, animals, microorganisms (including, but not limited to, bacteria, viruses, fungi, rickettsiae or protozoa), or infectious substances, or a recombinant or synthesized molecule, whatever their origin and method of production...\"\n\nA rather informal terminology of individual toxins relates them to the anatomical location where their effects are most notable:\n\nOn a broader scale, toxins may be classified as either exotoxins, being excreted by an organism, or endotoxins, that are released mainly when bacteria are lysed.\n\nThe term \"biotoxin\" is sometimes used to explicitly confirm the biological origin. Biotoxins are further classified into fungal biotoxins, or short mycotoxins, microbial biotoxins, plant biotoxins, short phytotoxins and animal biotoxins.\n\nToxins produced by microorganisms are important virulence determinants responsible for microbial pathogenicity and/or evasion of the host immune response.\n\nBiotoxins vary greatly in purpose and mechanism, and can be highly complex (the venom of the cone snail contains dozens of small proteins, each targeting a specific nerve channel or receptor), or relatively small protein.\n\nBiotoxins in nature have two primary functions:\n\n\nSome of the more well known types of biotoxins include:\n\nThe term \"environmental toxin\" can sometimes explicitly include synthetic contaminants such as industrial pollutants and other artificially made toxic substances. As this contradicts most formal definitions of the term \"toxin\", it is important to confirm what the researcher means when encountering the term outside of microbiological contexts.\n\nEnvironmental toxins from food chains that may be dangerous to human health include:\n\nThe Toxicology and Environmental Health Information Program (TEHIP) at the United States National Library of Medicine (NLM) maintains a comprehensive toxicology and environmental health web site that includes access to toxins-related resources produced by TEHIP and by other government agencies and organizations. This web site includes links to databases, bibliographies, tutorials, and other scientific and consumer-oriented resources. TEHIP also is responsible for the Toxicology Data Network (TOXNET), an integrated system of toxicology and environmental health databases that are available free of charge on the web.\n\nTOXMAP is a Geographic Information System (GIS) that is part of TOXNET. TOXMAP uses maps of the United States to help users visually explore data from the United States Environmental Protection Agency's (EPA) Toxics Release Inventory and Superfund Basic Research Programs.\n\nOne of the bottlenecks in peptide/protein-based therapy is their toxicity. Recently, \"in silico\" models for predicting toxicity of peptides and proteins, developed by Gajendra Pal Singh Raghava's group, predict toxicity with reasonably good accuracy. The prediction models are based on machine learning technique and quantitative matrix using various properties of peptides. The prediction tool is freely accessible to public in the form of web server.\n\nWhen used non-technically, the term \"toxin\" is often applied to any toxic substance, even though the term toxicant would be more appropriate. Toxic substances not directly of biological origin are also termed poisons and many non-technical and lifestyle journalists follow this usage to refer to toxic substances in general.\n\nIn the context of quackery and alternative medicine, the term \"toxin\" is used to refer to any substance alleged to cause ill health. This could range from trace amounts of potentially dangerous pesticides, to supposedly harmful substances produced in the body by intestinal fermentation (auto-intoxication), to food ingredients such as table sugar, monosodium glutamate (MSG), and aspartame.\n\n\n", "id": "23740", "title": "Toxin"}
{"url": "https://en.wikipedia.org/wiki?curid=40239247", "text": "Mycobiota\n\nMycobiota (plural noun, no singular) are a group of all the fungi present in a particular geographic region (e.g. \"the mycobiota of Ireland\") or habitat type (e.g. \"the mycobiota of cocoa\").\n\nThere is a peer-reviewed mycological journal titled \"Mycobiota\".\n\nMycobiota exist on the surface and in the gastrointestinal system of humans.\n", "id": "40239247", "title": "Mycobiota"}
{"url": "https://en.wikipedia.org/wiki?curid=41655528", "text": "Psammon\n\nPsammon (from Greek \"psammos\", \"sand\") is a group of organisms inhabiting coastal sand moist — biota buried in sediments. Psammon is a part of water fauna, along with periphyton, plankton, nekton, and benthos. Psammon is also sometimes considered a part of benthos due to its near-bottom distribution. Psammon term is commonly used to refer to freshwater reservoirs such as lakes.\n", "id": "41655528", "title": "Psammon"}
{"url": "https://en.wikipedia.org/wiki?curid=41678745", "text": "Globoid (botany)\n\nA globoid is a spherical crystalline inclusion in a protein body found in seed tissues that contains phytate and other nutrients for plant growth. These are found in several plants, including wheat and the genus \"Cucurbita\". These nutrients are eventually completely depleted during seedling growth. In \"Cucurbita maxima\" globoids form as early as the 3rd day of seedling growth. They are located in conjunction with a larger crystalloid. They are electron–dense and vary widely in size.\n", "id": "41678745", "title": "Globoid (botany)"}
{"url": "https://en.wikipedia.org/wiki?curid=41710955", "text": "Perennation\n\nIn botany, perennation is the ability of organisms, particularly plants, to survive from one germinating season to another, especially under unfavourable conditions such as drought or winter. It typically involves development of a perennating organ, which stores enough nutrients to sustain the organism during the unfavourable season, and develops into one or more new plants the following year. Common forms of perennating organs are storage organs (e.g. tubers and rhizomes), and buds. Perennation is closely related with vegetative reproduction, as the organisms commonly use the same organs for both survival and reproduction.\n\n", "id": "41710955", "title": "Perennation"}
{"url": "https://en.wikipedia.org/wiki?curid=42402409", "text": "Experimental factor ontology\n\nExperimental factor ontology, also known as EFO, is an open-access ontology of experimental variables particularly those used in molecular biology. The ontology covers variables which include aspects of disease, anatomy, cell type, cell lines, chemical compounds and assay information. EFO is developed and maintained at the EMBL-EBI as a cross-cutting resource for the purposes of curation, querying and data integration in resources such as Ensembl, ChEMBL and Expression Atlas.\n\nThe original aim of EFO was to describe experimental variables in the EBI's Expression Atlas resource. This consisted primarily of disease, anatomical regions and cell types. By December 2013 the scope had grown to include several other EMBL-EBI resources and several external projects including CellFinder, cell lines from the ENCODE project and phenotype to SNP information in the NHGRI's Catalog of Published Genome-Wide Association Studies.\n\nEFO makes use of existing biomedical ontologies from the Open Biomedical Ontologies collection in order to improve interoperability with other resources which may also use these same ontologies, such as ChEBI and the Ontology for Biomedical Investigations.\n\nAll data in the database is non-proprietary or is derived from a non-proprietary source. It is thus freely accessible and available to anyone. In addition, each data item is fully traceable and explicitly referenced to the original source.\n\nThe EFO data is available through a public web interface, BioPortal Web Service hosted at the National Centre for Biomedical Ontology (NCBO) and downloads.\n\n\n", "id": "42402409", "title": "Experimental factor ontology"}
{"url": "https://en.wikipedia.org/wiki?curid=170736", "text": "Understory\n\nIn forestry and ecology, understory (or understorey, underbrush, undergrowth) comprises plant life growing beneath the forest canopy without penetrating it to any great extent, but above the forest floor. Only a small percentage of light penetrates the canopy so understory vegetation is generally shade tolerant. The understory typically consists of trees stunted through lack of light, other small trees with low light requirements, saplings, shrubs, vines and undergrowth. Small trees such as holly and dogwood are understory specialists.\n\nIn temperate deciduous forests, many understory plants start into growth earlier than the canopy trees to make use of the greater availability of light at this time of year. A gap in the canopy caused by the death of a tree stimulates the potential emergent trees into competitive growth as they grow upwards to fill the gap. These trees tend to have straight trunks and few lower branches. At the same time, the bushes, undergrowth and plant life on the forest floor become more dense. The understory experiences greater humidity than the canopy, and the shaded ground does not vary in temperature as much as open ground. This causes a proliferation of ferns, mosses and fungi and encourages nutrient recycling, which provides favorable habitats for many animals and plants.\n\nThe understory is the underlying layer of vegetation in a forest or wooded area, especially the trees and shrubs growing between the forest canopy and the forest floor. \nPlants in the understory comprise an assortment of seedlings and saplings of canopy trees together with specialist understory shrubs and herbs. Young canopy trees often persist in the understory for decades as suppressed juveniles until an opening in the forest overstory permits their growth into the canopy. In contrast understory shrubs complete their life cycles in the shade of the forest canopy. Some smaller tree species, such as dogwood and holly, rarely grow tall and generally are understory trees.\n\nThe canopy of a rainforest is typically about 10m (33ft) thick, and intercepts around 95% of the sunlight. The understory receive less intense light than plants in the canopy and such light as does penetrate is impoverished in wavelengths of light that are most effective for photosynthesis. Understory plants therefore must be shade tolerant—they must be able to photosynthesize adequately using such light as does reach their leaves. They often are able to use wavelengths that canopy plants cannot. In temperate deciduous forests towards the end of the leafless season, understory plants take advantage of the shelter of the still leafless canopy plants to \"leaf out\" before the canopy trees do. This is important because it provides the understory plants with a window in which to photosynthesize without the canopy shading them. This brief period (usually 1–2 weeks) is often a crucial period in which the plant can maintain a net positive carbon balance over the course of the year.\n\nAs a rule forest understories also experience higher humidity than exposed areas. The forest canopy reduces solar radiation, so the ground does not heat up or cool down as rapidly as open ground. Consequently, the understory dries out more slowly than more exposed areas do. The greater humidity encourages epiphytes such as ferns and mosses, and allows fungi and other decomposers to flourish. This drives nutrient cycling, and provides favorable microclimates for many animals and plants, such as the pygmy marmoset.\n\n", "id": "170736", "title": "Understory"}
{"url": "https://en.wikipedia.org/wiki?curid=6257637", "text": "Oophagy\n\nOophagy ( ) sometimes ovophagy, literally \"egg eating\", is the practice of embryos feeding on eggs produced by the ovary while still inside the mother's uterus. The word oophagy is formed from the classical Greek ᾠόν (ōion, egg) and classical Greek φᾱγεῖν (phāgein, to eat). In contrast, adelphophagy is the cannibalism of a multi-celled embryo.\n\nOophagy is thought to occur in all sharks in the order Lamniformes and has been recorded in the bigeye thresher (\"Alopias superciliosus\"), the pelagic thresher (\"A. pelagicus\"), the shortfin mako (\"Isurus oxyrinchus\") and the porbeagle (\"Lamna nasus\") among others. It also occurs in the tawny nurse shark (\"Nebrius ferrugineus\"), and in the family Pseudotriakidae.\n\nThis practice may lead to larger embryos or prepare it for a predatory lifestyle.\n\nThere are variations in the extent of oophagy among the different shark species. The grey nurse shark (\"Carcharias taurus\") practices intrauterine cannibalism, the first developed embryo consuming both additional eggs and any other developing embryos. Slender smooth-hounds\n(\"Gollum attenuatus\"), form egg capsules which contain 30-80 ova within which only one ovum develops while all other ova are ingested and packed to an external yolk sac. The embryo then develops normally without ingesting further eggs.\n\nOophagy is also used to refer to more general egg-eating behaviours such as those practised by some snakes. Similarly, the term can be used to describe the destruction of non-queen eggs in nests of certain social wasps, bees, and ants. This is seen in the wasp species \"Polistes biglumis\" and \"Polistes humilis\". Oophagy has been observed in \"Leptothorax acervorum\" and \"Parachartergus fraternus\", where oophagy is practiced to increase energy circulation and consume more protein. \"Polistes fuscatus\" use oophagy as a method to establish a dominance hierarchy; dominant females will eat the eggs of subordinate females such that they no longer produce eggs, possibly due to the unnecessary expending of energy and resources. This behavior has also been observed in some bee species. Bee species include \"Xylocopa sulcatipes\" and \"Bombus ruderatus\", where queen bees will eat the larva deposited by workers or ejected them from the nest in order to maintain dominance over the female workers.\n\n", "id": "6257637", "title": "Oophagy"}
{"url": "https://en.wikipedia.org/wiki?curid=322376", "text": "Ecological succession\n\nEcological succession is the process of change in the species structure of an ecological community over time. The time scale can be decades (for example, after a wildfire), or even millions of years after a mass extinction.\n\nThe community begins with relatively few pioneering plants and animals and develops through increasing complexity until it becomes stable or self-perpetuating as a climax community. The \"engine\" of succession, the cause of ecosystem change, is the impact of established species upon their own environments. A consequence of living is the sometimes subtle and sometimes overt alteration of one's own environment.\n\nIt is a phenomenon or process by which an ecological community undergoes more or less orderly and predictable changes following a disturbance or the initial colonization of a new habitat. Succession may be initiated either by formation of new, unoccupied habitat, such as from a lava flow or a severe landslide, or by some form of disturbance of a community, such as from a fire, severe windthrow, or logging. Succession that begins in new habitats, uninfluenced by pre-existing communities is called primary succession, whereas succession that follows disruption of a pre-existing community is called secondary succession.\n\nSuccession was among the first theories advanced in ecology. The study of succession remains at the core of ecological science. Ecological succession was first documented in the Indiana Dunes of Northwest Indiana which led to efforts to preserve the Indiana Dunes. Exhibits on ecological succession are displayed in the Hour Glass, a museum in Ogden Dunes.\n\nPrecursors of the idea of ecological succession go back to the beginning of the 19th century. The French naturalist Adolphe Dureau de la Malle was the first to make use of the word \"succession\" concerning the vegetation development after forest clear-cutting. In 1859 Henry David Thoreau wrote an address called \"The Succession of Forest Trees\" in which he described succession in an oak-pine forest. \"It has long been known to observers that squirrels bury nuts in the ground, but I am not aware that any one has thus accounted for the regular succession of forests.\" The Austrian botanist Anton Kerner published a study about the succession of plants in the Danube river basin in 1863.\n\nHenry Chandler Cowles, at the University of Chicago, developed a more formal concept of succession. Inspired by studies of Danish dunes by Eugen Warming, Cowles studied vegetation development on sand dunes on the shores of Lake Michigan (the Indiana Dunes). He recognized that vegetation on dunes of different ages might be interpreted as different stages of a general trend of vegetation development on dunes (an approach to the study of vegetation change later termed space-for-time substitution, or chronosequence studies). He first published this work as a paper in the \"Botanical Gazette\" in 1899 (\"The ecological relations of the vegetation of the sand dunes of Lake Michigan\"). In this classic publication and subsequent papers, he formulated the idea of primary succession and the notion of a sere—a repeatable sequence of community changes specific to particular environmental circumstances.\n\nFrom about 1900 to 1960, however, understanding of succession was dominated by the theories of Frederic Clements, a contemporary of Cowles, who held that seres were highly predictable and deterministic and converged on a climatically determined stable climax community regardless of starting conditions. Clements explicitly analogized the successional development of ecological communities with ontogenetic development of individual organisms, and his model is often referred to as the pseudo-organismic theory of community ecology. Clements and his followers developed a complex taxonomy of communities and successional pathways.\n\nHenry Gleason offered a contrasting framework as early as the 1920s. The Gleasonian model was more complex and much less deterministic than the Clementsian. It differs most fundamentally from the Clementsian view in suggesting a much greater role of chance factors and in denying the existence of coherent, sharply bounded community types. Gleason argued that species distributions responded individualistically to environmental factors, and communities were best regarded as artifacts of the juxtaposition of species distributions. Gleason's ideas, first published in 1926, were largely ignored until the late 1950s.\n\nTwo quotes illustrate the contrasting views of Clements and Gleason. Clements wrote in 1916:\nwhile Gleason, in his 1926 paper, said:\nGleason's ideas were, in fact, more consistent with Cowles' original thinking about succession. About Clements' distinction between primary succession and secondary succession, Cowles wrote (1911):\n\nA more rigorous, data-driven testing of successional models and community theory generally began with the work of Robert Whittaker and John Curtis in the 1950s and 1960s. Succession theory has since become less monolithic and more complex. J. Connell and R. Slatyer attempted a codification of successional processes by mechanism. Among British and North American ecologists, the notion of a stable climax vegetation has been largely abandoned, and successional processes have come to be seen as much less deterministic, with important roles for historical contingency and for alternate pathways in the actual development of communities. Debates continue as to the general predictability of successional dynamics and the relative importance of equilibrial vs. non-equilibrial processes. Former Harvard professor F. A. Bazzaz introduced the notion of \"scale\" into the discussion, as he considered that at local or small area scale the processes are stochastic and patchy, but taking bigger regional areas into consideration, certain tendencies can not be denied.\n\nThe trajectory of successional change can be influenced by site conditions, by the character of the events initiating succession (perturbations), by the interactions of the species present, and by more stochastic factors such as availability of colonists or seeds or weather conditions at the time of disturbance. Some of these factors contribute to predictability of succession dynamics; others add more probabilistic elements. Two important perturbation factors today are human actions and climatic change.\n\nIn general, communities in early succession will be dominated by fast-growing, well-dispersed species (opportunist, fugitive, or r-selected life-histories). As succession proceeds, these species will tend to be replaced by more competitive (k-selected) species.\n\nTrends in ecosystem and community properties in succession have been suggested, but few appear to be general. For example, species diversity almost necessarily increases during early succession as new species arrive, but may decline in later succession as competition eliminates opportunistic species and leads to dominance by locally superior competitors. Net Primary Productivity, biomass, and trophic properties all show variable patterns over succession, depending on the particular system and site.\n\nEcological succession was formerly seen as having a stable end-stage called the climax, sometimes referred to as the 'potential vegetation' of a site, and shaped primarily by the local climate. This idea has been largely abandoned by modern ecologists in favor of nonequilibrium ideas of ecosystems dynamics. Most natural ecosystems experience disturbance at a rate that makes a \"climax\" community unattainable. Climate change often occurs at a rate and frequency sufficient to prevent arrival at a climax state. Additions to available species pools through range expansions and introductions can also continually reshape communities.\n\nThe development of some ecosystem attributes, such as soil properties and nutrient cycles, are both influenced by community properties, and, in turn, influence further successional development. This feed-back process may occur only over centuries or millennia. Coupled with the stochastic nature of disturbance events and other long-term (e.g., climatic) changes, such dynamics make it doubtful whether the 'climax' concept ever applies or is particularly useful in considering actual vegetation.\n\nSuccessional dynamics beginning with colonization of an area that has not been previously occupied by an ecological community, such as newly exposed rock or sand surfaces, lava flows, newly exposed glacial tills, etc., are referred to as primary succession. The stages of primary succession include pioneer plants (lichens and mosses), grassy stage, smaller shrubs, and trees. Animals begin to return when there is food there for them to eat. When it is a fully functioning ecosystem, it has reached the climax community stage. For example, parts of Acadia National Park in Maine went through primary succession.\nSuccessional dynamics following severe disturbance or removal of a pre-existing community are called secondary succession. Dynamics in secondary succession are strongly influenced by pre-disturbance conditions, including soil development, seed banks, remaining organic matter, and residual living organisms. Because of residual fertility and pre-existing organisms, community change in early stages of secondary succession can be relatively rapid. In a fragmented old field habitat created in eastern Kansas, woody plants \"colonized more rapidly (per unit area) on large and nearby patches\".\n\nSecondary succession is much more commonly observed and studied than primary succession. Particularly common types of secondary succession include responses to natural disturbances such as fire, flood, and severe winds, and to human-caused disturbances such as logging and agriculture. As an example, secondary succession has been occurring in Shenandoah National Park following the 1995 flood of the Mormon River, which destroyed plant and animal life. Today, plant and animal species are beginning to return.\n\nUnlike secondary succession, these types of vegetation change are not dependent on disturbance but are periodic changes arising from fluctuating species interactions or recurring events. These models modify the climax concept towards one of dynamic states.\n\nAutogenic succession can be brought by changes in the soil caused by the organisms there. These changes include accumulation of organic matter in litter or humic layer, alteration of soil nutrients, or change in the pH of soil due to the plants growing there. The structure of the plants themselves can also alter the community. For example, when larger species like trees mature, they produce shade on to the developing forest floor that tends to exclude light-requiring species. Shade-tolerant species will invade the area.\n\nAllogenic succession is caused by external environmental influences and not by the vegetation. For example, soil changes due to erosion, leaching or the deposition of silt and clays can alter the nutrient content and water relationships in the ecosystems. Animals also play an important role in allogenic changes as they are pollinators, seed dispersers and herbivores. They can also increase nutrient content of the soil in certain areas, or shift soil about (as termites, ants, and moles do) creating patches in the habitat. This may create regeneration sites that favor certain species.\n\nClimatic factors may be very important, but on a much longer time-scale than any other. Changes in temperature and rainfall patterns will promote changes in communities. As the climate warmed at the end of each ice age, great successional changes took place. The tundra vegetation and bare glacial till deposits underwent succession to mixed deciduous forest. The greenhouse effect resulting in increase in temperature is likely to bring profound Allogenic changes in the next century. Geological and climatic catastrophes such as volcanic eruptions, earthquakes, avalanches, meteors, floods, fires, and high wind also bring allogenic changes.\n\nIn 1916, Frederic Clements published a descriptive theory of succession and advanced it as a general ecological concept. His theory of succession had a powerful influence on ecological thought. Clements' concept is usually termed classical ecological theory.\nAccording to Clements, succession is a process involving several phases:\n\n\nA seral community is an intermediate stage found in an ecosystem advancing towards its climax community. In many cases more than one seral stage evolves until climax conditions are attained. A \"prisere\" is a collection of seres making up the development of an area from non-vegetated surfaces to a climax community. Depending on the substratum and climate, different seres are found.\n\nSuccession theory was developed primarily by botanists. The study of succession applied to whole ecosystems initiated in the writings of Ramon Margalef, while Eugene Odum’s publication of \"The Strategy of Ecosystem Development\" is considered its formal starting point.\n\nAnimal life also exhibit changes with changing communities. In lichen stage the fauna is sparse. It comprises few mites, ants and spiders living in the cracks and crevices. The fauna undergoes a qualitative increase during herb grass stage. The animals found during this stage include nematodes, insects larvae, ants, spiders, mites, etc. The animal population increases and diversifies with the development of forest climax community. The fauna consists of invertebrates like slugs, snails, worms, millipedes, centipedes, ants, bugs; and vertebrates such as squirrels, foxes, mice, moles, snakes, various birds, salamanders and frogs.\n\nSuccession of micro-organisms including fungi and bacteria occurring within a microhabitat is known as microsuccession or serule. This type of succession occurs in recently disturbed communities or newly available habitat, for example in recently dead trees, animal droppings, exposed glacial till, etc. Microbial communities may also change due to products secreted by the bacteria present. Changes of pH in a habitat could provide ideal conditions for a new species to inhabit the area. In some cases the new species may outcompete the present ones for nutrients leading to the primary species demise. Changes can also occur by microbial succession with variations in water availability and temperature. Theories of macroecology have only recently been applied to microbiology and so much remains to be understood about this growing field. A recent study of microbial succession evaluated the balances between stochastic and deterministic processes in the bacterial colonization of a salt marsh chronosequence. The results of this study show that, much like in macro succession, early colonization (primary succession) is mostly influenced by stochasticity while secondary succession of these bacterial communities was more strongly influenced by deterministic factors.\n\nAccording to classical ecological theory, succession stops when the sere has arrived at an equilibrium or steady state with the physical and biotic environment. Barring major disturbances, it will persist indefinitely. This end point of succession is called climax.\n\nThe final or stable community in a sere is the \"climax community\" or \"climatic vegetation\". It is self-perpetuating and in equilibrium with the physical habitat. There is no net annual accumulation of organic matter in a climax community. The annual production and use of energy is balanced in such a community.\n\n\n\nThere are three schools of interpretations explaining the climax concept:\n\n\nThe theory of alternative stable states suggests there is not one end point but many which transition between each other over ecological time.\n\nThe forests, being an ecological system, are subject to the species succession process. There are \"opportunistic\" or \"pioneer\" species that produce great quantities of seed that are disseminated by the wind, and therefore can colonize big empty extensions. They are capable of germinating and growing in direct sunlight. Once they have produced a \"closed canopy\", the lack of direct sun radiation at soil makes it difficult for their own seedlings to develop. It is then the opportunity for shade-tolerant species to become established under the protection of the pioneers. When the pioneers die, the shade-tolerant species replace them. These species are capable of growing beneath the canopy, and therefore, in the absence of catastrophes, will stay. For this reason it is then said the stand has reached its climax. When a catastrophe occurs, the opportunity for the pioneers opens up again, provided they are present or within a reasonable range.\n\nAn example of pioneer species, in forests of northeastern North America are \"Betula papyrifera\" (White birch) and \"Prunus serotina\" (Black cherry), that are particularly well-adapted to exploit large gaps in forest canopies, but are intolerant of shade and are eventually replaced by other shade-tolerant species in the absence of disturbances that create such gaps.\n\nThings in nature are not black and white, and there are intermediate stages. It is therefore normal that between the two extremes of light and shade there is a gradient, and there are species that may act as pioneer or tolerant, depending on the circumstances. It is of paramount importance to know the tolerance of species in order to practice an effective silviculture.\n\n", "id": "322376", "title": "Ecological succession"}
{"url": "https://en.wikipedia.org/wiki?curid=44339334", "text": "Rimose\n\nRimose is an adjective used to describe a surface that is cracked or fissured. The term is often used in describing crustose lichens. A rimose surface of a lichen is sometimes contrasted to the surface being areolate. Areolate is an extreme form of being rimose, where the cracks or fissures are so deep that they create island-like pieces called areoles, which look the \"islands\" of mud on the surface of a dry lake bed. Rimose and areolate are contrasted with being verrucose, or \"warty\". Verrucose surfaces have warty bumps which are distinct, but not separated by cracks.\n", "id": "44339334", "title": "Rimose"}
{"url": "https://en.wikipedia.org/wiki?curid=17554500", "text": "Environment (biophysical)\n\nThe biophysical environment is the biotic and abiotic surrounding of an organism or population, and consequently includes the factors that have an influence in their survival, development, and evolution. The biophysical environment can vary in scale from microscopic to global in extent. It can also be subdivided according to its attributes. Examples include the marine environment, the atmospheric environment and the terrestrial environment. The number of biophysical environments is countless, given that each living organism has its own environment.\n\nThe term \"environment\" is often used as a short form for the biophysical environment, e.g. the UK's Environment Agency. The expression \"\"the environment\"\" often refers to a singular global environment in relation to humanity.\n\nAll life that has survived must have adapted to conditions of its environment. Temperature, light, humidity, soil nutrients, etc., all influence any species, within any environment. However life in turn modifies, in various forms, its conditions. Some long term modifications along the history of our planet have been significant, such as the incorporation of oxygen to the atmosphere. This process consisted in the breakdown of carbon dioxide by anaerobic microorganisms that used the carbon in their metabolism and released the oxygen to the atmosphere. This led to the existence of oxygen-based plant and animal life, the great oxygenation event. Other interactions are more immediate and simple, such as the smoothing effect that forests have on the temperature cycle, compared to neighboring unforested areas.\n\nEnvironmental science is the study of the interactions within the biophysical environment. Part of this scientific discipline is the investigation of the effect of human activity on the environment. Ecology, a sub-discipline of biology and a part of environmental sciences, is often mistaken as a study of human induced effects on the environment. Environmental studies is a broader academic discipline that is the systematic study of interaction of humans with their environment. It is a broad field of study that includes the natural environment, built environments and social environments.\n\nEnvironmentalism is a broad social and philosophical movement that, in a large part, seeks to minimise and compensate the negative effect of human activity on the biophysical environment. The issues of concern for environmentalists usually relate to the natural environment with the more important ones being climate change, species extinction, pollution, and old growth forest loss.\n\nOne of the studies related include employing Geographic Information Science to study the biophysical environment.\n\n\n", "id": "17554500", "title": "Environment (biophysical)"}
{"url": "https://en.wikipedia.org/wiki?curid=28990145", "text": "Proteostasis\n\nProteostasis, a portmanteau of the words protein and homeostasis, is the concept that there are competing and integrated biological pathways within cells that control the biogenesis, folding, trafficking and degradation of proteins present within and outside the cell. The concept of proteostasis maintenance is central to understanding the cause of diseases associated with excessive protein misfolding and degradation leading to loss-of-function phenotypes, as well as aggregation-associated degenerative disorders. Therefore, adapting proteostasis should enable the restoration of proteostasis once its loss leads to pathology. Cellular proteostasis is key to ensuring successful development, healthy aging, resistance to environmental stresses, and to minimize homeostasis perturbations by pathogens such as viruses. Mechanisms by which proteostasis is ensured include regulated protein translation, chaperone assisted protein folding and protein degradation pathways. Adjusting each of these mechanisms to the demand for proteins is essential to maintain all cellular functions relying on a correctly folded proteome.\n\nOne of the first points of regulation for proteostasis is during translation. This is accomplished via the structure of the ribosome, a complex central to translation. These two characteristics shape the way the protein folds and influences the proteins future interactions. The synthesis of a new peptide chain using the ribosome is very slow and the ribosome can even be stalled when it encounters a rare codon, a codon found at low concentrations in the cell. These pauses provide an opportunity for an individual protein domain to have the necessary time to become folded before the production of following domains. This facilitates the correct folding of multi-domain proteins.\nThe newly synthesized peptide chain exits the ribosome into the cellular environment through the narrow ribosome exit channel (width: 10Å to 20Å, length 80Å). Due to space restriction in the exit channel the nascent chain already forms secondary and limited tertiary structures. For example, an alpha helix is one such structural property that is commonly induced in this exit channel. At the same time the exit channel also prevents premature folding by impeding large scale interactions within the peptide chain which would require more space.\n\nIn order to maintain protein homeostasis post-translationally, the cell makes use of molecular chaperones sometimes including chaperonins, which aid in the assembly or disassembly of proteins. They recognize exposed segments of hydrophobic amino acids in the nascent peptide chain and then work to promote the proper formation of noncovalent interactions that lead to the desired folded state. \nChaperones begin to assist in protein folding as soon as a nascent chain longer than 60 amino acids emerges from the ribosome exit channel. One of the most studied ribosome binding chaperones is trigger factor. Trigger Factor works to stabilize the peptide, promotes its folding, prevents aggregation, and promotes refolding of denatured model substrates. Trigger factor not only directly works to properly fold the protein but also recruits other chaperones to the ribosome, such as Hsp70. Hsp70 surrounds an unfolded peptide chain, thereby preventing aggregation and promoting folding.,\n\nChaperonins are a special class of chaperones that promote native state folding by cyclically encapsulating the peptide chain. Chaperonins are divided into two groups. Group 1 chaperonins are commonly found in bacteria, chloroplasts, and mitochondria. Group 2 chaperonins are found in both the cytosol of eukaryotic cells as well as in archaea. Group 2 chaperonins also contain an additional helical component which acts as a lid for the cylindrical protein chamber, unlike Group 1 which instead relies on an extra cochaperone to act as a lid. All chaperonins exhibit two states (open and closed), between which they can cycle. This cycling process is important during the folding of an individual polypeptide chain as it helps to avoid undesired interactions as well as to prevent the peptide from entering into kinetically trapped states.\n\nThe third component of the proteostasis network is the protein degradation machinery. Protein degradation occurs in proteostasis when the cellular signals indicate the need to decrease overall cellular protein levels. The effects of protein degradation can be local, with the cell only experiencing effects from the loss of the degraded protein itself or widespread, with the entire protein landscape changing due to loss of other proteins’ interactions with the degraded protein. Multiple substrates are targets for proteostatic degradation. These degradable substrates include nonfunctional protein fragments produced from ribosomal stalling during translation, misfolded or unfolded proteins, aggregated proteins, and proteins that are no longer needed to carry out cellular function. Several different pathways exist for carrying out these degradation processes. When proteins are determined to be unfolded or misfolded, they are typically degraded via the unfolded protein response (UPR) or endoplasmic-reticulum-associated protein degradation (ERAD). Substrates that are unfolded, misfolded, or no longer required for cellular function can also be ubiquitin tagged for degradation by ATP dependent proteases, such as the proteasome in eukaryotes or ClpXP in prokaryotes. Autophagy, or self engulfment, lysosomal targeting, and phagocytosis (engulfment of waste products by other cells) can also be used as proteostatic degradation mechanisms.\n\nProtein misfolding is detected by mechanisms that are specific for the cellular compartment in which they occur. Distinct surveillance mechanisms that respond unfolded protein have been characterized in the cytoplasm, ER and mitochondria. This response acts locally in a cell autonomous fashion but can also extend to intercellular signaling to protect the organism from anticipated proteotoxic stress.\n\nCellular stress response pathways detect and alleviate proteotoxic stress which is triggered by imbalances in proteostasis. The cell-autonomous regulation occurs through direct detection of misfolded proteins or inhibition of pathway activation by sequestering activating components in response to heat shock. Cellular responses to this stress signaling include transcriptional activation of chaperone expression, increased efficiency in protein trafficking and protein degradation and translational reduction.\n\nThe cytosolic HSR is mainly mediated by the transcription factor family HSF (heat shock family). HSF is constitutively bound by Hsp90. Upon a proteotoxic stimulus Hsp90 is recruited away from HSF which can then bind to heat response elements in the DNA and upregulate gene expression of proteins involved in the maintenance of proteostasis.\n\nThe unfolded protein response in the endoplasmatic reticulum (ER) is activated by imbalances of unfolded proteins inside the ER and the proteins mediating protein homeostasis. Different “detectors” - such as IRE1, ATF6 and PERK - can recognize misfolded proteins in the ER and mediate transcriptional responses which help alleviate the effects of ER stress.\n\nThe mitochondrial unfolded protein response detects imbalances in protein stoichiometry of mitochondrial proteins and misfolded proteins. The expression of mitochondrial chaperones is upregulated by the activation of the transcription factors ATF-1 and/or DVE-1 with UBL-5.\n\nStress responses can also be triggered in a non-cell autonomous fashion by intercellular communication. The stress that is sensed in one tissue could thereby be communicated to other tissues to protect the proteome of the organism or to regulate proteostasis systemically. Cell non-autonomous activation can occur for all three stress responses.\n\nWork on the model organism \"C. elegans\" has shown that neurons play a role in this intercellular communication of cytosolic HSR. Stress induced in the neurons of the worm can in the long run protect other tissues such as muscle and intestinal cells from chronic proteotoxicity. Similarly ER and mitochondrial UPR in neurons are relayed to intestinal cells . These systemic responses have been implicated in mediating not only systemic proteostasis but also influence organismal aging.\n\nDysfunction in proteostasis can arise from errors in or misregulation of protein folding. The classic examples are missense mutations and deletions that change the thermodynamic and kinetic parameters for the protein folding process. These mutations are often inherited and range in phenotypic severity from having no noticeable effect to embryonic lethality. Disease develops when these mutations render a protein significantly more susceptible to misfolding, aggregation, and degradation. If these effects only alter the mutated protein, the negative consequences will only be local loss of function. However, if these mutations occur in a chaperone or a protein that interacts with many other proteins, dramatic global alterations in the proteostasis boundary will occur. Examples of diseases resulting from proteostatic changes from errors in protein folding include cystic fibrosis, Huntington’s disease, Alzheimer’s disease, lysosomal storage disorders, and others.\n\nSmall animal model systems have been and continue to be instrumental in the identification of functional mechanisms that safeguard proteostasis. Model systems of diverse misfolding-prone disease proteins have so far revealed numerous chaperone and co-chaperone modifiers of proteotoxicity.\n\nThe unregulated cell division that marks cancer development requires increased protein synthesis for cancer cell function and survival. This increased protein synthesis is typically seen in proteins that modulate cell metabolism and growth processes. Cancer cells are sometimes susceptible to drugs that inhibit chaperones and disrupt proteostasis, such as Hsp90 inhibitors or proteasome inhibitors. Furthermore, cancer cells tend to produce misfolded proteins, which are removed mainly by proteolysis. Inhibitors of proteolysis allow accumulation of both misfolded protein aggregates, as well as apoptosis signaling proteins in cancer cells. This can can change the sensitivity of cancer cells to antineoplastic drugs; cancer cells either die at a lower drug concentration, or survive, depending on the type of proteins that accumulate, and the function these proteins have. Proteasome inhibitor bortezomib was the first drug of this type to receive approval for treatment of multiple myeloma.\n\nA hallmark of cellular proteostatic networks is their ability to adapt to stress via protein regulation. Metabolic disease, such as that associated with obesity, alters the ability of cellular proteostasis networks adapt to stress, often with detrimental health effects. For example, when insulin production exceeds the cell’s insulin secretion capacity, proteostatic collapse occurs and chaperone production is severely impaired. This disruption leads to the disease symptoms exhibited in individuals with diabetes.\n\nOver time, the proteostasis network becomes burdened with proteins modified by reactive oxygen species and metabolites that induce oxidative damage. These byproducts can react with cellular proteins to cause misfolding and aggregation (especially in nondividing cells like neurons). This risk is particularly high for intrinsically disordered proteins. The IGFR-1 pathway has been shown in \"C. elegans\" to protect against these harmful aggregates, and some experimental work has suggested that upregulation of insulin growth factor receptor 1 (IGFR-1) may stabilize proteostatic network and prevent detrimental effects of aging. Expression of the chaperome, the ensemble of chaperones and co-chaperones that interact in a complex network of molecular folding machines to regulate proteome function, is dramatically repressed in human aging brains and in the brains of patients with neurodegenerative diseases. Functional assays in \"C. elegans\" and human cells have identified a conserved chaperome sub-network of 16 chaperone genes, corresponding to 28 human orthologs as a proteostasis safeguard in aging and age-onset neurodegenerative disease.\n\nThere are two main approaches that have been used for therapeutic development targeting the proteostatic network: pharmacologic chaperones and proteostasis regulators. The principle behind designing pharmacologic chaperones for intervention in diseases of proteostasis is to design small molecules that stabilize proteins exhibiting borderline stability. Previously, this approach has been used to target and stabilize G-protein coupled receptors, neurotransmitter receptors, glycosidases, lysosomal storage proteins, and the mutant CFTR protein that causes cystic fibrosis and transthyretin, which can misfiled and aggregate leading to amyloidoses. Vertex Pharmaceuticals and Pfizer sell regulatory agency approved pharmacologic chaperones for ameliorating cystic fibrosis and the transthyretin amyloidoses, respectively. Amicus sells a regulatory agency approved pharmacologic chaperone for Fabry disease–a lysosomal storage disease.\nThe principle behind proteostasis regulators is different, these molecules alter the biology of protein folding and / or degradation by altering the stoichiometry of the proteostasis network components in a given sub cellular compartment. For example, some proteostasis regulators initiate stress responsive signaling, such as the unfolded protein response, which transcriptionally reprograms the endoplasmic reticulum proteostasis network. It has been suggested that this approach could even be applied prophylactically, such as upregulating certain protective pathways before experiencing an anticipated severe cellular stress. One theoretical mechanism for this approach includes upregulating the heat shock response response to rescue proteins from degradation during cellular stress.\n\n\n", "id": "28990145", "title": "Proteostasis"}
{"url": "https://en.wikipedia.org/wiki?curid=14644243", "text": "Taxonomic rank\n\nIn biological classification, taxonomic rank is the relative level of a group of organisms (a taxon) in a taxonomic hierarchy. Examples of taxonomic ranks are species, genus, family, order, class, phylum, kingdom, domain, etc.\n\nA given rank subsumes under it less general categories, that is, more specific descriptions of life forms. Above it, each rank is classified within more general categories of organisms and groups of organisms related to each other through inheritance of traits or features from common ancestors. The rank of any \"species\" and the description of its \"genus\" is \"basic\"; which means that to identify a particular organism, it is usually not necessary to specify ranks other than these first two.\n\nConsider a particular species, the red fox, \"Vulpes vulpes\": its next rank, the genus \"Vulpes\", comprises all the \"true\" foxes. Their closest relatives are in the immediately higher rank, the family Canidae, which includes dogs, wolves, jackals, and all foxes; the next higher rank, the order Carnivora, includes caniforms (mustelids, bears, seals, and all those mentioned above), and feliforms (cats, hyenas, mongooses, civets), plus other carnivorous mammals. As one group of the class Mammalia, all of the above are classified among those with backbones, in the Chordata phylum rank; and with them among all the animals, in the Animalia kingdom rank. Finally, all of the above will find their earliest relatives somewhere in their domain rank Eukarya.\n\nThe \"International Code of Zoological Nomenclature\" defines \"rank\" as: \"The level, for nomenclatural purposes, of a taxon in a taxonomic hierarchy (e.g. all families are for nomenclatural purposes at the same rank, which lies between superfamily and subfamily).\"\n\nIn his landmark publications, such as the \"Systema Naturae\", Carl Linnaeus used a ranking scale limited to: kingdom, class, order, genus, species, and one rank below species. Today, nomenclature is regulated by the nomenclature codes. There are seven main taxonomic ranks: kingdom, phylum or division, class, order, family, genus, species. In addition, the \"domain\" (proposed by Carl Woese) is now widely used as one of the fundamental ranks, although it is not mentioned in any of the nomenclature codes. Also, this term represents a synonym for the category of dominion (lat. dominium), introduced by Moore in 1974. Unlike Moore, Whoese et al. (1990) did not suggest a Latin term for this category, which represents a further argument supporting the accurately introduced term dominion.\n\nA taxon is usually assigned a rank when it is given its formal name. The basic ranks are species and genus. When an organism is given a species name it is assigned to a genus, and the genus name is part of the species name.\n\nThe species name is also called a binomial, that is, a two-term name. For example, the zoological name for the human species is \"Homo sapiens\". This is usually italicized in print and underlined when italics are not available. In this case, \"Homo\" is the generic name and it is capitalized; \"sapiens\" indicates the species and it is not capitalized.\n\nThere are definitions of the following taxonomic ranks in the International Code of Zoological Nomenclature: superfamily, family, subfamily, tribe, subtribe, genus, subgenus, species, subspecies.\n\nThe International Code of Zoological Nomenclature divides names into \"family-group names\", \"genus-group names\" and \"species-group names\". The Code explicitly mentions:\n\n\"Family\"\n\n\"Genus\"\n\"Species\"\n\nThe rules in the Code apply to the ranks of superfamily to subspecies, and only to some extent to those above the rank of superfamily. In the \"genus group\" and \"species group\" no further ranks are allowed. Among zoologists, additional terms such as \"species group\", \"species subgroup\", \"species complex\" and \"superspecies\" are sometimes used for convenience as extra, but unofficial, ranks between the subgenus and species levels in taxa with many species (e.g. the genus \"Drosophila\").\n\nAt higher ranks (family and above) a lower level may be denoted by adding the prefix \"\"infra\"\", meaning \"lower\", to the rank. For example, \"infra\"order (below suborder) or \"infra\"family (below subfamily).\n\n\nAccording to Art 3.1 of the International Code of Nomenclature for algae, fungi, and plants (ICN) the most important ranks of taxa are: kingdom, division or phylum, class, order, family, genus, and species. According to Art 4.1 the secondary ranks of taxa are tribe, section, series, variety and form. There is an indeterminate number of ranks. The ICN explicitly mentions:\nprimary ranks\nkingdom (\"regnum\")\n\ndivision or phylum (\"divisio\", \"phylum\")\n\nclass (\"classis\")\n\norder (\"ordo\")\nfamily (\"familia\")\n\ngenus (\"genus\")\n\nspecies (\"species\")\n\nThere are definitions of the following taxonomic categories in the International Code of Nomenclature for Cultivated Plants: cultivar group, cultivar, grex.\n\nThe rules in the ICN apply primarily to the ranks of family and below, and only to some extent to those above the rank of family. Also see descriptive botanical names.\n\nTaxa at the rank of genus and above have a botanical name in one part (unitary name); those at the rank of species and above (but below genus) have a botanical name in two parts (binary name); all taxa below the rank of species have a botanical name in three parts (an infraspecific name). To indicate the rank of the infraspecific name, a \"connecting term\" is needed. Thus \"Poa secunda\" subsp. \"juncifolia\", where \"subsp.\" is an abbreviation for \"subspecies\", is the name of a subspecies of \"Poa secunda\".\n\nHybrids can be specified either by a \"hybrid formula\" that specifies the parentage, or may be given a name. For hybrids receiving a hybrid name, the same ranks apply, prefixed with \"notho\" (Greek: 'bastard'), with nothogenus as the highest permitted rank.\n\nIf a different term for the rank was used in an old publication, but the intention is clear, botanical nomenclature specifies certain substitutions:\n\nClassifications of five species follow: the fruit fly so familiar in genetics laboratories (\"Drosophila melanogaster\"), humans (\"Homo sapiens\"), the peas used by Gregor Mendel in his discovery of genetics (\"Pisum sativum\"), the \"fly agaric\" mushroom \"Amanita muscaria\", and the bacterium \"Escherichia coli\". The eight major ranks are given in bold; a selection of minor ranks are given as well.\n\n\nTaxa above the genus level are often given names based on the type genus, with a standard termination. The terminations used in forming these names depend on the kingdom (and sometimes the phylum and class) as set out in the table below.\n\nPronunciations given are the most Anglicized. More Latinate pronunciations are also common, particularly rather than for stressed \"a\".\n\n\nThere is an indeterminate number of ranks, as a taxonomist may invent a new rank at will, at any time, if they feel this is necessary. In doing so, there are some restrictions, which will vary with the nomenclature code which applies.\n\nThe following is an artificial synthesis, solely for purposes of demonstration of relative rank (but see notes), from most general to most specific:\n\nRanks are assigned based on subjective dissimilarity, and do not fully reflect the gradational nature of variation within nature. In most cases, higher taxonomic groupings arise further back in time: not because the rate of diversification was higher in the past, but because each subsequent diversification event results in an increase of diversity and thus increases the taxonomic rank assigned by present-day taxonomists. Furthermore, some groups have many described species not because they are more diverse than other species, but because they are more easily sampled and studied than other group.\n\nOf these many ranks, the most basic is species. However, this is not to say that a taxon at any other rank may not be sharply defined, or that any species is guaranteed to be sharply defined. It varies from case to case. Ideally, a taxon is intended to represent a clade, that is, the phylogeny of the organisms under discussion, but this is not a requirement.\n\nClassification, in which all taxa have formal ranks, cannot adequately reflect knowledge about phylogeny; at the same time, if taxon names are dependent on ranks, rank-free taxa can't be supplied with names. This problem is dissolved in cladoendesis, where the specially elaborated rank-free nomenclatures are used.\n\nThere are no rules for how many species should make a genus, a family, or any other higher taxon (that is, a taxon in a category above the species level). It should be a natural group (that is, non-artificial, non-polyphyletic), as judged by a biologist, using all the information available to them. Equally ranked higher taxa in different phyla are not necessarily equivalent (e.g., it is incorrect to assume that families of insects are in some way evolutionarily comparable to families of mollusks). For animals, at least the phylum rank is usually associated with a certain body plan, which is also, however, an arbitrary criterion.\n\n\n", "id": "14644243", "title": "Taxonomic rank"}
{"url": "https://en.wikipedia.org/wiki?curid=38051839", "text": "Postglacial vegetation\n\nPostglacial vegetation refers to plants that colonize the newly exposed substrate after a glacial retreat. The term \"postglacial\" typically refers to processes and events that occur after the departure of glacial ice or glacial climates.\n\nClimate change is the main force behind changes in species distribution and abundance. Repeated changes in climate throughout the Quaternary Period are thought to have had a significant impact on the current vegetative species diversity present today. Functional and phylogenetic diversity are considered to be closely related to changing climatic conditions, this indicates that trait differences are extremely important in long term responses to climate change. During the transition from the last glaciation of the Pleistocene to the Holocene period, climate warming resulted in the expansion of taller plants and larger seed bearing plants which resulted in lower proportions of vegetative regeneration. Hence, low temperatures can be strong environmental filters that prevent tall and large-seeded plants from establishing in postglacial environments.\nThroughout Europe vegetation dynamics within the first half of the Holocene appear to have been influenced mainly by climate and the reorganization of atmospheric circulation associated with the disappearance of the North American ice sheet. This is evident in the rapid increase of forestation and changing biomes during the postglacial period between 11500ka and 8000ka before the present.\nVegetation development periods of post-glacial land forms on Ellesmere Island, Northern Canada, is assumed to have been at least ca. 20,000 years in duration. This slow progression is mostly due to climatic restrictions such as an estimated annual rainfall amount of only 64mm and a mean annual temperature of -19.7 degrees Celsius. The length in time of vegetation development observed on Ellesmere Island is evidence that post glacial vegetation development is much more restricted in the Arctic and colder climates as compared to milder climatic regions such as the boreal, temperate and tropical zones.\n\nAs land became exposed following the glaciation of the last ice age, a variety of geographic settings ranging from the tropics to the Arctic and Antarctic became available for the establishment of vegetation. Species that now exist on formerly glaciated terrain must have undergone a change in distribution of hundreds to thousands of kilometers, or have evolved from other taxa that have once done so in the past. In a newly developing environment, plant growth is often strongly influenced by the introduction of new organisms into that environment, where competitive or mutuallistic relationships may develop. Often, competitive balances are eventually reached and species abundances remain somewhat constant over a period of generations. \nStudies done on the Norwegian Island of Svalbard, have been very useful in understanding the behavior of postglacial vegetation. Studies show that many vascular plants that are considered pioneers of vegetation development, eventually become less frequent. For example, the abundance of species such as Braya purpurascens has fallen nearly 30% due to the introduction of new species in the area.\n\nArctic vegetation has distinct postglacial development characteristics compared to more temperate zones of lower latitudes. A study of postglacial moraines conducted in the Canadian Arctic on Ellesmere Island have found that dwarf shrubs of Dryas integrifolia and Cassiope tetragona are often good indicators of vegetation development and progression. Dwarf shrubs have been found to increase with the age of the moraine, with Dryas integrifolia becoming the most predominant. As well the cover of vegetation, including lichens and bryophytes showed consistent increase with the moraine age, suggesting directional vegetation development. It is also suggested that part of the high proportions of polypoids occurring in arctic floras is the result of speciation as continental ice-sheets withdrew.\nPollen diagrams from northern Quebec, Canada, show advances throughout the Holocene of post-glacial vegetation development. The initial phase of open vegetation began about 6000 years before the present. Following deglaciation, shrub and herbaceous tundra plants dominated for a brief period of time. Plants such as the Larix laricina, Populus and Juniperus, were also important in the initial vegetation development. Some species that followed later include: Alnus crispa, and Betula. Though later vegetation development was mainly dominated by Picea, shortly following deglaciation, they reached their present day limit. Today black spruce is mainly dominant throughout much of northern Quebec.\nContinental U.S. is considered to have strongly contributed to the re-establishment of postglacial vegetation in Canada following the last ice age. Roughly 300 taxa of vascular plants and mosses that were found to have existed below the extent of the last glacial period within the United States are also found to have migrated to Canada. These patterns are recorded within either pollen or macro fossils.\n\nStudies done by Reitalu, (2015) have found that human impact throughout much of Europe has negatively influenced plant diversity by suppressing the establishment of tall-growing, large seeded taxa. Although human influence has facilitated many Ruderal species, this is believed to have led to an overall decrease in phylogenetic diversity.\n\nMany pollen diagrams around the world indicate that major climate changes caused the last continental ice sheets to retreat, leading to dramatic effects on the distribution and abundance of plants. By converting pollen data into plant functional type (PFT) assemblages and interpolating the data, researchers have been able to reconstruct postglacial vegetation patterns around the world. Core sampling and analysis of lake sediments that contain pollen and other plant remains are often used to obtain good records of past pollination cycles. Such paleorecords preserved in lake sediments can be used to reconstruct the history of post glacial vegetation. Lake sediments have an advantage over other core sampling sites, such as fen and bog peats, as they provide no overwhelming local pollen components. As well, lake sediments contain stratigraphic changes in soil character, which are useful for understanding changes in vegetation development over a period of time. Macrofossils that are obtained from sedimentary deposits are also useful for constructing the history of changing postglacial vegetation.\n\n\n", "id": "38051839", "title": "Postglacial vegetation"}
{"url": "https://en.wikipedia.org/wiki?curid=1964701", "text": "Photoperiodism\n\nPhotoperiodism is the physiological reaction of organisms to the length of day or night. It occurs in plants and animals. Photoperiodism can also be defined as the developmental responses of plants to the relative lengths of light and dark periods.\n\nMany flowering plants (angiosperms) use a photoreceptor protein, such as phytochrome or cryptochrome, to sense seasonal changes in night length, or photoperiod, which they take as signals to flower. In a further subdivision, \"obligate\" photoperiodic plants absolutely require a long or short enough night before flowering, whereas \"facultative\" photoperiodic plants are more likely to flower under one condition.\n\nPhytochrome comes in two forms: pr and pfr. Red light (which is present during the day) converts phytochrome to its active form (pfr). This then triggers the plant to grow. In turn, far-red light is present in the shade or in the dark and this converts phytochrome from pfr to pr. Pr is the inactive form of phytochrome and will not allow for plant growth. This system of pfr to pr conversion allows the plant to sense when it is night and when it is day. Pfr can also be convereted back to Pr by a process known as dark reversion, where long periods of darkness trigger the conversion of Pfr. This is important in regards to plant flowering. Experiments by Halliday et al. showed that manipulations of the red-to far-red ratio in Arabidopsis can alter flowering. They discovered that plants tend to flower later when exposed to more red light, proving that red light is inhibitory to flowering. Other experiments have proven this by exposing plants to extra red-light in the middle of the night. A short-day plant will not flower if light is turned on for a few minutes in the middle of the night and a long-day plant can flower if exposed to more red-light in the middle of the night.\n\nCryptochromes are another type of photoreceptor that is important in photoperiodism. Cryptochromes absorb blue light and UV-A. Cryptochromes entrain the circadian clock to light. It has been found that both cryptochrome and phytochrome abundance relies on light and the amount of cryptochrome can change depending on day-length. This shows how important both of the photoreceptors are in regards to determining day-length.\n\nIn 1920, W. W. Garner and H. A. Allard published their discoveries on photoperiodism and felt it was the length of daylight that was critical, but it was later discovered that the length of the night was the controlling factor. Photoperiodic flowering plants are classified as \"long-day plants\" or \"short-day plants\" even though night is the critical factor because of the initial misunderstanding about daylight being the controlling factor. Along with long-day plants and short-day plants, there are plants that fall into a \"dual-day length category\". These plants are either long-short-day plants (LSDP) or short-long-day plants (SLDP). LSDPs flower after a series of long days followed by short days whereas SLDPs flower after a series of short days followed by long days. Each plant has a different length critical photoperiod, or critical night length.\n\nModern biologists believe that it is the coincidence of the active forms of phytochrome or cryptochrome, created by light during the daytime, with the rhythms of the circadian clock that allows plants to measure the length of the night. Other than flowering, photoperiodism in plants includes the growth of stems or roots during certain seasons and the loss of leaves. Artificial lighting can be used to induce extra-long days.\n\nLong-day plants flower when the night length falls below their critical photoperiod. These plants typically flower in the northern hemisphere during late spring or early summer as days are getting longer. In the northern hemisphere, the longest day of the year (summer solstice) is on or about 21 June. After that date, days grow shorter (i.e. nights grow longer) until 21 December (the winter solstice). This situation is reversed in the southern hemisphere (i.e., longest day is 21 December and shortest day is 21 June).\n\nSome long-day obligate plants are:\n\nSome long-day facultative plants are:\n\nShort-day plants flower when the night lengths exceed their critical photoperiod. They cannot flower under short nights or if a pulse of artificial light is shone on the plant for several minutes during the night; they require a continuous period of darkness before floral development can begin. Natural nighttime light, such as moonlight or lightning, is not of sufficient brightness or duration to interrupt flowering.\n\nIn general, short-day (i.e.long-night) plants flower as days grow shorter (and nights grow longer) after 21 June in the northern hemisphere, which is during summer or fall. The length of the dark period required to induce flowering differs among species and varieties of a species.\n\nPhotoperiodism affects flowering by inducing the shoot to produce floral buds instead of leaves and lateral buds.\n\nSome short-day facultative plants are:\n\n\nDay-neutral plants, such as cucumbers, roses, and tomatoes, do not initiate flowering based on photoperiodism. Instead, they may initiate flowering after attaining a certain overall developmental stage or age, or in response to alternative environmental stimuli, such as vernalisation (a period of low temperature).\n\nDaylength, and thus knowledge of the season of the year, is vital to many animals. A number of biological and behavioural changes are dependent on this knowledge. Together with temperature changes, photoperiod provokes changes in the color of fur and feathers, migration, entry into hibernation, sexual behaviour, and even the resizing of sexual organs.\n\nThe singing frequency of birds such as the canary depends on the photoperiod. In the spring, when the photoperiod increases (more daylight), the male canary's testes grow. As the testes grow, more androgens are secreted and song frequency increases. During autumn, when the photoperiod decreases (less daylight), the male canary's testes regress and androgen levels drop dramatically, resulting in decreased singing frequency. Not only is singing frequency dependent on the photoperiod but the song repertoire is also. The long photoperiod of spring results in a greater song repertoire. Autumn's shorter photoperiod results in a reduction in song repertoire. These behavioral photoperiod changes in male canaries are caused by changes in the song center of the brain. As the photoperiod increases, the high vocal center (HVC) and the robust nucleus of the archistriatum (RA) increase in size. When the photoperiod decreases, these areas of the brain regress.\n\nIn mammals, daylength is registered in the suprachiasmatic nucleus (SCN), which is informed by retinal light-sensitive ganglion cells, which are not involved in vision. The information travels through the retinohypothalamic tract (RHT). Some mammals are highly seasonal, while humans' seasonality is largely believed to be evolutionary baggage.\n\n\n", "id": "1964701", "title": "Photoperiodism"}
{"url": "https://en.wikipedia.org/wiki?curid=18952199", "text": "Xerophile\n\nA xerophile () is an extremophilic organism that can grow and reproduce in conditions with a low availability of water, also known as water activity. Water activity (a) is measured as the humidity above a substance relative to the humidity above pure water (Aw = 1.0). Xerophiles are \"xerotolerant\", meaning tolerant of dry conditions. They often can survive in environments with water activity below 0.8. Typically xerotolerance is used with respect to matric drying, where a substance has a low water concentration. These environments include arid desert soils. The term osmotolerance is typically applied to organisms that can grow in solutions with high solute concentrations (salts, sugars), such as halophiles.\n\nThe common food preservation method of reducing water activities may not prevent the growth of xerophilic organisms, often resulting in food spoilage. Some mold and yeast species are xerophilic. Mold growth on bread is an example of food spoilage by xerophilic organisms.\n\nExamples of xerophiles include \"Trichosporonoides nigrescens\" and cacti.\n\n", "id": "18952199", "title": "Xerophile"}
{"url": "https://en.wikipedia.org/wiki?curid=21780446", "text": "Species\n\nIn biology, a species is the basic unit of biological classification and a taxonomic rank, as well as a unit of biodiversity, but it has proven difficult to find a satisfactory definition. Scientists and conservationists need a species definition which allows them to work, regardless of the theoretical difficulties. If as Linnaeus thought, species were fixed, there would be no problem, but evolutionary processes cause species to change continually, and to grade into one another. A species is often defined as the largest group of organisms in which two individuals can produce fertile offspring, typically by sexual reproduction. While this definition is often adequate, when looked at more closely it is problematic. For example, with hybridisation, in a species complex of hundreds of similar microspecies, or in a ring species, the boundaries between closely related species become unclear. Among organisms that reproduce only asexually, the concept of a reproductive species breaks down, and each clone is potentially a microspecies. Problems also arise when dealing with fossils, since reproduction cannot be examined; the concept of the chronospecies is therefore used in palaeontology. Other ways of defining species include their karyotype, DNA sequence, morphology, behaviour or ecological niche.\n\nAll species are given a two-part name, a \"binomial\". The first part of a binomial is the genus to which the species belongs. The second part is called the specific name or the specific epithet (in botanical nomenclature, also sometimes in zoological nomenclature). For example, \"Boa constrictor\" is one of four species of the \"Boa\" genus.\n\nSpecies were seen from the time of Aristotle until the 18th century as fixed kinds that could be arranged in a hierarchy, the great chain of being. In the 19th century, biologists grasped that species could evolve given sufficient time. Charles Darwin's 1859 book \"The Origin of Species\" explained how species could arise by natural selection. That understanding was greatly extended in the 20th century through genetics and population ecology. Genetic variability arises from mutations and recombination, while organisms themselves are mobile, leading to geographical isolation and genetic drift with varying selection pressures. Genes can sometimes be exchanged between species by horizontal gene transfer; new species can arise rapidly through hybridisation and polyploidy; and species may become extinct for a variety of reasons. Viruses are a special case, driven by a balance of mutation and selection, and can be treated as quasispecies.\n\nAs a practical matter, species concepts may be used to define species that are then used to measure biodiversity, though whether this is a good measure is disputed, as other measures are possible.\n\nIn his biology, Aristotle used the term γένος (génos) to mean a kind, such as a bird or fish, and εἶδος (eidos) to mean a specific form within a kind, such as (within the birds) the crane, eagle, crow, or sparrow. These terms were translated into Latin as \"genus\" and \"species\", though they do not correspond to the Linnean terms thus named; today the birds are a class, the cranes are a family, and the crows a genus. A kind was distinguished by its attributes; for instance, a bird has feathers, a beak, wings, a hard-shelled egg, and warm blood. A form was distinguished by being shared by all its members, the young inheriting any variations they might have from their parents. Aristotle believed all kinds and forms to be distinct and unchanging. His approach remained influential until the Renaissance.\n\nWhen observers in the Early Modern period began to develop systems of organization for living things, they placed each kind of animal or plant into a context. Many of these early delineation schemes would now be considered whimsical: schemes included consanguinity based on colour (all plants with yellow flowers) or behaviour (snakes, scorpions and certain biting ants). John Ray (1686), an English naturalist, was the first to attempt a biological definition of the term \"species\", as follows:\n\nIn the 18th century, the Swedish scientist Carl Linnaeus classified organisms according to shared physical characteristics, and not simply based upon differences. He established the idea of a taxonomic hierarchy of classification based upon observable characteristics and intended to reflect natural relationships. At the time, however, it was still widely believed that there was no organic connection between species, no matter how similar they appeared. This view was influenced by European scholarly and religious education, which held that the categories of life are dictated by God, forming an Aristotelian hierarchy, the \"scala naturae\" or great chain of being. However, whether or not it was supposed to be fixed, the \"scala\" (a ladder) inherently implied the possibility of climbing.\n\nBy the 19th century, naturalists understood that species could change form over time, and that the history of the planet provided enough time for major changes. Jean-Baptiste Lamarck, in his 1809 \"Zoological Philosophy\", described the transmutation of species, proposing that a species could change over time, in a radical departure from Aristotelian thinking.\n\nIn 1859, Charles Darwin and Alfred Russel Wallace provided a compelling account of evolution and the formation of new species. Darwin argued that it was populations that evolved, not individuals, by natural selection from naturally occurring variation among individuals. This required a new definition of species. Darwin concluded that species are what they appear to be: ideas, provisionally useful for naming groups of interacting individuals, writing:\n\nThe commonly used names for kinds of organisms are often ambiguous: \"cat\" could mean the domestic cat, \"Felis catus\", or the cat family, Felidae. Another problem with common names is that they often vary from place to place, so that puma, cougar, catamount, panther, painter and mountain lion all mean \"Puma concolor\" in various parts of America, while \"panther\" may also mean the jaguar (\"Panthera onca\") of Latin America or the leopard (\"Panthera pardus\") of Africa and Asia. In contrast, the scientific names of species are chosen to be unique and universal; they are in two parts used together: the genus as in \"Puma\", and the specific epithet as in \"concolor\".\n\nA species is given a taxonomic name when a type specimen is described formally, in a publication that assigns it a unique scientific name. The description typically provides means for identifying the new species, differentiating it from other previously described and related or confusable species and provides a validly published name (in botany) or an available name (in zoology) when the paper is accepted for publication. The type material is usually held in a permanent repository, often the research collection of a major museum or university, that allows independent verification and the means to compare specimens. Describers of new species are asked to choose names that, in the words of the International Code of Zoological Nomenclature, are \"appropriate, compact, euphonious, memorable, and do not cause offence.\"\n\nBooks and articles sometimes intentionally do not identify species fully and use the abbreviation \"sp.\" in the singular or \"spp.\" (standing for \"species pluralis\", the Latin for multiple species) in the plural in place of the specific name or epithet (e.g. \"Canis\" sp.) This commonly occurs when authors are confident that some individuals belong to a particular genus but are not sure to which exact species they belong, as is common in paleontology. Authors may also use \"spp.\" as a short way of saying that something applies to many species within a genus, but not to all. If scientists mean that something applies to all species within a genus, they use the genus name without the specific name or epithet. The names of genera and species are usually printed in italics. Abbreviations such as \"sp.\" should not be italicised. When a species identity is not clear a specialist may use \"cf.\" before the epithet to indicate that confirmation is required. The abbreviations \"nr.\" (near) or \"aff.\" (affine) may be used when the identity is unclear but when the species appears to be similar to the species mentioned after.\n\nWith the rise of online databases, codes have been devised to provide identifiers for species that are already defined, including:\n\nThe naming of a particular species, including which genus (and higher taxa) it is placed in, is a \"hypothesis\" about the evolutionary relationships and distinguishability of that group of organisms. As further information comes to hand, the hypothesis may be confirmed or refuted. Sometimes, especially in the past when communication was more difficult, taxonomists working in isolation have given two distinct names to individual organisms later identified as the same species. When two named species are discovered to be of the same species, the older species name is given priority and usually retained, and the newer name considered as a junior synonym, a process called \"synonymisation\". Dividing a taxon into multiple, often new, taxa is called \"splitting\". Taxonomists are often referred to as \"lumpers\" or \"splitters\" by their colleagues, depending on their personal approach to recognising differences or commonalities between organisms.\n\nThe nomenclatural codes that guide the naming of species, including the ICZN for animals and the ICN for plants, do not make rules for defining the boundaries of the species. Research can change the boundaries, also known as circumscription, based on new evidence. Species may then need to be distinguished by the boundary definitions used, and in such cases the names may be qualified with \"sensu stricto\" (\"in the narrow sense\") to denote usage in the exact meaning given by an author such as the person who named the species, while the antonym \"sensu lato\" (\"in the broad sense\") denotes a wider usage, for instance including other subspecies. Other abbreviations such as \"auct.\" (\"author\") and \"non.\" (\"not\") may be used to further clarify the sense in which the specified authors delineated or described the species.\n\nMost modern textbooks make use of Ernst Mayr's 1942 definition, known as the Biological Species Concept as a basis for further discussion on the definition of species. It is also called a reproductive or isolation concept. This defines a species as\n\nIt has been argued that this definition is a natural consequence of the effect of sexual reproduction on the dynamics of natural selection. Mayr's use of the adjective \"potentially\" has been a point of debate; some interpretations exclude unusual or artificial matings that occur only in captivity, or that involve animals capable of mating but that do not normally do so in the wild.\n\nIt is difficult to define a species in a way that applies to all organisms. The debate about species delimitation is called the species problem. The problem was recognized even in 1859, when Darwin wrote in \"On the Origin of Species\":\n\nA simple textbook definition, following Mayr's concept, works well for most multi-celled organisms, but breaks down in several situations:\n\n\nSpecies identification is made difficult by discordance between molecular and morphological investigations; these can be categorized as two types: (i) one morphology, multiple lineages (e.g. morphological convergence, cryptic species) and (ii) one lineage, multiple morphologies (e.g. phenotypic plasticity, multiple life-cycle stages). In addition, horizontal gene transfer (HGT) makes it difficult to define a species. All species definitions assume that an organism acquires its genes from one or two parents very like the \"daughter\" organism, but that is not what happens in HGT. There is strong evidence of HGT between very dissimilar groups of prokaryotes, and at least occasionally between dissimilar groups of eukaryotes, including some crustaceans and echinoderms.\n\nThe evolutionary biologist James Mallet concludes that\n\nThe species concept is further weakened by the existence of microspecies, groups of organisms, including many plants, with very little genetic variability, usually forming species aggregates. For example, the dandelion \"Taraxacum officinale\" and the blackberry \"Rubus fruticosus\" are aggregates with many microspecies—perhaps 400 in the case of the blackberry and over 200 in the dandelion, complicated by hybridisation, apomixis and polyploidy, making gene flow between populations difficult to determine, and their taxonomy debatable. Species complexes occur in insects such as \"Heliconius\" butterflies, vertebrates such as \"Hypsiboas\" treefrogs, and fungi such as the fly agaric.\n\nNatural hybridisation presents a challenge to the concept of a reproductively isolated species, as fertile hybrids permit gene flow between two populations. For example, the carrion crow \"Corvus corone\" and the hooded crow \"Corvus cornix\" appear and are classified as separate species, yet they hybridise freely where their geographical ranges overlap.\n\nA ring species is a connected series of neighbouring populations, each of which can sexually interbreed with adjacent related populations, but for which there exist at least two \"end\" populations in the series, which are too distantly related to interbreed, though there is a potential gene flow between each \"linked\" population. Such non-breeding, though genetically connected, \"end\" populations may co-exist in the same region thus closing the ring. Ring species thus present a difficulty for any species concept that relies on reproductive isolation. However, ring species are at best rare. Proposed examples include the herring gull-lesser black-backed gull complex around the North pole, the \"Ensatina eschscholtzii\" group of 19 populations of salamanders in America, and the greenish warbler in Asia, but many so-called ring species have turned out to be the result of misclassification leading to questions on whether there really are any ring species.\n\nBiologists and taxonomists have made many attempts to define species, beginning from morphology and moving towards genetics. Early taxonomists such as Linnaeus had no option but to describe what they saw: this was later formalised as the typological or morphological species concept. Mayr emphasised reproductive isolation, but this, like other species concepts, is hard or even impossible to test. Later biologists have tried to refine Mayr's definition with the recognition and cohesion concepts, among others. Many of the concepts are quite similar or overlap, so they are not easy to count: the biologist R. L. Mayden recorded about 24 concepts, and the philosopher of science John Wilkins counted 26. Wilkins further grouped the species concepts into seven basic kinds of concepts: (1) agamospecies for asexual organisms (2) biospecies for reproductively isolated sexual organisms (3) ecospecies based on ecological niches (4) evolutionary species based on lineage (5) genetic species based on gene pool (6) morphospecies based on form or phenotype and (7) taxonomic species, a species as determined by a taxonomist.\n\nA typological species is a group of organisms in which individuals conform to certain fixed properties (a type), so that even pre-literate people often recognise the same taxon as do modern taxonomists. The clusters of variations or phenotypes within specimens (such as longer or shorter tails) would differentiate the species. This method was used as a \"classical\" method of determining species, such as with Linnaeus early in evolutionary theory. However, different phenotypes are not necessarily different species (e.g. a four-winged \"Drosophila\" born to a two-winged mother is not a different species). Species named in this manner are called \"morphospecies\".\n\nIn the 1970s, Robert R. Sokal, Theodore J. Crovello and Peter Sneath proposed a variation on this, a phenetic species, defined as a set of organisms with a similar phenotype to each other, but a different phenotype from other sets of organisms. It differs from the morphological species concept in including a numerical measure of distance or similarity to cluster entities based on multivariate comparisons of a reasonably large number of phenotypic traits.\n\nA mate-recognition species is a group of sexually reproducing organisms that recognize one another as potential mates. Expanding on this to allow for post-mating isolation, a cohesion species is the most inclusive population of individuals having the potential for phenotypic cohesion through intrinsic cohesion mechanisms; no matter whether populations can hybridize successfully, they are still distinct cohesion species if the amount of hybridization is insufficient to completely mix their respective gene pools. A further development of the recognition concept is provided by the biosemiotic concept of species.\n\nIn microbiology, genes can move freely even between distantly related bacteria, possibly extending to the whole bacterial domain. As a rule of thumb, microbiologists have assumed that kinds of Bacteria or Archaea with 16S ribosomal RNA gene sequences more similar than 97% to each other need to be checked by DNA-DNA hybridisation to decide if they belong to the same species or not. This concept was narrowed in 2006 to a similarity of 98.7%.\n\nDNA-DNA hybridisation is outdated, and results have sometimes led to misleading conclusions about species, as with the pomarine and great skua. Modern approaches compare sequence similarity using computational methods.\n\nDNA barcoding has been proposed as a way to distinguish species suitable even for non-specialists to use. The so-called barcode is a region of mitochondrial DNA within the gene for cytochrome c oxidase. A database, Barcode of Life Data Systems (BOLD) contains DNA barcode sequences from over 190,000 species. However, scientists such as Rob DeSalle have expressed concern that classical taxonomy and DNA barcoding, which they consider a misnomer, need to be reconciled, as they delimit species differently. Genetic introgression mediated by endosymbionts and other vectors can further make barcodes ineffective in the identification of species.\n\nA phylogenetic or cladistic species is an evolutionarily divergent lineage, one that has maintained its hereditary integrity through time and space. A cladistic species is the smallest group of populations that can be distinguished by a unique set of morphological or genetic traits. Molecular markers may be used to determine genetic similarities in the nuclear or mitochondrial DNA of various species. For example, in a study done on fungi, studying the nucleotide characters using cladistic species produced the most accurate results in recognising the numerous fungi species of all the concepts studied. Versions of the Phylogenetic Species Concept may emphasize monophyly or diagnosability.\n\nUnlike the Biological Species Concept, a cladistic species does not rely on reproductive isolation, so it is independent of processes that are integral in other concepts. It works for asexual lineages, and can detect recent divergences, which the Morphological Species Concept cannot. However, it does not work in every situation, and may require more than one polymorphic locus to give an accurate result. The concept may lead to splitting of existing species, for example of Bovidae, into many new ones.\n\nAn evolutionary species, suggested by George Gaylord Simpson in 1951, is \"an entity composed of organisms which maintains its identity from other such entities through time and over space, and which has its own independent evolutionary fate and historical tendencies\". This differs from the biological species concept in embodying persistence over time. Wiley and Mayden state that they see the evolutionary species concept as \"identical\" to Willi Hennig's species-as-lineages concept, and assert that the biological species concept, \"the several versions\" of the phylogenetic species concept, and the idea that species are of the same kind as higher taxa are not suitable for biodiversity studies (with the intention of estimating the number of species accurately). They further suggest that the concept works for both asexual and sexually-reproducing species.\n\nAn ecological species is a set of organisms adapted to a particular set of resources, called a niche, in the environment. According to this concept, populations form the discrete phenetic clusters that we recognise as species because the ecological and evolutionary processes controlling how resources are divided up tend to produce those clusters.\n\nA genetic species as defined by Robert Baker and Robert Bradley is a set of genetically isolated interbreeding populations. This is similar to Mayr's Biological Species Concept, but stresses genetic rather than reproductive isolation. In the 21st century, a genetic species can be established by comparing DNA sequences, but other methods were available earlier, such as comparing karyotypes (sets of chromosomes) and allozymes (enzyme variants).\n\nAn evolutionarily significant unit (ESU) or \"wildlife species\" is a population of organisms considered distinct for purposes of conservation.\n\nIn palaeontology, with only comparative anatomy (morphology) from fossils as evidence, the concept of a chronospecies can be applied. During anagenesis (evolution, not necessarily involving branching), palaeontologists seek to identify a sequence of species, each one derived from the one before through continuous, slow and more or less uniform change. In such a time sequence, palaeontologists assess how much change is required for a morphologically distinct form to be considered a different species from its ancestors.\n\nViruses have enormous populations, are doubtfully living since they consist of little more than a string of DNA or RNA in a protein coat, and mutate rapidly. All of these factors make conventional species concepts largely inapplicable. A viral quasispecies is a group of genotypes related by similar mutations, competing within a highly mutagenic environment, and hence governed by a mutation–selection balance. It is predicted that a viral quasispecies at a low but evolutionarily neutral and highly connected (that is, flat) region in the fitness landscape will outcompete a quasispecies located at a higher but narrower fitness peak in which the surrounding mutants are unfit, \"the quasispecies effect\" or the \"survival of the flattest\". There is no suggestion that a viral quasispecies resembles a traditional biological species.\n\nSpecies are subject to change, whether by evolving into new species, exchanging genes with other species, merging with other species or by becoming extinct.\n\nThe evolutionary process by which biological populations evolve to become distinct or reproductively isolated as species is called speciation. Charles Darwin was the first to describe the role of natural selection in speciation in his 1859 book \"The Origin of Species\". Speciation depends on a measure of reproductive isolation, a reduced gene flow. This occurs most easily in allopatric speciation, where populations are separated geographically and can diverge gradually as mutations accumulate. Reproductive isolation is threatened by hybridisation, but this can be selected against once a pair of populations have incompatible alleles of the same gene, as described in the Bateson–Dobzhansky–Muller model. A different mechanism, phyletic speciation, involves one lineage gradually changing over time into a new and distinct form, without increasing the number of resultant species.\n\nHorizontal gene transfer between organisms of different species, either through hybridisation, antigenic shift, or reassortment, is sometimes an important source of genetic variation. Viruses can transfer genes between species. Bacteria can exchange plasmids with bacteria of other species, including some apparently distantly related ones in different phylogenetic domains, making analysis of their relationships difficult, and weakening the concept of a bacterial species.\n\nLouis-Marie Bobay and Howard Ochman suggest, based on analysis of the genomes of many types of bacteria, that they can often be grouped \"into communities that regularly swap genes\", in much the same way that plants and animals can be grouped into reproductively isolated breeding populations. Bacteria may thus form species, analogous to Mayr's biological species concept, consisting of asexually reproducing populations that exchange genes by homologous recombination.\n\nA species is extinct when the last individual of that species dies, but it may be functionally extinct well before that moment. It is estimated that over 99 percent of all species that ever lived on Earth, some five billion species, are now extinct. Some of these were in mass extinctions such as those at the ends of the Permian, Triassic and Cretaceous periods. Mass extinctions had a variety of causes including volcanic activity, climate change, and changes in oceanic and atmospheric chemistry, and they in turn had major effects on Earth's ecology, atmosphere, land surface, and waters. Another form of extinction is through the assimilation of one species by another through hybridization. The resulting single species has been termed as a \"compilospecies\".\n\nBiologists and conservationists need to categorise and identify organisms in the course of their work. Difficulty assigning organisms reliably to a species constitutes a threat to the validity of research results, for example making measurements of how abundant a species is in an ecosystem moot. Paul Michael-Agapow and colleagues found that surveys using a phylogenetic species concept reported 48% more species and accordingly smaller populations and ranges than those using nonphylogenetic concepts; they note that this \"taxonomic inflation\" could cause a false appearance of change to the number of endangered species and consequent political and practical difficulties. The evolutionary biologist Jody Hey observes that there is an inherent conflict between the desire to understand the processes of speciation and the need to identify and to categorise. Conservation laws in many countries make special provisions to prevent species from going extinct. Hybridization zones between two species, one that is protected and one that is not, have sometimes led to conflicts between law-makers, land owners and conservationists. One of the classic cases in North America is that of the protected northern spotted owl which hybridizes with the unprotected California spotted owl and the barred owl; this has led to legal debates.\n\nThe botanist Brent D. Mishler states that the species problem is created by the many ways that people want to use the species category, but argues that the solution is to abandon the traditional ranks and just use monophyletic groups of different inclusivess. A species is then \"simply the least inclusive taxon\" of whatever type. He argues that ecology, evolution, and conservation work are all better served by this approach. Since in his view species are not comparable, counting them is not a valid measure of biodiversity, and he calls for new ways to measure it, noting that other authors such as R. I. Vane-Wright and colleagues have proposed quantitative measures for phylogenetic biodiversity that make use of \"the number of branch points, and possibly branch lengths, separating the tips on the tree\".\n\n\n", "id": "21780446", "title": "Species"}
{"url": "https://en.wikipedia.org/wiki?curid=2953441", "text": "Canopy (biology)\n\nIn biology, the canopy is the aboveground portion of a plant community or crop, formed by the collection of individual plant crowns.\n\nIn forest ecology, canopy also refers to the upper layer or habitat zone, formed by mature tree crowns and including other biological organisms (epiphytes, lianas, arboreal animals, etc.).\n\nSometimes the term canopy is used to refer to the extent of the outer layer of leaves of an individual tree or group of trees. Shade trees normally have a dense canopy that blocks light from lower growing plants.\n\nCanopy structure is the organization or spatial arrangement (three-dimensional geometry) of a plant canopy. Leaf Area Index (LAI), leaf area per unit ground area, is a key measure used to understand and compare plant canopies. It is also taller than the understory layer.\n\nDominant and co-dominant canopy trees form the uneven canopy layer. Canopy trees are able to photosynthesize relatively rapidly due to abundant light, so it supports the majority of primary productivity in forests. The canopy layer provides protection from strong winds and storms, while also intercepting sunlight and precipitation, leading to a relatively sparsely vegetated understory layer.\n\nForest canopies are home to unique flora and fauna not found in other layers of forests. The highest terrestrial biodiversity resides in the canopy of tropical rainforests. Many rainforest animals have evolved to live solely in the canopy, and never touch the ground.\n\nThe canopy of a rainforest is typically about 10m thick, and intercepts around 95% of sunlight. The canopy is below the emergent layer, a sparse layer of very tall trees, typically one or two per hectare. With an abundance of water and a near ideal temperature in rainforests, light and nutrients are two factors that limit tree growth from the understory to the canopy.\n\nIn the permaculture and forest gardening community, the canopy is the highest of seven layers.\n\n\n\n", "id": "2953441", "title": "Canopy (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=21392957", "text": "Phototaxis\n\nPhototaxis is a kind of taxis, or locomotory movement, that occurs when a whole organism moves towards or away from stimulus of light. This is advantageous for phototrophic organisms as they can orient themselves most efficiently to receive light for photosynthesis. Phototaxis is called positive if the movement is in the direction of increasing light intensity and negative if the direction is opposite.\n\nTwo types of positive phototaxis are observed in prokaryotes. The first is called scotophobotaxis (from the word \"scotophobia\"), which is observed only under a microscope. This occurs when a bacterium swims by chance out of the area illuminated by the microscope. Entering darkness signals the cell to reverse flagella rotation direction and reenter the light. The second type of phototaxis is true phototaxis, which is a directed movement up a gradient to an increasing amount of light. This is analogous to positive chemotaxis except that the attractant is light rather than a chemical.\n\nPhototactic responses are observed in many organisms such as \"Serratia marcescens\", \"Tetrahymena\", and \"Euglena\". Each organism has its own specific biological cause for a phototactic response, many of which are incidental and serve no end purpose.\n\nPhototaxis in zooplankton is well studied in the marine annelid \"Platynereis dumerilii\":\n\n\"Platynereis dumerilii\" trochophore and metatrochophore larvae are positively phototactic. Phototaxis there is mediated by simple eyespots that consists of a pigment cell and a photoreceptor cell. The photoreceptor cell synapses directly onto ciliated cells, which are used for swimming. The eyespots do not give spatial resolution, therefore the larvae are rotating to scan their environment for the direction where the light is coming from.\n\n\"Platynereis dumerilii\" nectochate larvae can switch between positive and negative phototaxis. Phototaxis there is mediated by two pairs of more complex pigment cup eyes. These eyes contain more photoreceptor cells that are shaded by pigment cells forming a cup. The photoreceptor cells do not synapse directly onto ciliated cells or muscle cells but onto inter-neurons of a processing center. This way the information of all four eye cups can be compared and a low resolution image of four pixels can be created telling the larvae where the light is coming from. This way the larva does not need to scan its environment by rotating. This is an adaption for living on the bottom of the sea the life style of the nectochaete larva while scanning rotation is more suited for living in the open water column, the life style of the trochophore larva. Phototaxis in the \"Platynereis dumerilii\" nectochate larva has a broad spectral range which is at least covered by three opsins that are expressed by the cup eyes: Two rhabdomeric opsins and a Go-opsin.\n\n\n", "id": "21392957", "title": "Phototaxis"}
{"url": "https://en.wikipedia.org/wiki?curid=230342", "text": "Keystone species\n\nA keystone species is a species that has a disproportionately large effect on its environment relative to its abundance. Such species are described as playing a critical role in maintaining the structure of an ecological community, affecting many other organisms in an ecosystem and helping to determine the types and numbers of various other species in the community. A keystone species is a plant or animal that plays a unique and crucial role in the way an ecosystem functions. Without keystone species, the ecosystem would be dramatically different or cease to exist altogether.\n\nThe role that a keystone species plays in its ecosystem is analogous to the role of a keystone in an arch. While the keystone is under the least pressure of any of the stones in an arch, the arch still collapses without it. Similarly, an ecosystem may experience a dramatic shift if a keystone species is removed, even though that species was a small part of the ecosystem by measures of biomass or productivity.\nIt became a popular concept in conservation biology. Although the concept is valued as a descriptor for particularly strong inter-species interactions, and it has allowed easier communication between ecologists and conservation policy-makers, it has been criticized for oversimplifying complex ecological systems.\n\nThe concept of the keystone species was introduced in 1969 by Robert T. Paine, a professor of zoology at the University of Washington. Paine developed the concept to explain his observations and experiments on the relationship between intertidal invertebrates. In his 1966 paper, \"Food Web Complexity and Species Diversity\", Paine described such a system in Makah Bay in Washington.\nIn his follow-up 1969 paper, Paine proposed the keystone species concept, using \"Pisaster ochraceus\", a species of starfish, and \"Mytilus californianus\", a species of mussel, as a primary example.\nThe concept became popular in conservation, and was deployed in a range of contexts and mobilized to engender support for conservation. Paine's contribution to the ecological theory has been summarized in a HHMI documentary. \n\nGiven that there are many historical definitions of the keystone species concept, and without a consensus on its exact definition, a list of examples best illustrates the concept of keystone species.\n\nA classic keystone species is a small predator that prevents a particular herbivorous species from eliminating dominant plant species. Since the prey numbers are low, the keystone predator numbers can be even lower and still be effective. Yet without the predators, the herbivorous prey would explode in numbers, wipe out the dominant plants, and dramatically alter the character of the ecosystem. The exact scenario changes in each example, but the central idea remains that through a chain of interactions, a non-abundant species has an out-sized impact on ecosystem functions. One example is the herbivorous weevil \"Euhrychiopsis lecontei\" and its suggested keystone effects on aquatic plant species diversity by foraging on nuisance Eurasian watermilfoil.\n\nSimilarly, the wasp species \"Agelaia vicina\" has been labeled a keystone species due to its unparalleled nest size, colony size, and high rate of brood production. The diversity of its prey and the quantity necessary to sustain its high rate of growth have a direct impact on local neighboring species.\n\nAs was described by Dr. Robert Paine in his 1966 paper, some sea stars (e.g., \"Pisaster ochraceus\") may prey on sea urchins, mussels, and other shellfish that have no other natural predators. If the sea star is removed from the ecosystem, the mussel population explodes uncontrollably, driving out most other species.\n\nSimilarly, sea otters protect kelp forests from damage by consuming sea urchins. Kelp \"roots\", called holdfasts, are merely anchors, and do not perform similar roles to the roots of terrestrial plants, which form large networks that acquire nutrients from the soil. In the absence of sea otters, sea urchins are released from predation pressure, increasing in abundance. Sea urchins rapidly consume nearshore kelp, severing the structures at the base. Where sea otters are present, sea urchins tend to be small and limited to crevices. Large nearshore kelp forests proliferate and serve as important habitat for a number of other species. Kelp also increase the productivity of the nearshore ecosystem through the addition of large quantities of secondary production.\nThese creatures need not be apex predators. Sea stars are prey for sharks, rays, and sea anemones. Sea otters are prey for orca.\n\nThe jaguar, whose numbers in Central and South America have been classified as near threatened, acts as a keystone predator by its widely varied diet, helping to balance the mammalian jungle ecosystem with its consumption of 87 different species of prey. The gray wolf is another known keystone species, as is the lion.\n\nKeystone mutualists are organisms that participate in mutually beneficial interactions, the loss of which would have a profound impact upon the ecosystem as a whole. For example, in the Avon Wheatbelt region of Western Australia, there is a period of each year when \"Banksia prionotes\" (acorn banksia) is the sole source of nectar for honeyeaters, which play an important role in pollination of numerous plant species. Therefore, the loss of this one species of tree would probably cause the honeyeater population to collapse, with profound implications for the entire ecosystem. Another example is frugivores such as the cassowary, which spreads the seeds of many different trees, and some will not grow unless they have been through a cassowary.\n\nAlthough the terms 'keystone' and 'engineer' are used interchangeably, they are not synonyms. In North America, the prairie dog is an ecosystem engineer. Prairie dog burrows provide the nesting areas for mountain plovers and burrowing owls. Prairie dog tunnel systems also help channel rainwater into the water table to prevent runoff and erosion, and can also serve to change the composition of the soil in a region by increasing aeration and reversing soil compaction that can be a result of cattle grazing. Prairie dogs also trim the vegetation around their colonies, perhaps to remove any cover for predators. Even grazing species such as plains bison, pronghorn, and mule deer have shown a proclivity for grazing on the same land used by prairie dogs. It is believed that they prefer the plant community which results after prairie dogs have foraged through the area.\n\nAnother well known ecosystem engineer and keystone species is the beaver, which transforms its territory from a stream to a pond or swamp. Beavers affect the environment first altering the edges of riparian areas by cutting down older trees to use for their dams. This allows younger trees to take their place. Beaver dams alter the riparian area they are established in. Depending on topography, soils, and many factors, these dams change the riparian edges of streams and rivers into wetlands, meadows, or riverine forests. These dams have shown to be beneficial to a myriad of species including amphibians, salmon, and song birds.\n\nIn the African savanna, the larger herbivores, especially the elephants, shape their environment. The elephants destroy trees, making room for the grass species. Without these animals, much of the savanna would turn into woodland.\nAustralian studies have found the parrotfish on the Great Barrier Reef is the sole species, within thousands of species of reef fish, that consistently scrapes and cleans the coral on the reef. Without these animals, the Great Barrier Reef would be under severe strain.\n\n", "id": "230342", "title": "Keystone species"}
{"url": "https://en.wikipedia.org/wiki?curid=394815", "text": "Introduced species\n\nAn introduced species (alien species, exotic species, non-indigenous species, or non-native species) is a species living outside its native distributional range, which has arrived there by human activity, either deliberate or accidental. Non-native species can have various effects on the local ecosystem. Introduced species that become established and spread beyond the place of introduction are called invasive species. The impact of introduced species is highly variable. Some have a negative effect on a local ecosystem, while other introduced species may have no negative effect or only minor impact. Some species have been introduced intentionally to combat pests. They are called biocontrols and may be regarded as beneficial as an alternative to pesticides in agriculture for example. In some instances the potential for being beneficial or detrimental in the long run remains unknown. \n\nThe effects of introduced species on natural environments have gained much scrutiny from scientists, governments, farmers and others.\n\nFormal definition from the United States Environmental Protection Agency: \"A species that has been intentionally or inadvertently brought into a region or area. Also called an exotic or non-native species.\"\n\nThere are many terms associated with introduced species that represent subsets of introduced species, and the terminology associated with introduced species is now in flux for various reasons. Examples of these terms are \"acclimatized\", \"adventive\", \"naturalized\", and \"immigrant\" species but those terms refer to a subset of introduced species. The term \"invasive\" is used to describe introduced species when the introduced species causes substantial damage to the area in which it was introduced.\n\nSubset descriptions:\n\nGeneral description of introduced species:\n\nIn the broadest and most widely used sense, an introduced species is synonymous with \"non-native\" and therefore applies as well to most garden and farm organisms; these adequately fit the basic definition given above. However, some sources add to that basic definition \"and are now reproducing in the wild\", which removes from consideration as \"introduced\" species that were raised or grown in gardens or farms that do not survive without tending by people. With respect to plants, these latter are in this case defined as either \"ornamental\" or \"cultivated\" plants.\n\nIntroduction of a species outside its native range is all that is required to be qualified as an \"introduced species\" such that one can distinguish between introduced species that may not occur except in cultivation, under domestication or captivity whereas others become established outside their native range and reproduce without human assistance. Such species might be termed \"naturalized\", \"established\", \"wild non-native species\". If they further spread beyond the place of introduction and cause damage to nearby species, they are called \"invasive\". The transition from introduction, to establishment and to invasion has been described in the context of plants. Introduced species are essentially \"non-native\" species. Invasive species are those introduced species that spreadwidely or quickly and cause harm, be that to the environment, human health, other valued resources or the economy. There have been calls from scientists to consider a species \"invasive\" only in terms of their spread and reproduction rather than the harm they may cause.\n\nAccording to a practical definition, an invasive species is one that has been introduced and become a pest in its new location, spreading (invading) by natural means. The term is used to imply both a sense of urgency and actual or potential harm. For example, U.S. Executive Order 13112 (1999) defines \"invasive species\" as \"an alien species whose introduction does or is likely to cause economic or environmental harm or harm to human health\". The biological definition of invasive species, on the other hand, makes no reference to the harm they may cause, only to the fact that they spread beyond the area of original introduction.\n\nAlthough some argue that \"invasive\" is a loaded word and harm is difficult to define, the fact of the matter is that organisms have and continue to be introduced to areas in which they are not native, sometimes with but usually without much regard to the harm that could result.\n\nFrom a regulatory perspective, it is neither desirable nor practical to list as undesirable or outright ban all non-native species (although the State of Hawaii has adopted an approach that comes close to this). Regulations require a definitional distinction between non-natives that are deemed especially onerous and all others. Introduced \"pest species\" that are \"officially listed\" as invasive, best fit the definition of an \"invasive species\". Early detection and rapid response is the most effective strategy for regulating a pest species and reducing economic and environmental impacts of an introduction \n\nIn Great Britain, the Wildlife and Countryside Act 1981 prevents the introduction of any animal not naturally occurring in the wild or any of a list of both animals or plants introduced previously and proved to be invasive.\n\nBy definition, a species is considered \"introduced\" when its transport into an area outside of its native range is human mediated. Introductions by humans can be described as either intentional or accidental. Intentional introductions have been motivated by individuals or groups who either (1) believe that the newly introduced species will be in some way beneficial to humans in its new location or, (2) species are introduced intentionally but with no regard to the potential impact. Unintentional or accidental introductions are most often a byproduct of human movements, and are thus unbound to human motivations. Subsequent range expansion of introduced species may or may not involve human activity.\n\nSpecies that humans intentionally transport to new regions can subsequently become successfully established in two ways. In the first case, organisms are purposely released for establishment in the wild. It is sometimes difficult to predict whether a species will become established upon release, and if not initially successful, humans have made repeated introductions to improve the probability that the species will survive and eventually reproduce in the wild. In these cases it is clear that the introduction is directly facilitated by human desires.\n\nIn the second case, species intentionally transported into a new region may escape from captive or cultivated populations and subsequently establish independent breeding populations. Escaped organisms are included in this category because their initial transport to a new region is human motivated.\n\nEconomic: Perhaps the most common motivation for introducing a species into a new place is that of economic gain. Non-native species can become such a common part of an environment, culture, and even diet that little thought is given to their geographic origin. For example, soybeans, kiwi fruit, wheat, honey bees, and all livestock except the American bison and the turkey are non-native species to North America. Collectively, non-native crops and livestock comprise 98% of US food. These and other benefits from non-natives are so vast that, according to the Congressional Research Service, they probably exceed the costs. \n\nOther examples of species introduced for the purposes of benefiting agriculture, aquaculture or other economic activities are widespread. Eurasian carp was first introduced to the United States as a potential food source. The apple snail was released in Southeast Asia with the intent that it be used as a protein source, and subsequently to places like Hawaii to establish a food industry. In Alaska, foxes were introduced to many islands to create new populations for the fur trade. About twenty species of African and European dung beetles have established themselves in Australia after deliberate introduction by the Australian Dung Beetle Project in an effort to reduce the impact of livestock manure. The timber industry promoted the introduction of Monterey pine (\"Pinus radiata\") from California to Australia and New Zealand as a commercial timber crop. These examples represent only a small subsample of species that have been moved by humans for economic interests.\n\nThe rise in the use of genetically modified organisms has added another potential economic advantage to introducing new/modified species into different environments. Companies such as Monsanto that earn much of their profit through the selling of genetically modified seeds has added to the controversy surrounding introduced species. The effect of genetically modified organisms varies from organism to organism and is still being researched today, however the rise of genetically modified organisms has added complexity to the conversations surrounding introduced species.\n\nIntroductions have also been important in supporting recreation activities or otherwise increasing human enjoyment. Numerous fish and game animals have been introduced for the purposes of sport fishing and hunting (earthworms as invasive species). The introduced amphibian (\"Ambystoma tigrinum\") that threatens the endemic California salamander (\"Ambystoma californiense\") was introduced to California as a source of bait for fishermen. Pet animals have also been frequently transported into new areas by humans, and their escapes have resulted in several successful introductions, such as those of feral cats and parrots.\n\nMany plants have been introduced with the intent of aesthetically improving public recreation areas or private properties. The introduced Norway maple for example occupies a prominent status in many of Canada's parks. The transport of ornamental plants for landscaping use has and continues to be a source of many introductions. Some of these species have escaped horticultural control and become invasive. Notable examples include water hyacinth, salt cedar, and purple loosestrife\n\nIn other cases, species have been translocated for reasons of \"cultural nostalgia,\" which refers to instances in which humans who have migrated to new regions have intentionally brought with them familiar organisms. Famous examples include the introduction of starlings to North America by Englishman Eugene Schieffelin, a lover of the works of Shakespeare and the chairman of the American Acclimatization Society, who, it is rumoured, wanted to introduce all of the birds mentioned in Shakespeare's plays into the United States. He deliberately released eighty starlings into Central Park in New York City in 1890, and another forty in 1891.\n\nYet another prominent example of an introduced species that became invasive is the European rabbit in Australia. Thomas Austin, a British landowner had rabbits released on his estate in Victoria because he missed hunting them. A more recent example is the introduction of the common wall lizard to North America by a Cincinnati boy, George Rau, around 1950 after a family vacation to Italy.\n\nIntentional introductions have also been undertaken with the aim of ameliorating environmental problems. A number of fast spreading plants such as kudzu have been introduced as a means of erosion control. Other species have been introduced as biological control agents to control invasive species and involves the purposeful introduction of a natural enemy of the target species with the intention of reducing its numbers or controlling its spread.\n\nA special case of introduction is the reintroduction of a species that has become locally endangered or extinct, done in the interests of conservation. Examples of successful reintroductions include wolves to Yellowstone National Park in the U.S., and the red kite to parts of England and Scotland. Introductions or translocations of species have also been proposed in the interest of genetic conservation, which advocates the introduction of new individuals into genetically depauperate populations of endangered or threatened species.\n\nThe above examples highlight the intent of humans to introduce species as a means of incurring some benefit. While these benefits have in some cases been realized, introductions have also resulted in unforeseen costs, particularly when introduced species take on characteristics of invasive species.\n\nUnintentional introductions occur when species are transported by human vectors. Increasing rates of human travel are providing accelerating opportunities for species to be accidentally transported into areas in which they are not considered native. For example, three species of rat (the black, Norway and Polynesian) have spread to most of the world as hitchhikers on ships. There are also numerous examples of marine organisms being transported in ballast water, one being the zebra mussel. Over 200 species have been introduced to the San Francisco Bay in this manner making it the most heavily invaded estuary in the world. There is also the accidental release of the Africanized honey bees (AHB), known colloquially as \"killer bees\" or Africanized bee to Brazil in 1957 and the Asian carps to the United States. The insect commonly known as the brown marmorated stink bug (\"Halyomorpha halys\") was introduced accidentally in Pennsylvania. Another form of unintentional introductions is when an intentionally introduced plant carries a parasite or herbivore with it. Some become invasive, for example the oleander aphid, accidentally introduced with the ornamental plant, oleander.\n\nMost accidentally or intentionally introduced species do not become invasive as the ones mentioned above. For instance Some 179 coccinellid species have been introduced to the U.S. and Canada; about 27 of these non-native species have become established, and only a handful can be considered invasive, including the intentionally introduced \"Harmonia axyridis\", multicolored Asian lady beetle. However the small percentage of introduced species that become invasive can produce profound ecological changes. In North America \"Harmonia axyridis\" has become the most abundant lady beetle and probably accounts for more observations than all the native lady beetles put together.\n\nMany non-native plants have been introduced into new territories, initially as either ornamental plants or for erosion control, stock feed, or forestry. Whether an exotic will become an invasive species is seldom understood in the beginning, and many non-native ornamentals languish in the trade for years before suddenly naturalizing and becoming invasive.\n\nPeaches, for example, originated in China, and have been carried to much of the populated world. Tomatoes are native to the Andes. Squash (pumpkins), maize (corn), and tobacco are native to the Americas, but were introduced to the Old World. Many introduced species require continued human intervention to survive in the new environment. Others may become feral, but do not seriously compete with natives, but simply increase the biodiversity of the area.\n\nDandelions are also introduced species to North America.\n\nA very troublesome marine species in southern Europe is the seaweed \"Caulerpa taxifolia\". \"Caulerpa\" was first observed in the Mediterranean Sea in 1984, off the coast of Monaco. By 1997, it had covered some 50 km². It has a strong potential to overgrow natural biotopes, and represents a major risk for sublittoral ecosystems. The origin of the alga in the Mediterranean was thought to be either as a migration through the Suez Canal from the Red Sea, or as an accidental introduction from an aquarium. Another troublesome plant species is the terrestrial plant \"Phyla canescens,\" which was intentionally introduced into many countries in North America, Europe, and Africa as an ornamental plant. This species has become invasive in Australia, where it threatens native rare plants and causes erosion and soil slumping around river banks. It has also become invasive in France where it has been listed as an invasive plant species of concern in the Mediterranean region, where it can form monocultures that threaten critical conservation habitats.\n\nJapanese knotweed grows profusely in many nations. Human beings introduced it into many places in the 19th century. It is a source of resveratrol, a dietary supplement.\n\nBear in mind that most introduced species do not become invasive. Examples of introduced animals that have become invasive include the gypsy moth in eastern North America, the zebra mussel and alewife in the Great Lakes, the Canada goose and gray squirrel in Europe, the muskrat in Europe and Asia, the cane toad and red fox in Australia, nutria in North America, Eurasia, and Africa, and the common brushtail possum in New Zealand. In Taiwan, the success of introduced bird species was related to their native range size and body size; larger species with larger native range sizes were found to have larger introduced range sizes.\n\nSome species, such as the brown rat, house sparrow, ring-necked pheasant and European starling, have been introduced very widely. In addition there are some agricultural and pet species that frequently become feral; these include rabbits, dogs, goats, fish, pigs and cats.\n\nWhen a new species is introduced, the species could potentially breed with members of native species, producing hybrids. The effect of the creating of hybrids can range from having little effect, a negative effect, to having devastating effects on native species. Potential negative effects include hybrids that are less fit for their environment resulting in a population decrease. This was seen in the Atlantic Salmon population when high levels of escape from Atlantic Salmon farms into the wild populations resulted in hybrids that had reduced survival. Potential positive effects include adding to the genetic diversity of the population which can increase the adaptation ability of the population and increase the number of healthy individuals within a population. This was seen in the introduction of guppies in Trinidad to encourage population growth and introduce new alleles into the population. The results of this introduction included increased levels of heterozygosity and a larger population size.\n\nIt has been hypothesized that invasive species of microbial life could contaminate a planetary body after the former is introduced by a space probe or spacecraft, either deliberately or unintentionally.\n\n", "id": "394815", "title": "Introduced species"}
{"url": "https://en.wikipedia.org/wiki?curid=19867926", "text": "Adventive species\n\nAn adventive species is a species that has arrived in a new locality. It may have had help from humans as an introduced species or it may not.\n\nThe term is now used by some writers in a more restricted sense than its initial usage. The earliest and most widespread concept among biologists is that of a species that has arrived in a specific geographic area from a different region (without further caveats). This is the forerunner of the term 'non-indigenous species', although it lacks the frequently invoked basis of the word 'introduced', which means different things to different writers. In this sense, the cattle egret (\"Bubulcus ibis\"), which arrived in North America by natural range expansion, the black rat (\"Rattus rattus\"), which is believed to have arrived as a hitchhiker aboard ships, and the kudzu vine (\"Pueraria lobata\"), which was introduced deliberately by humans, are all adventive species and have established populations. Common adventive species include herbivorous insects.\n\nThe later and more limited concept is that of a species that has arrived in a specific geographic area from a different region, but its population is not self-sustaining. Population numbers are only increased through re-introduction. After some time, an adventive species may become naturalized; or, some populations do not sustain themselves reproductively, but exist because of continued influx from elsewhere. Such a non-sustaining population, or the individuals within it, are said to be adventive. Cultivated plants are a major source of adventive populations. It is estimated that 10-20% of adventive species used in biological control programs eventually become naturalized.\n\nWe can readily see how this second (later) concept applies to cultivated plants. Those that grow within the confines of culture are ‘adventive’; those that grow outside those confines are ‘naturalized’. But the concept falls apart when applied to the far more numerous species of invertebrate animals and microorganisms: extremely few of these are cultured, and by the time they are detected in nature they tend to be established (‘naturalized’).\n\n", "id": "19867926", "title": "Adventive species"}
{"url": "https://en.wikipedia.org/wiki?curid=185901", "text": "Subspecies\n\nIn biological classification, subspecies (abbreviated \"subsp.\" or \"ssp.\"; \"plural\": \"subspecies\") is either a taxonomic rank subordinate to species, or a taxonomic unit in that rank. A subspecies cannot be recognized independently: a species will either be recognized as having no subspecies at all or at least two (including any that are extinct).\n\nIn zoology, under the International Code of Zoological Nomenclature, the subspecies is the only taxonomic rank below that of species that can receive a name. In botany and mycology, under the International Code of Nomenclature for algae, fungi, and plants, other infraspecific ranks, such as variety, may be named. In bacteriology and virology, under standard bacterial nomenclature and virus nomenclature, there are recommendations but not strict requirements for recognizing other important infraspecific ranks.\n\nA taxonomist decides whether to recognize a subspecies or not. A common way to decide is that organisms belonging to different subspecies of the same species are capable of interbreeding and producing fertile offspring, but they do not usually interbreed in nature due to geographic isolation, sexual selection, or other factors. The differences between subspecies are usually less distinct than the differences between species.\n\nIn zoology, the \"International Code of Zoological Nomenclature\" (4th edition, 1999) accepts only one rank below that of species, namely the rank of subspecies. Other groupings, \"infrasubspecific entities\" do not have names regulated by the \"ICZN\". Such forms have no official \"ICZN\" status, though they may be useful in describing altitudinal or geographical clines, pet breeds, transgenic animals, etc. While the scientific name of a species is a binomen, the scientific name of a subspecies is a trinomen - a binomen followed by a subspecific name. A tiger's binomen is \"Panthera tigris\", so for a Sumatran tiger the trinomen is, for example, \"Panthera tigris sumatrae\".\n\nIn bacteriology, the only rank below species that is regulated explicitly by the code of nomenclature is \"subspecies\", but infrasubspecific taxa are extremely important in bacteriology; Appendix 10 of the code lays out some recommendations that are intended to encourage uniformity in describing such taxa. Names published before 1992 in the rank of \"variety\" are taken to be names of subspecies (see \"International Code of Nomenclature of Bacteria\").\n\nIn botany, subspecies is one of many ranks below that of species, such as variety, subvariety, form, and subform. The subspecific name is preceded by \"subsp.\" or \"ssp.\", as \"Schoenoplectus californicus\" ssp. \"tatora\" (totora). A botanical name consists of at most three parts. An infraspecific name includes the species binomial, and one infraspecific epithet, such as subspecies or variety.\n\nIn zoological nomenclature, when a species is split into subspecies, the originally described population is retained as the \"nominotypical subspecies\" or \"nominate subspecies\", which repeats the same name as the species. For example, \"Motacilla alba alba\" (often abbreviated \"Motacilla a. alba\") is the nominotypical subspecies of the white wagtail (\"Motacilla alba\").\n\nThe subspecies name that repeats the species name is referred to in botanical nomenclature as the subspecies \"autonym\", and the subspecific taxon as the \"autonymous subspecies\".\n\nWhen zoologists disagree over whether a certain population is a subspecies or a full species, the species name may be written in parentheses. Thus \"Larus (argentatus) smithsonianus\" means the American herring gull; the notation within the parentheses means that some consider it a subspecies of a larger herring gull species and therefore call it \"Larus argentatus smithsonianus\", while others consider it a full species and therefore call it \"Larus smithsonianus\" (and the user of the notation is not taking a position).\n\nA subspecies is a taxonomic rank below species – the only recognized rank in the zoological code, and one of three main ranks below species in the botanical code. When geographically separate populations of a species exhibit recognizable phenotypic differences, biologists may identify these as separate subspecies; a subspecies is a recognized local variant of a species. Botanists and mycologists have the choice of ranks lower than subspecies, such as variety (varietas) or form (forma), to recognize smaller differences between populations.\n\nIn biological terms, rather than in relation to nomenclature, a \"polytypic\" species has two or more subspecies, races, or more generally speaking, populations that need a separate description. These are separate groups that are clearly distinct from one another and do not generally interbreed (although there may be a relatively narrow hybridization zone), but which may interbreed if given the chance to do so. These subspecies, races, or populations, can be named as subspecies by zoologists, or in more varied ways by botanists and microbiologists.\n\nA \"monotypic\" species has no distinct population or races, or rather one race comprising the whole species. A taxonomist would not name a subspecies within such a species. Monotypic species can occur in several ways:\n\n\n", "id": "185901", "title": "Subspecies"}
{"url": "https://en.wikipedia.org/wiki?curid=9668061", "text": "Legion (biology)\n\nThe legion, in biological classification, is a non-obligatory taxonomic rank within the Linnaean hierarchy sometimes used in zoology.\n\nIn zoological taxonomy, the legion is:\n\nLegions may be grouped into superlegions or subdivided into sublegions, and these again into infralegions.\n\nLegions and their super/sub/infra groups have been employed in some classifications of birds and mammals. Full use is made of all of these (along with cohorts and supercohorts) in, for example, McKenna and Bell's classification of mammals.\n\n", "id": "9668061", "title": "Legion (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=51259558", "text": "Chaperome\n\nChaperome refers to the ensemble of all cellular molecular chaperone and co-chaperone proteins that assist protein folding of misfolded proteins or folding intermediates in order to ensure native protein folding and function, to antagonize aggregation-related proteotoxicity and ensuing protein loss-of-function or protein misfolding-diseases such as the neurodegenerative diseases Alzheimer's, Huntington's or Parkinson's disease, as well as to safeguard cellular proteostasis and proteome balance.\n\nThe term chaperome was first coined in a 2006 publication in Cell by Balch and co-workers on the finding that down-regulation of the Hsp90 co-chaperone Aha1 rescues misfolding of CFTR in cystic fibrosis to describe the overall chaperone folding environment, or the \"chaperome\". In 2014, Brehme and co-workers systematically studied the expression dynamics of the full human chaperome comprising ~300 human chaperones and co-chaperones in human aging brains and in brains of patients with neurodegenerative diseases. Integration with chaperome-wide functional RNA interference (RNAi) perturbation experiments in worm and in human cells led to the identification of a chaperome sub-network that safeguards proteostasis in aging and neurodegenerative diseases.\n\nRecently, a comprehensive literature survey reviewed the literature since the release of the human genome sequence in 2000 for systematic studies in small animal model systems and highlighted the power of model systems to unveil those key chaperone modifiers of proteotoxicity out of the large number represented in the wider human chaperome that could inform targets and strategies for therapeutic regulation of chaperone functionality.\n\n", "id": "51259558", "title": "Chaperome"}
{"url": "https://en.wikipedia.org/wiki?curid=30876740", "text": "Phototropism\n\nPhototropism is the growth of an organism which responds to a light stimulus. It is most often observed in plants, but can also occur in other organisms such as fungi. The cells on the plant that are farthest from the light have a chemical called auxin that reacts when phototropism occurs. This causes the plant to have elongated cells on the farthest side from the light. Phototropism is one of the many plant tropisms or movements which respond to external stimuli. Growth towards a light source is called positive phototropism, while growth away from light is called negative phototropism (skototropism). Most plant shoots exhibit positive phototropism, and rearrange their chloroplasts in the leaves to maximize photosynthetic energy and promote growth. Roots usually exhibit negative phototropism, although gravitropism may play a larger role in root behavior and growth. Some vine shoot tips exhibit negative phototropism, which allows them to grow towards dark, solid objects and climb them. The combination of phototropism and gravitropism allow plants to grow in the correct direction.\n\nThere are several signaling molecules that help the plant determine where the light source is coming from which helps the plant, and this activates several genes, which change the hormone gradients allowing the plant to grow towards the light. The very tip of the plant known as the coleoptile is necessary in light sensing. The middle portion of the coleoptile is the area where the shoot curvature occurs. The Cholodny–Went hypothesis, developed in the early 20th century, predicts that in the presence of asymmetric light, auxin will move towards the shaded side and promote elongation of the cells on that side to cause the plant to curve towards the light source. Auxins activate proton pumps, decreasing the pH in the cells on the dark side of the plant. This acidification of the cell wall region activates enzymes known as expansins which break bonds in the cell wall structure, making the cell walls less rigid. In addition, the acidic environment causes disruption of hydrogen bonds in the cellulose that makes up the cell wall. The decrease in cell wall strength causes cells to swell, exerting the mechanical pressure that drives phototropic movement.\n\nA second group of genes, \"PIN\" genes, have been found to play a major role in phototropism. They are auxin transporters, so it is thought that they are responsible for the polarization of auxin. Specifically \"PIN3\" has been identified as the primary auxin carrier. It is possible that phototropins receive light and inhibit the activity of PINOID kinase (PID), which then promotes the activity of \"PIN3\". This activation of \"PIN3\" leads to asymmetric distribution of auxin, which then leads to asymmetric elongation of cells in the stem. \"pin3\" mutants had shorter hypocotyls and roots than the wild-type, and the same phenotype was seen in plants grown with auxin efflux inhibitors. Using anti-PIN3 immunogold labeling, movement of the PIN3 protein was observed. PIN3 is normally localized to the surface of hypocotyl and stem, but is also internalized in the presence of Brefeldin A (BFA), an exocytosis inhibitor. This mechanism allows PIN3 to be repositioned in response to an environmental stimulus. PIN3 and PIN7 proteins were thought to play a role in pulse-induced phototropism. The curvature responses in the \"pin3\" mutant were reduced significantly, but only slightly reduced in \"pin7\" mutants. There is some redundancy among \"PIN1\", \"PIN3\", and \"PIN7\", but it is thought that PIN3 plays a greater role in pulse-induced phototropism.\n\nThere are phototropins that are highly expressed in the upper region of coleoptiles. The two main phototropins are phot1 and phot2. \"phot2\" single mutants have phototropic responses like that of the wild-type, but \"phot1 phot2\" double mutants do not show any phototropic responses. The amounts of \"PHOT1\" and \"PHOT2\" present are different depending on the age of the plant and the intensity of the light. There is a high amount of \"PHOT2\" present in mature \"Arabidopsis\" leaves and this was also seen in rice orthologs. The expression of \"PHOT1\" and \"PHOT2\" changes depending on the presence of blue or red light. There was a downregulation of \"PHOT1\" mRNA in the presence of light, but upregulation of PHOT2 transcript. The levels of mRNA and protein present in the plant were dependent upon the age of the plant. This suggests that the phototropin expression levels change with the maturation of the leaves.\nMature leaves contain chloroplasts that are essential in photosynthesis. Chloroplast rearrangement occurs in different light environments to maximize photosynthesis. There are several genes involved in plant phototropism including the \"NPH1\" and \"NPL1\" gene. They are both involved in chloroplast rearrangement. The \"nph1\" and \"npl1\" double mutants were found to have reduced phototropic responses. In fact, the two genes are both redundant in determining the curvature of the stem.\n\nIn 2012, Sakai and Haga outlined how different auxin concentrations could be arising on shaded and lighted side of the stem, giving birth to phototropic response. Five models in respect to stem phototropism have been proposed, using \"Arabidopsis thaliana\" as the study plant.\n\nIn the first model incoming light deactivates auxin on the light side of the plant allowing the shaded part to continue growing and eventually bend the plant over towards the light.\n\nIn the second model light inhibits auxin biosynthesis on the light side of the plant, thus decreasing the concentration of auxin relative to the unaffected side.\n\nIn the third model there is a horizontal flow of auxin from both the light and dark side of the plant. Incoming light causes more auxin to flow from the exposed side to the shaded side, increasing the concentration of auxin on the shaded side and thus more growth occurring.\n\nIn the fourth model it shows the plant receiving light to inhibit auxin basipetal down to the exposed side, causing the auxin to only flow down the shaded side.\n\nModel five encompasses elements of both model 3 and 4. The main auxin flow in this model comes from the top of the plant vertically down towards the base of the plant with some of the auxin travelling horizontally from the main auxin flow to both sides of the plant. Receiving light inhibits the horizontal auxin flow from the main vertical auxin flow to the irradiated exposed side. And according to the study by Sakai and Haga, the observed asymmetric auxin distribution and subsequent phototropic response in hypocotyls seems most consistent with this fifth scenario.\n\nPhototropism in plants such as \"Arabidopsis thaliana\" is directed by blue light receptors called phototropins. Other photosensitive receptors in plants include phytochromes that sense red light and cryptochromes that sense blue light. Different organs of the plant may exhibit different phototropic reactions to different wavelengths of light. Stem tips exhibit positive phototropic reactions to blue light, while root tips exhibit negative phototropic reactions to blue light. Both root tips and most stem tips exhibit positive phototropism to red light. Cryptochromes are photoreceptors that absorb blue/ UV-A light, and they help control the circadian rhythm in plants and timing of flowering. Phytochromes are photoreceptors that sense red/far-red light, but they also absorb blue light. The combination of responses from phytochromes and cryptochromes allow the plant to respond to various kinds of light. Together phytochromes and cryptochromes inhibit gravitropism in hypocotyls and contribute to phototropism.\n\n\n", "id": "30876740", "title": "Phototropism"}
{"url": "https://en.wikipedia.org/wiki?curid=52318777", "text": "Matriphagy\n\nMatriphagy is the condition where organisms feed on their own mother. This behaviour is commonly found in spiders.\n\n", "id": "52318777", "title": "Matriphagy"}
{"url": "https://en.wikipedia.org/wiki?curid=69440", "text": "Edge effects\n\nIn ecology, edge effects are changes in population or community structures that occur at the boundary of five or more habitats. Areas with small habitat fragments exhibit especially pronounced edge effects that may extend throughout the range. As the edge effects increase, the boundary habitat allows for greater biodiversity.\n\n\nHeight can create borders between patches as well.\n\nEnvironmental conditions enable certain species of plants and animals to colonize habitat borders. Plants that colonize tend to be shade-intolerant and tolerant of dry conditions, such as shrubs and vines. Animals that colonize tend to be those that require two or more habitats, such as white-tailed and mule deer, elk, cottontail rabbits, blue jays, and robins. Some animals travel between habitats, while edge species are restricted to edges. Larger patches include more individuals and therefore have increased biodiversity. The width of the patch also influences diversity: an edge patch must be more pronounced than just a stark border in order to develop gradients of edge effects. \n\nAnimals traveling between communities can create travel lanes along borders, which in turn increases light reaching plants along the lanes and promotes primary production. As more light reaches the plants, greater numbers and sizes can thrive. Increased primary production can increase numbers of herbivorous insects, followed by nesting birds and so on up the trophic levels.\n\nIn the case of wide and/or overgrown borders, some species can become restricted to one side of the border despite having the ability to inhabit the other. Sometimes, the edge effects result in abiotic and biotic conditions which diminish natural variation and threaten the original ecosystem. Detrimental edge effects are also seen in physical and chemical conditions of border species. For instance, fertilizer from an agricultural field could invade a bordering forest and contaminate the habitat. The three factors affecting edges can be summarized:\n\nHuman activity creates edges through development and agriculture. Often, the changes are detrimental to both the size of the habitat and to species. Examples of human impacts include:\n\nWhen edges divide any natural ecosystem and the area outside the boundary is a disturbed or unnatural system, the natural ecosystem can be seriously affected for some distance in from the edge. In 1971, Odum wrote, 'The tendency for increased variety and diversity at community junctions is known as the \"edge effect\"... It is common knowledge that the density of songbirds is greater on estates, campuses and similar settings...as compared with tracts of uniform forest.'. In a forest where the adjacent land has been cut, creating an open/forest boundary, sunlight and wind penetrate to a much greater extent, drying out the interior of the forest close to the edge and encouraging growth of opportunistic species there. Air temperature, vapor pressure deficit, soil moisture, light intensity and levels of photosynthetically active radiation (PAR) all change at edges.\n\nOne study estimated that the amount of Amazon Basin area modified by edge effects exceeded the area that had been cleared. \"In studies of Amazon forest fragments, micro-climate effects were evident up to 100m (330ft.) into the forest interior.\" The smaller the fragment, the more susceptible it is to fires spreading from nearby cultivated fields. Forest fires are more common close to edges due to increased light availability that leads to increased desiccation and increased understory growth. Increased understory biomass provides fuel that allows pasture fires to spread into the forests. Increased fire frequency since the 1990s is among the edge effects that are slowly transforming Amazonian forests. The changes in temperature, humidity and light levels promote invasion of non-forest species, including invasive species. The overall effect of these fragment processes is that all forest fragments tend to lose native biodiversity depending on fragment size and shape, isolation from other forest areas, and the forest matrix.\n\nThe amount of forest edge is orders of magnitude greater now in the United States than when the Europeans first began settling North America. Some species have benefited from this fact, for example, the brown-headed cowbird, which is a brood parasite that lays its eggs in the nests of songbirds nesting in forest near the forest boundary. Another example of a species benefiting from the proliferation of forest edge is poison ivy.\n\nConversely, Dragonflies eat mosquitoes, but have more trouble than mosquitoes surviving around the edges of human habitation. Thus, trails and hiking areas near human settlements often have more mosquitoes than do deep forest habitats. Grasses, huckleberries, flowering currants and shade-intolerant trees such as the Douglas-fir all thrive in edge habitats.\n\nIn the case of developed lands juxtaposed to wild lands, problems with invasive exotics often result. Species such as kudzu, Japanese honeysuckle and multiflora rose have damaged natural ecosystems. Beneficially, the open spots and edges provide places for species that thrive where there is more light and vegetation that is close to the ground. Deer and elk benefit particularly as their principal diet is that of grass and shrubs which are found only on the edges of forested areas.\n\nEdge effects also apply to succession, when vegetation spreads rather than losing to competitors. Different species are suited either to the edges or to central sections of the habitat, resulting in a varied distribution. Edges also vary with orientation: edges on the north or south receive less or more sun than the opposite side (depending on hemisphere), producing varying vegetation patterns.\n\nThe phenomenon of increased variety of plants as well as animals at the community junction (ecotone) is also called the edge effect and is essentially due to a locally broader range of suitable environmental conditions or ecological niches.\n\nEdge effects in biological assays refer to artifacts in data that are caused by the position of the wells on a screening plate rather than a biological effect.\n\nThe edge effect in scanning electron microscopy is the phenomenon in which the number of secondary and/or backscattered electrons that escape the sample and reach the detector is higher at an edge than at a surface. The interaction volume spreads far below the surface, but secondary electrons can only escape when close to the surface (generally about 10 nm, although this depends on the material). However, when the electron beam impacts an area close to the edge, electrons that are generated below an impact point that is close to an edge but that is far below the surface may be able to escape through the vertical surface instead.\n\n\n", "id": "69440", "title": "Edge effects"}
{"url": "https://en.wikipedia.org/wiki?curid=52307847", "text": "Sociome\n\n\"This article is about the concept of the Sociome\"\n\nThe Sociome is a concept used by scientists in Biology and Sociology referring to the dimensions of existence that are social. The term is also an indication of the convergence of systems biology and the study of society as a complex system that has begun to occur among early 21st Century scientists. Just as the \"phenome\" is typically thought of as the set of expressed phenotypes of an organism, the sociome can be thought of as the set of observed characteristics of societies. For example, while all societies consisting of humans might be thought of as having the potential to become egalitarian social democracies, not all observed societies are egalitarian or social democracies. Thus, the sociome can also be thought of indirectly as an ideal type of the unrealized potential of any given organization of social beings.\n\nThe first known usage of the term \"sociome\" was in 2001 by Daichi Kamiyama. The term has also been utilized by sociologist Adam Thomas Perzynski. The two scientists differ in their usage. Kamiyama's study describes a new scientific \"era of the sociome (Sociology[+ome])\" characterized by the study of the social activities of molecules. This usage is an anthropomorphism of social behavior, wherein molecules are described as having the ability to socialize. Perzynski's social scientific usage varies from this considerably. While Sociology is the study of society, behavior and social relationships, the sociome is the characterization and quantification of patterns, variables, activities, relationships and attributes across all societies that exist and can be studied. The suffix -ome has been used primarily in biology, as in genome, proteome, microbiome, metabolome and phenome. Basu and colleagues have used the term \"sociome\" to refer to a sort of standardized approach to the characterization of geocoded social attributes (e.g. neighborhood level). In 2014, Del Savio and colleagues discussed the blurring of the boundaries between disciplines, and increased enthusiasm for the \"sociome\" concept and its importance for research in social science, epigenetics and epidemiology, with cautionary advice about the risks rooted in the marred history of Sociobiology\n\nStill other authors have referred to sociomics as the bidirectional interplay between the field of Science and Technology Studies and all other \"-omics\" fields.\n\nThe -omics Wikipedia entry had previously listed sociome as a proposed new name for sociology, although it is unclear whether this has ever actually been proposed by any credible source. Still others have proposed that sociome is the object of study of Sociometry.\n", "id": "52307847", "title": "Sociome"}
{"url": "https://en.wikipedia.org/wiki?curid=6682", "text": "Clade\n\nA clade (from , \"klados\", \"branch\") is a group of organisms that consists of a common ancestor and all its lineal descendants, and represents a single \"branch\" on the \"tree of life\".\n\nThe common ancestor may be an individual, a population, a species (extinct or extant), and so on right up to a kingdom. Clades are nested, one in another, as each branch in turn splits into smaller branches. These splits reflect evolutionary history as populations diverged and evolved independently. Clades are termed monophyletic (Greek: \"one clan\") groups.\n\nOver the last few decades, the cladistic approach has revolutionized biological classification and revealed surprising evolutionary relationships among organisms. Increasingly, taxonomists try to avoid naming taxa that are not clades; that is, taxa that are not monophyletic. Some of the relationships between organisms that the molecular biology arm of cladistics has revealed are that fungi are closer relatives to animals than they are to plants, archaea are now considered different from bacteria, and multicellular organisms may have evolved from archaea.\n\nThe term \"clade\" was coined in 1957 by the biologist Julian Huxley to refer to the result of cladogenesis, a concept Huxley borrowed from Bernhard Rensch.\n\nMany commonly named groups are clades, for example, rodents or insects; because in each case, their name refers to a common ancestor with all its descendant branches. Rodents, for example, are a branch of mammals that split off after the end of the period when the clade Dinosauria stopped being the dominant terrestrial vertebrates 66 million years ago. The original population and all its descendants are a clade. The rodent clade corresponds to the order Rodentia, and insects to the class Insecta. These clades include smaller clades, such as chipmunk or ant, each of which consists of even smaller clades. The clade \"rodent\" is in turn included in the mammal, vertebrate and animal clades.\n\nThe idea of a clade did not exist in pre-Darwinian Linnaean taxonomy, which was based by necessity only on internal or external morphological similarities between organisms – although as it happens, many of the better known animal groups in Linnaeus' original Systema Naturae (notably among the vertebrate groups) do represent clades. The phenomenon of convergent evolution is, however, responsible for many cases where there are misleading similarities in the morphology of groups that evolved from different lineages.\n\nWith the increasing realization in the first half of the 19th century that species had changed and split through the ages, classification increasingly came to be seen as branches on the evolutionary tree of life. The publication of Darwin's theory of evolution in 1859 gave this view increasing weight. Thomas Henry Huxley, an early advocate of evolutionary theory, proposed a revised taxonomy based on clades.<ref name=\"original text w/ figures\">Huxley, T.H. (1876): Lectures on Evolution. \"New York Tribune\". Extra. no 36. In Collected Essays IV: pp 46-138 original text w/ figures</ref> For example, he grouped birds with reptiles, based on fossil evidence.\n\nGerman biologist Emil Hans Willi Hennig (1913 – 1976) is considered to be the founder of cladistics.\nHe proposed a classification system that represented repeated branchings of the family tree, as opposed to the previous systems, which put organisms on a \"ladder\", with supposedly more \"advanced\" organisms at the top.\n\nTaxonomists have increasingly worked to make the taxonomic system reflect evolution. When it comes to naming, however, this principle is not always compatible with the traditional rank-based nomenclature. In the latter, only taxa associated with a rank can be named, yet there are not enough ranks to name a long series of nested clades; also, taxon names cannot be defined in a way that guarantees them to refer to clades. For these and other reasons, phylogenetic nomenclature has been developed; it is still controversial.\n\nA clade is by definition monophyletic, meaning that it contains one ancestor (which can be an organism, a population, or a species) and all its descendants. The ancestor can be known or unknown; any and all members of a clade can be extant or extinct.\n\nThe science that tries to reconstruct phylogenetic trees and thus discover clades is called phylogenetics or cladistics, the latter term coined by Ernst Mayr (1965), derived from \"clade\". The results of phylogenetic/cladistic analyses are tree-shaped diagrams called \"cladograms\"; they, and all their branches, are phylogenetic hypotheses.\n\nThree methods of defining clades are featured in phylogenetic nomenclature: node-, stem-, and apomorphy-based (see here for detailed definitions).\n\nThe relationship between clades can be described in several ways:\n\n\n\n\"Clade\" is the title of a novel by James Bradley, who chose it both because of its biological meaning but also because of the larger implications of the word.\n\nAn episode of \"Elementary\" was titled “Dead Clade Walking” and dealt with a case involving a rare fossil.\n\n", "id": "6682", "title": "Clade"}
{"url": "https://en.wikipedia.org/wiki?curid=1504065", "text": "Biological target\n\nA biological target is anything within a living organism to which some other entity (like an endogenous ligand or a drug) is directed and/or binds, resulting in a change in its behavior or function. Examples of common classes of biological targets are proteins and nucleic acids. The definition is context-dependent, and can refer to the biological target of a pharmacologically active drug compound, the receptor target of a hormone (like insulin), or some other target of an external stimulus. Biological targets are most commonly proteins such as enzymes, ion channels, and receptors.\n\nThe external stimulus (\"i.e.\", the drug or ligand) physically binds to (\"hits\") the biological target. The interaction between the substance and the target may be:\n\nDepending on the nature of the stimulus, the following can occur:\n\nThe term \"biological target\" is frequently used in pharmaceutical research to describe the native protein in the body whose activity is modified by a drug resulting in a specific effect, which may be a desirable therapeutic effect or an unwanted adverse effect. In this context, the biological target is often referred to as a drug target. The most common drug targets of currently marketed drugs include:\n\n\nIdentifying the biological origin of a disease, and the potential targets for intervention, is the first step in the discovery of a medicine using the reverse pharmacology approach. A number of different target discovery approaches have been identified such as drug affinity responsive target stability (DARTS).\n\nDatabases containing biological targets information:\n\nThese biological targets are conserved across species, making pharmaceutical pollution of the environment a danger to species who possess the same targets. For example, the synthetic estrogen in human contraceptives, 17-R-ethinylestradiol, has been shown to increase the feminization of fish downstream from sewage treatment plants, thereby unbalancing reproduction and creating an additional selective pressure on fish survival. Pharmaceuticals are usually found at ng/L to lowµg/L concentrations in the aquatic environment. Adverse effects may occur in non-target species as a consequence of specific drug target interactions. Therefore, evolutionarily well-conserved drug targets are likely to be associated with an increased risk for non-targeted pharmacological effects.\n\n", "id": "1504065", "title": "Biological target"}
{"url": "https://en.wikipedia.org/wiki?curid=23671373", "text": "Corneous\n\nCorneous is a biological and medical term meaning horny, in other words made out of a substance similar to that of horns and hooves in some mammals. \n\nThe word is generally used to describe natural or pathological anatomical structures made out of a hard layer of protein. In mammals this protein would usually be keratin.\n\nThe word corneous is also often used to describe the operculum of a snail, a gastropod mollusc. Not all gastropods have opercula, but in the great majority of those that do have one, the operculum is corneous. (However in several genera within a few families including the marine Naticidae and the terrestrial Pomatiidae, the operculum is primarily calcareous, in other words mostly made of calcium carbonate.) Corneous opercula are made out of the protein conchiolin.\n\n", "id": "23671373", "title": "Corneous"}
{"url": "https://en.wikipedia.org/wiki?curid=53942957", "text": "Smash and Grab (biology)\n\nSmash and Grab is the name given to a technique developed by Charles S. Hoffman and Fred Winston used in molecular biology to rescue plasmids from yeast transformants into \"Escherichia coli\" in order to amplify and purify them. In addition, it can be used to prepare yeast genomic DNA (and DNA from tissue samples) for Southern blot analyses or polymerase chain reaction (PCR).\n", "id": "53942957", "title": "Smash and Grab (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=53664", "text": "Kingdom (biology)\n\nIn biology, kingdom (Latin: regnum, plural regna) is the second highest taxonomic rank, just below domain. Kingdoms are divided into smaller groups called phyla. Traditionally, some textbooks from the United States used a system of six kingdoms (Animalia, Plantae, Fungi, Protista, Archaea/Archaeabacteria, and Bacteria/Eubacteria) while textbooks in Great Britain, India, Australia, Latin America and other countries used five kingdoms (Animalia, Plantae, Fungi, Protista and Monera). Some recent classifications based on modern cladistics have explicitly abandoned the term \"kingdom\", noting that the traditional kingdoms are not monophyletic, i.e., do not consist of all the descendants of a common ancestor.\n\nWhen Carl Linnaeus introduced the rank-based system of nomenclature into biology in 1735, the highest rank was given the name \"kingdom\" and was followed by four other main or principal ranks: class, order, genus and species. Later two further main ranks were introduced, making the sequence kingdom, phylum or division, class, order, family, genus and species. In the 1960s a rank was introduced above kingdom, namely domain (or empire), so that kingdom is no longer the highest rank.\n\nPrefixes can be added so \"subkingdom\" (\"subregnum\") and \"infrakingdom\" (also known as \"infraregnum\") are the two ranks immediately below kingdom. Superkingdom may be considered as an equivalent of domain or empire or as an independent rank between kingdom and domain or subdomain. In some classification systems the additional rank \"branch\" (Latin: \"ramus\") can be inserted between subkingdom and infrakingdom, e.g., Protostomia and Deuterostomia in the classification of Cavalier-Smith.\n\nFrom around the mid-1970s onwards, there was an increasing emphasis on comparisons of genes at the molecular level (initially ribosomal RNA genes) as the primary factor in classification; genetic similarity was stressed over outward appearances and behavior. Taxonomic ranks, including kingdoms, were to be groups of organisms with a common ancestor, whether monophyletic (\"all\" descendants of a common ancestor) or paraphyletic (\"only some\" descendants of a common ancestor). Based on such RNA studies, Carl Woese thought life could be divided into three large divisions and referred to them as the \"three primary kingdom\" model or \"urkingdom\" model. In 1990, the name \"domain\" was proposed for the highest rank. This term represents a synonym for the category of dominion (lat. dominium), introduced by Moore in 1974. Unlike Moore, Woese et al. (1990) did not suggest a Latin term for this category, which represents a further argument supporting the accurately introduced term dominion.\nWoese divided the prokaryotes (previously classified as the Kingdom Monera) into two groups, called Eubacteria and Archaebacteria, stressing that there was as much genetic difference between these two groups as between either of them and all eukaryotes.\n\nAccording to genetic data, although eukaryote groups such as plants, fungi, and animals may look different, they are more closely related to each other than they are to either the Eubacteria or Archaea. It was also found that the eukaryotes are more closely related to the Archaea than they are to the Eubacteria. Although the primacy of the Eubacteria-Archaea divide has been questioned, it has been upheld by subsequent research. There is no consensus on how many kingdoms exist in the classification scheme proposed by Woese.\n\nIn 2004, a review article by Simpson and Roger noted that the Protista were \"a grab-bag for all eukaryotes that are not animals, plants or fungi\". They held that only monophyletic groups should be accepted as formal ranks in a classification and that - while this approach had been impractical previously (necessitating \"literally dozens of eukaryotic 'kingdoms'\") - it had now become possible to divide the eukaryotes into \"just a few major groups that are probably all monophyletic\".\n\nOn this basis, the diagram opposite (redrawn from their article) showed the real \"kingdoms\" (their quotation marks) of the eukaryotes. A classification which followed this approach was produced in 2005 for the International Society of Protistologists, by a committee which \"worked in collaboration with specialists from many societies\". It divided the eukaryotes into the same six \"supergroups\". The published classification deliberately did not use formal taxonomic ranks, including that of \"kingdom\".\n\nIn this system the multicellular animals (Metazoa) are descended from the same ancestor as both the unicellular choanoflagellates and the fungi which form the Opisthokonta. Plants are thought to be more distantly related to animals and fungi.\n\nHowever, in the same year as the International Society of Protistologists' classification was published (2005), doubts were being expressed as to whether some of these supergroups were monophyletic, particularly the Chromalveolata, and a review in 2006 noted the lack of evidence for several of the supposed six supergroups.\n\n, there is widespread agreement that the Rhizaria belong with the Stramenopiles and the Alveolata, in a clade dubbed the SAR supergroup, so that Rhizaria is not one of the main eukaryote groups. Beyond this, there does not appear to be a consensus.\nRogozin \"et al.\" in 2009 noted that \"The deep phylogeny of eukaryotes is an extremely difficult and controversial problem.\" , there appears to be a consensus that the 2005 six supergroup model does not reflect the true phylogeny of the eukaryotes and hence how they should be classified, although there is no agreement as to the model which should replace it.\n\nThe classification of living things into animals and plants is an ancient one. Aristotle (384–322 BC) classified animal species in his \"History of Animals\", while his pupil Theophrastus (c. 371–c. 287 BC) wrote a parallel work, the \"Historia Plantarum\", on plants.\n\nCarl Linnaeus (1707–1778) laid the foundations for modern biological nomenclature, now regulated by the Nomenclature Codes, in 1735. He distinguished two kingdoms of living things: \"Regnum Animale\" ('animal kingdom') and \"Regnum Vegetabile\" ('vegetable kingdom', for plants). Linnaeus also included minerals in his classification system, placing them in a third kingdom, \"Regnum Lapideum\".\n\nIn 1674, Antonie van Leeuwenhoek, often called the \"father of microscopy\", sent the Royal Society of London a copy of his first observations of microscopic single-celled organisms. Until then, the existence of such microscopic organisms was entirely unknown. Despite this, Linnaeus did not include any microscopic creatures in his original taxonomy.\n\nAt first, microscopic organisms were classified within the animal and plant kingdoms. However, by the mid-19th century, it had become clear to many that \"the existing dichotomy of the plant and animal kingdoms [had become] rapidly blurred at its boundaries and outmoded\". In 1866, Ernst Haeckel proposed a third kingdom of life, the Protista, for \"neutral organisms\" which were neither animal nor plant. Haeckel revised the content of this kingdom a number of times before settling on a division based on whether organisms were unicellular (Protista) or multicellular (animals and plants).\n\nThe development of the electron microscope revealed important distinctions between those unicellular organisms whose cells do not have a distinct nucleus (prokaryotes) and those unicellular and multicellular organisms whose cells do have a distinct nucleus (eukaryotes). In 1938, Herbert F. Copeland proposed a four-kingdom classification, elevating the protist classes of bacteria (Monera) and blue-green algae (Phycochromacea) to phyla in the novel Kingdom Monera.\n\nThe importance of the distinction between prokaryotes and eukaryotes gradually became apparent. In the 1960s, Stanier and van Niel popularised Édouard Chatton's much earlier proposal to recognise this division in a formal classification. This required the creation, for the first time, of a rank above kingdom, a \"superkingdom\" or \"empire\", later called a \"domain\".\n\nThe differences between fungi and other organisms regarded as plants had long been recognised by some; Haeckel had moved the fungi out of Plantae into Protista after his original classification, but was largely ignored in this separation by scientists of his time. Robert Whittaker recognized an additional kingdom for the Fungi. The resulting five-kingdom system, proposed in 1969 by Whittaker, has become a popular standard and with some refinement is still used in many works and forms the basis for new multi-kingdom systems. It is based mainly upon differences in nutrition; his Plantae were mostly multicellular autotrophs, his Animalia multicellular heterotrophs, and his Fungi multicellular saprotrophs. The remaining two kingdoms, Protista and Monera, included unicellular and simple cellular colonies. The five kingdom system may be combined with the two empire system:\n\nIn the Whittaker system, Plantae included some algae. In other systems (e.g., Margulis system), Plantae included just the land plants (Embryophyta).\n\nDespite the development from two kingdoms to five among most scientists, some authors as late as 1975 continued to employ a traditional two-kingdom system of animals and plants, dividing the plant kingdom into Subkingdoms Prokaryota (bacteria and cyanophytes), Mycota (fungi and supposed relatives), and Chlorota (algae and land plants).\n\nThomas Cavalier-Smith thought at first, as it was nearly consensually admitted at that time, that the difference between \"eubacteria\" and \"archaebacteria\" was so great (particularly considering the genetic distance of ribosomal genes) that they needed to be separated into two different kingdoms, hence splitting the empire Bacteria into two kingdoms. He then divided Eubacteria into two subkingdoms: Negibacteria (Gram negative bacteria) and Posibacteria (Gram positive bacteria).\n\nTechnological advances in electron microscopy allowed the separation of the Chromista from the Plantae kingdom. Indeed, the chloroplast of the chromists is located in the lumen of the endoplasmic reticulum instead of in the cytosol. Moreover, only chromists contain chlorophyll c. Since then, many non-photosynthetic phyla of protists, thought to have secondarily lost their chloroplasts, were integrated into the kingdom Chromista.\n\nFinally, some protists lacking mitochondria were discovered. As mitochondria were known to be the result of the endosymbiosis of a proteobacterium, it was thought that these amitochondriate eukaryotes were primitively so, marking an important step in eukaryogenesis. As a result, these amitochondriate protists were separated from the protist kingdom, giving rise to the, at the same time, superkingdom and kingdom Archezoa. This was known as the Archezoa hypothesis. This superkingdom was opposed to the Metakaryota superkingdom, grouping together the five other eukaryotic kingdoms (Animalia, Protozoa, Fungi, Plantae and Chromista).\n\nIn 1998, Cavalier-Smith published a six-kingdom model, which has been revised in subsequent papers. The version published in 2009 is shown below. (Compared to the version he published in 2004, the alveolates and the rhizarians have been moved from Kingdom Protozoa to Kingdom Chromista.) Cavalier-Smith no longer accepts the importance of the fundamental eubacteria–archaebacteria divide put forward by Woese and others and supported by recent research. His Kingdom Bacteria includes Archaebacteria as a phylum of the subkingdom Unibacteria which comprises only one other phylum: the Posibacteria. The two subkingdoms Unibacteria and Negibacteria of kingdom Bacteria (sole kingdom of empire Prokaryota) are opposed according to their membrane topologies. The bimembranous-unimembranous transition is thought to be far more fundamental than the long branch of genetic distance of Archaebacteria, viewed as having no particular biological significance. Cavalier-Smith does not accept the requirement for taxa to be monophyletic (\"holophyletic\" in his terminology) to be valid. He defines Prokaryota, Bacteria, Negibacteria, Unibacteria and Posibacteria as valid paraphyletic (therefore \"monophyletic\" in the sense he uses this term) taxa, marking important innovations of biological significance (in regard of the concept of biological niche).\n\nIn the same way, his paraphyletic kingdom Protozoa includes the ancestors of Animalia, Fungi, Plantae and Chromista. The advances of phylogenetic studies allowed Cavalier-Smith to realize that all the phyla thought to be archezoans (i.e. primitively amitochondriate eukaryotes) had in fact secondarily lost their mitochondria, most of the time by transforming them into new organelles: hydrogenosomes. This means that all living eukaryotes are in fact metakaryotes, according to the significance of the term given by Cavalier-Smith. Some of the members of the defunct kingdom Archezoa, like the phylum Microsporidia, were reclassified into kingdom Fungi. Others were reclassified in kingdom Protozoa like Metamonada which is now part of infrakingdom Excavata.\n\nThe diagram below does not represent an evolutionary tree.\n\nCavalier-Smith and his collaborators revised the classification in 2015, and published it in \"PLOS ONE\". In this scheme they reintroduced the division of prokaryotes into two kingdoms, Bacteria (=Eubacteria) and Archaea (=Archaebacteria). This is based on the consensus in the Taxonomic Outline of Bacteria and Archaea (TOBA) and the Catalogue of Life.\n\nA summary of the different kinds of proposed classification schemes presented in this article is summarized in the table below.\n\nThe kingdom-level classification of life is still widely employed as a useful way of grouping organisms, notwithstanding some problems with this approach:\n\nThere is ongoing debate as to whether viruses, obligate intracellular parasites that lack metabolism and are not capable of replication outside of a host, can be included in the tree of life. If included, their placement in the tree would be problematic since it is suspected that viruses have arisen multiple times, and they have a penchant for accumulating nucleotide sequences from their hosts. A principal reason for inclusion comes from the discovery of unusually large and complex viruses, such as Mimivirus, that possess typical cellular genes.\n\n\n\n", "id": "53664", "title": "Kingdom (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=41244", "text": "Hybrid (biology)\n\nIn biology, a hybrid, or crossbreed, is the result of combining the qualities of two organisms of different breeds, varieties, species or genera through sexual reproduction. Hybrids are not always intermediates between their parents (such as in blending inheritance), but can show hybrid vigour, often growing larger or taller than either parent. The concept of a hybrid is interpreted differently in animal and plant breeding, where there is interest in the individual parentage. In genetics, attention is focused on the numbers of chromosomes. In taxonomy, a key question is how closely related the parent species are.\n\nSpecies are reproductively isolated by strong barriers to hybridisation, which include morphological differences, differing times of fertility, mating behaviors and cues, and physiological rejection of sperm cells or the developing embryo. Some act before fertilization and others after it. Similar barriers exist in plants, with differences in flowering times, pollen vectors, inhibition of pollen tube growth, somatoplastic sterility, cytoplasmic-genic male sterility and the structure of the chromosomes. A few animal species and many plant species, however, are the result of hybrid speciation, including important crop plants such as wheat, where the number of chromosomes has been doubled.\n\nHuman impact on the environment has resulted in an increase in the intrabreeding between species, with introduced species worldwide, which has resulted in an increase in hybridization. The genetic mixing may threaten many species with extinction, while genetic erosion in crop plants may be damaging the gene pools of many species for future breeding. Many commercially useful fruits, flowers, garden herbs and trees have been produced by hybridization. One flower, \"Oenothera lamarckiana\", was central to early genetics research into mutationism and polyploidy.\n\nHybrid humans existed during prehistoric ancient times. \nMythological hybrids appear in human culture in forms as diverse as the Minotaur, blends of animals, humans and mythical beasts such as centaurs and sphinxes, and the Nephilim of the Biblical apocrypha described as the wicked sons of fallen angels and attractive women.\n\nThe term hybrid is derived from Latin \"\", used for crosses such as of a tame sow and a wild boar. The term came into popular use in English in the 19th century, though examples of its use have been found from the early 17th century. Conspicuous hybrids are popularly named with portmanteau words, starting in the 1920s with the breeding of tiger–lion hybrids (liger and tigon).\n\nFrom the point of view of animal and plant breeders, there are several kinds of hybrid formed from crosses within a species, such as between different breeds.\nSingle cross hybrids result from the cross between two true-breeding organisms which produces an F1 hybrid (first filial generation). The cross between two different homozygous lines produces an F1 hybrid that is heterozygous; having two alleles, one contributed by each parent and typically one is dominant and the other recessive. Typically, the F1 generation is also phenotypically homogeneous, producing offspring that are all similar to each other.\nDouble cross hybrids result from the cross between two different F1 hybrids (i.e., there are four unrelated grandparents).\nThree-way cross hybrids result from the cross between an F1 hybrid and an inbred line.\nTriple cross hybrids result from the crossing of two different three-way cross hybrids.\nTop cross (or \"topcross\") hybrids result from the crossing of a top quality or pure-bred male and a lower quality female, intended to improve the quality of the offspring, on average.\n\nPopulation hybrids result from the crossing of plants or animals in one population with those of another population. These include interspecific hybrids or crosses between different breeds.\n\nIn horticulture, the term stable hybrid is used to describe an annual plant that, if grown and bred in a small monoculture free of external pollen (e.g., an air-filtered greenhouse) produces offspring that are \"true to type\" with respect to phenotype; i.e., a true-breeding organism.\n\nHybridization can occur in the zones where geographical subspecies overlap. For example, the butterfly \"Limenitis arthemis\" has two major subspecies in North America, \"L. a. arthemis\" (the white admiral) and \"L. a. astyanax\" (the red-spotted purple). The white admiral has a bright, white band on its wings, while the red-spotted purple has cooler blue-green shades. Hybridization occurs between a narrow area across New England, southern Ontario, and the Great Lakes, the \"suture region\". It is at these regions that the subspecies were formed.\n\nFrom the point of view of genetics, several different kinds of hybrid can be distinguished.\nA genetic hybrid carries two different alleles of the same gene, where for instance one allele may code for a lighter coat colour than the other.\nA structural hybrid results from the fusion of gametes that have differing structure in at least one chromosome, as a result of structural abnormalities.\nA numerical hybrid results from the fusion of gametes having different haploid numbers of chromosomes.\nA permanent hybrid results when only the heterozygous genotype occurs, as in \"Oenothera lamarckiana\", because all homozygous combinations are lethal. In the early history of genetics, Hugo De Vries supposed these were caused by mutation.\n\nFrom the point of view of taxonomy, hybrids differ according to their parentage.\nHybrids between different subspecies (such as between the Bengal tiger and Siberian tiger) are called intra-specific hybrids. \nInterspecific hybrids are the offspring from interspecies mating; these sometimes result in hybrid speciation.\nIntergeneric hybrids result from matings between different genera, such as between sheep and goats.\nInterfamilial hybrids, such as between chickens and guineafowl or pheasants, are reliably described but extremely rare.\nInterordinal hybrids (between different orders) are few, but have been made with the sea urchin \"Strongylocentrotus purpuratus\" (female) and the sand dollar \"Dendraster excentricus\" (male).\n\nWhen two distinct types of organisms breed with each other, the resulting hybrids typically have intermediate traits (e.g., one plant parent has red flowers, the other has white, and the hybrid, pink flowers). Commonly, hybrids also combine traits seen only separately in one parent or the other (e.g., a bird hybrid might combine the yellow head of one parent with the orange belly of the other).\n\nInterspecific hybrids are bred by mating individuals from two species, normally from within the same genus. The offspring display traits and characteristics of both parents, but are often sterile, preventing gene flow between the species. Sterility is often attributed to the different number of chromosomes between the two species. For example, donkeys have 62 chromosomes, horses have 64 chromosomes, and mules or hinnies have 63 chromosomes. Mules, hinnies, and other normally sterile interspecific hybrids cannot produce viable gametes, because differences in chromosome structure prevent appropriate pairing and segregation during meiosis, meiosis is disrupted, and viable sperm and eggs are not formed. However, fertility in female mules has been reported with a donkey as the father.\n\nA variety of mechanisms limit the success of hybridization, including the large genetic difference between most species. Barriers include morphological differences, differing times of fertility, mating behaviors and cues, and physiological rejection of sperm cells or the developing embryo. Some act before fertilization; others after it.\n\nIn plants, some barriers to hybridization include blooming period differences, different pollinator vectors, inhibition of pollen tube growth, somatoplastic sterility, cytoplasmic-genic male sterility and structural differences of the chromosomes.\n\nA few animal species are the result of hybridization. The Lonicera fly is a natural hybrid. The American red wolf appears to be a hybrid of the gray wolf and the coyote, although its taxonomic status has been a subject of controversy. The European edible frog is a semi-permanent hybrid between pool frogs and marsh frogs; its population requires the continued presence of at least one of the parent species. Cave paintings indicate that the European bison is a natural hybrid of the aurochs and the steppe bison.\n\nPlant Hybridization is more commonplace compared to animal hybridization. Many crop species are hybrids, including notably the polyploid wheats: some have four sets of chromosomes (tetraploid) or six (hexaploid), while other wheat species have (like most eukaryotic organisms) two sets (diploid), so hybridization events likely involved the doubling of chromosome sets, causing immediate genetic isolation.\n\nHybridization may be important in speciation in some plant groups. However, homoploid hybrid speciation (not increasing the number of sets of chromosomes) may be rare: by 1997, only 8 natural examples had been fully described. Experimental studies suggest that hybridization offers a rapid route to speciation, a prediction confirmed by the fact that early generation hybrids and ancient hybrid species have matching genomes, meaning that once hybridization has occurred, the new genome can remain stable.\n\nMany hybrid zones are known where the ranges of two species meet, and hybrids are continually produced in great numbers. These hybrid zones are useful as biological model systems for studying the mechanisms of speciation. Recently DNA analysis of a bear shot by a hunter in the North West Territories confirmed the existence of naturally-occurring and fertile grizzly–polar bear hybrids.\n\nHybrids are not as might be expected always intermediate between their parents (as if there were blending inheritance), but are sometimes stronger than either parent variety, a phenomenon called heterosis, hybrid vigour, or heterozygote advantage. This is most common with plant hybrids. A transgressive phenotype is a phenotype that displays more extreme characteristics than either of the parent lines. Plant breeders use several techniques to produce hybrids, including line breeding and the formation of complex hybrids. An economically important example is hybrid maize (corn), which provides a considerable seed yield advantage over open pollinated varieties. Hybrid seed dominates the commercial maize seed market in the United States, Canada and many other major maize-producing countries.\n\nIn a hybrid, any trait that falls outside the range of parental variation (and is thus not simply intermediate between its parents) is considered heterotic. \"Positive heterosis\" produces more robust hybrids, they might be stronger or bigger; while the term \"negative heterosis\" refers to weaker or smaller hybrids. Heterosis is common in both animal and plant hybrids. For example, hybrids between a lion and a tigress (\"ligers\") are much larger than either of the two progenitors, while \"tigons\" (lioness × tiger) are smaller. Similarly, the hybrids between the common pheasant (\"Phasianus colchicus\") and domestic fowl (\"Gallus gallus\") are larger than either of their parents, as are those produced between the common pheasant and hen golden pheasant (\"Chrysolophus pictus\"). Spurs are absent in hybrids of the former type, although present in both parents.\n\nHybridization is greatly influenced by human impact on the environment, through effects such as habitat fragmentation and species introductions. Such impacts make it difficult to conserve the genetics of populations undergoing introgressive hybridization. Humans have introduced species worldwide to environments for a long time, both intentionally for purposes such as biological control, and unintentionally, as with accidental escapes of individuals. Introductions can drastically affect populations, including through hybridization.\n\nThere is a kind of continuum with three semi-distinct categories dealing with anthropogenic hybridization: hybridization without introgression, hybridization with widespread introgression (backcrossing with one of the parent species), and hybrid swarms (highly variable populations with much interbreeding as well as backcrossing with the parent species). Depending on where a population falls along this continuum, the management plans for that population will change. Hybridization is currently an area of great discussion within wildlife management and habitat management. Global climate change is creating other changes such as difference in population distributions which are indirect causes for an increase in anthropogenic hybridization.\n\nConservationists disagree on when is the proper time to give up on a population that is becoming a hybrid swarm, or to try and save the still existing pure individuals. Once a population becomes a complete mixture, the goal becomes to conserve those hybrids to avoid their loss. Conservationists treat each case on its merits, depending on detecting hybrids within the population. It is nearly impossible to formulate a uniform hybridization policy, because hybridization can occur beneficially when it occurs \"naturally\", and when hybrid swarms are the only remaining evidence of prior species, they need to be conserved as well.\n\nRegionally developed ecotypes can be threatened with extinction when new alleles or genes are introduced that alter that ecotype. This is sometimes called genetic mixing. Hybridization and introgression, which can happen in natural and hybrid populations, of new genetic material can lead to the replacement of local genotypes if the hybrids are more fit and have breeding advantages over the indigenous ecotype or species. These hybridization events can result from the introduction of non-native genotypes by humans or through habitat modification, bringing previously isolated species into contact. Genetic mixing can be especially detrimental for rare species in isolated habitats, ultimately affecting the population to such a degree that none of the originally genetically distinct population remains.\n\nIn agriculture and animal husbandry, the Green Revolution's use of conventional hybridization increased yields by breeding \"high-yielding varieties\". The replacement of locally indigenous breeds, compounded with unintentional cross-pollination and crossbreeding (genetic mixing), has reduced the gene pools of various wild and indigenous breeds resulting in the loss of genetic diversity. Since the indigenous breeds are often well-adapted to local extremes in climate and have immunity to local pathogens, this can be a significant genetic erosion of the gene pool for future breeding. Therefore, commercial plant geneticists strive to breed \"widely adapted\" cultivars to counteract this tendency.\n\nFamiliar examples of equid hybrids are the mule, a cross between a female horse and a male donkey, and the hinny, a cross between a female donkey and a male horse. Pairs of complementary types like the mule and hinny are called reciprocal hybrids.\nAmong many other mammal crosses are hybrid camels, crosses between a bactrian camel and a dromedary. The first known instance of hybrid speciation in marine mammals was discovered in 2014. The clymene dolphin (\"Stenella clymene\") is a hybrid of two Atlantic species, the spinner and striped dolphins.\n\nCagebird breeders sometimes breed bird hybrids known as mules between species of finch, such as goldfinch × canary.\n\nAmong amphibians, Japanese giant salamanders and Chinese giant salamanders have created hybrids that threaten the survival of Japanese giant salamanders because of competition for similar resources in Japan.\n\nAmong fish, a group of about fifty natural hybrids between Australian blacktip shark and the larger common blacktip shark was found by Australia's eastern coast in 2012.\n\nAmong insects, so-called killer bees were accidentally created during an attempt to breed a strain of bees that would both produce more honey and be better adapted to tropical conditions. It was done by crossing a European honey bee and an African bee.\n\nThe \"Colias eurytheme\" and \"C. philodice\" butterflies have retained enough genetic compatibility to produce viable hybrid offspring. Hybrid speciation may have produced the diverse \"Heliconius\" butterflies, but that is disputed.\n\nPlant species often hybridize more readily than animal species, and the resulting hybrids are fertile more often. Many plant species are the result of hybridization, combined with polyploidy, which duplicates the chromosomes. Chromosome duplication allows orderly meiosis and so viable seed can be produced.\n\nPlant hybrids are generally given names that include an \"×\" (not in italics), such as \"Platanus\" × \"acerifolia\" for the London plane, a natural hybrid of \"P. orientalis\" (oriental plane) and \"P. occidentalis\" (American sycamore).\n\nPlant species that are genetically compatible may not hybridize in nature for various reasons, including geographical isolation, differences in flowering period, or differences in pollinators. Species that are brought together by humans in gardens may hybridize naturally, or hybridization can be facilitated by human efforts, such as altered flowering period or artificial pollination. Hybrids are sometimes created by humans to produce improved plants that have some of the characteristics of each of the parent species. Much work is now being done with hybrids between crops and their wild relatives to improve disease-resistance or climate resilience for both agricultural and horticultural crops.\n\nSome crop plants are hybrids from different genera (intergeneric hybrids), such as Triticale, × \"Triticosecale\", a wheat–rye hybrid. Most modern and ancient wheat breeds are themselves hybrids; bread wheat, \"Triticum aestivum\", is a hexaploid hybrid of three wild grasses. Several commercial fruits including loganberry (\"Rubus\" × \"loganobaccus\") and grapefruit (\"Citrus\" × \"paradisi\") are hybrids, as are garden herbs such as peppermint (\"Mentha\" × \"piperita\"), and trees such as the London plane (\"Platanus\" × \"acerifolia\"). Among many natural plant hybrids is \"Iris albicans\", a sterile hybrid that spreads by rhizome division, and \"Oenothera lamarckiana\", a flower that was the subject of important experiments by Hugo de Vries that produced an understanding of polyploidy.\n\nSterility in a non-polyploid hybrid is often a result of chromosome number; if parents are of differing chromosome pair number, the offspring will have an odd number of chromosomes, which leaves them unable to produce chromosomally-balanced gametes. While that is undesirable in a crop such as wheat, for which growing a crop that produces no seeds would be pointless, it is an attractive attribute in some fruits. Triploid bananas and watermelons are intentionally bred because they produce no seeds and are also parthenocarpic.\n\nThere is evidence of hybridisation between modern humans and other species of the genus \"Homo\". In 2010, the Neanderthal genome project showed that 1–4% of DNA from all people living today, apart from most Sub-Saharan Africans, are of Neanderthal heritage. Analyzing the genomes of 600 Europeans and East Asians found that combining them covered 20% of the Neanderthal genome that is in the modern human population. Ancient human populations lived and interbred with Neanderthals, Denisovans, and at least one other extinct \"Homo\" species. Thus, Neanderthal and Denisovan DNA has been incorporated into human DNA by introgression.\n\nIn 1998, a complete prehistorical skeleton found in Portugal, the Lapedo child, had features of both anatomically modern humans and Neanderthals. Some ancient human skulls with especially large nasal cavities and unusually shaped braincases represent human-Neanderthal hybrids. A 37,000- to 42,000-year-old human jawbone found in Romania's Oase cave contains traces of Neanderthal ancestry from only four to six generations earlier. All genes from Neanderthals in the current human population are descended from Neanderthal fathers and human mothers. A Neanderthal skull unearthed in Italy in 1957 reveals Neanderthal mitochondrial DNA, which is passed on through only the maternal lineage, but the skull has a chin shape similar to modern humans. It is proposed that it was the offspring of a Neanderthal mother and a human father.\n\nFolk tales and myths sometimes contain mythological hybrids; the Minotaur was the offspring of a human, Pasiphaë, and a white bull. More often, they are composites of the physical attributes of two or more kinds of animals, mythical beasts, and humans, with no suggestion that they are the result of interbreeding, as in the centaur (man/horse), chimera (goat/lion/snake), hippocamp (fish/horse), and sphinx (woman/lion). The Old Testament mentions a first generation of half-human hybrid giants, the Nephilim, while the apocryphal Book of Enoch describes the Nephilim as the wicked sons of fallen angels and attractive women.\n\n", "id": "41244", "title": "Hybrid (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=1791725", "text": "Chemotroph\n\nChemotrophs are organisms that obtain energy by the oxidation of electron donors in their environments. These molecules can be organic (chemoorganotrophs) or inorganic (chemolithotrophs). The chemotroph designation is in contrast to phototrophs, which utilize solar energy. Chemotrophs can be either autotrophic or heterotrophic. Chemotrophs are commonly found in ocean floors where sunlight cannot reach them because they are not dependent on solar energy. Ocean floors often contain underwater volcanos that can provide heat to substitute sunlight for warmth.\n\nChemoautotrophs (or chemotrophic autotroph) (\"Greek\": Chemo (χημία) = chemical, auto (αὐτός) = self, troph (τροφιά) = nourishment), in addition to deriving energy from chemical reactions, synthesize all necessary organic compounds from carbon dioxide. Chemoautotrophs use inorganic energy sources such as hydrogen sulfide, elemental sulfur, ferrous iron, molecular hydrogen, and ammonia. Most chemoautotrophs are extremophiles, bacteria or archaea that live in hostile environments (such as deep sea vents) and are the primary producers in such ecosystems. Chemoautotrophs generally fall into several groups: methanogens, halophiles, sulfur oxidizers and reducers, nitrifiers, anammox bacteria, and thermoacidophiles. An example of one of these prokaryotes would be Sulfolobus. Chemolithotrophic growth can be dramatically fast, such as \"Hydrogenovibrio crunogenus\" with a doubling time around one hour. \n\nThe term \"chemosynthesis\", coined in 1897 by Wilhelm Pfeffer, originally was defined as the energy production by oxidation of inorganic substances in association with autotrophy - what would be named today as chemolithoautotrophy. Later, the term would include also the chemoorganoautotrophy, that is, it can be seen as a synonym of chemoautotrophy.\n\nChemoheterotrophs (or chemotrophic heterotrophs) (\"Gr\": Chemo (χημία) = chemical, hetero (ἕτερος) = (an)other, troph (τροφιά) = nourishment) are unable to fix carbon to form their own organic compounds. Chemoheterotrophs can be chemolithoheterotrophs, utilizing inorganic energy sources such as sulfur or chemoorganoheterotrophs, utilizing organic energy sources such as carbohydrates, lipids, and proteins. Other chemoheterotrophs include humans, mushrooms and lithotrophic bacteria. \n\nIn the deep oceans, iron-oxidizing bacteria derive their energy needs by oxidizing ferrous iron (Fe) to ferric iron (Fe). The electron conserved from this reaction reduces the respiratory chain and can be thus used in the synthesis of ATP by forward electron transport or NADH by reverse electron transport, replacing or augmenting traditional phototrophism.\n\nManganese-oxidizing bacteria also make use of igneous lava rocks in much the same way; by oxidizing manganous manganese (Mn) into manganic (Mn) manganese. Manganese is more scarce than iron oceanic crust, but is much easier for bacteria to extract from igneous glass. In addition, each manganese oxidation donates two electrons to the cell versus one for each iron oxidation, though the amount of ATP or NADH that can be synthesised in couple to these reactions varies with pH and specific reaction thermodynamics in terms of how much of a Gibbs free energy change there is during the oxidation reactions versus the energy change required for the formation of ATP or NADH, all of which vary with concentration, pH etc. Much still remains unknown about manganese-oxidizing bacteria because they have not been cultured and documented to any great extent.\n\n\n\n1. Katrina Edwards. \"Microbiology of a Sediment Pond and the Underlying Young, Cold,\nHydrologically Active Ridge Flank\". Woods Hole Oceanographic Institution.\n\n2. Coupled Photochemical and Enzymatic Mn(II) Oxidation Pathways of a Planktonic Roseobacter-Like Bacterium \nColleen M. Hansel and Chris A. Francis* \nDepartment of Geological and Environmental Sciences, Stanford University, Stanford, California 94305-2115 \nReceived 28 September 2005/ Accepted 17 February 2006\n", "id": "1791725", "title": "Chemotroph"}
{"url": "https://en.wikipedia.org/wiki?curid=54850522", "text": "Oxford Dictionary of Biology\n\nOxford Dictionary of Biology (often abbreviated to ODB) is a multiple editions dictionary published by the English Oxford University Press. With more than 5,500 entries, it contains comprehensive information in English on topics relating to biology, biophysics, and biochemistry. The first edition was published in 1985 as \"A Concise Dictionary of Biology\". The seventh edition, \"A Dictionary of Biology\", was published in 2015 and it was edited by Robert Hine and Elizabeth Martin.\n\nRobert Hine studied at Kings College London and University of Aberdeen and since 1984 he has contributed to numerous journals and books.\n\nThe sixth and the seventh edition edition of the \"ODB\" are available online for members of subscribed institutions and for subscribed individuals via Oxford Reference.\n\nThe first edition of \"Oxford Dictionary of Biology\" was first published in 1985 and the seventh edition in 2015.\n\n", "id": "54850522", "title": "Oxford Dictionary of Biology"}
{"url": "https://en.wikipedia.org/wiki?curid=13185451", "text": "Runt\n\nIn a group of animals (usually a litter of animals born in multiple births), a runt is a member which is smaller or weaker than the others. Due to its small size, a runt in a litter faces obvious disadvantage, including difficulties in competing with its siblings for survival and possible rejection by its mother. Therefore, in the wild, a runt is less likely to survive infancy.\n\nEven among domestic animals, runts often face rejection. They may be placed under the direct care of an experienced animal breeder, although the animal's size and weakness coupled with the lack of natural parental care make this difficult. Some tamed animals are the result of reared runts.\n\n\n\n", "id": "13185451", "title": "Runt"}
{"url": "https://en.wikipedia.org/wiki?curid=8327598", "text": "Substrate (biology)\n\nIn biology, a substrate is the surface on which an organism (such as a plant, fungus, or animal) lives. A substrate can include biotic or abiotic materials and animals. For example, encrusting algae that lives on a rock (its substrate) can be itself a substrate for an animal that lives on top of the algae. \n\n\nRequirements for animal cell and tissue culture are the same as described for plant cell, tissue and organ culture (In Vitro Culture Techniques: The Biotechnological Principles). Desirable requirements are (i) air conditioning of a room, (ii) hot room with temperature recorder, (iii) microscope room for carrying out microscopic work where different types of microscopes should be installed, (iv) dark room, (v) service room, (vi) sterilization room for sterilization of glassware and culture media, and (vii) preparation room for media preparation, etc. In addition the storage areas should be such where following should be kept properly : (i) liquids-ambient (4-20°C), (ii) glassware-shelving, (iii) plastics-shelving, (iv) small items-drawers, (v) specialized equipments-cupboard, slow turnover, (vi) chemicals-sidled containers.\n\nThere are many types of vertebrate cells that require support for their growth in vitro otherwise they will not grow properly. Such cells are called anchorage-dependent cells. Therefore, a large number of substrates which may be adhesive (e.g. plastic, glass, palladium, metallic surfaces, etc.) or non-adhesive (e.g. agar, agarose, etc.) types may be used as discussed below:\n\n\n", "id": "8327598", "title": "Substrate (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=55989372", "text": "Microenvironment (biology)\n\nMicroenvironment in biology is defined as the normal cells, molecules, and blood vessels that surround and feed a particular cellular area. The microenvironment in a tumor is different from that of a healthy organ; the microenvironment can affect how the tumor grows and spreads.\nThe term originated in the oncology literature in 1975 and has been used with increasing frequency in the biomedical literature since the 2000´s. \n", "id": "55989372", "title": "Microenvironment (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=13980", "text": "Homeostasis\n\nHomeostasis can be defined as the stable condition of an organism and of its internal environment; or as the maintenance or regulation of the stable condition, or its equilibrium; or simply as the balance of bodily functions. The stable condition is the condition of optimal functioning for the organism, and is dependent on many variables, such as body temperature and fluid balance, being kept within certain pre-set limits. Other variables include the pH of extracellular fluid, the concentrations of sodium, potassium and calcium ions, as well as that of the blood sugar level, and these need to be regulated despite changes in the environment, diet, or level of activity. Each of these variables is controlled by one or more regulators or homeostatic mechanisms, which together maintain life.\n\nHomeostasis is brought about by a natural resistance to change in the optimal conditions, and equilibrium is maintained by many regulatory mechanisms. All homeostatic control mechanisms have at least three interdependent components for the variable being regulated: a receptor, a control centre, and an effector. The receptor is the sensing component that monitors and responds to changes in the environment, either external or internal. Receptors include thermoreceptors, and mechanoreceptors. Control centres include the respiratory centre, and the renin-angiotensin system. An effector is the target acted on, to bring about the change back to the normal state. At the cellular level, receptors include nuclear receptors that bring about changes in gene expression through up-regulation or down-regulation, and act in negative feedback mechanisms. An example of this is in the control of bile acids in the liver.\n\nSome centres such as the renin-angiotensin system, control more than one variable. When the receptor senses a stimulus, it reacts by sending an action potential to a control centre. The control centre sets the maintenance range, the acceptable upper and lower limits, for the particular variable such as temperature. The control center responds to the signal by determining an appropriate response and sending signals to an effector which can be one or more muscles, an organ, or a gland. When the signal is received and acted on, negative feedback is fed back to the receptor that stops the need for further signalling.\n\nThe concept of the regulation of the internal environment was described by French physiologist Claude Bernard in 1865, and the word \"homeostasis\" was coined by Walter Bradford Cannon in 1926. Homeostasis is an almost exclusively biological term, referring to the concepts described by Bernard and Cannon, concerning the constancy of the internal environment in which the cells of the body live and survive. The term cybernetics is applied to technological control systems such as thermostats, which function as homeostatic mechanisms, but is often defined much more broadly than the biological term of homeostasis.\n\nThe word \"homeostasis\" () uses combining forms of \"homeo-\" and \"-stasis\", New Latin from Greek: ὅμοιος \"homoios\", \"similar\" and στάσις \"stasis\", \"standing still\", yielding the idea of \"staying the same\".\n\nThe metabolic processes of all organisms can only take place in very specific physical and chemical environments. The conditions vary with each organism, and with whether the chemical processes take place inside the cell or in the interstitial fluid bathing the cells. The best known homeostatic mechanisms in humans and other mammals are regulators that keep the composition of the extracellular fluid (or the \"internal environment\") constant, especially with regard to the temperature, pH, osmolality, and the concentrations of sodium, potassium, glucose, carbon dioxide, and oxygen. However, a great many other homeostatic mechanisms, encompassing many aspects of human physiology, control other entities in the body. Where the levels of variables are higher or lower than those needed, they are often prefixed with \"hyper-\" and \"hypo-\", respectively such as hyperthermia and hypothermia and hypertension and hypotension.\nIf an entity is homeostatically controlled it does not imply that its value is necessarily absolutely steady in health. Core body temperature is, for instance, regulated by a homeostatic mechanism with temperature sensors in, amongst others, the hypothalamus of the brain. However, the set point of the regulator is regularly reset. For instance, core body temperature in humans varies during the course of the day (i.e. has a circadian rhythm), with the lowest temperatures occurring at night, and the highest in the afternoons. Other normal temperature variations include those related to the menstrual cycle. The temperature regulator's set point is reset during infections to produce a fever. Organisms are capable of adjusting somewhat to varied conditions such as temperature changes or oxygen levels at altitude, by a process of acclimatisation.\n\nHomeostasis does not govern every activity in the body. For instance the signal (be it via neurons or hormones) from the sensor to the effector is, of necessity, highly variable in order to convey information about the direction and magnitude of the error detected by the sensor. Similarly the effector’s response needs to be highly adjustable to reverse the error – in fact it should be very nearly in proportion (but in the opposite direction) to the error that is threatening the internal environment. For instance, the arterial blood pressure in mammals is homeostatically controlled, and measured by stretch receptors in the walls of the aortic arch and carotid sinuses at beginnings of the internal carotid arteries. The sensors send messages via sensory nerves to the medulla oblongata of the brain indicating whether the blood pressure has fallen or risen, and by how much. The medulla oblongata then distributes messages along motor or efferent nerves belonging to the autonomic nervous system to a wide variety of effector organs, whose activity is consequently changed to reverse the error in the blood pressure. One of the effector organs is the heart whose rate is stimulated to rise (tachycardia) when the arterial blood pressure falls, or to slow down (bradycardia) when the pressure rises above set point. Thus the heart rate (for which there is no sensor in the body) is not homeostatically controlled, but is one of effector responses to errors in the arterial blood pressure. Another example is the rate of sweating. This is one of the effectors in the homeostatic control of body temperature, and therefore highly variable in rough proportion to the heat load that threatens to destabilize the body’s core temperature, for which there is a sensor in the hypothalamus of the brain.\n\nMammals regulate their core temperature using input from thermoreceptors in the hypothalamus, brain, spinal cord, internal organs, and great veins. Apart from the internal regulation of temperature, a process called allostasis can come into play that adjusts behaviour to adapt to the challenge of very hot or cold extremes (and to other challenges). These adjustments may include seeking shade and reducing activity, or seeking warmer conditions and increasing activity, or huddling.\nBehavioural thermoregulation takes precedence over physiological thermoregulation since necessary changes can be affected more quickly and physiological thermoregulation is limited in its capacity to respond to extreme temperatures.\n\nWhen core temperature falls, the blood supply to the skin is reduced by intense vasoconstriction. The blood flow to the limbs (which have a large surface area) is similarly reduced, and returned to the trunk via the deep veins which lie alongside the arteries (forming venae comitantes). This acts as a counter-current exchange system which short-circuits the warmth from the arterial blood directly into the venous blood returning into the trunk, causing minimal heat loss from the extremities in cold weather. The subcutaneous limb veins are tightly constricted, not only reducing heat loss from this source, but also forcing the venous blood into the counter-current system in the depths of the limbs.\n\nThe metabolic rate is increased, initially by non-shivering thermogenesis, followed by shivering thermogenesis if the earlier reactions are insufficient to correct the hypothermia.\n\nWhen core temperature rises are detected by thermoreceptors, the sweat glands in the skin are stimulated via cholinergic sympathetic nerves to secrete sweat onto the skin, which, when it evaporates, cools the skin and the blood flowing through it. Panting is an alternative effector in many vertebrates, which cools the body also by the evaporation of water, but this time from the mucous membranes of the throat and mouth.\n\nBlood sugar levels are regulated within fairly narrow limits. In mammals the primary sensors for this are the beta cells of the pancreatic islets. The beta cells respond to a rise in the blood sugar level by secreting insulin into the blood, and simultaneously inhibiting their neighboring alpha cells from secreting glucagon into the blood. This combination (high blood insulin levels and low glucagon levels) act on effector tissues, chief of which are the liver, fat cells and muscle cells. The liver is inhibited from producing glucose, taking it up instead, and converting it to glycogen and triglycerides. The glycogen is stored in the liver, but the triglycerides are secreted into the blood as very low-density lipoprotein (VLDL) particles which are taken up by adipose tissue, there to be stored as fats. The fat cells take up glucose through special glucose transporters (GLUT4), whose numbers in the cell wall are increased as a direct effect of insulin acting on these cells. The glucose that enters the fat cells in this manner is converted into triglycerides (via the same metabolic pathways as are used by the liver) and then stored in those fat cells together with the VLDL-derived triglycerides that were made in the liver. Muscle cells also take glucose up through insulin-sensitive GLUT4 glucose channels, and convert it into muscle glycogen.\n\nA fall in blood glucose, causes insulin secretion to be stopped, and glucagon to be secreted from the alpha cells into the blood. This inhibits the uptake of glucose from the blood by the liver, fats cells and muscle. Instead the liver is strongly stimulated to manufacture glucose from glycogen (through glycogenolysis) and from non-carbohydrate sources (such as lactate and de-aminated amino acids) using a process known as gluconeogenesis. The glucose thus produced is discharged into the blood correcting the detected error (hypoglycemia). The glycogen stored in muscles remains in the muscles, and is only broken down, during exercise, to glucose-6-phosphate and thence to pyruvate to be fed into the citric acid cycle or turned into lactate. It is only the lactate and the waste products of the citric acid cycle that are returned to the blood. The liver can take up only the lactate, and by the process of energy consuming gluconeogenesis convert it back to glucose.\n\nChanges in the levels of oxygen, carbon dioxide and plasma pH are sent to the respiratory center, in the brainstem where they are regulated.\nThe partial pressure of oxygen and carbon dioxide in the arterial blood is monitored by the peripheral chemoreceptors (PNS) in the carotid artery and aortic arch. A change in the partial pressure of carbon dioxide is detected as altered pH in the cerebrospinal fluid by central chemoreceptors (CNS) in the medulla oblongata of the brainstem. Information from these sets of sensors is sent to the respiratory center which activates the effector organs – the diaphragm and other muscles of respiration. An increased level of carbon dioxide in the blood, or a decreased level of oxygen, will result in a deeper breathing pattern and increased respiratory rate to bring the blood gases back to equilibrium.\n\nToo little carbon dioxide, and, to a lesser extent, too much oxygen in the blood can temporarily halt breathing, a condition known as apnea, which freedivers use to prolong the time they can stay underwater.\n\nThe partial pressure of carbon dioxide is more of a deciding factor in the monitoring of pH. However, at high altitude (above 2500 m) the monitoring of the partial pressure of oxygen takes priority, and hyperventilation keeps the oxygen level constant. With the lower level of carbon dioxide, to keep the pH at 7.4 the kidneys secrete hydrogen ions into the blood, and excrete bicarbonate into the urine. This is important in the acclimatization to high altitude.\n\nThe kidneys measure the oxygen content rather than the partial pressure of oxygen in the arterial blood. When the oxygen content of the blood is chronically low, oxygen-sensitive cells secrete erythropoietin (EPO) into the blood. The effector tissue is the red bone marrow which produces red blood cells (RBCs)(erythrocytes). The increase in RBCs leads to an increased hematocrit in the blood, and subsequent increase in hemoglobin that increases the oxygen carrying capacity. This is the mechanism whereby high altitude dwellers have higher hematocrits than sea-level residents, and also why persons with pulmonary insufficiency or right-to-left shunts in the heart (through which venous blood by-passes the lungs and goes directly into the systemic circulation) have similarly high hematocrits.\n\nRegardless of the partial pressure of oxygen in the blood, the amount of oxygen that can be carried, depends on the hemoglobin content. The partial pressure of oxygen may be sufficient for example in anemia, but the hemoglobin content will be insufficient and subsequently as will be the oxygen content. Given enough supply of iron, vitamin B12 and folic acid, EPO can stimulate RBC production, and hemoglobin and oxygen content restored to normal.\n\nHigh pressure receptors called baroreceptors in the walls of the aortic arch and carotid sinus (at the beginning of the internal carotid artery) monitor the arterial blood pressure. Rising pressure is detected when the walls of the arteries stretch due to an increase in blood volume. This causes heart muscle cells to secrete the hormone atrial natriuretic peptide (ANP) into the blood. This acts on the kidneys to inhibit the secretion of renin and aldosterone causing the release of sodium, and accompanying water into the urine, thereby reducing the blood volume.\nThis information is then conveyed, via afferent nerve fibers, to the solitary nucleus in the medulla oblongata. From here motor nerves belonging to the autonomic nervous system are stimulated to influence the activity of chiefly the heart and the smallest diameter arteries, called arterioles. The arterioles are the main resistance vessels in the arterial tree, and small changes in diameter cause large changes in the resistance to flow through them. When the arterial blood pressure rises the arterioles are stimulated to dilate making it easier for blood to leave the arteries, thus deflating them, and bringing the blood pressure down, back to normal. At the same time the heart is stimulated via cholinergic parasympathetic nerves to beat more slowly (called bradycardia), ensuring that the inflow of blood into the arteries is reduced, thus adding to the reduction in pressure, and correction of the original error.\n\nIf the pressure in the arteries falls, the opposite reflex is elicited: constriction of the arterioles, and a speeding up of the heart rate (called tachycardia). If the drop in blood pressure is very rapid or excessive, the medulla oblongata stimulates the adrenal medulla, via \"preganglionic\" sympathetic nerves, to secrete epinephrine (adrenaline) into the blood. This hormone enhances the tachycardia and causes severe vasoconstriction of the arterioles to all but the essential organ in the body (especially the heart, lungs and brain). These reactions usually correct the low arterial blood pressure (hypotension) very effectively.\n\nThe plasma ionized calcium (Ca) concentration is very tightly controlled by a pair of homeostatic mechanisms. The sensor for the first one is situated in the parathyroid glands, where the chief cells sense the Ca level by means of specialized calcium receptors in their membranes. The sensors for the second are the parafollicular cells in the thyroid gland. The parathyroid chief cells secrete parathyroid hormone (PTH) in response to a fall in the plasma ionized calcium level; the parafollicular cells of the thyroid gland secrete calcitonin in response to a rise in the plasma ionized calcium level.\n\nThe effector organs of the first homeostatic mechanism are the bones, the kidney, and, via a hormone released into the blood by the kidney in response to high PTH levels in the blood, the duodenum and jejunum. Parathyroid hormone (in high concentrations in the blood) causes bone resorption, releasing calcium into the plasma. This is a very rapid action which can correct a threatening hypocalcemia within minutes. High PTH concentrations cause the excretion of phosphate ions via the urine. Since phosphates combine with calcium ions to form insoluble salts, a decrease in the level of phosphates in the blood, releases free calcium ions into the plasma ionized calcium pool. PTH has a second action on the kidneys. It stimulates the manufacture and release, by the kidneys, of calcitriol into the blood. This steroid hormone acts on the epithelial cells of the upper small intestine, increasing their capacity to absorb calcium from the gut contents into the blood.\n\nThe second homeostatic mechanism, with its sensors in the thyroid gland, releases calcitonin into the blood when the blood ionized calcium rises. This hormone acts primarily on bone, causing the rapid removal of calcium from the blood and depositing it, in insoluble form, in the bones.\n\nThe two homeostatic mechanisms working through PTH on the one hand, and calcitonin on the other can very rapidly correct any impending error in the plasma ionized calcium level by either removing calcium from the blood and depositing it in the skeleton, or by removing calcium from it. The skeleton acts as an extremely large calcium store (about 1 kg) compared with the plasma calcium store (about 180 mg). Longer term regulation occurs through calcium absorption or loss from the gut.\n\nThe homeostatic mechanism which controls the plasma sodium concentration is rather more complex than most of the other homeostatic mechanisms described on this page.\nThe sensor is situated in the juxtaglomerular apparatus of kidneys, which senses the plasma sodium concentration in a surprisingly indirect manner. Instead of measuring it directly in the blood flowing past the juxtaglomerular cells, these cells respond to the sodium concentration in the renal tubular fluid after it has already undergone a certain amount of modification in the proximal convoluted tubule and loop of Henle. These cells also respond to rate of blood flow through the juxtaglomerular apparatus, which, under normal circumstances, is directly proportional to the arterial blood pressure, making this tissue an ancillary arterial blood pressure sensor.\n\nIn response to a lowering of the plasma sodium concentration, or to a fall in the arterial blood pressure, the juxtaglomerular cells release renin into the blood. Renin is an enzyme which cleaves a decapeptide (a short protein chain, 10 amino acids long) from a plasma α-2-globulin called angiotensinogen. This decapeptide is known as angiotensin I. It has no known biological activity. However, when the blood circulates through the lungs a pulmonary capillary endothelial enzyme called angiotensin-converting enzyme (ACE) cleaves a further two amino acids from angiotensin I to form an octapeptide known as angiotensin II. Angiotensin II is a hormone which acts on the adrenal cortex, causing the release into the blood of the steroid hormone, aldosterone. Angiotensin II also acts on the smooth muscle in the walls of the arterioles causing these small diameter vessels to constrict, thereby restricting the outflow of blood from the arterial tree, causing the arterial blood pressure to rise. This therefore reinforces the measures described above (under the heading of \"Arterial blood pressure\"), which defend the arterial blood pressure against changes, especially hypotension.\n\nThe angiotensin II-stimulated aldosterone released from the zona glomerulosa of the adrenal glands has an effect on particularly the epithelial cells of the distal convoluted tubules and collecting ducts of the kidneys. Here it causes the reabsorption of sodium ions from the renal tubular fluid, in exchange for potassium ions which are secreted from the blood plasma into the tubular fluid to exit the body via the urine. The reabsorption of sodium ions from the renal tubular fluid halts further sodium ion losses from the body, and therefore preventing the worsening of hyponatremia. The hyponatremia can only be \"corrected\" by the consumption of salt in the diet. However, it is not certain whether a \"salt hunger\" can be initiated by hyponatremia, or by what mechanism this might come about.\n\nWhen the plasma sodium ion concentration is higher than normal (hypernatremia), the release of renin from the juxtaglomerular apparatus is halted, ceasing the production of angiotensin II, and its consequent aldosterone-release into the blood. The kidneys respond by excreting sodium ions into the urine, thereby normalizing the plasma sodium ion concentration. The low angiotensin II levels in the blood lower the arterial blood pressure as an inevitable concomitant response.\n\nThe reabsorption of sodium ions from the tubular fluid as a result of high aldosterone levels in the blood does not, of itself, cause renal tubular water to be returned to the blood from the distal convoluted tubules or collecting ducts. This is because sodium is reabsorbed in exchange for potassium and therefore causes only a modest change in the osmotic gradient between the blood and the tubular fluid. Furthermore, the epithelium of the distal convoluted tubules and collecting ducts is impermeable to water in the absence of antidiuretic hormone (ADH) in the blood. ADH is part of the control of fluid balance. Its levels in the blood vary with the osmolality of the plasma, which is measured in the hypothalamus of the brain. Aldosterone's action on the kidney tubules prevents sodium loss to the extracellular fluid (ECF). So there is no change in the osmolality of the ECF, and therefore no change in the ADH concentration of the plasma. However, low aldosterone levels cause a loss of sodium ions from the ECF, which could potentially cause a change in extracellular osmolality and therefore of ADH levels in the blood.\n\nHigh potassium concentrations in the plasma cause depolarization of the zona glomerulosa cells’ membranes in the outer layer of the adrenal cortex. This causes the release of aldosterone into the blood.\n\nAldosterone acts primarily on the distal convoluted tubules and collecting ducts of the kidneys, stimulating the excretion of potassium ions into the urine. It does so, however, by activating the basolateral Na/K pumps of the tubular epithelial cells. These sodium/potassium exchangers pump three sodium ions out of the cell, into the interstitial fluid and two potassium ions into the cell from the interstitial fluid. This creates an ionic concentration gradient which results in the reabsorption of sodium (Na) ions from the tubular fluid into the blood, and secreting potassium (K) ions from the blood into the urine (lumen of collecting duct).\n\nThe total amount of water in the body needs to be kept in balance. Fluid balance involves keeping the fluid volume stabilised, and also keeping the levels of electrolytes in the extracellular fluid stable. Fluid balance is maintained by the process of osmoregulation and by behaviour. Osmotic pressure is detected by osmoreceptors in the median preoptic nucleus in the hypothalamus. Measurement of the plasma osmolality to give an indication of the water content of the body, relies on the fact that water losses from the body, (through unavoidable water loss through the skin which is not entirely waterproof and therefore always slightly moist, water vapor in the exhaled air, sweating, vomiting, normal feces and especially diarrhea) are all hypotonic, meaning that they are less salty than the body fluids (compare, for instance, the taste of saliva with that of tears. The latter have almost the same salt content as the extracellular fluid, whereas the former is hypotonic with respect to plasma. Saliva does not taste salty, whereas tears are decidedly salty). Nearly all normal and abnormal losses of body water therefore cause the extracellular fluid to become hypertonic. Conversely excessive fluid intake dilutes the extracellular fluid causing the hypothalamus to register hypotonic hyponatremia conditions.\n\nWhen the hypothalamus detects a hypertonic extracellular environment, it causes the secretion of an antidiuretic hormone (ADH) called vasopressin which acts on the effector organ, which in this case is the kidney. The effect of vasopressin on the kidney tubules is to reabsorb water from the distal convoluted tubules and collecting ducts, thus preventing aggravation of the water loss via the urine. The hypothalamus simultaneously stimulates the nearby thirst center causing an almost irresistible (if the hypertonicity is severe enough) urge to drink water. The cessation of urine flow prevents the hypovolemia and hypertonicity from getting worse; the drinking of water corrects the defect.\n\nHypo-osmolality results in very low plasma ADH levels. This results in the inhibition of water reabsorption from the kidney tubules, causing high volumes of very dilute urine to be excreted, thus getting rid of the excess water in the body.\n\nUrinary water loss, when the body water homeostat is intact, is a \"compensatory\" water loss, \"correcting\" any water excess in the body. However, since the kidneys cannot generate water, the thirst reflex is the all important second effector mechanism of the body water homeostat, \"correcting\" any water deficit in the body.\n\nThe plasma pH can be altered by respiratory changes in the partial pressure of carbon dioxide; or altered by metabolic changes in the carbonic acid to bicarbonate ion ratio. The bicarbonate buffer system regulates the ratio of carbonic acid to bicarbonate to be equal to 1:20, at which ratio the blood pH is 7.4 (as explained in the Henderson–Hasselbalch equation). A change in the plasma pH gives an acid–base imbalance.\nIn acid–base homeostasis there are two mechanisms that can help regulate the pH. Respiratory compensation a mechanism of the respiratory center, adjusts the partial pressure of carbon dioxide by changing the rate and depth of breathing, to bring the pH back to normal. The partial pressure of carbon dioxide also determines the concentration of carbonic acid, and the bicarbonate buffer system can also come into play. Renal compensation can help the bicarbonate buffer system.\nThe sensor for the plasma bicarbonate concentration is not known for certain. It is very probable that the renal tubular cells of the distal convoluted tubules are themselves sensitive to the pH of the plasma. The metabolism of these cells produces carbon dioxide, which is rapidly converted to hydrogen and bicarbonate through the action of carbonic anhydrase. When the ECF pH falls (becoming more acidic) the renal tubular cells excrete hydrogen ions into the tubular fluid to leave the body via urine. Bicarbonate ions are simultaneously secreted into the blood that decreases the carbonic acid, and consequently raises the plasma pH. The converse happens when the plasma pH rises above normal: bicarbonate ions are excreted into the urine, and hydrogen ions released into the plasma.\n\nWhen hydrogen ions are excreted into the urine, and bicarbonate into the blood, the latter combine with the excess hydrogen ions in the plasma that stimulated the kidneys to perform this operation. The resulting reaction in the plasma is the formation of carbonic acid which is in equilibrium with the plasma partial pressure of carbon dioxide. This is tightly regulated to ensure that there is no excessive build-up of carbonic acid or bicarbonate. \nof carbonic acid or bicarbonate ions in the blood plasma. The overall effect is therefore that hydrogen ions are lost in the urine when the pH of the plasma falls. The concomitant rise in the plasma bicarbonate mops up the increased hydrogen ions (caused by the fall in plasma pH) and the resulting excess carbonic acid is disposed of in the lungs as carbon dioxide. This restores the normal ratio between bicarbonate and the partial pressure of carbon dioxide and therefore the plasma pH.\nThe converse happens when a high plasma pH stimulates the kidneys to secrete hydrogen ions into the blood and to excrete bicarbonate into the urine. The hydrogen ions combine with the excess bicarbonate ions in the plasma, once again forming an excess of carbonic acid which can be exhaled, as carbon dioxide, in the lungs, keeping the plasma bicarbonate ion concentration, the partial pressure of carbon dioxide and, therefore, the plasma pH, constant.\n\nCerebrospinal fluid (CSF) allows for regulation of the distribution of substances between cells of the brain, and neuroendocrine factors, to which slight changes can cause problems or damage to the nervous system. For example, high glycine concentration disrupts temperature and blood pressure control, and high CSF pH causes dizziness and syncope.\n\nInhibitory neurons in the central nervous system play a homeostatic role in the balance of neuronal activity between excitation and inhibition. Inhibitory neurons using GABA, make compensating changes in the neuronal networks preventing runaway levels of excitation. An imbalance between excitation and inhibition is seen to be implicated in a number of neuropsychiatric disorders.\n\nThe neuroendocrine system is the mechanism by which the hypothalamus maintains homeostasis, regulating metabolism, reproduction, eating and drinking behaviour, energy utilization, osmolarity and blood pressure.\n\nThe regulation of metabolism, is carried out by hypothalamic interconnections to other glands. \nThree endocrine glands of the hypothalamic–pituitary–gonadal axis (HPG axis) often work together and have important regulatory functions. Two other regulatory endocrine axes are the hypothalamic–pituitary–adrenal axis (HPA axis) and the hypothalamic–pituitary–thyroid axis (HPT axis).\n\nThe liver also has many regulatory functions of the metabolism. An important function is the production and control of bile acids. Too much bile acid can be toxic to cells and its synthesis can be inhibited by activation of FXR a nuclear receptor.\n\nAt the cellular level, homeostasis is carried out by several mechanisms including transcriptional regulation that can alter the activity of genes in response to changes.\n\nThe amount of energy taken in through nutrition needs to match the amount of energy used. To achieve energy homeostasis appetite is regulated by two hormones, grehlin and leptin. Grehlin stimulates hunger and the intake of food and leptin acts to signal satiety (fullness).\n\nMany diseases are the result of a homeostatic failure. Almost any homeostatic component can malfunction, either as a result of an inherited defect, an inborn error of metabolism, or an acquired disease. Some homeostatic mechanisms have inbuilt redundancies, which ensures that life is not immediately threatened if a component malfunctions; but sometimes a homeostatic malfunction can result in serious disease, which can be fatal if not treated. A well known example of a homeostatic failure is shown in type 1 diabetes mellitus. Here blood sugar regulation is unable to function because the beta cells of the pancreatic islets are destroyed and cannot produce the necessary insulin. The blood sugar rises in a condition known as hyperglycemia.\n\nThe plasma ionized calcium homeostat can be disrupted by the constant, unchanging, over-production of parathyroid hormone by a parathyroid adenoma resulting in the typically features of hyperparathyroidism, namely high plasma ionized Ca levels and the resorption of bone, which can lead to spontaneous fractures. The abnormally high plasma ionized calcium concentrations cause conformational changes in many cell-surface proteins (especially ion channels and hormone or neurotransmitter receptors) giving rise to lethargy, muscle weakness, anorexia, constipation and labile emotions.\n\nThe body water homeostat can be compromised by the inability to secrete ADH in response to even the normal daily water losses via the exhaled air, the feces, and insensible sweating. On receiving a zero blood ADH signal, the kidneys produce huge unchanging volumes of very dilute urine, causing dehydration and death if not treated.\n\nAs organisms age, the efficiency of their control systems becomes reduced. The inefficiencies gradually result in an unstable internal environment that increases the risk of illness, and leads to the physical changes associated with aging.\n\nVarious chronic diseases are kept under control by homeostatic compensation, which masks a problem by compensating for it (making up for it) in another way. However, the compensating mechanisms eventually wear out or are disrupted by a new complicating factor (such as the advent of a concurrent acute viral infection), which sends the body reeling through a new cascade of events. Such decompensation unmasks the underlying disease, worsening its symptoms. Common examples include decompensated heart failure, kidney failure, and liver failure.\n\nIn the Gaia hypothesis, James Lovelock stated that the entire mass of living matter on Earth (or any planet with life) functions as a vast homeostatic superorganism that actively modifies its planetary environment to produce the environmental conditions necessary for its own survival. In this view, the entire planet maintains several homeostats (the primary one being temperature homeostasis). Whether this sort of system is present on Earth is open to debate. However, some relatively simple homeostatic mechanisms are generally accepted. For example, it is sometimes claimed that when atmospheric carbon dioxide levels rise, certain plants may be able to grow better and thus act to remove more carbon dioxide from the atmosphere. However, warming has exacerbated droughts, making water the actual limiting factor on land. When sunlight is plentiful and atmospheric temperature climbs, it has been claimed that the phytoplankton of the ocean surface waters, acting as global sunshine, and therefore heat sensors, may thrive and produce more dimethyl sulfide (DMS). The DMS molecules act as cloud condensation nuclei, which produce more clouds, and thus increase the atmospheric albedo, and this feeds back to lower the temperature of the atmosphere. However, rising sea temperature has stratified the oceans, separating warm, sunlit waters from cool, nutrient-rich waters. Thus, nutrients have become the limiting factor, and plankton levels have actually fallen over the past 50 years, not risen. As scientists discover more about Earth, vast numbers of positive and negative feedback loops are being discovered, that, together, maintain a metastable condition, sometimes within very broad range of environmental conditions.\n\nPredictive homeostasis is an anticipatory response to an expected challenge in the future, such as the stimulation of insulin secretion by gut hormones which enter the blood in response to a meal. This insulin secretion occurs before the blood sugar level rises, lowering the blood sugar level in anticipation of a large influx into the blood of glucose resulting from the digestion of carbohydrates in the gut. Such anticipatory reactions are open loop systems which are based, essentially, on \"guess work\", and are not self-correcting. Anticipatory responses always require a closed loop negative feedback system to correct the 'over-shoots' and 'under-shoots' to which the anticipatory systems are prone.\n\nThe term has come to be used in other fields, for example:\n\nAn actuary may refer to \"risk homeostasis\", where (for example) people who have anti-lock brakes have no better safety record than those without anti-lock brakes, because the former unconsciously compensate for the safer vehicle via less-safe driving habits. Previous to the innovation of anti-lock brakes, certain maneuvers involved minor skids, evoking fear and avoidance: Now the anti-lock system moves the boundary for such feedback, and behavior patterns expand into the no-longer punitive area. It has also been suggested that ecological crises are an instance of risk homeostasis in which a particular behavior continues until proven dangerous or dramatic consequences actually occur.\n\nSociologists and psychologists may refer to \"stress homeostasis\", the tendency of a population or an individual to stay at a certain level of stress, often generating artificial stresses if the \"natural\" level of stress is not enough.\n\nJean-François Lyotard, a postmodern theorist, has applied this term to societal 'power centers' that he describes in \"The Postmodern Condition\", as being 'governed by a principle of homeostasis,' for example, the scientific hierarchy, which will sometimes ignore a radical new discovery for years because it destabilizes previously accepted norms.\n\nFamiliar technological homeostatic mechanisms include:\n\n\n", "id": "13980", "title": "Homeostasis"}
{"url": "https://en.wikipedia.org/wiki?curid=937971", "text": "Endemism\n\nEndemism is the ecological state of a species being unique to a defined geographic location, such as an island, nation, country or other defined zone, or habitat type; organisms that are indigenous to a place are not endemic to it if they are also found elsewhere. The extreme opposite of endemism is cosmopolitan distribution. An alternative term for a species that is endemic is precinctive, which applies to species (and subspecific categories) that are restricted to a defined geographical area.\n\nThe word \"endemic\" is from New Latin \"endēmicus\", from Greek ενδήμος, \"endēmos\", \"native\". \"Endēmos\" is formed of \"en\" meaning \"in\", and \"dēmos\" meaning \"the people\". The term \"precinctive\" has been suggested by some scientists, and was first used in botany by MacCaughey in 1917. It is the equivalent of \"endemism\". \"Precinction\" was perhaps first used by Frank and McCoy. \"Precinctive\" seems to have been coined by David Sharp when describing the Hawaiian fauna in 1900: \"I use the word precinctive in the sense of 'confined to the area under discussion' ... 'precinctive forms' means those forms that are confined to the area specified.\" That definition excludes artificial confinement of examples by humans in far-off botanical gardens or zoological parks.\n\nPhysical, climatic, and biological factors can contribute to endemism. The orange-breasted sunbird is exclusively found in the fynbos vegetation zone of southwestern South Africa. The glacier bear is found only in limited places in Southeast Alaska. Political factors can play a part if a species is protected, or actively hunted, in one jurisdiction but not another.\n\nThere are two subcategories of endemism: paleoendemism and neoendemism. Paleoendemism refers to species that were formerly widespread but are now restricted to a smaller area. Neoendemism refers to species that have recently arisen, such as through divergence and reproductive isolation or through hybridization and polyploidy in plants.\n\nEndemic types or species are especially likely to develop on geographically and biologically isolated areas such as islands and remote island groups, such as Hawaii, the Galápagos Islands, and Socotra; they can equally develop in biologically isolated areas such as the highlands of Ethiopia, or large bodies of water far from other lakes, like Lake Baikal. \"Hydrangea hirta\" is an example of an endemic species found in Japan.\n\nEndemics can easily become endangered or extinct if their restricted habitat changes, particularly—but not only—due to human actions, including the introduction of new organisms. There were millions of both Bermuda petrels and \"Bermuda cedars\" (actually \"junipers\") in Bermuda when it was settled at the start of the seventeenth century. By the end of the century, the petrels were thought extinct. Cedars, already ravaged by centuries of shipbuilding, were driven nearly to extinction in the twentieth century by the introduction of a parasite. Bermuda petrels and cedars are now rare, as are other species endemic to Bermuda.\n\nPrincipal causes of habitat degradation and loss in highly endemistic ecosystems include agriculture, urban growth, surface mining, mineral extraction, logging operations and slash-and-burn agriculture.\n\n", "id": "937971", "title": "Endemism"}
{"url": "https://en.wikipedia.org/wiki?curid=99358", "text": "Biogeography\n\nBiogeography is the study of the distribution of species and ecosystems in geographic space and through geological time. Organisms and biological communities often vary in a regular fashion along geographic gradients of latitude, elevation, isolation and habitat area. Phytogeography is the branch of biogeography that studies the distribution of plants. Zoogeography is the branch that studies distribution of animals.\n\nKnowledge of spatial variation in the numbers and types of organisms is as vital to us today as it was to our early human ancestors, as we adapt to heterogeneous but geographically predictable environments. Biogeography is an integrative field of inquiry that unites concepts and information from ecology, evolutionary biology, geology, and physical geography.\n\nModern biogeographic research combines information and ideas from many fields, from the physiological and ecological constraints on organismal dispersal to geological and climatological phenomena operating at global spatial scales and evolutionary time frames.\n\nThe short-term interactions within a habitat and species of organisms describe the ecological application of biogeography. Historical biogeography describes the long-term, evolutionary periods of time for broader classifications of organisms. Early scientists, beginning with Carl Linnaeus, contributed to the development of biogeography as a science. Beginning in the mid-18th century, Europeans explored the world and discovered the biodiversity of life.\n\nThe scientific theory of biogeography grows out of the work of Alexander von Humboldt (1769–1859), Hewett Cottrell Watson (1804–1881), Alphonse de Candolle (1806–1893), Alfred Russel Wallace (1823–1913), Philip Lutley Sclater (1829–1913) and other biologists and explorers.\n\nThe patterns of species distribution across geographical areas can usually be explained through a combination of historical factors such as: speciation, extinction, continental drift, and glaciation. Through observing the geographic distribution of species, we can see associated variations in sea level, river routes, habitat, and river capture. Additionally, this science considers the geographic constraints of landmass areas and isolation, as well as the available ecosystem energy supplies.\n\nOver periods of ecological changes, biogeography includes the study of plant and animal species in: their past and/or present living \"refugium\" habitat; their interim living sites; and/or their survival locales. As writer David Quammen put it, \"...biogeography does more than ask \"Which species?\" and \"Where\". It also asks \"Why?\" and, what is sometimes more crucial, \"Why not?\".\"\n\nModern biogeography often employs the use of Geographic Information Systems (GIS), to understand the factors affecting organism distribution, and to predict future trends in organism distribution.\nOften mathematical models and GIS are employed to solve ecological problems that have a spatial aspect to them.\n\nBiogeography is most keenly observed on the world's islands. These habitats are often much more manageable areas of study because they are more condensed than larger ecosystems on the mainland. Islands are also ideal locations because they allow scientists to look at habitats that new invasive species have only recently colonized and can observe how they disperse throughout the island and change it. They can then apply their understanding to similar but more complex mainland habitats. Islands are very diverse in their biomes, ranging from the tropical to arctic climates. This diversity in habitat allows for a wide range of species study in different parts of the world.\n\nOne scientist who recognized the importance of these geographic locations was Charles Darwin, who remarked in his journal \"The Zoology of Archipelagoes will be well worth examination\". Two chapters in \"On the Origin of Species\" were devoted to geographical distribution.\n\nThe first discoveries that contributed to the development of biogeography as a science began in the mid-18th century, as Europeans explored the world and discovered the biodiversity of life. During the 18th century most views on the world were shaped around religion and for many natural theologists, the bible. Carl Linnaeus, in the mid-18th century, initiated the ways to classify organisms through his exploration of undiscovered territories. When he noticed that species were not as perpetual as he believed, he developed the Mountain Explanation to explain the distribution of biodiversity. When Noah’s ark landed on Mount Ararat and the waters receded, the animals dispersed throughout different elevations on the mountain. This showed different species in different climates proving species were not constant. Linnaeus’ findings set a basis for ecological biogeography. Through his strong beliefs in Christianity, he was inspired to classify the living world, which then gave way to additional accounts of secular views on geographical distribution. He argued that the structure of an animal was very closely related to its physical surroundings. This was important to a George Louis Buffon’s rival theory of distribution.\n\nClosely after Linnaeus, Georges-Louis Leclerc, Comte de Buffon observed shifts in climate and how species spread across the globe as a result. He was the first to see different groups of organisms in different regions of the world. Buffon saw similarities between some regions which led him to believe that at one point continents were connected and then water separated them and caused differences in species. His hypotheses were described by his books, Histoire Naturelle, and Générale et Particulière, in which he argued that varying geographical regions would have different forms of life. This was inspired by his observations comparing the Old and New World, as he determined distinct variations of species from the two regions. Buffon believed there was a single species creation event, and that different regions of the world were homes for varying species, which is an alternate view than that of Linnaeus. Buffon's law eventually became a principle of biogeography by explaining how similar environments were habitats for comparable types of organisms. Buffon also studied fossils which led him to believe that the earth was over tens of thousands of years old, and that humans had not lived there long in comparison to the age of the earth.\n\nFollowing this period of exploration came the Age of Enlightenment in Europe, which attempted to explain the patterns of biodiversity observed by Buffon and Linnaeus. At the end of the 18th century, Alexander von Humboldt, known as the “founder of plant geography”, developed the concept of physique generale to demonstrate the unity of science and how species fit together. As one of the first to contribute empirical data to the science of biogeography through his travel as an explorer, he observed differences in climate and vegetation. The earth was divided into regions which he defined as tropical, temperate, and arctic and within these regions there were similar forms of vegetation. This ultimately enabled him to create the isotherm, which allowed scientists to see patterns of life within different climates. He contributed his observations to findings of botanical geography by previous scientists, and sketched this description of both the biotic and abiotic features of the earth in his book, Cosmos.\n\nAugustin de Candolle contributed to the field of biogeography as he observed species competition and the several differences that influenced the discovery of the diversity of life. He was a Swiss botanist and created the first Laws of Botanical Nomenclature in his work, Prodromus. He discussed plant distribution and his theories eventually had a great impact on Charles Darwin, who was inspired to consider species adaptations and evolution after learning about botanical geography. De Candolle was the first to describe the differences between the small-scale and large-scale distribution patterns of organisms around the globe.\n\nIn the 19th century, several additional scientists contributed new theories to further develop the concept of biogeography. Charles Lyell, being one of the first contributors in the 19th century, developed the Theory of Uniformitarianism after studying fossils. This theory explained how the world was not created by one sole catastrophic event, but instead from numerous creation events and locations. Uniformitarianism also introduced the idea that the Earth was actually significantly older than was previously accepted. Using this knowledge, Lyell concluded that it was possible for species to go extinct. Since he noted that earth’s climate changes, he realized that species distribution must also change accordingly. Lyell argued that climate changes complemented vegetation changes, thus connecting the environmental surroundings to varying species. This largely influenced Charles Darwin in his development of the theory of evolution.\n\nCharles Darwin was a natural theologist who studied around the world, and most importantly in the Galapagos Islands. Darwin introduced the idea of natural selection, as he theorized against previously accepted ideas that species were static or unchanging. His contributions to biogeography and the theory of evolution were different from those of other explorers of his time, because he developed a mechanism to describe the ways that species changed. His influential ideas include the development of theories regarding the struggle for existence and natural selection. Darwin’s theories started a biological segment to biogeography and empirical studies, which enabled future scientists to develop ideas about the geographical distribution of organisms around the globe.\n\nAlfred Russel Wallace studied the distribution of flora and fauna in the Amazon Basin and the Malay Archipelago in the mid-19th century. His research was essential to the further development of biogeography, and he was later nicknamed the \"father of Biogeography\". Wallace conducted fieldwork researching the habits, breeding and migration tendencies, and feeding behavior of thousands of species. He studied butterfly and bird distributions in comparison to the presence or absence of geographical barriers. His observations led him to conclude that the number of organisms present in a community was dependent on the amount of food resources in the particular habitat. Wallace believed species were dynamic by responding to biotic and abiotic factors. He and Philip Sclater saw biogeography as a source of support for the theory of evolution as they used Darwin's conclusion to explain how biogeography was similar to a record of species inheritance. Key findings, such as the sharp difference in fauna either side of the Wallace Line, and the sharp difference that existed between North and South America prior to their relatively recent faunal interchange, can only be understood in this light. Otherwise, the field of biogeography would be seen as a purely descriptive one.\n\nMoving on to the 20th century, Alfred Wegener introduced the Theory of Continental Drift in 1912, though it was not widely accepted until the 1960s. This theory was revolutionary because it changed the way that everyone thought about species and their distribution around the globe. The theory explained how continents were formerly joined together in one large landmass, Pangea, and slowly drifted apart due to the movement of the plates below Earth’s surface. The evidence for this theory is in the geological similarities between varying locations around the globe, fossil comparisons from different continents, and the jigsaw puzzle shape of the landmasses on Earth. Though Wegener did not know the mechanism of this concept of Continental Drift, this contribution to the study of biogeography was significant in the way that it shed light on the importance of environmental and geographic similarities or differences as a result of climate and other pressures on the planet. Importantly, late in his career Wegener recognised that testing his theory required measurement of continental movement rather than inference from fossils species distributions.\n\nThe publication of \"The Theory of Island Biogeography\" by Robert MacArthur and E.O. Wilson in 1967 showed that the species richness of an area could be predicted in terms of such factors as habitat area, immigration rate and extinction rate. This added to the long-standing interest in island biogeography. The application of island biogeography theory to habitat fragments spurred the development of the fields of conservation biology and landscape ecology.\n\nClassic biogeography has been expanded by the development of molecular systematics, creating a new discipline known as phylogeography. This development allowed scientists to test theories about the origin and dispersal of populations, such as island endemics. For example, while classic biogeographers were able to speculate about the origins of species in the Hawaiian Islands, phylogeography allows them to test theories of relatedness between these populations and putative source populations in Asia and North America.\n\nBiogeography continues as a point of study for many life sciences and geography students worldwide, however it may be under different broader titles within institutions such as ecology or evolutionary biology.\n\nIn recent years, one of the most important and consequential developments in biogeography has been to show how multiple organisms, including mammals like monkeys and reptiles like lizards, overcame barriers such as large oceans that many biogeographers formerly believed were impossible to cross. See also Oceanic dispersal.\n\nBiogeography now incorporates many different fields including but not limited to physical geography, geology, botany and plant biology, zoology, and general biology. A biogeographer’s main focus is on what environmental factors and what the influence of humans do to the distribution of the specific species of study. In terms of applications of biogeography as a science today, technological advances have allowed satellite imaging and processing of the Earth. Two main types of satellite imaging that are important within modern biogeography are Global Production Efficiency Model (GLO-PEM) and Geographic Information Systems (GIS). GLO-PEM uses satellite-imaging gives “repetitive, spatially contiguous, and time specific observations of vegetation.” These observations are on a global scale. GIS can show certain processes on the earth’s surface like whale locations, sea surface temperatures, and bathymetry. Current scientists also use coral reefs to delve into the history of biogeography through the fossilized reefs.\n\nPaleobiogeography goes one step further to include paleogeographic data and considerations of plate tectonics. Using molecular analyses and corroborated by fossils, it has been possible to demonstrate that perching birds evolved first in the region of Australia or the adjacent Antarctic (which at that time lay somewhat further north and had a temperate climate). From there, they spread to the other Gondwanan continents and Southeast Asia – the part of Laurasia then closest to their origin of dispersal – in the late Paleogene, before achieving a global distribution in the early Neogene. Not knowing that at the time of dispersal, the Indian Ocean was much narrower than it is today, and that South America was closer to the Antarctic, one would be hard pressed to explain the presence of many \"ancient\" lineages of perching birds in Africa, as well as the mainly South American distribution of the suboscines.\n\nPaleobiogeography also helps constrain hypotheses on the timing of biogeographic events such as vicariance and geodispersal, and provides unique information on the formation of regional biotas. For example, data from species-level phylogenetic and biogeographic studies tell us that the Amazonian fish fauna accumulated in increments over a period of tens of millions of years, principally by means of allopatric speciation, and in an arena extending over most of the area of tropical South America (Albert & Reis 2011). In other words, unlike some of the well-known insular faunas (Galapagos finches, Hawaiian drosophilid flies, African rift lake cichlids), the species-rich Amazonian ichthyofauna is not the result of recent adaptive radiations.\n\nFor freshwater organisms, landscapes are divided naturally into discrete drainage basins by watersheds, episodically isolated and reunited by erosional processes. In regions like the Amazon Basin (or more generally Greater Amazonia, the Amazon basin, Orinoco basin, and Guianas) with an exceptionally low (flat) topographic relief, the many waterways have had a highly reticulated history over geological time. In such a context, stream capture is an important factor affecting the evolution and distribution of freshwater organisms. Stream capture occurs when an upstream portion of one river drainage is diverted to the downstream portion of an adjacent basin. This can happen as a result of tectonic uplift (or subsidence), natural damming created by a landslide, or headward or lateral erosion of the watershed between adjacent basins.\n\nBiogeography is a synthetic science, related to geography, biology, soil science, geology, climatology, ecology and evolution.\n\nSome fundamental concepts in biogeography include:\n\nThe study of comparative biogeography can follow two main lines of investigation:\n\nThere are many types of biogeographic units used in biogeographic regionalisation schemes, as there are many criteria (species composition, physiognomy, ecological aspects) and hierarchization schemes: biogeographic realms (or ecozones), bioregions (\"sensu stricto\"), ecoregions, zoogeographical regions, floristic regions, vegetation types, biomes, etc.\n\nThe terms biogeographic unit, biogeographic area or bioregion \"sensu lato\", can be used for these categories, regardless of rank.\n\nRecently, a International Code of Area Nomenclature was proposed for biogeography.\n\n\n\n", "id": "99358", "title": "Biogeography"}
{"url": "https://en.wikipedia.org/wiki?curid=216226", "text": "Vegetation\n\nVegetation is an assemblage of plant species and the ground cover they provide. It is a general term, without specific reference to particular taxa, life forms, structure, spatial extent, or any other specific botanical or geographic characteristics. It is broader than the term \"flora\" which refers to species composition. Perhaps the closest synonym is plant community, but \"vegetation\" can, and often does, refer to a wider range of spatial scales than that term does, including scales as large as the global. Primeval redwood forests, coastal mangrove stands, sphagnum bogs, desert soil crusts, roadside weed patches, wheat fields, cultivated gardens and lawns; all are encompassed by the term \"vegetation\".\n\nThe vegetation type is defined by characteristic dominant species, or a common aspect of the assemblage, such as an elevation range or environmental commonality. The contemporary use of \"vegetation\" approximates that of ecologist Frederic Clements' term earth cover, an expression still used by the Bureau of Land Management. Natural vegetation refers to plant life undisturbed by humans in its growth and which is controlled by the climatic conditions of that region.\n\nThe distinction between vegetation (the general appearance of a community) and flora (the taxonomic composition of a community) was first made by Jules Thurmann (1849). Prior to this, the two terms (vegetation and flora) were used indiscriminately, and still are in some contexts. Augustin de Candolle (1820) also made a similar distinction, but he used the terms \"station\" (habitat type) and \"habitation\" (botanical region). Later, the concept of vegetation would influence the usage of the term biome, with the inclusion of the animal element.\n\nOther concepts similar to vegetation are \"physiognomy of vegetation\" (Humboldt, 1805, 1807) and \"formation\" (Grisebach, 1838, derived from \"\"Vegetationsform\"\", Martius, 1824).\n\nDeparting from Linnean taxonomy, Humboldt established a new science, dividing plant geography between taxonomists who studied plants as taxa and geographers who studied plants as vegetation. The physiognomic approach in the study of vegetation is common among biogeographers working on vegetation on a world scale, or when there is a lack of taxonomic knowledge of some place (e.g., in the tropics, where biodiversity is commonly high).\n\nThe concept \"vegetation type\" is more ambiguous. For some authors, it includes, besides physiognomy, floristic and habitat aspects. Furthermore, the phytosociological approach in the study of vegetation relies upon a fundamental unit, the plant association, which is defined upon flora.\n\nAn influential, clear and simple classification scheme for types of vegetation was produced by Wagner & von Sydow (1888). Other important works with a physiognomic approach includes Grisebach (1872), Warming (1895, 1909), Schimper (1898), Tansley and Chipp (1926), Rübel (1930), Burtt Davy (1938), Beard (1944, 1955), André Aubréville (1956, 1957), Trochain (1955, 1957), Küchler (1967), Ellenberg and Mueller-Dombois (1967) (see vegetation classification).\n\nThere are many approaches for the classification of vegetation (physiognomy, flora, ecology, etc.). Much of the work on vegetation classification comes from European and North American ecologists, and they have fundamentally different approaches. In North America, vegetation types are based on a combination of the following criteria: climate pattern, plant habit, phenology and/or growth form, and dominant species. In the current US standard (adopted by the Federal Geographic Data Committee (FGDC), and originally developed by UNESCO and The Nature Conservancy), the classification is hierarchical and incorporates the non-floristic criteria into the upper (most general) five levels and limited floristic criteria only into the lower (most specific) two levels. In Europe, classification often relies much more heavily, sometimes entirely, on floristic (species) composition alone, without explicit reference to climate, phenology or growth forms. It often emphasizes indicator or diagnostic species which may distinguish one classification from another.\n\nIn the FGDC standard, the hierarchy levels, from most general to most specific, are: \"system, class, subclass, group, formation, alliance, \"and\" association\". The lowest level, or association, is thus the most precisely defined, and incorporates the names of the dominant one to three (usually two) species of a type. An example of a vegetation type defined at the level of class might be \"\"Forest, canopy cover > 60%\"\"; at the level of a formation as \"\"Winter-rain, broad-leaved, evergreen, sclerophyllous, closed-canopy forest\"\"; at the level of alliance as \"\"Arbutus menziesii\" forest\"; and at the level of association as \"\"Arbutus menziesii-Lithocarpus densiflora\" forest\", referring to Pacific madrone-tanoak forests which occur in California and Oregon, USA. In practice, the levels of the alliance and/or association are the most often used, particularly in vegetation mapping, just as the Latin binomial is most often used in discussing particular species in taxonomy and in general communication.\n\nVictoria in Australia classifies its vegetation by Ecological Vegetation Class.\n\nLike all the biological systems, plant communities are temporally and spatially dynamic; they change at all possible scales. Dynamism in vegetation is defined primarily as changes in species composition and/or vegetation structure.\n\nTemporally, a large number of processes or events can cause change, but for sake of simplicity they can be categorized roughly as either abrupt or gradual. Abrupt changes are generally referred to as disturbances; these include things like wildfires, high winds, landslides, floods, avalanches and the like. Their causes are usually external (exogenous) to the community—they are natural processes occurring (mostly) independently of the natural processes of the community (such as germination, growth, death, etc.). Such events can change vegetation structure and composition very quickly and for long time periods, and they can do so over large areas. Very few ecosystems are without some type of disturbance as a regular and recurring part of the long term system dynamic. Fire and wind disturbances are particularly common throughout many vegetation types worldwide. Fire is particularly potent because of its ability to destroy not only living plants, but also the seeds, spores, and living meristems representing the potential next generation, and because of fire's impact on fauna populations, soil characteristics and other ecosystem elements and processes (for further discussion of this topic see fire ecology).\n\nTemporal change at a slower pace is ubiquitous; it comprises the field of ecological succession. Succession is the relatively gradual change in structure and taxonomic composition that arises as the vegetation itself modifies various environmental variables over time, including light, water and nutrient levels. These modifications change the suite of species most adapted to grow, survive and reproduce in an area, causing floristic changes. These floristic changes contribute to structural changes that are inherent in plant growth even in the absence of species changes (especially where plants have a large maximum size, i.e. trees), causing slow and broadly predictable changes in the vegetation. Succession can be interrupted at any time by disturbance, setting the system either back to a previous state, or off on another trajectory altogether. Because of this, successional processes may or may not lead to some static, final state. Moreover, accurately predicting the characteristics of such a state, even if it does arise, is not always possible. In short, vegetative communities are subject to many variables that together set limits on the predictability of future conditions.\n\nAs a general rule, the larger an area under consideration, the more likely the vegetation will be heterogeneous across it. Two main factors are at work. First, the temporal dynamics of disturbance and succession are increasingly unlikely to be in synchrony across any area as the size of that area increases. That is, different areas will be at different developmental stages due to different local histories, particularly their times since last major disturbance. This fact interacts with inherent environmental variability (e.g. in soils, climate, topography, etc.), which is also a function of area. Environmental variability constrains the suite of species that can occupy a given area, and the two factors together interact to create a mosaic of vegetation conditions across the landscape. Only in agricultural or horticultural systems does vegetation ever approach perfect uniformity. In natural systems, there is always heterogeneity, although its scale and intensity will vary widely. A natural grassland may he is the homogeneous when compared to the same area of partially burned forest.\n\n\n\n\n\n", "id": "216226", "title": "Vegetation"}
{"url": "https://en.wikipedia.org/wiki?curid=542725", "text": "Effector (biology)\n\nIn biochemistry, an effector molecule is usually a small molecule that selectively binds to a protein and regulates its biological activity. In this manner, effector molecules act as ligands that can increase or decrease enzyme activity, gene expression, or cell signalling. Effector molecules can also directly regulate the activity of some mRNA molecules (riboswitches).\n\nIn some cases, proteins can be considered to function as effector molecules, especially in cellular signal transduction cascades.\n\nThe term \"effector\" is used in other fields of biology. For instance, the effector end of a neuron is the terminus where an axon makes contact with the muscle or organ that it stimulates or suppresses.\n\nAllosteric effectors can bind to regulatory proteins involved in RNA transcription in order to change its activity. In this way activator proteins become active to bind to the DNA to promote RNA Polymerase and repressor proteins become inactive and RNA polymerase can bind to the DNA.\n\nBacterial effector proteins are injected by bacterial cells, usually pathogens, into the cells of their host. The injection is mediated by specialized secretion systems, e.g. the type III secretion system (TTSS or T3SS).\n\nFungal effectors are secreted by pathogenic or beneficial fungi into and around host cells by invasive hyphae to disable defense components or facilitate colonization. Protein secretion systems in fungi involve the Spitzenkörper.\n\nPlant pathogenic fungi use two distinct effector secretion systems and each secretory pathway is specific to an effector family:\n\n", "id": "542725", "title": "Effector (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=515758", "text": "Fungicide\n\nFungicides are biocidal chemical compounds or biological organisms used to kill parasitic fungi or their spores. A fungistatic inhibits their growth. Fungi can cause serious damage in agriculture, resulting in critical losses of yield, quality, and profit. Fungicides are used both in agriculture and to fight fungal infections in animals. Chemicals used to control oomycetes, which are not fungi, are also referred to as fungicides, as oomycetes use the same mechanisms as fungi to infect plants.\n\nFungicides can either be contact, translaminar or systemic. Contact fungicides are not taken up into the plant tissue and protect only the plant where the spray is deposited. Translaminar fungicides redistribute the fungicide from the upper, sprayed leaf surface to the lower, unsprayed surface. Systemic fungicides are taken up and redistributed through the xylem vessels. Few fungicides move to all parts of a plant. Some are locally systemic, and some move upwardly.\n\nMost fungicides that can be bought retail are sold in a liquid form. A very common active ingredient is sulfur, present at 0.08% in weaker concentrates, and as high as 0.5% for more potent fungicides. Fungicides in powdered form are usually around 90% sulfur and are very toxic. Other active ingredients in fungicides include neem oil, rosemary oil, jojoba oil, the bacterium \"Bacillus subtilis\", and the beneficial fungus \"Ulocladium oudemansii\".\n\nFungicide residues have been found on food for human consumption, mostly from post-harvest treatments. Some fungicides are dangerous to human health, such as vinclozolin, which has now been removed from use. Ziram is also a fungicide that is thought to be toxic to humans if exposed to chronically. A number of fungicides are also used in human health care.\n\nPlants and other organisms have chemical defenses that give them an advantage against microorganisms such as fungi. Some of these compounds can be used as fungicides:\n\n\nWhole live or dead organisms that are efficient at killing or inhibiting fungi can sometimes be used as fungicides:\n\n\nPathogens respond to the use of fungicides by evolving resistance. In the field several mechanisms of resistance have been identified. The evolution of fungicide resistance can be gradual or sudden. In qualitative or discrete resistance, a mutation (normally to a single gene) produces a race of a fungus with a high degree of resistance. Such resistant varieties also tend to show stability, persisting after the fungicide has been removed from the market. For example, sugar beet leaf blotch remains resistant to azoles years after they were no longer used for control of the disease. This is because such mutations have a high selection pressure when the fungicide is used, but there is low selection pressure to remove them in the absence of the fungicide.\n\nIn instances where resistance occurs more gradually, a shift in sensitivity in the pathogen to the fungicide can be seen. Such resistance is polygenic – an accumulation of many mutations in different genes, each having a small additive effect. This type of resistance is known as quantitative or continuous resistance. In this kind of resistance, the pathogen population will revert to a sensitive state if the fungicide is no longer applied.\n\nLittle is known about how variations in fungicide treatment affect the selection pressure to evolve resistance to that fungicide. Evidence shows that the doses that provide the most control of the disease also provide the largest selection pressure to acquire resistance, and that lower doses decrease the selection pressure.\n\nIn some cases when a pathogen evolves resistance to one fungicide, it automatically obtains resistance to others – a phenomenon known as cross resistance. These additional fungicides are normally of the same chemical family or have the same mode of action, or can be detoxified by the same mechanism. Sometimes negative cross resistance occurs, where resistance to one chemical class of fungicides leads to an increase in sensitivity to a different chemical class of fungicides. This has been seen with carbendazim and diethofencarb.\n\nThere are also recorded incidences of the evolution of multiple drug resistance by pathogens – resistance to two chemically different fungicides by separate mutation events. For example, \"Botrytis cinerea\" is resistant to both azoles and dicarboximide fungicides.\n\nThere are several routes by which pathogens can evolve fungicide resistance. The most common mechanism appears to be alteration of the target site, in particular as a defence against single site of action fungicides. For example, Black Sigatoka, an economically important pathogen of banana, is resistant to the QoI fungicides, due to a single nucleotide change resulting in the replacement of one amino acid (glycine) by another (alanine) in the target protein of the QoI fungicides, cytochrome b. It is presumed that this disrupts the binding of the fungicide to the protein, rendering the fungicide ineffective. Upregulation of target genes can also render the fungicide ineffective. This is seen in DMI-resistant strains of \"Venturia inaequalis\".\n\nResistance to fungicides can also be developed by efficient efflux of the fungicide out of the cell. \"Septoria tritici\" has developed multiple drug resistance using this mechanism. The pathogen had five ABC-type transporters with overlapping substrate specificities that together work to pump toxic chemicals out of the cell.\n\nIn addition to the mechanisms outlined above, fungi may also develop metabolic pathways that circumvent the target protein, or acquire enzymes that enable metabolism of the fungicide to a harmless substance.\n\nThe fungicide resistance action committee (FRAC) has several recommended practices to try to avoid the development of fungicide resistance, especially in at-risk fungicides including \"Strobilurins\" such as azoxystrobin.\n\nProducts should not be used in isolation, but rather as mixture, or alternate sprays, with another fungicide with a different mechanism of action. The likelihood of the pathogen's developing resistance is greatly decreased by the fact that any resistant isolates to one fungicide will be killed by the other; in other words, two mutations would be required rather than just one. The effectiveness of this technique can be demonstrated by Metalaxyl, a phenylamide fungicide. When used as the sole product in Ireland to control potato blight (\"Phytophthora infestans\"), resistance developed within one growing season. However, in countries like the UK where it was marketed only as a mixture, resistance problems developed more slowly.\n\nFungicides should be applied only when absolutely necessary, especially if they are in an at-risk group. Lowering the amount of fungicide in the environment lowers the selection pressure for resistance to develop.\n\nManufacturers’ doses should always be followed. These doses are normally designed to give the right balance between controlling the disease and limiting the risk of resistance development. Higher doses increase the selection pressure for single-site mutations that confer resistance, as all strains but those that carry the mutation will be eliminated, and thus the resistant strain will propagate. Lower doses greatly increase the risk of polygenic resistance, as strains that are slightly less sensitive to the fungicide may survive.\n\nIt is also recommended that where possible fungicides are used only in a protective manner, rather than to try to cure already-infected crops. Far fewer fungicides have curative/eradicative ability than protectant. Thus, fungicide preparations advertised as having curative action may have only one active chemical; a single fungicide acting in isolation increases the risk of fungicide resistance.\n\nIt is better to use an integrative pest management approach to disease control rather than relying on fungicides alone. This involves the use of resistant varieties and hygienic practices, such as the removal of potato discard piles and stubble on which the pathogen can overwinter, greatly reducing the titre of the pathogen and thus the risk of fungicide resistance development.\n\n\n", "id": "515758", "title": "Fungicide"}
{"url": "https://en.wikipedia.org/wiki?curid=149463", "text": "Insecticide\n\nInsecticides are substances used to kill insects. They include ovicides and larvicides used against insect eggs and larvae, respectively. Insecticides are used in agriculture, medicine, industry and by consumers. Insecticides are claimed to be a major factor behind the increase in the 20th-century's agricultural productivity. Nearly all insecticides have the potential to significantly alter ecosystems; many are toxic to humans and/or animals; some become concentrated as they spread along the food chain.\n\nInsecticides can be classified into two major groups: systemic insecticides, which have residual or long term activity; and contact insecticides, which have no residual activity.\n\nFurthermore, one can distinguish three types of insecticide. 1. Natural insecticides, such as nicotine, pyrethrum and neem extracts, made by plants as defenses against insects. 2. Inorganic insecticides, which are metals. 3. Organic insecticides, which are organic chemical compounds, mostly working by contact.\n\nThe mode of action describes how the pesticide kills or inactivates a pest. It provides another way of classifying insecticides. Mode of action is important in understanding whether an insecticide will be toxic to unrelated species, such as fish, birds and mammals.\n\nInsecticides may be repellent or non-repellent. Social insects such as ants cannot detect non-repellents and readily crawl through them. As they return to the nest they take insecticide with them and transfer it to their nestmates. Over time, this eliminates all of the ants including the queen. This is slower than some other methods, but usually completely eradicates the ant colony.\n\nInsecticides are distinct from non-insecticidal repellents, which repel but do not kill.\n\nSystemic insecticides become incorporated and distributed systemically throughout the whole plant. When insects feed on the plant, they ingest the insecticide. Systemic insecticides produced by transgenic plants are called plant-incorporated protectants (PIPs). For instance, a gene that codes for a specific Bacillus thuringiensis biocidal protein was introduced into corn and other species. The plant manufactures the protein, which kills the insect when consumed.\n\nContact insecticides are toxic to insects upon direct contact. These can be inorganic insecticides, which are metals and include arsenates, copper and fluorine compounds, which are less commonly used, and the commonly used sulfur. Contact insecticides can be organic insecticides, i.e. organic chemical compounds, synthetically produced, and comprising the largest numbers of pesticides used today. Or they can be natural compounds like pyrethrum, neem oil etc.\nContact insecticides usually have no residual activity.\n\nEfficacy can be related to the quality of pesticide application, with small droplets, such as aerosols often improving performance.\n\nMany organic compounds are produced by plants for the purpose of defending the host plant from predation. A trivial case is tree rosin, which is a natural insecticide. Specifically, the production of oleoresin by conifer species is a component of the defense response against insect attack and fungal pathogen infection. Many fragrances, e.g. oil of wintergreen, are in fact antifeedants.\n\nFour extracts of plants are in commercial use: pyrethrum, rotenone, neem oil, and various essential oils\n\nTransgenic crops that act as insecticides began in 1996 with a genetically modified potato that produced the Cry protein, derived from the bacterium Bacillus thuringiensis, which is toxic to beetle larvae such as the Colorado potato beetle. The technique has been expanded to include the use of RNA interference RNAi that fatally silences crucial insect genes. RNAi likely evolved as a defense against viruses. Midgut cells in many larvae take up the molecules and help spread the signal. The technology can target only insects that have the silenced sequence, as was demonstrated when a particular RNAi affected only one of four fruit fly species. The technique is expected to replace many other insecticides, which are losing effectiveness due to the spread of pesticide resistance.\n\nMany plants exude substances to repel insects. Premier examples are substances activated by the enzyme myrosinase. This enzyme converts glucosinolates to various compounds that are toxic to herbivorous insects. One product of this enzyme is allyl isothiocyanate, the pungent ingredient in horseradish sauces.\n\nThe myrosinase is released only upon crushing the flesh of horseradish. Since allyl isothiocyanate is harmful to the plant as well as the insect, it is stored in the harmless form of the glucosinolate, separate from the myrosinase enzyme.\n\n\"Bacillus thuringiensis\" is a bacterial disease that affects Lepidopterans and some other insects. Toxins produced by strains of this bacterium are used as a larvicide against caterpillars, beetles, and mosquitoes. Toxins from \"Saccharopolyspora spinosa\" are isolated from fermentations and sold as Spinosad. Because these toxins have little effect on other organisms, they are considered more environmentally friendly than synthetic pesticides. The toxin from \"B. thuringiensis\" (Bt toxin) has been incorporated directly into plants through the use of genetic engineering. Other biological insecticides include products based on entomopathogenic fungi (e.g., \"Beauveria bassiana\", \"Metarhizium anisopliae\"), nematodes (e.g., \"Steinernema feltiae\") and viruses (e.g., \"Cydia pomonella\" granulovirus).\n\nA major emphasis of organic chemistry is the development of chemical tools to enhance agricultural productivity. Insecticides represent a major area of emphasis. Many of the major insecticides are inspired by biological analogues. Many others are completely alien to nature.\n\nThe best known organochloride, DDT, was created by Swiss scientist Paul Müller. For this discovery, he was awarded the 1948 Nobel Prize for Physiology or Medicine. DDT was introduced in 1944. It functions by opening sodium channels in the insect's nerve cells. The contemporaneous rise of the chemical industry facilitated large-scale production of DDT and related chlorinated hydrocarbons.\n\nOrganophosphates are another large class of contact insecticides. These also target the insect's nervous system. Organophosphates interfere with the enzymes acetylcholinesterase and other cholinesterases, disrupting nerve impulses and killing or disabling the insect. Organophosphate insecticides and chemical warfare nerve agents (such as sarin, tabun, soman, and VX) work in the same way. Organophosphates have a cumulative toxic effect to wildlife, so multiple exposures to the chemicals amplifies the toxicity. In the US, organophosphate use declined with the rise of substitutes.\n\nCarbamate insecticides have similar mechanisms to organophosphates, but have a much shorter duration of action and are somewhat less toxic.\n\nPyrethroid pesticides mimic the insecticidal activity of the natural compound pyrethrum, the biopesticide found in pyrethrins. These compounds are nonpersistent sodium channel modulators and are less toxic than organophosphates and carbamates. Compounds in this group are often applied against household pests.\n\nNeonicotinoids are synthetic analogues of the natural insecticide nicotine (with much lower acute mammalian toxicity and greater field persistence). These chemicals are acetylcholine receptor agonists. They are broad-spectrum systemic insecticides, with rapid action (minutes-hours). They are applied as sprays, drenches, seed and soil treatments. Treated insects exhibit leg tremors, rapid wing motion, stylet withdrawal (aphids), disoriented movement, paralysis and death. Imidacloprid may be the most common. It has recently come under scrutiny for allegedly pernicious effects on honeybees and its potential to increase the susceptibility of rice to planthopper attacks.\n\nRyanoids are synthetic analogues with the same mode of action as ryanodine, a naturally occurring insecticide extracted from \"Ryania speciosa\" (Flacourtiaceae). They bind to calcium channels in cardiac and skeletal muscle, blocking nerve transmission. Only one such insecticide is currently registered, Rynaxypyr, generic name chlorantraniliprole.\n\nInsect growth regulator (IGR) is a term coined to include insect hormone mimics and an earlier class of chemicals, the benzoylphenyl ureas, which inhibit chitin (exoskeleton) biosynthesis in insects Diflubenzuron is a member of the latter class, used primarily to control caterpillars that are pests. The most successful insecticides in this class are the juvenoids (juvenile hormone analogues). Of these, methoprene is most widely used. It has no observable acute toxicity in rats and is approved by World Health Organization (WHO) for use in drinking water cisterns to combat malaria. Most of its uses are to combat insects where the adult is the pest, including mosquitoes, several fly species, and fleas. Two very similar products, hydroprene and kinoprene, are used for controlling species such as cockroaches and white flies. Methoprene was registered with the EPA in 1975. Virtually no reports of resistance have been filed. A more recent type of IGR is the ecdysone agonist tebufenozide (MIMIC), which is used in forestry and other applications for control of caterpillars, which are far more sensitive to its hormonal effects than other insect orders.\n\nSome insecticides kill or harm other creatures in addition to those they are intended to kill. For example, birds may be poisoned when they eat food that was recently sprayed with insecticides or when they mistake an insecticide granule on the ground for food and eat it.\n\nSprayed insecticide may drift from the area to which it is applied and into wildlife areas, especially when it is sprayed aerially.\n\nThe development of DDT was motivated by desire to replace more dangerous or less effective alternatives. DDT was introduced to replace lead and arsenic-based compounds, which were in widespread use in the early 1940s.\n\nDDT was brought to public attention by Rachel Carson's book \"Silent Spring\". One side-effect of DDT is to reduce the thickness of shells on the eggs of predatory birds. The shells sometimes become too thin to be viable, reducing bird populations. This occurs with DDT and related compounds due to the process of bioaccumulation, wherein the chemical, due to its stability and fat solubility, accumulates in organisms' fatty tissues. Also, DDT may biomagnify, which causes progressively higher concentrations in the body fat of animals farther up the food chain. The near-worldwide ban on agricultural use of DDT and related chemicals has allowed some of these birds, such as the peregrine falcon, to recover in recent years. A number of organochlorine pesticides have been banned from most uses worldwide. Globally they are controlled via the Stockholm Convention on persistent organic pollutants. These include: aldrin, chlordane, DDT, dieldrin, endrin, heptachlor, mirex and toxaphene.\n\nInsecticides can kill bees and may be a cause of pollinator decline, the loss of bees that pollinate plants, and colony collapse disorder (CCD), in which worker bees from a beehive or Western honey bee colony abruptly disappear. Loss of pollinators means a reduction in crop yields. Sublethal doses of insecticides (i.e. imidacloprid and other neonicotinoids) affect bee foraging behavior. However, research into the causes of CCD was inconclusive as of June 2007.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "149463", "title": "Insecticide"}
{"url": "https://en.wikipedia.org/wiki?curid=991169", "text": "Rodenticide\n\nRodenticides, colloquially rat poison, are typically non-specific pest control chemicals made and sold for the purpose of killing rodents.\n\nSome rodenticides are lethal after one exposure while others require more than one. Rodents are disinclined to gorge on an unknown food (perhaps reflecting an adaptation to their inability to vomit), preferring to sample, wait and observe whether it makes them or other rats sick. This phenomenon of bait shyness or poison shyness is the rationale for poisons that kill only after multiple doses.\n\nBesides being directly toxic to the mammals that ingest them, including dogs, cats, and humans, many rodenticides present a secondary poisoning risk to animals that hunt or scavenge the dead corpses of rats.\n\nAnticoagulants are defined as chronic (death occurs one to two weeks after ingestion of the lethal dose, rarely sooner), single-dose (second generation) or multiple-dose (first generation) rodenticides, acting by effective blocking of the vitamin K cycle, resulting in inability to produce essential blood-clotting factors — mainly coagulation factors II (prothrombin) and VII (proconvertin).\n\nIn addition to this specific metabolic disruption, massive toxic doses of 4-hydroxycoumarin, 4-thiochromenone and indandione anticoagulants cause damage to tiny blood vessels (capillaries), increasing their permeability, causing diffuse internal bleeding. These effects are gradual, developing over several days. In the final phase of the intoxication, the exhausted rodent collapses due to hemorrhagic shock or severe anemia and dies calmly. The question of whether the use of these rodenticides can be considered humane has been raised.\n\nThe main benefit of anticoagulants over other poisons is that the time taken for the poison to induce death means that the rats do not associate the damage with their feeding habits.\n\n\nVitamin K has been suggested, and successfully used, as antidote for pets or humans accidentally or intentionally exposed to anticoagulant poisons. Some of these poisons act by inhibiting liver functions and in advanced stages of poisoning, several blood-clotting factors are absent, and the volume of circulating blood is diminished, so that a blood transfusion (optionally with the clotting factors present) can save a person who has been poisoned, an advantage over some older poisons.\n\nMetal phosphides have been used as a means of killing rodents and are considered single-dose fast acting rodenticides (death occurs commonly within 1–3 days after single bait ingestion). A bait consisting of food and a phosphide (usually zinc phosphide) is left where the rodents can eat it. The acid in the digestive system of the rodent reacts with the phosphide to generate the toxic phosphine gas. This method of vermin control has possible use in places where rodents are resistant to some of the anticoagulants, particularly for control of house and field mice; zinc phosphide baits are also cheaper than most second-generation anticoagulants, so that sometimes, in the case of large infestation by rodents, their population is initially reduced by copious amounts of zinc phosphide bait applied, and the rest of population that survived the initial fast-acting poison is then eradicated by prolonged feeding on anticoagulant bait. Inversely, the individual rodents, that survived anticoagulant bait poisoning (rest population) can be eradicated by pre-baiting them with nontoxic bait for a week or two (this is important to overcome bait shyness, and to get rodents used to feeding in specific areas by specific food, especially in eradicating rats) and subsequently applying poisoned bait of the same sort as used for pre-baiting until all consumption of the bait ceases (usually within 2–4 days). These methods of alternating rodenticides with different modes of action gives actual or almost 100% eradications of the rodent population in the area, if the acceptance/palatability of baits are good (i.e., rodents feed on it readily).\n\nZinc phosphide is typically added to rodent baits in a concentration of 0.75% to 2.0%. The baits have strong, pungent garlic-like odor due to the phosphine liberated by hydrolysis. The odor attracts (or, at least, does not repel) rodents, but has an repulsive effect on other mammals. Birds, notably wild turkeys, are not sensitive to the smell, and will feed on the bait, and thus become collateral damage.\n\nThe tablets or pellets (usually aluminium, calcium or magnesium phosphide for fumigation/gassing) may also contain other chemicals which evolve ammonia, which helps reduce the potential for spontaneous combustion or explosion of the phosphine gas.\n\nMetal phosphides do not accumulate in the tissues of poisoned animals, so the risk of secondary poisoning is low.\n\nBefore the advent of anticoagulants, phosphides were the favored kind of rat poison. During World War II, they came into use in United States because of shortage of strychnine due to the Japanese occupation of the territories where the strychnine tree is grown. Phosphides are rather fast-acting rat poisons, resulting in the rats dying usually in open areas, instead of in the affected buildings.\n\nPhosphides used as rodenticides include:\n\nCalciferols (vitamins D), cholecalciferol (vitamin D) and ergocalciferol (vitamin D) are used as rodenticides. They are toxic to rodents for the same reason they are important to humans: they affect calcium and phosphate homeostasis in the body. Vitamins D are essential in minute quantities (few IUs per kilogram body weight daily, only a fraction of a milligram), and like most fat soluble vitamins, they are toxic in larger doses, causing hypervitaminosis. If the poisoning is severe enough (that is, if the dose of the toxin is high enough), it leads to death. In rodents that consume the rodenticidal bait, it causes hypercalcemia, raising the calcium level, mainly by increasing calcium absorption from food, mobilising bone-matrix-fixed calcium into ionised form (mainly monohydrogencarbonate calcium cation, partially bound to plasma proteins, [CaHCO]), which circulates dissolved in the blood plasma. After ingestion of a lethal dose, the free calcium levels are raised sufficiently that blood vessels, kidneys, the stomach wall and lungs are mineralised/calcificated (formation of calcificates, crystals of calcium salts/complexes in the tissues, damaging them), leading further to heart problems (myocardial tissue is sensitive to variations of free calcium levels, affecting both myocardial contractibility and excitation propagation between atrias and ventriculas), bleeding (due to capillary damage) and possibly kidney failure. It is considered to be single-dose, cumulative (depending on concentration used; the common 0.075% bait concentration is lethal to most rodents after a single intake of larger portions of the bait) or sub-chronic (death occurring usually within days to one week after ingestion of the bait). Applied concentrations are 0.075% cholecalciferol and 0.1% ergocalciferol when used alone, which can kill a rodent or a rat.\n\nThere is an important feature of calciferols toxicology, that they are synergistic with anticoagulant toxicants, that means, that mixtures of anticoagulants and calciferols in same bait are more toxic than a sum of toxicities of the anticoagulant and the calciferol in the bait, so that a massive hypercalcemic effect can be achieved by a substantially lower calciferol content in the bait, and vice versa, a more pronounced anticoagulant/hemorrhagic effects are observed if the calciferol is present. This synergism is mostly used in calciferol low concentration baits, because effective concentrations of calciferols are more expensive than effective concentrations of most anticoagulants.\n\nThe first application of a calciferol in rodenticidal bait was in the Sorex product Sorexa D (with a different formula than today's Sorexa D), back in the early 1970s, which contained 0.025% warfarin and 0.1% ergocalciferol. Today, Sorexa CD contains a 0.0025% difenacoum and 0.075% cholecalciferol combination. Numerous other brand products containing either 0.075-0.1% calciferols (e.g. Quintox) alone or alongside an anticoagulant are marketed.\n\nThe Merck Veterinary Manual states the following:\n\nAlthough this rodenticide [cholecalciferol] was introduced with claims that it was less toxic to nontarget species than to rodents, clinical experience has shown that rodenticides containing cholecalciferol are a significant health threat to dogs and cats. Cholecalciferol produces hypercalcemia, which results in systemic calcification of soft tissue, leading to renal failure, cardiac abnormalities, hypertension, CNS depression and GI upset. Signs generally develop within 18-36 hours of ingestion and can include depression, anorexia, polyuria and polydipsia. As serum calcium concentrations increase, clinical signs become more severe. ... GI smooth muscle excitability decreases and is manifest by anorexia, vomiting and constipation. ... Loss of renal concentrating ability is a direct result of hypercalcemia. As hypercalcemia persists, mineralization of the kidneys results in progressive renal insufficiency.\"\n\nAdditional anticoagulant renders the bait more toxic to pets as well as human. Upon single ingestion, solely calciferol-based baits are considered generally safer to birds than second generation anticoagulants or acute toxicants. A specific antidote for calciferol intoxication is calcitonin, a hormone that lowers the blood levels of calcium. The therapy with commercially available calcitonin preparations is, however, expensive.\n\nOther chemical poisons include:\n\nIn some countries, fixed three-component rodenticides, i.e., anticoagulant + antibiotic + vitamin D, are used. Associations of a second-generation anticoagulant with an antibiotic and/or vitamin D are considered to be effective even against most resistant strains of rodents, though some second generation anticoagulants (namely brodifacoum and difethialone), in bait concentrations of 0.0025% to 0.005% are so toxic that resistance is unknown, and even rodents resistant to other rodenticides are reliably exterminated by application of these most toxic anticoagulants.\n\nMore environmentally-safe preparations, such as powdered corn cob, have been developed and were approved in the EU and patented in the US in 2013. These preparations rely on dehydration to cause death.\n\nOne of the potential problems when using rodenticides is that dead or weakened rodents may be eaten by other wildlife, either predators or scavengers. Members of the public deploying rodenticides may not be aware of this or may not follow the product's instructions closely enough.\n\nThe faster a rodenticide acts, the more critical this problem may be. For the fast-acting rodenticide bromethalin, for example, there is no diagnostic test or antidote.\n\nThis has led environmental researchers to conclude that low strength, long duration rodenticides (generally first generation anticoagulants) are the best balance between maximum effect and minimum risk.\n\nIn 2008, after assessing human health and ecological effects, as well as benefits, the US Environmental Protection Agency (EPA) announced measures to reduce risks associated with ten rodenticides. New restrictions by sale and distribution restrictions, minimum package size requirements, use site restriction, and tamper resistant products would have taken effect in 2011. The regulations were delayed pending a legal challenge by manufacturer Reckitt-Benkiser.\n\nThe entire rat populations of several islands have been eradicated, most notably Campbell Island, New Zealand (11,300 ha), Hawadax Island, Alaska (formerly known as Rat Island, 2,670 ha) and Canna, Scotland (1,030 ha, declared rat-free in 2008).\n\nAlberta, Canada, through a combination of climate and control, is believed to be rat-free.\n\n\n\n", "id": "991169", "title": "Rodenticide"}
{"url": "https://en.wikipedia.org/wiki?curid=965323", "text": "Antimicrobial\n\nAn antimicrobial is an agent that kills microorganisms or stops their growth. Antimicrobial medicines can be grouped according to the microorganisms they act primarily against. For example, antibiotics are used against bacteria and antifungals are used against fungi. They can also be classified according to their function. Agents that kill microbes are called microbicidal, while those that merely inhibit their growth are called biostatic. The use of antimicrobial medicines to treat infection is known as antimicrobial chemotherapy, while the use of antimicrobial medicines to prevent infection is known as antimicrobial prophylaxis.\n\nThe main classes of antimicrobial agents are disinfectants (\"nonselective antimicrobials\" such as bleach), which kill a wide range of microbes on non-living surfaces to prevent the spread of illness, antiseptics (which are applied to living tissue and help reduce infection during surgery), and antibiotics (which destroy microorganisms within the body). The term \"antibiotic\" originally described only those formulations derived from living organisms but is now also applied to synthetic antimicrobials, such as the sulphonamides, or fluoroquinolones. The term also used to be restricted to antibacterials (and is often used as a synonym for them by medical professionals and in medical literature), but its context has broadened to include all antimicrobials. Antibacterial agents can be further subdivided into bactericidal agents, which kill bacteria, and bacteriostatic agents, which slow down or stall bacterial growth. In response, further advancements in antimicrobial technologies have resulted in solutions that can go beyond simply inhibiting microbial growth. Instead, certain types of porous media have been developed to kill microbes on contact.\n\nAntimicrobial use is known to have been common practice for at least 2000 years. Ancient Egyptians and ancient Greeks used specific molds and plant extracts to treat infection. \n\nIn the 19th century, microbiologists such as Louis Pasteur and Jules Francois Joubert observed antagonism between some bacteria and discussed the merits of controlling these interactions in medicine. In 1928, Alexander Fleming became the first to discover a natural antimicrobial fungus known as \"Penicillium rubens\" and named the extracted substance penicillin which in 1942 was successfully used to treat a \"Streptococcus\" infection.\n\nAntibacterials are used to treat bacterial infections. The drug toxicity to humans and other animals from antibacterials is generally considered low.(depends) Prolonged use of certain antibacterials can decrease the number of gut flora, which may have a negative impact on health. Consumption of probiotics and reasonable eating can help to replace destroyed gut flora. Stool transplants may be considered for patients who are having difficulty recovering from prolonged antibiotic treatment, as for recurrent \"Clostridium difficile\" infections.\n\nThe discovery, development and use of antibacterials during the 20th century has reduced mortality from bacterial infections. The antibiotic era began with the pneumatic application of nitroglycerine drugs, followed by a “golden” period of discovery from about 1945 to 1970, when a number of structurally diverse and highly effective agents were discovered and developed. since 1980 the introduction of new antimicrobial agents for clinical use has declined, in part because of the enormous expense of developing and testing new drugs. In parallel there has been an alarming increase in antimicrobial resistance of bacteria, fungi, parasites and some viruses to multiple existing agents.\n\nAntibacterials are among the most commonly used drugs and among the drugs commonly misused by physicians, for example, in viral respiratory tract infections. As a consequence of widespread and injudicious use of antibacterials, there has been an accelerated emergence of antibiotic-resistant pathogens, resulting in a serious threat to global public health. The resistance problem demands that a renewed effort be made to seek antibacterial agents effective against pathogenic bacteria resistant to current antibacterials. Possible strategies towards this objective include increased sampling from diverse environments and application of metagenomics to identify bioactive compounds produced by currently unknown and uncultured microorganisms as well as the development of small-molecule libraries customized for bacterial targets.\n\nAntifungals are used to kill or prevent further growth of fungi. In medicine, they are used as a treatment for infections such as athlete's foot, ringworm and thrush and work by exploiting differences between mammalian and fungal cells. They kill off the fungal organism without dangerous effects on the host. Unlike bacteria, both fungi and humans are eukaryotes. Thus, fungal and human cells are similar at the molecular level, making it more difficult to find a target for an antifungal drug to attack that does not also exist in the infected organism. Consequently, there are often side effects to some of these drugs. Some of these side effects can be life-threatening if the drug is not used properly.\n\nAs well as their use in medicine, antifungals are frequently sought after to control mold growth in damp or wet home materials. Sodium bicarbonate (baking soda) blasted on to surfaces acts as an antifungal. Another antifungal serum applied after or without blasting by soda is a mix of hydrogen peroxide and a thin surface coating that neutralizes mold and encapsulates the surface to prevent spore release. Some paints are also manufactured with an added antifungal agent for use in high humidity areas such as bathrooms or kitchens. Other antifungal surface treatments typically contain variants of metals known to suppress mold growth e.g. pigments or solutions containing copper, silver or zinc. These solutions are not usually available to the general public because of their toxicity.\n\nAntiviral drugs are a class of medication used specifically for treating viral infections. Like antibiotics, specific antivirals are used for specific viruses. They are relatively harmless to the host and therefore can be used to treat infections. They should be distinguished from viricides, which actively deactivate virus particles outside the body.\n\nMany antiviral drugs are designed to treat infections by retroviruses, mostly HIV. Important antiretroviral drugs include the class of protease inhibitors. Herpes viruses, best known for causing cold sores and genital herpes, are usually treated with the nucleoside analogue acyclovir. Viral hepatitis is caused by five unrelated hepatotropic viruses (A-E) and can be treated with antiviral drugs depending on the type of infection. influenza A and B viruses have become resistant to neuraminidase inhibitors such as oseltamivir and the search for new substances is on.\n\nAntiparasitics are a class of medications indicated for the treatment of infection by parasites, such as nematodes, cestodes, trematodes, infectious protozoa, and amoebae. Like all antimicrobials against intracellular microbes, they must kill the infecting pest without serious damage to the host.\n\nA wide range of chemical and natural compounds are used as antimicrobials. Organic acids are used widely as antimicrobials in food products, e.g. lactic acid, citric acid, acetic acid, and their salts, either as ingredients, or as disinfectants. For example, beef carcasses often are sprayed with acids, and then rinsed or steamed, to reduce the prevalence of \"E. coli\".\n\nCopper-alloy surfaces have natural intrinsic antimicrobial properties and can kill microorganisms such as \"E. coli\", MRSA and \"Staphylococcus\". The United States Environmental Protection Agency has approved the registration of 355 such antibacterial copper alloys. As a public hygienic measure in addition to regular cleaning, antimicrobial copper alloys are being installed in some healthcare facilities and in subway transit systems. Other heavy metal cations such as Hg and Pb have antimicrobial activities, but can be toxic.\n\nTraditional herbalists used plants to treat infectious disease. Many of these plants have been investigated scientifically for antimicrobial activity, and some plant products have been shown to inhibit the growth of pathogenic microorganisms. A number of these agents appear to have structures and modes of action that are distinct from those of the antibiotics in current use, suggesting that cross-resistance with agents already in use may be minimal.\n\nMany essential oils included in herbal pharmacopoeias are claimed to possess antimicrobial activity, with the oils of bay, cinnamon, clove and thyme reported to be the most potent in studies with foodborne bacterial pathogens. Active constituents include terpenoid chemicals and other secondary metabolites. Despite their prevalent use in alternative medicine, essential oils have seen limited use in mainstream medicine. While 25 to 50% of pharmaceutical compounds are plant-derived, none are used as antimicrobials, though there has been increased research in this direction. Barriers to increased usage in mainstream medicine include poor regulatory oversight and quality control, mislabeled or misidentified products, and limited modes of delivery.\n\nAccording to the U.S. Environmental Protection Agency (EPA), and defined by the Federal Insecticide, Fungicide, and Rodenticide Act, antimicrobial pesticides are used in order to control growth of microbes through disinfection, sanitation, or reduction of development and to protect inanimate objects, industrial processes or systems, surfaces, water, or other chemical substances from contamination, fouling, or deterioration caused by bacteria, viruses, fungi, protozoa, algae, or slime.\n\nAntimicrobial pesticide products\nThe EPA monitors products, such as disinfectants/sanitizers for use in hospitals or homes, in order to ascertain efficacy. Products that are meant for public health are therefore under this monitoring system—ones used for drinking water, swimming pools, food sanitation, and other environmental surfaces. These pesticide products are registered under the premise that, when used properly, they do not demonstrate unreasonable side effects to humans or the environment. Even once certain products are on the market, the EPA continues to monitor and evaluate them to make sure they maintain efficacy in protecting public health.\n\nPublic health products regulated by the EPA are divided into three categories:\n\nAntimicrobial pesticide safety\nAccording to a 2010 CDC report, health-care workers can take steps to improve their safety measures against antimicrobial pesticide exposure. Workers are advised to minimize exposure to these agents by wearing protective equipment, gloves, and safety glasses. Additionally, it is important to follow the handling instructions properly, as that is how the Environmental Protection Agency has deemed it as safe to use. Employees should be educated about the health hazards, and encouraged to seek medical care if exposure occurs.\n\nOzone can kill microorganisms in air and water, such as municipal drinking-water systems, swimming pools and spas, and the laundering of clothes.\n\nBoth dry and moist heat are effective in eliminating microbial life. For example, jars used to store preserves such as jam can be sterilized by heating them in a conventional oven. Heat is also used in pasteurization, a method for slowing the spoilage of foods such as milk, cheese, juices, wines and vinegar. Such products are heated to a certain temperature for a set period of time, which greatly reduces the number of harmful microorganisms.\n\nFoods are often irradiated to kill harmful pathogens. Common sources of radiation used in food sterilization include cobalt-60 (a gamma emitter), electron beams and x-rays. Ultraviolet light is also used to disinfect drinking water, both in small scale personal-use systems and larger scale community water purification systems.\n\n\n", "id": "965323", "title": "Antimicrobial"}
{"url": "https://en.wikipedia.org/wiki?curid=387814", "text": "Spermicide\n\nSpermicide is a contraceptive substance that destroys sperm, inserted vaginally prior to intercourse to prevent pregnancy. As a contraceptive, spermicide may be used alone. However, the pregnancy rate experienced by couples using only spermicide is higher than that of couples using other methods. Usually, spermicides are combined with contraceptive barrier methods such as diaphragms, condoms, cervical caps, and sponges. Combined methods are believed to result in lower pregnancy rates than either method alone.\n\nSpermicides are unscented, clear, unflavored, non-staining, and lubricative.\n\nThe most common active ingredient of spermicides is nonoxynol-9. Spermicides containing nonoxynol-9 are available in many forms, such as jelly (gel), films, and foams. Used alone, spermicides have a perfect use failure rate of 18% per year when used correctly and consistently, and 28% failure rate per year in typical use.\n\nThis list of examples was provided by the Mayo Clinic \n\nNonoxynol-9 is the primary chemical in spermicides to inhibit sperm motility. Active secondary spermicidal ingredients can include octoxynol-9, benzalkonium chloride and menfegol. These secondary ingredients aren't mainstream in the United States, being the only active ingredient adopted the one cited above. Preventing sperm motility will inhibit the sperm from travelling towards the egg moving down the fallopian tubes to the uterus. In fact the deep proper insertion of spermicide should effectively block the cervix so that sperm cannot make it past the cervix, to the uterus, or the fallopian tubes. Scholars K.T. Bartman and et al. study observing the distribution of spermicide containing Nonoxynol-9 in the vaginal tract, showed “After 10 min the gel spread within the vaginal canal providing a contiguous covering of the epithelium of variable thickness.” The sole goal of spermicide is to prevent fertilization.\n\nMenfegol is a spermicide manufactured as a foaming tablet. It is available only in Europe.\n\nOctoxynol-9 was previously a common spermicide, but was removed from the U.S. market in 2002 after manufacturers failed to perform new studies required by the FDA.\n\nThe spermicides benzalkonium chloride and sodium cholate are used in some contraceptive sponges. Benzalkonium chloride might also be available in Canada as a suppository.\n\nThe 2008 Ig Nobel Prize (a parody of the Nobel Prizes) in Chemistry was awarded to Sheree Umpierre, Joseph Hill, and Deborah Anderson, for discovering that Coca-Cola is an effective spermicide, and to C.Y. Hong, C.C. Shieh, P. Wu, and B.N. Chiang for proving it is not.\n\nLemon juice solutions have been shown to immobilize sperm in the laboratory, as has Krest Bitter Lemon drink. While the authors of the Krest Bitter Lemon study suggested its use as a postcoital douche, this is unlikely to be effective, as sperm begin leaving the ejaculate (out of the reach of any douche) within 1.5 minutes of deposition. No published studies appear to have been done on the effectiveness of lemon juice preparations in preventing pregnancy, though they are advocated by some as 'natural' spermicides.\n\nLactic acid preparations have also been shown to have some spermicidal effect, and commercial lactic acid-based spermicides are available. However, no published studies on the effectiveness of lactic acid in preventing pregnancy appear to have been done since 1936. Thomas Moench, a former assistant professor of medicine, has said that research into acids as spermicides has \"pretty much been abandoned.\"\n\nExtractives of the neem plant such as neem oil have also been proposed as spermicides based on laboratory studies. Animal studies of creams and pessaries derived from neem have shown they have contraceptive effects, however trials in humans to determine its effectiveness in preventing pregnancy have not yet been conducted.\n\nSpermicides are believed to increase the contraceptive effectiveness of condoms.\n\nHowever, condoms that are spermicidally lubricated by the manufacturer have a shorter shelf life and may cause urinary-tract infections in women. The World Health Organization says that spermicidally lubricated condoms should no longer be promoted. However, they recommend using a nonoxynol-9 lubricated condom over no condom at all.\n\nSpermicides used alone are only about 70 to 80 percent effective. When spermicides are used in conjunction with condoms and other barrier methods there is a 97 percent effective rate for pregnancy prevention.\n\nTemporary local skin irritation involving the vulva, vagina, or penis is the most common problem associated with spermicide use.\n\nFrequent use (2 times or more a day) of nonoxynol-9 containing spermicide is inadvisable if STI/HIV exposure is likely, because in this situation there is increased vulvovaginal epithelial disruption and increased risk of HIV acquisition.\n\nIn 2007, the United States Food and Drug Administration (FDA) mandated that labels for nonoxynol-9 over-the-counter (OTC) contraceptive products carry a new warning saying they do not protect against STDs and HIV/AIDS.\n\nThe first written record of spermicide use is found in the Kahun Papyrus, an Egyptian document dating to 1850 BCE. It described a pessary of crocodile dung and fermented dough. It is believed that the low pH of the dung may have had a spermicidal effect.\n\nFurther formulations are found in the Ebers Papyrus from approximately 1500 BCE. It recommended mixing seed wool, acacia, dates and honey, and placing the mixture in the vagina. It probably had some effectiveness, in part as a physical barrier due to the thick, sticky consistency, and also because of the lactic acid (a known spermicide) formed from the acacia.\n\nWritings by Soranus, a 2nd-century Greek physician, contained formulations for a number of acidic concoctions claimed to be spermicidal. His instructions were to soak wool in one of the mixtures, then place near the cervix.\n\nLaboratory testing of substances to see if they inhibited sperm motility began in the 1800s. Modern spermicides nonoxynol-9 and menfegol were developed from this line of research. However, many other substances of dubious contraceptive value were also promoted. Especially after the prohibition of contraception in the U.S. by the 1873 Comstock Act, spermicides—the most popular of which was Lysol—were marketed only as \"feminine hygiene\" products and were not held to any standard of effectiveness. Worse, many manufacturers recommended using the products as a douche \"after\" intercourse, too late to affect all the sperm. Medical estimates during the 1930s placed the pregnancy rate of women using many over-the-counter spermicides at seventy percent per year.\n\nSpermicides have been around for a long time. Ancient Egyptians used to use them to regulate family size and pregnancy. Spermicide remedies included ingredients like acacia gum, sour milk, crocodile dung and natron minerals were mixed with plant fibers and honey and formed into a pessary. Modern researchers A. Pakrashi et al. have found acacia to be spermicidal. Ttriterpene saponins from Acacia auriculiformis were found to have sperm immobilizing effect in vitro. This acacia derivative successfully prevented sperm entry into the cervical mucus, disrupted spermatozoa plasma membrane and disintegrated the acrosomal cap. Today it is understood that the more acidic the vagina is, sperm death is increased due to a hostile environment.\n\nA misconception about spermicides existed in the 1980s and 1990s. A 1988 literature review article noted that \"in vitro\" studies of nonoxynol-9 and other spermicides showed inactivation of STD pathogens, including HIV. But a 2002 systemic review and meta-analysis of nine randomized controlled trials of vaginal nonoxynol-9 for HIV and STI prevention involving more than 5,000 women (predominantly sex workers) found no statistically significant reduction in risk of HIV and STIs, but found a small statistically significant increase in genital lesions among nonoxynol-9 spermicide users. And in a high-risk population using a nonoxynol-9 vaginal gel more than three applications per day on average, the risk of HIV acquisition was increased.\n\n", "id": "387814", "title": "Spermicide"}
{"url": "https://en.wikipedia.org/wiki?curid=7143953", "text": "Antiparasitic\n\nAntiparasitics are a class of medications which are indicated for the treatment of parasitic diseases, such as those caused by helminths, amoeba, ectoparasites, parasitic fungi, and protozoa, among others. Antiparasitics target the parasitic agents of the infections by destroying them or inhibiting their growth; they are usually effective against a limited number of parasites within a particular class. Antiparasitics are one of the antimicrobial drugs which include antibiotics that target bacteria, and antifungals that target fungi. They may be administered orally, intravenously or topically.\n\nBroad-Spectrum antiparasitics, analogous to broad-spectrum antibiotics for bacteria, are antiparasitic drugs with efficacy in treating a wide range of parasitic infections caused by parasites from different classes.\n\n\n\n\n\n\n\n\nAntiparasitics treat parasitic diseases, which impact an estimated 2 billion people.\n\nAntiparastics may be given via a variety of routes depending on the specific medication, including oral, topical, and intravenous.\n\nEarly antiparasitics were ineffective, frequently toxic to patients, and difficult to administer due to the difficulty in distinguishing between the host and the parasite.\n\nBetween 1975 and 1999 only 13 of 1,300 new drugs were antiparasitics, which raised concerns that insufficient incentives existed to drive development of new treatments for diseases that disproportionately target low-income countries. This led to new public sector and public-private partnerships (PPPs), including investment by the Bill and Melinda Gates Foundation. Between 2000 and 2005, twenty new antiparasitic agents were developed or in development. In 2005, a new antimalarial cost approximately $300 million to develop with a 50% failure rate.\n\n", "id": "7143953", "title": "Antiparasitic"}
{"url": "https://en.wikipedia.org/wiki?curid=49197", "text": "Antiviral drug\n\nAntiviral drugs are a class of medication used specifically for treating viral infections rather than bacterial ones. Most antivirals are used for specific viral infections, while a broad-spectrum antiviral is effective against a wide range of viruses. Unlike most antibiotics, antiviral drugs do not destroy their target pathogen; instead they inhibit their development.\n\nAntiviral drugs are one class of antimicrobials, a larger group which also includes antibiotic (also termed antibacterial), antifungal and antiparasitic drugs, or antiviral drugs based on monoclonal antibodies. Most antivirals are considered relatively harmless to the host, and therefore can be used to treat infections. They should be distinguished from viricides, which are not medication but deactivate or destroy virus particles, either inside or outside the body. Natural antivirals are produced by some plants such as eucalyptus.\n\nMost of the antiviral drugs now available are designed to help deal with HIV, herpes viruses, the hepatitis B and C viruses, and influenza A and B viruses. Researchers are working to extend the range of antivirals to other families of pathogens.\n\nDesigning safe and effective antiviral drugs is difficult, because viruses use the host's cells to replicate. This makes it difficult to find targets for the drug that would interfere with the virus without also harming the host organism's cells. Moreover, the major difficulty in developing vaccines and anti-viral drugs is due to viral variation.\n\nThe emergence of antivirals is the product of a greatly expanded knowledge of the genetic and molecular function of organisms, allowing biomedical researchers to understand the structure and function of viruses, major advances in the techniques for finding new drugs, and the pressure placed on the medical profession to deal with the human immunodeficiency virus (HIV), the cause of acquired immunodeficiency syndrome (AIDS).\n\nThe first experimental antivirals were developed in the 1960s, mostly to deal with herpes viruses, and were found using traditional trial-and-error drug discovery methods. Researchers grew cultures of cells and infected them with the target virus. They then introduced into the cultures chemicals which they thought might inhibit viral activity, and observed whether the level of virus in the cultures rose or fell. Chemicals that seemed to have an effect were selected for closer study.\n\nThis was a very time-consuming, hit-or-miss procedure, and in the absence of a good knowledge of how the target virus worked, it was not efficient in discovering effective antivirals which had few side effects. Only in the 1980s, when the full genetic sequences of viruses began to be unraveled, did researchers begin to learn how viruses worked in detail, and exactly what chemicals were needed to thwart their reproductive cycle.\n\nViruses consist of a genome and sometimes a few enzymes stored in a capsule made of protein (called a capsid), and sometimes covered with a lipid layer (sometimes called an 'envelope'). Viruses cannot reproduce on their own, and instead propagate by subjugating a host cell to produce copies of themselves, thus producing the next generation.\n\nResearchers working on such \"rational drug design\" strategies for developing antivirals have tried to attack viruses at every stage of their life cycles. Some species of mushrooms have been found to contain multiple antiviral chemicals with similar synergistic effects.\nViral life cycles vary in their precise details depending on the type of virus, but they all share a general pattern:\n\n\nSeveral factors including cost, vaccination stigma, and acquired resistance limit the effectiveness of antiviral therapies. These issues are explored via a health policy perspective.\n\nRising Costs\n\nCost is an important factor that limits access to antivirals therapies in the United States and internationally. The recommended treatment regimen for hepatitis C virus infection, for example, includes sofosbuvir-velpatasvir (Epclusa) and ledipasvir-sofosbuvir (Harrvoni). A twelve-week supply of these drugs amount to $113,400 and $89,712, respectively. These drugs can be manufactured generically at a cost of $100 - $250 per 12 week treatment. Pharmaceutical companies attribute the majority of these costs to research and development expenses. However, critics point to monopolistic market conditions that allow manufacturers to increase prices without facing a reduction in sales, leading to higher profits at patient's expense. Intellectual property laws, anti-importation policies, and the slow pace of FDA review limit alternative options. Recently, private-public research partnerships have been established to promote expedited, cost-effective research.\n\nWhile most antivirals treat viral infection, vaccines are a preemptive first line of defense against pathogens. Vaccination involves the introduction (i.e. via injection) of a small amount of typically inactivated or attenuated antigenic material to stimulate an individual’s immune system. The immune system responds by developing white blood cells to specifically combat the introduced pathogen, resulting in adaptive immunity. Vaccination in a population results in herd immunity and greatly improved population health, with significant reductions in viral infection and disease.\n\nVaccination policy in the United States consists of public and private vaccination requirements. For instance, public schools require students to receive vaccinations (termed \"vaccination schedule\") for viruses and bacteria such as diphtheria, pertussis, and tetanus (DTaP), measles, mumps, rubella (MMR), varicella (chickenpox), hepatitis B, rotavirus, polio, and more. Private institutions might require annual influenza vaccination. The Center for Disease Control and Prevention has estimated that routine immunization of newborns prevents about 42,000 deaths and 20 million cases of disease each year, saving about $13.6 billion.\n\nDespite their successes, there is plenty of stigma surrounding vaccines that cause people to be incompletely vaccinated. These \"gaps\" in vaccination result in unnecessary infection, death, and costs. There are two major reasons for incomplete vaccination:\nAlthough the American Academy of Pediatrics endorses universal immunization, they note that physicians should respect parents’ refusal to vaccinate their children after sufficient advising and provided the child does not face a significant risk of infection. Parents can also cite religious reasons to avoid public school vaccination mandates, but this reduces herd immunity and increases risk of viral infection.\n\nVaccines bolster the body's immune system to better attack viruses in the \"complete particle\" stage, outside of the organism's cells. They traditionally consist of an attenuated (a live weakened) or inactivated (killed) version of the virus. These vaccines can, in very rare cases, harm the host by inadvertently infecting the host with a full-blown viral occupancy. Recently \"subunit\" vaccines have been devised that consist strictly of protein targets from the pathogen. They stimulate the immune system without doing serious harm to the host. In either case, when the real pathogen attacks the subject, the immune system responds to it quickly and blocks it.\n\nVaccines are very effective on stable viruses, but are of limited use in treating a patient who has already been infected. They are also difficult to successfully deploy against rapidly mutating viruses, such as influenza (the vaccine for which is updated every year) and HIV. Antiviral drugs are particularly useful in these cases.\n\nFollowing the HPTN 052 study and PARTNER study, there is significant evidence to demonstrate that antiretroviral drugs inhibit transmission when the person living with HIV has been undetectable for 6 months or longer.\n\nUse and Distribution\n\nGuidelines regarding viral diagnoses and treatments change frequently and limit quality care. Even when physicians diagnose older patients with influenza, use of antiviral treatment can be low. Provider knowledge of antiviral therapies can improve patient care, especially in geriatric medicine. Furthermore, in local health departments (LHDs) with access to antivirals, guidelines may be unclear, causing delays in treatment. With time-sensitive therapies, delays could lead to lack of treatment.\nOverall, national guidelines regarding infection control and management standardize care and improve patient and health care worker safety. Guidelines such as those provided by the Centers for Disease Control and Prevention (CDC) during the 2009 flu pandemic caused by the H1N1 virus, recommend antiviral treatment regimens, clinical assessment algorithms for coordination of care, and antiviral chemoprophylaxis guidelines for exposed persons, among others. Roles of pharmacists and pharmacies have also expanded to meet the needs of public during public health emergencies.\n\nStockpiling\n\nPublic Health Emergency Preparedness initiatives are managed by the CDC via the Office of Public Health Preparedness and Response. Funds aim to support communities in preparing for public health emergencies, including pandemic influenza. Also managed by the CDC, the Strategic National Stockpile (SNS) consists of bulk quantities of medicines and supplies for use during such emergencies. Antiviral stockpiles prepare for shortages of antiviral medications in cases of public health emergencies. During the H1N1 pandemic in 2009-2010, guidelines for SNS use by local health departments was unclear, revealing gaps in antiviral planning. For example, local health departments that received antivirals from the SNS did not have transparent guidance on the use of the treatments. The gap made it difficult to create plans and policies for their use and future availabilities, causing delays in treatment.\n\nThe general idea behind modern antiviral drug design is to identify viral proteins, or parts of proteins, that can be disabled. These \"targets\" should generally be as unlike any proteins or parts of proteins in humans as possible, to reduce the likelihood of side effects. The targets should also be common across many strains of a virus, or even among different species of virus in the same family, so a single drug will have broad effectiveness. For example, a researcher might target a critical enzyme synthesized by the virus, but not the patient, that is common across strains, and see what can be done to interfere with its operation.\n\nOnce targets are identified, candidate drugs can be selected, either from drugs already known to have appropriate effects, or by actually designing the candidate at the molecular level with a computer-aided design program.\n\nThe target proteins can be manufactured in the lab for testing with candidate treatments by inserting the gene that synthesizes the target protein into bacteria or other kinds of cells. The cells are then cultured for mass production of the protein, which can then be exposed to various treatment candidates and evaluated with \"rapid screening\" technologies.\n\nOne anti-viral strategy is to interfere with the ability of a virus to infiltrate a target cell. The virus must go through a sequence of steps to do this, beginning with binding to a specific \"receptor\" molecule on the surface of the host cell and ending with the virus \"uncoating\" inside the cell and releasing its contents. Viruses that have a lipid envelope must also fuse their envelope with the target cell, or with a vesicle that transports them into the cell, before they can uncoat.\n\nThis stage of viral replication can be inhibited in two ways:\n\nThis strategy of designing drugs can be very expensive, and since the process of generating anti-idiotypic antibodies is partly trial and error, it can be a relatively slow process until an adequate molecule is produced.\n\nA very early stage of viral infection is viral entry, when the virus attaches to and enters the host cell. A number of \"entry-inhibiting\" or \"entry-blocking\" drugs are being developed to fight HIV. HIV most heavily targets the immune system's white blood cells known as \"helper T cells\", and identifies these target cells through T-cell surface receptors designated \"CD4\" and \"CCR5\". Attempts to interfere with the binding of HIV with the CD4 receptor have failed to stop HIV from infecting helper T cells, but research continues on trying to interfere with the binding of HIV to the CCR5 receptor in hopes that it will be more effective.\n\nHIV infects a cell through fusion with the cell membrane, which requires two different cellular molecular participants, CD4 and a chemokine receptor (differing depending on the cell type). Approaches to blocking this virus/cell fusion have shown some promise in preventing entry of the virus into a cell. At least one of these entry inhibitors—a biomimetic peptide marketed under the brand name Fuzeon—has received FDA approval and has been in use for some time. Potentially, one of the benefits from the use of an effective entry-blocking or entry-inhibiting agent is that it potentially may not only prevent the spread of the virus within an infected individual but also the spread from an infected to an uninfected individual.\n\nOne possible advantage of the therapeutic approach of blocking viral entry (as opposed to the currently dominant approach of viral enzyme inhibition) is that it may prove more difficult for the virus to develop resistance to this therapy than for the virus to mutate or evolve its enzymatic protocols.\n\nInhibitors of uncoating have also been investigated.\n\nAmantadine and rimantadine have been introduced to combat influenza. These agents act on penetration and uncoating.\n\nPleconaril works against rhinoviruses, which cause the common cold, by blocking a pocket on the surface of the virus that controls the uncoating process. This pocket is similar in most strains of rhinoviruses and enteroviruses, which can cause diarrhea, meningitis, conjunctivitis, and encephalitis.\n\nSome scientists are making the case that a vaccine against rhinoviruses, the predominant cause of the common cold, is achievable.\nVaccines that combine dozens of varieties of rhinovirus at once are effective in stimulating antiviral antibodies in mice and monkeys, researchers have reported in Nature Communications in 2016.\n\nThe quest for a vaccine against rhinoviruses may have seemed quixotic, because there are more than 100 varieties circulating around the world. But the immune system can handle the challenge.\n\nRhinoviruses are the most common cause of the common cold; other viruses such as respiratory syncytial virus, parainfluenza virus and adenoviruses can cause them too. Rhinoviruses also exacerbate asthma attacks. Although rhinoviruses come in many varieties, they do not drift to the same degree that influenza viruses do. A mixture of 50 inactivated rhinovirus types should be able to stimulate neutralizing antibodies against all of them to some degree.\n\nA second approach is to target the processes that synthesize virus components after a virus invades a cell.\n\nOne way of doing this is to develop nucleotide or nucleoside analogues that look like the building blocks of RNA or DNA, but deactivate the enzymes that synthesize the RNA or DNA once the analogue is incorporated. This approach is more commonly associated with the inhibition of reverse transcriptase (RNA to DNA) than with \"normal\" transcriptase (DNA to RNA).\n\nThe first successful antiviral, aciclovir, is a nucleoside analogue, and is effective against herpesvirus infections. The first antiviral drug to be approved for treating HIV, zidovudine (AZT), is also a nucleoside analogue.\n\nAn improved knowledge of the action of reverse transcriptase has led to better nucleoside analogues to treat HIV infections. One of these drugs, lamivudine, has been approved to treat hepatitis B, which uses reverse transcriptase as part of its replication process. Researchers have gone further and developed inhibitors that do not look like nucleosides, but can still block reverse transcriptase.\n\nAnother target being considered for HIV antivirals include RNase H – which is a component of reverse transcriptase that splits the synthesized DNA from the original viral RNA.\n\nOn 10 August 2011 researchers at MIT announced the publication of a new method of inhibiting RNA, the process selectively affected infected cells. The team named the process \"Double-stranded RNA Activated Caspase Oligomerizer\" (DRACO). According to the lead researcher \"In theory, [DRACO] should work against all viruses.\"\n\nAnother target is integrase, which integrate the synthesized DNA into the host cell genome.\n\nOnce a virus genome becomes operational in a host cell, it then generates messenger RNA (mRNA) molecules that direct the synthesis of viral proteins. Production of mRNA is initiated by proteins known as transcription factors. Several antivirals are now being designed to block attachment of transcription factors to viral DNA.\n\nGenomics has not only helped find targets for many antivirals, it has provided the basis for an entirely new type of drug, based on \"antisense\" molecules. These are segments of DNA or RNA that are designed as complementary molecule to critical sections of viral genomes, and the binding of these antisense segments to these target sections blocks the operation of those genomes. A phosphorothioate antisense drug named fomivirsen has been introduced, used to treat opportunistic eye infections in AIDS patients caused by cytomegalovirus, and other antisense antivirals are in development. An antisense structural type that has proven especially valuable in research is morpholino antisense.\n\nMorpholino oligos have been used to experimentally suppress many viral types:\n\n\nYet another antiviral technique inspired by genomics is a set of drugs based on ribozymes, which are enzymes that will cut apart viral RNA or DNA at selected sites. In their natural course, ribozymes are used as part of the viral manufacturing sequence, but these synthetic ribozymes are designed to cut RNA and DNA at sites that will disable them.\n\nA ribozyme antiviral to deal with hepatitis C has been suggested, and ribozyme antivirals are being developed to deal with HIV. An interesting variation of this idea is the use of genetically modified cells that can produce custom-tailored ribozymes. This is part of a broader effort to create genetically modified cells that can be injected into a host to attack pathogens by generating specialized proteins that block viral replication at various phases of the viral life cycle.\n\nInterference with post translational modifications or with targeting of viral proteins in the cell is also possible.\n\nSome viruses include an enzyme known as a protease that cuts viral protein chains apart so they can be assembled into their final configuration. HIV includes a protease, and so considerable research has been performed to find \"protease inhibitors\" to attack HIV at that phase of its life cycle. Protease inhibitors became available in the 1990s and have proven effective, though they can have unusual side effects, for example causing fat to build up in unusual places. Improved protease inhibitors are now in development.\n\nProtease inhibitors have also been seen in nature. A protease inhibitor was isolated from the Shiitake mushroom (\"Lentinus edodes\"). The presence of this may explain the Shiitake mushrooms noted antiviral activity \"in vitro\".\n\nRifampicin acts at the assembly phase.\n\nThe final stage in the life cycle of a virus is the release of completed viruses from the host cell, and this step has also been targeted by antiviral drug developers. Two drugs named zanamivir (Relenza) and oseltamivir (Tamiflu) that have been recently introduced to treat influenza prevent the release of viral particles by blocking a molecule named neuraminidase that is found on the surface of flu viruses, and also seems to be constant across a wide range of flu strains.\n\nA second category of tactics for fighting viruses involves encouraging the body's immune system to attack them, rather than attacking them directly. Some antivirals of this sort do not focus on a specific pathogen, instead stimulating the immune system to attack a range of pathogens.\n\nOne of the best-known of this class of drugs are interferons, which inhibit viral synthesis in infected cells. One form of human interferon named \"interferon alpha\" is well-established as part of the standard treatment for hepatitis B and C, and other interferons are also being investigated as treatments for various diseases.\n\nA more specific approach is to synthesize antibodies, protein molecules that can bind to a pathogen and mark it for attack by other elements of the immune system. Once researchers identify a particular target on the pathogen, they can synthesize quantities of identical \"monoclonal\" antibodies to link up that target. A monoclonal drug is now being sold to help fight respiratory syncytial virus in babies, and antibodies purified from infected individuals are also used as a treatment for hepatitis B.\n\nAntiviral resistance can be defined by a decreased susceptibility to a drug through either a minimally effective, or completely ineffective, treatment response to prevent associated illnesses from a particular virus. The issue inevitably remains a major obstacle to antiviral therapy as it has developed to almost all specific and effective antimicrobials, including antiviral agents.\n\nThe Centers for Disease Control and Prevention (CDC) inclusively recommends those six months and older to get a yearly vaccination to protect from influenza A viruses (H1N1) and (H3N2) and up to two influenza B viruses (depending on the vaccination). Comprehensive protection starts by ensuring vaccinations are current and complete. The three FDA-approved neuraminidase antiviral flu drugs available in the United States, recommended by the CDC, include: oseltamivir (Tamiflu®), zanamivir (Relenza®), and peramivir (Rapivab®).\n\nA study published in 2009 in Nature Biotechnology emphasized the urgent need for augmentation of oseltamivir (Tamiflu®) stockpiles with additional antiviral drugs including zanamivir (Relenza®). This finding was based on a performance evaluation of these drugs supposing the 2009 H1N1 'Swine Flu' neuraminidase (NA) were to acquire the Tamiflu-resistance (His274Tyr) mutation which is currently widespread in seasonal H1N1 strains.\n\nOrigin of antiviral resistance\n\nThe genetic makeup of viruses is constantly changing and therefore may alter the virus resistant to the treatments currently available. Viruses can become resistant through spontaneous or intermittent mechanisms throughout the course of an antiviral treatment. Immunocompromised patients, more often than immunocompetent patients, hospitalized with pneumonia are at the highest risk of developing oseltamivir resistance during treatment. Subsequent to exposure to someone else with the flu, those who received oseltamivir for \"post-exposure prophylaxis\" are also at higher risk of resistance.\n\nDetection of antiviral resistance\n\nNational and international surveillance is performed by the CDC to determine effectiveness of the current FDA-approved antiviral flu drugs. Public health officials use this information to make current recommendations about the use of flu antiviral medications. WHO further recommends in-depth epidemiological investigations to control potential transmission of the resistant virus and prevent future progression. As novel treatments and detection techniques to antiviral resistance are enhanced so can the establishment of strategies to combat the inevitable emergence of antiviral resistance.\n\n", "id": "49197", "title": "Antiviral drug"}
{"url": "https://en.wikipedia.org/wiki?curid=1365811", "text": "Avicide\n\nAn avicide is any substance (normally, a chemical) which can be used to kill birds. \n\nCommonly used avicides include strychnine, DRC-1339 (3-chloro-4-methylaniline hydrochloride, Starlicide) and CPTH (3-chloro-p-toluidine, the free base of Starlicide), and Avitrol (4-aminopyridine). Chloralose is also used as an avicide. In the past, highly concentrated formulations of parathion in diesel oil were also used, applied by aircraft spraying over the nesting colonies of the birds.\nIt is impossible to minimize risk from avicides for non-targets species.\n\n", "id": "1365811", "title": "Avicide"}
{"url": "https://en.wikipedia.org/wiki?curid=13652511", "text": "Microbicide\n\nA microbicide is any biocidal compound or substance whose purpose is to reduce the infectivity of microbes, such as viruses or bacteria. One example is wood tar.\n\n", "id": "13652511", "title": "Microbicide"}
{"url": "https://en.wikipedia.org/wiki?curid=414144", "text": "Sterilization (microbiology)\n\nSterilization (or sterilisation) refers to any process that eliminates, removes, kills, or deactivates all forms of life and other biological agents (such as fungi, bacteria, viruses, spore forms, prions, unicellular eukaryotic organisms such as Plasmodium, etc.) present in a specified region, such as a surface, a volume of fluid, medication, or in a compound such as biological culture media. Sterilization can be achieved through various means, including: heat, chemicals, irradiation, high pressure, and filtration. Sterilization is distinct from disinfection, sanitization, and pasteurization, in that sterilization kills, deactivates, or eliminates all forms of life and other biological agents which are present.\n\nOne of the first steps toward sterilization was made by Nicolas Appert who discovered that thorough application of heat over a suitable period slowed the decay of foods and various liquids, preserving them for safe consumption for a longer time than was typical. Canning of foods is an extension of the same principle and has helped to reduce food borne illness (\"food poisoning\"). Other methods of sterilizing foods include food irradiation and high pressure (pascalization).\n\nIn general, surgical instruments and medications that enter an already aseptic part of the body (such as the bloodstream, or penetrating the skin) must be sterile. Examples of such instruments include scalpels, hypodermic needles, and artificial pacemakers. This is also essential in the manufacture of parenteral pharmaceuticals.\n\nPreparation of injectable medications and intravenous solutions for fluid replacement therapy requires not only sterility but also well-designed containers to prevent entry of adventitious agents after initial product sterilization.\n\nMost medical and surgical devices used in healthcare facilities are made of materials that are able to go under steam sterilization. However, since 1950, there has been an increase in medical devices and instruments made of materials (e.g., plastics) that require low-temperature sterilization. Ethylene oxide gas has been used since the 1950s for heat- and moisture-sensitive medical devices. Within the past 15 years, a number of new, low-temperature sterilization systems (e.g., hydrogen peroxide gas plasma, peracetic acid immersion, ozone) have been developed and are being used to sterilize medical devices.\nSteam sterilization is the most widely used and the most dependable. Steam sterilization is nontoxic, inexpensive, rapidly microbicidal, sporicidal, and rapidly heats and penetrates fabrics.\n\nThere are strict international rules to protect the contamination of Solar System bodies from biological material from Earth. Standards vary depending on both the type of mission and its destination; the more likely a planet is considered to be habitable, the stricter the requirements are.\n\nMany components of instruments used on spacecraft cannot withstand very high temperatures, so techniques not requiring excessive temperatures are used as tolerated, including heating to at least 120 °C, chemical sterilization, oxidization, ultraviolet, and irradiation.\n\nThe aim of sterilization is the reduction of initially present microorganisms or other potential pathogens. The degree of sterilization is commonly expressed by multiples of the decimal reduction time, or D-value, denoting the time needed to reduce the initial number formula_1 to one tenth (formula_2) of its original value. Then the number of microorganisms formula_3 after sterilization time formula_4 is given by:\nThe D-value is a function of sterilization conditions and varies with the type of microorganism, temperature, water activity, pH etc.. For steam sterilization (see below) typically the temperature (in °Celsius) is given as index.\n\nTheoretically, the likelihood of survival of an individual microorganism is never zero. To compensate for this, the overkill method is often used. Using the overkill method, sterilization is performed by sterilizing for longer than is required to kill the bioburden present on or in the item being sterilized. This provides a sterility assurance level (SAL) equal to the probability of a non-sterile unit.\n\nFor high-risk applications such as medical devices and injections, a sterility assurance level of at least 10 is required by the United States Food and Drug Administration (FDA).\n\nA widely used method for heat sterilization is the autoclave, sometimes called a converter or steam sterilizer. Autoclaves use steam heated to 121-134 °C under pressure. To achieve sterility, the article is placed in a chamber and heated by injected steam until the article reaches a time and temperature setpoint. Almost all the air is removed from the chamber, because air is undesired in the moist heat sterilization process (this is one trait that differ from a typical pressure cooker used for food cooking). The article is held at the temperature setpoint for a period of time which varies depending on what bioburden is present on the article being sterilized and its resistance (D-value) to steam sterilization. A general cycle would be anywhere between 3 and 15 minutes, (depending on the generated heat) at 121 °C at 100 kPa, which is sufficient to provide a sterility assurance level of 10 for a product with a bioburden of 10 and a D-value of 2.0 minutes. Following sterilization, liquids in a pressurized autoclave must be cooled slowly to avoid boiling over when the pressure is released. This may be achieved by gradually depressurizing the sterilization chamber and allowing liquids to evaporate under a negative pressure, while cooling the contents.\n\nProper autoclave treatment will inactivate all resistant bacterial spores in addition to fungi, bacteria, and viruses, but is not expected to eliminate all prions, which vary in their resistance. For prion elimination, various recommendations state 121-132 °C for 60 minutes or 134 °C for at least 18 minutes. The 263K scrapie prion is inactivated relatively quickly by such sterilization procedures; however, other strains of scrapie, and strains of CJD and BSE are more resistant. Using mice as test animals, one experiment showed that heating BSE positive brain tissue at 134-138 °C for 18 minutes resulted in only a 2.5 log decrease in prion infectivity.\n\nMost autoclaves have meters and charts that record or display information, particularly temperature and pressure as a function of time. The information is checked to ensure that the conditions required for sterilization have been met. Indicator tape is often placed on packages of products prior to autoclaving, and some packaging incorporates indicators. The indicator changes color when exposed to steam, providing a visual confirmation.\n\nBioindicators can also be used to independently confirm autoclave performance. Simple bioindicator devices are commercially available based on microbial spores. Most contain spores of the heat resistant microbe \"Geobacillus stearothermophilus\" (formerly \"Bacillus stearothermophilus\"), which is extremely resistant to steam sterilization. Biological indicators may take the form of glass vials of spores and liquid media, or as spores on strips of paper inside glassine envelopes. These indicators are placed in locations where it is difficult for steam to reach to verify that steam is penetrating there.\n\nFor autoclaving, cleaning is critical. Extraneous biological matter or grime may shield organisms from steam penetration. Proper cleaning can be achieved through physical scrubbing, sonication, ultrasound, or pulsed air.\nPressure cooking and canning is analogous to autoclaving, and when performed correctly renders food sterile.\n\nMoist heat causes the destruction of microorganisms by denaturation of macromolecules, primarily proteins. This method is a faster process than dry heat sterilization.\n\nDry heat was the first method of sterilization and is a longer process than moist heat sterilization. The destruction of microorganisms through the use of dry heat is a gradual phenomenon. With longer exposure to lethal temperatures, the number of killed microorganisms increases. Forced ventilation of hot air can be used to increase the rate at which heat is transferred to an organism and reduce the temperature and amount of time needed to achieve sterility. At higher temperatures, shorter exposure times are required to kill organisms. This can reduce heat-induced damage to food products.\nThe standard setting for a hot air oven is at least two hours at 160 °C. A rapid method heats air to 190 °C for 6 minutes for unwrapped objects and 12 minutes for wrapped objects. Dry heat has the advantage that it can be used on powders and other heat-stable items that are adversely affected by steam (e.g. it does not cause rusting of steel objects).\n\nFlaming is done to loops and straight-wires in microbiology labs. Leaving the loop in the flame of a Bunsen burner or alcohol lamp until it glows red ensures that any infectious agent is inactivated. This is commonly used for small metal or glass objects, but not for large objects (see Incineration below). However, during the initial heating infectious material may be sprayed from the wire surface before it is killed, contaminating nearby surfaces and objects. Therefore, special heaters have been developed that surround the inoculating loop with a heated cage, ensuring that such sprayed material does not further contaminate the area. Another problem is that gas flames may leave carbon or other residues on the object if the object is not heated enough. A variation on flaming is to dip the object in 70% or higher ethanol, then briefly touch the object to a Bunsen burner flame. The ethanol will ignite and burn off rapidly, leaving less residue than a gas flame.\n\nIncineration is a waste treatment process that involves the combustion of organic substances contained in waste materials. This method also burns any organism to ash. It is used to sterilize medical and other biohazardous waste before it is discarded with non-hazardous waste. Bacteria incinerators are mini furnaces that incinerate and kill off any microorganisms that may be on an inoculating loop or wire.\n\nNamed after John Tyndall, Tyndallization is an obsolete and lengthy process designed to reduce the level of activity of sporulating bacteria that are left by a simple boiling water method. The process involves boiling for a period (typically 20 minutes) at atmospheric pressure, cooling, incubating for a day, and then repeating the process a total of three to four times. The incubation periods are to allow heat-resistant spores surviving the previous boiling period to germinate to form the heat-sensitive vegetative (growing) stage, which can be killed by the next boiling step. This is effective because many spores are stimulated to grow by the heat shock. The procedure only works for media that can support bacterial growth, and will not sterilize non-nutritive substrates like water. Tyndallization is also ineffective against prions.\n\nGlass bead sterilizers work by heating glass beads to 250 °C. Instruments are then quickly doused in these glass beads, which heat the object while physically scraping contaminants off their surface. Glass bead sterilizers were once a common sterilization method employed in dental offices as well as biological laboratories, but are not approved by the U.S. Food and Drug Administration (FDA) and Centers for Disease Control and Prevention (CDC) to be used as a sterilizers since 1997. They are still popular in European as well as Israeli dental practices although there are no current evidence-based guidelines for using this sterilizer.\n\nChemicals are also used for sterilization. Heating provides a reliable way to rid objects of all transmissible agents, but it is not always appropriate if it will damage heat-sensitive materials such as biological materials, fiber optics, electronics, and many plastics. In these situations chemicals, either as gases or in liquid form, can be used as sterilants. While the use of gas and liquid chemical sterilants avoids the problem of heat damage, users must ensure that article to be sterilized is chemically compatible with the sterilant being used. In addition, the use of chemical sterilants poses new challenges for workplace safety, as the properties that make chemicals effective sterilants usually make them harmful to humans.\n\nEthylene oxide (EO, EtO) gas treatment is one of the common methods used to sterilize, pasteurize, or disinfect items because of its wide range of material compatibility. It is also used to process items that are sensitive to processing with other methods, such as radiation (gamma, electron beam, X-ray), heat (moist or dry), or other chemicals. Ethylene oxide treatment is the most common sterilization method, used for approximately 70% of total sterilizations, and for over 50% of all disposable medical devices.\n\nEthylene oxide treatment is generally carried out between 30 °C and 60 °C with relative humidity above 30% and a gas concentration between 200 and 800 mg/l. Typically, the process lasts for several hours. Ethylene oxide is highly effective, as it penetrates all porous materials, and it can penetrate through some plastic materials and films. Ethylene oxide kills all known microorganisms such as bacteria (including spores), viruses, and fungi (including yeasts and moulds), and is compatible with almost all materials even when repeatedly applied. It is flammable, toxic, and carcinogenic; however, with a reported potential for some adverse health effects when not used in compliance with published requirements. Ethylene oxide sterilizers and processes require biological validation after sterilizer installation, significant repairs or process changes.\n\nThe traditional process consists of a preconditioning phase (in a separate room or cell), a processing phase (more commonly in a vacuum vessel and sometimes in a pressure rated vessel), and an aeration phase (in a separate room or cell) to remove ethylene oxide residues and lower by-products such as ethylene chlorohydrin (EC or ECH) and, of lesser importance, ethylene glycol (EG). An alternative process, known as all-in-one processing, also exists for some products whereby all three phases are performed in the vacuum or pressure rated vessel. This latter option can facilitate faster overall processing time and residue dissipation.\n\nThe most common ethylene oxide processing method is the gas chamber method. To benefit from economies of scale, ethylene oxide has traditionally been delivered by filling a large chamber with a combination of gaseous ethylene oxide either as pure ethylene oxide, or with other gases used as diluents (chlorofluorocarbons (CFCs), hydrochlorofluorocarbons (HCFCs), or carbon dioxide).\n\nEthylene oxide is still widely used by medical device manufacturers. Since ethylene oxide is explosive at concentrations above 3%, ethylene oxide was traditionally supplied with an inert carrier gas such as a CFC or HCFC. The use of CFCs or HCFCs as the carrier gas was banned because of concerns of ozone depletion. These halogenated hydrocarbons are being replaced by systems using 100% ethylene oxide because of regulations and the high cost of the blends. In hospitals, most ethylene oxide sterilizers use single use cartridges because of the convenience and ease of use compared to the former plumbed gas cylinders of ethylene oxide blends.\n\nIt is important to adhere to patient and healthcare personnel government specified limits of ethylene oxide residues in and/or on processed products, operator exposure after processing, during storage and handling of ethylene oxide gas cylinders, and environmental emissions produced when using ethylene oxide.\n\nThe U.S. Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit (PEL) at 1 ppm calculated as an eight-hour time weighted average (TWA) [29 CFR 1910.1047] and 5 ppm as a 15-minute excursion limit (EL). The National Institute for Occupational Safety and Health (NIOSH) immediately dangerous to life and health limit (IDLH) for ethylene oxide is 800 ppm. The odor threshold is around 500 ppm, so ethylene oxide is imperceptible until concentrations well above the OSHA PEL. Therefore, OSHA recommends that continuous gas monitoring systems be used to protect workers using ethylene oxide for processing. Employees' health records must be maintained during employment and after termination of employment for 30 years.\n\nNitrogen dioxide (NO) gas is a rapid and effective sterilant for use against a wide range of microorganisms, including common bacteria, viruses, and spores. The unique physical properties of NO gas allow for sterilant dispersion in an enclosed environment at room temperature and ambient pressure. The mechanism for lethality is the degradation of DNA in the spore core through nitration of the phosphate backbone, which kills the exposed organism as it absorbs NO. This degradation occurs at even very low concentrations of the gas. NO has a boiling point of 21 °C at sea level, which results in a relatively high saturated vapour pressure at ambient temperature. Because of this, liquid NO may be used as a convenient source for the sterilant gas. Liquid NO is often referred to by the name of its dimer, dinitrogen tetroxide (NO). Additionally, the low levels of concentration required, coupled with the high vapour pressure, assures that no condensation occurs on the devices being sterilized. This means that no aeration of the devices is required immediately following the sterilization cycle. NO is also less corrosive than other sterilant gases, and is compatible with most medical materials and adhesives.\n\nThe most-resistant organism (MRO) to sterilization with NO gas is the spore of Geobacillus stearothermophilus, which is the same MRO for both steam and hydrogen peroxide sterilization processes. The spore form of G. stearothermophilus has been well characterized over the years as a biological indicator in sterilization applications. Microbial inactivation of G. stearothermophilus with NO gas proceeds rapidly in a log-linear fashion, as is typical of other sterilization processes. Noxilizer, Inc. has commercialized this technology to offer contract sterilization services for medical devices at its Baltimore, Maryland (U.S.) facility. This has been demonstrated in Noxilizer’s lab in multiple studies and is supported by published reports from other labs. These same properties also allow for quicker removal of the sterilant and residuals through aeration of the enclosed environment. The combination of rapid lethality and easy removal of the gas allows for shorter overall cycle times during the sterilization (or decontamination) process and a lower level of sterilant residuals than are found with other sterilization methods.\n\nOzone is used in industrial settings to sterilize water and air, as well as a disinfectant for surfaces. It has the benefit of being able to oxidize most organic matter. On the other hand, it is a toxic and unstable gas that must be produced on-site, so it is not practical to use in many settings.\n\nOzone offers many advantages as a sterilant gas; ozone is a very efficient sterilant because of its strong oxidizing properties (E= 2.076 vs SHE) capable of destroying a wide range of pathogens, including prions without the need for handling hazardous chemicals since the ozone is generated within the sterilizer from medical grade oxygen. The high reactivity of ozone means that waste ozone can be destroyed by passing over a simple catalyst that reverts it to oxygen and ensures that the cycle time is relatively short. The disadvantage of using ozone is that the gas is very reactive and very hazardous. The NIOSH immediately dangerous to life and health limit for ozone is smaller than the IDLH for ethylene oxide. Documentation for Immediately Dangerous to Life or Health Concentrations (IDLH): NIOSH Chemical Listing and Documentation of Revised IDLH Values (as of 3/1/95) and OSHA has set the PEL for ozone at calculated as an time weighted average (29 CFR 1910.1000, Table Z-1). The Canadian Center for Occupation Health and Safety provides an excellent summary of the health effects of exposure to ozone. The sterilant gas manufacturers include many safety features in their products but prudent practice is to provide continuous monitoring to below the OSHA PEL to provide a rapid warning in the event of a leak. Monitors for determining workplace exposure to ozone are commercially available.\n\nGlutaraldehyde and formaldehyde solutions (also used as fixatives) are accepted liquid sterilizing agents, provided that the immersion time is sufficiently long. To kill all spores in a clear liquid can take up to 22 hours with glutaraldehyde and even longer with formaldehyde. The presence of solid particles may lengthen the required period or render the treatment ineffective. Sterilization of blocks of tissue can take much longer, due to the time required for the fixative to penetrate. Glutaraldehyde and formaldehyde are volatile, and toxic by both skin contact and inhalation. Glutaraldehyde has a short shelf life (<2 weeks), and is expensive. Formaldehyde is less expensive and has a much longer shelf life if some methanol is added to inhibit polymerization to paraformaldehyde, but is much more volatile. Formaldehyde is also used as a gaseous sterilizing agent; in this case, it is prepared on-site by depolymerization of solid paraformaldehyde. Many vaccines, such as the original Salk polio vaccine, are sterilized with formaldehyde.\n\nHydrogen peroxide, in both liquid and as vaporized hydrogen peroxide (VHP), is another chemical sterilizing agent. Hydrogen peroxide is strong oxidant, which allows it to destroy a wide range of pathogens. Hydrogen peroxide is used to sterilize heat or temperature sensitive articles such as rigid endoscopes. In medical sterilization hydrogen peroxide is used at higher concentrations, ranging from around 35% up to 90%. The biggest advantage of hydrogen peroxide as a sterilant is the short cycle time. Whereas the cycle time for ethylene oxide may be 10 to 15 hours, some modern hydrogen peroxide sterilizers have a cycle time as short as 28 minutes.\n\nDrawbacks of hydrogen peroxide include material compatibility, a lower capability for penetration and operator health risks. Products containing cellulose, such as paper, cannot be sterilized using VHP and products containing nylon may become brittle. The penetrating ability of hydrogen peroxide is not as good as ethylene oxide and so there are limitations on the length and diameter of lumens that can be effectively sterilized and guidance is available from the sterilizer manufacturers. Hydrogen peroxide is primary irritant and the contact of the liquid solution with skin will cause bleaching or ulceration depending on the concentration and contact time. It is relatively non-toxic when diluted to low concentrations, but is a dangerous oxidizer at high concentrations (> 10% w/w). The vapour is also hazardous, primarily affecting the eyes and respiratory system. Even short term exposures can be hazardous and NIOSH has set the Immediately Dangerous to Life and Health Level (IDLH) at 75 ppm, less than one tenth the IDLH for ethylene oxide (800 ppm). Prolonged exposure to lower concentrations can cause permanent lung damage and consequently, OSHA has set the permissible exposure limit to 1.0 ppm, calculated as an 8-hour time weighted average. Sterilizer manufacturers go to great lengths to make their products safe through careful design and incorporation of many safety features, though there are still workplace exposures of hydrogen peroxide from gas sterilizers are documented in the FDA MAUDE database. When using any type of gas sterilizer, prudent work practices will include good ventilation, a continuous gas monitor for hydrogen peroxide and good work practices and training.\n\nVaporized hydrogen peroxide (VHP) is used to sterilize large enclosed and sealed areas such as entire rooms and aircraft interiors.\n\nAlthough toxic, VHP breaks down in a short time from H2O2 to water H2O + O2.\n\nPeracetic acid (0.2%) is a recognized sterilant by the FDA for use in sterilizing medical devices such as endoscopes.\n\nPrions are highly resistant to chemical sterilization. Treatment with aldehydes such as formaldehyde have actually been shown to increase prion resistance. Hydrogen peroxide (3%) for one hour was shown to be ineffective, providing less than 3 logs (10) reduction in contamination. Iodine, formaldehyde, glutaraldehyde, and peracetic acid also fail this test (one hour treatment). Only chlorine, phenolic compounds, guanidinium thiocyanate, and sodium hydroxide (NaOH) reduce prion levels by more than 4 logs; chlorine (too corrosive to use on certain objects) and NaOH are the most consistent. Many studies have shown the effectiveness of sodium hydroxide.\n\nSterilization can be achieved using electromagnetic radiation such as electron beams, X-rays, gamma rays, or irradiation by subatomic particles. Electromagnetic or particulate radiation can be energetic enough to ionize atoms or molecules (ionizing radiation), or less energetic (non-ionizing radiation).\n\nUltraviolet light irradiation (UV, from a germicidal lamp) is useful for sterilization of surfaces and some transparent objects. Many objects that are transparent to visible light absorb UV. UV irradiation is routinely used to sterilize the interiors of biological safety cabinets between uses, but is ineffective in shaded areas, including areas under dirt (which may become polymerized after prolonged irradiation, so that it is very difficult to remove). It also damages some plastics, such as polystyrene foam if exposed for prolonged periods of time.\nThe safety of irradiation facilities is regulated by the United Nations International Atomic Energy Agency and monitored by the different national Nuclear Regulatory Commissions. The incidents that have occurred in the past are documented by the agency and thoroughly analyzed to determine root cause and improvement potential. Such improvements are then mandated to retrofit existing facilities and future design.\n\nGamma radiation is very penetrating, and is commonly used for sterilization of disposable medical equipment, such as syringes, needles, cannulas and IV sets, and food. It is emitted by a radioisotope, usually cobalt-60(Co) or caesium-137 (Cs), which have photon energies of up to 1.3 and 0.66 MeV respectively.\n\nUse of a radioisotope requires shielding for the safety of the operators while in use and in storage. With most designs, the radioisotope is lowered into a water-filled source storage pool, which absorbs radiation and allows maintenance personnel to enter the radiation shield. One variant keeps the radioisotope under water at all times and lowers the product to be irradiated into the water towards the source in hermetic bells; no further shielding is required for such designs. Other uncommonly used designs use dry storage, providing movable shields that reduce radiation levels in areas of the irradiation chamber. An incident in Decatur Georgia, US, where water-soluble caesium-137 leaked into the source storage pool, requiring NRC intervention has led to use of this radioisotope being almost entirely discontinued in favour of the more costly, non-water-soluble cobalt-60. Cobalt-60 gamma photons have about twice the energy, and hence greater penetrating range, of Caesium-137 radiation.\n\nElectron beam processing is also commonly used for sterilization. Electron beams use an on-off technology and provide a much higher dosing rate than gamma or x-rays. Due to the higher dose rate, less exposure time is needed and thereby any potential degradation to polymers is reduced. Because electrons carry a charge, electron beams are less penetrating than either gamma or x-rays. Facilities rely on substantial concrete shields to protect workers and the environment from radiation exposure.\n\nX-rays: high-energy X-rays (produced by bremsstrahlung) allow irradiation of large packages and pallet loads of medical devices. They are sufficiently penetrating to treat multiple pallet loads of low-density packages with very good dose uniformity ratios. X-ray sterilization does not require chemical or radioactive material: high-energy X-rays are generated at high intensity by an X-ray generator that does not require shielding when not in use. X-rays are generated by bombarding a dense material (target) such as tantalum or tungsten with high-energy electrons in a process known as bremsstrahlung conversion. These systems are energy-inefficient, requiring much more electrical energy than other systems for the same result.\n\nIrradiation with X-rays, gamma rays, or electrons does not make materials radioactive, because the energy used is too low. Generally an energy of at least 10 MeV is needed to induce radioactivity in a material. Neutrons and very high-energy particles can make materials radioactive, but have good penetration, whereas lower energy particles (other than neutrons) cannot make materials radioactive, but have poorer penetration.\n\nSterilization by irradiation with gamma rays may however in some cases affect material properties.\n\nIrradiation is used by the United States Postal Service to sterilize mail in the Washington, D.C. area. Some foods (e.g. spices, ground meats) are sterilized by irradiation.\n\nSubatomic particles may be more or less penetrating and may be generated by a radioisotope or a device, depending upon the type of particle.\n\nFluids that would be damaged by heat, irradiation or chemical sterilization, such as drug products, can be sterilized by microfiltration using membrane filters. This method is commonly used for heat labile pharmaceuticals and protein solutions in medicinal drug processing. A microfilter with pore size 0.2 µm will usually effectively remove microorganisms. Some staphylococcal species have, however, been shown to be flexible enough to pass through 0.22 µm filters. In the processing of biologics, viruses must be removed or inactivated, requiring the use of nanofilters with a smaller pore size (20-50 nm) are used. Smaller pore sizes lower the flow rate, so in order to achieve higher total throughput or to avoid premature blockage, pre-filters might be used to protect small pore membrane filters. Tangential flow filtration (TFF) and alternating tangential flow (ATF) systems also reduce particulate accumulation and blockage.\n\nMembrane filters used in production processes are commonly made from materials such as mixed cellulose ester or polyethersulfone (PES). The filtration equipment and the filters themselves may be purchased as pre-sterilized disposable units in sealed packaging or must be sterilized by the user, generally by autoclaving at a temperature that does not damage the fragile filter membranes. To ensure proper functioning of the filter, the membrane filters are integrity tested post-use and sometimes before use. The nondestructive integrity test assures the filter is undamaged and is a regulatory requirement. Typically, terminal pharmaceutical sterile filtration is performed inside of a cleanroom to prevent contamination.\n\nInstruments that have undergone sterilization can be maintained in such condition by containment in sealed packaging until use.\n\nAseptic technique is the act of maintaining sterility during procedures.\n\n\n", "id": "414144", "title": "Sterilization (microbiology)"}
{"url": "https://en.wikipedia.org/wiki?curid=16963757", "text": "Virucide\n\nA virucide (pronounced /ˈvī-rə-ˌsīd/ and alternatively spelled \"viricide\" and \"viruscide\") is an agent (physical or chemical) that deactivates or destroys viruses. This differs from an antiviral drug, which inhibits the development of the virus.\n\n\n", "id": "16963757", "title": "Virucide"}
{"url": "https://en.wikipedia.org/wiki?curid=216223", "text": "Biocide\n\nA biocide is defined in the European legislation as a chemical substance or microorganism intended to destroy, deter, render harmless, or exert a controlling effect on any harmful organism by chemical or biological means. The US Environmental Protection Agency (EPA) uses a slightly different definition for biocides as \"a diverse group of poisonous substances including preservatives, insecticides, disinfectants, and pesticides used for the control of organisms that are harmful to human or animal health or that cause damage to natural or manufactured products\". When compared, the two definitions roughly imply the same, although the US EPA definition includes plant protection products and some veterinary medicines.\n\nThe terms \"biocides\" and \"pesticides\" are regularly interchanged, and often confused with \"plant protection products\". To clarify this, pesticides include both biocides and plant protection products, where the former regards substances for non-food and feed purposes and the latter regards substances for food and feed purposes.\n\nWhen discussing biocides a distinction should be made between the biocidal active substance and the biocidal product. The biocidal active substances are mostly chemical compounds, but can also be microorganisms (e.g. bacteria). Biocidal products contain one or more biocidal active substances and may contain other non-active co-formulants that ensure the effectiveness as well as the desired pH, viscosity, colour, odour, etc. of the final product. Biocidal products are available on the market for use by professional and/or non-professional consumers.\n\nAlthough most of the biocidal active substances have a relative high toxicity, there are also examples of active substances with low toxicity, such as , which exhibit their biocidal activity only under certain specific conditions such as in closed systems. In such cases, the biocidal product is the combination of the active substance and the device that ensures the intended biocidal activity, i.e. suffocation of rodents by in a closed system trap. Another example of biocidal products available to consumers are products impregnated with biocides (also called treated articles), such as clothes and wristbands impregnated with insecticides, socks impregnated with antibacterial substances etc.\n\nBiocides are commonly used in medicine, agriculture, forestry, and industry. Biocidal substances and products are also employed as anti-fouling agents or disinfectants under other circumstances: chlorine, for example, is used as a short-life biocide in industrial water treatment but as a disinfectant in swimming pools. Many biocides are synthetic, but there are naturally occurring biocides classified as natural biocides, derived from, e.g., bacteria and plants.\n\nA biocide can be:\n\nIn Europe the biocidal products are divided into different product types (PT), based on their intended use. These product types, 22 in total under the BPR, are grouped into four main groups, namely disinfectants, preservatives, pest control, and other biocidal products. For example, the main group \"disinfectants\" contains products to be used for human hygiene (PT 1) and veterinary hygiene (PT 3), main group \"preservatives\" contains wood preservatives (PT 8), the main group \"for pest control\" contains rodenticides (PT 14) and repellents and attractants (PT 19), while the main group \"other biocidal products\" contains antifouling products (PT 21). It should noted that one active substance can be used in several product types, such as for example sulfuryl fluoride, which is approved for use as a wood preservative (PT 8) as well as an insecticide (PT 18).\n\nBiocides can be added to other materials (typically liquids) to protect them against biological infestation and growth. For example, certain types of quaternary ammonium compounds (quats) are added to pool water or industrial water systems to act as an algicide, protecting the water from infestation and growth of algae. It is often impractical to store and use poisonous chlorine gas for water treatment, so alternative methods of adding chlorine are used. These include hypochlorite solutions, which gradually release chlorine into the water, and compounds like sodium dichloro-s-triazinetrione (dihydrate or anhydrous), sometimes referred to as \"dichlor\", and trichloro-s-triazinetrione, sometimes referred to as \"trichlor\". These compounds are stable while solids and may be used in powdered, granular, or tablet form. When added in small amounts to pool water or industrial water systems, the chlorine atoms hydrolyze from the rest of the molecule forming hypochlorous acid (HOCl) which acts as a general biocide killing germs, micro-organisms, algae, and so on. Halogenated hydantoin compounds are also used as biocides.\n\nAn innovation is the use of copper and its alloys (brasses, bronzes, cupronickel, copper-nickel-zinc, and others) as biocidal surfaces to destroy a wide range of microorganisms (\"E. coli\" O157:H7, methicillin-resistant \"Staphylococcus aureus\" (MRSA), \"Staphylococcus\", \"Clostridium difficile\", influenza A virus, adenovirus, and fungi).\nThe United States Environmental Protection Agency has approved the registration of 355 different antimicrobial copper alloys that kill \"E. coli\" O157:H7, methicillin-resistant \"Staphylococcus aureus\" (MRSA), \"Staphylococcus\", \"Enterobacter aerogenes,\" and \"Pseudomonas aeruginosa\" in less than 2 hours of contact. As a public hygienic measure in addition to regular cleaning, antimicrobial copper alloys are being installed in healthcare facilities and in a subway transit system.\n\nBecause biocides are intended to kill living organisms, many biocidal products pose significant risk to human health and welfare. Great care is required when handling biocides and appropriate protective clothing and equipment should be used. The use of biocides can also have significant adverse effects on the natural environment. Anti-fouling paints, especially those utilising organic tin compounds such as TBT, have been shown to have severe and long-lasting impacts on marine eco-systems and such materials are now banned in many countries for commercial and recreational vessels (though sometimes still used for naval vessels). \n\nDisposal of used or unwanted biocides must be undertaken carefully to avoid serious and potentially long-lasting damage to the environment.\n\nThe classification of biocides in the \"Biocidal Products Regulation (EU) 528/2012)(BPR)\" is broken down into 22 product types (i.e. application categories), with several comprising multiple subgroups:\n\nMAIN GROUP 1: Disinfectants and general biocidal products\n\n\nMAIN GROUP 2: Preservatives\n\n\nMAIN GROUP 3: Pest control\n\n\nMAIN GROUP 4: Other biocidal products\n\n\nThe global demand on biocides for use in industrial and consumer goods was estimated at US$6.4 billion in 2008, roughly 3% up from the previous year. Affected by the global economic crisis, the market will remain quite sluggish by 2010. The industry overall is further burdened by ever stricter regulations. The market saw a wave of consolidation in 2008, as producers are looking for measures to control cost and to strengthen market position.\nThe most important application area, in quantitative terms, is industrial and public water treatment.\n\nThe EU regulatory framework for biocides has for years been defined by the Directive 98/8/EC, also known as the Biocidal Products Directive (BPD). The BPD was revoked by the Biocidal Products Regulation 528/2012 (BPR), which entered into force on 17 July 2012 with the application date of September 1, 2013. Several Technical Notes for Guidance (TNsG) have been developed to facilitate the implementation of the BPR and to assure a common understanding of its obligations. According to the EU legislation, biocidal products need authorisation to be placed or to remain on the market. Competent Authorities of the EU member states are responsible for assessing and approving the active substances contained in the biocides. The BPR follows some of the principles set previously under the REACH Regulation (Registration, Evaluation, Authorisation and Restrictions of Chemicals) and the coordination of the risk assessment process for both REACH and BPR are mandated to the European Chemicals Agency (ECHA), which assures the harmonization and integration of risk characterization methodologies between the two regulations.\n\nThe biocides legislation puts emphasis on making the Regulation compatible with the World Trade Organisation (WTO) rules and requirements and with the Global Harmonised System for Classification and Labelling (GHS), as well as with the OECD programme on testing methods. Exchange of information requires the use of the OECD harmonised templates implemented in IUCLID – the International Unified Chemical Information Data System (see ECHA and OECD websites).\n\nMany biocides in the US are regulated under the Federal Pesticide Law (FIFRA) and its subsequent amendments, although some fall under the Federal Food, Drugs and Cosmetic Act, which includes plant protection products (see websites below). In Europe, the plant protection products are placed on the market under another regulatory framework, managed by the European Food Safety Authority (EFSA).\n\nDue to their intrinsic properties and patterns of use, biocides, such as rodenticides or insecticides, can cause adverse effects in humans, animals and the environment and should therefore be used with the utmost care. For example, the anticoagulants used for rodent control have caused toxicity in non-target species, such as predatory birds, due to their long half-life after ingestion by target species (i.e. rats and mice) and high toxicity to non-target species. Pyrethroids used as insecticides have been shown to cause unwanted effects in the environment, due to their unspecific toxic action, also causing toxic effects in non-target aquatic organisms.\n\nIn light of potential adverse effects, and to ensure a harmonised risk assessment and management, the EU regulatory framework for biocides has been established with the objective of ensuring a high level of protection of human and animal health and the environment. To this aim, it is required that risk assessment of biocidal products is carried out before they can be placed on the market. A central element in the risk assessment of the biocidal products are the utilization instructions that defines the dosage, application method and number of applications and thus the exposure of humans and the environment to the biocidal substance.\n\nHumans may be exposed to biocidal products in different ways in both occupational and domestic settings. Many biocidal products are intended for industrial sectors or professional uses only, whereas other biocidal products are commonly available for private use by non-professional users. In addition, potential exposure of non-users of biocidal products (i.e. the general public) may occur indirectly via the environment, for example through drinking water, the food chain, as well as through atmospheric and residential exposure. Particular attention should be paid to the exposure of vulnerable sub-populations, such as the elderly, pregnant women, and children. Also pets and other domestic animals can be exposed indirectly following the application of biocidal products. Furthermore, exposure to biocides may vary in terms of route (inhalation, dermal contact, and ingestion) and pathway (food, drinking water, residential, occupational) of exposure, level, frequency and duration.\n\nThe environment can be exposed directly due to the outdoor use of biocides or as the result of indoor use followed by release to the sewage system after e.g. wet cleaning of a room in which a biocide is used. Upon this release a biocidal substance can pass a sewage treatment plant (STP) and, based on its physical chemical properties, partition to sewage sludge, which in turn can be used for soil amendments thereby releasing the substance into the soil compartment. Alternatively, the substance can remain in the water phase in the STP and subsequently end up in the water compartment such as surface water etc. Risk assessment for the environment focuses on protecting the environmental compartments (air, water and soil) by performing hazard assessments on key species, which represent the food chain within the specific compartment. Of special concern is a well functioning STP, which is elemental in many removal processes. The large variety in biocidal applications leads to complicated exposure scenarios that need to reflect the intended use and possible degradation pathways, in order to perform an accurate risk assessment for the environment. Further areas of concern are endocrine disruption, PBT-properties, secondary poisoning, and mixture toxicity.\n\nBiocidal products are often composed of mixtures of one or more active substances together with co-formulants such as stabilisers, preservatives and colouring agents. Since these substances may act together to produce a combination effect, an assessment of the risk from each of these substances alone may underestimate the real risk from the product as a whole. Several concepts are available for predicting the effect of a mixture on the basis of known toxicities and concentrations of the single components. Approaches for mixture toxicity assessments for regulatory purposes typically advocate assumptions of additive effects;. This means that each substance in the mixture is assumed to contribute to a mixture effect in direct proportion to its concentration and potency. In a strict sense, the assumption is thereby that all substances act by the same mode or mechanism of action. Compared to other available assumptions, this concentration addition model (or dose addition model) can be used with commonly available (eco)toxicity data and effect data together with estimates of e.g. LC50, EC50, PNEC, AEL. Furthermore, assumptions of additive effects from any given mixture are generally considered as a more precautionary approach compared to other available predictive concepts.\n\nThe potential occurrence of synergistic effects presents a special case, and may occur for example when one substance increases the toxicity of another, e.g. if substance A inhibits the detoxification of substance B. Currently, predictive approaches cannot account for this phenomenon. Gaps in our knowledge of the modes of action of substances as well as circumstances under which such effects may occur (e.g. mixture composition, exposure concentrations, species and endpoints) often hamper predictive approaches. Indications that synergistic effects might occur in a product will warrant either a more precautionary approach, or product testing.\n\nAs indicated above, the risk assessment of biocides in EU hinges for a large part by the development of specific emission scenario documents (ESDs) for each product type, which is essential for assessing its exposure of man and the environment. Such ESDs provide detailed scenarios to be used for an initial worse case exposure assessment and for subsequent refinements. ESDs are developed in close collaboration with the OECD Task Force on Biocides and the OECD Exposure Assessment Task Force and are publicly available from websites managed by the Joint Research Centre and OECD (see below). Once ESDs become available they are introduced in the European Union System for the Evaluation of Substances (EUSES), an IT tool supporting the implementation of the risk assessment principles set in the Technical Guidance Document for the Risk Assessment of Biocides (TGD). EUSES enables government authorities, research institutes and chemical companies to carry out rapid and efficient assessments of the general risks posed by substances to man and the environment.\nOnce a biocidal active substance is allowed onto the list of approved active substances, its specifications become a reference source of that active substance (so called 'reference active substance'). Thus, when an alternative source of that active substance appears (e.g. from a company that have not participated in the Review Programme of active substances) or when a change appears in the manufacturing location and/or manufacturing process of a reference active substance, then a technical equivalence between these different sources needs to be established with regard to the chemical composition and hazard profile. This is to check if the level of hazard posed to health and environment by the active substance from the secondary source is comparable to the initial assessed active substance.\n\nIt goes without saying that biocidal products must be used in an appropriate and controlled way. The amount utilized of an active substance should be minimized to that necessary to reach the desired effects thereby reducing the load on the environment and the linked potential adverse effects.\nIn order to define the conditions of use and to ensure that the product fulfils its intended uses, efficacy assessments are carried out as an essential part of the risk assessment. Within the efficacy assessment the target organisms, the effective concentrations, including any thresholds or dependence of the effects on concentrations, the likely concentrations of the active substance used in the products, the mode of action, and the possible occurrence of resistance, cross resistance or tolerance is evaluated. A product cannot be authorized if the desired effect cannot be reached at a dose without posing unacceptable risks to human health or the environment. Appropriate management strategies needs to be taken to avoid the buildup of (cross)resistance. Last but not least, other fundamental elements are the instructions of use, the risk management measures and the risk communication, which is under responsibility of the EU member states.\n\nWhile biocides can have severe effects on human health and/or the environment, their benefits should not be overlooked. To provide some examples, without the above-mentioned rodenticides, crops and food stocks might be seriously affected by rodent activity, or diseases like Leptospirosis might be spread more easily, since rodents can be a vector for diseases. It is difficult to imagine hospitals, food industry premises without using disinfectants or using untreated wood for telephone poles. Another example of benefit is the fuel saving of antifouling substances applied to ships to prevent the buildup of biofilm and subsequent fouling organisms on the hulls which increase the drag during navigation.\n\n\n\n", "id": "216223", "title": "Biocide"}
{"url": "https://en.wikipedia.org/wiki?curid=40735500", "text": "Stylisin\n\nStylisins (1 and 2) are antimicrobial cyclic heptapeptides isolated from marine sponge.\n", "id": "40735500", "title": "Stylisin"}
{"url": "https://en.wikipedia.org/wiki?curid=20082432", "text": "Fluorenol\n\nFluorenol is an alcohol derivative of fluorene. In the most significant isomer, fluoren-9-ol or 9-hydroxyfluorene, the hydroxy group is located on the bridging carbon between the two benzene rings. Hydroxyfluorene can be converted to fluorenone by oxidation. It is a white-cream colored solid at room temperature.\n\nFluorenol is toxic to aquatic organisms including algae, bacteria, and crustaceans. Fluorenol was patented as an insecticide in 1939, and is an algaecide against the green algae \"Dunaliella bioculata\".\n\nIts toxicity and carcinogenicity in humans are unknown.\n\nA study published by chemists working for the biopharmaceutical company Cephalon to develop a successor to the eugeroic modafinil reported that the corresponding fluorenol derivative was 39% more effective than modafinil at keeping mice awake over a 4-hour period. It is a weak dopamine reuptake inhibitor with an IC of 9 μM, notably 59% weaker than modafinil (IC = 3.70 μM) despite being a stronger eugeroic, potentially making it even less liable for addiction. It also showed no affinity for cytochrome P450 2C19, unlike modafinil.\n\n", "id": "20082432", "title": "Fluorenol"}
{"url": "https://en.wikipedia.org/wiki?curid=42985529", "text": "Paenibacterin\n\nPaenibacterin is a lipopeptide antimicrobial agent produced by \"Paenibacillus thiaminolyticus\".\n", "id": "42985529", "title": "Paenibacterin"}
{"url": "https://en.wikipedia.org/wiki?curid=6699055", "text": "Antiprotozoal\n\nAntiprotozoal agents (ATC code: ATC P01) is a class of pharmaceuticals used in treatment of protozoan infection.\n\nProtozoans have little in common with each other (for example, \"Entamoeba histolytica\", an unikont eukaryotic organism, is less closely related to \"Naegleria fowleri\", a bikont eukaryotic organism, than it is to \"Homo sapiens\", which belongs to the unikont phylogenetic group) and so agents effective against one pathogen may not be effective against another.\n\nThey can be grouped by mechanism or by organism. Recent papers have also proposed the use of viruses to treat infections caused by protozoa.\n\nAntiprotozoals are used to treat protozoal infections, which include amebiasis, giardiasis, cryptosporidiosis, microsporidiosis, malaria, babesiosis, trypanosomiasis, Chagas disease, leishmaniasis, and toxoplasmosis. Currently, many of the treatments for these infections are limited by their toxicity.\n\nThe mechanisms of antiprotozoal drugs differ significantly drug to drug. For example, it appears that eflornithine, a drug used to treat trypanosomiasis, inhibits ornithine decarboxylase, while the aminoglycoside antibiotic/antiprotozoals used to treat leishmaniasis are thought to inhibit protein synthesis.\n\n", "id": "6699055", "title": "Antiprotozoal"}
{"url": "https://en.wikipedia.org/wiki?curid=53916", "text": "Herbicide\n\nHerbicide(s), also commonly known as weedkillers, are chemical substances used to control unwanted plants. Selective herbicides control specific weed species, while leaving the desired crop relatively unharmed, while non-selective herbicides (sometimes called \"total weedkillers\" in commercial products) can be used to clear waste ground, industrial and construction sites, railways and railway embankments as they kill all plant material with which they come into contact. Apart from selective/non-selective, other important distinctions include \"persistence\" (also known as \"residual action\": how long the product stays in place and remains active), \"means of uptake\" (whether it is absorbed by above-ground foliage only, through the roots, or by other means), and \"mechanism of action\" (how it works). Historically, products such as common salt and other metal salts were used as herbicides, however these have gradually fallen out of favor and in some countries a number of these are banned due to their persistence in soil, and toxicity and groundwater contamination concerns. Herbicides have also been used in warfare and conflict.\n\nModern herbicides are often synthetic mimics of natural plant hormones which interfere with growth of the target plants. The term organic herbicide has come to mean herbicides intended for organic farming. Some plants also produce their own natural herbicides, such as the genus \"Juglans\" (walnuts), or the tree of heaven; such action of natural herbicides, and other related chemical interactions, is called allelopathy. Due to herbicide resistance - a major concern in agriculture - a number of products combine herbicides with different means of action. Integrated pest management may use herbicides alongside other pest control methods.\n\nIn the US in 2007, about 83% of all herbicide usage, determined by weight applied, was in agriculture. In 2007, world pesticide expenditures totaled about $39.4 billion; herbicides were about 40% of those sales and constituted the biggest portion, followed by insecticides, fungicides, and other types. Smaller quantities are used in forestry, pasture systems, and management of areas set aside as wildlife habitat.\n\nPrior to the widespread use of chemical herbicides, cultural controls, such as altering soil pH, salinity, or fertility levels, were used to control weeds. Mechanical control (including tillage) was also (and still is) used to control weeds.\n\nAlthough research into chemical herbicides began in the early 20th century, the first major breakthrough was the result of research conducted in both the UK and the US during the Second World War into the potential use of herbicides in war. The first modern herbicide, 2,4-D, was first discovered and synthesized by W. G. Templeman at Imperial Chemical Industries. In 1940, he showed that \"Growth substances applied appropriately would kill certain broad-leaved weeds in cereals without harming the crops.\" By 1941, his team succeeded in synthesizing the chemical. In the same year, Pokorny in the US achieved this as well.\n\nIndependently, a team under Juda Hirsch Quastel, working at the Rothamsted Experimental Station made the same discovery. Quastel was tasked by the Agricultural Research Council (ARC) to discover methods for improving crop yield. By analyzing soil as a dynamic system, rather than an inert substance, he was able to apply techniques such as perfusion. Quastel was able to quantify the influence of various plant hormones, inhibitors and other chemicals on the activity of microorganisms in the soil and assess their direct impact on plant growth. While the full work of the unit remained secret, certain discoveries were developed for commercial use after the war, including the 2,4-D compound.\n\nWhen 2,4-D was commercially released in 1946, it triggered a worldwide revolution in agricultural output and became the first successful selective herbicide. It allowed for greatly enhanced weed control in wheat, maize (corn), rice, and similar cereal grass crops, because it kills dicots (broadleaf plants), but not most monocots (grasses). The low cost of 2,4-D has led to continued usage today, and it remains one of the most commonly used herbicides in the world. Like other acid herbicides, current formulations use either an amine salt (often trimethylamine) or one of many esters of the parent compound. These are easier to handle than the acid.\n\nThe triazine family of herbicides, which includes atrazine, were introduced in the 1950s; they have the current distinction of being the herbicide family of greatest concern regarding groundwater contamination. Atrazine does not break down readily (within a few weeks) after being applied to soils of above neutral pH. Under alkaline soil conditions, atrazine may be carried into the soil profile as far as the water table by soil water following rainfall causing the aforementioned contamination. Atrazine is thus said to have \"carryover\", a generally undesirable property for herbicides.\n\nGlyphosate (Roundup) was introduced in 1974 for nonselective weed control. Following the development of glyphosate-resistant crop plants, it is now used very extensively for selective weed control in growing crops. The pairing of the herbicide with the resistant seed contributed to the consolidation of the seed and chemistry industry in the late 1990s.\n\nMany modern chemical herbicides used in agriculture and gardening are specifically formulated to decompose within a short period after application. This is desirable, as it allows crops and plants to be planted afterwards, which could otherwise be affected by the herbicide. However, herbicides with low residual activity (i.e., that decompose quickly) often do not provide season-long weed control and do not ensure that weed roots are killed beneath construction and paving (and cannot emerge destructively in years to come), therefore there remains a role for weedkiller with high levels of persistence in the soil.\n\nHerbicides are classified/grouped in various ways e.g. according to the activity, timing of application, method of application, mechanism of action, chemical family. This gives rise to a considerable level of terminology related to herbicides and their use.\n\n\n\n\n\n\nHerbicides are often classified according to their site of action, because as a general rule, herbicides within the same site of action class will produce similar symptoms on susceptible plants. Classification based on site of action of herbicide is comparatively better as herbicide resistance management can be handled more properly and effectively. Classification by mechanism of action (MOA) indicates the first enzyme, protein, or biochemical step affected in the plant following application.\n\n\nOne of the most important methods for preventing, delaying, or managing resistance is to reduce the reliance on a single herbicide mode of action. To do this, farmers must know the mode of action for the herbicides they intend to use, but the relatively complex nature of plant biochemistry makes this difficult to determine. Attempts were made to simplify the understanding of herbicide mode of action by developing a classification system that grouped herbicides by mode of action. Eventually the Herbicide Resistance Action Committee (HRAC) and the Weed Science Society of America (WSSA) developed a classification system. The WSSA and HRAC systems differ in the group designation. Groups in the WSSA and the HRAC systems are designated by numbers and letters, respectively. The goal for adding the “Group” classification and mode of action to the herbicide product label is to provide a simple and practical approach to deliver the information to users. This information will make it easier to develop educational material that is consistent and effective. It should increase user’s awareness of herbicide mode of action and provide more accurate recommendations for resistance management. Another goal is to make it easier for users to keep records on which herbicide mode of actions are being used on a particular field from year to year.\n\nDetailed investigations on chemical structure of the active ingredients of the registered herbicides showed that some moieties (moiety is a part of a molecule that may include either whole functional groups or parts of functional groups as substructures; a functional group has similar chemical properties whenever it occurs in different compounds) have the same mechanisms of action. According to Forouzesh \"et al\". 2015, these moieties have been assigned to the names of chemical families and active ingredients are then classified within the chemical families accordingly. Knowing about herbicide chemical family grouping could serve as a short-term strategy for managing resistance to site of action.<ref name=\"10.4141/P05-193\"></ref>\n\nMost herbicides are applied as water-based sprays using ground equipment. Ground equipment varies in design, but large areas can be sprayed using self-propelled sprayers equipped with long booms, of with spray nozzles spaced every apart. Towed, handheld, and even horse-drawn sprayers are also used. On large areas, herbicides may also at times be applied aerially using helicopters or airplanes, or through irrigation systems (known as chemigation).\n\nA further method of herbicide application developed around 2010, involves ridding the soil of its active weed seed bank rather than just killing the weed. This can successfully treat annual plants but not perennials. Researchers at the Agricultural Research Service found that the application of herbicides to fields late in the weeds' growing season greatly reduces their seed production, and therefore fewer weeds will return the following season. Because most weeds are annuals, their seeds will only survive in soil for a year or two, so this method will be able to destroy such weeds after a few years of herbicide application.\n\nWeed-wiping may also be used, where a wick wetted with herbicide is suspended from a boom and dragged or rolled across the tops of the taller weed plants. This allows treatment of taller grassland weeds by direct contact without affecting related but desirable shorter plants in the grassland sward beneath. The method has the benefit of avoiding spray drift. In Wales, a scheme offering free weed-wiper hire was launched in 2015 in an effort to reduce the levels of MCPA in water courses.\n\nHerbicide volatilisation or spray drift may result in herbicide affecting neighboring fields or plants, particularly in windy conditions. Sometimes, the wrong field or plants may be sprayed due to error.\n\nHerbicides have widely variable toxicity in addition to acute toxicity from occupational exposure levels.\n\nSome herbicides cause a range of health effects ranging from skin rashes to death. The pathway of attack can arise from intentional or unintentional direct consumption, improper application resulting in the herbicide coming into direct contact with people or wildlife, inhalation of aerial sprays, or food consumption prior to the labeled preharvest interval. Under some conditions, certain herbicides can be transported via leaching or surface runoff to contaminate groundwater or distant surface water sources. Generally, the conditions that promote herbicide transport include intense storm events (particularly shortly after application) and soils with limited capacity to adsorb or retain the herbicides. Herbicide properties that increase likelihood of transport include persistence (resistance to degradation) and high water solubility.\n\nPhenoxy herbicides are often contaminated with dioxins such as TCDD; research has suggested such contamination results in a small rise in cancer risk after occupational exposure to these herbicides. Triazine exposure has been implicated in a likely relationship to increased risk of breast cancer, although a causal relationship remains unclear.\n\nHerbicide manufacturers have at times made false or misleading claims about the safety of their products. Chemical manufacturer Monsanto Company agreed to change its advertising after pressure from New York attorney general Dennis Vacco; Vacco complained about misleading claims that its spray-on glyphosate-based herbicides, including Roundup, were safer than table salt and \"practically non-toxic\" to mammals, birds, and fish (though proof that this was ever said is hard to find). Roundup is toxic and has resulted in death after being ingested in quantities ranging from 85 to 200 ml, although it has also been ingested in quantities as large as 500 ml with only mild or moderate symptoms. The manufacturer of Tordon 101 (Dow AgroSciences, owned by the Dow Chemical Company) has claimed Tordon 101 has no effects on animals and insects, in spite of evidence of strong carcinogenic activity of the active ingredient Picloram in studies on rats.\n\nThe risk of Parkinson's disease has been shown to increase with occupational exposure to herbicides and pesticides. The herbicide paraquat is suspected to be one such factor.\n\nAll commercially sold, organic and nonorganic herbicides must be extensively tested prior to approval for sale and labeling by the Environmental Protection Agency. However, because of the large number of herbicides in use, concern regarding health effects is significant. In addition to health effects caused by herbicides themselves, commercial herbicide mixtures often contain other chemicals, including inactive ingredients, which have negative impacts on human health.\n\nCommercial herbicide use generally has negative impacts on bird populations, although the impacts are highly variable and often require field studies to predict accurately. Laboratory studies have at times overestimated negative impacts on birds due to toxicity, predicting serious problems that were not observed in the field. Most observed effects are due not to toxicity, but to habitat changes and the decreases in abundance of species on which birds rely for food or shelter. Herbicide use in silviculture, used to favor certain types of growth following clearcutting, can cause significant drops in bird populations. Even when herbicides which have low toxicity to birds are used, they decrease the abundance of many types of vegetation on which the birds rely. Herbicide use in agriculture in Britain has been linked to a decline in seed-eating bird species which rely on the weeds killed by the herbicides. Heavy use of herbicides in neotropical agricultural areas has been one of many factors implicated in limiting the usefulness of such agricultural land for wintering migratory birds.\n\nFrog populations may be affected negatively by the use of herbicides as well. While some studies have shown that atrazine may be a teratogen, causing demasculinization in male frogs, the U.S. Environmental Protection Agency (EPA) and its independent Scientific Advisory Panel (SAP) examined all available studies on this topic and concluded that \"atrazine does not adversely affect amphibian gonadal development based on a review of laboratory and field studies.\"\n\nThe health and environmental effects of many herbicides is unknown, and even the scientific community often disagrees on the risk. For example, a 1995 panel of 13 scientists reviewing studies on the carcinogenicity of 2,4-D had divided opinions on the likelihood 2,4-D causes cancer in humans. , studies on phenoxy herbicides were too few to accurately assess the risk of many types of cancer from these herbicides, even though evidence was stronger that exposure to these herbicides is associated with increased risk of soft tissue sarcoma and non-Hodgkin lymphoma. Furthermore, there is some suggestion that herbicides can play a role in sex reversal of certain organisms that experience temperature-dependent sex determination, which could theoretically alter sex ratios.\n\nWeed resistance to herbicides has become a major concern in crop production worldwide.<ref name=\"10.1111/wre.12153\"></ref> Resistance to herbicides is often attributed to lack of rotational programmes of herbicides and to continuous applications of herbicides with the same sites of action. Thus, a true understanding of the sites of action of herbicides is essential for strategic planning of herbicide-based weed control.\n\nPlants have developed resistance to atrazine and to ALS-inhibitors, and more recently, to glyphosate herbicides. Marestail is one weed that has developed glyphosate resistance. Glyphosate-resistant weeds are present in the vast majority of soybean, cotton and corn farms in some U.S. states. Weeds that can resist multiple other herbicides are spreading. Few new herbicides are near commercialization, and none with a molecular mode of action for which there is no resistance. Because most herbicides could not kill all weeds, farmers rotated crops and herbicides to stop resistant weeds. During its initial years, glyphosate was not subject to resistance and allowed farmers to reduce the use of rotation.\n\nA family of weeds that includes waterhemp (Amaranthus rudis) is the largest concern. A 2008-9 survey of 144 populations of waterhemp in 41 Missouri counties revealed glyphosate resistance in 69%. Weeds from some 500 sites throughout Iowa in 2011 and 2012 revealed glyphosate resistance in approximately 64% of waterhemp samples. The use of other killers to target \"residual\" weeds has become common, and may be sufficient to have stopped the spread of resistance From 2005 through 2010 researchers discovered 13 different weed species that had developed resistance to glyphosate. But since then only two more have been discovered. Weeds resistant to multiple herbicides with completely different biological action modes are on the rise. In Missouri, 43% of samples were resistant to two different herbicides; 6% resisted three; and 0.5% resisted four. In Iowa 89% of waterhemp samples resist two or more herbicides, 25% resist three, and 10% resist five.\n\nFor southern cotton, herbicide costs has climbed from between $50 and $75 per hectare a few years ago to about $370 per hectare in 2013. Resistance is contributing to a massive shift away from growing cotton; over the past few years, the area planted with cotton has declined by 70% in Arkansas and by 60% in Tennessee. For soybeans in Illinois, costs have risen from about $25 to $160 per hectare.\n\nDow, Bayer CropScience, Syngenta and Monsanto are all developing seed varieties resistant to herbicides other than glyphosate, which will make it easier for farmers to use alternative weed killers. Even though weeds have already evolved some resistance to those herbicides, Powles says the new seed-and-herbicide combos should work well if used with proper rotation.\n\nResistance to herbicides can be based on one of the following biochemical mechanisms: \n\nWorldwide experience has been that farmers tend to do little to prevent herbicide resistance developing, and only take action when it is a problem on their own farm or neighbor’s. Careful observation is important so that any reduction in herbicide efficacy can be detected. This may indicate evolving resistance. It is vital that resistance is detected at an early stage as if it becomes an acute, whole-farm problem, options are more limited and greater expense is almost inevitable. Table 1 lists factors which enable the risk of resistance to be assessed. An essential pre-requisite for confirmation of resistance is a good diagnostic test. Ideally this should be rapid, accurate, cheap and accessible. Many diagnostic tests have been developed, including glasshouse pot assays, petri dish assays and chlorophyll fluorescence. A key component of such tests is that the response of the suspect population to a herbicide can be compared with that of known susceptible and resistant standards under controlled conditions. Most cases of herbicide resistance are a consequence of the repeated use of herbicides, often in association with crop monoculture and reduced cultivation practices. It is necessary, therefore, to modify these practices in order to prevent or delay the onset of resistance or to control existing resistant populations. A key objective should be the reduction in selection pressure. An integrated weed management (IWM) approach is required, in which as many tactics as possible are used to combat weeds. In this way, less reliance is placed on herbicides and so selection pressure should be reduced.\n\nOptimising herbicide input to the economic threshold level should avoid the unnecessary use of herbicides and reduce selection pressure. Herbicides should be used to their greatest potential by ensuring that the timing, dose, application method, soil and climatic conditions are optimal for good activity. In the UK, partially resistant grass weeds such as \"Alopecurus myosuroides\" (blackgrass) and \"Avena\" spp. (wild oat) can often be controlled adequately when herbicides are applied at the 2-3 leaf stage, whereas later applications at the 2-3 tiller stage can fail badly. Patch spraying, or applying herbicide to only the badly infested areas of fields, is another means of reducing total herbicide use.\n\nTable 1. Agronomic factors influencing the risk of herbicide resistance development\n\nWhen resistance is first suspected or confirmed, the efficacy of alternatives is likely to be the first consideration. The use of alternative herbicides which remain effective on resistant populations can be a successful strategy, at least in the short term. The effectiveness of alternative herbicides will be highly dependent on the extent of cross-resistance. If there is resistance to a single group of herbicides, then the use of herbicides from other groups may provide a simple and effective solution, at least in the short term. For example, many triazine-resistant weeds have been readily controlled by the use of alternative herbicides such as dicamba or glyphosate. If resistance extends to more than one herbicide group, then choices are more limited. It should not be assumed that resistance will automatically extend to all herbicides with the same mode of action, although it is wise to assume this until proved otherwise. In many weeds the degree of cross-resistance between the five groups of ALS inhibitors varies considerably. Much will depend on the resistance mechanisms present, and it should not be assumed that these will necessarily be the same in different populations of the same species. These differences are due, at least in part, to the existence of different mutations conferring target site resistance. Consequently, selection for different mutations may result in different patterns of cross-resistance. Enhanced metabolism can affect even closely related herbicides to differing degrees. For example, populations of \"Alopecurus myosuroides\" (blackgrass) with an enhanced metabolism mechanism show resistance to pendimethalin but not to trifluralin, despite both being dinitroanilines. This is due to differences in the vulnerability of these two herbicides to oxidative metabolism. Consequently, care is needed when trying to predict the efficacy of alternative herbicides.\n\nThe use of two or more herbicides which have differing modes of action can reduce the selection for resistant genotypes. Ideally, each component in a mixture should:\nNo mixture is likely to have all these attributes, but the first two listed are the most important. There is a risk that mixtures will select for resistance to both components in the longer term. One practical advantage of sequences of two herbicides compared with mixtures is that a better appraisal of the efficacy of each herbicide component is possible, provided that sufficient time elapses between each application. A disadvantage with sequences is that two separate applications have to be made and it is possible that the later application will be less effective on weeds surviving the first application. If these are resistant, then the second herbicide in the sequence may increase selection for resistant individuals by killing the susceptible plants which were damaged but not killed by the first application, but allowing the larger, less affected, resistant plants to survive. This has been cited as one reason why ALS-resistant \"Stellaria media\" has evolved in Scotland recently (2000), despite the regular use of a sequence incorporating mecoprop, a herbicide with a different mode of action.\n\nRotation of herbicides from different chemical groups in successive years should reduce selection for resistance. This is a key element in most resistance prevention programmes. The value of this approach depends on the extent of cross-resistance, and whether multiple resistance occurs owing to the presence of several different resistance mechanisms. A practical problem can be the lack of awareness by farmers of the different groups of herbicides that exist. In Australia a scheme has been introduced in which identifying letters are included on the product label as a means of enabling farmers to distinguish products with different modes of action.\n\nHerbicide resistance became a critical problem in Australian agriculture, after many Australian sheep farmers began to exclusively grow wheat in their pastures in the 1970s. Introduced varieties of ryegrass, while good for grazing sheep, compete intensely with wheat. Ryegrasses produce so many seeds that, if left unchecked, they can completely choke a field. Herbicides provided excellent control, while reducing soil disrupting because of less need to plough. Within little more than a decade, ryegrass and other weeds began to develop resistance. In response Australian farmers changed methods. By 1983, patches of ryegrass had become immune to Hoegrass, a family of herbicides that inhibit an enzyme called acetyl coenzyme A carboxylase.\n\nRyegrass populations were large, and had substantial genetic diversity, because farmers had planted many varieties. Ryegrass is cross-pollinated by wind, so genes shuffle frequently. To control its distribution farmers sprayed inexpensive Hoegrass, creating selection pressure. In addition, farmers sometimes diluted the herbicide in order to save money, which allowed some plants to survive application. When resistance appeared farmers turned to a group of herbicides that block acetolactate synthase. Once again, ryegrass in Australia evolved a kind of \"cross-resistance\" that allowed it to rapidly break down a variety of herbicides. Four classes of herbicides become ineffective within a few years. In 2013 only two herbicide classes, called Photosystem II and long-chain fatty acid inhibitors, were effective against ryegrass.\n\n\nRecently, the term \"organic\" has come to imply products used in organic farming. Under this definition, an organic herbicide is one that can be used in a farming enterprise that has been classified as organic. Depending on the application, they may be less effective than synthetic herbicides and are generally used along with cultural and mechanical weed control practices.\n\nHomemade organic herbicides include:\n\n\n\n\n", "id": "53916", "title": "Herbicide"}
{"url": "https://en.wikipedia.org/wiki?curid=30458850", "text": "Diflubenzuron\n\nDiflubenzuron is an insecticide of the benzoylurea class. It is used in forest management and on field crops to selectively control insect pests, particularly forest tent caterpillar moths, boll weevils, gypsy moths, and other types of moths. It is widely used larvicide in India for control of mosquito larvae by public health authorities. Diflubenzuron is approved by the WHO Pesticide Evaluation Scheme.\n\nThe mechanism of action of diflubenzuron involves inhibiting the production of chitin which is used by an insect to build its exoskeleton. It triggers insect larvae to molt early without a properly formed exoskeleton, resulting in the death of the larvae. \n\nDiflubenzuron has been evaluated by the United States Environmental Protection Agency (EPA), and it is classified as non-carcinogenic. 4-Chloroaniline, a metabolite of diflubenzuron which has been classified as a carcinogen, is produced after diflubenzuron has been ingested. The small amount converted to 4-chloroaniline after ingestion is not sufficient to cause cancer.\n\nA commercial preparation containing diflubenzuron is sold under the trade name Adept and is used as an insect growth regulator designed to kill fungus gnat larvae in commercial greenhouses. It is applied to infected soil and will kill fungus gnat larvae for 30-60 days from a single application. Although it is targeted at fungus gnat larvae, care should be taken in applying it as it is highly toxic to most aquatic invertebrates. It has no toxic effects on adult insects, only insect larvae are affected. Diflubenzuron can cause serious foliar injury to plants in the spurge family and certain types of begonia, particularly poinsettias, hibiscus and reiger begonia and should not be applied to these plant varieties. \n\nDiflubenzuron is used as a larvicide in the cattle farming industry. Sold under the name Vigilante, it is formulated as a bolus and is used to control fly populations.\n", "id": "30458850", "title": "Diflubenzuron"}
{"url": "https://en.wikipedia.org/wiki?curid=54247835", "text": "Biosolarization\n\nBiosolarization is an alternative technology to soil fumigation used in agriculture. It is closely related to biofumigation and soil solarization, or the use of solar power to control nematodes, bacteria, fungi and other pests that damage crops. In solarization, the soil is mulched and covered with a tarp to trap solar radiation and heat the soil to a temperature that kills pests. Biosolarization adds the use of organic amendments or compost to the soil before it is covered with plastic, which speeds up the solarization process by decreasing the soil treatment time through increased microbial activity. Research conducted in Spain on the use of biosolarization in strawberry fruit production has shown it to be a sustainable and cost effective option. The practice of biosolarization is being used among small agricultural operations in California. Biosolarization is a growing practice in response to the need for methods for organic soil solarization. The option for more widespread use of biosolarization is being studied by researchers at the Western Center for Agricultural Health and Safety at the University of California at Davis in order to validate the effectiveness of biosolarization in commercial agriculture in California, where it has the potential to greatly reduce the use of conventional fumigants.\n", "id": "54247835", "title": "Biosolarization"}
{"url": "https://en.wikipedia.org/wiki?curid=48340", "text": "Pesticide\n\nPesticides are substances that are meant to control pests (including weeds). The term pesticide includes all of the following: herbicide, insecticides (which may include insect growth regulators, termiticides, \"etc.\") nematicide, molluscicide, piscicide, avicide, rodenticide, bactericide, insect repellent, animal repellent, antimicrobial, fungicide, disinfectant (antimicrobial), and sanitizer. The most common of these are herbicides which account for approximately 80% of all pesticide use. Most pesticides are intended to serve as plant protection products (also known as crop protection products), which in general, protect plants from weeds, fungi, or insects.\n\nIn general, a pesticide is a chemical or biological agent (such as a virus, bacterium, or fungus) that deters, incapacitates, kills, or otherwise discourages pests. Target pests can include insects, plant pathogens, weeds, molluscs, birds, mammals, fish, nematodes (roundworms), and microbes that destroy property, cause nuisance, or spread disease, or are disease vectors. Although pesticides have benefits, some also have drawbacks, such as potential toxicity to humans and other species. \n\nThe Food and Agriculture Organization (FAO) has defined \"pesticide\" as:\n\nPesticides can be classified by target organism (e.g., herbicides, insecticides, fungicides, rodenticides, and pediculicides – see table), chemical structure (e.g., organic, inorganic, synthetic, or biological (biopesticide), although the distinction can sometimes blur), and physical state (e.g. gaseous (fumigant)). Biopesticides include microbial pesticides and biochemical pesticides. Plant-derived pesticides, or \"botanicals\", have been developing quickly. These include the pyrethroids, rotenoids, nicotinoids, and a fourth group that includes strychnine and scilliroside.\n\nMany pesticides can be grouped into chemical families. Prominent insecticide families include organochlorines, organophosphates, and carbamates. Organochlorine hydrocarbons (e.g., DDT) could be separated into dichlorodiphenylethanes, cyclodiene compounds, and other related compounds. They operate by disrupting the sodium/potassium balance of the nerve fiber, forcing the nerve to transmit continuously. Their toxicities vary greatly, but they have been phased out because of their persistence and potential to bioaccumulate. Organophosphate and carbamates largely replaced organochlorines. Both operate through inhibiting the enzyme acetylcholinesterase, allowing acetylcholine to transfer nerve impulses indefinitely and causing a variety of symptoms such as weakness or paralysis. Organophosphates are quite toxic to vertebrates and have in some cases been replaced by less toxic carbamates. Thiocarbamate and dithiocarbamates are subclasses of carbamates. Prominent families of herbicides include phenoxy and benzoic acid herbicides (e.g. 2,4-D), triazines (e.g., atrazine), ureas (e.g., diuron), and Chloroacetanilides (e.g., alachlor). Phenoxy compounds tend to selectively kill broad-leaf weeds rather than grasses. The phenoxy and benzoic acid herbicides function similar to plant growth hormones, and grow cells without normal cell division, crushing the plant's nutrient transport system. Triazines interfere with photosynthesis. Many commonly used pesticides are not included in these families, including glyphosate.\n\nPesticides can be classified based upon their biological mechanism function or application method. Most pesticides work by poisoning pests. A systemic pesticide moves inside a plant following absorption by the plant. With insecticides and most fungicides, this movement is usually upward (through the xylem) and outward. Increased efficiency may be a result. Systemic insecticides, which poison pollen and nectar in the flowers, may kill bees and other needed pollinators.\n\nIn 2009, the development of a new class of fungicides called paldoxins was announced. These work by taking advantage of natural defense chemicals released by plants called phytoalexins, which fungi then detoxify using enzymes. The paldoxins inhibit the fungi's detoxification enzymes. They are believed to be safer and greener.\n\nPesticides are used to control organisms that are considered to be harmful. For example, they are used to kill mosquitoes that can transmit potentially deadly diseases like West Nile virus, yellow fever, and malaria. They can also kill bees, wasps or ants that can cause allergic reactions. Insecticides can protect animals from illnesses that can be caused by parasites such as fleas. Pesticides can prevent sickness in humans that could be caused by moldy food or diseased produce. Herbicides can be used to clear roadside weeds, trees, and brush. They can also kill invasive weeds that may cause environmental damage. Herbicides are commonly applied in ponds and lakes to control algae and plants such as water grasses that can interfere with activities like swimming and fishing and cause the water to look or smell unpleasant. Uncontrolled pests such as termites and mold can damage structures such as houses. Pesticides are used in grocery stores and food storage facilities to manage rodents and insects that infest food such as grain. Each use of a pesticide carries some associated risk. Proper pesticide use decreases these associated risks to a level deemed acceptable by pesticide regulatory agencies such as the United States Environmental Protection Agency (EPA) and the Pest Management Regulatory Agency (PMRA) of Canada.\n\nDDT, sprayed on the walls of houses, is an organochlorine that has been used to fight malaria since the 1950s. Recent policy statements by the World Health Organization have given stronger support to this approach. However, DDT and other organochlorine pesticides have been banned in most countries worldwide because of their persistence in the environment and human toxicity. DDT use is not always effective, as resistance to DDT was identified in Africa as early as 1955, and by 1972 nineteen species of mosquito worldwide were resistant to DDT.\n\nIn 2006 and 2007, the world used approximately of pesticides, with herbicides constituting the biggest part of the world pesticide use at 40%, followed by insecticides (17%) and fungicides (10%). In 2006 and 2007 the U.S. used approximately of pesticides, accounting for 22% of the world total, including of conventional pesticides, which are used in the agricultural sector (80% of conventional pesticide use) as well as the industrial, commercial, governmental and home & garden sectors. Pesticides are also found in majority of U.S. households with 78 million out of the 105.5 million households indicating that they use some form of pesticide. As of 2007, there were more than 1,055 active ingredients registered as pesticides, which yield over 20,000 pesticide products that are marketed in the United States.\n\nThe US used some 1 kg (2.2 pounds) per hectare of arable land compared with: 4.7 kg in China, 1.3 kg in the UK, 0.1 kg in Cameroon, 5.9 kg in Japan and 2.5 kg in Italy. Insecticide use in the US has declined by more than half since 1980 (.6%/yr), mostly due to the near phase-out of organophosphates. In corn fields, the decline was even steeper, due to the switchover to transgenic Bt corn.\n\nFor the global market of crop protection products, market analysts forecast revenues of over 52 billion US$ in 2019.\n\nPesticides can save farmers' money by preventing crop losses to insects and other pests; in the U.S., farmers get an estimated fourfold return on money they spend on pesticides. One study found that not using pesticides reduced crop yields by about 10%. Another study, conducted in 1999, found that a ban on pesticides in the United States may result in a rise of food prices, loss of jobs, and an increase in world hunger.\n\nThere are two levels of benefits for pesticide use, primary and secondary. Primary benefits are direct gains from the use of pesticides and secondary benefits are effects that are more long-term.\n\n\nEvery dollar ($1) that is spent on pesticides for crops yields four dollars ($4) in crops saved. This means based that, on the amount of money spent per year on pesticides, $10 billion, there is an additional $40 billion savings in crop that would be lost due to damage by insects and weeds. In general, farmers benefit from having an increase in crop yield and from being able to grow a variety of crops throughout the year. Consumers of agricultural products also benefit from being able to afford the vast quantities of produce available year-round. The general public also benefits from the use of pesticides for the control of insect-borne diseases and illnesses, such as malaria. The use of pesticides creates a large job market within the agrichemical sector.\n\nOn the cost side of pesticide use there can be costs to the environment, costs to human health, as well as costs of the development and research of new pesticides.\n\nPesticides may cause acute and delayed health effects in people who are exposed. Pesticide exposure can cause a variety of adverse health effects, ranging from simple irritation of the skin and eyes to more severe effects such as affecting the nervous system, mimicking hormones causing reproductive problems, and also causing cancer. A 2007 systematic review found that \"most studies on non-Hodgkin lymphoma and leukemia showed positive associations with pesticide exposure\" and thus concluded that cosmetic use of pesticides should be decreased. There is substantial evidence of associations between organophosphate insecticide exposures and neurobehavioral alterations. Limited evidence also exists for other negative outcomes from pesticide exposure including neurological, birth defects, and fetal death.\n\nThe American Academy of Pediatrics recommends limiting exposure of children to pesticides and using safer alternatives:\n\nOwing to inadequate regulation and safety precautions, 99% of pesticide related deaths occur in developing countries that account for only 25% of pesticide usage.\n\nOne study found pesticide self-poisoning the method of choice in one third of suicides worldwide, and recommended, among other things, more restrictions on the types of pesticides that are most harmful to humans.\n\nA 2014 epidemiological review found associations between autism and exposure to certain pesticides, but noted that the available evidence was insufficient to conclude that the relationship was causal.\n\nThe World Health Organization and the UN Environment Programme estimate that each year, 3 million workers in agriculture in the developing world experience severe poisoning from pesticides, about 18,000 of whom die. According to one study, as many as 25 million workers in developing countries may suffer mild pesticide poisoning yearly. There are several careers aside from agriculture that may also put individuals at risk of health effects from pesticide exposure including pet groomers, groundskeepers, and fumigators.\n\nPesticide use is widespread in Latin America, as around US $3 billion are spend each year in the region. It has been recorded that pesticide poisonings have been increasing each year for the past two decades. It was estimated that 50–80% of the cases are unreported. It is indicated by studies that organophosphate and carbamate insecticides are the most frequent source of pesticide poisoning.\n\nPesticide use raises a number of environmental concerns. Over 98% of sprayed insecticides and 95% of herbicides reach a destination other than their target species, including non-target species, air, water and soil. Pesticide drift occurs when pesticides suspended in the air as particles are carried by wind to other areas, potentially contaminating them. Pesticides are one of the causes of water pollution, and some pesticides are persistent organic pollutants and contribute to soil contamination.\n\nIn addition, pesticide use reduces biodiversity, contributes to pollinator decline, destroys habitat (especially for birds), and threatens endangered species.\nPests can develop a resistance to the pesticide (pesticide resistance), necessitating a new pesticide. Alternatively a greater dose of the pesticide can be used to counteract the resistance, although this will cause a worsening of the ambient pollution problem.\n\nThe Stockholm Convention on Persistent Organic Pollutants, listed 9 of the 12 most dangerous and persistent organic chemicals that were (now mostly obsolete) organochlorine pesticides. Since chlorinated hydrocarbon pesticides dissolve in fats and are not excreted, organisms tend to retain them almost indefinitely. Biological magnification is the process whereby these chlorinated hydrocarbons (pesticides) are more concentrated at each level of the food chain. Among marine animals, pesticide concentrations are higher in carnivorous fishes, and even more so in the fish-eating birds and mammals at the top of the ecological pyramid. Global distillation is the process whereby pesticides are transported from warmer to colder regions of the Earth, in particular the Poles and mountain tops. Pesticides that evaporate into the atmosphere at relatively high temperature can be carried considerable distances (thousands of kilometers) by the wind to an area of lower temperature, where they condense and are carried back to the ground in rain or snow.\n\nIn order to reduce negative impacts, it is desirable that pesticides be degradable or at least quickly deactivated in the environment. Such loss of activity or toxicity of pesticides is due to both innate chemical properties of the compounds and environmental processes or conditions. For example, the presence of halogens within a chemical structure often slows down degradation in an aerobic environment. Adsorption to soil may retard pesticide movement, but also may reduce bioavailability to microbial degraders.\n\nA study on the human health and environmental costs due to pesticides in the United States was estimated at $9.6 billion: offset by about $40 billion in increased agricultural production:\n\nAdditional costs include the registration process and the cost of purchasing pesticides. The registration process can take several years to complete (there are 70 different types of field test) and can cost $50–70 million for a single pesticide. Annually the United States spends $10 billion on pesticides.\n\nAlternatives to pesticides are available and include methods of cultivation, use of biological pest controls (such as pheromones and microbial pesticides), genetic engineering, and methods of interfering with insect breeding. Application of composted yard waste has also been used as a way of controlling pests. These methods are becoming increasingly popular and often are safer than traditional chemical pesticides. In addition, EPA is registering reduced-risk conventional pesticides in increasing numbers.\n\nCultivation practices include polyculture (growing multiple types of plants), crop rotation, planting crops in areas where the pests that damage them do not live, timing planting according to when pests will be least problematic, and use of trap crops that attract pests away from the real crop. Trap crops have successfully controlled pests in some commercial agricultural systems while reducing pesticide usage; however, in many other systems, trap crops can fail to reduce pest densities at a commercial scale, even when the trap crop works in controlled experiments. In the U.S., farmers have had success controlling insects by spraying with hot water at a cost that is about the same as pesticide spraying.\n\nRelease of other organisms that fight the pest is another example of an alternative to pesticide use. These organisms can include natural predators or parasites of the pests. Biological pesticides based on entomopathogenic fungi, bacteria and viruses cause disease in the pest species can also be used.\n\nInterfering with insects' reproduction can be accomplished by sterilizing males of the target species and releasing them, so that they mate with females but do not produce offspring. This technique was first used on the screwworm fly in 1958 and has since been used with the medfly, the tsetse fly, and the gypsy moth. However, this can be a costly, time consuming approach that only works on some types of insects.\n\nAgroecology emphasize nutrient recycling, use of locally available and renewable resources, adaptation to local conditions, utilization of microenvironments, reliance on indigenous knowledge and yield maximization while maintaining soil productivity. Agroecology also emphasizes empowering people and local communities to contribute to development, and encouraging “multi-directional” communications rather than the conventional “top-down” method.\n\nThe term \"push-pull\" was established in 1987 as an approach for integrated pest management (IPM). This strategy uses a mixture of behavior-modifying stimuli to manipulate the distribution and abundance of insects. \"Push\" means the insects are repelled or deterred away from whatever resource that is being protected. \"Pull\" means that certain stimuli (semiochemical stimuli, pheromones, food additives, visual stimuli, genetically altered plants, etc.) are used to attract pests to trap crops where they will be killed. There are numerous different components involved in order to implement a Push-Pull Strategy in IPM.\n\nMany case studies testing the effectiveness of the push-pull approach have been done across the world. The most successful push-pull strategy was developed in Africa for subsistence farming. Another successful case study was performed on the control of \"Helicoverpa\" in cotton crops in Australia. In Europe, the Middle East, and the United States, push-pull strategies were successfully used in the controlling of \"Sitona lineatus\" in bean fields.\n\nSome advantages of using the push-pull method are less use of chemical or biological materials and better protection against insect habituation to this control method. Some disadvantages of the push-pull strategy is that if there is a lack of appropriate knowledge of behavioral and chemical ecology of the host-pest interactions then this method becomes unreliable. Furthermore, because the push-pull method is not a very popular method of IPM operational and registration costs are higher.\n\nSome evidence shows that alternatives to pesticides can be equally effective as the use of chemicals. For example, Sweden has halved its use of pesticides with hardly any reduction in crops. In Indonesia, farmers have reduced pesticide use on rice fields by 65% and experienced a 15% crop increase. A study of Maize fields in northern Florida found that the application of composted yard waste with high carbon to nitrogen ratio to agricultural fields was highly effective at reducing the population of plant-parasitic nematodes and increasing crop yield, with yield increases ranging from 10% to 212%; the observed effects were long-term, often not appearing until the third season of the study.\n\nHowever, pesticide resistance is increasing. In the 1940s, U.S. farmers lost only 7% of their crops to pests. Since the 1980s, loss has increased to 13%, even though more pesticides are being used. Between 500 and 1,000 insect and weed species have developed pesticide resistance since 1945.\n\nPesticides are often referred to according to the type of pest they control. Pesticides can also be considered as either biodegradable pesticides, which will be broken down by microbes and other living beings into harmless compounds, or persistent pesticides, which may take months or years before they are broken down: it was the persistence of DDT, for example, which led to its accumulation in the food chain and its killing of birds of prey at the top of the food chain. Another way to think about pesticides is to consider those that are chemical pesticides are derived from a common source or production method.\n\nSome examples of chemically-related pesticides are:\n\nNeonicotinoids are a class of neuro-active insecticides chemically similar to nicotine. Imidacloprid, of the neonicotanoid family, is the most widely used insecticide in the world. In the late 1990s neonicotinoids came under increasing scrutiny over their environmental impact and were linked in a range of studies to adverse ecological effects, including honey-bee colony collapse disorder (CCD) and loss of birds due to a reduction in insect populations. In 2013, the European Union and a few non EU countries restricted the use of certain neonicotinoids.\n\nOrganophosphates affect the nervous system by disrupting acetylcholinesterase activity, the enzyme that regulates acetylcholine, a neurotransmitter. Most organophosphates are insecticides. They were developed during the early 19th century, but their effects on insects, which are similar to their effects on humans, were discovered in 1932. Some are very poisonous. However, they usually are not persistent in the environment.\n\nCarbamate pesticides affect the nervous system by disrupting an enzyme that regulates acetylcholine, a neurotransmitter. The enzyme effects are usually reversible. There are several subgroups within the carbamates.\n\nThey were commonly used in the past, but many have been removed from the market due to their health and environmental effects and their persistence (e.g., DDT, chlordane, and toxaphene).\n\nThey were developed as a synthetic version of the naturally occurring pesticide pyrethrin, which is found in chrysanthemums. They have been modified to increase their stability in the environment. Some synthetic pyrethroids are toxic to the nervous system.\n\nThe following sulfonylureas have been commercialized for weed control: amidosulfuron, azimsulfuron, bensulfuron-methyl, chlorimuron-ethyl, ethoxysulfuron, flazasulfuron, flupyrsulfuron-methyl-sodium, halosulfuron-methyl, imazosulfuron, nicosulfuron, oxasulfuron, primisulfuron-methyl, pyrazosulfuron-ethyl, rimsulfuron, sulfometuron-methyl\nSulfosulfuron, terbacil, bispyribac-sodium, cyclosulfamuron, and pyrithiobac-sodium. Nicosulfuron, triflusulfuron methyl, and chlorsulfuron are broad-spectrum herbicides that kill plants weeds or pests by inhibiting the enzyme acetolactate synthase. In the 1960s, more than crop protection chemical was typically applied, while sulfonylureates allow as little as 1% as much material to achieve the same effect.\n\nBiopesticides are certain types of pesticides derived from such natural materials as animals, plants, bacteria, and certain minerals. For example, canola oil and baking soda have pesticidal applications and are considered biopesticides. Biopesticides fall into three major classes:\n\n\nPesticides that are related to the type of pests are:\nThe term pesticide also include these substances:\n\nDefoliants: Cause leaves or other foliage to drop from a plant, usually to facilitate harvest.\nDesiccants: Promote drying of living tissues, such as unwanted plant tops.\nInsect growth regulators: Disrupt the molting, maturity from pupal stage to adult, or other life processes of insects. \nPlant growth regulators: Substances (excluding fertilizers or other plant nutrients) that alter the expected growth, flowering, or reproduction rate of plants.\nWood preservatives: They are used to make wood resistant to insects, fungus, and other pests.\n\nIn most countries, pesticides must be approved for sale and use by a government agency.\n\nIn Europe, recent EU legislation has been approved banning the use of highly toxic pesticides including those that are carcinogenic, mutagenic or toxic to reproduction, those that are endocrine-disrupting, and those that are persistent, bioaccumulative and toxic (PBT) or very persistent and very bioaccumulative (vPvB). Measures were approved to improve the general safety of pesticides across all EU member states.\n\nThough pesticide regulations differ from country to country, pesticides, and products on which they were used are traded across international borders. To deal with inconsistencies in regulations among countries, delegates to a conference of the United Nations Food and Agriculture Organization adopted an International Code of Conduct on the Distribution and Use of Pesticides in 1985 to create voluntary standards of pesticide regulation for different countries. The Code was updated in 1998 and 2002. The FAO claims that the code has raised awareness about pesticide hazards and decreased the number of countries without restrictions on pesticide use.\n\nThree other efforts to improve regulation of international pesticide trade are the United Nations London Guidelines for the Exchange of Information on Chemicals in International Trade and the United Nations Codex Alimentarius Commission. The former seeks to implement procedures for ensuring that prior informed consent exists between countries buying and selling pesticides, while the latter seeks to create uniform standards for maximum levels of pesticide residues among participating countries. Both initiatives operate on a voluntary basis.\n\nPesticides safety education and pesticide applicator regulation are designed to protect the public from pesticide misuse, but do not eliminate all misuse. Reducing the use of pesticides and choosing less toxic pesticides may reduce risks placed on society and the environment from pesticide use. Integrated pest management, the use of multiple approaches to control pests, is becoming widespread and has been used with success in countries such as Indonesia, China, Bangladesh, the U.S., Australia, and Mexico. IPM attempts to recognize the more widespread impacts of an action on an ecosystem, so that natural balances are not upset. New pesticides are being developed, including biological and botanical derivatives and alternatives that are thought to reduce health and environmental risks. In addition, applicators are being encouraged to consider alternative controls and adopt methods that reduce the use of chemical pesticides.\n\nPesticides can be created that are targeted to a specific pest's lifecycle, which can be environmentally more friendly. For example, potato cyst nematodes emerge from their protective cysts in response to a chemical excreted by potatoes; they feed on the potatoes and damage the crop. A similar chemical can be applied to fields early, before the potatoes are planted, causing the nematodes to emerge early and starve in the absence of potatoes.\n\nIn the United States, the Environmental Protection Agency (EPA) is responsible for regulating pesticides under the Federal Insecticide, Fungicide, and Rodenticide Act (FIFRA) and the Food Quality Protection Act (FQPA)\n\nStudies must be conducted to establish the conditions in which the material is safe to use and the effectiveness against the intended pest(s). The EPA regulates pesticides to ensure that these products do not pose adverse effects to humans or the environment. Pesticides produced before November 1984 continue to be reassessed in order to meet the current scientific and regulatory standards. All registered pesticides are reviewed every 15 years to ensure they meet the proper standards. During the registration process, a label is created. The label contains directions for proper use of the material in addition to safety restrictions. Based on acute toxicity, pesticides are assigned to a Toxicity Class.\n\nSome pesticides are considered too hazardous for sale to the general public and are designated restricted use pesticides. Only certified applicators, who have passed an exam, may purchase or supervise the application of restricted use pesticides. Records of sales and use are required to be maintained and may be audited by government agencies charged with the enforcement of pesticide regulations. These records must be made available to employees and state or territorial environmental regulatory agencies.\n\nThe EPA regulates pesticides under two main acts, both of which amended by the Food Quality Protection Act of 1996. In addition to the EPA, the United States Department of Agriculture (USDA) and the United States Food and Drug Administration (FDA) set standards for the level of pesticide residue that is allowed on or in crops. The EPA looks at what the potential human health and environmental effects might be associated with the use of the pesticide.\n\nIn addition, the U.S. EPA uses the National Research Council's four-step process for human health risk assessment: (1) Hazard Identification, (2) Dose-Response Assessment, (3) Exposure Assessment, and (4) Risk Characterization.\n\nRecently Kaua'i County (Hawai'i) passed Bill No. 2491 to add an article to Chapter 22 of the county's code relating to pesticides and GMOs. The bill strengthens protections of local communities in Kaua'i where many large pesticide companies test their products.\n\nSince before 2000 BC, humans have utilized pesticides to protect their crops. The first known pesticide was elemental sulfur dusting used in ancient Sumer about 4,500 years ago in ancient Mesopotamia. The Rig Veda, which is about 4,000 years old, mentions the use of poisonous plants for pest control. By the 15th century, toxic chemicals such as arsenic, mercury, and lead were being applied to crops to kill pests. In the 17th century, nicotine sulfate was extracted from tobacco leaves for use as an insecticide. The 19th century saw the introduction of two more natural pesticides, pyrethrum, which is derived from chrysanthemums, and rotenone, which is derived from the roots of tropical vegetables. Until the 1950s, arsenic-based pesticides were dominant. Paul Müller discovered that DDT was a very effective insecticide. Organochlorines such as DDT were dominant, but they were replaced in the U.S. by organophosphates and carbamates by 1975. Since then, pyrethrin compounds have become the dominant insecticide. Herbicides became common in the 1960s, led by \"triazine and other nitrogen-based compounds, carboxylic acids such as 2,4-dichlorophenoxyacetic acid, and glyphosate\".\n\nThe first legislation providing federal authority for regulating pesticides was enacted in 1910; however, decades later during the 1940s manufacturers began to produce large amounts of synthetic pesticides and their use became widespread. Some sources consider the 1940s and 1950s to have been the start of the \"pesticide era.\" Although the U.S. Environmental Protection Agency was established in 1970 and amendments to the pesticide law in 1972, pesticide use has increased 50-fold since 1950 and 2.3 million tonnes (2.5 million short tons) of industrial pesticides are now used each year. Seventy-five percent of all pesticides in the world are used in developed countries, but use in developing countries is increasing. A study of USA pesticide use trends through 1997 was published in 2003 by the National Science Foundation's Center for Integrated Pest Management.\n\nIn the 1960s, it was discovered that DDT was preventing many fish-eating birds from reproducing, which was a serious threat to biodiversity. Rachel Carson wrote the best-selling book \"Silent Spring\" about biological magnification. The agricultural use of DDT is now banned under the Stockholm Convention on Persistent Organic Pollutants, but it is still used in some developing nations to prevent malaria and other tropical diseases by spraying on interior walls to kill or repel mosquitoes.\n\n\n\n\n\n\n\n", "id": "48340", "title": "Pesticide"}
{"url": "https://en.wikipedia.org/wiki?curid=2095134", "text": "Oligodynamic effect\n\nThe oligodynamic effect (from Greek oligos \"few\", and dynamis \"force\") is a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations. The effect was discovered by Karl Wilhelm von Nägeli, although he did not identify the cause. Brass doorknobs and silverware both exhibit this effect to an extent.\n\nThe metals react with thiol (-SH) or amine (-NH) groups of proteins, a mode of action to which microorganisms may develop resistance. Such resistance may be transmitted by plasmids.\n\nAluminium acetate (Burow's solution) is used as an astringent mild antiseptic. Aluminium-based antiperspirant ingredients (\"aluminium salts\") such as aluminium chlorohydrate, activated aluminium chlorohydrates, and aluminium-zirconium-glycine (AZG) complexes work by forming superficial plugs in the sweat ducts, reducing the flow of perspiration.\n\nOrthoesters of diarylstibinic acids are fungicides and bactericides, used in paints, plastics, and fibers. Trivalent organic antimony was used in therapy for schistosomiasis.\n\nFor many decades, arsenic was used medicinally to treat syphilis. It is still used in sheep dips, rat poisons, wood preservatives, weed killers, and other pesticides. Arsenic is also still used for murder by poisoning, for which use it has a long and continuing history in both literature and fact.\n\nBarium polysulfide is a fungicide and acaricide used in fruit and grape growing.\n\nBismuth compounds have been used because of their astringent, antiphlogistic, bacteriostatic, and disinfecting actions. In dermatology bismuth subgallate is still used in vulnerary salves and powders as well as in antimycotics. In the past, bismuth has also been used to treat syphilis and malaria.\n\nBoric acid esters derived from glycols (example, organo-borate formulation, \"Biobor JF\") are being used for the control of microorganisms in fuel systems containing water.\n\nBrass vessels release a small amount of copper ions into stored water, thus killing fecal bacterial counts as high as 1 million bacteria per milliliter.\n\nCopper sulfate mixed with lime is used as a fungicide and antihelminthic. Copper sulfate is used chiefly to destroy green algae (algicide) that grow in reservoirs, stock ponds, swimming pools, and fish tanks. Copper 8-hydroxyquinoline is sometimes included in paint to prevent mildew.\n\nPaint containing copper is used on boat bottoms to prevent barnacle growth\n\nGold is used in dental inlays and inhibits the growth of bacteria.\n\nPhysicians prescribed various forms of lead to heal ailments ranging from constipation to infectious diseases such as the plague. Lead was also used to preserve or sweeten wine. Lead arsenate is used in insecticides and herbicides. Some organic lead compounds are used as industrial biocides: thiomethyl triphenyllead is used as an antifungal agent, cotton preservative, and lubricant additive; thiopropyl triphenyllead as a rodent repellant; tributyllead acetate as a wood and cotton preservative; tributyllead imidazole as a lubricant additive and cotton preservative.\n\nPhenylmercuric borate and acetate were used for disinfecting mucous membranes at an effective concentration of 0.07% in aqueous solutions. Due to toxicological and ecotoxicological reasons phenylmercury salts are no longer in use. However, some surgeons use mercurochrome despite toxicological objections. Dental amalgam used in fillings inhibits bacterial reproduction.\n\nOrganic mercury compounds have been used as topical disinfectants (thimerosal, nitromersol and merbromin) and preservatives in medical preparations (thimerosal) and grain products (both methyl and ethyl mercurials). Mercury was used in the treatment of syphilis. Calomel was commonly used in infant teething powders in the 1930s and 1940s. Mercurials are also used agriculturally as insecticides and fungicides.\n\nThe toxicity of nickel to bacteria, yeasts, and fungi differs considerably.\n\nThe metabolism of bacteria is adversely affected by silver ions at concentrations of 0.01–0.1 mg/L. Therefore, even less soluble silver compounds, such as silver chloride, also act as bactericides or germicides, but not the much less soluble silver sulfide. In the presence of atmospheric oxygen, metallic silver also has a bactericidal effect due to the formation of silver oxide, which is soluble enough to cause it. Bactericidal concentrations are reduced rapidly by adding colloidal silver, which has a high surface area. Even objects with a solid silver surface (e.g., table silver, silver coins, or silver foil) have a bactericidal effect. Silver drinking vessels were carried by military commanders on expeditions for protection against disease. It was once common to place silver foil or even silver coins on wounds for the same reason.\n\nSilver sulfadiazine is used as an antiseptic ointment for extensive burns. An equilibrium dispersion of colloidal silver with dissolved silver ions can be used to purify drinking water at sea. Silver is incorporated into medical implants and devices such as catheters. Surfacine (silver iodide) is a relatively new antimicrobial for application to surfaces. Silver-impregnated wound dressings have proven especially useful against antibiotic-resistant bacteria. Silver nitrate is used as a hemostatic, antiseptic and astringent. At one time, many states required that the eyes of newborns be treated with a few drops of silver nitrate to guard against an infection of the eyes called gonorrheal neonatal ophthalmia, which the infants might have contracted as they passed through the birth canal. Silver ions are increasingly incorporated into many hard surfaces, such as plastics and steel, as a way to control microbial growth on items such as toilet seats, stethoscopes, and even refrigerator doors. Among the newer products being sold are plastic food containers infused with silver nanoparticies, which are intended to keep food fresher, and silver-infused athletic shirts and socks, which claim to minimize odors.\n\nThallium compounds such as thallium sulfate have been used for impregnating wood and leather to kill fungal spores and bacteria, and for the protection of textiles from attack by moths. Thallium sulfate has been used as a depilatory and in the treatment of venereal disease, skin fungal infections, and tuberculosis.\n\nTetrabutyltin is used as an antifouling paint for ships, for the prevention of slimes in industrial recirculating water systems, for combating freshwater snails that cause bilharzia, as a wood and textile preservative, and as a disinfectant. Tricyclohexyltin hydroxide is used as an acaricide. Triphenyltin hydroxide and triphenyltin acetate are used as fungicides.\n\nZinc oxide is used as a weak antiseptic (and sunscreen), and in paints as a white pigment and mold-growth inhibitor. Zinc chloride is a common ingredient in mouthwashes and deodorants, and zinc pyrithione is an ingredient in antidandruff shampoos. Galvanized (zinc-coated) fittings on roofs impede the growth of algae. Copper- and zinc-treated shingles are available. Zinc iodide and zinc sulfate are used as topical antiseptics.\n\nBesides the individual toxic effects of each metal, a wide range of metals are nephrotoxic in humans and/or in animals. Some metals and their compounds are carcinogenic to humans. A few metals, such as lead and mercury, can cross the placental barrier and impact fetal development. Several (cadmium, zinc, copper, and mercury) can induce special protein complexes called metallothioneins.\n\n", "id": "2095134", "title": "Oligodynamic effect"}
{"url": "https://en.wikipedia.org/wiki?curid=2639400", "text": "Blue ice (aviation)\n\nBlue ice, in the context of aviation, is frozen sewage material that has leaked mid-flight from commercial aircraft lavatory waste systems. It is a mixture of human biowaste and liquid disinfectant that freezes at high altitude. The name comes from the blue color of the disinfectant. Airlines are not allowed to dump their waste tanks in mid-flight, and pilots have no mechanism by which to do so; however, leaks sometimes do occur. \nThere were at least 27 documented incidents of blue ice impacts in the United States between 1979 and 2003. These incidents typically happen under airport landing paths as the mass warms sufficiently to detach from the plane during its descent. A rare incident of falling blue ice causing damage to the roof of a home was reported on October 20, 2006 in Chino, California. A similar incident was reported in Leicester, UK, in 2007.\n\nIn 1971, a chunk of ice from an aircraft tore a large hole in the roof of the Essex Street Chapel in Kensington, London, and was one trigger for the demolition of the building.\n\nIn November 2011 a chunk of ice, the size of an orange, broke through the roof of a private house in Ratingen-Hösel, Germany.\n\nIn February 2013 a \"football sized\" ball of blue ice smashed through a conservatory roof in Clanfield, Hampshire causing around £10,000 worth of damage.\n\nIn October 2016 a chunk of ice tore a hole in a private house in Amstelveen, The Netherlands.\n\nBlue ice can also be dangerous to the aircraft itself; the National Transportation Safety Board has recorded three very similar incidents where waste from lavatories caused damage to the leaking aircraft. All involved Boeing 727s, and in all cases waste from a leaking lavatory hit one of the engines, mounted at the rear of the aircraft, causing a power loss. The flights made safe emergency landings with the two remaining engines. Nobody was injured.\nOnly one report specifically mentions ice, while another mentions \"soft body FOD\" (foreign object damage), indicating that the damage was caused by a relative soft object like ice or a bird and not a metallic object or a stone.\n\nBlue ice became known to many people from the last 2003 episode of the HBO series \"Six Feet Under\", in which a foot-sized chunk drops on a bystander. A similar incident occurs in the 1996 television series \"Early Edition\" episode “Frostbite” when the main character saves a man from being crushed by a chunk of blue ice. It was also mentioned in \"The Big Bang Theory\". This also happened in an episode of \"\". The title of the 1992 film \"Blue Ice\" is a reference to the phenomenon. The 2001 film \"Joe Dirt\" finds the title character (played by David Spade) proudly displaying a large chunk of \"blue ice\" which he has mistaken for a meteorite, and the topic has also been covered on the TV show \"MANswers\". Blue ice was also featured in an episode of the television series \"MythBusters\". Blue ice is a cause of death in season 4 of \"1000 Ways to Die\".\n\n", "id": "2639400", "title": "Blue ice (aviation)"}
{"url": "https://en.wikipedia.org/wiki?curid=18626487", "text": "Biomedical waste\n\nBiomedical waste is any kind of waste containing infectious (or potentially infectious) materials. It may also include waste associated with the generation of biomedical waste that visually appears to be of medical or laboratory origin (e.g., packaging, unused bandages, infusion kits, etc.), as well research laboratory waste containing biomolecules or organisms that are restricted from environmental release. As detailed below, discarded sharps are considered biomedical waste whether they are contaminated or not, due to the possibility of being contaminated with blood and their propensity to cause injury when not properly contained and disposed of. Biomedical waste is a type of biowaste.\n\nBiomedical waste may be solid or liquid. Examples of infectious waste include discarded blood, sharps, unwanted microbiological cultures and stocks, identifiable body parts (including those as a result of amputation), other human or animal tissue, used bandages and dressings, discarded gloves, other medical supplies that may have been in contact with blood and body fluids, and laboratory waste that exhibits the characteristics described above. Waste sharps include potentially contaminated used (and unused discarded) needles, scalpels, lancets and other devices capable of penetrating skin.\n\nBiomedical waste is generated from biological and medical sources and activities, such as the diagnosis, prevention, or treatment of diseases. Common generators (or producers) of biomedical waste include hospitals, health clinics, nursing homes, emergency medical services, medical research laboratories, offices of physicians, dentists, and veterinarians, home health care, and morgues or funeral homes. In healthcare facilities (i.e., hospitals, clinics, doctor's offices, veterinary hospitals and clinical laboratories), waste with these characteristics may alternatively be called medical or clinical waste.\n\nBiomedical waste is distinct from normal trash or general waste, and differs from other types of hazardous waste, such as chemical, radioactive, universal or industrial waste. Medical facilities generate waste hazardous chemicals and radioactive materials. While such wastes are normally not infectious, they require proper disposal. Some wastes are considered \"multihazardous,\" such as tissue samples preserved in formalities.\n\nDisposal of this waste is an environmental concern, as many medical wastes are classified as \"infectious\" or \"biohazardous\" and could potentially lead to the spread of infectious disease. The most common danger for humans is the infection which also affects other living organisms in the region. Daily exposure to the waste (landfill) leads to accumulation of harmful substances or microbes in the person's body.\n\nA 1990 report by the U.S. Agency for Toxic Substances and Disease Registry concluded that the general public is not likely to be adversely affected by biomedical waste generated in the traditional healthcare setting. They found, however, that biomedical waste from those settings may pose an injury and exposure risks via occupational contact with medical waste for doctors, nurses, and janitorial, laundry and refuse workers. Further, there are opportunities for the general public to come into contact medical waste, such as needles used illicitly outside healthcare settings, or biomedical waste generated via home health care.\n\nBiomedical waste must be properly managed and disposed of to protect the environment, general public and workers, especially healthcare and sanitation workers who are at risk of exposure to biomedical waste as an occupational hazard. Steps in the management of biomedical waste include generation, accumulation, handling, storage, treatment, transport and disposal.\n\nDisposal occurs off-site, at a location that is different from the site of generation. Treatment may occur on-site or off-site. On-site treatment of large quantities of biomedical waste usually requires the use of relatively expensive equipment, and is generally only cost effective for very large hospitals and major universities who have the space, labor and budget to operate such equipment. Off-site treatment and disposal involves hiring of a biomedical waste disposal service (also called a truck service) whose employees are trained to collect and haul away biomedical waste in special containers (usually cardboard boxes, or reusable plastic bins) for treatment at a facility designed to handle biomedical waste.\n\nBiomedical waste should be collected in containers that are leak-proof and sufficiently strong to prevent breakage during handling. Containers of biomedical waste are marked with a biohazard symbol. The container, marking, and labels are often red.\n\nDiscarded sharps are usually collected in specialized boxes, often called \"needle boxes\".\n\nSpecialized equipment is required to meet OSHA 29 CFR 1910.1450 and EPA 40 CFR 264.173. standards of safety. Minimal recommended equipment include a fume hood and primary and secondary waste containers to capture potential overflow. Even beneath the fume hood, containers containing chemical contaminants should remain closed when not in use. An open funnel placed in the mouth of a waste container has been shown to allow significant evaporation of chemicals into the surrounding atmosphere, which is then inhaled by laboratory personnel, and contributes a primary component to the threat of completing the fire triangle. To protect the health and safety of laboratory staff as well as neighboring civilians and the environment, proper waste management equipment, such as the Burkle funnel in Europe and the ECO Funnel in the U.S., should be utilized in any department which deals with chemical waste. It is to be dumped after treatment.\n\nStorage refers to keeping the waste until it is treated on-site or transported off-site for treatment or disposal. There are many options and containers for storage. Regulatory agencies may limit the time waste can remain in storage. Handling is the act of moving biomedical waste between the point of generation, accumulation areas, storage locations and on-site treatment facilities. Workers who handle biomedical waste must observe \"standard precautions.\"\n\nThe goals of biomedical waste treatment are to reduce or eliminate the waste's hazards, and usually to make the waste unrecognizable. Treatment should render the waste safe for subsequent handling and disposal. There are several treatment methods that can accomplish these goals.\n\nBiomedical waste is often incinerated. An efficient incinerator will destroy pathogens and sharps. Source materials are not recognizable in the resulting ash.\n\nAn autoclave may also be used to treat biomedical waste. An autoclave uses steam and pressure to sterilize the waste or reduce its microbiological load to a level at which it may be safely disposed of. Many healthcare facilities routinely use an autoclave to sterilize medical supplies. If the same autoclave is used to sterilize supplies and treat biomedical waste, administrative controls must be used to prevent the waste operations from contaminating the supplies. Effective administrative controls include operator training, strict procedures, and separate times and space for processing biomedical waste.\n\nFor liquids and small quantities, a 1–10% solution of bleach can be used to disinfect biomedical waste. Solutions of sodium hydroxide and other chemical disinfectants may also be used, depending on the waste's characteristics. Other treatment methods include heat, alkaline digesters and the use of microwaves.\n\nFor autoclaves and microwave systems, a shredder may be used as a final treatment step to render the waste unrecognizable.\n\nIn the UK, clinical waste and the way it is to be handled is closely regulated. Applicable legislation includes the Environmental Protection Act 1990 (Part II), Waste Management Licensing Regulations 1994, and the Hazardous Waste Regulations (England & Wales) 2005, as well as the Special Waste Regulations in Scotland.\n\nIn the United States, biomedical waste is usually regulated as medical waste. In 1988 the U.S. federal government passed The Medical Waste Tracking Act which set the standards for governmental regulation of medical waste. After the Act expired in 1991, States were given the responsibility to regulate and pass laws concerning the disposal of medical waste. All fifty states vary in their regulations from no regulations to very strict.\n\nIn addition to on-site treatment or pickup by a biomedical waste disposal firm for off-site treatment, a mail-back disposal option exists in the United States. In mail-back biomedical waste disposal, the waste is shipped through the U.S. postal service instead of transport by private hauler. While currently available in all 50 U.S. states, mail-back medical waste disposal is limited to very strict postal regulations (i.e., collection and shipping containers must be approved by the postal service for use) and only available by a handful of companies.\n\nThe Bio-medical Waste (Management and Handling) Rules, 1998 and further amendments were passed for the regulation of bio-medical waste management. On 28 th Mar 2016 Biomedical Waste Management Rules 2016 were also notified by Central Govt. Each state's Pollution Control Board or Pollution control Committee will be responsible for implementing the new legislation.\n\nIn India,though there are a number of different disposal methods,the situation is desultory and most are harmful rather than helpful. If body fluids are present, the material needs to be incinerated or put into an autoclave. Although this is the proper method, most medical facilities fail to follow the regulations. It is often found that biomedical waste is dumped into the ocean, where it eventually washes up on shore, or in landfills due to improper sorting or negligence when in the medical facility. Improper disposal can lead to many diseases in animals as well as humans. For example, animals, such as cows in Pondicherry, India, are consuming the infected waste and eventually, these infections can be transported to humans who consume their meat or milk. Large number of unregistered clinics and institutions also generate bio-medical waste which is not controlled. \n\nDue to the competition to improve quality and so as to get accreditation from agencies like ISO, NABH, JCI, many private organizations have initiated proper bio-medical waste disposal but still the gap is huge.<br>\n\nMany studies took place in Gujarat, India regarding the knowledge of workers in facilities such as hospitals, nursing homes, or home health. It was found that 26% of doctors and 43% of paramedical staff were unaware of the risks related to biomedical wastes. After extensively looking at the different facilities, many were undeveloped in the area regarding biomedical waste. The rules and regulations in India work with The Bio-medical Waste (Management and Handling) Rules from 1998, yet a large number of health care facilities were found to be sorting the waste incorrectly. \n\nThe latest guidelines for segregation of bio-medical waste recommend the following color coding - <br>\nRed Bag - Syringes (without needles), soiled gloves, catheters, IV tubes etc should be all disposed of in a red colored bag, which will later be incinerated.<br>\nYellow Bag - All dressings, bandages and cotton swabs with body fluids, blood bags, human anatomical waste, body parts are to be discarded in yellow bags.<br>\nCarboard box with blue marking - Glass vials, ampules, other glass ware is to be discarded in a cardboard box with a blue marking/sticker. <br>\nWhite Puncture Proof Container (PPC) - Needles, sharps, blades are disposed of in a white translucent puncture proof container.<br>\nBlack Bags - These are to be used for non-bio-medical waste. In a hospital setup, this includes stationary, vegetable and fruit peels, leftovers, packaging including that from medicines, disposable caps, disposable masks, disposable shoe-covers, disposable tea cups, cartons, sweeping dust, kitchen waste etc.\n\n", "id": "18626487", "title": "Biomedical waste"}
{"url": "https://en.wikipedia.org/wiki?curid=11979055", "text": "Banana peel\n\nA banana peel, also called banana skin in British English, is the outer covering of the banana fruit. Banana peels are used as food for animals, in water purification, for manufacturing of several biochemical products as well as for jokes and comical situations.\n\nThere are several methods to remove a peel from a banana.\n\nBananas are a popular fruit consumed worldwide with a yearly production of over 165 million tonnes in 2011. Once the peel is removed, the fruit can be eaten raw or cooked and the peel is generally discarded. Because of this removal of the banana peel, a significant amount of organic waste is generated.\n\nBanana peels are sometimes used as feedstock for cattle, goats, pigs, monkeys, poultry, fish, zebras and several other species, typically on small farms in regions where bananas are grown. There are some concerns over the impact of tannins contained in the peels on animals that consume them.\n\nThe nutritional value of banana peel depends on the stage of maturity and the cultivar; for example plantain peels contain less fibre than dessert banana peels, and lignin content increases with ripening (from 7 to 15% dry matter). On average, banana peels contain 6-9% dry matter of protein and 20-30% fibre (measured as NDF). Green plantain peels contain 40% starch that is transformed into sugars after ripening. Green banana peels contain much less starch (about 15%) when green than plantain peels, while ripe banana peels contain up to 30% free sugars.\n\nBanana peels are also used for water purification, to produce ethanol, cellulase, laccase, as fertilizer and in composting.\n\nBanana peel is also part of the classic physical comedy slapstick visual gag, the \"slipping on a banana peel\". This gag was already seen as classic in 1920s America. It can be traced to the late 19th century, when banana peel waste was considered a public hazard in a number of American towns. Although banana peel-slipping jokes date to at least 1854, they became much more popular, beginning in the late-1860s, when the large-scale importation of bananas made them more readily available. Before banana peel jokes came into vogue, orange peels, and sometimes peach skins, or fruit peels/peelings/or skins, generally, were funny, as well as dangerous. Slipping on a banana peel was at one point a real concern with municipal ordinances governing the disposal of the peel.\n\nThe coefficient of friction of banana peel on a linoleum surface was measured at just 0.07, about half that of lubricated metal on metal. Researchers attribute this to the crushing of the natural polysaccharide follicular gel, releasing a homogenous sol. This unsurprising finding was awarded the 2014 Ig Nobel Prize for physics.\n\nMost people peel a banana by cutting or snapping the stem and divide the peel into sections while pulling them away from the bared fruit. Another way of peeling a banana is done in the opposite direction, from the end with the brownish floral residue. A way usually perceived as \"upside down\". This way is also known as the \"monkey method\", since it is how monkeys are said to peel bananas.\n\nWhen the tip of a banana is pinched with two fingers, it will split and the peel comes off in two clean sections. The inner fibres, or \"strings\", between the fruit and the peel will remain attached to the peel and the stem of the banana can be used as a handle when eating the banana.\n\n\n", "id": "11979055", "title": "Banana peel"}
{"url": "https://en.wikipedia.org/wiki?curid=1101623", "text": "Human waste\n\nHuman waste (or human excreta) is a waste type usually used to refer to byproducts of digestion, such as feces and urine. Human waste can be collected, treated and disposed of or reused in many different ways, depending on the sanitation system that is in place and the type of toilet being used. \n\nThe situation differs vastly across the world, with many people in developing countries having to resort to open defecation where human waste is deposited in the environment, for lack of other options. Others can use flush toilets where the human waste is mixed with water and stored in septic tanks or sent through sewer pipes to be transported to sewage treatment plants. \n\nChildren's excreta can be disposed of in diapers and mixed with municipal solid waste. Diapers are also sometimes dumped directly into the environment, leading to public health risks.\n\nThe term \"human waste\" is used in the general media to mean several things, such as sewage, sewage sludge, blackwater - in fact anything that may contain some human feces. In the stricter sense of the term, human waste is in fact human excreta, i.e. urine and feces, with or without water being mixed in. For example, dry toilets collect human waste without the addition of water.\n\nHuman waste is considered a biowaste, as it is a vector for both viral and bacterial diseases. It can be a serious health hazard if it gets into sources of drinking water. The World Health Organisation (WHO) reports that nearly 2.2 million people die annually from diseases caused by contaminated water. A major accomplishment of human civilization has been the reduction of disease transmission via human waste through the practice of hygiene and sanitation, which can employ a variety of different technologies.\n\nEven high-mountains are not free from human waste. Each year, millions of mountaineers visit high-mountain areas. They generate tonnes of feces and urine annually which cause environmental pollution. The authorities of mountain regions, especially in the Global South countries do not pay enough attention to the problem of excreta from mountaineers. Human feces pose a greater threat to the mountain environment than uncontrolled deposit of urine, due to the higher pathogen content of feces.\n\nMethods of processing depend on the type of human waste: \nThe amount of water mixed with human waste can be reduced by the use of waterless urinals and composting toilets and by recycling greywater. The most common method of human waste treatment in rural areas where municipal sewage systems are unavailable is the use of septic tank systems. In remote rural places without sewage or septic systems, small populations allow for the continued use of honey buckets and sewage lagoons (see anaerobic lagoon) without the threat of disease presented by places with denser populations. Bucket toilets are used by rural villages in Alaska where, due to permafrost, conventional waste treatment systems cannot be utilized.\n\nHuman waste in the form of wastewater (sewage) is used to irrigate and fertilize fields in many parts of the developing world where fresh water is unavailable. There is great potential for wastewater agriculture to produce more food for consumers in urban areas, as long as there is sufficient education about the dangers of eating such food uncooked.\n\n", "id": "1101623", "title": "Human waste"}
{"url": "https://en.wikipedia.org/wiki?curid=1405654", "text": "Thermolabile\n\nThermolabile refers to a substance which is subject to destruction, decomposition, or change in response to heat. This term is often used to describe biochemical substances.\n\nFor example, many bacterial exotoxins are thermolabile and can be easily inactivated by the application of moderate heat. \nEnzymes are also thermolabile and lose their activity when the temperature rises.\nLoss of activity in such toxins and enzymes is likely due to change in the three-dimensional structure of the toxin protein during exposure to heat.\nIn pharmaceutical compounds, heat generated during grinding may lead to degradation of thermolabile compounds.\n\nThis is of particular use in testing gene function. This is done by intentionally creating mutants which are thermolabile. Growth below the permissive temperature allows normal protein function, while increasing the temperature above the permissive temperature ablates activity, likely by denaturing the protein.\n\nThermolabile enzymes are also studied for their applications in DNA replication techniques, such as PCR, where thermostable enzymes are necessary for proper DNA replication. Enzyme function at higher temperatures may be enhanced with trehalose, which opens up the possibility of using normally thermolabile enzymes in DNA replication. \n\n", "id": "1405654", "title": "Thermolabile"}
{"url": "https://en.wikipedia.org/wiki?curid=533069", "text": "Type (biology)\n\nIn biology, a type is a particular specimen (or in some cases a group of specimens) of an organism to which the scientific name of that organism is formally attached. In other words, a type is an example that serves to anchor or centralize the defining features of that particular taxon. In older usage (pre-1900 in botany), a type was a taxon rather than a specimen.\n\nA taxon is a scientifically named grouping of organisms with other like organisms, a set that includes some organisms and excludes others, based on a detailed published description (for example a species description) and on the provision of type material, which is usually available to scientists for examination in a major museum research collection, or similar institution.\n\nAccording to a precise set of rules laid down in the International Code of Zoological Nomenclature (ICZN) and the International Code of Nomenclature for algae, fungi, and plants (ICN), the scientific name of every taxon is almost always based on one particular \"specimen\", or in some cases specimens. Types are of great significance to biologists, especially to taxonomists. Types are usually physical specimens that are kept in a museum or herbarium research collection, but failing that, an image of an individual of that taxon has sometimes been designated as a type. Describing species and appointing type specimens is part of scientific nomenclature and alpha taxonomy.\n\nWhen identifying material, a scientist attempts to apply a taxon name to a specimen or group of specimens based on his or her understanding of the relevant taxa, based on (at least) having read the type description(s), preferably based on an examination of all the type material of all of the relevant taxa. If there is more than one named type that all appear to be the same taxon, then the oldest name takes precedence, and is considered to be the correct name of the material in hand. If on the other hand the taxon appears never to have been named at all, then the scientist or another qualified expert picks a type specimen and publishes a new name and an official description.\n\nThis process is crucial to the science of biological taxonomy. People's ideas of how living things should be grouped change and shift over time. How do we know that what we call \"\"Canis lupus\"\" is the same thing, or approximately the same thing, as what they will be calling \"\"Canis lupus\"\" in 200 years' time? It is possible to check this because there is a particular wolf specimen preserved in Sweden and everyone who uses that name – no matter what else they may mean by it – will include that particular specimen.\n\nDepending on the nomenclature code applied to the organism in question, a type can be a specimen, a culture, an illustration, or (under the bacteriological code) a description. Some codes consider a subordinate taxon to be the type, but under the botanical code the type is always a specimen or illustration.\n\nFor example, in the research collection of the Natural History Museum in London, there is a bird specimen numbered 1886.6.24.20. This is a specimen of a kind of bird commonly known as the spotted harrier, which currently bears the scientific name \"Circus assimilis\". This particular specimen is the holotype for that species; the name \"Circus assimilis\" refers, by definition, to the species of that particular specimen. That species was named and described by Jardine and Selby in 1828, and the holotype was placed in the museum collection so that other scientists might refer to it as necessary.\n\nNote that at least for type specimens there is no requirement for a \"typical\" individual to be used. Genera and families, particularly those established by early taxonomists, tend to be named after species that are more \"typical\" for them, but here too this is not always the case and due to changes in systematics \"cannot\" be. Hence, the term name-bearing type or onomatophore is sometimes used, to denote the fact that biological types do not define \"typical\" individuals or taxa, but rather fix a scientific name to a specific operational taxonomic unit. Type specimens are theoretically even allowed to be aberrant or deformed individuals or color variations, though this is rarely chosen to be the case, as it makes it hard to determine to which population the individual belonged.\n\nThe usage of the term \"type\" is somewhat complicated by slightly different uses in botany and zoology. In the \"PhyloCode\", type-based definitions are replaced by phylogenetic definitions.\n\nIn some older taxonomic works the word \"type\" has sometimes been used differently. The meaning was similar in the first \"Laws of Botanical Nomenclature\", but has a meaning closer to the term taxon in some other works:\nTranslation: This single character permits [one to] distinguish this type from all other species of the section ... After studying the diverse forms, I came to consider them as belonging to the one and the same specific type.\n\nIn botanical nomenclature, a \"type\" (\"typus\", \"nomenclatural type\"), \"is that element to which the name of a taxon is permanently attached.\" (article 7.1) In botany a type is either a specimen or an illustration. A specimen is a real plant (or one or more parts of a plant or a lot of small plants), dead and kept safe, \"curated\", in a herbarium (or the equivalent for fungi). Examples of where an illustration may serve as a type include:\n\nNote that a type does not determine the circumscription of the taxon. For example, the common dandelion is a controversial taxon: some botanists consider it to consist of over a hundred species, and others regard it as a single species. The type of the name \"Taraxacum officinale\" is the same whether the circumscription of the species includes all those small species (\"Taraxacum officinale\" is a \"big\" species) or whether the circumscription is limited to only one small species among the other hundred (\"Taraxacum officinale\" is a \"small\" species). The name \"Taraxacum officinale\" is the same and the type of the name is the same, but the extent of what the name actually applies to varies greatly. Setting the circumscription of a taxon is done by a taxonomist in a publication.\n\nMiscellaneous notes:\n\nThe \"ICN\" provides a listing of the various kinds of type (article 9), the most important of which is the holotype. These are\n\nNote that the word \"type\" appears in botanical literature as a part of some older terms that have no status under the \"ICN\": for example a clonotype.\n\nIn zoological nomenclature, the type of a species (or subspecies) is a specimen (or series of specimens), the type of a genus (or subgenus) is a species, and the type of a suprageneric taxon (e.g., family, etc.) is a genus. Names higher than superfamily rank do not have types. A \"name-bearing type\" \"provides the objective standard of reference whereby the application of the name of a nominal taxon can be determined.\"\n\n\nAlthough in reality biologists may examine many specimens (when available) of a new taxon before writing an official published species description, nonetheless, under the formal rules for naming species (the International Code of Zoological Nomenclature), a single type must be designated, as part of the published description.\n\nA type description must include a diagnosis (typically, a discussion of similarities to and differences from closely related species), and an indication of where the type specimen or specimens are deposited for examination. The geographical location where a type specimen was originally found is known as its '. In the case of parasites, the term ' (or symbiotype) is used to indicate the host organism from which the type specimen was obtained.\n\nZoological collections are maintained by universities and museums. Ensuring that types are kept in good condition and made available for examination by taxonomists are two important functions of such collections. And, while there is only one \"holotype\" designated, there can be other \"type\" specimens, the following of which are formally defined:\n\nWhen a single specimen is clearly designated in the original description, this specimen is known as the \"holotype\" of that species. The holotype is typically placed in a major museum, or similar well-known public collection, so that it is freely available for later examination by other biologists.\n\nWhen the original description designated a holotype, there may still be additional specimens listed in the type series and those are termed paratypes. These are not name-bearing types.\n\nAn allotype is a specimen of the opposite sex to the holotype, designated from among paratypes. It was also formerly used for a specimen that shows features not seen in the holotype of a fossil. The term is not regulated by the ICZN.\n\nA neotype is a specimen later selected to serve as the single type specimen when an original holotype has been lost or destroyed or where the original author never cited a specimen.\n\nA syntype is any one of two or more specimens that is listed in a species description where no holotype was designated; historically, syntypes were often explicitly designated as such, and under the present ICZN this is a requirement, but modern attempts to publish species description based on syntypes are generally frowned upon by practicing taxonomists, and most are gradually being replaced by lectotypes. Those that still exist are still considered name-bearing types.\n\nA lectotype is a specimen later selected to serve as the single type specimen for species originally described from a set of syntypes.\nIn zoology, a lectotype is a kind of name-bearing type. When a species was originally described on the basis of a name-bearing type consisting of multiple specimens, one of those may be designated as the lectotype. Having a single name-bearing type reduces the potential for confusion, especially considering that it is not uncommon for a series of syntypes to contain specimens of more than one species.\n\nA notable example is that Carl Linnaeus is the lectotype for the species \"Homo sapiens\".\n\nA paralectotype is any additional specimen from among a set of syntypes, after a lectotype has been designated from among them. These are not name-bearing types.\n\nA special case in Protistans where the type consists of two or more specimens of \"directly related individuals representing distinct stages in the life cycle\"; these are collectively treated as a single entity, and lectotypes cannot be designated from among them.\n\nAn ergatotype is a specimen selected to represent a worker member in hymenopterans which have polymorphic castes.\n\nType illustrations have also been used by zoologists, as in the case of the Réunion parakeet, which is known only from historical illustrations and descriptions.\n\nRecently, some species have been described where the type specimen was released alive back into the wild, such as the Bulo Burti boubou (a bushshrike), described as \"Laniarius liberatus\", in which the species description included DNA sequences from blood and feather samples. Assuming there is no future question as to the status of such a species, the absence of a type specimen does not invalidate the name, but it may be necessary in the future to designate a neotype for such a taxon, should any questions arise. However, in the case of the bushshrike, ornithologists have argued that the specimen was a rare and hitherto unknown color morph of a long-known species, using only the available blood and feather samples. While there is still some debate on the need to deposit actual killed individuals as type specimens, it can be observed that given proper vouchering and storage, tissue samples can be just as valuable should disputes about the validity of a species arise.\n\nThe various types listed above are necessary because many species were described one or two centuries ago, when a single type specimen, a holotype, was often not designated. Also, types were not always carefully preserved, and intervening events such as wars and fires have resulted in destruction of original type material. The validity of a species name often rests upon the availability of original type specimens; or, if the type cannot be found, or one has never existed, upon the clarity of the description.\n\nThe ICZN has existed only since 1961, when the first edition of the Code was published. The ICZN does not always demand a type specimen for the historical validity of a species, and many \"type-less\" species do exist. The current edition of the Code, Article 75.3, prohibits the designation of a neotype unless there is \"an exceptional need\" for \"clarifying the taxonomic status\" of a species (Article 75.2).\n\nThere are many other permutations and variations on terms using the suffix \"-type\" (e.g., allotype, cotype, topotype, generitype, isotype, isoneotype, isolectotype, etc.) but these are not formally regulated by the Code, and a great many are obsolete and/or idiosyncratic. However, some of these categories can potentially apply to genuine type specimens, such as a neotype; e.g., isotypic/topotypic specimens are preferred to other specimens, when they are available at the time a neotype is chosen (because they are from the same time and/or place as the original type).\n\nThe term fixation is used by the Code for the declaration of a name-bearing type, whether by original or subsequent designation.\n\nEach genus must have a designated type species (the term \"genotype\" was once used for this but has been abandoned because the word has become much better known as the term for a different concept in genetics). The description of a genus is usually based primarily on its type species, modified and expanded by the features of other included species. The generic name is permanently associated with the name-bearing type of its type species.\n\nIdeally, a type species best exemplifies the essential characteristics of the genus to which it belongs, but this is subjective and, ultimately, technically irrelevant, as it is not a requirement of the Code. If the type species proves, upon closer examination, to belong to a pre-existing genus (a common occurrence), then all of the constituent species must be either moved into the pre-existing genus, or disassociated from the original type species and given a new generic name; the old generic name passes into synonymy and is abandoned unless there is a pressing need to make an exception (decided case-by-case, via petition to the International Commission on Zoological Nomenclature).\n\nA type genus is that genus from which the name of a family or subfamily is formed. As with type species, the type genus is not necessarily the most representative, but is usually the earliest described, largest or best known genus. It is not uncommon for the name of a family to be based upon the name of a type genus that has passed into synonymy; the family name does not need to be changed in such a situation.\n\n\n", "id": "533069", "title": "Type (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=23759925", "text": "Field metabolic rate\n\nField metabolic rate (FMR) refers to a measurement of the metabolic rate of a free-living animal in the wild.\n\nMeasurement of the Field metabolic rate is made using the doubly labeled water method, although alternative techniques, such as monitoring heart rates, can also be used. The advantages and disadvantages of the alternative approaches have been reviewed by Butler, \"et al.\" Several summary reviews have been published.\n", "id": "23759925", "title": "Field metabolic rate"}
{"url": "https://en.wikipedia.org/wiki?curid=10013115", "text": "Mechanism (biology)\n\nIn the science of biology, a mechanism is a system of casually interacting parts and processes that produce one or more effects. Scientists explain phenomena by describing mechanisms that could produce the phenomena. For example, natural selection is a mechanism of biological evolution; other mechanisms of evolution include genetic drift, mutation, and gene flow. In ecology, mechanisms such as predation and host-parasite interactions produce change in ecological systems. In practice, no description of a mechanism is ever complete because not all details of the parts and processes of a mechanism are fully known. For example, natural selection is a mechanism of evolution that includes countless, inter-individual interactions with other individuals, components, and processes of the environment in which natural selection operates.\n\nMany characterizations/definitions of mechanisms in the philosophy of science/biology have been provided in the past decades. For example, one influential characterization of neuro- and molecular biological mechanisms is as follows: mechanisms are entities and activities organized such that they are productive of regular changes from start to termination conditions (Peter Machamer, Lindley Darden, & Carl Craver 2000; 'MDC' hereafter). Other characterizations have been proposed by Stuart Glennan (1996, 2002), who articulates an interactionist account of mechanisms, and William Bechtel (1993, 2006), who emphasizes parts and operations (cf. MDC).\n\nThe MDC characterization is as follows: mechanisms are entities and activities organized such that they are productive of changes from start conditions to termination conditions. There are three distinguishable aspects of this characterization:\n\n\n\n\nMechanisms in science/biology have reappeared as a subject of philosophical analysis and discussion in the last several decades because of a variety of factors, many of which relate to metascientific issues such as explanation and causation. For example, the decline of Covering Law (CL) models of explanation, e.g., Hempel's deductive-nomological model, has stimulated interest how mechanisms might play an explanatory role in certain domains of science, especially higher-level disciplines such as biology (i.e., neurobiology, molecular biology, neuroscience, and so on). This is not just because of the philosophical problem of giving some account of what \"laws of nature,\" which CL models encounter, but also the incontrovertible fact that most biological phenomena are not characterizable in nomological terms (i.e., in terms of lawful relationships). For example, protein biosynthesis does not occur according to any law, and therefore, on the DN model, no explanation for the biosynthesis phenomenon could be given.\n\nMechanistic explanations come in many forms. Wesley Salmon proposed what he called the \"ontic\" conception of explanation, which states that explanations are mechanisms and causal processes \"in the world\". There are two such kinds of explanation: \"etiological\" and \"constitutive\". Salmon focused primarily on etiological explanation, with respect to which one explains some phenomenon \"P\" by identifying its causes (and, thus, locating it within the causal structure of the world). Constitutive (or componential) explanation, on the other hand, involves describing the components of a mechanism \"M\" that is productive of (or causes) \"P.\" Indeed, whereas (a) Noam Chomsky differentiates between descriptive and explanatory adequacy, where the former is defined as the adequacy of a theory to account for at least all the items in the domain (which need explaining), and the latter as the adequacy of a theory to account for no more than those domain items, and (b) past philosophies of science differentiate between descriptions of phenomena and explanations of those phenomena, in the non-ontic context of mechanism literature, descriptions and explanations seem to be identical. This is to say, to explain a mechanism \"M\" is to describe it (specify its components, as well as background, enabling, and so on, conditions that constitute, in the case of a linear mechanism, its \"start conditions\").\n\n", "id": "10013115", "title": "Mechanism (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=25652972", "text": "No-analog (ecology)\n\nNo-analog (variants may omit the hyphen and/or use the British English \"analogue\"), or alternately \"novel\", climatic conditions (\"no-analog climates\") or biological communities (\"no-analog communities\") in paleoecology and ecological forecasting are those without current equivalents.\n\n\n", "id": "25652972", "title": "No-analog (ecology)"}
{"url": "https://en.wikipedia.org/wiki?curid=6173781", "text": "Polychotomy\n\nA polychotomy (päl′i kät′ə mē; plural \"polychotomies\") is a division or separation into many parts or classes. Polychotomy is a generalization of dichotomy, which is a polychotomy of exactly two parts. In evolutionary biology, the term polychotomy can also be considered a historically based misspelling of polytomy.\n\n\n", "id": "6173781", "title": "Polychotomy"}
{"url": "https://en.wikipedia.org/wiki?curid=32649239", "text": "Resource (biology)\n\nIn Biology and Ecology, a resource is a substance or object in the environment required by an organism for normal growth, maintenance, and reproduction. Resources can be consumed by one organism and, as a result, become unavailable to another organism. For plants key resources are light, nutrients, water, and place to grow. For animals key resources are food, water, and territory. \n\nTerrestrial plants require particular resources for photosynthesis and to complete their life cycle of germination, growth, reproduction, and dispersal: \n\nAnimals resources particular resources for metabolism and to complete their life cycle of gestation, birth, growth, and reproduction:\n\n\nResource availability plays a central role in ecological processes:\n\n\n", "id": "32649239", "title": "Resource (biology)"}
