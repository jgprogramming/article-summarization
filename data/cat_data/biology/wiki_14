{"url": "https://en.wikipedia.org/wiki?curid=53354695", "text": "Michael P. Snyder\n\nMichael Snyder (born 1955) is an American genomicist, systems biologist, and entrepreneur. He is the Stanford B. Ascherman Professor and Chair of Genetics and Director of Genomics and Personalized Medicine at Stanford University School of Medicine.\n\nSnyder was born 1955 and grew up outside of Pottstown, Pennsylvania. His father, Kermit Snyder, was an accountant and his mother, Phyllis Snyder, was an elementary school teacher. Snyder attended Owen J Roberts High school in Pottstown, Pennsylvania. He won a Bausch & Lomb science award and attended the University of Rochester, NY where he received a B.A. in Chemistry and Biology. Upon graduation Snyder worked as a research assistant with Karl Drlica at the University of Rochester.\nIn 1978 Snyder started graduate school in Biology at the California Institute of Technology where he trained under Dr. Norman Davidson. Recombinant DNA was relatively new at the time and by using this technology to clone a set of gene encoding Drosophila cuticle proteins, Snyder discovered that related genes are often co-associated with one another in the genome . He also discovered one of the first pseudogenes in eukaryotes and made the fundamental discovery that transposons often land in open promoter regions of eukaryotic genes when he discovered a new transposon, HMS Beagle, located in the promoter of an inactivated Drosophila cuticle gene .\n\nSnyder completed his postdoctoral training at Stanford School of Medicine in the laboratory of Dr. Ronald Davis. There he was involved in several projects including establishment of successful cloning of genes using antibodies (lambagt11;) . The expression libraries he created were used widely by thousands of laboratories worldwide.\n\nSnyder moved to Yale as an Assistant Professor in 1986 in the Department of Biology. His laboratory worked on chromosome segregation and cell polarity for which he discovered a number of important genes involved in these processes . His laboratory proposed the first models by which eucaryotes select sites of cell growth .\n\nHe was promoted to Associate Professor with tenure in 1994, and when the Biology Department split in the Molecular, Cellular and Developmental Biology (MCDB) and Ecology and Evolutionary Biology, he became chair of the new MCDB department. During his six years as chair the Department doubled in size and tripled in research funding. He was also the Director for the Center for Genomics and Proteomics at Yale University.\n\nIn 2009 Snyder moved to Stanford University to Chair the Genetics Department and to direct the Center for Genomics and Personalized Medicine. Since 2010 the U.S. News & World Report has ranked Stanford University first or tied for first in Genetics, Genomics and Bioinformatics.\n\nSnyder was elected and has served as President of US Human Proteome Organization (2006–2008) and is currently President of the Human Proteome Organization. He has served on numerous scientific advisory committees (e.g. EMBL Scientific Advisory Committee) and is on the Genetics Society Board of Directors (2006–2009). He has organized many scientific meetings.\n\nSnyder has been Principal Investigator of Center of Excellence in the Genome Sciences (CEGS) (2001–2011), NIH Training Grants in Genomics and Proteomics (first at Yale, now at Stanford) (2004–present), and is coDirector of the CIRM Center for Stem Cell Genomics and Director or the Center for Genome of Gene Regulation. He has been a Principal Investigator in the ENCODE project since its inception in 2003.\n\nIn addition to contributions to the field of cell biology, Snyder’s laboratory has invented a number of novel systems-wide and genomics technologies, and his laboratory has used these to make fundamental biological discoveries.\n\nAt a time when most laboratories were studying one or a limited number of genes at a time, Snyder’s laboratory set up the first large scale systems project to study all yeast genes and proteins simultaneously using a transposon tagging strategy to analyze gene expression, protein localization and gene disruption . This was the first large-scale systems analysis of genes and proteins in any organism and launched the field of functional genomics. The libraries and approaches were widely utilized by many laboratories around the world and launched the concept of open sharing and reagents, prior to publication. With Dr. Patrick Brown’s laboratory, the Snyder laboratory invented ChIP-chip (which they later morphed into ChIP-seq to carry out the first genome wide mapping of transcription factor binding sites. Initially established for yeast, they later applied the methods to humans . This method was foundational for multiple multicenter consortia projects including the Encyclopedia of DNA Elements project (ENCODE; ). \nTheir laboratory constructed the first human chromosome array and later the first whole genome array to map TF binding sites and novel transcribed regions of the genome. They later invented RNA-seq to better map transcriptomes, both protein coding and noncoding . Today, this technique is widely employed in the molecular biology field. \nWith the advent of high throughput DNA sequencing technologies, the Snyder laboratory was the first to sequence an organism using such technology, at a time when most groups thought the technology was too error-prone to be useful. They sequenced \"Acinetobacter Baummanii\", a human pathogen with low error rates . They invented paired end sequencing using new high throughput sequencing technologies and used this to demonstrate that there was ten times as much structural variation (SV) in the human genome a previously realized and that most SV deletions and insertions were due to nonhomologous recombination, a surprising finding at the time, since most SVs were proposed to be due to homologous recombination events.\nBeyond the genome, the Snyder lab was also the first to set up protein and proteome microarrays for the large-scale characterization of protein function and antibody reactivity . They demonstrated many novel biological activities of protein kinases and other yeast proteins and showed they can be useful for autoantibody profiling .\n\nThrough their genomics efforts the Snyder laboratory has found that there are many more TF binding sites than were previously appreciated , with more potential regulatory sequences than RNA coding segments in the human genome (10% versus 3%) . In addition to TF binding sites, the Snyder lab discovered that twice as much of the human genome is transcribed into the mature RNA , revealing the widespread occurrence of lincRNAs. These lincRNAs have since been shown to have a diverse array of interesting cellular functions.\nMuch attention has been paid to understanding the differences individuals and species. The Snyder laboratory was the first to show that transcription factor binding sites vary greatly among people and closely related species, demonstrating that much of the diversity among individuals and closely related species resides at the level of gene regulation , rather than the genes themselves. Much of this variation resides in distal regulatory elements called enhancers.\n\nUsing the same in-depth omics approaches he applied to yeast, upon his move to Stanford in 2009, Snyder began to apply systems-wide analysis to human health . The Snyder laboratory carried out the first deep longitudinal profiling of one person using multi-'omics technologies (genomics, transcriptomics, proteomics, metabolomics, etc.). This deep profiling used genomics for the first time to predict disease risk and follow disease onset at a level not previously achieved . This work caused a stir in the genetics and medical fields and was deemed a landmark paper in the journal Cell's 40th anniversary celebration in 2012. This approach of collecting longitudinal deep data on humans is now being applied by many groups worldwide. The Snyder lab has recently demonstrated that self-tracking using wearable biosensor can be used for monitoring health and illness . Together these studies demonstrate the power of using longitudinal tracking and big data to manage human health.\n\nSnyder has been a co-founder of a number of successful Biotechnology companies. These include: Exelixis, Protometrix (purchased by Life Technologies, now part of Thermo Fisher), Affomix (purchased by Illumina), Personalis, SensOmics and Qbio. He also sits on the board of numerous other biotechnology companies.\n\nSnyder has received the following awards:\n\n\nHe has been listed in the \"Most Cited Scientists since 2014\" and has given many distinguished and named lectureships. Since 2009 these include:\n\n\nSnyder has authored over 500 published manuscripts.\n\nHe has authored a popular book: \"Genomics and Personalized Medicine: What Everyone Needs to Know\". Oxford University Press. 2016. It describes the utility of genome sequencing, other omics technologies and big data in medicine and prospects for the future.\n\n", "id": "53354695", "title": "Michael P. Snyder"}
{"url": "https://en.wikipedia.org/wiki?curid=53517673", "text": "Transcriptional amplification\n\nIn genetics, transcriptional amplification is the process in which the total amounts of messenger RNA (mRNA) molecules from expressed genes are increased during disease, development, or in response to stimuli.\n\nAt the subset of genes expressed in a given cell, the transcribing activity of RNA Polymerase II results in mRNA production. Transcriptional amplification is specifically defined as the increase in per-cell abundance of this set of expressed mRNAs. Transcriptional amplification is caused by changes in the amount or activity of transcription-regulating proteins.\n\nGene expression is regulated by numerous types of proteins that directly or indirectly influence transcription by RNA Polymerase II. As opposed to transcriptional activators or repressors that selectively activate or repress specific genes, amplifiers of transcription act globally on the set of initially expressed genes.\n\nSeveral known regulators of transcriptional amplification have been characterized including the oncogene Myc., the Rett syndrome protein MECP2, and the BET bromodomain protein BRD4. In particular, the Myc protein amplifies transcription by binding to promoters and enhancers of active genes where it directly recruits the transcription elongation factor P-TEFb. Furthermore, the BRD4 protein is a regulator of Myc activity.\n\nCommonly used gene expression experiments interrogate the expression of one (qPCR) or many (microarray, RNA-Seq) genes. These techniques generally measure relative mRNA levels and employ normalization methods that assume only a small number of genes show altered expression. Instead, single cell or cell count normalized absolute measurements of mRNA abundance are required to reveal transcriptional amplification. Additionally, global measurements of mRNA or total mRNA per cell can also uncover evidence for transcriptional amplification.\n\nCells in which transcription has been amplified have additional suggestive hallmarks that amplification has occurred. Cells with increased mRNA levels may be larger, consistent with an increased abundance of gene products. This increase in the amount of gene product may result in a decreased doubling time.\n\nTranscriptional amplification has been implicated in cancer, Rett syndrome, heart disease, Down syndrome, and cellular aging. In cancer, Myc driven transcriptional amplification is posited to help tumor cells overcome rate-limiting constraints in growth and proliferation. Drugs that target the transcription or mRNA processing machinery are known to be particularly effective against Myc-driven tumor models, suggesting that dampening of transcriptional amplification can have anti-tumor effects. Similarly, small molecules targeting the BET bromodomain protein BRD4, which is up-regulated during heart failure, can block cardiac hypertrophy in mouse models. In Rett syndrome, which is caused by loss of function of the transcriptional regulator MeCP2, MeCP2 was shown to specifically amplify transcription in neurons and not neuronal precursors. Restoration of MeCP2 reverses disease symptoms associated with Rett syndrome\n", "id": "53517673", "title": "Transcriptional amplification"}
{"url": "https://en.wikipedia.org/wiki?curid=55309", "text": "Blood type\n\nA blood type (also called a blood group) is a classification of blood based on the presence and absence of antibodies and also based on the presence or absence of inherited antigenic substances on the surface of red blood cells (RBCs). These antigens may be proteins, carbohydrates, glycoproteins, or glycolipids, depending on the blood group system. Some of these antigens are also present on the surface of other types of cells of various tissues. Several of these red blood cell surface antigens can stem from one allele (or an alternative version of a gene) and collectively form a blood group system.\nBlood types are inherited and represent contributions from both parents. A total of 35 human blood group systems are now recognized by the International Society of Blood Transfusion (ISBT). The two most important ones are ABO and the RhD antigen; they determine someone's blood type (A, B, AB and O, with +, − or Null denoting RhD status).\n\nA complete blood type would describe a full set of 30 substances on the surface of RBCs, and an individual's blood type is one of many possible combinations of blood-group antigens. Across the 35 blood groups, over 600 different blood-group antigens have been found.Almost always, an individual has the same blood group for life, but very rarely an individual's blood type changes through addition or suppression of an antigen in infection, malignancy, or autoimmune disease. Another more common cause in blood type change is a bone marrow transplant. Bone-marrow transplants are performed for many leukemias and lymphomas, among other diseases. If a person receives bone marrow from someone who is a different ABO type (e.g., a type A patient receives a type O bone marrow), the patient's blood type will eventually convert to the donor's type.\n\nSome blood types are associated with inheritance of other diseases; for example, the Kell antigen is sometimes associated with McLeod syndrome. Certain blood types may affect susceptibility to infections, an example being the resistance to specific malaria species seen in individuals lacking the Duffy antigen. The Duffy antigen, presumably as a result of natural selection, is less common in ethnic groups from areas with a high incidence of malaria.\n\nIn human blood there are two antigens and antibodies. The two antigens are antigen A and antigen B. The two antibodies are antibody A and antibody B. The \nAntigens are present in the red blood cells and the antibodies in the serum. \nRegarding the antigen property of the blood all human beings can be classified into 4 groups, those with antigen A (group A), those with antigen B (group B), those with both antigen A and B (group AB) and those with neither antigen (group O). The antibodies present together with the antigens are found as follows:\n\n1. Antigen A with antibody B<br>\n2. Antigen B with antibody A<br>\n3. Antigen AB has no antibodies<br>\n4. Antigen nil (group O) with antibody A and B.\n\nThere is an agglutination reaction between similar antigen and antibody (for example, antigen A agglutinates the antibody A and antigen B agglutinates the antibody B). Thus, transfusion can be considered safe as long as the serum of the recipient does not contain antibodies for the blood cell antigens of the donor.\n\nThe \"ABO system\" is the most important blood-group system in human-blood transfusion. The associated anti-A and anti-B antibodies are usually \"immunoglobulin M\", abbreviated IgM, antibodies. ABO IgM antibodies are produced in the first years of life by sensitization to environmental substances such as food, bacteria, and viruses. The original terminology used by Karl Landsteiner in 1901 for the classification was A/B/C; in later publications \"C\" became \"O\". \"O\" is often called \"0\" (\"zero\", or \"null\") in other languages.\n\nThe Rh system (Rh meaning \"Rhesus\") is the second most significant blood-group system in human-blood transfusion with currently 50 antigens. The most significant Rh antigen is the D antigen, because it is the most likely to provoke an immune system response of the five main Rh antigens. It is common for D-negative individuals not to have any anti-D IgG or IgM antibodies, because anti-D antibodies are not usually produced by sensitization against environmental substances. However, D-negative individuals can produce IgG anti-D antibodies following a sensitizing event: possibly a fetomaternal transfusion of blood from a fetus in pregnancy or occasionally a blood transfusion with D positive RBCs. Rh disease can develop in these cases. Rh negative blood types are much less common in Asian populations (0.3%) than they are in European populations (15%).\nThe presence or absence of the Rh(D) antigen is signified by the + or − sign, so that, for example, the A− group is ABO type A and does not have the Rh (D) antigen.\n\nAs with many other genetic traits, the distribution of ABO and Rh blood groups varies significantly between populations.\n\nThirty-three blood-group systems have been identified by the International Society for Blood Transfusion in addition to the common ABO and Rh systems. Thus, in addition to the ABO antigens and Rh antigens, many other antigens are expressed on the RBC surface membrane. For example, an individual can be AB, D positive, and at the same time M and N positive (MNS system), K positive (Kell system), Le or Le negative (Lewis system), and so on, being positive or negative for each blood group system antigen. Many of the blood group systems were named after the patients in whom the corresponding antibodies were initially encountered.\n\nTransfusion medicine is a specialized branch of hematology that is concerned with the study of blood groups, along with the work of a blood bank to provide a transfusion service for blood and other blood products. Across the world, blood products must be prescribed by a medical doctor (licensed physician or surgeon) in a similar way as medicines.\n\nMuch of the routine work of a blood bank involves testing blood from both donors and recipients to ensure that every individual recipient is given blood that is compatible and is as safe as possible. If a unit of incompatible blood is transfused between a donor and recipient, a severe acute hemolytic reaction with hemolysis (RBC destruction), renal failure and shock is likely to occur, and death is a possibility. Antibodies can be highly active and can attack RBCs and bind components of the complement system to cause massive hemolysis of the transfused blood.\n\nPatients should ideally receive their own blood or type-specific blood products to minimize the chance of a transfusion reaction. Risks can be further reduced by cross-matching blood, but this may be skipped when blood is required for an emergency. Cross-matching involves mixing a sample of the recipient's serum with a sample of the donor's red blood cells and checking if the mixture \"agglutinates\", or forms clumps. If agglutination is not obvious by direct vision, blood bank technicians usually check for agglutination with a microscope. If agglutination occurs, that particular donor's blood cannot be transfused to that particular recipient. In a blood bank it is vital that all blood specimens are correctly identified, so labelling has been standardized using a barcode system known as ISBT 128.\n\nThe blood group may be included on identification tags or on tattoos worn by military personnel, in case they should need an emergency blood transfusion. Frontline German Waffen-SS had blood group tattoos during World War II.\n\nRare blood types can cause supply problems for blood banks and hospitals. For example, Duffy-negative blood occurs much more frequently in people of African origin, and the rarity of this blood type in the rest of the population can result in a shortage of Duffy-negative blood for these patients. Similarly for RhD negative people, there is a risk associated with travelling to parts of the world where supplies of RhD negative blood are rare, particularly East Asia, where blood services may endeavor to encourage Westerners to donate blood.\n\nPregnant women may carry a fetus with a blood type which is different from their own. In those cases, the mother can make IgG blood group antibodies. This can happen if some of the fetus' blood cells pass into the mother's blood circulation (e.g. a small fetomaternal hemorrhage at the time of childbirth or obstetric intervention), or sometimes after a therapeutic blood transfusion. This can cause Rh disease or other forms of hemolytic disease of the newborn (HDN) in the current pregnancy and/or subsequent pregnancies. Sometimes this is lethal for the fetus; in these cases it is called hydrops fetalis. If a pregnant woman is known to have anti-D antibodies, the Rh blood type of a fetus can be tested by analysis of fetal DNA in maternal plasma to assess the risk to the fetus of Rh disease. One of the major advances of twentieth century medicine was to prevent this disease by stopping the formation of Anti-D antibodies by D negative mothers with an injectable medication called Rho(D) immune globulin. Antibodies associated with some blood groups can cause severe HDN, others can only cause mild HDN and others are not known to cause HDN.\n\nTo provide maximum benefit from each blood donation and to extend shelf-life, blood banks fractionate some whole blood into several products. The most common of these products are packed RBCs, plasma, platelets, cryoprecipitate, and fresh frozen plasma (FFP). FFP is quick-frozen to retain the labile clotting factors V and VIII, which are usually administered to patients who have a potentially fatal clotting problem caused by a condition such as advanced liver disease, overdose of anticoagulant, or disseminated intravascular coagulation (DIC).\n\nUnits of packed red cells are made by removing as much of the plasma as possible from whole blood units.\n\nClotting factors synthesized by modern recombinant methods are now in routine clinical use for hemophilia, as the risks of infection transmission that occur with pooled blood products are avoided.\n\n\nAn Rh D-negative patient who does not have any anti-D antibodies (never being previously sensitized to D-positive RBCs) can receive a transfusion of D-positive blood once, but this would cause sensitization to the D antigen, and a female patient would become at risk for hemolytic disease of the newborn. If a D-negative patient has developed anti-D antibodies, a subsequent exposure to D-positive blood would lead to a potentially dangerous transfusion reaction. Rh D-positive blood should never be given to D-negative women of child bearing age or to patients with D antibodies, so blood banks must conserve Rh-negative blood for these patients. In extreme circumstances, such as for a major bleed when stocks of D-negative blood units are very low at the blood bank, D-positive blood might be given to D-negative females above child-bearing age or to Rh-negative males, providing that they did not have anti-D antibodies, to conserve D-negative blood stock in the blood bank. The converse is not true; Rh D-positive patients do not react to D negative blood.\n\nThis same matching is done for other antigens of the Rh system as C, c, E and e and for other blood group systems with a known risk for immunization such as the Kell system in particular for females of child-bearing age or patients with known need for many transfusions.\n\nBlood plasma compatibility is the inverse of red blood cell compatibility. Type AB plasma carries neither anti-A nor anti-B antibodies and can be transfused to individuals of any blood group; but type AB patients can only receive type AB plasma. Type O carries both antibodies, so individuals of blood group O can receive plasma from any blood group, but type O plasma can be used only by type O recipients.\n\nRh D antibodies are uncommon, so generally neither D negative nor D positive blood contain anti-D antibodies. If a potential donor is found to have anti-D antibodies or any strong atypical blood group antibody by antibody screening in the blood bank, they would not be accepted as a donor (or in some blood banks the blood would be drawn but the product would need to be appropriately labeled); therefore, donor blood plasma issued by a blood bank can be selected to be free of D antibodies and free of other atypical antibodies, and such donor plasma issued from a blood bank would be suitable for a recipient who may be D positive or D negative, as long as blood plasma and the recipient are ABO compatible.\n\nIn transfusions of packed red blood cells, individuals with type O Rh D negative blood are often called universal donors. Those with type AB Rh D positive blood are called universal recipients. However, these terms are only generally true with respect to possible reactions of the recipient's anti-A and anti-B antibodies to transfused red blood cells, and also possible sensitization to Rh D antigens. One exception is individuals with hh antigen system (also known as the Bombay phenotype) who can only receive blood safely from other hh donors, because they form antibodies against the H antigen present on all red blood cells.\n\nBlood donors with exceptionally strong anti-A, anti-B or any atypical blood group antibody may be excluded from blood donation. In general, while the plasma fraction of a blood transfusion may carry donor antibodies not found in the recipient, a significant reaction is unlikely because of dilution.\n\nAdditionally, red blood cell surface antigens other than A, B and Rh D, might cause adverse reactions and sensitization, if they can bind to the corresponding antibodies to generate an immune response. Transfusions are further complicated because platelets and white blood cells (WBCs) have their own systems of surface antigens, and sensitization to platelet or WBC antigens can occur as a result of transfusion.\n\nFor transfusions of plasma, this situation is reversed. Type O plasma, containing both anti-A and anti-B antibodies, can only be given to O recipients. The antibodies will attack the antigens on any other blood type. Conversely, AB plasma can be given to patients of any ABO blood group, because it does not contain any anti-A or anti-B antibodies.\n\nTypically, blood type tests are performed through addition of a blood sample to a solution containing antibodies corresponding to each antigen. The presence of an antigen on the surface of the blood cells is indicated by agglutination. An alternative system for blood type determination involving no antibodies was developed in 2017 at Imperial College London which makes use of paramagnetic molecularly imprinted polymer nanoparticles with affinity for specific blood antigens. In these tests, rather than agglutination, a positive result is indicated by decolorization as red blood cells which bind to the nanoparticles are pulled toward a magnet and removed from solution.\n\nIn addition to the current practice of serologic testing of blood types, the progress in molecular diagnostics allows the increasing use of blood group genotyping. In contrast to serologic tests reporting a direct blood type phenotype, genotyping allows the prediction of a phenotype based on the knowledge of the molecular basis of the currently known antigens. This allows a more detailed determination of the blood type and therefore a better match for transfusion, which can be crucial in particular for patients with needs for many transfusions to prevent allo-immunization.\n\nTwo blood group systems were discovered by Karl Landsteiner during early experiments with blood transfusion: the ABO group in 1901 and in co-operation with Alexander S. Wiener the Rhesus group in 1937. Development of the Coombs test in 1945,\nthe advent of transfusion medicine, and the understanding of ABO hemolytic disease of the newborn led to discovery of more blood groups, and now 33 human blood group systems are recognized by the International Society of Blood Transfusion (ISBT), and in the 33 blood groups, over 600 blood group antigens have been found; many of these are rare or are mainly found in certain ethnic groups.\n\nCzech serologist Jan Janský is credited with the first classification of blood into the four types (A, B, AB, O) in 1907, which remains in use today. Blood types have been used in forensic science and were formerly used to demonstrate impossibility of paternity (e.g., a type AB man cannot be the father of a type O infant), but both of these uses are being replaced by genetic fingerprinting, which provides greater certainty.\n\nAccording to the Austrian Federal Ministry of Health the original terminology used by Karl Landsteiner in 1901 for the classification is A, B and 0 (\"zero\"); the \"O\" (\"oh\") found in the ABO group system is actually a subsequent variation occurred during the translation process, probably due to the similar shape between the number 0 and the letter O.\n\nA popular belief in Japan is that a person's ABO blood type is predictive of their personality, character, and compatibility with others. This belief is also widespread in South Korea and Taiwan. The theory reached Japan in a 1927 psychologist's report, and the government of the time commissioned a study aimed at breeding better soldiers. Interest in the theory faded in the 1930s. Ultimately, the discovery of DNA in the following decades indicated that DNA instead had an important role in both heredity generally and personality specifically. Interest in the theory was revived in the 1970s by Masahiko Nomi, a broadcaster with a background in law rather than science. The theory is widely accepted in Japanese and South Korean popular culture.\n\n\n\n", "id": "55309", "title": "Blood type"}
{"url": "https://en.wikipedia.org/wiki?curid=53584324", "text": "Parasitic chromosome\n\n\"Parasitic chromosomes\" are ¨selfish¨ chromosomes that propagate throughout cell divisions, even if they have no benefit to the overall organism's survival. As well, Parasitic chromosomes can persist even if slightly detrimental to survival, as is characteristic of some selfish genetic elements. Parasitic Chromosomes are often B chromosomes, such that they are not necessarily present in the majority of the species population and are not needed for basic life functions, in contrast to A chromosomes. Parasitic Chromosomes are classified as selfish genetic elements.\n\nParasitic chromosomes, if detrimental to an organism's survival, often are selected against by natural selection over time, but if the chromosome is able to act like a selfish DNA element, it can spread throughout a population. An example of a parasitic chromosome is the b24 chromosome in grasshoppers.\n", "id": "53584324", "title": "Parasitic chromosome"}
{"url": "https://en.wikipedia.org/wiki?curid=53631033", "text": "DNA spiking\n\nDNA spiking, also known as custom spiking, is the differing ratio of bases at a single degenerate position when synthesizing oligonucleotides. DNA spiking can include either equal or unequal proportions of bases at a given position (for example, 10% Adenine, 75% Guanine, 5% Cytosine & 10% Thymine). As an example, with the degenerate code R = A + G, 50% of time that ¨R¨ position is adenine and the other 50% of the time it is guanine. However, with DNA Spiking, the R position could be adenine 70% of the time and guanine 30% of the time. The proportions do not need to be 70:30, the ratios can be anything else such as 12:82 and 64:36.\n\nDNA spiking can also refer to a spike control in PCR, which is when DNA is added to a sample that will provide some signal (e.g. a plasmid or some synthetic DNA with a specific known sequence) to a reaction, and seeing if the reaction will amplify. This method is used to discover if the PCR method is working correctly, as in a PCR machine it may not amplify DNA properly, so by adding spiked DNA it can be observed how much DNA is produced. This is then compared to the amount of DNA that would be theoretically predicted if the machine was working properly so that any malfunctions can be discovered.\n\nRNA spike-in\n", "id": "53631033", "title": "DNA spiking"}
{"url": "https://en.wikipedia.org/wiki?curid=12796", "text": "Genotype\n\nThe genotype – a term coined by Danish botanist, plant physiologist and geneticist Wilhelm Johannsen in 1903 – is the part (DNA sequence) of the genetic makeup of a cell, and therefore of an organism or individual, which determines a specific characteristic (phenotype) of that cell/organism/individual. Genotype is one of three factors that determine phenotype, the other two being inherited epigenetic factors, and non-inherited environmental factors. DNA mutations which are acquired rather than inherited, such as cancer mutations, are not part of the individual's genotype; hence, scientists and physicians sometimes talk for example about the (geno)type of a particular cancer, that is the genotype of the disease as distinct from the diseased.\n\nAn example of how genotype determines a characteristic is petal color in a pea plant.\n\nThe genotype of an organism is the inherited map it carries within its genetic code. Not all organisms with the same genotype look or act the same way because appearance and behavior are modified by environmental and developmental conditions. Likewise, not all organisms that look alike necessarily have the same genotype.\nOne's genotype differs subtly from one's genomic sequence. A sequence is an absolute measure of base composition of an individual, or a representative of a species or group; a genotype typically implies a measurement of how an individual \"differs\" or is specialized within a group of individuals or a species. So, typically, one refers to an individual's genotype with regard to a particular gene of interest and, in polyploid individuals, it refers to what combination of alleles the individual carries (see homozygous, heterozygous). The genetic constitution of an organism is referred to as its genotype, such as the letters Bb (B – dominant genotype and b – recessive genotype).\n\nAny given gene will usually cause an observable change in an organism, known as the phenotype. The terms genotype and phenotype are distinct for at least two reasons:\n\n\nA simple example to illustrate genotype as distinct from phenotype is the flower colour in pea plants (see Gregor Mendel). There are three available genotypes, PP (homozygous dominant), Pp (heterozygous), and pp (homozygous recessive). All three have different genotypes but the first two have the same phenotype (purple) as distinct from the third (white).\n\nA more technical example to illustrate genotype is the single nucleotide polymorphism or SNP. A SNP occurs when corresponding sequences of DNA from different individuals differ at one DNA base, for example where the sequence AAGCCTA changes to AAGCTTA. This contains two alleles : C and T. SNPs typically have three genotypes, denoted generically AA Aa and aa. In the example above, the three genotypes would be CC, CT and TT. Other types of genetic marker, such as microsatellites, can have more than two alleles, and thus many different genotypes.\n\nPenetrance is the proportion of individuals showing a specified genotype in their phenotype under a given set of environmental conditions.\n\nThe distinction between genotype and phenotype is commonly experienced when studying family patterns for certain hereditary diseases or conditions, for example, hemophilia. Humans and most animals are diploid; thus there are two alleles for any given gene. These alleles can be the same (homozygous) or different (heterozygous), depending on the individual (see zygote). With a dominant allele, the offspring is guaranteed to inherit the trait in question irrespective of the second allele.\n\nIn the case of an albino with a recessive allele (aa), the phenotype depends upon the other allele (Aa, aA, aa or AA). An affected person mating with a heterozygous individual (Aa or aA, also carrier) there is a 50-50 chance the offspring will be albino's phenotype. If a heterozygote mates with another heterozygote, there is 75% chance passing the gene on and only a 25% chance that the gene will be displayed. A homozygous dominant (AA) individual has a normal phenotype and no risk of abnormal offspring. A homozygous recessive individual has an abnormal phenotype and is guaranteed to pass the abnormal gene onto offspring.\n\nIn the case of hemophilia, it is sex-linked thus only carried on the X chromosome. Only females can be a carrier in which the abnormality is not displayed. This woman has a normal phenotype, but runs a 50-50 chance, with an unaffected partner, of passing her abnormal gene on to her offspring. If she mated with a man with haemophilia (another carrier) there would be a 75% chance of passing on the gene.\n\n\"Genotyping\" is the process of elucidating the genotype of an individual with a biological assay. Also known as a \"genotypic assay\", techniques include PCR, DNA fragment analysis, allele specific oligonucleotide (ASO) probes, DNA sequencing, and nucleic acid hybridization to DNA microarrays or beads. Several common genotyping techniques include restriction fragment length polymorphism (\"RFLP\"), terminal restriction fragment length polymorphism (\"t-RFLP\"), amplified fragment length polymorphism (\"AFLP\"), and multiplex ligation-dependent probe amplification (\"MLPA\").\n\nDNA fragment analysis can also be used to determine such disease causing genetics aberrations as microsatellite instability (\"MSI\"), \"trisomy\" or aneuploidy, and loss of heterozygosity (\"LOH\"). MSI and LOH in particular have been associated with cancer cell genotypes for colon, breast and cervical cancer.\n\nThe most common chromosomal aneuploidy is a trisomy of chromosome 21 which manifests itself as Down syndrome. Current technological limitations typically allow only a fraction of an individual's genotype to be determined efficiently.\n\n\n", "id": "12796", "title": "Genotype"}
{"url": "https://en.wikipedia.org/wiki?curid=53703043", "text": "Sex-linked barring\n\nSex-linked barring is a plumage pattern on individual feathers in chickens, which is characterized by alternating pigmented and apigmented bars. The pigmented bar can either contain red pigment (phaeomelanin) or black pigment (eumelanin) whereas the apigmented bar is always white. The locus is therefore often referred to as an ‘eumelanin diluter’ or ‘melanin disruptor’. Typical sex-linked barred breeds include the Barred Plymouth Rock, Delaware, Old English Crele Games as well as Coucou de Renne.\n\nThe presence of a white bar on a dark background is distinguishing sex-linked barring from Autosomal barring, another plumage pattern in chickens which is created by a black bar on a light color background (white/ beige or brown) as exemplified by the breed Egyptian Fayoumi. The absence of pigment in the white bar has been attributed to a lack of pigment producing cells (melanocytes) in the feather follicle during feather growth. Initially it was proposed that this lack was the result of cell death as a consequence of the \"B\" locus mutation but later research demonstrated that the lack is the result of premature cell differentiation rather than apoptosis.\n\nMale chicken of traditional sex-linked barred breeds like the Barred Plymouth Rock usually show much wider and clearer white bands than females of the same breed. Further characteristics of sex-linked barred chickens are the dilution of skin pigment in the legs as well as a white dot at the top of the head of freshly hatched chicks which can be used for autosexing: homozygous males have a much bigger spot than hemizygous females.\n\nSex-linked barring has been established as the dominant locus \"B\" by traditional mendelian genetics in the beginning of the 20th century. The responsible gene was predicted to be located on the Z chromosome and since male birds are homogametic (ZZ), they can be either hetero- or homozygous for sex-linked barring. Females are always hemizygous at this locus (ZW).\nIn 2010 Swedish scientist have identified four mutations located in or around the tumor suppressor locus \"CDKN2A\", which appear to be associated with sex-linked barring. The four mutations are organized in 3 different alleles named \"B0\", \"B1\" and \"B2\". All alleles carry two non-coding mutation located in regulatory regions of the gene (the promoter and intron) but only \"B1\" and \"B2\" carry two additional missense mutations in a functional important domain of the protein. The \"B1\" allele is causing the typical sex-linked barring phenotype/ appearance and is present in most modern sex-linked barred chicken breeds. Females or male chickens carrying the \"B2\" allele in the heterozygous condition show a defined barring pattern but in the homozygous condition, males are essentially white with very little pigmentation. This phenotype has been initially described as a distinct but closely related mutation, however, it was later assigned to the same gene and termed ‘Sex-linked Dilution'.\n\nThe \"B0\" allele only carries the two non-coding mutations and it’s contribution to the barring pattern remained unknown as it only occurred in breeds that also carry the Dominant white mutation which is masking the effect at the \"B\" locus.). Recently scientists have removed the Dominant white mutation from chickens of those lines and were able to show that those chickens show a very light barring pattern. They named the phenotype ‘Sex-linked Extreme Dilution’. As chickens with the \"B0\" allele show the weakest barring pattern compared to those that have the coding mutation in addition, the scientists propose an evolutionary scenario in which the non-coding mutations occurred first and the two missense mutations later in time and independently. As only the combination of both non-coding and missense mutations give the desired and pronounced barring pattern, \"B0\" alone is not present in modern Sex-linked barred breeds anymore.\n\nScientists were able to show that either both or one of the non-coding mutations present in all \"B alleles\", cause an up-regulation of the activity of \"CDKN2A\". ). With more of the gene product, which is called ARF (Alternate Reading frame Protein) in the cell, more of p53 is protected from degradation. p53 is a transcription factor which in turn activates more genes involved in cell cycle regulation and apoptosis. As a consequence the cell stops dividing and starts to prematurely produce pigment.\nThe missense mutations in the \"B1\" and \"B2\" allele, however, have an opposite effect. Both missense mutations lead to a malfunctioning ARF protein, which is counteracting the effect of the higher activity of the gene to some degree. The premature production of pigment is still obvious but less strong as observed in the \"B0\" allele. It is intriguing that the missense mutation in the \"B1\" allele is much more disruptive for the protein function than the one in the \"B2\" allele and the scientists believe that this is the reason for the observed phenotypic differences between those two alleles.\n\nWhen the melanocyte progenitor cells start to migrate up from the bottom of the follicle into the barbs where they will make pigment, they further divide until a sufficient number of pigment cells is achieved. As a consequence of the up regulation of \"CDKN2A\", most cells will stop dividing and make new cells but instead start producing pigment- a black bar is emerging from the feather. But eventually there will not be enough pigment cells anymore. As they are recruited from the bottom of the feather follicle, the feather keeps on growing, creating the white bar. With the new set of pigment cells, the cyclic behavior starts again, creating alternating pigmented and apigmented bars.\n\nMutations in \"CDKN2A\" have been associated with the occurrence of familial melanoma in humans. Changes in its gene product ARF often cause the cell to lose their ability for self-induced cell death or cell cycle arrest, which are mechanisms of cells to manage uncontrolled cell divisions and therefore the occurrence of cancer. It is intriguing that chickens carrying the \"B1\" or \"B2\" allele with a malfunctioning ARF do not show any higher prevalence to any type of cancer and are usually considered very sturdy and easy to keep breeds. It is also astonishing that the majority of the egg and meat production industry is relying on chickens, which have a defect in a tumor suppressor gene.\n", "id": "53703043", "title": "Sex-linked barring"}
{"url": "https://en.wikipedia.org/wiki?curid=44284", "text": "Noncoding DNA\n\nIn genomics and related disciplines, noncoding DNA sequences are components of an organism's DNA that do not encode protein sequences. Some noncoding DNA is transcribed into functional non-coding RNA molecules (e.g. transfer RNA, ribosomal RNA, and regulatory RNAs). Other functions of noncoding DNA include the transcriptional and translational regulation of protein-coding sequences, scaffold attachment regions, origins of DNA replication, centromeres and telomeres.\n\nThe amount of noncoding DNA varies greatly among species. Often, only a small percentage of the genome is responsible for coding proteins, but a rising percentage is being shown to have regulatory functions. When there is much non-coding DNA, a large proportion appears to have no biological function, as predicted in the 1960s. Since that time, this non-functional portion has controversially been called junk DNA.\n\nThe international Encyclopedia of DNA Elements (ENCODE) project uncovered, by direct biochemical approaches, that at least 80% of human genomic DNA has biochemical activity. Though this was not necessarily unexpected due to previous decades of research discovering many functional noncoding regions, some scientists criticized the conclusion for conflating biochemical activity with biological function. \nEstimates for the biologically functional fraction of our genome based on comparative genomics range between 8 and 15%. However, others have argued against relying solely on estimates from comparative genomics due to its limited scope. Non-coding DNA has been found to be involved in epigenetic activity and complex networks of genetic interactions, and is being explored in evolutionary developmental biology.\n\nThe amount of total genomic DNA varies widely between organisms, and the proportion of coding and noncoding DNA within these genomes varies greatly as well. For example, it was originally suggested that over 98% of the human genome does not encode protein sequences, including most sequences within introns and most intergenic DNA, whilst 20% of a typical prokaryote genome is noncoding.\n\nWhile overall genome size, and by extension the amount of noncoding DNA, are correlated to organism complexity, there are many exceptions. For example, the genome of the unicellular \"Polychaos dubium\" (formerly known as \"Amoeba dubia\") has been reported to contain more than 200 times the amount of DNA in humans. The pufferfish \"Takifugu rubripes\" genome is only about one eighth the size of the human genome, yet seems to have a comparable number of genes; approximately 90% of the \"Takifugu\" genome is noncoding DNA. The extensive variation in nuclear genome size among eukaryotic species is known as the C-value enigma or C-value paradox. Most of the genome size difference appears to lie in the noncoding DNA.\n\nIn 2013, a new \"record\" for the most efficient eukaryotic genome was discovered with \"Utricularia gibba\", a bladderwort plant that has only 3% noncoding DNA and 97% of coding DNA. Parts of the noncoding DNA were being deleted by the plant and this suggested that noncoding DNA may not be as critical for plants, even though noncoding DNA is useful for humans. Other studies on plants have discovered crucial functions in portions of noncoding DNA that were previously thought to be negligible and have added a new layer to the understanding of gene regulation.\n\nNoncoding RNAs are functional RNA molecules that are not translated into protein. Examples of noncoding RNA include ribosomal RNA, transfer RNA, Piwi-interacting RNA and microRNA.\n\nMicroRNAs are predicted to control the translational activity of approximately 30% of all protein-coding genes in mammals and may be vital components in the progression or treatment of various diseases including cancer, cardiovascular disease, and the immune system response to infection.\n\nCis-regulatory elements are sequences that control the transcription of a nearby gene. Many such elements are involved in the evolution and control of development. Cis-elements may be located in 5' or 3' untranslated regions or within introns. Trans-regulatory elements control the transcription of a distant gene.\n\nPromoters facilitate the transcription of a particular gene and are typically upstream of the coding region. Enhancer sequences may also exert very distant effects on the transcription levels of genes.\n\nIntrons are non-coding sections of a gene, transcribed into the precursor mRNA sequence, but ultimately removed by RNA splicing during the processing to mature messenger RNA. Many introns appear to be mobile genetic elements.\n\nStudies of group I introns from \"Tetrahymena\" protozoans indicate that some introns appear to be selfish genetic elements, neutral to the host because they remove themselves from flanking exons during RNA processing and do not produce an expression bias between alleles with and without the intron. Some introns appear to have significant biological function, possibly through ribozyme functionality that may regulate tRNA and rRNA activity as well as protein-coding gene expression, evident in hosts that have become dependent on such introns over long periods of time; for example, the \"trnL-intron\" is found in all green plants and appears to have been vertically inherited for several billions of years, including more than a billion years within chloroplasts and an additional 2–3 billion years prior in the cyanobacterial ancestors of chloroplasts.\n\nPseudogenes are DNA sequences, related to known genes, that have lost their protein-coding ability or are otherwise no longer expressed in the cell. Pseudogenes arise from retrotransposition or genomic duplication of functional genes, and become \"genomic fossils\" that are nonfunctional due to mutations that prevent the transcription of the gene, such as within the gene promoter region, or fatally alter the translation of the gene, such as premature stop codons or frameshifts. Pseudogenes resulting from the retrotransposition of an RNA intermediate are known as processed pseudogenes; pseudogenes that arise from the genomic remains of duplicated genes or residues of inactivated genes are nonprocessed pseudogenes.\n\nWhile Dollo's Law suggests that the loss of function in pseudogenes is likely permanent, silenced genes may actually retain function for several million years and can be \"reactivated\" into protein-coding sequences and a substantial number of pseudogenes are actively transcribed. Because pseudogenes are presumed to change without evolutionary constraint, they can serve as a useful model of the type and frequencies of various spontaneous genetic mutations.\n\nTransposons and retrotransposons are mobile genetic elements. Retrotransposon repeated sequences, which include long interspersed nuclear elements (LINEs) and short interspersed nuclear elements (SINEs), account for a large proportion of the genomic sequences in many species. Alu sequences, classified as a short interspersed nuclear element, are the most abundant mobile elements in the human genome. Some examples have been found of SINEs exerting transcriptional control of some protein-encoding genes.\n\nEndogenous retrovirus sequences are the product of reverse transcription of retrovirus genomes into the genomes of germ cells. Mutation within these retro-transcribed sequences can inactivate the viral genome.\n\nOver 8% of the human genome is made up of (mostly decayed) endogenous retrovirus sequences, as part of the over 42% fraction that is recognizably derived of retrotransposons, while another 3% can be identified to be the remains of s. Much of the remaining half of the genome that is currently without an explained origin is expected to have found its origin in transposable elements that were active so long ago (> 200 million years) that random mutations have rendered them unrecognizable. Genome size variation in at least two kinds of plants is mostly the result of retrotransposon sequences.\n\nTelomeres are regions of repetitive DNA at the end of a chromosome, which provide protection from chromosomal deterioration during DNA replication.\n\nThe term \"junk DNA\" became popular in the 1960s. According to T. Ryan Gregory, a genomic biologist, David Comings was the first to discuss the nature of junk DNA explicitly in 1972, and he applied the term to all noncoding DNA. The term was formalized in 1972 by Susumu Ohno, who noted that the mutational load from deleterious mutations placed an upper limit on the number of functional loci that could be expected given a typical mutation rate. Ohno hypothesized that mammal genomes could not have more than 30,000 loci under selection before the \"cost\" from the mutational load would cause an inescapable decline in fitness, and eventually extinction. This prediction remains robust, with the human genome containing approximately 20,000 genes. Another source for Ohno's theory was the observation that even closely related species can have widely (orders-of-magnitude) different genome sizes, which had been dubbed the C-value paradox in 1971.\n\nThough the fruitfulness of the term \"junk DNA\" has been questioned on the grounds that it provokes a strong \"a priori\" assumption of total non-functionality and though some have recommended using more neutral terminology such as \"noncoding DNA\" instead; \"junk DNA\" remains a label for the portions of a genome sequence for which no discernible function has been identified and that through comparative genomics analysis appear under no functional constraint suggesting that the sequence itself has provided no adaptive advantage. Since the late 70s it has become apparent that the majority of non-coding DNA in large genomes finds its origin in the selfish amplification of transposable elements, of which W. Ford Doolittle and Carmen Sapienza in 1980 wrote in the journal \"Nature\": \"When a given DNA, or class of DNAs, of unproven phenotypic function can be shown to have evolved a strategy (such as transposition) which ensures its genomic survival, then no other explanation for its existence is necessary.\" The amount of junk DNA can be expected to depend on the rate of amplification of these elements and the rate at which non-functional DNA is lost. In the same issue of \"Nature\", Leslie Orgel and Francis Crick wrote that junk DNA has \"little specificity and conveys little or no selective advantage to the organism\". The term occurs mainly in popular science and in a colloquial way in scientific publications, and it has been suggested that its connotations may have delayed interest in the biological functions of noncoding DNA.\n\nSeveral lines of evidence indicate that some \"junk DNA\" sequences are likely to have unidentified functional activity and that the process of exaptation of fragments of originally selfish or non-functional DNA has been commonplace throughout evolution. In 2012, the ENCODE project, a research program supported by the National Human Genome Research Institute, reported that 76% of the human genome's noncoding DNA sequences were transcribed and that nearly half of the genome was in some way accessible to genetic regulatory proteins such as transcription factors.\n\nHowever, the suggestion by ENCODE that over 80% of the human genome is biochemically functional has been criticized by other scientists, who argue that neither accessibility of segments of the genome to transcription factors nor their transcription guarantees that those segments have biochemical function and that their transcription is selectively advantageous. Furthermore, the much lower estimates of functionality prior to ENCODE were based on genomic conservation estimates across mammalian lineages.\n\nIn response to such views, other scientists argue that the wide spread transcription and splicing that is observed in the human genome directly by biochemical testing is a more accurate indicator of genetic function than genomic conservation because conservation estimates are relative due to incredible variations in genome sizes of even closely related species, it is partially tautological, and these estimates are not based on direct testing for functionality on the genome. Conservation estimates may be used to provide clues to identify possible functional elements in the genome, but it does not limit or cap the total amount of functional elements that could possibly exist in the genome since elements that do things at the molecular level can be missed by comparative genomics. Furthermore, much of the apparent junk DNA is involved in epigenetic regulation and appears to be necessary for the development of complex organisms.\n\nIn a 2014 paper, ENCODE researchers tried to address \"the question of whether nonconserved but biochemically active regions are truly functional\". They noted that in the literature, functional parts of the genome have been identified differently in previous studies depending on the approaches used. There have been three general approaches used to identify functional parts of the human genome: genetic approaches (which rely on changes in phenotype), evolutionary approaches (which rely on conservation) and biochemical approaches (which rely on biochemical testing and was used by ENCODE). All three have limitations: genetic approaches may miss functional elements that do not manifest physically on the organism, evolutionary approaches have difficulties using accurate multispecies sequence alignments since genomes of even closely related species vary considerably, and with biochemical approaches, though having high reproducibility, the biochemical signatures do not always automatically signify a function.\n\nThey noted that 70% of the transcription coverage was less than 1 transcript per cell. They noted that this \"larger proportion of genome with reproducible but low biochemical signal strength and less evolutionary conservation is challenging to parse between specific functions and biological noise\". Furthermore, assay resolution often is much broader than the underlying functional sites so some of the reproducibly “biochemically active but selectively neutral” sequences are unlikely to serve critical functions, especially those with lower-level biochemical signal. To this they added, \"However, we also acknowledge substantial limitations in our current detection of constraint, given that some human-specific functions are essential but not conserved and that disease-relevant regions need not be selectively constrained to be functional.\" On the other hand, they argued that the 12–15% fraction of human DNA under functional constraint, as estimated by a variety of extrapolative evolutionary methods, may still be an underestimate. They concluded that in contrast to evolutionary and genetic evidence, biochemical data offer clues about both the molecular function served by underlying DNA elements and the cell types in which they act. Ultimately genetic, evolutionary, and biochemical approaches can all be used in a complementary way to identify regions that may be functional in human biology and disease.\n\nSome critics have argued that functionality can only be assessed in reference to an appropriate null hypothesis. In this case, the null hypothesis would be that these parts of the genome are non-functional and have properties, be it on the basis of conservation or biochemical activity, that would be expected of such regions based on our general understanding of molecular evolution and biochemistry. According to these critics, until a region in question has been shown to have additional features, beyond what is expected of the null hypothesis, it should provisionally be labelled as non-functional.\n\nMany noncoding DNA sequences must have some important biological function. This is indicated by comparative genomics studies that report highly conserved regions of noncoding DNA, sometimes on time-scales of hundreds of millions of years. This implies that these noncoding regions are under strong evolutionary pressure and positive selection. For example, in the genomes of humans and mice, which diverged from a common ancestor 65–75 million years ago, protein-coding DNA sequences account for only about 20% of conserved DNA, with the remaining 80% of conserved DNA represented in noncoding regions. Linkage mapping often identifies chromosomal regions associated with a disease with no evidence of functional coding variants of genes within the region, suggesting that disease-causing genetic variants lie in the noncoding DNA. The significance of noncoding DNA mutations in cancer was explored in April 2013.\n\nNoncoding genetic polymorphisms play a role in infectious disease susceptibility, such as hepatitis C. Moreover, noncoding genetic polymorphisms contribute to susceptibility to Ewing sarcoma, an aggressive pediatric bone cancer.\n\nSome specific sequences of noncoding DNA may be features essential to chromosome structure, centromere function and homolog recognition in meiosis.\n\nAccording to a comparative study of over 300 prokaryotic and over 30 eukaryotic genomes, eukaryotes appear to require a minimum amount of non-coding DNA. The amount can be predicted using a growth model for regulatory genetic networks, implying that it is required for regulatory purposes. In humans the predicted minimum is about 5% of the total genome.\n\nOver 10% of 32 mammalian genomes may function through the formation of specific RNA secondary structures. The study used comparative genomics to identify compensatory DNA mutations that maintain RNA base-pairings, a distinctive feature of RNA molecules. Over 80% of the genomic regions presenting evolutionary evidence of RNA structure conservation do not present strong DNA sequence conservation.\n\nNoncoding DNA separate genes from each other with long gaps, so mutation in one gene or part of a chromosome, for example deletion or insertion, does not have a frameshift effect on the whole chromosome. When genome complexity is relatively high, like in the case of human genome, not only between different genes, but also inside many genes, there are gaps of introns to protect the entire coding segment and minimise the changes caused by mutation. Non-coding DNA may perhaps serve to decrease the probability of gene disruption during chromosomal crossover.\n\nSome noncoding DNA sequences determine the expression levels of various genes, both those that are transcribed to proteins and those that themselves are involved in gene regulation.\n\nSome noncoding DNA sequences determine where transcription factors attach. A transcription factor is a protein that binds to specific non-coding DNA sequences, thereby controlling the flow (or transcription) of genetic information from DNA to mRNA.\n\nAn operator is a segment of DNA to which a repressor binds. A repressor is a DNA-binding protein that regulates the expression of one or more genes by binding to the operator and blocking the attachment of RNA polymerase to the promoter, thus preventing transcription of the genes. This blocking of expression is called repression.\n\nAn enhancer is a short region of DNA that can be bound with proteins (trans-acting factors), much like a set of transcription factors, to enhance transcription levels of genes in a gene cluster..\n\nA silencer is a region of DNA that inactivates gene expression when bound by a regulatory protein. It functions in a very similar way as enhancers, only differing in the inactivation of genes.\n\nA promoter is a region of DNA that facilitates transcription of a particular gene when a transcription factor binds to it. Promoters are typically located near the genes they regulate and upstream of them.\n\nA genetic insulator is a boundary element that plays two distinct roles in gene expression, either as an enhancer-blocking code, or rarely as a barrier against condensed chromatin. An insulator in a DNA sequence is comparable to a linguistic word divider such as a comma in a sentence, because the insulator indicates where an enhanced or repressed sequence ends.\n\nShared sequences of apparently non-functional DNA are a major line of evidence of common descent.\n\nPseudogene sequences appear to accumulate mutations more rapidly than coding sequences due to a loss of selective pressure. This allows for the creation of mutant alleles that incorporate new functions that may be favored by natural selection; thus, pseudogenes can serve as raw material for evolution and can be considered \"protogenes\".\n\nA statistical distinction between coding and noncoding DNA sequences has been found. It has been observed that nucleotides in non-coding DNA sequences display long range power law correlations while coding sequences do not.\n\nPolice sometimes gather DNA as evidence for purposes of forensic identification. As described in \"Maryland v. King\", a 2013 U.S. Supreme Court decision:\n\n\n", "id": "44284", "title": "Noncoding DNA"}
{"url": "https://en.wikipedia.org/wiki?curid=49129093", "text": "CDX4 (gene)\n\nHomeobox protein CDX-4 is a protein that in humans is encoded by the CDX4 gene. This gene is a member of the caudal-related homeobox transcription factor family that also includes CDX1 and CDX2.\n\nThe transcription factor encoded by the CDX4 gene participates in the formation of extra-embryonic tissues, anterior-posterior patterning and blood formation during embryogenesis. It does so through the regulation of Hox gene expression. \n\nBefore placentation takes place, CDX4 plays a role in its development. CDX4 mutants are born healthy and are fertile, however its importance is revealed in compound CDX mutants. Compound mutants carrying one CDX2 null allele and homozygous null for CDX4 fail to generate posterior tissue caudal to the hindlimbs and most of these embryos die around embryonic day 10.5 from lack of placental development. Around 10% of this phenotype may progress to full term, but then die shortly after birth. Upon inspection the morphogenesis of ano-rectal and urethral tissues was observed. \n\nThe most well described function of CDX genes are their role in caudal body formation. Transcription factors of the CDX gene family, in part control Hox gene expression by responding to signaling molecules Retinoic Acid, Wnt, and FGF. The redundant contribution of CDX4 in axial elongation is shown in that neither CDX4 null or CDX1/CDX4 compound mutants appear with impaired axial elongation. However, CDX4 does have a role in determining pancreatic B-cell number, specifying anterior-posterior location of the foregut organs including the pancreas and liver. Thus, an abnormal state is shown in embryos deficient in CDX4 by posteriorly shifted pancreas, liver and small intestines. \n\nIn blood formation, CDX4 regulation of Hox genes is necessary for the specification of hematopoietic cell fate during embryogenesis. This is demonstrated by the fact that blood deficiencies in CDX4 mutants can be rescued by the over expression of certain Hox genes. \n\nKnockout models have been generated in mice as described in CDX4’s role in caudal body formation.\n", "id": "49129093", "title": "CDX4 (gene)"}
{"url": "https://en.wikipedia.org/wiki?curid=8564084", "text": "Numt\n\nNUMT, pronounced \"new might,” is an acronym for “\"nu\"clear \"m\"i\"t\"ochondrial DNA segment” coined by molecular evolutionary biologist, Jose V. Lopez, which describes a transposition of any type of cytoplasmic mitochondrial DNA into the nuclear genome of the eukaryotic organisms. More and more NUMT sequences, with different size and length, in the diverse number of Eukaryotes, have been detected as more whole genome sequencing of different organisms accumulate. In fact, NUMTs have often been unintentionally discovered by researchers who were looking for mtDNA. NUMTs have been reported in all studied eukaryotes, and nearly all mitochondrial genome regions can be integrated into the nuclear genome. However, NUMTs differ in number and size across different species. Such differences may be accounted for by interspecific variation in such factors as germline stability and mitochondria number.\n\nAfter the release of the mtDNA to the cytoplasm, due to the mitochondrial alteration and morphological changes, mtDNA is transferred into the nucleus by one of the various predicted methods and are eventually inserted by double-stranded break repair processes into the nuclear DNA. Not only has any correlation been found between the fraction of noncoding DNA and NUMT abundance in the genome but NUMTs are also proven to have non-random distribution and a higher likelihood of being inserted in the certain location of genome compare to others. Depending on the location of the insertion, NUMTs might perturb the function of the genes. In addition, De novo integration of NUMT pseudogenes into the nuclear genome has an adverse effect in some cases, promoting various disorders and aging. The presence of NUMT fragments in the genome is not problematic in all species; for instance, it is shown that sequences of mitochondrial origin promote nuclear DNA replication in \"Saccharomyces cerevisiae\". Although, the extended translocation of mtDNA fragments and their co-amplification with free mitochondrial DNA has been problematic in the diagnosis of mitochondrial disorders, in the study of population genetics, and phylogenetic analyses, scientists have used NUMTs as the genetic markers to figure out the relative rate of nuclear and mitochondrial mutation and recreating the evolutionary tree.\n\nMitochondria, as a major energy factory of the cell, was previously a free-living prokaryote that invades the eukaryotic cells by the endosymbiosis theory, which gained acceptance around the 1970s. Under this theory, symbiotic organelles gradually transferred their genes to the eukaryotic genome, implying that mtDNA was gradually integrated into the nuclear genome. Despite the metabolic alterations and functional adaptations in the host eukaryotes, circular mitochondrial DNA is contained within the organelles. Containing 37 genes, mitochondrial DNA has an essential role in the production of necessary compounds, such as required enzymes for the proper function of mitochondria. Specifically, it has been suggested that certain genes (such as the genes for cytochrome oxidase subunits I and II) within the organelle are necessary to regulate redox balance throughout membrane-associated electron transport chains. These parts of the mitochondrial genome have been reported to be the most frequently employed. Mitochondria is not the only location within which the cell mtDNA, mitochondrial DNA, can be found; sometimes transfer of mitochondrial DNA from organelles to the nucleus can occur; the evidence of such translocation has been seen through the comparison of mitochondrial DNA sequence with the genome sequence of the counterparts. The integration and recombination of cytoplasmic mtDNA into the nuclear DNA is called Nuclear Mitochondrial DNA, which is abbreviated as NUMT. \nThe possible presence of organelle DNA inside the nuclear genome was suggested after finding of homologous structure to the mitochondrial DNA, which was shortly after the discovery of the presence of an independent DNA within the organelles in 1967. This topic stayed untouched until the 1980s. Initial evidence that DNA could move among cell compartments came when fragments of chloroplast DNA were found in the maize mitochondrial genome with the help of cross-hybridization, between chloroplast and mitochondrial DNA, and physical mapping of homologous regions. After this initial observation, Ellis coined the name \"promiscuous DNA\" in order to signify the transfer of DNA intracellularly from one organelle to the other and is the presence of organelle DNA in multiple cellular compartments. This is not only an important discovery on its own, but is also highly informative and helpful for understanding the evolutionary process and the time period different occurrence might take place. The searching for mtDNA in nuclear DNA continued until 1994 when the recent remarkable transposition of 7.9 kb of a typically 17.0-kb mitochondrial genome to a specific nuclear chromosomal position in the domestic cat was reported. This is the time that NUMT was coined to designate the large stretches of mitochondrial DNA in the genome.\n\nUp to now, the whole genomes of many eukaryotes, both vertebrate, and invertebrate, have been sequenced and NUMT was observed in the nuclear genome of various organisms, including yeast, \"Podospora\", sea urchin, locust, honey bee, \"Tribolium\", rat, maize, rice, and primates. In Plasmodium, \"Anopheles gambiae\", and \"Aedes aegypti\" mosquitoes NUMT can barely be detected. In contrast, the conserved fragments of NUMT have now few were identified in genome data for \"Ciona intestinalis\", \"Neurospora crassa\", \"Schizosaccharomyces pombe\", \"Caenorhabditis elegans\", \"Drosophila melanogaster\", and \"Rattus norvegicus\". Antunes and Ramos were found the presence of NUMT in the fish genome for the first time in 2005 using of BLASTN, MAFFT, very vigorous genome mappings, and phylogenic analysis. Across the animal kingdom, \"Apis mellifera\", from phylum \"Arthropoda\", and \"Hydra magnipapillata\", from phylum \"Cnidaria\", are respectively the first and second animals with the highest ratio of NUMTs to the total size of the nuclear genome while \"Monodelphis Domestica\", or Gray short-tailed opossum, is the record holder for NUMT frequency among vertebrates. Similar to animals, NUMTs are abundant in the plants and the longest NUMT fragment known so far, a 620-kb partially duplicated insertion of the 367-kb mtDNA of \"Arabidopsis thaliana\", was reported.\n\nNUMT insertion into the nuclear genome and its persistence in the nuclear genome initiated by physical delivery of mitochondrial DNA to the nucleus. This step follows by the mtDNA integration into the genome through a non-homologous end joining mechanism during double-strand break repair process as envisioned by studying baker’s yeast, Saccharomyces Cerevisiae; and terminates by intragenomic dynamics of amplification, mutation, or deletion, which also known as post-insertion modifications. The mechanism of mtDNA transfer into nucleus has not yet fully understood.\n\nTransfer of the released mtDNA into the nucleus: The first step in the transferring process is the release of mtDNA into the cytoplasm. Thorsness and Fox demonstrated the rate of relocation of mtDNA from mitochondria into the nucleus using \"ura3- \" yeast strain with an engineered \"URA3 \"plasmid, required gene for uracil biosynthesis, in the mitochondria. During the propagation of such yeast strains carrying a nuclear \"ura3 \" mutation, plasmid DNA that escapes from the mitochondrion to the nucleus, complements the uracil biosynthetic defect, restoring growth in the absence of uracil, and easily scored phenotype. The rate of DNA transfer from the mitochondria to the nucleus was estimated as 2 x 10-5 per cell per generation while the opposite, in the case of \"cox2\" mutant, the rate of the transfer of plasmid from the nucleus to the mitochondria is apparently at least 100,000 times less. Many factors control the rate of mtDNA escapes from mitochondria to the nucleus. The higher rate of mutation in mtDNA in comparison with nDNA in the cells of many organisms is an important factor promoting the transfer of mitochondrial genes into the nuclear genome. One of the intergenic factors results in the higher destruction rate of mitochondrial macromolecules, including mtDNA, is the presence of high level of reactive oxygen species (ROS), generated in mitochondria as the by-products in ATP synthesis mechanism. Some other factors influencing the escape of mtDNA from mitochondria include the action of mutagenic agents and other forms of cellular stress that can damage mitochondria or their membranes, which proves that is possible to assume that exogenous damaging agents (ionizing radiation and chemical genotoxic agents) increase the rate of mtDNA escape into the cytoplasm. Thorsness and Fox continued their research to find the endogenous factors affecting mtDNA escape into the nucleus. They isolated and studied 21 nuclear mutants with different combinations of mutations in at least 12 nuclear loci called the \"yme\" (yeast mitochondrial escape) mutations, in different environmental conditions since some of these mutations cause temperature sensitivity. They found out these mutations which perturb mitochondrial functions, due to the alteration of gene products, affect mitochondrial integrity and led to mtDNA escape to the cytoplasm. Additionally, defects in the proteins change the rate of mtDNA transfer into the nucleus. For instance, in the case of \"yme1 \"mutant, abnormal mitochondria are targeted for degradation by the vacuole, with the help of \"pep4 \", a major proteinase, and degradation increases mtDNA escape to the nucleus through the process of mitophagy. In addition, Thorsness and Campbell found that by disruption of \"pep4\", the frequency of mtDNA escape in \"yme1\" strains decreases. Similarly, the disruption of \"PRC1\", which encodes carboxypeptidase Y, lowers the rate of mtDNA escape in \"yme1\" yeast. Evidence shows that mitophagy is one of the possible ways for mtDNA transfer into the nucleus and determined to be the most supported pathway up to now. Some other possible pathways are shown in figure 1. The first pathway, as it was explained, is a \"yme1\"mutant that results in inactivation of \"YMe1p\" protein, a mitochondrial-localized ATP-dependent metalloproteinase, leading to high escape rate of mtDNA to the nucleus. Mitochondria of \"yme1\" strain are taken up for degradation by the vacuole more frequently than the wild-type strain. Moreover, cytological investigations have suggested several other possible pathways in the diverse number of species, including a lysis of the mitochondrial compartment, direct physical connection and membrane fusion between mitochondria and nucleus, and encapsulation of mitochondrial compartments inside the nucleus, as shown in figure 1.\n\nPre-insertion preparation: After reaching the nucleus, mtDNA has to enter the nuclear genome. The rate of mtDNA incorporation into the nuclear genome can be expected to depend on the DSB number in nDNA, the activity of DSB repair systems, and the rate of mtDNA escape from organelles. MtDNA insertion comprises three main processes, shown in figure 2; first, the mtDNA has to have the proper form and sequence; in other words, the mtDNA has to be edited which gives a rise to the new edited site in the polynucleotide structure. Mitochondrial DNA is not universal and, in animals similar to plants, mitochondrial editing shows very erratic patterns of taxon-specific occurrence. As shown in figure 2, there are three possible ways that mtDNA can become prepared to be inserted into the nuclear DNA. The process mainly depends on the time mtDNA transfers into the nucleus. As shown in figure 2b, direct integration of unedited mtDNA fragments into the nuclear genomes is the most plausible and the evidence both found in plants, Arabidopsis genome, and animals with the help of different methods, including BLAST-based analysis. In this case, mtDNA is transferred into the nucleus whereby editing and introns arise in the mitochondrion later. If a gene, for instance, was transferred to the nucleus in one lineage before mitochondrial editing evolved, but remained in the organelle in other lineages where editing arose, the nuclear copy would appear more similar to an edited transcript than to the remaining mitochondrial copies at the edited sites. Another represented and less supported model, figure 2a, is the cDNA-mediated model, which intron-contained mtDNA enters the nucleus and by reverse transcription of spliced and edited mitochondrial transcript, it becomes integrated into the nDNA. The third proposed mechanism is the direct transfer and integration of intronless mtDNA into the nucleus, figure 2c, whereby editing and introns in the mitochondrion come and go during evolution. In this case, the introduction and removal of the intron, as well as, reverse transcription occur within mitochondria and the final product, the edited intronless mtDNA, will integrate into nDNA after being transferred into the nucleus.\n\nInsertion into the nuclear genome:After the preparatory step is over, mtDNA is ready to be inserted into the nuclear genome. Based on NUMT integration site and the analyzed obtained results from baker’s yeast experiment, Blanchard and Schmidt hypothesized that mtDNA are inserted into the double-stranded break (DSB) via non-homologous end joining machinery. The hypothesis is found to be widely accepted. Later analyses were consistent with the involvement of NHEJ in NUMT integration in humans. These processes occur in both somatic and germline cells. In animals and humans, however, the capability of DSB repair in germline cells depends on the oogenetic and spermatogenetic stage, nonetheless, due the low repair activity, mature sperms are incapable of DSB repair. Additionally, DSB can also be repaired by homologous recombination (HR), which is more accurate and introduces fewer errors in the process of repair, while, has not yet seen in the process of mtDNA insertion;. Apart from canonical NHEJ, DSBs are repaired via a mechanism that involves sequences containing a few homologous nucleotides at the ends of a DSB to be ligated. This mechanism is known as microhomology-mediated end joining abbreviated as MMEJ. MMEJ is the most mutagenic DSB repair mechanism due to generating deletions, insertion of various sizes, and other genome rearrangements in mammalians. As shown in figure 3, the processes of mtDNA insertion and DSB repair include few steps which are DNA segment alignment, DNA end-processing, DNA synthesis, and ligation. In each step, certain protein complexes are required to facilitate the occurrence of the indicated events. As shown in figure 3, in NHEJ, the \"Ku70/Ku80\" heterodimer and DNA-dependent protein kinase \"(DNA-PK)\", for bringing DNA fragments end together, the Artemis nuclease and polynucleotide kinase 3' phosphatase \" (PNKP) \", for the end processing, X family DNA polymerases \" (Pol μ and Pol λ) \" and terminal deoxynucleotidyl transferase \" (TdT) \", for DNA synthesis, and \"XLF/XRCC4/LigIV \" complex, for completing the repair and joining the ends via a phosphodiester bond, are the protein complexes involved in DSB repair process in many higher organisms. DNA polymerases \" (Pol μ and Pol λ) \" and \"XLF/XRCC4/LigIV \"complex are shared between two NHEJ and MMEJ repair machinery and have the same responsibility in both repair processes. The first step of MMEJ is done by \"WRN \", Artemis, \"DNA-PK \", and \"XRCC4\" protein complexes which process the ends of DSB and mtDNA fragments in addition to aligning them in order for polymerases and ligases to be able to complete NUMT insertion (figure 3).\n\nPost-insertion modification:The complex pattern of NUMT in comparison with the single mitochondrial piece, the appearance of non-continuous mitochondrial DNA in the nuclear genome, and possibly, different orientation of these fragments are the evidence of post-insertion processes of NUMT within the nuclear genome. The causation of these complex patterns might be the result of multiple NUMT insertions at insertional hotspots. In addition, duplication after insertion contributes to NUMT diversity. NUMTs have no self-replicating mechanism or transposition mechanism; therefore, NUMT duplication is expected to occur in tandem or to involve larger segmental duplication at rates representative of the rest of the genome. Evidence for NUMT duplications that are not in proximity to other NUMTs is present in many genomes and probably happens as part of segmental duplication. However, duplications of recent human-specific NUMTs as part of segmental duplication seem to be rare; in humans, only a few NUMTs are found to have overlap with segmental duplication, and those NUMTs were found in only one of the copies while missing from the others, clearly demonstrating that the NUMTs were inserted subsequent to the duplication events. Deletion is another NUMT post-insertional modification method that has not yet been studied in the same amount of detail as an insertion. Constant erosion of phylogenic signals and high mutation rate in animal mtDNA make recognition of such modification, especially deletion, difficult. Studying the cases in which the presence–absence pattern of NUMTs does not agree with the phylogenetic tree, should make detection of recent NUMT losses possible by the means of using multiple genome alignments with the presence of an outgroup. Bensasson and his team members used this method to estimate the oldest inserted NUMT in human, which dated around 58 million years ago.\n\nAs the number of mitochondria and their functional level differs across eukaryotic organisms, the length, structure, and sequence of NUMTs vary dramatically. Researchers have found that the recent NUMT insertions are derived from different segments of the mitochondrial genome, including the D-loop and, in some extreme cases, a number of, nearly, the full-length mitochondrial genome. The sequence, frequency, size distribution, and even the difficulties of finding these sequences in the genome vary substantially among species. The majority of DNA fragments transferred from mitochondria and plastids into the nuclear genome are less than 1 kb in size. Yet, extremely large fragments of organelle DNA are found in some the plant genomes.\n\nAs the genome evolves and alters over time by mutation, the number of NUMT in the genome differs over the course of evolution. NUMT enters the nucleus and inserts in the nDNA at different stages of the time. Due to constant mutation and instability of NUMT, the resemblance of this genome stretch to the mtDNA varies widely across the kingdom Animalia and even within the certain genome. For instance, the latest number of NUMT recorded in the human genome is 755 fragments which range from 39 bp to almost the entire mitochondrial sequence in size. There are 33 paralogous sequences with over 80% sequence similarity and of a greater length than 500 bp. Moreover, not all the NUMT fragments in the genome are the result of mtDNA migration; some are the outcome of amplification after insertion. Old NUMTs are found to be more abundant in the human genome than the recent integrants, indicating that mtDNA can be amplified once inserted. Dayama used the high yield new technique for the exact detection of the number of NUMT in the human genome is called \"dinumt\". This method enables him and his team members to identify NUMT insertions, of all sizes, in the whole genomes sequenced using paired-end sequencing technology with a greater sensitivity. They applied \"dinumt \"to 999 individuals from the 1000 Genomes and Human Genome Diversity Project (HGDP) projects and conducted an updated enrichment analysis in humans using these polymorphic insertions. Further investigation and genotyping of the discovered NUMT also proves age, origin, and sequence characteristics, and assessed their potential impact on ongoing studies of mitochondrial heteroplasmy.\n\nAs previously mentioned, mtDNA is inserted into the nuclear genome only when a DSB is produced by endogenous or exogenous damaging factors. However, mtDNA is not inserted at any location within the genome. Moreover, there is no correlation between the fraction of noncoding DNA and NUMT abundance; In addition, Antunes and Ramos found that old NUMTs are inserted preferentially into the known and predicted loci, as inferred for recent NUMTs in the human genome, during their vigorous work on NUMT sequence in fishes using BLASTN analysis method. Therefore, based on these studies, the insertion of NUMT in nuclear genome is found to be non-random. One of the best studies proving the non-random distribution and insertion of NUMTs in the nuclear genome is done by Tsuji and his teammates. Using the LAST method instead of BLAST, which makes computing E-value with higher accuracy possible and does not under-represent the repetitive elements in NUMT flanks, Tsuji and his teammate became able to characterize the location of NUMT insertion precisely. They found out that NUMT fragments tend to be inserted in the regions with high local DNA curvature or bendability and high A+T rich oligomers, especially TAT. Moreover, NUMTs are mostly inserted into open chromatin regions. Using the same method, Tsuji showed that NUMTs are not usually clustered together and the NUMTs produced by D-loop are usually under-represented which evident more vividly in monkey and human compare to rats and mouse due to the total length of their NUMTs. However Tsuji also found that retrotransposon structure is highly enriched in NUMT flanks and most NUMTs are inserted in close proximity of retrotransposon while only a few, 10 out of 557 NUMTs, were inserted within a retrotransposon, they could not find any clear relation the size of non-coding DNA and the number of NUMT.\n\nNUMTs are not utterly functionless and certain functions are being associated with them. Although the insertion of NUMTs was previously considered functionless pseudogenes, recent human NUMTs are shown to be a potentially mutagenic process that could damage the functional integrity of the human genome. The accumulation of mutation in NUMT, post-insertional alteration, mutagenic mechanism of NUMT insertion, MMEJ and NHEJ, DSB, as well as the place in which insertion hot spot is located can cause mutation and dramatic alterations of the genome structure at the integration site, interfere with the function of the genome, and exert substantial effects on the expression of genetic information. Moreover, Integration of mtDNA sequences substantially affects the spatial organization of nDNA and may play an important role in the evolution of eukaryotic genomes. In addition to the negative effect of mtDNA, those conserved old NUMTs in the genome are likely to represent evolutionary successes and they should be considered as a potential evolutionary mechanism for the enhancement of genomic coding regions. Moreover, Chatre and Ricchetti with the utilization of Two-dimensional gel electrophoresis, plasmid construct, mutagenesis, in a sillico analysis of ACS motifs, and plasmid loss rate assay found that migratory mitochondrial DNAs can impact the replication of the nuclear region in which they are inserted. Through their functional evidence, they showed that sequences of mitochondrial origin promote nDNA replication in \"Saccharomyces cerevisiae \". NUMTs are rich 11-bp ARS core-A consensus sequence (ACS), which its presence in the matches to these consensus motifs, in the \"Saccharomyces cerevisiae \"origin of replication, is necessary but not sufficient for the function of replication origin and any mutation in this consensus causes the reduction or loss of DNA replication activity. Given the high density of ACS motifs, some NUMTs appear essentially as ACS carriers. In contrast, replication efficiency is higher in those yeast strains that have plasmids containing both NUMT and ARS. They also found that some NUMTs can work as an independent replication fork and late chromosomal origins and NUMTs located close to or within ARS provide key sequence elements for replication. Thus, NUMTs can act as the independent origins, when inserted in an appropriate genomic context or affect the efficiency of pre-existing origins.\n\nDisease and Disorders: NUMT insertion into the genome can be problematic. Transposition of NUMTs into genome has also been associated with human diseases. De novo integration of NUMT pseudogenes into the nuclear genome has an adverse effect in some cases, promoting various disorders and aging. MtDNA integration into coding genes in the germline cells has dramatic consequences for embryo development and, in many cases, is lethal. Few NUMT pseudogenes associated with diseases are found within exons or at the exon–intron boundaries of human genes. For example, the patients with mucolipidosis syndrome inherit a mutation caused by insertion of a 93bp fragment of mitochondrial ND5 into exon 2 of the R403C mucolipin gene. This is the first case of a heritable disorder due to the NUMT insert. Despite the small treatment group, Stem Cell transplant found to be effective and lysosomal enzyme levels seemed to normalize after transplant in at least one case. The Pallister–Hall syndrome, a developmental disorder, in another example, where a functional disorder of a key developmental gene results from a \"de novo\" insertion of a 72bp mtDNA fragment into \"GLI3\" exon 14 in chromosome 7, which results in central and postaxial polydactyly, bifid epiglottis, imperforate anus, renal abnormalities including cystic malformations, renal hypoplasia, ectopic ureteral implantation, and pulmonary segmentation anomalies such as bilateral bilobed lungs. A splice site mutation in the human gene for plasma factor VII that causes severe plasma factor VII deficiency, bleeding disease, results from a 251-bp NUMT insertion. As the last known example, a 36-bp insertion in exon 9 of the USH1C gene associated with Usher syndrome type IC is the NUMT. No certain curse has yet found for Usher syndrome, however, a current clinical study on 18 volunteers is taking place to determine the influence of UshStat both in a short and a long-term period. This study has been started in September 2013 and is estimated to be done by October 2023.\n\nAging: Several studies indicated that de novo appearance of NUMT pseudogenes in the genome of somatic cells may be of etiological importance for carcinogenesis and aging. To show the relation between aging and NUMT in the nuclear genome, Cheng and Ivessa used \"yme1-1\" mutant strains of Saccharomyces Cerevisiae that have a higher rate of mtDNA migration. The method is exactly the same as the method Thorsness and Fox used to determine the important mechanisms and factors for mtDNA migration into the nucleus. They found out the yeast strains with elevated migration rates of mtDNA fragments to the nucleus showed accelerated chronological aging, whereas, strains with decreased mtDNA transfer rates to the nucleus exhibited an extended CLS, chronological life span which could possibly be due to the effect of NUMT on nuclear processes including DNA replication, recombination, and repair as well as gene transcription. The effect of NUMT on the higher Eukaryotic organisms was investigated by Caro and his teammates in the rats as a model organism. Using a real-time PCR quantification, in situ hybridization of mtDNA to nDNA, and comparison of young and old rats, Caro and his crew not only could determine the high concentration of cytochrome oxidase III and 16S rRNA from mtDNA in both young and old rats, but they also could find out the increase in the number of mitochondrial sequences in nDNA as the rat gets older. Thus, based on these findings, mitochondria can be a major trigger of aging, but the final target could also be the nucleus.\n\nCancer: The most dreadful impact of NUMT insertion happens when the mtDNA is inserted into the regulatory region or nuclear structural genes and disrupts or alters the vital cell processes. For instance, in primary low-grade brain neoplasms, fluorescent in situ hybridization analysis helped with the recognition of mtDNA localized in the nucleus in correlation with an overall increase in mtDNA content in the cell. This ontogenically early event is important in the etiology of these tumors. Similarly, in hepatoma cells mtDNA sequences are present in the nuclear genome at a higher copy number in contrast with the normal tissues. Another example would be HeLa nDNA that contains sequences which hybridize with mtDNA fragments of approximately 5 kb. An analysis showed that nDNA of malignant cells contains sequences of the mitochondrial \"cytochrome oxidase I\", \"ND4 \", \"ND4L \", and 12S rRNA genes. Based on these findings, mtDNA fragments were assumed to act as a mobile genetic element in the initiation of carcinogenesis. Southern blotting is the method used to determine the frequency of mitochondrial insertion in nDNA of the normal and the tumor cells of mice and rats, which proved that the mtDNA sequences are far more numerous and abundant in nDNA of rodent tumor cells in comparison with normal cells. Using FISH probes, PCR and data sequencing, mapping and comparison, Ju and his teammate found that the mitochondrial-nuclear genome fusions occur at a similar rate per base pair of DNA as interchromosomal nuclear rearrangements, indicating the presence of a high frequency of contact between mitochondrial and nuclear DNA in some somatic cells. Also, Ju and his teammates investigated the timing of somatic mtDNA integration into the nuclear genome by assessing cases in which a metastatic sample had been sequenced in addition to the primary tumor. In some cases, mtDNA transfers into the nucleus in somatic cells are very frequent and can occur after neoplastic formation and during the course of subclonal evolution of cancer which suggest that this event occurs in the common ancestral cancer clones or in normal somatic cells prior to the neoplastic change. These findings demonstrated that the presence of direct correlation between NUMT and cancer in different body organs. Understanding the relation, the timing of the NUMT insertion, location of the insertion, and disrupted genes would help with producing more powerful and effective medicine.\n\nAlthough understanding non-random insertion of NUMT and carrying out certain function after insertion, helps with revealing the structure and determining the complete function of the genome, especially human genome, NUMTs have been used as experimental tools and have been beneficial in different biological fields even before having any knowledge about the function of NUMTs. For instance, NUMTs can be used not only as genetic markers but also as a tool for understanding the relative rate of mutation in the nucleus and the mitochondria as well as recreating evolutionary trees. The continuing process of NUMT integration into the nuclear genome is evidenced by the finding of NUMTs that have been inserted into the human genome after the human–chimpanzee divergence. Some of these NUMTs are variable with respect to the genomic presence or absence, indicating that they have only arisen recently in the human population, permitting them to be used as genetic markers of lineage. Using a protocol based on genome alignment to estimate the number of NUMT in closely related species, Hazkani-Covo and Graur could not only identify evolutionary events that may have affected NUMT composition in each genome but could also reconstruct the NUMT makeup in the common ancestor of human and chimpanzee. NUMTs can be also used to compare the rate of nonfunctional nuclear sequence evolution to that of functional mtDNA and determine the rate of evolution by the rate of mutation accumulation along NUMT sequences over time. The least selectively constrained regions are the segments with the most divergence from the mitochondrial sequence. One of the most promising applications of NUMT study is its use in the study of nuclear mutation. In metazoans, NUMTs are considered non-functional. Therefore, nuclear mutations can be distinguished from mitochondrial changes and the study of nucleotide substitution, insertion, and deletion would be possible. Additionally, the homology of paralogous NUMT sequences with the mtDNA allows testing for local sequence effects on mutation. All these information obtained from the study of NUMT fragments could be used to understand mitochondrial evolution as well as evolutionary processes throughout the history.\n\nNUMTs offer an opportunity to study ancient diversity of mitochondrial lineages and to discover prehistoric interspecies hybridization. Ancient hybridization have been first detected (using NUMTs) in bristletails, then in colobine monkeys, and, most recently, in a direct human ancestor. The hominid hybridization happened about the time of human/chimpanzee/gorilla separation. This latter study concerns a human NUMT shared with chimpanzee and gorilla. Joint phylogeny of the three NUMT sequences and the mitochondrial genomes of great apes implies that a common ancestor of the three NUMTs has been transferred to human/chimp/gorilla lineage from a hominid species separated from them by about 4.5 million years of mtDNA evolution. While hybridization of this magnitude is not unheard of among primates, its occurrence in the direct human lineage, around the critical time of human/ape speciation, is a startling result. Additional NUMTs with similar phylogenies indicate that such events may be not unique.\n\nAnother problem arose from the presence of NUMT in the genome associated with the hardship of concluding the exact number of mitochondrial insertions into the nDNA. Determining the exact number of NUMT pseudogenes for a species is difficult task for several reasons. One reason that makes detection of NUMT sequences more difficult is the alteration of these sequences by mutation and deletion. Two further substantial obstacles make recognition of NUMT very difficult; first is the lack of correlation between the proportion of noncoding nDNA and the number of NUMT inserts in the nuclear genome . That is, NUMT insertion could occur in the known or predicted coding region, both intron and exon, rather than only in intergenic and intronic region. Second, mitochondrial DNA integrated into animal nuclear genomes is primarily limited to animals with circular mitochondrial genomes without introns. NUMT studies are not available in animals with linear mitochondrial genomes or those with intron-containing mitochondria. Therefore, despite all the available advanced technologies, it remains to be determined whether NUMT transposition differences exist between circular and linear mtDNAs.\n\nThese difficulties to detect the presence of NUMT can be problematic. Translocated mitochondrial sequences in the nuclear genome have the potential to get amplified in addition to, or even instead of, the authentic target mtDNA sequence which can seriously confound population genetic and phylogenetic analyses since mtDNA has been widely used for population mapping, evolutionary and phylogenic studies, species identification by DNA barcode, diagnosis of various pathologies, and forensic medicine. This simultaneous amplification of NUMT with free extrachromosomal mtDNA, additionally, prevents one from determining the exact number of NUMT fragments in the genome of different organisms, such as \"Aedes aegypti\" mosquitoes, especially those in which extended translocation of mtDNA fragments occur. This makes the diagnosis of certain mitochondrial disorders challenging. For instance, a large NUMT pseudogene was found on chromosome 1, while more recent analysis of the same sequence led to a conclusion that sperm mtDNA has mutations that cause low sperm mobility. Another example would be the recent report describing a heteroplasmic mtDNA molecule containing five linked missense mutations dispersed over the contiguous mtDNA CO1 and CO2 genes in Alzheimer’s disease patients, however, the more recent studies using PCR, restriction endonuclease site variant assays, and phylogenic analysis proposed that the nuclear CO1 and CO2 sequences revealed that they diverged from modern human mtDNAs early in hominid evolution about 770,000 years before and these preserved NUMTs could cause Alzheimer’s disease. One of the possible ways of preventing from such erroneous result is an amplification and comparison of heterogeneous sequence, comprises both mtDNA and nDNA, with the obtained results from Sanger sequencing of purified and enriched mtDNA as shown in figure 4. Although this method is easy and only a few primers are required, it will prevent from a substantial error in phylogenetic studies of a population and all the previously mentioned false results.\n\n", "id": "8564084", "title": "Numt"}
{"url": "https://en.wikipedia.org/wiki?curid=53073581", "text": "BEND2 (protein)\n\nBEND2 is a protein that in humans is encoded by the BEND2 gene. It is also found in other vertebrates, including mammals, birds, and reptiles. The expression of BEND2 in \"Homo sapiens\" is regulated and occurs at high levels in the skeletal muscle tissue of the male testis and in the bone marrow. The presence of the BEN domains in the BEND2 protein indicates that this protein may be involved in chromatin modification and regulation.\n\nBEND2 stands for BEN domain containing 2 and is also known as CXorf20 (HGNC ID: 28509).\n\nThe locus for BEND2 is on the minus strand of the X chromosome at Xp22.13. The gene is approximately 58 kilobases in length.\n\nBEND2 contains 14 exons which undergo alternative splicing to create five transcript variants that vary from 4,720 base pairs (bp) to 2,144 bp in the mature mRNA. The longest and most complete transcript of the gene, variant 1, encodes isoform 1 of the BEND2 protein (NP_699177.2).\n\nThe untranslated regions (UTR) flanking the coding sequence of BEND2 at the 5' and 3' end of the mature mRNA molecule contain sites for RNA-binding proteins, including RBMX, pum2, and EIF4B as well as microRNA binding sites. The 5'UTR also contains an upstream in-frame stop codon and the 3'UTR contains a polyadenylation signal sequence.\n\nThe predicted molecular weight is 87.9 kDal.\n\nThe predicted isoelectric point is pH 5.07.\n\nThe internal composition is enriched for serine residues.\n\nCorresponding to the five alternative transcripts of BEND2, the protein encoded by this gene is found in two isoforms (1 and 2) as well as three predicted structures (X1, X2, and X3). These isoforms range from 813 to 645 amino acids in length. Isoform 1 is 799 amino acids in length.\n\nThe presence of nuclear localization signals within the amino acid sequence or primary structure of the BEND2 protein leads to a prediction of subcellular localization in the nucleus. The pat7 [(P-X(1-3)-(3-4K/R)] signal and a nuclear bipartite signal are both found near the N-terminus of the protein.\nThe secondary structure for BEND2 is unclear, in particular at the N-terminus, which is poorly conserved between orthologs. The C-terminus contains two BEN domains, which are predicted to form a series of alpha helices.\n\nBased on its primary structure, BEND2 is predicted to undergo N-terminus acetylation, glycation of several lysine residues, SUMOlation, a SUMO interaction at the N-terminus, S-palmitoylation, and extensive phosphorylation.\n\nBEND2 is found to interact with the following proteins through experimental yeast two-hybrid screens or pull down assays. \n\nBEND2 has two BEN domains at its C-terminus. BEN domains are found in a diverse array of proteins and are predicted to be important for chromatin remodeling as well as for the recruitment of chromatin-modifying factors utilized during the process of transcriptional regulation of gene expression. BEN domains are predicted to form four alpha helices that allow this domain to interact with its DNA target.\n\nDai et al. 2013 showed that the \"Drosophila melanogaster\" Insensitive (Insv) gene and corresponding protein has no domains of known chemical function yet it contains a single BEN domain. They illustrated the activity of the Insv protein in transcriptional regulation of genes and obtained a crystal structure of two Insv BEN domains interacting with their DNA target site.\n\nThe expression of the BEND2 gene is regulated and it is therefore not ubiquitously expressed in the human body. High expression occurs in the testis and in the bone marrow. The NCBI EST profile for this gene shows expression only in the testis and in the muscle.\n\nThe promoter regulating expression of BEND2 (GXP_2567556) is 1255 base pairs in length and is located directly upstream of the BEND2 gene. It regulates transcription of all five transcriptional variants of BEND2. Genomatix's MatInspector program predicted 418 transcription factor binding sites within the BEND2 promoter, including for SRY, neurogenin, interferon regulatory factor-3 (IRF-3), Ikaros2, and TCF/LEF-1.\n\nThe BEND2 protein has no known paralogs within the human genome.\n\nThe BEND2 gene belongs to a family of human genes known as \"BEN-domain containing”. This includes BANP (BEND1), BEND3, BEND4, BEND5, BEND6, BEND7, NACC1 (BEND8), and NACC2 (BEND9). The loci for these genes are spread throughout the human genome. Each of these genes contains between one and four BEN domains. Except for at these motifs, the genes of the BEN family do not have similar sequences.\n\nThe BEND2 gene is conserved across evolutionary time as it has 114 known orthologs in a wide range of vertebrate species including mammals, birds, crocodilia, and amphibians. The BEND2 protein has 42 known orthologs. The C-terminus of the protein, the location of its BEN domains, is highly conserved; however, the N-terminus is not well conserved, even within the order of Primates. \nBEND2 is predicted to be a DNA-binding protein due to the presence of BEN domains at its C-terminus, a hypothesis supported by its localization to the nucleus, the transcription factors found in its promoter region, and the nature of the proteins it interacts with. Though the precise function of the BEND2 protein is not yet well understood by the scientific community, BEN domains have been found to be important regulators of transcription.\n\nInterestingly, the diseases that have been linked to BEND2 are related to the central nervous system (CNS) though expression of the gene is not highly observed in these tissues. \n", "id": "53073581", "title": "BEND2 (protein)"}
{"url": "https://en.wikipedia.org/wiki?curid=53028376", "text": "C10orf71 (gene)\n\nC10orf71 is a gene located on chromosome 10 open reading frame 71. It is primarily understood that this gene is moderately expressed in muscle tissue and cardiac tissue.\n\nThe cytogenic locus is found at 10q11.23. C10orf71 encodes 28294 base pairs (bp) within chromosome 10 at 49299193-49327487 bp (refer to Figure 1). It is located on the plus strand and is flanked by several other genes as seen in Figure 1.\n\nThe mRNA sequence of C10orf71 has 3 exons and 10 stop codons in the favorable splice form. The two alternative splice forms had 47 and 75 stop codons interspersed throughout the sequence so they were not utilized to obtain further sequence information. The main splice form that was analyzed had the ten stop codons interspersed throughout the 5' and 3' UTR, which was why this splice form was utilized to further analyze (refer to Table 1 for details of stop codons and exons). The mRNA of the Homo sapiens ortholog of C10orf71was 5286 bp in length. \n\nThe mature C10orf71 protein of the \"Homo sapiens\" homolog is 1435 amino acids (aa) in length and weighs approximately 156.5 kDa. This homolog has an isoelectric point of 5.94. The range of pH values from \"Homo sapiens\" to the latest ortholog analyzed, \"Rhincodon types\", ranged from 5.94-6.93, with it gradually increasing as it went later in the divergence of the ortholog (refer to Table 2 for ortholog comparison). \n\nC10orf71 is predicted to be a non-transmembrane, soluble protein. It is predicted to be a nuclear protein with 91.3% confidence with it being fairly confident to be a nuclear protein throughout the orthologs (see Table 3 for ortholog comparison). There was one positive charge cluster found in C10orf71 protein sequence, that is located from amino acids 1165-1193. This cluster was moderately conserved throughout the orthologs analyzed. There was also a mixed charge cluster found in the \"Homo sapiens\"' sequence of this protein, located from amino acid 750-778, although this cluster was not highly conserved throughout the analyzed orthologs. There was one repeat sequence found as well, TASKPPA, located at amino acids 163-169 and 116-1172. This protein is Proline and Serine rich as well. \n\nOne confirmed domain of unknown function (DUF) was found within the C10orf71 protein sequence, DUF4585 (as shown in Figure 2). DUF4585 is located on the \"Homo sapiens\" protein sequence from amino acid 311-334. DUF4585 was highly conserved throughout the orthologs that were analyzed. There was also a small vacuolar targeting motif (VAC) found within the analyzed protein sequence spanning amino acids 543-546 (refer to Figure 2).\n\nThe mature C10orf71 protein contains nuclear localization signals (NLS), pat4 (RKPK at aa 382, RPRK at aa 640, KRRK at aa 1190) and pat7 (PPWRKPK at aa 379 and PWRKPKT at aa 380) with an NLS score pf 0.94. A secondary structure was constructed with a 6.1% confidence level (see Figure 2). A predicted 3-D structure of C10orf71 protein can be seen in Figure 3.\n\nThere were seven GlcNAc O-glycosylation sites predicted within the protein sequence found at amino acids 116, 120, 139, 165, 468, 470, and 844 (Fig. 2). There were also several phosphorylation sites found interspersed throughout the sequence. One propeptide cleavage site was predicted at amino acid 38. There were three predicted sumoylation sites found at amino acids 599, 890, and 1176.\n\nC10orf71 was found to be highly expressed in cardiac, muscle, and liver tissue (biology).\n\nThere were 6 possible promoters found in the sequence. Promoter GXP_6729162 is 1403 bp in length. This promoter had several transcription factors of interest including those involved with myocytes.\n\nThere is little scientific information known about the function of C10orf71.\n\nThere was a total of 25 proteins generated that were predicted to interact with C10orf71 (\"Homo sapiens\" ortholog). Most of the interactions predicted were physical interactions with C10orf71. These interactions were discovered through a variety of mechanisms including, but not limited to: affinity chromatography, microarray analysis, and tandem mass spectrometry among others. Refer to Table 4 for details about the interacting proteins of C10orf71.\n\nThere are currently no known paralogs to the C10orf71 gene.\n\nC10orf71 is known to have 68 orthologs in various species including primates (11 species), rodents (8 species), Laurasiatheria carnivores (14 species), Placental mammals (38 species), Sauropsida birds and reptiles (7 species), and fish (11 species). The highly conserved sequences are primarily from primates with the identity percentage of these species being >90%, whereas species such as reptiles, birds, and fish had an identity percentage ≤30%. Refer to Table 3 for additional information on dates of divergence, sequence length, and sequence identity and similarity for orthologs. C10orf71 is not present in prokaryotes, archaea, or fungi.\n\nA phylogenetic tree was constructed for the orthologs that were analyzed in comparison to \"Homo sapiens.\" With the species of latest divergence being \"Rhincodon types,\" or the whale shark.\n\nC10orf71's rate of divergence was faster than that of fibrinogen or Cytochrome C as seen in Figure 5.\n\nThere was a microarray experiment that also showed evidence that C10orf71's expression was lowered in skeletal muscle tissues that experienced sepsis. There was clinical significance found in the expression level of C10orf71 in an experiment looking at those with Myotonic dystrophy. One microarray analysis produced results that showed C10orf71's expression level decreased in those with prostate cancer as well.\n", "id": "53028376", "title": "C10orf71 (gene)"}
{"url": "https://en.wikipedia.org/wiki?curid=53827714", "text": "Human germline engineering\n\nHuman germline engineering is the process by which the genome of an individual is edited in such a way that the change is heritable. This is achieved through genetic alterations within the germinal cells, or the reproductive cells, such as the oocyte and spermatogonium. Human germline engineering should not be confused with gene therapy. Gene therapy consists of altering somatic cells, which are all cells in the body that are not involved in reproduction. While gene therapy does change the genome of the targeted cells, these cells are not within the germline, so the alterations are not heritable and cannot be passed on to the next generation.\n\nThe first attempt to edit the human germline was reported in 2015, when a group of Chinese scientists used the gene editing technique CRISPR/Cas9 to edit single-celled, non-viable embryos to see the effectiveness of this technique. This attempt was rather unsuccessful; only a small fraction of the embryos successfully spliced the new genetic material and many of the embryos contained a large amount of random mutations. The non-viable embryos that were used contained an extra set of chromosomes, which may have been problematic. In 2016, another similar study was performed in China which also used non-viable embryos with extra sets of chromosomes. This study showed very similar results to the first; there were successful integrations of the desired gene, yet the majority of the attempts failed, or produced undesirable mutations.\n\nThe most recent, and arguably most successful, experiment in August of 2017 attempted the correction of the heterozygous \"MYBPC3\" mutation associated with Hypertrophic Cardiomyopathy in human embryos with precise CRISPR–Cas9 targeting. 52% of human embryos were successfully edited to retain only the wild type normal copy of MYBPC3 gene, the rest of the embryos were mosaic, where some cells in the zygote contained the normal gene copy and some contained the mutation.\nCurrently, there are no successfully engineered humans, but there are many prospective uses such as curing genetic diseases and disorders. In the first study published regarding human germline engineering, the researchers attempted to edit the \"HBB\" gene which codes for the human β-globin protein. Mutations in the \"HBB\" gene result in the disorder β-thalassaemia, which can be fatal. Perfect editing of the genome in patients who have these \"HBB\" mutations would result in copies of the gene which do not possess any mutations, effectively curing the disease. The importance of editing the germline would be to pass on this normal copy of the \"HBB\" genes to future generations.\n\nAnother possible use of human germline engineering would be eugenic modifications to humans which would result in what are known as \"designer babies\". The concept of a \"designer baby\" is that its entire genetic composition could be selected for. In an extreme case, people would be able to effectively create the offspring that they want, with traits of their choosing. Not only does human germline engineering allow for the selection of specific traits, but it also allows for enhancement of these traits. Using human germline editing for selection and enhancement is currently very heavily scrutinized, and the main driving force behind the movement of trying to ban human germline engineering.\n\nThe topic of human germline engineering is a widely debated topic. Currently, 15 of 22 Western European nations have outlawed human germline engineering. There is no current legislation in the United States that explicitly prohibits germline engineering, however, the \"Consolidated Appropriation Act of 2016\" banned the use of U.S. Food and Drug Administration (FDA) funds to engage in research regarding human germline modifications.\n\nAs it stands, there is much controversy surrounding human germline engineering. The scientific community, and global community, are quite divided regarding whether or not human germline engineering should be practiced or not. It is currently banned in many of the leading, developed countries, and highly regulated in the others due to ethical issues. The large debate lies in the possibility of eugenics if human germline engineering were to be practiced clinically. This topic is hotly debated because the side opposing human germline modification believes that it will be used to create humans with \"perfect\", or \"desirable\" traits. Those in favor of human germline modification see it as a potential medical tool, or a medical cure for certain diseases that lie within the genetic code. There is a debate as to if this is morally acceptable as well. While typically there is a clash between religion and science, the topic of human germline engineering has shown some unity between the two fields. Several religious positions have been published with regards to human germline engineering. According to them, many see germline modification as being more moral than the alternative, which would be either discarding of the embryo, or birth of a diseased human. The main conditions when it comes to whether or not it is morally and ethically acceptable lie within the intent of the modification, and the conditions in which the engineering is done. \n\nAnother very interesting point on the debate of whether or not it is ethical and moral to engineer the human germline is a perspective of looking at past technologies and how they have evolved. Dr. Gregory Stock discusses the use of several diagnostic tests used to monitor current pregnancies and several diagnostic tests that can be done to determine the health of embryos. Such tests include amniocentesis, ultrasounds, and other preimplantation genetic diagnostic tests. These tests are quite common, and reliable, as we talk about them today; however, in the past when they were first introduced, they too were scrutinized.\n\nOne of the main arguments against human germline engineering lies in the ethical feeling that it will dehumanize children. At an extreme, parents may be able to completely design their own child, and there is a fear that this will transform children into objects, rather than human beings. There is also a large opposition as people state that by engineering the human germline, there is an attempt at \"playing God\", and there is a strong opposition to this. One final, and very possible issue that causes a strong opposition of this technology is one that lies within the scientific community itself. Inevitably, this technology would be used for enhancements to the genome, which would likely cause many more to use these same enhancements. By doing this, the genetic diversity of the human race and the human gene pool as we know it would slowly and surely diminish. Despite the controversy surrounding the topic of human germline engineering, it is slowly and very carefully making its way into many labs around the world. These experiments are highly regulated, and they do not include the use of viable human embryos, which allows scientists to refine the techniques, without posing a threat to any real human beings.\n\n", "id": "53827714", "title": "Human germline engineering"}
{"url": "https://en.wikipedia.org/wiki?curid=833076", "text": "Arthur Balfour Professor of Genetics\n\nThe Arthur Balfour Professorship of Genetics is the senior professorship in genetics at the University of Cambridge, founded in 1912. It is thought to be the oldest Chair of Genetics in the English speaking world.\n\nThe chair was endowed by Reginald Baliol Brett, 2nd Viscount Esher, according to whom the money (£20,000) was \"placed in [his] hands\" by an anonymous benefactor. A condition of the endowment was that the first appointee to the chair would be chosen jointly by the Prime Minister H. H. Asquith and the former Prime Minister Arthur Balfour.\n\n", "id": "833076", "title": "Arthur Balfour Professor of Genetics"}
{"url": "https://en.wikipedia.org/wiki?curid=54147200", "text": "National Centre for Plant Genetic Resources: Polish Genebank\n\nThe National Centre for Plant Genetic Resources: Polish Genebank (NCPGR) is a research unit in the Plant Breeding and Acclimatization Institute – National Research Institute. NCPGR is the coordinator and implementer of the National Crop Plant Genetic Resources Protection Programme. The Programme aims to protect the biodiversity of crop plants endangered by genetic erosion in Poland, and is funded by the Ministry of Agriculture. The main tasks include collection of crop and wild plant populations and varieties threatened by genetic erosion, description and evaluation of collected materials, and preservation of their viability and genetic purity. The Programme is an implementation of provisions laid down in international treaties ratified by Poland:\n\nNCPGR collects populations and cultivated varieties of crop and wild plants threatened with genetic erosion. Collected materials are characterised, evaluated and documented.\n\nSeed samples and clones are maintained in viable state and genetic purity. NCPGR exchanges samples with other institutions worldwide and provides initial plant materials for breeding and research programs.\n\nThe Laboratory organizes collecting expeditions, during which plant genetic resources are obtained. Plants are collected from natural sites or obtained from farmers or on local markets. Collected material is reproduced and stored in a gene bank. The Laboratory also carries out studies of variation and genetic structure of selected species and prepares initial materials of selected species for practical breeding.\n\nThe Laboratory covers drawing up documentation of genetic resources of crop plants and exchanges information with other genebanks. It also obtains seed samples of crop plants from national and international breeding centres. As for the long-term conservation, seed samples need to be prepared and have their viability tested before they are stored in temperature controlled chambers. NCPGR has three long-term storage chambers (-18 °C) and five medium-term storage chambers (0 °C).\n\nThe seeds exposed to long-term preservation are tested for viability and biochemical changes. Seed samples with low seed viability or with insufficient seed amount are forwarded for regeneration. The Laboratory provides seed samples for breeding, research and education.\n\nThe Laboratory searches for and collects local and foreign plant species with increased ability to adapt to extreme conditions of devastated environment. Another objective is regeneration and maintenance of viability of alternative plant collection for reclamation of devastated lands and lands periodically excluded from agricultural use.\n\nSelected plant species are analyzed for having the ability to colonize different types of industrial and communal derivative areas. There is also description of morphological and other useful plant traits carried out when it comes to plants which have high biological yielding potential to serve as a source of energy and for other economic uses.\n\nThe Laboratory prepares initial material for the breeding programmes of alternative crops.\n\nThe key activity of Botanical Garden is evaluation and preservation of grass genetic resources. The plants originated from various climatic zones are maintained in different, highly specialized collections. Natural resources of Botanical Garden include:\n\n\n", "id": "54147200", "title": "National Centre for Plant Genetic Resources: Polish Genebank"}
{"url": "https://en.wikipedia.org/wiki?curid=8406655", "text": "Introduction to genetics\n\nGenetics is the study of genes—what they are, what they do, and how they work. Genes inside the nucleus of a cell are strung together in such a way that the sequence carries information: that information determines how living organisms inherit various features (phenotypic traits). For example, offspring produced by sexual reproduction usually look similar to each of their parents because they have inherited some of each of their parents' genes. Genetics identifies which features are inherited, and explains how these features pass from generation to generation. In addition to inheritance, genetics studies how genes are turned on and off to control what substances are made in a cell—gene expression; and how a cell divides—mitosis or meiosis.\n\nSome phenotypic traits can be seen, such as eye color while others can only be detected, such as blood type or intelligence. Traits determined by genes can be modified by the animal's surroundings (environment): for example, the general design of a tiger's stripes is inherited, but the specific stripe pattern is determined by the tiger's surroundings. Another example is a person's height: it is determined by both genetics and nutrition.\n\nChromosomes are tiny packages which contain one DNA molecule and its associated proteins. Humans have 46 chromosomes (23 pairs). This number varies between species—for example, many primates have 24 pairs. Meiosis creates special cells, sperm in males and eggs in females, which only have 23 chromosomes. These two cells merge into one during the fertilization stage of sexual reproduction, creating a zygote. In a zygote, a nucleic acid double helix divides, with each single helix occupying one of the daughter cells, resulting in half the normal number of genes. By the time the zygote divides again, genetic recombination has created a new embryo with 23 pairs of chromosomes, half from each parent. Mating and resultant mate choice result in sexual selection. In normal cell division (mitosis) is possible when the double helix separates, and a complement of each separated half is made, resulting in two identical double helices in one cell, with each occupying one of the two new daughter cells created when the cell divides.\n\nChromosomes all contain DNA made up of four nucleotides, abbreviated C (cytosine), G (guanine), A (adenine), or T (thymine), which line up in a particular sequence and make a long string. There are two strings of nucleotides coiled around one another in each chromosome: a double helix. C on one string is always opposite from G on the other string; A is always opposite T. There are about 3.2 billion nucleotide pairs on all the human chromosomes: this is the human genome. The order of the nucleotides carries genetic information, whose rules are defined by the genetic code, similar to how the order of letters on a page of text carries information. Three nucleotides in a row—a triplet—carry one unit of information: a codon. \n\nThe genetic code not only controls inheritance: it also controls gene expression, which occurs when a portion of the double helix is uncoiled, exposing a series of the nucleotides, which are within the interior of the DNA. This series of exposed triplets (codons) carries the information to allow machinery in the cell to \"read\" the codons on the exposed DNA, which results in the making of RNA molecules. RNA in turn makes either amino acids or microRNA, which are responsible for all of the structure and function of a living organism; i.e. they determine all the features of the cell and thus the entire individual. Closing the uncoiled segment turns off the gene. \nHeritability means the information in a given gene is not always exactly the same in every individual in that species, so the same gene in different individuals does not give exactly the same instructions. Each unique form of a single gene is called an allele; different forms are collectively called polymorphisms. As an example, one allele for the gene for hair color and skin cell pigmentation could instruct the body to produce black pigment, producing black hair and pigmented skin; while a different allele of the same gene in a different individual could give garbled instructions that would result in a failure to produce any pigment, giving white hair and no pigmented skin: albinism. Mutations are random changes in genes creating new alleles, which in turn produce new traits, which could help, harm, or have no new effect on the individual's likelihood of survival; thus, mutations are the basis for evolution.\n\nGenes are pieces of DNA that contain information for synthesis of ribonucleic acids (RNAs) or polypeptides. Genes are inherited as units, with two parents dividing out copies of their genes to their offspring. This process can be compared with mixing two hands of cards, shuffling them, and then dealing them out again. Humans have two copies of each of their genes, and make copies that are found in eggs or sperm—but they only include \"one\" copy of each type of gene. An egg and sperm join to form a complete set of genes. The eventually resulting offspring has the same number of genes as their parents, but for any gene one of their two copies comes from their father, and one from their mother.\n\nThe effects of this mixing depend on the types (the alleles) of the gene. If the father has two copies of an allele for red hair, and the mother has two copies for brown hair, all their children get the two alleles that give different instructions, one for red hair and one for brown. The hair color of these children depends on how these alleles work together. If one allele dominates the instructions from another, it is called the \"dominant\" allele, and the allele that is overridden is called the \"recessive\" allele. In the case of a daughter with alleles for both red and brown hair, brown is dominant and she ends up with brown hair.\n\nAlthough the red color allele is still there in this brown-haired girl, it doesn't show. This is a difference between what you see on the surface (the traits of an organism, called its phenotype) and the genes within the organism (its genotype). In this example you can call the allele for brown \"B\" and the allele for red \"b\". (It is normal to write dominant alleles with capital letters and recessive ones with lower-case letters.) The brown hair daughter has the \"brown hair phenotype\" but her genotype is Bb, with one copy of the B allele, and one of the b allele.\n\nNow imagine that this woman grows up and has children with a brown-haired man who also has a Bb genotype. Her eggs will be a mixture of two types, one sort containing the B allele, and one sort the b allele. Similarly, her partner will produce a mix of two types of sperm containing one or the other of these two alleles. When the transmitted genes are joined up in their offspring, these children have a chance of getting either brown or red hair, since they could get a genotype of BB = brown hair, Bb = brown hair or bb = red hair. In this generation, there is therefore a chance of the recessive allele showing itself in the phenotype of the children—some of them may have red hair like their grandfather.\n\nMany traits are inherited in a more complicated way than the example above. This can happen when there are several genes involved, each contributing a small part to the end result. Tall people tend to have tall children because their children get a package of many alleles that each contribute a bit to how much they grow. However, there are not clear groups of \"short people\" and \"tall people\", like there are groups of people with brown or red hair. This is because of the large number of genes involved; this makes the trait very variable and people are of many different heights. Despite a common misconception, the green/blue eye traits are also inherited in this complex inheritance model. Inheritance can also be complicated when the trait depends on interaction between genetics and environment. For example, malnutrition does not change traits like eye color, but can stunt growth.\n\nSome diseases are hereditary and run in families; others, such as infectious diseases, are caused by the environment. Other diseases come from a combination of genes and the environment. Genetic disorders are diseases that are caused by a single allele of a gene and are inherited in families. These include Huntington's disease, Cystic fibrosis or Duchenne muscular dystrophy. Cystic fibrosis, for example, is caused by mutations in a single gene called \"CFTR\" and is inherited as a recessive trait.\n\nOther diseases are influenced by genetics, but the genes a person gets from their parents only change their risk of getting a disease. Most of these diseases are inherited in a complex way, with either multiple genes involved, or coming from both genes and the environment. As an example, the risk of breast cancer is 50 times higher in the families most at risk, compared to the families least at risk. This variation is probably due to a large number of alleles, each changing the risk a little bit. Several of the genes have been identified, such as \"BRCA1\" and \"BRCA2\", but not all of them. However, although some of the risk is genetic, the risk of this cancer is also increased by being overweight, drinking a lot of alcohol and not exercising. A woman's risk of breast cancer therefore comes from a large number of alleles interacting with her environment, so it is very hard to predict.\n\nThe function of genes is to provide the information needed to make molecules called proteins in cells. Cells are the smallest independent parts of organisms: the human body contains about 100 trillion cells, while very small organisms like bacteria are just one single cell. A cell is like a miniature and very complex factory that can make all the parts needed to produce a copy of itself, which happens when cells divide. There is a simple division of labor in cells—genes give instructions and proteins carry out these instructions, tasks like building a new copy of a cell, or repairing damage. Each type of protein is a specialist that only does one job, so if a cell needs to do something new, it must make a new protein to do this job. Similarly, if a cell needs to do something faster or slower than before, it makes more or less of the protein responsible. Genes tell cells what to do by telling them which proteins to make and in what amounts.\n\nProteins are made of a chain of 20 different types of amino acid molecules. This chain folds up into a compact shape, rather like an untidy ball of string. The shape of the protein is determined by the sequence of amino acids along its chain and it is this shape that, in turn, determines what the protein does. For example, some proteins have parts of their surface that perfectly match the shape of another molecule, allowing the protein to bind to this molecule very tightly. Other proteins are enzymes, which are like tiny machines that alter other molecules.\n\nThe information in DNA is held in the sequence of the repeating units along the DNA chain. These units are four types of nucleotides (A,T,G and C) and the sequence of nucleotides stores information in an alphabet called the genetic code. When a gene is read by a cell the DNA sequence is copied into a very similar molecule called RNA (this process is called transcription). Transcription is controlled by other DNA sequences (such as promoters), which show a cell where genes are, and control how often they are copied. The RNA copy made from a gene is then fed through a structure called a ribosome, which translates the sequence of nucleotides in the RNA into the correct sequence of amino acids and joins these amino acids together to make a complete protein chain. The new protein then folds up into its active form. The process of moving information from the language of RNA into the language of amino acids is called translation.\n\nIf the sequence of the nucleotides in a gene changes, the sequence of the amino acids in the protein it produces may also change—if part of a gene is deleted, the protein produced is shorter and may not work any more. This is the reason why different alleles of a gene can have different effects in an organism. As an example, hair color depends on how much of a dark substance called melanin is put into the hair as it grows. If a person has a normal set of the genes involved in making melanin, they make all the proteins needed and they grow dark hair. However, if the alleles for a particular protein have different sequences and produce proteins that can't do their jobs, no melanin is produced and the person has white skin and hair (albinism).\n\nGenes are copied each time a cell divides into two new cells. The process that copies DNA is called DNA replication. It is through a similar process that a child inherits genes from its parents, when a copy from the mother is mixed with a copy from the father.\n\nDNA can be copied very easily and accurately because each piece of DNA can direct the creation of a new copy of its information. This is because DNA is made of two strands that pair together like the two sides of a zipper. The nucleotides are in the center, like the teeth in the zipper, and pair up to hold the two strands together. Importantly, the four different sorts of nucleotides are different shapes, so for the strands to close up properly, an A nucleotide must go opposite a T nucleotide, and a G opposite a C. This exact pairing is called base pairing.\n\nWhen DNA is copied, the two strands of the old DNA are pulled apart by enzymes; then they pair up with new nucleotides and then close. This produces two new pieces of DNA, each containing one strand from the old DNA and one newly made strand. This process is not predictably perfect as proteins attach to a nucleotide while they are building and cause a change in the sequence of that gene. These changes in DNA sequence are called mutations. Mutations produce new alleles of genes. Sometimes these changes stop the functioning of that gene or make it serve another advantageous function, such as the melanin genes discussed above. These mutations and their effects on the traits of organisms are one of the causes of evolution.\n\nA population of organisms evolves when an inherited trait becomes more common or less common over time. For instance, all the mice living on an island would be a single population of mice: some with white fur, some gray. If over generations, white mice became more frequent and gray mice less frequent, then the color of the fur in this population of mice would be evolving. In terms of genetics, this is called an increase in allele frequency.\n\nAlleles become more or less common either by chance in a process called genetic drift, or by natural selection. In natural selection, if an allele makes it more likely for an organism to survive and reproduce, then over time this allele becomes more common. But if an allele is harmful, natural selection makes it less common. In the above example, if the island were getting colder each year and snow became present for much of the time, then the allele for white fur would favor survival, since predators would be less likely to see them against the snow, and more likely to see the gray mice. Over time white mice would become more and more frequent, while gray mice less and less.\n\nMutations create new alleles. These alleles have new DNA sequences and can produce proteins with new properties. So if an island was populated entirely by black mice, mutations could happen creating alleles for white fur. The combination of mutations creating new alleles at random, and natural selection picking out those that are useful, causes adaptation. This is when organisms change in ways that help them to survive and reproduce. Many such changes, studied in evolutionary developmental biology, affect the way the embryo develops into an adult body.\n\nSince traits come from the genes in a cell, putting a new piece of DNA into a cell can produce a new trait. This is how genetic engineering works. For example, rice can be given genes from a maize and a soil bacteria so the rice produces beta-carotene, which the body converts to Vitamin A. This can help children suffering from Vitamin A deficiency. Another gene being put into some crops comes from the bacterium \"Bacillus thuringiensis\"; the gene makes a protein that is an insecticide. The insecticide kills insects that eat the plants, but is harmless to people. In these plants, the new genes are put into the plant before it is grown, so the genes are in every part of the plant, including its seeds. The plant's offspring inherit the new genes, which has led to concern about the spread of new traits into wild plants.\n\nThe kind of technology used in genetic engineering is also being developed to treat people with genetic disorders in an experimental medical technique called gene therapy. However, here the new gene is put in after the person has grown up and become ill, so any new gene is not inherited by their children. Gene therapy works by trying to replace the allele that causes the disease with an allele that works properly.\n\n\n", "id": "8406655", "title": "Introduction to genetics"}
{"url": "https://en.wikipedia.org/wiki?curid=3725756", "text": "European Society of Gene and Cell Therapy\n\nThe European Society of Gene and Cell Therapy (ESGCT) is a non-profit organisation for educational and scientific purposes. The aim of ESGCT is to promote fundamental and clinical research in gene therapy, cell therapy, and genetic vaccines by facilitating education, the exchange of information and technology and by serving as a professional adviser to stakeholder communities and regulatory bodies in Europe.\n\nESGCT was founded in January 1992 as the European Working Group on Human Gene Transfer and Therapy (EWGT). The Society was renamed the European Society of Gene Therapy (ESGT) in 1998. In 2007, the Society's name was changed again to ESGCT to better reflect that the fields of gene and cell therapy can not be readily separated from one another. In 2017 ESGCT is celebrating its 25th anniversary at the Annual Congress in Berlin. \n\nESGCT seeks to support scientists and clinicians working in the fields of gene and cell therapy and to promote awareness and understanding of gene and cell therapy and the vast amount of related research in Europe. For the past 25 years, the society has seen many developments in gene and cell therapy. The development of new and improved delivery vectors and advances in genome engineering have opened up new possibilities, while the knowledge gained through clinical trials and long-term follow up of patients has contributed not only to the fields of gene and cell therapy, but also to the wider medical and life sciences. With these developments, more potential applications of gene and cell therapy and more elegant and life-changing treatments than ever before are within our grasp. With these possibilities come challenges both for the field and the wider society as treatments need to make the transition from lab bench to patient bedside and ethical questions need to be answered. Through its activities, ESGCT seeks to stimulate further exchange of scientific knowledge and expertise, promote collaboration between and within clinics, academic research and the pharmaceutical and biotechnology industry. Furthermore, ESGCT is committed to the training and support of young investigators entering the field and to facilitating an informed dialogue between the scientific community, policy makers and society as a whole.\n\nESGCT organises an Annual Congress, which brings together scientists working in the fields of gene and cell therapy from throughout Europe and beyond [ref: website]. The Congress is typically held in October or November in a major European city. The Congress is a platform for highlighting the latest research and techniques through the scientific sessions, which include keynote lectures, invited speakers, selected speakers, poster sessions and exhibitions. Furthermore, the Congress is an opportunity to develop relationships and collaborations. The public day, where key concepts in gene and cell therapy along with the latest research are presented in the local language to a general audience, is an integral part of the Congress. The Congress may be organised solely by ESGCT, but is often organised in collaboration with a national society for gene and cell therapy in Europe, for example the German Society for Gene Therapy DG-GT in 2017, or with closely related scientific societies, such as the International Society for Stem Cell Research (ISSCR) in 2016. Since 2016, ESGCT also organises an annual Spring School. This meeting is aimed at students and young investigators who are new to the field of gene and cell therapy. The Spring School aims to provide an intensive training course and the opportunity to interact with current and future leaders in the field. Recently, ESGCT has started initiatives to engage its members and the wider community, including patients and members of the public. These initiatives include a heatmap that shows the location and details of gene and cell therapy research in Europe, blogs written by the society's members and news items accessible to non-specialists available on the ESGCT website.\n\nThe Board of ESGCT consists of 10 eminent scientists working in the fields of gene and/or cell therapy. Furthermore, one or two graduate students may be co-opted, non-voting, members of the Board. The presidency of ESGCT normally runs for 2 years. Presidents are elected at the Annual General Meeting that takes place at the Annual Congress. The following people have been elected President of the Society\n\nThe Annual Congress of the ESGCT take place in a different location in Europe every year to reflect the international character of the society. Since the first congress in 1993, Congresses have taken place in\n\n", "id": "3725756", "title": "European Society of Gene and Cell Therapy"}
{"url": "https://en.wikipedia.org/wiki?curid=54363640", "text": "Necrofauna\n\nNecrofauna are species that were previously extinct and have been biologically revived or recreated by the process of de-extinction.\n\nNecrofauna are proxies or imitations of their former species and not identical replicas. Due to a number of technological, biological and environmental factors, they are considered a new type organism altogether. Revive and Restore, a nonprofit organization that supports pursuing de-extinction research with transparency to the public, describes the creation of necrofauna as a result of “transfer[ing] the genes that define the extinct species into the genome of the related species, effectively converting it into a living version of the extinct creature.\"\n\nWhile the existence of necrofauna is still largely hypothetical, a Pyrenean ibex was the first and only animal to have undergone the de-extinction process with moderate success. The \"unextinct\" or revived animal was born, but then died several minutes later due to a lung defect.\n\nNecrofauna and the de-extinction movement are highly controversial within conservation and scientific circles. Many conservationists argue that creating necrofauna could potentially distract from the urgency of saving endangered species that are still alive. Concerns have also been raised pertaining to the possible damaging impacts ecology could face in the wake of introducing a new species into an environment.\n\nThe term \"necrofauna\" is a portmanteau consisting of two morphemes. The first morpheme, “necro,” comes from the Greek prefix \"necro\", meaning death. “Fauna,” meanwhile, refers to the animals that inhabit a particular time period or environment and is derived from the Greek name \"Fauna\", the Roman goddess of earth and fertility.\n\nAlex Steffen is referred to as the first person to coin the neologism “necrofauna” in Jason Mark’s \"Earth Island Journal\" article titled “Back From the Dead.” The word was used in the context of describing the phenomenon of \"charismatic necrofauna,\" which expresses the possibility that only certain charismatic species may be chosen as candidates for de-extinction based on human preferences, or that such resurrection efforts could distract from helping less \"charismatic\" species that are currently endangered.\n", "id": "54363640", "title": "Necrofauna"}
{"url": "https://en.wikipedia.org/wiki?curid=35224853", "text": "Non-helical models of DNA structure\n\nIn the history of molecular biology, non-helical or \"side-by-side\" models of DNA were proposed in the 1970s as a challenge to the standard double-helical model. The non-helical models attempted to solve problems relating to the topology of circular DNA chromosomes during replication. These theories were briefly considered seriously as a minority viewpoint, but they were later largely rejected due to X-ray crystallography of DNA duplexes and later the nucleosome core particle, as well as the discovery of topoisomerases, and these non-double-helical models are not currently accepted by the mainstream scientific community.\n\nThe strands of a helical DNA duplex, if covalently closed into a circular structure, are topologically linked, and thus cannot be separated without breaking one or both strands. This sort of structure is referred to as \"plectonemic\". In circular DNA replication, these twists must be removed for the daughter strands to separate. In contrast, the strands of a non-helical DNA duplex would be topologically non-linked, and can be separated without strand breakage. This sort of structure is referred to as \"paranemic\".\n\nNote that the term \"non-helical\" refers to \"net\" helicity - there can be no helical twist at all, as in the figure at the top of this article, or there could be an equal number of right-handed and left-handed twists, as in the Rodley structure below. The abbreviation \"TN\", to be used to refer to any DNA structure whose strands are topologically non-linked, has been proposed.\n\nThe double-helix model of DNA structure was first published in the journal \"Nature\" by James D. Watson and Francis Crick in 1953 with further details in 1954.) Their work was based upon the crucial X-ray diffraction image of DNA - labeled as \"Photo 51\" - from Rosalind Franklin in 1952, followed by her more highly clarified DNA image with Raymond Gosling, Maurice Wilkins, Alexander Stokes, and Herbert Wilson, as well as base-pairing chemical and biochemical information by Erwin Chargaff. Wilkins and colleagues also reported on X-ray patterns of the state of \"in vivo\" B-DNA in partially oriented salmon sperm heads. Crick, Wilkins, and Watson each received one-third of the 1962 Nobel Prize in Physiology or Medicine for their contributions to this discovery. Franklin, whose breakthrough X-ray diffraction data was used to formulate the DNA structure, died in 1958, and thus was ineligible to be nominated for a Nobel Prize.\n\nIn 1963, John Cairns published autoradiographs of the \"E. coli\" chromosome, showing that it was a single circular molecule that is replicated at a moving locus - the \"replication fork\" - at which both new DNA strands are being synthesized. Cairns, at that time, recognized a topological problem in that the helical twists between the daughter strands would have to be removed if the strands are to separate during cell replication. He proposed that the twists were unwound by means of a hypothetical \"swivel\" around which the chromosome would be free to rotate as necessary for the un-winding and re-winding operations. However, such a structure would have to have about 400,000 helical twists in the Watson–Crick model, each of which would have to be removed in as little as 20 minutes, if the strands are to separate during cell replication. This leads to the apparent need of the chromosome - or at least some part of it - to be spinning at speeds up to 6900–9000 rpm throughout the life of the bacterial cell.\n\nIn 1976, Gordon Rodley and his co-workers published the first non-helical DNA structure, which they named the \"side-by-side\" (SBS) structure. A similar model was published a few months later by Sasisekharan and his associates. The SBS structure was hypothetical, and was not based primarily upon physical evidence. It was created to provide a theoretical solution to the angular momentum problem in DNA replication, which Rodley believed to be otherwise unsolvable.\n\nThe SBS structure is a modification of the original Watson-Crick structure where, instead of one fully right-handed helical twist every 10 base pairs, there is only a \"half\"-twist of 5 base pairs’ length, followed immediately by a half-twist in the \"opposite\" direction (that is, a left-handed half-twist, also of 5 base pairs’ length). This alternating pattern of right- and left-handed half-twists is repeated throughout the length of the chromosome which, topologically speaking, would have no net twists. Such a chromosome does not need to either spin or unwind during replication. Rodley in later published work makes it clear that it was not the intention of his team that the model utilized alternating half turns of opposite helical sense, but lengths of DNA having alternating helical senses deployed in the available space along the DNA molecule that would not exactly be half turns.\n\nOne key observation in support of a double-helical structure is that the individual single strands of small circular viral and plasmid DNA are inseparable under the usual types of denaturation conditions, supporting a plectonemic, \"i.e.,\" twisted structure whose strands are topologically locked together. While human and bacterial chromosomes are far too large to isolate intact, but rather break into innumerable small fragments when extracted from their respective cells, there exist many species of small viral and plasmid chromosomes which because of their small sizes are stable in solution, and remain intact even with vigorous handling. Early studies quickly revealed that the strands of duplex circular chromosomes \"do not separate\" when subjected to conditions which readily denature linear DNA. For example, if calf thymus DNA (a heterogeneous collection of linear fragments) is merely boiled, the strands separate, and when slowly cooled, they re-anneal. In contrast, when the circular chromosomes of such organisms as the replicative form (RF) of the bacteriophage φX174, or the mammalian virus polyoma, are boiled, nothing at all happens to the native structure - they are completely resistant to thermal denaturation. Without a doubt, the single most skepticism-inspiring aspect of non-helical DNA theory was the failure of the strands of circular chromosomes to separate under conditions where the strands of linear DNA would readily do so. This was generally regarded as evidence that the strands were plectonemically wound together (\"i.e.,\" topologically linked).\n\nThe publication of the Rodley structure produced a small but significant ripple of interest in the molecular biological community. However, the level of interest was sufficiently high so that Crick - by that time one of the world’s most influential scientists - publicly suggested that the SBS structure (whose existence he doubted) should be laid to rest by intentionally creating it. Then, he reasoned, its properties could be ascertained, and then shown to be abnormal with respect to the known properties of native DNA. An investigation by Stettler \"et al\". followed closely on the heels of the Crick suggestion. They isolated the intact circular single strands of a circular duplex viral chromosome and then re-annealed them, giving a product they declared to be a \"base-paired circular duplex with no net helical twist\". If so, this would in fact be the Rodley \"side-by-side\" structure. They introduced a new terminology for this product, calling it \"Form V\" (extending the nomenclature of Forms I–IV used in sedimentation studies of DNA), and then went on to demonstrate that its electrophoretic mobility was clearly different from that of the native chromosome.\n\nThe results of this investigation were not accepted by some authors, who agreed that it was established satisfactorily that \"Form V\" is different from native DNA, but did not establish that \"Form V\" has the SBS structure. While the strands of native circular DNA are known not to be separable under common denaturing conditions, in 1996, following his earlier theoretical studies in 1969, Tai Te Wu reported having found conditions under which fully intact single strands of two different circular duplex plasmids could be separated. Wu believed that, because these plasmids have D-loops with DNA–RNA duplexes from RNA transcription only on their sense side, and since DNA–RNA interactions are stronger than DNA–DNA interactions, the sense and anti-sense strands would thus have different electrophoretic mobilities. He employed a very low voltage, so that the electrophoresis required 48 hours to go to completion. DNA sequencing then indicated that each of the two bands were enriched in one of the two strands of the chromosome. Separately, the Stetter \"et al\". experiment was revisited in 2009 by Youcheng Xu, who noted that brief re-annealing times (\"i.e.\" 20–30 minutes) resulted in anomalous structures which did not co-migrate with routinely prepared DNA topoisomers, while prolonged re-annealing times (in his case, 72 hours at 4º) resulted in structures which did co-migrate with the topoisomers, suggesting normal base-pairing. They interpreted their data as supporting the possibility that the two strands inside the native DNA double helix are winding ambidextrously rather than plectonemically, with left-handed and right-handed regions coexisting in a zero linking number topoisomer.\n\nThe discovery of topoisomerases, enzymes that can change the linking number of circular nucleic acids and thus \"unwind\" the replicating bacterial chromosome, was largely seen by the mainstream scientific community to satisfy the topological objection to the Watson–Crick helical structure. The later X-ray crystallography data for the nucleosome, which showed a helical DNA structure wrapped around the protein nucleosome core particle, was considered to provide further affirmation for the existence of the helical structure \"in vivo\". Non-double-helical models are not currently accepted by the mainstream scientific community.\n\nEnzymes capable of unwinding and rewinding DNA helical twists, namely topoisomerases and gyrases, are essential for life, and necessary for DNA replication in various \"in vitro\" synthesis systems.\n", "id": "35224853", "title": "Non-helical models of DNA structure"}
{"url": "https://en.wikipedia.org/wiki?curid=55219073", "text": "Miscarriage risks\n\nMiscarriage risks are those circumstances, conditions, and substances that increase the risk of miscarriage. Some risks are modifiable and can be changed. Other risks cannot be modified and can't be changed. Risks can be firmly tied to miscarriages and others are still under investigation. In addition, there are those circumstances and treatments that have not been found effective in preventing miscarriage. When a woman keeps having miscarriages, infertility is present.\n", "id": "55219073", "title": "Miscarriage risks"}
{"url": "https://en.wikipedia.org/wiki?curid=55319895", "text": "Genetic ecology\n\nGenetic ecology is the study of the stability and expression of varying genetic material within abiotic mediums. Typically, genetic data is not thought of outside of any organism save for criminal for criminal forensics. However, genetic material has the ability to be taken up by various organisms that exist within an abiotic medium through natural transformations that may occur. Thus, this field of study focuses on interaction, exchange, and expression of genetic material that may not be shared by species had they not been in the same environment.\n\nE.B. Ford was the first geneticist to begin work in this field of study. E.B. Ford worked mostly during the 1950s and is most noted for his work with \"Maniola jurtina\" and published a book entitled \"Ecological Genetics\" in 1975. This type of evolutionary biological study was only possible after gel electrophoresis had been designed in 1937. Prior to this, a high throughput method for DNA analysis did not exist. This field of study began to become more popular following the 1980s with the development of polymerase chain reaction (PCR 1985) and poly-acrylamide gel electrophoresis (p. 1967). With this technology, segments of DNA could be sequenced, amplified, and proteins produced using bacterial transformations. The genetic material along with the proteins could be analyzed and more correct phylogenetic trees could be created. \n\nSince E.B. Ford's research, multiple other genetic ecologists have continued study within the field of genetic ecology such as PT Hanford Alina von Thaden, and many others.\n\nGenetic information may transfer throughout an ecosystem in multiple ways. The first of which, on the smallest scale, being bacterial gene transfer (see bacterial transformations). Bacteria have the ability to exchange DNA. This DNA exchange, or horizontal gene transfer, may provide various species of bacteria with the genetic information they need to survive in an environment. This can help many bacterial species survive within an environment.\n\nA similar event has the ability to happen between plants and bacteria. For example, Agrobacterium tumefaciens has the ability to introduce genes into plants to cause the development of Gall disease. This occurs through genetic transfer between the \"A. tumefaciens\" and between the plant in question.\n\nIn fact, a similar event occurs each time viral infections occur within living organisms. The viruses, whether positive or negative sense viruses, require a living organism to replicate their genes and produce more viruses. Once a virus is inside a living organism, it utilizes polymerases, ribosomes, and other biomolecules to replicate its own genetic material and to produce more virus genetic material similar to the original virus. Thus, gene transfer may occur through many varying means. Thus, the study of this gene transfer throughout each ecosystem, whether it be through a bacterial ecosystem or through the ecosystem of an organism, genetic ecology is the study of this gene transfer and its causes.\n", "id": "55319895", "title": "Genetic ecology"}
{"url": "https://en.wikipedia.org/wiki?curid=54870147", "text": "Chromosome 2q Deletion\n\nChromosome 2q deletion is a chromosome abnormality that occurs when there is a missing copy of the genetic material located on the long arm (q) of chromosome 2. The severity of the condition and the signs and symptoms depend on the size and location of the deletion and which genes are involved. Features that often occur in people with chromosome 2q deletion include developmental delay, intellectual disability, behavioral problems, and distinctive facial features. Most cases are not inherited, but people can pass the deletion on to their children. Treatment is based on the signs and symptoms present in each person.\n", "id": "54870147", "title": "Chromosome 2q Deletion"}
{"url": "https://en.wikipedia.org/wiki?curid=53165774", "text": "Nuclear sexing\n\nNuclear sexing is a technique for genetic sex determination in those species where XX chromosome pair is present. Nuclear sexing can be done by identifying Barr body, a drumstick like appendage located in the rim of the nucleus in somatic cells. Barr body is the inactive X chromosome which lies condensed in the nucleus of somatic cells. A typical human female has only one Barr body per somatic cell, while a typical human male has none. Though a Barr body can be sought in any human nucleated cell, circulating mononuclear cells are commonly used for this purpose. These cells are cultured, and treated with chemicals such as colcemid to arrest mitosis in metaphase.\nA minimum of 30 percent of sex chromatin indicates genetic female sex.\n", "id": "53165774", "title": "Nuclear sexing"}
{"url": "https://en.wikipedia.org/wiki?curid=145563", "text": "Prophase\n\nProphase (from the Greek πρό, \"before\" and φάσις, \"stage\") is the first stage of cell division in both mitosis and meiosis. Beginning after interphase, DNA has already been replicated when the cell enters prophase. The main occurrences in prophase are the condensation of the chromatin and the disappearance of the nucleolus.\n\nMicroscopy can be used to visualize condensed chromosomes as they move through meiosis and mitosis.\n\nVarious DNA stains are used to treat cells such that condensing chromosomes can be visualized as the move through prophase.\n\nThe giemsa G-banding technique is commonly used to identify mammalian chromosomes, utilizing the technology on plant cells was difficult due to the high degree of chromosome compaction in plant cells. G-banding was fully realized for plant chromosomes in 1990. During both meiotic and mitotic prophase, giemsa staining can be applied to cells to elicit G-banding in chromosomes. Silver staining, a more modern technology, in conjunction with giesma staining can be used to image the synaptonemal complex throughout the various stages of meiotic prophase. To perform G-banding, chromosomes must be fixed, and thus it is not possible to perform on living cells.\n\nFluorescent stains such as DAPI can be used in both live plant and animal cells. These stains do not band chromosomes, but instead allow for DNA probing of specific regions and genes. Use of fluorescent microscopy has vastly improved spatial resolution.\n\nProphase is the first stage of mitosis in animal cells, and the second stage of mitosis in plant cells.  At the start of prophase there are two identical copies of each chromosome in the cell due to replication in interphase. These copies are referred to as sister chromatids and are attached by DNA element called the centromere. The main events of prophase are: the condensation of chromosomes, the movement of the centrosomes, the formation of the mitotic spindle, and the beginning of nucleoli break down.\n\nDNA that was replicated in interphase is condensed from molecules with lengths reaching 4 cm to chromosomes that are measured in micrograms. This process employs the condensin complex. Condensed chromosomes consist of two sister chromatids joined at the centromere.\n\nDuring prophase in animal cells, centrosomes move far enough apart to be resolved using a light microscope. Microtubule activity in each centrosome is increased due to recruitment of γ-tubulin. Replicated centrosomes from interphase move apart towards opposite poles of the cell, powered by centrosome associated motor proteins. Interdigitated interpolar microtubules from each centrosome interact with each other, helping to move the centrosomes to opposite poles.\n\nMicrotubules involved in the interphase scaffolding break down as the replicated centrosomes separate. The movement of centrosomes to opposite poles is accompanied in animal cells by the organization of individual radial microtubule arrays (asters) by each centromere. Interpolar microtubules from both centrosomes interact, joining the sets of microtubules and forming the basic structure of the mitotic spindle. In cells without centrioles chromosomes can nucleate microtubule assembly into the mitotic apparatus. In plant cells, microtubules gather at opposite poles and begin to form the spindle apparatus at locations called foci. The mitotic spindle is of great importance in the process of mitosis and will eventually segregate the sister chromatids in metaphase.\n\nThe nucleoli begin to break down in prophase, resulting in the discontinuation of ribosome production. This indicates a redirection of cellular energy from general cellular metabolism to cellular division. The nuclear envelope stays intact during this process.\n\nMeiosis involves two rounds of chromosome segregation and thus undergoes prophase twice, resulting in prophase I and prophase II. Prophase I the most complex phase in all of meiosis because homologous chromosomes must pair and exchange genetic information. Prophase II is very similar to mitotic prophase.\n\nProphase I is divided into five phases: leptotene, zygotene, pachytene, diplotene, and diakinesis. In addition to the events that occur in mitotic prophase, several crucial events occur within these phases such as pairing of homologous chromosomes and the reciprocal exchange of genetic material between these homologous chromosomes. Prophase I occurs at different speeds dependent on species and sex. Many species arrest meiosis in diplotene of prophase I until ovulation. In humans, decades can pass as oocytes remain arrested in prophase I only to quickly complete meiosis I prior to ovulation.\n\nIn the first stage of prophase I, lepotene (from the Greek for “delicate”), chromosomes begin to condense. Each chromosome is in a haploid state and consists of two sister chromatids; however, the chromatin of the sister chromatids is not yet condensed enough to be resolvable in microscopy. Homologous regions within homologous chromosome pairs begin to associate with each other. \n\nIn the second phase of prophase I, zygotene (from the Greek for “conjugation”), all maternally and paternally derived chromosomes have found their homologous partner. The homologous pairs then undergo synapsis, a process by which the synaptonemal complex (a proteinaceous structure) aligns corresponding regions of genetic information on maternally and paternally derived non-sister chromatids of homologous chromosome pairs.  The paired homologous chromosome bound by the synaptonemal complex are referred to as bivalents or tetrads. Sex (X and Y) chromosomes do not fully synapse because only a small region of the chromosomes are homologous. \n\nThe nucleolus moves from a central to a peripheral position in the nucleus.\n\nThe third phase of prophase I, pachytene (from the Greek for “thick”), begins at the completion of synapsis.  Chromatin has condensed enough that chromosomes can now be resolved in microscopy. Structures called recombination nodules form on the synaptonemal complex of bivalents. These recombination nodules facilitate genetic exchange between the non-sister chromatids of the synaptonemal complex in an event known as crossing-over or genetic recombination. Multiple recombination events can occur on each bivalent. In humans, an average of 2-3 events occur on each chromosome.\n\nIn the fourth phase of prophase I, diplotene (from the Greek for “twofold”), crossing-over is completed. Homologous chromosomes retain a full set of genetic information; however, the homologous chromosomes are now of mixed maternal and paternal descent. Visible junctions called chaismata hold the homologous chromosomes together at locations where recombination occurred as the synaptonemal complex dissolves. It is at this stage where meiotic arrest occurs in many species.\n\nIn the fifth and final phase of prophase I, diakinesis (from the Greek for “double movement”), full chromatin condensation has occurred and all four sister chromatids can be seen in bivalents with microscopy. As in mitotic prophase, meiotic prophase ends with the spindle apparatus beginning to form, and the nuclear membrane beginning to break down.\n\nProphase II of meiosis is very similar to prophase of mitosis. The most noticeable difference is that prophase II occurs with a haploid number of chromosomes as opposed to the diploid number in mitotic prophase.  In both animal and plant cells chromosomes may de-condense during telophase I requiring them to re-condense in prophase II.  If chromosomes do not need to re-condense, prophase II often proceeds very quickly as is seen in the model organism Arabidopsis.\n\nThe most notable difference between prophase in plant cells and animal cells occurs because plant cells lack centrioles. The organization of the spindle apparatus is associated instead with foci at opposite poles of the cell or is mediated by chromosomes. Another notable difference is preprophase, and additional step in plant mitosis that results in formation of the preprophase band, a structure composed of microtubules. In mitotic prophase I of plants this band disappears.\n\nProphase I in mitosis is the most complex iteration of prophase that occurs in both plant cells and animal cells.  To ensure pairing of homologous chromosomes and recombination of genetic material occurs properly, there are cellular checkpoints in place. The meiotic checkpoint network is a DNA damage response system that controls double strand break repair, chromatin structure, and the movement and pairing of chromosomes. The system consists of multiple pathways (including the meiotic recombination checkpoint) that prevent the cell from entering metaphase I with errors due to recombination.\n\n", "id": "145563", "title": "Prophase"}
{"url": "https://en.wikipedia.org/wiki?curid=19595", "text": "Mendelian inheritance\n\nMendelian inheritance is a type of biological inheritance that follows the laws originally proposed by Gregor Mendel in 1865 and 1866 and re-discovered in 1900. These laws were initially controversial. When Mendel's theories were integrated with the Boveri–Sutton chromosome theory of inheritance by Thomas Hunt Morgan in 1915, they became the core of classical genetics. Ronald Fisher combined these ideas with the theory of natural selection in his 1930 book \"The Genetical Theory of Natural Selection\", putting evolution onto a mathematical footing and forming the basis for population genetics within the modern evolutionary synthesis.\n\nThe principles of Mendelian inheritance were named for and first derived by Gregor Johann Mendel, a nineteenth-century Austrian monk who formulated his ideas after conducting simple hybridisation experiments with pea plants (\"Pisum sativum\") he had planted in the garden of his monastery. Between 1856 and 1863, Mendel cultivated and tested some 5,000 pea plants. From these experiments, he induced two generalizations which later became known as \"Mendel's Principles of Heredity\" or \"Mendelian inheritance\". He described these principles in a two-part paper, \"Versuche über Pflanzen-Hybriden\" (\"Experiments on Plant Hybridization\"), that he read to the Natural History Society of Brno on 8 February and 8 March 1865, and which was published in 1866.\n\nMendel's conclusions were largely ignored by the vast majority. Although they were not completely unknown to biologists of the time, they were not seen as generally applicable, even by Mendel himself, who thought they only applied to certain categories of species or traits. A major block to understanding their significance was the importance attached by 19th-century biologists to the apparent blending of inherited traits in the overall appearance of the progeny, now known to be due to multi-gene interactions, in contrast to the organ-specific binary characters studied by Mendel. In 1900, however, his work was \"re-discovered\" by three European scientists, Hugo de Vries, Carl Correns, and Erich von Tschermak. The exact nature of the \"re-discovery\" has been debated: De Vries published first on the subject, mentioning Mendel in a footnote, while Correns pointed out Mendel's priority after having read De Vries' paper and realizing that he himself did not have priority. De Vries may not have acknowledged truthfully how much of his knowledge of the laws came from his own work and how much came only after reading Mendel's paper. Later scholars have accused Von Tschermak of not truly understanding the results at all.\n\nRegardless, the \"re-discovery\" made Mendelism an important but controversial theory. Its most vigorous promoter in Europe was William Bateson, who coined the terms \"genetics\" and \"allele\" to describe many of its tenets. The model of heredity was contested by other biologists because it implied that heredity was discontinuous, in opposition to the apparently continuous variation observable for many traits. Many biologists also dismissed the theory because they were not sure it would apply to all species. However, later work by biologists and statisticians such as Ronald Fisher showed that if multiple Mendelian factors were involved in the expression of an individual trait, they could produce the diverse results observed, and thus showed that Mendelian genetics is compatible with natural selection. Thomas Hunt Morgan and his assistants later integrated Mendel's theoretical model with the chromosome theory of inheritance, in which the chromosomes of cells were thought to hold the actual hereditary material, and created what is now known as classical genetics, a highly successful foundation which eventually cemented Mendel's place in history.\n\nMendel's findings allowed scientists such as Fisher and J.B.S. Haldane to predict the expression of traits on the basis of mathematical probabilities. An important aspect of Mendel's success can be traced to his decision to start his crosses only with plants he demonstrated were true-breeding. He only measured discrete (binary) characteristics, such as color, shape, and position of the seeds, rather than quantitatively variable characteristics. He expressed his results numerically and subjected them to statistical analysis. His method of data analysis and his large sample size gave credibility to his data. He had the foresight to follow several successive generations (F2, F3) of pea plants and record their variations. Finally, he performed \"test crosses\" (backcrossing descendants of the initial hybridization to the initial true-breeding lines) to reveal the presence and proportions of recessive characters.\n\nMendel discovered that, when he crossed purebred white flower and purple flower pea plants (the parental or P generation), the result was not a blend. Rather than being a mix of the two, the offspring (known as the F generation) was purple-flowered. When Mendel self-fertilized the F generation pea plants, he obtained a purple flower to white flower ratio in the F generation of 3 to 1. The results of this cross are tabulated in the Punnett square to the right.\n\nHe then conceived the idea of heredity units, which he called \"factors\". Mendel found that there are alternative forms of factors—now called genes—that account for variations in inherited characteristics. For example, the gene for flower color in pea plants exists in two forms, one for purple and the other for white. The alternative \"forms\" are now called alleles. For each biological trait, an organism inherits two alleles, one from each parent. These alleles may be the same or different. An organism that has two identical alleles for a gene is said to be homozygous for that gene (and is called a homozygote). An organism that has two different alleles for a gene is said be heterozygous for that gene (and is called a heterozygote).\n\nMendel hypothesized that allele pairs separate randomly, or segregate, from each other during the production of gametes: egg and sperm. Because allele pairs separate during gamete production, a sperm or egg carries only one allele for each inherited trait. When sperm and egg unite at fertilization, each contributes its allele, restoring the paired condition in the offspring. This is called the Law of Segregation. Mendel also found that each pair of alleles segregates independently of the other pairs of alleles during gamete formation. This is known as the Law of Independent Assortment.\n\nThe genotype of an individual is made up of the many alleles it possesses. An individual's physical appearance, or phenotype, is determined by its alleles as well as by its environment. The presence of an allele does not mean that the trait will be expressed in the individual that possesses it. If the two alleles of an inherited pair differ (the heterozygous condition), then one determines the organism’s appearance and is called the dominant allele; the other has no noticeable effect on the organism’s appearance and is called the recessive allele. Thus, in the example above the dominant purple flower allele will hide the phenotypic effects of the recessive white flower allele. This is known as the Law of Dominance but it is not a transmission law: it concerns the expression of the genotype. The upper case letters are used to represent dominant alleles whereas the lowercase letters are used to represent recessive alleles.\n\nIn the pea plant example above, the capital \"B\" represents the dominant allele for purple flowers and lowercase \"b\" represents the recessive allele for white flowers. Both parental plants were true-breeding, and one parental variety had two alleles for purple flowers (\"BB\") while the other had two alleles for white flowers (\"bb\"). As a result of fertilization, the F hybrids each inherited one allele for purple flowers and one for white. All the F hybrids (\"Bb\") had purple flowers, because the dominant \"B\" allele has its full effect in the heterozygote, while the recessive \"b\" allele has no effect on flower color. For the F plants, the ratio of plants with purple flowers to those with white flowers (3:1) is called the phenotypic ratio. The genotypic ratio, as seen in the Punnett square, is 1 \"BB\" : 2 \"Bb\" : 1 \"bb\".\n\nThe Law of Segregation states that every individual organism contains two alleles for each trait, and that these alleles segregate (separate) during meiosis such that each gamete contains only one of the alleles. An offspring thus receives a pair of alleles for a trait by inheriting homologous chromosomes from the parent organisms: one allele for each trait from each parent.\n\nMolecular proof of this principle was subsequently found through observation of meiosis by two scientists independently, the German botanist Oscar Hertwig in 1876, and the Belgian zoologist Edouard Van Beneden in 1883. Paternal and maternal chromosomes get separated in meiosis and the alleles with the traits of a character are segregated into two different gametes. Each parent contributes a single gamete, and thus a single, randomly successful allele copy to their offspring and fertilization.\n\nThe Law of Independent Assortment states that alleles for separate traits are passed independently of one another from parents to offspring. That is, the biological selection of an allele for one trait has nothing to do with the selection of an allele for any other trait. Mendel found support for this law in his dihybrid cross experiments (Fig. 1). In his monohybrid crosses, an idealized 3:1 ratio between dominant and recessive phenotypes resulted. In dihybrid crosses, however, he found a 9:3:3:1 ratios (Fig. 2). This shows that each of the two alleles is inherited independently from the other, with a 3:1 phenotypic ratio for each.\n\nIndependent assortment occurs in eukaryotic organisms during meiotic prophase I, and produces a gamete with a mixture of the organism's chromosomes. The physical basis of the independent assortment of chromosomes is the random orientation of each bivalent chromosome along the metaphase plate with respect to the other bivalent chromosomes. Along with crossing over, independent assortment increases genetic diversity by producing novel genetic combinations.\n\nThere are many violations of independent assortment due to genetic linkage.\n\nOf the 46 chromosomes in a normal diploid human cell, half are maternally derived (from the mother's egg) and half are paternally derived (from the father's sperm). This occurs as sexual reproduction involves the fusion of two haploid gametes (the egg and sperm) to produce a new organism having the full complement of chromosomes. During gametogenesis—the production of new gametes by an adult—the normal complement of 46 chromosomes needs to be halved to 23 to ensure that the resulting haploid gamete can join with another gamete to produce a diploid organism. An error in the number of chromosomes, such as those caused by a diploid gamete joining with a haploid gamete, is termed aneuploidy.\n\nIn independent assortment, the chromosomes that result are randomly sorted from all possible maternal and paternal chromosomes. Because zygotes end up with a random mix instead of a pre-defined \"set\" from either parent, chromosomes are therefore considered assorted independently. As such, the zygote can end up with any combination of paternal or maternal chromosomes. Any of the possible variants of a zygote formed from maternal and paternal chromosomes will occur with equal frequency. For human gametes, with 23 pairs of chromosomes, the number of possibilities is 2 or 8,388,608 possible combinations. The zygote will normally end up with 23 chromosomes pairs, but the origin of any particular chromosome will be randomly selected from paternal or maternal chromosomes. This contributes to the genetic variability of progeny.\nMendel's Law of Dominance states that recessive alleles will always be masked by dominant alleles. Therefore, a cross between a homozygous dominant and a homozygous recessive will always express the dominant phenotype, while still having a heterozygous genotype.\nThe Law of Dominance can be explained easily with the help of a mono hybrid cross experiment:-\nIn a cross between two organisms pure for any pair (or pairs) of contrasting traits (characters), the character that appears in the F1 generation is called \"dominant\" and the one which is suppressed (not expressed) is called \"recessive.\"\nEach character is controlled by a pair of dissimilar factors. Only one of the characters expresses. The one which expresses in the F1 generation is called Dominant.\nHowever, the law of dominance is not universally applicable.\n\nA Mendelian trait is one that is controlled by a single locus in an inheritance pattern. In such cases, a mutation in a single gene can cause a disease that is inherited according to Mendel's laws. Examples include sickle-cell anemia, Tay-Sachs disease, cystic fibrosis and xeroderma pigmentosa. A disease controlled by a single gene contrasts with a multi-factorial disease, like arthritis, which is affected by several loci (and the environment) as well as those diseases inherited in a non-Mendelian fashion.\n\nMendel explained inheritance in terms of discrete factors—genes—that are passed along from generation to generation according to the rules of probability. Mendel's laws are valid for all sexually reproducing organisms, including garden peas and human beings. However, Mendel's laws stop short of explaining some patterns of genetic inheritance. For most sexually reproducing organisms, cases where Mendel's laws can strictly account for the patterns of inheritance are relatively rare. Often, the inheritance patterns are more complex.\n\nThe F offspring of Mendel's pea crosses always looked like one of the two parental varieties. In this situation of \"complete dominance,\" the dominant allele had the same phenotypic effect whether present in one or two copies. But for some characteristics, the F hybrids have an appearance \"in between\" the phenotypes of the two parental varieties. A cross between two four o'clock (\"Mirabilis jalapa\") plants shows this common exception to Mendel's principles. Some alleles are neither dominant nor recessive. The F generation produced by a cross between red-flowered (RR) and white flowered (WW) \"Mirabilis jalapa\" plants consists of pink-colored flowers (RW). Which allele is dominant in this case? Neither one. This third phenotype results from flowers of the heterzygote having less red pigment than the red homozygotes. Cases in which one allele is not completely dominant over another are called incomplete dominance. In incomplete dominance, the heterozygous phenotype lies somewhere between the two homozygous phenotypes.\n\nA similar situation arises from codominance, in which the phenotypes produced by both alleles are clearly expressed. For example, in certain varieties of chicken, the allele for black feathers is codominant with the allele for white feathers. Heterozygous chickens have a color described as \"erminette\", speckled with black and white feathers. Unlike the blending of red and white colors in heterozygous four o'clocks, black and white colors appear separately in chickens. Many human genes, including one for a protein that controls cholesterol levels in the blood, show codominance, too. People with the heterozygous form of this gene produce two different forms of the protein, each with a different effect on cholesterol levels.\n\nIn Mendelian inheritance, genes have only two alleles, such as \"a\" and \"A\". In nature, such genes exist in several different forms and are therefore said to have multiple alleles. A gene with more than two alleles is said to have multiple alleles. An individual, of course, usually has only two copies of each gene, but many different alleles are often found within a population. One of the best-known examples is coat color in rabbits. A rabbit's coat color is determined by a single gene that has at least four different alleles. The four known alleles display a pattern of simple dominance that can produce four coat colors. Many other genes have multiple alleles, including the human genes for ABO blood type.\n\nFurthermore, many traits are produced by the interaction of several genes. Traits controlled by two or more genes are said to be polygenic traits. \"Polygenic\" means \"many genes.\" For example, at least three genes are involved in making the reddish-brown pigment in the eyes of fruit flies. Polygenic traits often show a wide range of phenotypes. The broad variety of skin color in humans comes about partly because at least four different genes probably control this trait.\n\n\n\n", "id": "19595", "title": "Mendelian inheritance"}
{"url": "https://en.wikipedia.org/wiki?curid=56027886", "text": "Family resemblance (anthropology)\n\nFamily resemblance refers to physical similarities shared between close relatives, especially between parents and children and between siblings. In psychology, the similarities of personality are also observed.\n\n\n", "id": "56027886", "title": "Family resemblance (anthropology)"}
{"url": "https://en.wikipedia.org/wiki?curid=825393", "text": "Crossbreed\n\nA crossbreed is an organism with purebred parents of two different breeds, varieties, or populations. \"Crossbreeding\", sometimes called \"designer crossbreeding\", is the process of breeding such an organism, often with the intention to create offspring that share the traits of both parent lineages, or producing an organism with hybrid vigor. While crossbreeding is used to maintain health and viability of organisms, irresponsible crossbreeding can also produce organisms of inferior quality or dilute a purebred gene pool to the point of extinction of a given breed of organism.\n\nA domestic animal of unknown ancestry, where the breed status of only one parent or grandparent is known, may also be called a crossbreed though the term \"mixed breed\" is technically more accurate. Outcrossing is a type of crossbreeding used within a purebred breed to increase the genetic diversity within the breed, particularly when there is a need to avoid inbreeding.\n\nIn animal breeding, \"crossbreeds\" are crosses within a single species, while \"hybrids\" are crosses between different species. In plant breeding terminology, the term \"crossbreed\" is uncommon. No universal term is used to distinguish hybridization or crossing within a population from those between populations, or even those between species.\n\nThe many newly developed and recognized breeds of domestic cat are crossbreeds between existing, well-established breeds (sometimes with limited hybridization with some wild species), to either combine selected traits from the foundation stock, or propagate a rare mutation without excessive inbreeding. However, some nascent breeds such as the Aegean cat are developed entirely from a local landrace population. Most experimental cat breeds are crossbreeds.\n\nIn cattle, there are systems of crossbreeding. In many crossbreeds, one is larger than the other. One is used when the purebred females are particularly adapted to a specific environment, and are crossed with purebred bulls from another environment to produce a generation having traits of both parents.\n\nThe large number of breeds of sheep, which vary greatly, creates an opportunity for crossbreeding to be used to tailor production of lambs to the goal of the individual stockman.\n\nResults of crossbreeding classic and woolly breeds of llama are unpredictable. The resulting offspring displays physical characteristics of either parent, or a mix of characteristics from both, periodically producing a fleeced llama. \nThe results are increasingly unpredictable when both parents are crossbreeds, with possibility of the offspring displaying characteristics of a grandparent, not obvious in either parent. \n\nA \"crossbred\" dog is a cross between two (sometimes more) known breeds, and is usually distinguished from a \"mixed-breed dog\", which has ancestry from many sources, some of which may not be known. Crossbreeds are popular, due to the belief that they have increased vigor without loss of attractiveness of the dog. Certain planned crossbreeding between purebred dogs of different breeds can produce puppies worth more than their purebred parents, due to a high demand.\n\nCrossbreeding in horses is often done with the intent of ultimately creating a new breed of horse. One type of modern crossbreeding in horses is used to create many of the warmblood breeds. Warmbloods are a type of horse used in the sport horse disciplines, usually registered in an open stud book by a studbook selection procedure that evaluates conformation, pedigree and, in some animals, a training or performance standard. Most warmblood breeds began as a cross of draft horse breeds on Thoroughbreds, but have, in some cases, developed over the past century to the point where they are considered to be a true-breeding population and have a closed stud book. Other types of recognized crossbreeding include that within the American Quarter Horse, which will register horses with one Thoroughbred parent and one registered Quarter Horse parent in the \"Appendix\" registry, and allow such animals full breed registration status as Quarter Horses if they meet a certain performance standard. Another well-known crossbred horse is the Anglo-Arabian, which may be produced by a purebred Arabian horse crossed on a Thoroughbred, or by various crosses of Anglo-Arabians with other Anglo-Arabians, as long as the ensuing animal never has more than 75% or less than 25% of each breed represented in its pedigree.\n\nA hybrid animal is one with parentage of two separate species, differentiating it from crossbred animals, which have parentage of the same species. Hybrids are usually, but not always, sterile.\n\nOne of the most ancient types of hybrid animal is the mule, a cross between a female horse and a male donkey. The liger is a hybrid cross between a male lion and female tiger. The yattle is a cross between a cow and a yak. Other crosses include the tigon (between a male tiger and female lion) and yakalo (between a yak and buffalo). The Incas recognized that hybrids of Lama glama (llama) and Lama pacos (alpaca) resulted in a hybrid with none of the advantages of either parent.\n\nAt one time it was thought that dogs and wolves were separate species, and the crosses between dogs and wolves were called wolf hybrids. Today wolves and dogs are both recognized as \"Canis lupus\", but the old term \"wolf hybrid\" is still used.\n\nA mixed-breed animal is defined as having undocumented or unknown parentage, while a crossbreed generally has known, usually purebred parents of two distinct breeds or varieties. A dog of unknown parentage is often called a mixed-breed dog, \"mutt\" or \"mongrel.\" A cat of unknown parentage is often referred to as domestic short-haired or domestic long-haired cat generically, and in some dialects is often called a \"moggy\". A horse of unknown bloodlines is a grade horse.\n\n", "id": "825393", "title": "Crossbreed"}
{"url": "https://en.wikipedia.org/wiki?curid=56073965", "text": "Plant–fungus horizontal gene transfer\n\nPlant–fungus horizontal gene transfer is the movement of genetic material between individuals in the plant and fungus kingdoms. Horizontal gene transfer is universal in fungi, viruses, bacteria, and other eukaryotes. Horizontal gene transfer research often focuses on prokaryotes because of the abundant sequence data from diverse lineages, and because it is assumed not to play a significant role in eukaryotes.\n\nMost plant–fungus horizontal gene transfer events are ancient and rare, but they may have provided important gene functions leading to wider substrate use and habitat spread for plants and fungi. Since these events are rare and ancient, they have been difficult to detect and remain relatively unknown. Plant–fungus interactions could play a part in a multi-horizontal gene transfer pathway among many other organisms.\n\nFungus–plant-mediated horizontal gene transfer can occur via phagotrophic mechanisms (mediated by phagotrophic eukaryotes) and nonphagotropic mechanisms. Nonphagotrophic mechanisms have been seen in the transmission of transposable elements, plastid-derived endosymbiotic gene transfer, prokaryote-derived gene transfer, \"Agrobacterium tumefaciens\"-mediated DNA transfer, cross-species hybridization events, and gene transfer between mitochondrial genes. Horizontal gene transfer could bypass eukaryotic barrier features like linear chromatin-based chromosomes, intron–exon gene structures, and the nuclear envelope.\n\nHorizontal gene transfer occurs between microorganisms sharing overlapping ecological niches and associations like parasitism or symbiosis. Ecological association can facilitate horizontal gene transfer in plants and fungi and is an unstudied factor in shared evolutionary histories.\n\nMost horizontal gene transfers from fungi into plants predate the rise of land plants. A greater genomic inventory of gene family and taxon sampling has been identified as a desirable prerequisite for identifying further plant–fungus events.\n\nEvidence for gene transfer between fungi and eukaryotes is discovered indirectly. Evidence is found in the unusual features of genetic elements. These features include: inconsistency between phylogeny across genetic elements, high DNA or amino acid similarity from phylogenetically distant organisms, irregular distribution of genetic elements in a variety of species, similar genes shared among species within a specific habitat or geography independent of their phylogenetic relationship, and gene characteristics inconsistent with the resident genome such as high guanine and cytosine content, codon usage, and introns.\n\nAlternative hypotheses and explanations for such findings include erroneous species phylogenies, inappropriate comparison of paralogous sequences, sporadic retention of shared ancestral characteristics, uneven rates of character change in other lineages, and introgressive hybridization.\n\nThe \"complexity hypothesis\" is a different approach to understanding why informational genes have less success in being transferred than operational genes. It has been proposed that informational genes are part of larger, more conglomerate systems, while operational genes are less complex, allowing them to be horizontally transferred at higher frequencies. The hypothesis incorporates the \"continual hypothesis\", which states that horizontal gene transfer is constantly occurring in operational genes.\n\nPlant–fungus horizontal gene transfer could take place during plant infection. There are many possible vectors, such as plant–fungus–insect interactions. The ability for fungi to infect other organisms provides this possible pathway.\n\nA fungus–plant pathway has been demonstrated in rice (\"Oryza sativa\") through ancestral lineages. A phylogeny was constructed from 1689 identified genes and all homologs available from the rice genome (3177 gene families). Fourteen candidate plant–fungus horizontal gene transfer events were identified, nine of which showed infrequent patterns of transfer between plants and fungi. From the phylogenetic analysis, horizontal gene transfer events could have contributed to the L-fucose permease sugar transporter, zinc binding alcohol dehydrogenase, membrane transporter, phospholipase/carboxylesterase, iucA/iucC family protein in siderophore biosynthesis, DUF239 domain protein, phosphate-response 1 family protein, a hypothetical protein similar to zinc finger (C2H2-type) protein, and another conserver hypothetical protein.\n\nSome plants may have obtained the shikimate pathway from symbiotic fungi. Plant shikimate pathway enzymes share similarities to prokaryote homologs and could have ancestry from a plastid progenitor genome. It is possible that the shikimate pathway and the pentafunctional \"arom\" have their ancient origins in eukaryotes or were conveyed by eukaryote–eukaryote horizontal gene transfer. The evolutionary history of the pathway could have been influenced by a prokaryote-to-eukaryote gene transfer event. Ascomycete fungi along with zygomycetes, basidiomycetes, apicomplexa, ciliates, and oomycetes retained elements of an ancestral pathway given through the bikont/unikont eukaryote root.\n\nFungi and bacteria could have contributed to the phenylpropanoid pathway in ancestral land plants for the synthesis of flavonoids and lignin through horizontal gene transfer. Phenylalanine ammonia lyase (PAL) is known to be present in fungi, such as Basidomiciete yeast like \"Rhodotorula\" and Ascomycetes such as \"Aspergillus\" and \"Neurospora\". These fungi participate in the catabolism of phenylalanine for carbon and nitrogen. PAL in some plants and fungi also has a tyrosine ammonia lyase (TAL) for the synthesis of p-coumaric acid into p-coumaroyl-CoA. PAL likely emerged from bacteria in an antimicrobial role. Horizontal gene transfer took place through an pre-Dikarya divergent fungal lineage and a \"Nostocale\" or soil-sediment bacterium through symbiosis. The fungal PAL was then transferred to an ancestor of a land plant by an ancient arbuscular mycorrhizal symbiosis that later developed in the phenylpropanoid pathway and land plant colonization. PAL enzymes in early bacteria and fungi could have contributed to protection against ultraviolet radiation, acted as a light capturing pigment, or assisted in antimicrobial defense.\n\n\"Sterigmatocystin\" gene transfer has been observed with \"Podospora anserina\" and \"Aspergillus\". Horizontal gene transfer in \"Aspergillus\" and \"Podospora\" contributed to fungal metabolic diversity in secondary metabolism. \"Aspergillus nidulans\" produces sterigmatocystin – a precursor to aflatoxins. \"Aspergillus\" was found to have horizontally transferred genes to \"Podospora anserina\". \"Podospora\" and \"Aspergillus\" show high conservation and microsynteny sterigmatocystin/aflatoxin clusters along with intergenic regions containing 14 binding sites for AfIR, a transcription factor for the activation of sterigmatocystin/aflatoxin biosynthetic genes. \"Aspergillus\" to \"Podospora\" represents a large metabolic gene transfer which could have contributed to fungal metabolic diversity. Transposable elements and other mobile genetic elements like plasmids and viruses could allow for chromosomal rearrangement and integration of foreign genetic material. Horizontal gene transfer could have significantly contributed to fungal genome remodeling and metabolic diversity.\n\nIn \"Stagonospora\" and \"Pyrenophora\", as well as in \"Fusarium\" and \"Alternaria\", horizontal gene transfer provides a powerful mechanism for fungi to acquire pathogenic capabilities to infect a new host plant. Horizontal gene transfer and interspecific hybridization between pathogenic species allow for hybrid offspring with an expanded host range. This can cause disease outbreaks on new crops when an encoded protein is able to cause pathogenicity.\nThe interspecific transfer of virulence factors in fungal pathogens has been shown between \"Stagonospora modorum\" and \"Pyrenophora tritici-repentis\", where a host-selective toxin from \"S. nodorum\" conferred virulence to \"P. tritici-repentis\" on wheat.\n\nIn \"Fusarium\", a nonpathogenic strain was experimentally converted into a pathogen and could have contributed to pathogen adaption in large genome portions. \"Fusarium graminearum\", \"Fusarium verticilliodes\", and \"Fusarium oxysprorum\" are maize and tomato pathogens that produce fumonisin mycotoxins that contaminate grain. These examples highlight the apparent polyphyletic origins of host specialization and the emergence of new pathogenic lineages distinct from genetic backgrounds. The ability to transfer genetic material could increase disease in susceptible plant populations.\n", "id": "56073965", "title": "Plant–fungus horizontal gene transfer"}
{"url": "https://en.wikipedia.org/wiki?curid=17064239", "text": "American Genetic Association\n\nThe American Genetic Association (AGA), formerly the \"American Breeders' Association\", is a USA-based learned society dedicated to the study of genetics. Founded in 1903, the organization publishes the \"Journal of Heredity\".\nThe American Genetic Association (AGA), formerly the American Breeders' Association, is a professional organization founded to encourage the study of comparative genetics and genomics, and to promote the application of genetic and genomic methods to the documentation, conservation, and management of organismal diversity.\n\nThe American Breeders Association held its first meeting in 1903 to discuss the “new” science of genetics that arose from Charles Darwin’s theory of evolution and Gregor Mendel’s discoveries of the laws of inheritance. The organization was established “to study the laws of breeding and to promote the improvement of plants and animals by the development of expert methods of breeding.” \n\nIn 1914, the American Breeders Association broadened its scope and became the American Genetic Association. Today, the AGA’s interests encompass evolutionary diversity and genomics across taxa and subject areas, including conservation genetics, phylogenetics, phylogeography, gene function, and the genetics of domestication.\n\nThe AGA disseminates progress in these fields through its publication, \"Journal of Heredity\". It supports research and scholarship through sponsorship of an annual President’s Symposium, special events awards, the Stephen J. O’Brien Award, and the Evolutionary, Ecological, or Conservation Genomics Research Awards.\n\nThe AGA is a 501(c)(3) corporation, incorporated in Oregon, Federal Tax ID 53-0204656.\n\nThe American Genetic Association’s annual conference, referred to as the President’s Symposium, is organized by the AGA President for that year. In recent years, these meetings have focused on a relevant or emerging theme in non-human genetics and genomics research.\n\nA special issue of the AGA’s \"Journal of Heredity\" is devoted each year to publishing proceedings of the symposium. These special issues are available without a subscription from the AGA and \"Journal of Heredity\" websites.\n\n2016: Local adaptation: from phenotype to genotype to fitness. Lynda Delph\n\n2015: Chromosome evolution: molecular mechanisms & evolutionary consequences. Catherine Peichel\n\n2014: Evolution and plasticity: adaptive responses by species to human-mediated changes to their ecosystems. Robin Waples\n\n2013: Speciation Continuum: A Discussion on the Origin of Species. Kerry Shaw\n\n2012: Recombination: Molecular Mechanisms & Evolutionary Consequences. Mohamed Noor\n\n2011: 2011: Genomics and Biodiversity. Scott V. Edwards\n\n2010: Conservation Genomics. H. Bradley Shaffer\n\n2009: The Genetics and Genomics of Environmental Change. David Rand\n\n2008: Genetics and Genomics of Behavior. Trudy Mackay\n\n2007: Mechanisms of Genome Evolution. Michael Lynch\n\nThe keynote speaker at the annual symposium gives the Key Distinguished Lecture. This lecture series was initially funded by a bequest to the AGA from Dr. Wilhemine Key for support of genetics initiatives for human welfare. Dr. Key earned her PhD from the University of Chicago in 1901, and taught at Lombard College, where Sewall Wright was her student. She later carried out pedigree studies of pioneer families that had emigrated from Germany to Pennsylvania in the late 18th century.\n\n[Bold names: An article based on the lecture is available from the Journal of Heredity online archives. Access is free for AGA members.]\n\n2016 Victoria Sork\n\n2015 Mark Kirkpatrick\n\n2014 David Reznick\n\n2013 Sergey Gavrilets\n\n2012 Brian Charlesworth\n\n2011 Robert K. Wayne\n\n2010 Oliver Ryder\n\n2009 Annie Schmidt\n\n2008 Mariana Wolfner\n\n2007 Sally Otto\n\n2006 Allen Orr\n\n2005 June Nasrallah\n\n2004 Walter Gehring\n\n2003 James F. Crow\n\n2002 Barbara Schaal\n\n2001 Masatoshi Nei\n\n2000 Terry Burke\n\n1999 Walter Fitch\n\n1998 Jeffrey Palmer\n\n1997 John Avise\n\n1996 Hampton Carson\n\n1995 Fotis Kafatos\n\n1994 Michael Clegg\n\n1993 Charles S. Levings III\n\n1992 Theodore R. F. Wright\n\n1991 Margaret G. Kidwell\n\n1990 William Provine\n\n1989 Norman Giles\n\n1988 Allan Wilson\n\n1987 Robert W. Allard\n\n1986 Bruce Wallace\n\n1985 Francisco J. Ayala\n\n1984 Harry Harris\n\n1983 Ray D. Owen\n\n1982 James V. Neel\n\n1981 Clement L. Markert\n\n1980 Edward O. Wilson\n\n1979 Victor A. McKusick\n\n1978 D. K. Balyaev\n\n1977 Margery Shaw\n\n1975 Jack R. Harlan\n\n1974 Walter E. Heston\n\n1972 James F. Crow\n\n1971 Harold H. Smith\n\n1970 Marvin S. Legator\n\n1969 John H. Heller\n\n1968 Arno G. Motulsky\n\n1967 Samuel H. Boyer\n\nThe AGA provides this annual award for graduate and post-doctoral researchers who are at a critical point in their research, where additional funds would allow them to conclude their research project and prepare it for publication.\n\nThese awards are open to any graduate student or postdoctoral fellow who is a member of the AGA at the time of application.\n\nThe program is not intended to fund an entire research project, to initiate new research projects, or to provide salary support. Proposals addressing genome-scale questions, or ecological, evolutionary and conservation genetics questions best addressed with genome-scale data, will be given priority for funding. Awards will generally range from $5,000 to $10,000, awarded to the PI or institution (no overhead is provided).\n\nThe AGA grants awards each year for support of special events that advance its mission, particularly to enable students to attend the event. Eligible events include specialized workshops and short courses, but any event relevant to promoting organismal genetic and genomic research will be considered. Events that could lead to \"Journal of Heredity\" articles will receive special consideration. Awards are usually between $5,000 and $15,000.\n\nThe Stephen J. O'Brien Award for the best student-authored article published in AGA’s \"Journal of Heredity\" honors Dr. O'Brien's many years of service as Editor-in-Chief of the journal. Awards are presented at the President’s Symposium in the year following publication. Award recipients receive a $2,000 prize and up to $1,500 toward expenses to attend the President’s Symposium.\n\nThe AGA has a governance council of nine elected members serving three-year staggered terms; an elected president, past-president and president-elect; elected secretary, and an appointed executive vice-president and treasurer. The Editor-in Chief of the journal is a non-voting member of the council. The council employs a manager and maintains an office in Newport, Oregon. The AGA is a 501(c)(3)c non-profit corporation incorporated in the state of Oregon (EIN 53-0204656).\n\nCurrent Officers and Council Members are available on the AGA’s website.\n\nGenetics, Genomics, Conservation genetics, Population genetics, Evolutionary biology\n\n", "id": "17064239", "title": "American Genetic Association"}
{"url": "https://en.wikipedia.org/wiki?curid=13457", "text": "Heredity\n\nHeredity is the passing on of traits from parents to their offspring, either through asexual reproduction or sexual reproduction; the offspring cells or organisms acquire the genetic information of their parents. Through heredity, variations between individuals can accumulate and cause species to evolve by natural selection. The study of heredity in biology is called genetics, which includes the field of epigenetics.\n\nIn humans, eye color is an example of an inherited characteristic: an individual might inherit the \"brown-eye trait\" from one of the parents. Inherited traits are controlled by genes and the complete set of genes within an organism's genome is called its genotype.\n\nThe complete set of observable traits of the structure and behavior of an organism is called its phenotype. These traits arise from the interaction of its genotype with the environment. As a result, many aspects of an organism's phenotype are not inherited. For example, suntanned skin comes from the interaction between a person's phenotype and sunlight; thus, suntans are not passed on to people's children. However, some people tan more easily than others, due to differences in their genotype: a striking example is people with the inherited trait of albinism, who do not tan at all and are very sensitive to sunburn.\n\nHeritable traits are known to be passed from one generation to the next via DNA, a molecule that encodes genetic information. DNA is a long polymer that incorporates four types of bases, which are interchangeable. The sequence of bases along a particular DNA molecule specifies the genetic information: this is comparable to a sequence of letters spelling out a passage of text. Before a cell divides through mitosis, the DNA is copied, so that each of the resulting two cells will inherit the DNA sequence. A portion of a DNA molecule that specifies a single functional unit is called a gene; different genes have different sequences of bases. Within cells, the long strands of DNA form condensed structures called chromosomes. Organisms inherit genetic material from their parents in the form of homologous chromosomes, containing a unique combination of DNA sequences that code for genes. The specific location of a DNA sequence within a chromosome is known as a locus. If the DNA sequence at a particular locus varies between individuals, the different forms of this sequence are called alleles. DNA sequences can change through mutations, producing new alleles. If a mutation occurs within a gene, the new allele may affect the trait that the gene controls, altering the phenotype of the organism.\n\nHowever, while this simple correspondence between an allele and a trait works in some cases, most traits are more complex and are controlled by multiple interacting genes within and among organisms. Developmental biologists suggest that complex interactions in genetic networks and communication among cells can lead to heritable variations that may underlie some of the mechanics in developmental plasticity and canalization.\n\nRecent findings have confirmed important examples of heritable changes that cannot be explained by direct agency of the DNA molecule. These phenomena are classed as epigenetic inheritance systems that are causally or independently evolving over genes. Research into modes and mechanisms of epigenetic inheritance is still in its scientific infancy, however, this area of research has attracted much recent activity as it broadens the scope of heritability and evolutionary biology in general. DNA methylation marking chromatin, self-sustaining metabolic loops, gene silencing by RNA interference, and the three dimensional conformation of proteins (such as prions) are areas where epigenetic inheritance systems have been discovered at the organismic level. Heritability may also occur at even larger scales. For example, ecological inheritance through the process of niche construction is defined by the regular and repeated activities of organisms in their environment. This generates a legacy of effect that modifies and feeds back into the selection regime of subsequent generations. Descendants inherit genes plus environmental characteristics generated by the ecological actions of ancestors. Other examples of heritability in evolution that are not under the direct control of genes include the inheritance of cultural traits, group heritability, and symbiogenesis. These examples of heritability that operate above the gene are covered broadly under the title of multilevel or hierarchical selection, which has been a subject of intense debate in the history of evolutionary science.\n\nWhen Charles Darwin proposed his theory of evolution in 1859, one of its major problems was the lack of an underlying mechanism for heredity. Darwin believed in a mix of blending inheritance and the inheritance of acquired traits (pangenesis). Blending inheritance would lead to uniformity across populations in only a few generations and then would remove variation from a population on which natural selection could act. This led to Darwin adopting some Lamarckian ideas in later editions of \"On the Origin of Species\" and his later biological works. Darwin's primary approach to heredity was to outline how it appeared to work (noticing that traits that were not expressed explicitly in the parent at the time of reproduction could be inherited, that certain traits could be sex-linked, etc.) rather than suggesting mechanisms.\n\nDarwin's initial model of heredity was adopted by, and then heavily modified by, his cousin Francis Galton, who laid the framework for the biometric school of heredity. Galton found no evidence to support the aspects of Darwin's pangenesis model, which relied on acquired traits.\n\nThe inheritance of acquired traits was shown to have little basis in the 1880s when August Weismann cut the tails off many generations of mice and found that their offspring continued to develop tails.\n\nScientists in Antiquity had a variety of ideas about heredity: Theophrastus proposed that male flowers caused female flowers to ripen; Hippocrates speculated that \"seeds\" were produced by various body parts and transmitted to offspring at the time of conception; and Aristotle thought that male and female fluids mixed at conception. Aeschylus, in 458 BC, proposed the male as the parent, with the female as a \"nurse for the young life sown within her\".\n\nAncient understandings of heredity transitioned to two debated doctrines in the 18th century. The Doctrine of Epigenesis and the Doctrine of Preformation were two distinct views of the understanding of heredity. The Doctrine of Epigenesis, originated by Aristotle, claimed that an embryo continually develops. The modifications of the parent’s traits are passed off to an embryo during its lifetime. The foundation of this doctrine was based on the theory of inheritance of acquired traits. In direct opposition, the Doctrine of Preformation claimed that “like generates like” where the germ would evolve to yield offspring similar to the parents. The Preformationist view believed procreation was an act of revealing what had been created long before. However, this was disputed by the creation of the cell theory in the 19th century, where the fundamental unit of life is the cell, and not some preformed parts of an organism. Various hereditary mechanisms, including blending inheritance were also envisaged without being properly tested or quantified, and were later disputed. Nevertheless, people were able to develop domestic breeds of animals as well as crops through artificial selection. The inheritance of acquired traits also formed a part of early Lamarckian ideas on evolution.\n\nDuring the 18th century, Dutch microscopist Antonie van Leeuwenhoek (1632–1723) discovered \"animalcules\" in the sperm of humans and other animals. Some scientists speculated they saw a \"little man\" (homunculus) inside each sperm. These scientists formed a school of thought known as the \"spermists\". They contended the only contributions of the female to the next generation were the womb in which the homunculus grew, and prenatal influences of the womb. An opposing school of thought, the ovists, believed that the future human was in the egg, and that sperm merely stimulated the growth of the egg. Ovists thought women carried eggs containing boy and girl children, and that the gender of the offspring was determined well before conception.\n\nThe idea of particulate inheritance of genes can be attributed to the Moravian monk Gregor Mendel who published his work on pea plants in 1865. However, his work was not widely known and was rediscovered in 1901. It was initially assumed that Mendelian inheritance only accounted for large (qualitative) differences, such as those seen by Mendel in his pea plants—and the idea of additive effect of (quantitative) genes was not realised until R. A. Fisher's (1918) paper, \"The Correlation Between Relatives on the Supposition of Mendelian Inheritance\" Mendel's overall contribution gave scientists a useful overview that traits were inheritable. His pea plant demonstration became the foundation of the study of Mendelian Traits. These traits can be traced on a single locus.\n\nIn the 1930s, work by Fisher and others resulted in a combination of Mendelian and biometric schools into the modern evolutionary synthesis. The modern synthesis bridged the gap between experimental geneticists and naturalists; and between both and palaeontologists, stating that:\n\n\nThe idea that speciation occurs after populations are reproductively isolated has been much debated. In plants, polyploidy must be included in any view of speciation. Formulations such as 'evolution consists primarily of changes in the frequencies of alleles between one generation and another' were proposed rather later. The traditional view is that developmental biology ('evo-devo') played little part in the synthesis, but an account of Gavin de Beer's work by Stephen Jay Gould suggests he may be an exception.\n\nAlmost all aspects of the synthesis have been challenged at times, with varying degrees of success. There is no doubt, however, that the synthesis was a great landmark in evolutionary biology. It cleared up many confusions, and was directly responsible for stimulating a great deal of research in the post-World War II era.\n\nTrofim Lysenko however caused a backlash of what is now called Lysenkoism in the Soviet Union when he emphasised Lamarckian ideas on the inheritance of acquired traits. This movement affected agricultural research and led to food shortages in the 1960s and seriously affected the USSR.\n\nThere is growing evidence that there is transgenerational inheritance of epigenetic changes in humans and other animals.\n\nAn allele is said to be dominant if it is always expressed in the appearance of an organism (phenotype) provided that at least one copy of it is present. For example, in peas the allele for green pods, \"G\", is dominant to that for yellow pods, \"g\". Thus pea plants with the pair of alleles either \"GG\" (homozygote) or \"Gg\" (heterozygote) will have green pods. The allele for yellow pods is recessive. The effects of this allele are only seen when it is present in both chromosomes, \"gg\" (homozygote).\n\nThe description of a mode of biological inheritance consists of three main categories:\n\nThese three categories are part of every exact description of a mode of inheritance in the above order. In addition, more specifications may be added as follows:\n\nDetermination and description of a mode of inheritance is also achieved primarily through statistical analysis of pedigree data. In case the involved loci are known, methods of molecular genetics can also be employed.\n\n", "id": "13457", "title": "Heredity"}
{"url": "https://en.wikipedia.org/wiki?curid=53653356", "text": "Elective genetic and genomic testing\n\nElective genetic and genomic testing are DNA tests performed for an individual who does not have an indication for testing. An elective genetic test analyzes selected sites in the human genome while an elective genomic test analyzes the entire human genome. Some elective genetic and genomic tests require a physician to order the test to ensure that individuals understand the risks and benefits of testing as well as the results. Other DNA-based tests, such as a genealogical DNA test do not require a physician’s order. Elective testing is generally not paid for by health insurance companies. With the advent of personalized medicine, also called precision medicine, an increasing number of individuals are undertaking elective genetic and genomic testing.\n\nGenetic testing for a variety of disorders has seen many advances starting with cytogenetics to evaluate human chromosomes for aneuploidy and other chromosome abnormalities. The development of molecular cytogenetics involving techniques such as fluorescence in situ hybridization (FISH) followed, permitting the detection of more subtle changes in the karyotype. Techniques to determine the precise sequence of nucleotides in DNA by DNA sequencing, notably Sanger sequencing was developed in the 1970s. In the 1980s the DNA microarray appeared, permitting laboratories to find copy number variants associated with disease that are below the level of detection of cytogenetics but too large to be detected by DNA sequencing. In recent years the development of high-throughput or next-generation sequencing has dramatically lowered the cost of DNA sequencing permitting laboratories to evaluate all 20,000 genes of the human genome at once through exome sequencing and whole genome sequencing. A catalogue of the many uses of these techniques can be found in the section: genetic testing. Most elective genetic and genomic testing employs either a DNA microarray or next-generation sequencing.\n\nHistorically, all laboratory tests were initiated and ordered by a physician or mandated by a state. Increasingly, patients and families have become more involved in their own health care. One outcome has been the growing availability of elective genetic and genomic testing that are initiated by a patient but still ordered by a physician. Additionally, elective genetic and genomic testing that does not require a physician's order called, direct-to-consumer genetic testing has recently entered the testing landscape.\n\nGenetic testing identifies changes in chromosomes, genes, or proteins; some are associated with human disease. There are many different clinical and non-clinical situations in which genetic testing is used.\n\nDiagnostic testing is used to identify or rule out a specific genetic or chromosomal condition when a particular disorder is suspected based on signs and symptoms present in the patient. Catalogues of more than 50,000 tests available worldwide can be found at GeneTests and Genetic Testing Registry.\n\nPredictive and pre-symptomatic testing is carried out in individuals who do not have evidence of the disease under investigation. This testing includes Mendelian conditions and polygenic diseases.\n\nCarrier testing is used to identify people who carry one copy of a gene change (also referred to as a variant or mutation) that, when present in two copies, causes a genetic disorder. Carrier testing is typically offered to individuals who are considering pregnancy or are already pregnant, have a family history of a specific genetic disorder and to people in ethnic backgrounds that have an increased risk of specific genetic conditions.\n\nPre-implantation genetic diagnosis (PGD) is used in conjunction with in-vitro fertilization. In-vitro fertilization is the process of combining an egg (oocyte) and sperm outside of the body with intent of fertilization. PGD is the testing of individual oocytes or embryos for a known genetic condition prior to transferring the embryo to the uterus. Used together, IVF and PGD allow for selection of embryos or oocytes presumably unaffected with the condition. PGD can be utilized by individuals or couples who are affected by a condition of genetic origin, or if both individuals are found to be carriers of a recessive genetic condition.\n\nPrenatal testing is diagnostic testing of a fetus before birth to detect abnormalities in the chromosomes or genes. Samples for this testing are obtained through invasive procedures such as amniocentesis or chorionic villus sampling. Prenatal testing is different from prenatal screening.\n\nNewborn screening screens infants a few days after birth to evaluate for evidence of treatable diseases. Most newborn screening uses tandem mass spectroscopy to detect biochemical abnormalities that suggest specific disorders. DNA-based newborn testing complements existing newborn screening methods and may replace it.\n\nPharmacogenomic tests (also called pharmacogenetics) provide information that can help predict how an individual will respond to a medication. Changes in certain genes affect drug pharmacodynamics (effects on drug receptors) and pharmacokinetics (drug uptake, distribution, and metabolism). Identifying these changes makes it possible to identify patients who are at increased risk for adverse effects from drugs or who are likely to be non-responders. Pharmacogenomic testing allows healthcare providers to tailor therapies by adjusting the dose or drug for an individual patient.\n\nIdentity testing is used to establish whether individuals are related to one another. It is commonly used to establish paternity but can be used to establish relatedness in adoption and immigration cases. It is also used in forensics.\n\nAncestry testing (also referred to as genetic genealogy) allows individuals to establish their country of origin and ethnic background and identify distant relatives and ancestors.\n\nSome phenotypic traits in humans have a well established genetic basis, while others involve many genes or are a complex mix of genes and environment.\n\nThere are many different types of genetic testing that exist. Each is designed to look at different types of genetic changes that can occur. At present, no single genetic test can detect all types of genetic changes. \nDNA Sequencing is a general term used to describe a method of testing that looks for single letter changes (single nucleotide variants) in the genetic code. It can also determine when a small number of letters are missing (deletions) or extra (duplications). Sequencing may be performed on a single gene, a group of genes (panel testing), most of the coding region or exons (whole exome sequencing), or most of the genome (whole genome sequencing). With time, this technology is expected to be able to detect any abnormality of the human genome.\n\nGenotyping is testing that looks at specific variants in a particular area of the genetic code. This technology is limited only to those specific variants that the test is designed to detect. SNP genotyping is a specific form of genotyping.\n\nDeletion/duplication testing is a type of testing designed to detect larger areas of the genetic code that are missing or extra. This technology does not detect single letter variants or very small deletions or duplications.\n\nPanel testing refers to testing for a specific subset of genes most often related to a particular condition. This usually involves sequencing and may also include deletion/duplication analysis. This is often referred to as multigene panel testing because testing simultaneously examines a number of different genes. For example, an individual may have panel testing for a group of genes known to be associated with a particular type of cancer such hereditary colon cancer or hereditary breast and ovarian cancer.\n\nArray or DNA microarrays look at copy number changes (missing or extra genetic material). This testing looks across a large portion of the genome for larger deletions or duplications (also referred to as copy number variation). This technology can not detect single letter changes or very small deletions or duplications.\n\nChromosome analysis, also known as karyotyping refers to testing that assesses whether the expected number of chromosomes are present, whether there is any rearrangement of the chromosomes, and also whether there are any large deletions or duplications. This technology can not detect single letter changes (single nucleotide variants) or small deletions or duplications.\n\nNon-invasive prenatal screening screens for specific chromosomal abnormalities such as Down Syndrome in a fetus using cell-free DNA. This screening can also provide information about fetal sex and rhesus (Rh) blood type. A blood sample is drawn from the pregnant mother. This sample contains DNA from the mother and fetus. The amount of fetal DNA is assessed to determine if there is extra fetal genetic material present that may indicate an increased risk that the fetus has Down Syndrome or other selected conditions. As this is a screening test, other diagnostic tests such as amniocentesis or chorionic villus sampling are needed to confirm a diagnosis.\n\nNewborn screening is a type of testing that assesses risk for certain genetic, endocrine, metabolic disorders, hearing loss and critical congenital heart defects. Each state determines the exact list of conditions that are screened. Early detection, diagnosis, and intervention can prevent death or disability and enable children to reach their full potential. The testing is performed from a few drops of blood collected in the newborn period, often by a heel stick. The exact method of testing may vary but often uses levels of specific analytes present in the blood of the baby. Because this is a screening test, additional testing is often necessary to confirm a diagnosis.\n\nPeople choose to have genetic testing for many reasons. Testing may be beneficial whether the test identifies a gene change or not. A negative result can eliminate the need for unnecessary checkups and screening tests in some cases. A positive result can direct a person toward available screening, management or treatment options.\n\n\nA patient's family history also known as genealogy, can provide important insight into medical conditions within the family. Given that many conditions have a genetic component, gathering an accurate family history can provide important information about an individuals' personal risk for many diseases. Healthcare providers can use family history information to assess a patient's risk for disease, recommend testing or screening, suggest diet or other lifestyle habits that may help reduce risk, as well as assess risk of passing conditions on to children. When obtaining a family history, it is helpful to gather health information for the following family members: grandparents, parents, siblings, aunts, uncles and first cousins, and children. In the genetic counseling community this is often referred to as a three generation family history. \n\nImportant information to gather about the individuals in the family include:\n\nSome families decide to work together to develop a family history, however, some family members may feel uncomfortable disclosing personal medical information. A number of tools are available to gather family history information. Patients should ask their healthcare provider if their institution has a specific form they prefer to have filled out. The U.S. Surgeon General has created a computerized tool called My Family Health Portrait to help patients create a family medical history.\n\nPrior to undergoing elective genetic testing, there are many factors that an individual should consider including the scope of testing and potential results in terms of changes to medical management, risk to family members, and impact on legal and financial matters.\n\n\nMany patients are concerned about the possibility of genetic discrimination, the idea that certain individuals or entities would use a patient’s genetic information against him or her in order to make employment, insurance policies, or other activities and services difficult or impossible to obtain. In 2008, a new federal law known as the Genetic Information Nondiscrimination Act (GINA) went into effect to help prevent such discrimination. GINA prohibits the use of genetic information to discriminate in health insurance and employment. GINA does not prevent all types of discrimination, however. For companies with fewer than 15 employees, these employment protections do not apply. GINA’s protections do not apply to the US military or to federal government employees. Additionally, life, disability, and long-term care insurance policies are not included among GINA’s protections. These may still continue to use genetic information to determine one’s eligibility for coverage and/or policy premiums. Because of these important exceptions, an individual considering elective genetic testing should discuss the possibility of genetic discrimination with his or her physician or genetic counselor. Some individuals choose to have certain insurance policies in place before undergoing whole genome sequencing so as to prevent future discrimination.\n\nWhen undergoing elective genetic testing, patients may expect to receive a variety of different results. In addition to results that may explain a particular symptom or answer a specific question the patient may have had, the scope of elective testing may reveal additional information. These “secondary findings” may include information about increased risk for both treatable and untreatable genetic diseases, carrier status for recessive conditions, and pharmacogenetic information. Most laboratories permit patients and families to decide what types of secondary findings (if any), they would like to receive. It is critical that patients understand the scope of potential results from elective testing and have the opportunity to opt in or out of various results.\n\nWhen considering elective genetic testing, it is important to take into account the type and goals of testing. Providers and patients should be familiar with differing testing methodologies the potential results from each test. For many individuals, factors such as test cost, scope, and deliverables, in combination with their specific clinical questions, play into the decision to undergo elective testing.\nIt is also important to recognize that potential results from elective genetic testing are constrained by the current limits of medical knowledge concerning the association between genetics and human disease. As knowledge of rare genetic factors that confer high risk, as well as common factors that confer lower risks, increases, we will have the ability to learn more about an individual's current and future health. \n\nDue to their advanced training, genetic counselors have a unique set of skills. Their clinical and psychosocial skills are used to help patients understand their genetic risks, determine which tests are most appropriate for their needs, and explain what the possible test results could mean for both the patient and the family. Clinical geneticists often work in tandem with a genetic counselor and play an important role in providing genetic testing, interpreting test results, and explaining the results. Given the ever-increasing number of elective genetic and genomic tests offered and the wide variety of issues raised by these tests (see pros & cons above), discussion with a clinical geneticist or genetic counselor may be helpful. Directories of genetics professionals can be found through the American College of Medical Genetics and Genomics and the National Society of Genetic Counselors.\n\nElective genetic and genomic testing will continue to evolve as the cost of genetic testing technology falls and patients become increasingly involved in their own health care. The rapid drop in cost of whole exome sequencing and whole genome sequencing in the last five years has resulted in the initiation of several large scale sequencing studies that are systematically evaluating the benefits and limitations of elective genetic and genomic testing. Many of these studies have specifically focused on healthy individuals pursuing elective WES or WGS. \n\nOther driving forces in the adoption of this type of testing include continued social empowerment of patients regarding their own health care and increasing private and government funded sequencing projects focused on better understanding the biological, environment, and behavioral factors that drive common disease with the hope of developing more effective ways to treat and manage disease. The Million Veteran Program is one example of a government funded project aimed at collecting data from veterans using questionnaires, health record information, and blood samples for testing, including genetic testing. Aimed at recruiting 1 million or more Americans to participate in the research cohort, The Precision Medicine Initiative will have a large impact on public awareness of precision medicine and the importance of using genetic information to treat and manage disease as well as optimize health.\nWhile elective testing is typically not paid for by health insurance companies, this may change as clinical utility continues to be demonstrated. \n\nFuture applications for elective genetic and genomic testing may include:\n\n\n\n", "id": "53653356", "title": "Elective genetic and genomic testing"}
{"url": "https://en.wikipedia.org/wiki?curid=1634427", "text": "Zymography\n\nZymography is an electrophoretic technique for the detection of hydrolytic enzymes, based on the substrate repertoire of the enzyme. Three types of zymography are used; \"in gel\" zymography, \"in situ\" zymography and \"in vivo\" zymography For instance, gelatin embedded in a polyacrylamide gel will be  digested by active gelatinases run through the gel. After Coomassie staining, areas of degradation are visible as clear bands against a darkly stained background.\n\nModern usage of the term zymography has been adapted to define the study and cataloging of fermented products, such as beer or wine, often by specific brewers or winemakers or within an identified category of fermentation such as with a particular strain of yeast or species of bacteria.\nZymography also refers to a collection of related, fermented products, considered as a body of work. For example, all of the beers produced by a particular brewery could collectively be referred to as its zymography. \n\nSee also Zymology or the applied science of zymography. Zymology relates to the biochemical processes of fermentation, especially the selection of fermenting yeast and bacteria in brewing, winemaking, and other fermented foods. For example, beer-making involves the application of top (ale) or bottom fermenting yeast(lager), to produce the desired variety of beer. The synthesis of the yeast can impact the flavor profile of the beer, i.e. diacetyl (taste or aroma of buttery, butterscotch). \n\nSamples are prepared in a standard, non-reducing loading buffer for SDS-PAGE. No reducing agent or boiling are necessary since these would interfere with refolding of the enzyme. A suitable substrate (e.g. gelatin or casein for protease detection) is embedded in the resolving gel during preparation of the acrylamide gel. Following electrophoresis, the SDS is removed from the gel (or zymogram) by incubation in unbuffered Triton X-100, followed by incubation in an appropriate digestion buffer, for an optimized length of time at 37 °C. The zymogram is subsequently stained (commonly with Amido Black or Coomassie Brilliant Blue), and areas of digestion appear as clear bands against a darkly stained background where the substrate has been degraded by the enzyme.\n\nThe standard protocol may require modifications depending on the sample enzyme; for instance, \"D. melanogaster\" digestive glycosidases generally survive reducing conditions (i.e. the presence of 2-mercaptoethanol or DTT), and to an extent, heating. Indeed, the separations following heating to 50 °C tend to exhibit a substantial increase in band resolution, without appreciable loss of activity.\n\nA common protocol used in the past for zymography of α-amylase activity was the so-called starch film protocol of W.W. Doane. Here a native PAGE gel was run to separate the proteins in a homogenate. Subsequently, a thin gel with starch dissolved (or more properly, suspended) in it was overlaid for a period of time on top of the original gel. The starch was then stained with Lugol's iodine.\n\nGel zymography is often used for the detection and analysis of enzymes produced by microorganisms. This has led to variations on the standard protocol e.g. mixed-substrate zymography.\n\nReverse zymography copolymerizes both the substrate and the enzyme with the acrylamide, and is useful for the demonstration of enzyme inhibitor activity. Following staining, areas of inhibition are visualized as dark bands against a clear (or lightly stained) background.\n\nIn imprint technique, the enzyme is separated by native gel electrophoresis and the gel is laid on top of a substrate treated agarose.\n\nZymography can also be applied to other types of enzymes, including xylanases, lipases and chitinases.\n\n", "id": "1634427", "title": "Zymography"}
{"url": "https://en.wikipedia.org/wiki?curid=1686285", "text": "Cycling probe technology\n\nCycling probe technology (CPT) is a molecular biological technique for detecting specific DNA sequences. CPT operates under isothermal conditions. In some applications, CPT offers an alternative to PCR. However, unlike PCR, CPT does not generate multiple copies of the target DNA itself, and the amplification of the signal is linear, in contrast to the geometric amplification of the target DNA in PCR. CPT uses a sequence specific chimeric probe which hybridizes to a complementary target DNA sequence and becomes a substrate for RNase H. Cleavage occurs at the RNA internucleotide linkages and results in dissociation of the probe from the target, thereby making it available for the next probe molecule. Integrated electrokinetic systems have been developed for use in CPT.\n", "id": "1686285", "title": "Cycling probe technology"}
{"url": "https://en.wikipedia.org/wiki?curid=1118963", "text": "Nonribosomal peptide\n\nNonribosomal peptides (NRP) are a class of peptide secondary metabolites, usually produced by microorganisms like bacteria and fungi. Nonribosomal peptides are also found in higher organisms, such as nudibranchs, but are thought to be made by bacteria inside these organisms. While there exist a wide range of peptides that are not synthesized by ribosomes, the term \"nonribosomal peptide\" typically refers to a very specific set of these as discussed in this article.\n\nNonribosomal peptides are synthesized by nonribosomal peptide synthetases, which, unlike the ribosomes, are independent of messenger RNA. Each nonribosomal peptide synthetase can synthesize only one type of peptide. Nonribosomal peptides often have cyclic and/or branched structures, can contain non-proteinogenic amino acids including -amino acids, carry modifications like \"N\"-methyl and \"N\"-formyl groups, or are glycosylated, acylated, halogenated, or hydroxylated. Cyclization of amino acids against the peptide \"backbone\" is often performed, resulting in oxazolines and thiazolines; these can be further oxidized or reduced. On occasion, dehydration is performed on serines, resulting in dehydroalanine. This is just a sampling of the various manipulations and variations that nonribosomal peptides can perform. Nonribosomal peptides are often dimers or trimers of identical sequences chained together or cyclized, or even branched.\n\nNonribosomal peptides are a very diverse family of natural products with an extremely broad range of biological activities and pharmacological properties. They are often toxins, siderophores, or pigments. Nonribosomal peptide antibiotics, cytostatics, and immunosuppressants are in commercial use.\n\n\nNonribosomal peptides are synthesized by one or more specialized nonribosomal peptide-synthetase (NRPS) enzymes. The NRPS genes for a certain peptide are usually organized in one operon in bacteria and in gene clusters in eukaryotes. However the first fungal NRP to be found was ciclosporin. It is synthesized by a single 1.6MDa NRPS. The enzymes are organized in modules that are responsible for the introduction of one additional amino acid. Each module consists of several domains with defined functions, separated by short spacer regions of about 15 amino acids.\n\nThe biosynthesis of nonribosomal peptides shares characteristics with the polyketide and fatty acid biosynthesis. Due to these structural and mechanistic similarities, some nonribosomal peptide synthetases contain polyketide synthase modules for the insertion of acetate or propionate-derived subunits into the peptide chain.\n\nThe order of modules and domains of a complete nonribosomal peptide synthetase is as follows:\n\n\n\n\n\n\nThe final peptide is often modified, e.g., by glycosylation, acylation, halogenation, or hydroxylation. The responsible enzymes are usually associated to the synthetase complex and their genes are organized in the same operons or gene clusters.\n\nTo become functional, the 4'-phospho-pantetheine sidechain of acyl-CoA molecules has to be attached to the PCP-domain by 4'PP transferases (Priming) and the S-attached acyl group has to be removed by specialized associated thioesterases (TE-II) (Deblocking).\n\nMost domains have a very broad substrate specificity and usually only the A-domain determines which amino acid is incorporated in a module. Ten amino acids that control substrate specificity and can be considered the 'codons' of nonribosomal peptide synthesis have been identified. The condensation C-domain is also believed to have substrate specificity, especially if located behind an epimerase E-domain-containing module where it functions as a 'filter' for the epimerized isomer.\n\nDue to the similarity with polyketide synthases (PKS), many secondary metabolites are, in fact, fusions of NRPs and polyketides. In essence, this occurs when PK modules follow NRP modules, and vice versa. Although there is high degree of similarity between the PCP domains of both types of sythetases, the mechanism of condensation is different from a chemical standpoint (claisen vs. transamidation).\n\n\n", "id": "1118963", "title": "Nonribosomal peptide"}
{"url": "https://en.wikipedia.org/wiki?curid=238301", "text": "Gel electrophoresis of nucleic acids\n\nNucleic acid electrophoresis is an analytical technique used to separate DNA or RNA fragments by size and reactivity. Nucleic acid molecules which are to be analyzed are set upon a viscous medium, the gel, where an electric field induces the nucleic acids (which are negatively charged due to their sugar-phosphate backbone) to migrate toward the anode (which is positively charged because this is an electrolytic rather than galvanic cell). The separation of these fragments is accomplished by exploiting the mobilities with which different sized molecules are able to pass through the gel. Longer molecules migrate more slowly because they experience more resistance within the gel. Because the size of the molecule affects its mobility, smaller fragments end up nearer to the anode than longer ones in a given period. After some time, the voltage is removed and the fragmentation gradient is analyzed. For larger separations between similar sized fragments, either the voltage or run time can be increased. Extended runs across a low voltage gel yield the most accurate resolution. Voltage is, however, not the sole factor in determining electrophoresis of nucleic acids.\n\nThe nucleic acid to be separated can be prepared in several ways before separation by electrophoresis. In the case of large DNA molecules, the DNA is frequently cut into smaller fragments using a DNA restriction endonuclease (or restriction enzyme). In other instances, such as PCR amplified samples, enzymes present in the sample that might affect the separation of the molecules are removed through various means before analysis. Once the nucleic acid is properly prepared, the samples of the nucleic acid solution are placed in the wells of the gel and a voltage is applied across the gel for a specified amount of time.\n\nThe DNA fragments of different lengths are visualized using a fluorescent dye specific for DNA, such as ethidium bromide. The gel shows bands corresponding to different nucleic acid molecules populations with different molecular weight. Fragment size is usually reported in \"nucleotides\", \"base pairs\" or \"kb\" (for thousands of base pairs) depending upon whether single- or double-stranded nucleic acid has been separated. Fragment size determination is typically done by comparison to commercially available DNA markers containing linear DNA fragments of known length.\n\nThe types of gel most commonly used for nucleic acid electrophoresis are agarose (for relatively long DNA molecules) and polyacrylamide (for high resolution of short DNA molecules, for example in DNA sequencing). Gels have conventionally been run in a \"slab\" format such as that shown in the figure, but capillary electrophoresis has become important for applications such as high-throughput DNA sequencing. Electrophoresis techniques used in the assessment of DNA damage include alkaline gel electrophoresis and pulsed field gel electrophoresis.\n\nFor short DNA segments such as 20 to 60 bp double stranded DNA, running them in Polyacrylamide gel (PAGE) will give better resolution(native condition). Similarly, RNA and single stranded DNA can be run and visualised by PAGE gels containing denaturing agents such as Urea. PAGE gels are widely used in techniques such as DNA foot printing, EMSA and other DNA-protein interaction techniques.\n\nThe measurement and analysis are mostly done with a specialized gel analysis software. Capillary electrophoresis results are typically displayed in a trace view called an electropherogram.\n\nA number of factors can affect the migration of nucleic acids: the dimension of the gel pores, the voltage used, the ionic strength of the buffer, and the concentration intercalating dye such as ethidium bromide if used during electrophoresis.\n\nThe gel sieves the DNA by the size of the DNA molecule whereby smaller molecules travel faster. Double-stranded DNA moves at a rate that is approximately inversely proportional to the logarithm of the number of base pairs. This relationship however breaks down with very large DNA fragments and it is not possible to separate them using standard agarose gel electrophoresis. The limit of resolution depends on gel composition and field strength. and the mobility of larger circular DNA may be more strongly affected than linear DNA by the pore size of the gel. Separation of very large DNA fragments requires pulse field gel electrophoresis (PFGE). In field inversion gel electrophoresis (FIGE, a kind of PFGE), it is possible to have \"band inversion\" - where large molecules may move faster than small molecules.\n\nThe conformation of the DNA molecule can significantly affect the movement of the DNA, for example, supercoiled DNA usually moves faster than relaxed DNA because it is tightly coiled and hence more compact. In a normal plasmid DNA preparation, multiple forms of DNA may be present, and gel from the electrophoresis of the plasmids would normally show a main band which would be the negatively supercoiled form, while other forms of DNA may appear as minor fainter bands. These minor bands may be nicked DNA (open circular form) and the relaxed closed circular form which normally run slower than supercoiled DNA, and the single-stranded form (which can sometimes appear depending on the preparation methods) may move ahead of the supercoiled DNA. The rate at which the various forms move however can change using different electrophoresis conditions, for example linear DNA may run faster or slower than supercoiled DNA depending on conditions, and the mobility of larger circular DNA may be more strongly affected than linear DNA by the pore size of the gel. Unless supercoiled DNA markers are used, the size of a circular DNA like plasmid therefore may be more accurately gauged after it has been linearized by restriction digest.\n\nDNA damage due to increased cross-linking will also reduce electrophoretic DNA migration in a dose-dependent way.\n\nCircular DNA are more strongly affected by ethidium bromide concentration than linear DNA if ethidium bromide is present in the gel during electrophoresis. All naturally occurring DNA circles are underwound, but ethidium bromide which intercalates into circular DNA can change the charge, length, as well as the superhelicity of the DNA molecule, therefore its presence during electrophoresis can affect its movement in gel. Increasing ethidium bromide intercalated into the DNA can change it from a negatively supercoiled molecule into a fully relaxed form, then to positively coiled superhelix at maximum intercalation. Agarose gel electrophoresis can be used to resolve circular DNA with different supercoiling topology.\n\nThe concentration of the gel determines the pore size of the gel which affect the migration of DNA. The resolution of the DNA changes with the percentage concentration of the gel. Increasing the agarose concentration of a gel reduces the migration speed and improves separation of smaller DNA molecules, while lowering gel concentration permits large DNA molecules to be separated. For a standard agarose gel electrophoresis, a 0.7% gives good separation or resolution of large 5–10kb DNA fragments, while 2% gel gives good resolution for small 0.2–1kb fragments. Up to 3% can be used for separating very tiny fragments but a vertical polyacrylamide gel would be more appropriate for resolving small fragments. High concentrations gel however requires longer run times (sometimes days) and high percentage gels are often brittle and may not set evenly. High percentage agarose gels should be run with PFGE or FIGE. Low percentage gels (0.1−0.2%) are fragile and may break. 1% gels are common for many applications.\n\nAt low voltages, the rate of migration of the DNA is proportional to the voltage applied, i.e. the higher the voltage, the faster the DNA moves. However, in increasing electric field strength, the mobility of high-molecular-weight DNA fragments increases differentially, and the effective range of separation decreases and resolution therefore is lower at high voltage. For optimal resolution of DNA greater than 2kb in size in standard gel electrophoresis, 5 to 8 V/cm is recommended. Voltage is also limited by the fact that it heats the gel and may cause the gel to melt if a gel is run at high voltage for a prolonged period, particularly for low-melting point agarose gel.\n\nThe mobility of DNA however may change in an unsteady field. In a field that is periodically reversed, the mobility of DNA of a particular size may drop significantly at a particular cycling frequency. This phenomenon can result in band inversion whereby larger DNA fragments move faster than smaller ones in PFGE.\n\nThe negative charge of its phosphate backbone moves the DNA towards the positively charged anode during electrophoresis. However, the migration of DNA molecules in solution, in the absence of a gel matrix, is independent of molecular weight during electrophoresis, i.e. there is no separation by size without a gel matrix. Hydrodynamic interaction between different parts of the DNA are cut off by streaming counterions moving in the opposite direction, so no mechanism exists to generate a dependence of velocity on length on a scale larger than screening length of about 10 nm. This makes it different from other processes such as sedimentation or diffusion where long-ranged hydrodynamic interaction are important.\n\nThe gel matrix is therefore responsible for the separation of DNA by size during electrophoresis, however the precise mechanism responsible the separation is not entirely clear. A number of models exists for the mechanism of separation of biomolecules in gel matrix, a widely accepted one is the Ogston model which treats the polymer matrix as a sieve consisting of randomly distributed network of inter-connected pores. A globular protein or a random coil DNA moves through the connected pores large enough to accommodate its passage, and the movement of larger molecules is more likely to be impeded and slowed down by collisions with the gel matrix, and the molecules of different sizes can therefore be separated in this process of sieving.\n\nThe Ogston model however breaks down for large molecules whereby the pores are significantly smaller than size of the molecule. For DNA molecules of size greater than 1 kb, a reptation model (or its variants) is most commonly used. This model assumes that the DNA can crawl in a \"snake-like\" fashion (hence \"reptation\") through the pores as an elongated molecule. At higher electric field strength, this turned into a biased reptation model, whereby the leading end of the molecule become strongly biased in the forward direction, and this leading edge pulls the rest of the molecule along. In the fully biased mode, the mobility reached a saturation point and DNA beyond a certain size cannot be separated. Perfect parallel alignment of the chain with the field however is not observed in practice as that would mean the same mobility for long and short molecules. Further refinement of the biased reptation model takes into account of the internal fluctuations of the chain.\n\nThe biased reptation model has also been used to explain the mobility of DNA in PFGE. The orientation of the DNA is progressively built up by reptation after the onset of a field, and the time it reached the steady state velocity is dependent on the size of the molecule. When the field is changed, larger molecules take longer to reorientate, it is therefore possible to discriminate between the long chains that cannot reach its steady state velocity from the short ones that travel most of the time in steady velocity. Other models, however, also exist.\n\nReal-time fluorescence microscopy of stained molecules showed more subtle dynamics during electrophoresis, with the DNA showing considerable elasticity as it alternately stretching in the direction of the applied field and then contracting into a ball, or becoming hooked into a U-shape when it gets caught on the polymer fibres. This observation may be termed the \"caterpillar\" model. Other model proposes that the DNA gets entangled with the polymer matrix, and the larger the molecule, the more likely it is to become entangled and its movement impeded.\n\nThe most common dye used to make DNA or RNA bands visible for agarose gel electrophoresis is ethidium bromide, usually abbreviated as EtBr. It fluoresces under UV light when intercalated into the major groove of DNA (or RNA). By running DNA through an EtBr-treated gel and visualizing it with UV light, any band containing more than ~20 ng DNA becomes distinctly visible. EtBr is a known mutagen, and safer alternatives are available, such as GelRed, produced by Biotium, which binds to the minor groove.\n\nSYBR Green I is another dsDNA stain, produced by Invitrogen. It is more expensive, but 25 times more sensitive, and possibly safer than EtBr, though there is no data addressing its mutagenicity or toxicity in humans.\n\nSYBR Safe is a variant of SYBR Green that has been shown to have low enough levels of mutagenicity and toxicity to be deemed nonhazardous waste under U.S. Federal regulations. It has similar sensitivity levels to EtBr, but, like SYBR Green, is significantly more expensive. In countries where safe disposal of hazardous waste is mandatory, the costs of EtBr disposal can easily outstrip the initial price difference, however.\n\nSince EtBr stained DNA is not visible in natural light, scientists mix DNA with negatively charged loading buffers before adding the mixture to the gel. Loading buffers are useful because they are visible in natural light (as opposed to UV light for EtBr stained DNA), and they co-sediment with DNA (meaning they move at the same speed as DNA of a certain length). Xylene cyanol and Bromophenol blue are common dyes found in loading buffers; they run about the same speed as DNA fragments that are 5000 bp and 300 bp in length respectively, but the precise position varies with percentage of the gel. Other less frequently used progress markers are Cresol Red and Orange G which run at about 125 bp and 50 bp, respectively.\n\nVisualization can also be achieved by transferring DNA after SDS-PAGE to a nitrocellulose membrane followed by exposure to a hybridization probe. This process is termed Southern blotting.\n\nFor fluorescent dyes, after electrophoresis the gel is illuminated with an ultraviolet lamp (usually by placing it on a light box, while using protective gear to limit exposure to ultraviolet radiation). The illuminator apparatus mostly also contains imaging apparatus that takes an image of the gel, after illumination with UV radiation. The ethidium bromide fluoresces reddish-orange in the presence of DNA, since it has intercalated with the DNA. The DNA band can also be cut out of the gel, and can then be dissolved to retrieve the purified DNA.\nThe gel can then be photographed usually with a digital or polaroid camera. Although the stained nucleic acid fluoresces reddish-orange, images are usually shown in black and white (see figures). UV damage to the DNA sample can reduce the efficiency of subsequent manipulation of the sample, such as ligation and cloning. Shorter wavelength UV radiations (302 or 312 nm) cause greater damage, for example exposure for as little as 45 seconds can significantly reduce transformation efficiency. Therefore if the DNA is to be use for downstream procedures, exposure to a shorter wavelength UV radiations should be limited, instead higher-wavelength UV radiation (365 nm) which cause less damage should be used. Higher wavelength radiations however produces weaker fluorescence, therefore if it is necessary to capture the gel image, a shorter wavelength UV light can be used a short time. Addition of Cytidine or guanosine to the electrophoresis buffer at 1 mM concentration may protect the DNA from damage. Alternatively, a blue light excitation source with a blue-excitable stain such as SYBR Green or GelGreen may be used.\n\nGel electrophoresis research often takes advantage of software-based image analysis tools, such as ImageJ.\n", "id": "238301", "title": "Gel electrophoresis of nucleic acids"}
{"url": "https://en.wikipedia.org/wiki?curid=779061", "text": "Phagemid\n\nA phagemid or phasmid is a plasmid that contains an f1 origin of replication from an f1 phage. It can be used as a type of cloning vector in combination with filamentous phage M13. A phagemid can be replicated as a plasmid, and also be packaged as single stranded DNA in viral particles. Phagemids contain an origin of replication (ori) for double stranded replication, as well as an f1 ori to enable single stranded replication and packaging into phage particles. Many commonly used plasmids contain an f1 ori and are thus phagemids. Similarly to a plasmid, a phagemid can be used to clone DNA fragments and be introduced into a bacterial host by a range of techniques, such as transformation and electroporation. However, infection of a bacterial host containing a phagemid with a 'helper' phage, for example VCSM13 or M13K07, provides the necessary viral components to enable single stranded DNA replication and packaging of the phagemid DNA into phage particles. The 'helper' phage infects the bacterial host by first attaching to the host cell's pilus and then, after attachment, transporting the phage genome into the cytoplasm of the host cell. Inside the cell, the phage genome triggers production of single stranded phagemid DNA in the cytoplasm. This phagemid DNA is then packaged into phage particles. The phage particles containing ssDNA are released from the bacterial host cell into the extracellular environment. Filamentous phages retard bacterial growth but, contrasting with the lambda phage and the T7 phage, are not generally lytic. Helper phages are usually engineered to package less efficiently (via a defective phage origin of replication) than the phagemid so that the resultant phage particles contain predominantly phagemid DNA. F1 Filamentous phage infection requires the presence of a pilus so only bacterial hosts containing the F-plasmid or its derivatives can be used to generate phage particles. Prior to the development of cycle sequencing, phagemids were used to generate single stranded DNA template for sequencing purposes. Today phagemids are still useful for generating templates for site-directed mutagenesis. Detailed characterisation of the filamentous phage life cycle and structural features lead to the development of phage display technology, in which a range of peptides and proteins can be expressed as fusions to phage coat proteins and displayed on the viral surface. The displayed peptides and polypeptides are associated with the corresponding coding DNA within the phage particle and so this technique lends itself to the study of protein-protein interactions and other ligand/receptor combinations.\n", "id": "779061", "title": "Phagemid"}
{"url": "https://en.wikipedia.org/wiki?curid=1567889", "text": "Triparental mating\n\nTriparental mating is a form of Bacterial conjugation where a conjugative plasmid present in one bacterial strain assists the transfer of a mobilizable plasmid present in a second bacterial strain into a third bacterial strain. Plasmids are introduced into bacteria for such purposes as transformation, cloning, or transposon mutagenesis. Triparental matings can help overcome some of the barriers to efficient plasmid mobilization. For instance, if the conjugative plasmid and the mobilizable plasmid are members of the same incompatibility group they do not need to stably coexist in the second bacterial strain for the mobilizable plasmid to be transferred.\n\n\nFive to seven days are required to determine if the \nplasmid was successfully introduced into the new bacterial \nstrain and confirm that there is no carryover of the helper or \ndonor strain. \n\nIn contrast, electroporation does not require a helper or \ndonor strain. This helps \navoid possible contamination with other strains. The introduction\nof the plasmid can be verified in the recipient \nstrain in two days, making electroporation a faster and \nmore efficient method of transformation. Electroporation however does not work with all bacteria and is mostly limited to well-characterized model organisms.\n\n\n", "id": "1567889", "title": "Triparental mating"}
{"url": "https://en.wikipedia.org/wiki?curid=770314", "text": "Blot (biology)\n\nA blot, in molecular biology and genetics, is a method of transferring proteins, DNA or RNA, onto a carrier (for example, a nitrocellulose, polyvinylidene fluoride (PVDF) or nylon membrane). In many instances, this is done after a gel electrophoresis, transferring the molecules from the gel onto the blotting membrane, and other times adding the samples directly onto the membrane. After the blotting, the transferred proteins, DNA or RNA are then visualized by colorant staining (for example, silver staining of proteins), autoradiographic visualization of radioactive labelled molecules (performed before the blot), or specific labelling of some proteins or nucleic acids. The latter is done with antibodies or hybridization probes that bind only to some molecules of the blot and have an enzyme joined to them. After proper washing, this enzymatic activity (and so, the molecules we search in the blot) is visualized by incubation with proper reactive, rendering either a colored deposit on the blot or a chemiluminiscent reaction which is registered by photographic film.\n\nA Southern blot is a method routinely used in molecular biology for detection of a specific DNA sequence in DNA samples. Southern blotting combines transfer of electrophoresis-separated DNA fragments to a filter membrane and subsequent fragment detection by probe hybridization.\n\nA Western blot is used for the detection of specific proteins in complex samples. Proteins are first separated by size using electrophoresis before being transferred to an appropriate blotting matrix (usually PVDF or nitrocellulose) and subsequent detection with antibodies.\n\nSimilar to a Western blot, the Far-Western blot uses protein–protein interactions to detect the presence of a specific protein immobilized on a blotting matrix. Antibodies are then used to detect the presence of the protein–protein complex, making the Far-Western blot a specific case of the Western blot. .\n\nA Southwestern blot is based on Southern blotting and is used to identify and characterize DNA-binding proteins by their ability to bind to specific oligonucleotide probes. The proteins are separated by gel electrophoresis and are subsequently transferred to nitrocellulose membranes similar to other types of blotting.\n\nThe Eastern blot is used for the detection of specific posttranslational modifications of proteins. Proteins are separated by gel electrophoresis before being transferred to a blotting matrix whereupon posttranslational modifications are detected by specific substrates (cholera toxin, concanavalin, phosphomolybdate, etc.) or antibodies.\n\nThe Far-Eastern blot is for the detection of lipid-linked oligosaccharides. High performance thin layer chromatography is first used to separate the lipids by physical and chemical characteristics, then transferred to a blotting matrix before the oligosaccharides are detected by a specific binding protein (i.e. antibodies or lectins).\n\nThe Northern blot is for the detection of specific RNA sequences in complex samples. Northern blotting first separates samples by size via gel electrophoresis before they are transferred to a blotting matrix and detected with labeled RNA probes.\n\nThe Reverse Northern blot differs from both Northern and Southern blotting in that DNA is first immobilized on a blotting matrix and specific sequences are detected with labeled RNA probes.\n\nA Dot blot is a special case of any of the above blots where the analyte is added directly to the blotting matrix (and appears as a \"dot\") as opposed to separating the sample by electrophoresis prior to blotting.\n\n\n", "id": "770314", "title": "Blot (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=1493534", "text": "International Nucleotide Sequence Database Collaboration\n\nThe International Nucleotide Sequence Database Collaboration (INSDC, http://insdc.org) consists of a joint effort to collect and disseminate databases containing DNA and RNA sequences. It involves the following computerized databases: DNA Data Bank of Japan (Japan), GenBank (USA) and the European Nucleotide Archive (UK). New and updated data on nucleotide sequences contributed by research teams to each of the three databases are synchronized on a daily basis through continuous interaction between the staff at each the collaborating organizations.\n\nThe DDBJ/EMBL/GenBank synchronization is maintained according to a number of guidelines which are produced and published by an International Advisory Board . The guidelines consist of a common definition of the feature tables for the databases, which regulate the content and syntax of the database entries, in the form of a common DTD or \"Document Type Definition\". \n\nThe syntax is called INSDSeq and its core consists of the letter sequence of the gene expression (amino acid sequence) and the letter sequence for nucleotide bases in the gene or decoded segment. In a DBFetch operation shows a typical INSD entry at the EBI database; the same entry at NCBI is here .\n\n\n", "id": "1493534", "title": "International Nucleotide Sequence Database Collaboration"}
{"url": "https://en.wikipedia.org/wiki?curid=21843", "text": "Nucleosome\n\nA nucleosome is a basic unit of DNA packaging in eukaryotes, consisting of a segment of DNA wound in sequence around eight histone protein cores. This structure is often compared to thread wrapped around a spool.\n\nNucleosomes form the fundamental repeating units of eukaryotic chromatin, which is used to pack the large eukaryotic genomes into the nucleus while still ensuring appropriate access to it (in mammalian cells approximately 2 m of linear DNA have to be packed into a nucleus of roughly 10 µm diameter). Nucleosomes are folded through a series of successively higher order structures to eventually form a chromosome; this both compacts DNA and creates an added layer of regulatory control, which ensures correct gene expression. Nucleosomes are thought to carry epigenetically inherited information in the form of covalent modifications of their core histones.\nNucleosomes were observed as particles in the electron microscope by Don and Ada Olins in 1974, and their existence and structure (as histone octamers surrounded by approximately 200 base pairs of DNA) were proposed by Roger Kornberg. The role of the nucleosome as a general gene repressor was demonstrated by Lorch et al. in vitro, and by Han and Grunstein in vivo in 1987 and 1988, respectively.\nThe nucleosome core particle consists of approximately 146 base pairs (bp) of DNA wrapped in 1.67 left-handed superhelical turns around a histone octamer consisting of 2 copies each of the core histones H2A, H2B, H3, and H4. Core particles are connected by stretches of \"linker DNA\", which can be up to about 80 bp long. Technically, a nucleosome is defined as the core particle plus one of these linker regions; however the word is often synonymous with the core particle. Genome-wide nucleosome positioning maps are now available for many model organisms including mouse liver and brain.\n\nLinker histones such as H1 and its isoforms are involved in chromatin compaction and sit at the base of the nucleosome near the DNA entry and exit binding to the linker region of the DNA. Non-condensed nucleosomes without the linker histone resemble \"beads on a string of DNA\" under an electron microscope.\n\nIn contrast to most eukaryotic cells, mature sperm cells largely use protamines to package their genomic DNA, most likely to achieve an even higher packaging ratio. Histone equivalents and a simplified chromatin structure have also been found in Archea, suggesting that eukaryotes are not the only organisms that use nucleosomes.\n\nPioneering structural studies in the 1980s by Aaron Klug's group provided the first evidence that an octamer of histone proteins wraps DNA around itself in about 1.7 turns of a left-handed superhelix. In 1997 the first near atomic resolution crystal structure of the nucleosome was solved by the Richmond group, showing the most important details of the particle. The human alpha-satellite palindromic DNA critical to achieving the 1997 nucleosome crystal structure was developed by the Bunick group at Oak Ridge National Laboratory in Tennessee. The structures of over 20 different nucleosome core particles have been solved to date, including those containing histone variants and histones from different species. The structure of the nucleosome core particle is remarkably conserved, and even a change of over 100 residues between frog and yeast histones results in electron density maps with an overall root mean square deviation of only 1.6Å.\n\nThe nucleosome core particle (shown in the figure) consists of about 146 bp of DNA wrapped in 1.67 left-handed superhelical turns around the histone octamer, consisting of 2 copies each of the core histones H2A, H2B, H3, and H4. Adjacent nucleosomes are joined by a stretch of free DNA termed \"linker DNA\" (which varies from 10 - 80 bp in length depending on species and tissue type).\n\nNucleosome core particles are observed when chromatin in interphase is treated to cause the chromatin to unfold partially. The resulting image, via an electron microscope, is \"beads on a string\". The string is the DNA, while each bead in the nucleosome is a core particle. The nucleosome core particle is composed of DNA and histone proteins.\n\nPartial DNAse digestion of chromatin reveals its nucleosome structure. Because DNA portions of nucleosome core particles are less accessible for DNAse than linking sections, DNA gets digested into fragments of lengths equal to multiplicity of distance between nucleosomes (180, 360, 540 base pairs etc.). Hence a very characteristic pattern similar to a ladder is visible during gel electrophoresis of that DNA. Such digestion can occur also under natural conditions during apoptosis (\"cell suicide\" or programmed cell death), because autodestruction of DNA typically is its role.\n\nThe core histone proteins contains a characteristic structural motif termed the \"histone fold,\" which consists of three alpha-helices (α1-3) separated by two loops (L1-2). In solution, the histones form H2A-H2B heterodimers and H3-H4 heterotetramers. Histones dimerise about their long α2 helices in an anti-parallel orientation, and, in the case of H3 and H4, two such dimers form a 4-helix bundle stabilised by extensive H3-H3’ interaction. The H2A/H2B dimer binds onto the H3/H4 tetramer due to interactions between H4 and H2B, which include the formation of a hydrophobic cluster. \nThe histone octamer is formed by a central H3/H4 tetramer sandwiched between two H2A/H2B dimers. Due to the highly basic charge of all four core histones, the histone octamer is stable only in the presence of DNA or very high salt concentrations.\n\nThe nucleosome contains over 120 direct protein-DNA interactions and several hundred water-mediated ones. Direct protein - DNA interactions are not spread evenly about the octamer surface but rather located at discrete sites. These are due to the formation of two types of DNA binding sites within the octamer; the α1α1 site, which uses the α1 helix from two adjacent histones, and the L1L2 site formed by the L1 and L2 loops. Salt links and hydrogen bonding between both side-chain basic and hydroxyl groups and main-chain amides with the DNA backbone phosphates form the bulk of interactions with the DNA. This is important, given that the ubiquitous distribution of nucleosomes along genomes requires it to be a non-sequence-specific DNA-binding factor. Although nucleosomes tend to prefer some DNA sequences over others, they are capable of binding practically to any sequence, which is thought to be due to the flexibility in the formation of these water-mediated interactions. In addition, non-polar interactions are made between protein side-chains and the deoxyribose groups, and an arginine side-chain intercalates into the DNA minor groove at all 14 sites where it faces the octamer surface.\nThe distribution and strength of DNA-binding sites about the octamer surface distorts the DNA within the nucleosome core. The DNA is non-uniformly bent and also contains twist defects. The twist of free B-form DNA in solution is 10.5 bp per turn. However, the overall twist of nucleosomal DNA is only 10.2 bp per turn, varying from a value of 9.4 to 10.9 bp per turn.\n\nThe histone tail extensions constitute up to 30% by mass of histones, but are not visible in the crystal structures of nucleosomes due to their high intrinsic flexibility, and have been thought to be largely unstructured. The N-terminal tails of histones H3 and H2B pass through a channel formed by the minor grooves of the two DNA strands, protruding from the DNA every 20 bp. The N-terminal tail of histone H4, on the other hand, has a region of highly basic amino acids (16-25), which, in the crystal structure, forms an interaction with the highly acidic surface region of a H2A-H2B dimer of another nucleosome, being potentially relevant for the higher-order structure of nucleosomes. This interaction is thought to occur under physiological conditions also, and suggests that acetylation of the H4 tail distorts the higher-order structure of chromatin.\n\nThe organization of the DNA that is achieved by the nucleosome cannot fully explain the packaging of DNA observed in the cell nucleus. Further compaction of chromatin into the cell nucleus is necessary, but is not yet well understood. The current understanding is that repeating nucleosomes with intervening \"linker\" DNA form a \"10-nm-fiber\", described as \"beads on a string\", and have a packing ratio of about five to ten. A chain of nucleosomes can be arranged in a \"30 nm fiber\", a compacted structure with a packing ratio of ~50 and whose formation is dependent on the presence of the H1 histone.\n\nA crystal structure of a tetranucleosome has been presented and used to build up a proposed structure of the 30 nm fiber as a two-start helix. \nThere is still a certain amount of contention regarding this model, as it is incompatible with recent electron microscopy data. Beyond this, the structure of chromatin is poorly understood, but it is classically suggested that the 30 nm fiber is arranged into loops along a central protein scaffold to form transcriptionally active euchromatin. Further compaction leads to transcriptionally inactive heterochromatin.\n\nAlthough the nucleosome is a very stable protein-DNA complex, it is not static and has been shown to undergo a number of different structural re-arrangements including nucleosome sliding and DNA site exposure. Depending on the context, nucleosomes can inhibit or facilitate transcription factor binding. Nucleosome positions are controlled by three major contributions: First, the intrinsic binding affinity of the histone octamer depends on the DNA sequence. Second, the nucleosome can be displaced or recruited by the competitive or cooperative binding of other protein factors. Third, the nucleosome may be actively translocated by ATP-dependent remodeling complexes.\n\nWork performed in the Bradbury laboratory showed that nucleosomes reconstituted onto the 5S DNA positioning sequence were able to reposition themselves translationally onto adjacent sequences when incubated thermally. Later work showed that this repositioning did not require disruption of the histone octamer but was consistent with nucleosomes being able to “slide” along the DNA \"in cis\". In 2008, It was further revealed that CTCF binding sites act as nucleosome positioning anchors so that, when used to align various genomic signals, multiple flanking nucleosomes can be readily identified. Although nucleosomes are intrinsically mobile, eukaryotes have evolved a large family of ATP-dependent chromatin remodelling enzymes to alter chromatin structure, many of which do so via nucleosome sliding. In 2012, Beena Pillai's laboratory has demonstrated that nucleosome sliding is one of the possible mechanism for large scale tissue specific expression of genes. The work shows that the transcription start site for genes expressed in a particular tissue, are nucleosome depleted while, the same set of genes in other tissue where they are not expressed, are nucleosome bound.\n\nWork from the Widom laboratory has shown that nucleosomal DNA is in equilibrium between a wrapped and unwrapped state. Measurements of these rates using time-resolved FRET revealed that DNA within the nucleosome remains fully wrapped for only 250 ms before it is unwrapped for 10-50 ms and then rapidly rewrapped. This implies that DNA does not need to be actively dissociated from the nucleosome but that there is a significant fraction of time during which it is fully accessible. Indeed, this can be extended to the observation that introducing a DNA-binding sequence within the nucleosome increases the accessibility of adjacent regions of DNA when bound. This propensity for DNA within the nucleosome to “breathe” is has important functional consequences for all DNA-binding proteins that operate in a chromatin environment. In particular, the dynamic breathing of nucleosomes plays an important role in restricting the advancement of RNA polymerase II during transcription elongation.\n\nPromoters of active genes have nucleosome free regions (NFR). This allows for promoter DNA accessibility to various proteins, such as transcription factors. Nucleosome free region typically spans for 200 nucleotides in \"S. cerevisae\" Well-positioned nucleosomes form boundaries of NFR. These nucleosomes are called +1-nucleosome and -1-nucleosome and are located at canonical distances downstream and upstream, respectively, from transcription start site. +1-nucleosome and several downstream nucleosomes also tend to incorporate H2A.Z histone variant\n\nEukaryotic genomes are ubiquitously associated into chromatin; however, cells must spatially and temporally regulate specific loci independently of bulk chromatin. In order to achieve the high level of control required to co-ordinate nuclear processes such as DNA replication, repair, and transcription, cells have developed a variety of means to locally and specifically modulate chromatin structure and function. This can involve covalent modification of histones, the incorporation of histone variants, and non-covalent remodelling by ATP-dependent remodeling enzymes.\n\nSince they were discovered in the mid-1960s, histone modifications have been predicted to affect transcription. The fact that most of the early post-translational modifications found were concentrated within the tail extensions that protrude from the nucleosome core lead to two main theories regarding the mechanism of histone modification. The first of the theories suggested that they may affect electrostatic interactions between the histone tails and DNA to “loosen” chromatin structure. Later it was proposed that combinations of these modifications may create binding epitopes with which to recruit other proteins. Recently, given that more modifications have been found in the structured regions of histones, it has been put forward that these modifications may affect histone-DNA and histone-histone interactions within the nucleosome core. Modifications (such as acetylation or phosphorylation) that lower the charge of the globular histone core are predicted to \"loosen\" core-DNA association; the strength of the effect depends on location of the modification within the core.\nSome modifications have been shown to be correlated with gene silencing; others seem to be correlated with gene activation. Common modifications include acetylation, methylation, or ubiquitination of lysine; methylation of arginine; and phosphorylation of serine. The information stored in this way is considered epigenetic, since it is not encoded in the DNA but is still inherited to daughter cells. The maintenance of a repressed or activated status of a gene is often necessary for cellular differentiation.\n\nAlthough histones are remarkably conserved throughout evolution, several variant forms have been identified. It is interesting to note that this diversification of histone function is restricted to H2A and H3, with H2B and H4 being mostly invariant. H2A can be replaced by H2AZ (which leads to reduced nucleosome stability) or H2AX (which is associated with DNA repair and T cell differentiation), whereas the inactive X chromosomes in mammals are enriched in macroH2A. H3 can be replaced by H3.3 (which correlates with activate genes and regulatory elements) and in centromeres H3 is replaced by CENPA.\n\nA number of distinct reactions are associated with the term ATP-dependent chromatin remodeling. Remodeling enzymes have been shown to slide nucleosomes along DNA, disrupt histone-DNA contacts to the extent of destabilizing the H2A/H2B dimer and to generate negative superhelical torsion in DNA and chromatin. Recently, the Swr1 remodeling enzyme has been shown to introduce the variant histone H2A.Z into nucleosomes. At present, it is not clear if all of these represent distinct reactions or merely alternative outcomes of a common mechanism. What is shared between all, and indeed the hallmark of ATP-dependent chromatin remodeling, is that they all result in altered DNA accessibility.\n\nStudies looking at gene activation \"in vivo\" and, more astonishingly, remodeling \"in vitro\" have revealed that chromatin remodeling events and transcription-factor binding are cyclical and periodic in nature. While the consequences of this for the reaction mechanism of chromatin remodeling are not known, the dynamic nature of the system may allow it to respond faster to external stimuli. A recent study indicates that nucleosome positions change significantly during mouse embryonic stem cell development, and these changes are related to binding of developmental transcription factors.\n\nStudies in 2007 have catalogued nucleosome positions in yeast and shown that nucleosomes are depleted in promoter regions and origins of replication.\nAbout 80% of the yeast genome appears to be covered by nucleosomes and the pattern of nucleosome positioning clearly relates to DNA regions that regulate transcription, regions that are transcribed and regions that initiate DNA replication. Most recently, a new study examined ‘’dynamic changes’’ in nucleosome repositioning during a global transcriptional reprogramming event to elucidate the effects on nucleosome displacement during genome-wide transcriptional changes in yeast (\"Saccharomyces cerevisiae\"). The results suggested that nucleosomes that were localized to promoter regions are displaced in response to stress (like heat shock). In addition, the removal of nucleosomes usually corresponded to transcriptional activation and the replacement of nucleosomes usually corresponded to transcriptional repression, presumably because transcription factor binding sites became more or less accessible, respectively. In general, only one or two nucleosomes were repositioned at the promoter to effect these transcriptional changes. However, even in chromosomal regions that were not associated with transcriptional changes, nucleosome repositioning was observed, suggesting that the covering and uncovering of transcriptional DNA does not necessarily produce a transcriptional event. After transcription, the rDNA region has to protected from any damage, it suggested HMGB proteins play a major role in protecting the nucleosome free region.\n\nNucleosomes can be assembled \"in vitro\" by either using purified native or recombinant histones. One standard technique of loading the DNA around the histones involves the use of salt dialysis. A reaction consisting of the histone octamers and a naked DNA template can be incubated together at a salt concentration of 2 M. By steadily decreasing the salt concentration, the DNA will equilibrate to a position where it is wrapped around the histone octamers, forming nucleosomes. In appropriate conditions, this reconstitution process allows for the nucleosome positioning affinity of a given sequence to be mapped experimentally.\n\nA recent advance in the production of nucleosome core particles with enhanced stability involves site-specific disulfide crosslinks. Two different crosslinks can be introduced into the nucleosome core particle. A first one crosslinks the two copies of H2A via an introduced cysteine (N38C) resulting in histone octamer which is stable against H2A/H2B dimer loss during nucleosome reconstitution. A second crosslink can be introduced between the H3 N-terminal histone tail and the nucleosome DNA ends via an incorporated convertible nucleotide. The DNA-histone octamer crosslink stabilizes the nucleosome core particle against DNA dissociation at very low particle concentrations and at elevated salt concentrations.\n\nNucleosomes are the basic packing unit of DNA built from histone proteins around which DNA is coiled. They serve as a scaffold for formation of higher order chromatin structure as well as for a layer of regulatory control of gene expression. Nucleosomes are quickly assembled onto newly synthesized DNA behind the replication fork. \n\nHistones H3 and H4 from dissembled old nucleosomes are kept in the vicinity and randomly distributed on the newly synthesized DNA. They are assembled by the chromatin assembly factor-1 (CAF-1) complex, which consists of three subunits (p150, p60, and p48). Newly synthesized H3 and H4 are assembled by the replication coupling assembly factor (RCAF). RCAF contains the subunit Asf1, which binds to newly synthesized H3 and H4 proteins. The old H3 and H4 proteins retain their chemical modifications which contributes to the passing down of the epigenetic signature. The newly synthesized H3 and H4 proteins are gradually acetylated at different lysine residues as part of the chromatin maturation process. It is also thought that the old H3 and H4 proteins in the new nucleosomes recruit histone modifying enzymes that mark the new histones, contributing to epigenetic memory. \n\nIn contrast to old H3 and H4, the old H2A and H2B histone proteins are release and therefore primarily new H2A and H2B protein are incorporated into new nucleosomes. H2A and H2B are assembled into dimers which are then loaded onto nucleosomes by the nucleosome assembly protein-1 (NAP-1) which also assists with nucleosome sliding. The nucleosomes are also spaced by ATP-dependent nucleosome-remodeling complexes containing enzymes such as Isw1 Ino80, and Chd1, and subsequently assembled into higher order structure. \n\nThe crystal structure of the nucleosome core particle () - different views showing details of histone folding and organization. Histones , , , and are coloured.\n\n", "id": "21843", "title": "Nucleosome"}
{"url": "https://en.wikipedia.org/wiki?curid=1832781", "text": "Superhelix\n\nA superhelix is a molecular structure in which a helix is itself coiled into a helix. This is significant to both proteins and genetic material, such as overwound circular DNA.\n\nThe earliest significant reference in molecular biology is from 1971, by F.B. Fuller:\n\nA geometric invariant of a space curve, the writhing number, is defined and studied. For the central curve of a twisted cord the writhing number measures the extent to which coiling of the central curve has relieved local twisting of the cord. This study originated in response to questions that arise in the study of supercoiled double-stranded DNA rings.\"\n\nAbout the writhing number, mathematician W.F. Pohl says: \n\nIt is well known that the writhing number is a standard measure of the global geometry of a closed space curve.\"\n\nContrary to intuition, a topological property, the linking number, arises from the geometric properties twist and writhe according to the following relationship:\n\nwhere \"L\" is the linking number, \"W\" is the writhe and \"T\" is the twist of the coil. \n\nThe linking number refers to the number of times that one strand wraps around the other. In DNA this property does not change and can only be modified by specialized enzymes called topoisomerases.\n\n\n", "id": "1832781", "title": "Superhelix"}
{"url": "https://en.wikipedia.org/wiki?curid=984726", "text": "Neuropeptide\n\nNeuropeptides are small protein-like molecules (peptides) used by neurons to communicate with each other. They are neuronal signaling molecules that influence the activity of the brain and the body in specific ways. Different neuropeptides are involved in a wide range of brain functions, including analgesia, reward, food intake, metabolism, reproduction, social behaviors, learning and memory.\n\nNeuropeptides are related to peptide hormones, and in some cases peptides that function in the periphery as hormones also have neuronal functions as neuropeptides. The distinction between neuropeptide and peptide hormone has to do with the cell types that release and respond to the molecule; neuropeptides are secreted from neuronal cells (primarily neurons but also glia for some peptides) and signal to neighboring cells (primarily neurons). In contrast, peptide hormones are secreted from neuroendocrine cells and travel through the blood to distant tissues where they evoke a response. Both neuropeptides and peptide hormones are synthesized by the same sets of enzymes, which include prohormone convertases and carboxypeptidases that selectively cleave the peptide precursor at specific processing sites to generate the bioactive peptides.\n\nNeuropeptides modulate neuronal communication by acting on cell surface receptors. Many neuropeptides are co-released with other small-molecule neurotransmitters. The human genome contains about 90 genes that encode precursors of neuropeptides. At present about 100 different peptides are known to be released by different populations of neurons in the mammalian brain. Neurons use many different chemical signals to communicate information, including neurotransmitters, peptides, and gasotransmitters. Peptides are unique among these cell-cell signaling molecules in several respects. One major difference is that peptides are not recycled back into the cell once secreted, unlike many conventional neurotransmitters (glutamate, dopamine, serotonin). Another difference is that after secretion, peptides are modified by extracellular peptidases; in some cases, these extracellular cleavages inactivate the biological activity, but in other cases the extracellular cleavages increase the affinity of a peptide for a particular receptor while decreasing its affinity for another receptor. These extracellular processing events add to the complexity of neuropeptides as cell-cell signaling molecules.\n\nMany populations of neurons have distinctive biochemical phenotypes. For example, in one subpopulation of about 3000 neurons in the arcuate nucleus of the hypothalamus, three anorectic peptides are co-expressed: α-melanocyte-stimulating hormone (α-MSH), galanin-like peptide, and cocaine-and-amphetamine-regulated transcript (CART), and in another subpopulation two orexigenic peptides are co-expressed, neuropeptide Y and agouti-related peptide (AGRP). These are not the only peptides in the arcuate nucleus; β-endorphin, dynorphin, enkephalin, galanin, ghrelin, growth-hormone releasing hormone, neurotensin, neuromedin U, and somatostatin are also expressed in subpopulations of arcuate neurons. These peptides are all released centrally and act on other neurons at specific receptors. The neuropeptide Y neurons also make the classical inhibitory neurotransmitter GABA.\n\nInvertebrates also have many neuropeptides. CCAP has several functions including regulating heart rate, allatostatin and proctolin regulate food intake and growth, bursicon controls tanning of the cuticle and corazonin has a role in cuticle pigmentation and moulting.\n\nPeptide signals play a role in information processing that is different from that of conventional neurotransmitters, and many appear to be particularly associated with specific behaviours. For example, oxytocin and vasopressin have striking and specific effects on social behaviours, including maternal behaviour and pair bonding.\n\nGenerally, peptides act at metabotropic or G-protein-coupled receptors expressed by selective populations of neurons. In essence they act as specific signals between one population of neurons and another. Neurotransmitters generally affect the excitability of other neurons, by depolarising them or by hyperpolarising them. Peptides have much more diverse effects; amongst other things, they can affect gene expression, local blood flow, synaptogenesis, and glial cell morphology. Peptides tend to have prolonged actions, and some have striking effects on behaviour.\n\nNeurons very often make both a conventional neurotransmitter (such as glutamate, GABA or dopamine) and one or more neuropeptides. Peptides are generally packaged in large dense-core vesicles, and the co-existing neurotransmitters in small synaptic vesicles. The large dense-core vesicles are often found in all parts of a neuron, including the soma, dendrites, axonal swellings (varicosities) and nerve endings, whereas the small synaptic vesicles are mainly found in clusters at presynaptic locations.Release of the large vesicles and the small vesicles is regulated differently.\n\nThe following is a list of neuroactive peptides coexisting with other neurotransmitters. Transmitter names are shown in bold.\n\nNorepinephrine (noradrenaline).\nIn neurons of the A2 cell group in the nucleus of the solitary tract), norepinephrine co-exists with:\n\nGABA\n\nAcetylcholine\n\nDopamine\n\nEpinephrine (adrenaline)\n\nSerotonin (5-HT)\n\nSome neurons make several different peptides. For instance,\nVasopressin co-exists with dynorphin and galanin in magnocellular neurons of the supraoptic nucleus and paraventricular nucleus, and with CRF (in parvocellular neurons of the paraventricular nucleus)\n\nOxytocin in the supraoptic nucleus co-exists with enkephalin, dynorphin, cocaine-and amphetamine regulated transcript (CART) and cholecystokinin.\n\nA 2006 discovery might have important implications for treatment of diabetes. Researchers at the Toronto Hospital for Sick Children injected capsaicin into NOD mice (Non-obese diabetic mice, a strain that is genetically predisposed to develop the equivalent of Type 1 diabetes) to kill the pancreatic sensory nerves. This treatment reduced the development of diabetes in these mice by 80%, suggesting a link between neuropeptides and the development of Type 1 diabetes. When the researchers injected the pancreas of the diabetic mice with substance P, they were cured of the diabetes for as long as 4 months. Also, insulin resistance (characteristic of type 2 diabetes) was reduced. These research results are in the process of being confirmed, and their applicability in humans will have to be established in the future. Any treatment that could result from this research is probably years away.\n\nThere are studies investigating the relation of neuropeptides and CNS disorders including depression.\n\n", "id": "984726", "title": "Neuropeptide"}
{"url": "https://en.wikipedia.org/wiki?curid=288307", "text": "Replica plating\n\nReplica plating is a microbiological technique in which one or more secondary Petri plates containing different solid (agar-based) selective growth media (lacking nutrients or containing chemical growth inhibitors such as antibiotics) are inoculated with the same colonies of microorganisms from a primary plate (or master dish), reproducing the original spatial pattern of colonies. The technique involves pressing a velveteen-covered disk, and then imprinting secondary plates with cells in colonies removed from the original plate by the material. Generally, large numbers of colonies (roughly 30-300) are replica plated due to the difficulty in streaking each out individually onto a separate plate. \nThe purpose of replica plating is to be able to compare the master plate and any secondary plates, typically to screen for a desired phenotype. For example, when a colony that was present on the primary plate (or master dish), fails to appear on a secondary plate, it shows that the colony was sensitive to a substance on that particular secondary plate. Common screenable phenotypes include auxotrophy and antibiotic resistance.\n\nReplica plating is especially useful for \"negative selection\". However, it is more correct to refer to \"negative screening\" instead of using the term 'selection'. For example, if one wanted to select colonies that were sensitive to ampicillin, the primary plate could be replica plated on a secondary Amp agar plate. The sensitive colonies on the secondary plate would die but the colonies could still be deduced from the primary plate since the two have the same spatial patterns from ampicillin resistant colonies. The sensitive colonies could then be picked off from the primary plate. Frequently the last plate will be non-selective. In the figure, a nonselective plate will be replica plated after the Amp+ plate to confirm that the absence of growth on the selective plate is due to the selection itself and not a problem with transferring cells. If one sees growth on the third (nonselective) plate but not the second one, the selective agent is responsible for the lack of growth. If the non-selective plate shows no growth, one cannot say whether viable cells were transferred at all, and no conclusions can be made about the presence or absence of growth on selective media. This is particularly useful if there are questions about the age or viability of the cells on the original plate. \n\nBy increasing the variety of secondary plates with different selective growth media, it is possible to rapidly screen a large number of individual isolated colonies for as many phenotypes as there are secondary plates. \n\nThe development of replica plating required two steps. The first step was to define the problem: a method of identifiably duplicating colonies. The second step was to devise a means to reliably implement the first step. Replica plating was first described by Esther Lederberg and Joshua Lederberg in 1952. \n", "id": "288307", "title": "Replica plating"}
{"url": "https://en.wikipedia.org/wiki?curid=1374663", "text": "Index of molecular biology articles\n\nThis is a list of topics in molecular biology. See also index of biochemistry articles.\n\n3' end - 3' flanking region - 5' end - 5' flanking region - 5'-ribose- 3' -\n\nacrylamide gels - adenine - adenosine deaminase deficiency - adenovirus - agarose gel electrophoresis - agarose gel - Alagille syndrome - alkaline lysis - allele - amino acids - amino terminus - amp resistance - amplification - amplicon - anchor sequence - animal model - anneal - anti-sense strand - antibiotic resistance - antibody - antisense - antisense strand - AP-1 site - apoptosis - assembled epitope - ataxia-telangiectasia - ATG or AUG - autoimmune lymphoproliferative syndrome - autoradiography - autosomal dominant - autosome - avidin -\n\nBAC - back mutation - bacteria - bacterial artificial chromosome - bacteriophage - bacteriophage lambda - band shift assay - base - base pair - binding site - biological organisation - biological process - biotin - birth defect - blotting - blunt end - bone marrow transplantation - box - BP - BRCA1 - BRCA2 -\n\nC terminus - cancer - candidate gene - Canonical sequence - cap - cap site - carboxyl terminus - carcinoma - carrier - CAT assay - CCAAT box - cDNA - cDNA clone - cDNA library - cell - centimorgan - centromere - chain terminator - chaperone protein - chromosome - Chromosomal translocation - chromosome walking - CIS - cistron - clone (genetics) - clone (noun) - clone (verb) - cloning - coding sequence - coding strand - codon - codon usage bias - competent - complementary - conformational epitope - congenital - consensus sequence - conservative substitution - conserved - contig - cosmid - craniosynostosis - cystic fibrosis - cytogenetic map - cytosine -\nCpG\n\ndatabase search - degeneracy (biology) - deletion - denaturation - denaturing gel - deoxyribonuclease (DNase) - deoxyribonucleic acid - deoxyribonucleotide - diabetes mellitus - dideoxy sequencing - dideoxyribonucleotide - diploid - direct repeat - DNA ligase -DNA Bank - DNA polymerase - DNA replication - DNA sequencing - DNase - dominant - dot blot - double helix - downstream (DNA) - downstream (transduction) - ds - duplex -\n\nE. coli - electrophoresis - electroporation - Ellis-van Creveld syndrome - end labeling - endonuclease - enhancer - enzyme - epitope - ethidium bromide - evolutionary clock - evolutionary footprinting - exon - exonuclease - expression - expression clone - expression vector -\n\nfamilial Mediterranean fever - fibroblasts - fluorescence in situ hybridization - footprinting - Fragile X syndrome - frameshift mutation - fusion protein -\n\ngel electrophoresis - gel shift - gel shift assay - gene - gene amplification - gene conversion - gene expression - gene mapping - gene pool - gene therapy - gene transfer - genetic code - genetic counseling - genetic map - genetic marker - genetic screening - genetically modified mouse - genome - genomic blot - genomic clone - genomic library - genotype - germ line - glycoprotein - glycosylation - Golgi apparatus - GRE - guanine -\n\nhairpin - haploid - haploinsufficiency - helix-loop-helix - hematopoietic stem cell - hemophilia - heteroduplex DNA - heterozygous - highly conserved sequence - Hirschsprung's disease - histone - hnRNA - holoprosencephaly - homologous recombination - homology - homozygous - host strain (bacterial) - human artificial chromosome - Human Genome Project - human immunodeficiency virus - Huntington's disease - hybridization - hybridoma - hydrophilicity plot -\n\nimmunoblot - immunoprecipitation - immunotherapy - in situ hybridization - in vitro translation - inducer - inherited - initiation codon - insert - insertion - insertion sequence - intellectual property rights - intergenic - intron - inverted repeat -\n\nJunk DNA\n\nkaryotype - kilobase - kinase - Klenow fragment - Knock-down - knock-out - knock-out experiment - knockout - Kozak sequence\n\nlambda - Laser capture microdissection - leucine zipper - leukemia - library - ligase - linear epitope - linkage - linker protein - lipofectin - locus - LOD score - lymphocyte -\n\nM13 phage - malformation - mapping - marker - melanoma - melting - Johann Mendel - Mendelian inheritance - message - messenger RNA - metaphase - microarray technology - microsatellite - missense mutation - mitochondrial DNA - mobility shift - molecular weight size marker - monoclonal antibody - monosomy - mouse model - mRNA - multicistronic message - multicopy plasmid - multiple cloning site - multiple endocrine neoplasia, type 1 - mutation -\n\nN terminus - native gel - nested PCR - neurofibromatosis - nick (DNA) - nick translation - Niemann-Pick disease, type C - non-coding DNA - non-coding strand - non-directiveness - nonconservative substitution - nonsense codon - nonsense mutation - nontranslated RNA - Northern blot - NT - nuclear run-on - nuclease - nuclease protection assay - nucleoside - nucleotide - nucleus -\n\noligo - oligodeoxyribonucleotide - oligonucleotide - oncogene - oncovirus - open reading frame - operator - operon - origin of replication -\n\np53 - package - palindromic sequence - Parkinson's disease - pBR322 - PCR - pedigree - peptide - peptide bond - phage - phagemid - phenotype - phosphatase, alkaline - phosphodiester bond - phosphorylation - physical map - plasmid - point mutation - poly-A track - polyA tail - polyacrylamide gel - polyclonal antibodies - polydactyly - polymerase - polymerase chain reaction - polymorphism - polynucleotide kinase - polypeptide - positional cloning - post-transcriptional regulation - post-translational modification - post-translational processing - post-translational regulation - PRE - Precursor mRNA - primary immunodeficiency - primary transcript - primer - primer extension - probe - processivity - promoter - pronucleus - prostate cancer - protease - protein - Protein translocation - proto-oncogene - pseudogene - pseudoknot - pseudorevertant - pulse sequence database - pulsed field gel electrophoresis - purine - pyrimidine\n\nrandom primed synthesis - reading frame - recessive - recognition sequence - recombinant DNA - recombination - recombination-repair - relaxed DNA - repetitive DNA - replica plating - reporter gene - repression - repressor - residue - response element - restriction - restriction endonuclease - restriction enzyme - restriction fragment - restriction fragment length polymorphism (RFLP) - restriction fragments - restriction map - restriction site - reticulocyte lysate - retrovirus - reverse transcriptase - reverse transcription - revertant - ribonuclease - ribonuclease - ribonucleic acid - riboprobe - ribosomal binding sequence - ribosome - ribozyme - risk communication - RNA polymerase - RNA splicing - RNAi - RNase - RNase protection assay - rRNA - RT-PCR - Run-on - runoff transcript\n\nS1 end mapping - S1 nuclease - screening - SDS-PAGE - secondary structure - selection - sense strand - sequence - sequence motif - sequence polymorphism - sequence-tagged site - sequential epitope - severe combined immunodeficiency - sex chromosome - sex-linked - Shine-Dalgarno sequence - shotgun cloning - shotgun cloning or sequencing - shotgun sequencing - shuttle vector - sickle-cell disease - side chain - sigma factor - signal peptidase - signal sequence - silent mutation - single nucleotide polymorphism - siRNA - site-directed mutagenesis - site-specific recombination - slot blot - SNP - snRNA - snRNP - solution hybridization - somatic cells - Southern blot - southwestern blot - SP6 RNA polymerase - spectral karyotype - splicing - Simple Sequence Repeats (SSR) - stable transfection - start codon - stem-loop - sticky end - stop codon - streptavidin - stringency - structural motif - sub-cloning - substitution - suicide gene - supercoil - syndrome -\n\nT7 RNA polymerase - taq polymerase - TATA box - technology transfer - template - termination codon - terminator - tertiary structure - tet resistance - thymine - tissue-specific expression - tm - trans - transcript - transcription - transcription factor - transcription/translation reaction - transcriptional start site - transfection - Transformation (genetics) - transformation (with respect to bacteria) - transfection (with respect to cultured cells) - transgene - transgenic - transient transfection - transition - translation - transposition - transposon - transversion - triplet - trisomy - tRNA - tumor suppressor - tumor suppressor gene -\n\nuntranslated RNA - upstream - upstream activator sequence - upstream DNA - upstream (transduction) - uracil -\n\nvector -\n\nWestern blot - wildtype - wobble position - Wolfram syndrome -\n\nYAC (yeast artificial chromosome)\n\nzinc finger -\n\n", "id": "1374663", "title": "Index of molecular biology articles"}
{"url": "https://en.wikipedia.org/wiki?curid=2165654", "text": "Gel extraction\n\nIn molecular biology, gel extraction or gel isolation is a technique used to isolate a desired fragment of intact DNA from an agarose gel following agarose gel electrophoresis. After extraction, fragments of interest can be mixed, precipitated, and enzymatically ligated together in several simple steps. This process, usually performed on plasmids, is the basis for rudimentary genetic engineering.\n\nAfter DNA samples are run on an agarose gel, extraction involves four basic steps: identifying the fragments of interest, isolating the corresponding bands, isolating the DNA from those bands, and removing the accompanying salts and stain.\n\nTo begin, UV light is shone on the gel in order to illuminate all the ethidium bromide-stained DNA. Care must be taken to avoid exposing the DNA to mutagenic radiation for longer than absolutely necessary. The desired band is identified and physically removed with a cover slip or razor blade. The removed slice of gel should contain the desired DNA inside. An alternative method, utilizing SYBR Safe DNA gel stain and blue-light illumination, avoids the DNA damage associated with ethidium bromide and UV light.\n\nSeveral strategies for isolating and cleaning the DNA fragment of interest exist.\n\nGel extraction kits are available from several major biotech manufacturers for a final cost of approximately 1–2 US$ per sample. \nProtocols included in these kits generally call for the dissolution of the gel-slice in 3 volumes of chaotropic agent at 50 °C, followed by application of the solution to a spin-column (the DNA remains in the column), a 70% ethanol wash (the DNA remains in the column, salt and impurities are washed out), and elution of the DNA in a small volume (30 µL) of water or buffer.\n\nThe gel fragment is placed in a dialysis tube that is permeable to fluids but impermeable to molecules at the size of DNA, thus preventing the DNA from passing through the membrane when soaked in TE buffer. An electric field is established around the tubing (in a way similar to gel electrophresis) long enough so that the DNA is removed from the gel but remains in the tube. The tube solution can then be pipetted out and will contain the desired DNA with minimal background.\n\nThe traditional method of gel extraction involves creating a folded pocket of Parafilm wax paper and placing the agarose fragment inside. The agarose is physically compressed with a finger into a corner of the pocket, partially liquifying the gel and its contents. The liquid droplets can then be directed out of the pocket onto an exterior piece of Parafilm, where they are pipetted into a small tube. A butanol extraction removes the ethidium bromide stain, followed by a phenol/chloroform extraction of the cleaned DNA fragment.\n\nThe disadvantage of gel isolation is that background can only be removed if it can be physically identified using the UV light. If two bands are very close together, it can be hard to separate them without some contamination. In order to clearly identify the band of interest, further restriction digests may be necessary. Restriction sites unique to unwanted bands of similar size can aid in breaking up these potential contaminants.\n", "id": "2165654", "title": "Gel extraction"}
{"url": "https://en.wikipedia.org/wiki?curid=1943021", "text": "Henderson limit\n\nThe value of the Henderson limit is defined as 2 × 10 Gy (J/kg).\n\nAlthough generalizable, the limit is defined in the context of biomolecular X-ray crystallography, where a typical experiment consists of exposing a single frozen crystal of a macromolecule (generally protein, DNA or RNA) to an intense X-ray beam. The beams that are diffracted are then analyzed towards obtaining an atomically resolved model of the crystal.\n\nThe limit is defined as the X-ray dose (energy per unit mass) a cryo-cooled crystal can absorb before the diffraction pattern decays to half of its original intensity. Such decay presents itself as a problem for crystallographers who require that the diffraction intensities decay as little as possible, to maximize the signal to noise ratio in order to determine accurate atomic models that describe the crystal.\n\nAlthough the process is still not fully understood, diffraction patterns of crystals typically decay with X-ray exposure due to a number of processes which non uniformly and irreversibly modify molecules that compose the crystal. These modifications induce disorder and thus decrease the intensity of Bragg diffraction. The processes behind these modifications include primary damage via the photo electric effect, covalent modification by free radicals, oxidation (methionine residues), reduction (disulfide bonds) and decarboxylation (glutamate, aspartate residues).\n\n", "id": "1943021", "title": "Henderson limit"}
{"url": "https://en.wikipedia.org/wiki?curid=2206793", "text": "Immunomagnetic separation\n\nImmunomagnetic separation (IMS) is a laboratory tool that can efficiently isolate cells out of body fluid or cultured cells. It can also be used as a method of quantifying the pathogenicity of food, blood or feces. DNA analysis have supported the combined use of both this technique and Polymerase Chain Reaction (PCR). Another laboratory separation tool is the affinity magnetic separation (AMS), which is more suitable for the isolation of prokaryotic cells.\n\nImmunomagnetic separation (IMS) is a method that deals with the isolation of cells, proteins, and nucleic acids within a cell culture or body fluid through the specific capture of biomolecules through the attachment of small-magnetized particles, beads, containing antibodies and lectins . These beads are coated to bind to targeted biomolecules, gently separated and goes through multiple cycles of washing to obtain targeted molecules bound to these super paramagnetic beads, which can differentiate based on strength of magnetic field and targeted molecules, are then eluted to collect supernatant and then are able to determine the concentration of specifically targeted biomolecules. The technique of immunomagenitc separation (IMS) obtains certain concentrations of specific molecules within targeted bacteria. \n\nA mixture of cell population will be put into a magnetic field where cells then are attached to super paramagnetic beads, specific example are Dynabeads (4.5-μm), will remain once excess substrate is removed binding to targeted antigen. Dynabeads consists of iron-containing cores, which is covered by a thin layer of a polymer shell allowing the absorption of biomolecules. The beads are coated with primary antibodies, specific-specific antibodies, lectins, enzymes, or streptavidin ; the linkage between magnetized beads coated materials are cleavable DNA linker cell separation from the beads when the culturing of cells is more desirable . \n\nMany of these beads have the same principles of separation; however, the presence and different strength s of magnetic fields requires certain sizes of beads, based on the ramifications of the separation of the cell population. The larger sized beads (>2μm) are the most commonly used range that was produced by Dynal (Dynal [UK] Ltd., Wirral, Mersyside, UK; Dynal, Inc., Lake Success, NY). Where as smaller beads (<100nm) are mostly used for MACS system that was produced by Miltenyi Biotech (Miltenyi Biotech Ltd., Bisley, Surrey, UK; Miltenyi Biotech Inc., Auburn, CA) \n\nImmunomagnetic separation (IMS) is used in a variety of scientific fields including molecular biology, microbiology, and immunology. (3) This technique of separation does not only consist of separation of cells within the blood, but can also be used for techniques of separation from primary tumors and in metastases research, through separation into component parts, creating a singular-cell delay, then allowing the suitable antibody to label the cell. In metastasis research this separation technique may become necessary to separate when given a cell population and wanting to isolate tumors cells in tumors, peripheral blood, and bone marrow. \n\nAntibodies coating paramagnetic beads will bind to antigens present on the surface of cells thus \"capturing\" the cells and facilitate the concentration of these bead-attached cells. The concentration process is created by a magnet placed on the side of the test tube bringing the beads to it.\nMACS systems (Magnetic Cell Separation system) :\n\nThrough the usage of smaller super paramagnetic beads (<100nm), which requires a stronger magnetic field to separate cells. Cells are labeled with primary antibodies and then MACS beads are coated with specific- specific antibodies. These labeled cell suspension is then put into a separation column in a strong magnetic field. The labeled cells are contained, magnetized, while in the magnetic field and the unlabeled cells are suspended, un-magnetized, to be collected. Once removed from magnetic field positive cells are eluted. These MACS beads are then incorporated by the cells allowing them to remain in the column because they do not intrude with the cell attachment to the culture surface to cell-cell interactions. A bead removal reagent is then applied to have an enzymatically release of the MACS beads allowing those cells to become relabeled with some other marker, which then is sorted. \n\n<references \\>\n", "id": "2206793", "title": "Immunomagnetic separation"}
{"url": "https://en.wikipedia.org/wiki?curid=2774973", "text": "Protein misfolding cyclic amplification\n\nProtein misfolding cyclic amplification (PMCA) is an amplification technique (conceptually like PCR but not involving nucleotides) to multiply misfolded prions originally developed by Soto and colleagues. It is a test for spongiform encephalopathies like BSE.\n\nThe technique initially incubates a small amount of abnormal prion with an excess of normal protein, so that some conversion takes place. The growing chain of misfolded protein is then blasted with ultrasound, breaking it down into smaller chains and so rapidly increasing the amount of abnormal protein available to cause conversions. By repeating the cycle, the mass of normal protein is rapidly changed into misfolded prion (termed PrPSc).\n\nPMCA was originally developed to, in vitro, mimic prion replication with a similar efficiency to the in vivo process, but with accelerated kinetics. PMCA is conceptually analogous to DNA amplification by PCR. In both systems a template grows at the expense of a substrate in a cyclic reaction, combining growing and multiplication of the template units.\n\nPMCA has been applied to replicate the misfolded protein from diverse species. The newly generated protein exhibits the same biochemical, biological, and structural properties as brain-derived PrPSc and strikingly it is infectious to wild type animals, producing a disease with similar characteristics as the illness produced by brain-isolated prions.\n\nThe technology has been automated, leading to a dramatic increase in the efficiency of amplification. Indeed, one round of PMCA cycling results in a 2500-fold increase in sensitivity of detection over western blotting, whereas 2 and 7 rounds of successive PMCA cyclings result in 6 million- and 3 billion-fold increases in sensitivity of detection over western blotting, a technique widely used in BSE surveillance in several countries.\n\nIt has been shown that PMCA is capable of detecting as little as a single molecule of oligomeric infectious PrPSc. PMCA possesses the ability to generate millions infectious units, starting with the equivalent to one PrPSc oligomer; well below the infectivity threshold. This data demonstrates that PMCA has a similar power of amplification as PCR techniques used to amplify DNA. It opens a great promise for development of a highly sensitive detection of PrPSc, and for understanding the molecular basis of prion replication. Indeed, PMCA has been used by various groups to PrPSc in blood of animals experimentally infected with prions during both the symptomatic and pre-symptomatic phases as well as in urine.\n\nThe PMCA technology has been used by several groups to understand the molecular mechanism of prion replication, the nature of the infectious agent, the phenomenon of prion strains and species barrier, the effect of cellular components, to detect PrPSc in tissues and biological fluids and to screen for inhibitors against prion replication. Recent studies by the groups of Supattapone and Ma were able to produce prion replication in vitro by PMCA using purified PrPC and recombinant PrPC with the sole addition of synthetic polyanions and lipids. These studies have shown that infectious prions can be produced in the absence of any other cellular component and constitute some of the strongest evidence in favor of the prion hypothesis.\n", "id": "2774973", "title": "Protein misfolding cyclic amplification"}
{"url": "https://en.wikipedia.org/wiki?curid=239462", "text": "Phosphodiesterase\n\nA phosphodiesterase (PDE) is an enzyme that breaks a phosphodiester bond. Usually, \"phosphodiesterase\" refers to cyclic nucleotide phosphodiesterases, which have great clinical significance and are described below. However, there are many other families of phosphodiesterases, including phospholipases C and D, autotaxin, sphingomyelin phosphodiesterase, DNases, RNases, and restriction endonucleases (which all break the phosphodiester backbone of DNA or RNA), as well as numerous less-well-characterized small-molecule phosphodiesterases.\n\nThe cyclic nucleotide phosphodiesterases comprise a group of enzymes that degrade the phosphodiester bond in the second messenger molecules cAMP and cGMP. They regulate the localization, duration, and amplitude of cyclic nucleotide signaling within subcellular domains. PDEs are therefore important regulators of signal transduction mediated by these second messenger molecules.\n\nThese multiple forms (isoforms or subtypes) of phosphodiesterase were isolated from rat brain using polyacrylamide gel electrophoresis in the early 1970s, and were soon afterward shown to be selectively inhibited by a variety of drugs in brain and other tissues.\n\nThe potential for selective phosphodiesterase inhibitors to be used as therapeutic agents was predicted in the 1970s. This prediction has now come to pass in a variety of fields (e.g. sildenafil as a PDE5 inhibitor and Rolipram as a PDE4 inhibitor).\n\nThe PDE nomenclature signifies the \"PDE family\" with an Arabic numeral, then a capital letter denotes the \"gene in that family\", and a second and final Arabic numeral then indicates the \"splice variant\" derived from a single gene (e.g., PDE1C3: family 1, gene C, splicing variant 3).\n\nThe superfamily of PDE enzymes is classified into 12 families, namely PDE1-PDE12, in mammals. The classification is based on:\n\nDifferent PDEs of the same family are functionally related despite the fact that their amino acid sequences can show considerable divergence. PDEs have different substrate specificities. Some are cAMP-selective hydrolases (PDE4, 7 and 8); others are cGMP-selective (PDE5, 6, and 9). Others can hydrolyse both cAMP and cGMP (PDE1, 2, 3, 10, and 11). PDE3 is sometimes referred to as cGMP-inhibited phosphodiesterase. Although PDE2 can hydrolyze both cyclic nucleotides, binding of cGMP to the regulatory GAF-B domain will increase cAMP affinity and hydrolysis to the detriment of cGMP. This mechanism, as well as others, allows for cross-regulation of the cAMP and cGMP pathways. PDE12 cleaves cAMP and oligoadenylates.\n\nPhosphodiesterase enzymes are often targets for pharmacological inhibition due to their unique tissue distribution, structural properties, and functional properties.\n\nInhibitors of PDE can prolong or enhance the effects of physiological processes mediated by cAMP or cGMP by inhibition of their degradation by PDE.\n\nSildenafil (Viagra) is an inhibitor of cGMP-specific phosphodiesterase type 5, which enhances the vasodilatory effects of cGMP in the corpus cavernosum and is used to treat erectile dysfunction. Sildenafil is also currently being investigated for its myo- and cardioprotective effects, with particular interest being given to the compound's therapeutic value in the treatment of Duchenne muscular dystrophy and benign prostatic hyperplasia.\n\nPDE inhibitors have been identified as new potential therapeutics in areas such as pulmonary arterial hypertension, coronary heart disease, dementia, depression, asthma, COPD, protozoal infections (including malaria) and schizophrenia.\n\nPDE also are important in seizure incidence. For example, PDE compromised the antiepileptic activity of adenosine. In addition, using of a PDE inhibitor (pentoxifylline) in pentylenetetrazole-induced seizure indicated the antiepileptic effect by increasing the time latency to seizure incidence and decreasing the seizure duration in vivo.\n\nCilostazol (Pletal) inhibits PDE3. This inhibition allows red blood cells to be more able to bend. This is useful in conditions such as intermittent claudication, as the cells can maneuver through constricted veins and arteries more easily.\n\nZaprinast inhibits the growth of asexual blood-stage malaria parasites (\"P. falciparum\") \"in vitro\" with an ED value of 35 μM, and inhibits PfPDE1, a \"P. falciparum\" cGMP-specific phosphodiesterase, with an IC value of 3.8 μM.\n\nXanthines such as caffeine and theobromine are cAMP-phosphodiesterase inhibitors. However, the inhibitory effect of xanthines on phosphodiesterases are only seen at dosages higher than what people normally consume.\nSildenafil are PDE V inhibitors and are widely used in the treatment of erectile dysfunction.\n", "id": "239462", "title": "Phosphodiesterase"}
{"url": "https://en.wikipedia.org/wiki?curid=1917790", "text": "Protein Information Resource\n\nThe Protein Information Resource (PIR), located at Georgetown University Medical Center (GUMC), is an integrated public bioinformatics resource to support genomic and proteomic research, and scientific studies.\n\nPIR was established in 1984 by the National Biomedical Research Foundation (NBRF) as a resource to assist researchers and customers in the identification and interpretation of protein sequence information. Prior to that, the NBRF compiled the first comprehensive collection of macromolecular sequences in the Atlas of Protein Sequence and Structure, published from 1964-1974 under the editorship of Margaret Dayhoff. Dr. Dayhoff and her research group pioneered in the development of computer methods for the comparison of protein sequences, for the detection of distantly related sequences and duplications within sequences, and for the inference of evolutionary histories from alignments of protein sequences.. \n\nWinona Barker and Robert Ledley assumed leadership of the project after the untimely death of Dr. Dayhoff in 1983. In 1999, Dr. Cathy H. Wu joined NBRF, and later on GUMC, to head the bioinformatics efforts of PIR, and has served first as Principal Investigator and, since 2001, as Director. \n\nFor four decades, PIR has provided many protein databases and analysis tools freely accessible to the scientific community, including the Protein Sequence Database (PSD), the first international database (see PIR-International), which grew out of Atlas of Protein Sequence and Structure.\n\nIn 2002, PIR along with its international partners, EBI (European Bioinformatics Institute) and SIB (Swiss Institute of Bioinformatics), were awarded a grant from NIH to create UniProt, a single worldwide database of protein sequence and function, by unifying the PIR-PSD, Swiss-Prot, and TrEMBL databases. , PIR offers a wide variety of resources mainly oriented to assist the propagation and standardization of protein annotation: PIRSF, iProClass, and iProLINK.\n\nThe Protein Ontology (PRO) is another popular database released by the Protein Information Resource.\n", "id": "1917790", "title": "Protein Information Resource"}
{"url": "https://en.wikipedia.org/wiki?curid=1154853", "text": "Pyrosequencing\n\nPyrosequencing is a method of DNA sequencing (determining the order of nucleotides in DNA) based on the \"sequencing by synthesis\" principle, in which the sequencing is performed by detecting the nucleotide incorporated by a DNA polymerase. Pyrosequencing relies on light detection based on a chain reaction when pyrophosphate is released.\n\nThe principle of Pyrosequencing was first described in 1993 by Bertil Pettersson, Mathias Uhlen and Pål Nyren by combining the solid phase sequencing method using streptavidin coated magnetic beads with recombinant DNA polymerase lacking 3´to 5´exonuclease activity (proof-reading) and luminescence detection using the firefly luciferase enzyme. A mixture of three enzymes (DNA polymerase, ATP sulfurylase and firefly luciferase) and a nucleotide (dNTP) are added to single stranded DNA to be sequenced and the incorporation of nucleotide is followed by measuring the light emitted. The intensity of the light determines if 0, 1 or more nucleotides have been incorporated, thus showing how many complementary nucleotides are present on the template strand. The nucleotide mixture is removed before the next nucleotide mixture is added. This process is repeated with each of the four nucleotides until the DNA sequence of the single stranded template is determined.\n\nA second solution-based method for Pyrosequencing was described in 1998 by Mostafa Ronaghi, Mathias Uhlen and Pål Nyren. In this alternative method, an additional enzyme apyrase is introduced to remove nucleotides that are not incorporated by the DNA polymerase. This enabled the enzyme mixture including the DNA polymerase, the luciferase and the apyrase to be added at the start and kept throughout the procedure, thus providing a simple set-up suitable for automation. An automated instrument based on this principle was introduced to the market the following year by the company Pyrosequencing.\n\nA third microfluidic variant of the Pyrosequencing method was described in 2005 by Jonathan Rothberg and co-workers at the company 454 Life Sciences. This alternative approach for Pyrosequencing was based on the original principal of attaching the DNA to be sequenced to a solid support and they showed that sequencing could be performed in a highly parallel manner using a microfabricated microarray. This allowed for high-throughput DNA sequencing and an automated instrument was introduced to the market. This became the first next generation sequencing instrument starting a new era in genomics research, with rapidly falling prices for DNA sequencing allowing whole genome sequencing at affordable prices.\n\n\"Sequencing by synthesis\" involves taking a single strand of the DNA to be sequenced and then synthesizing its complementary strand enzymatically. The pyrosequencing method is based on detecting the activity of DNA polymerase (a DNA synthesizing enzyme) with another chemoluminescent enzyme. Essentially, the method allows sequencing a single strand of DNA by synthesizing the complementary strand along it, one base pair at a time, and detecting which base was actually added at each step. The template DNA is immobile, and solutions of A, C, G, and T nucleotides are sequentially added and removed from the reaction. Light is produced only when the nucleotide solution complements the first unpaired base of the template. The sequence of solutions which produce chemiluminescent signals allows the determination of the sequence of the template.\n\nFor the solution-based version of Pyrosequencing, the single-strand DNA (ssDNA) template is hybridized to a sequencing primer and incubated with the enzymes DNA polymerase, ATP sulfurylase, luciferase and apyrase, and with the substrates adenosine 5´ phosphosulfate (APS) and luciferin.\n\nCurrently, a limitation of the method is that the lengths of individual reads of DNA sequence are in the neighborhood of 300-500 nucleotides, shorter than the 800-1000 obtainable with chain termination methods (e.g. Sanger sequencing). This can make the process of genome assembly more difficult, particularly for sequences containing a large amount of repetitive DNA.\n\nThe company Pyrosequencing AB in Uppsala, Sweden was founded with venture capital provided by HealthCap in order to commercialize machinery and reagents for sequencing short stretches of DNA using the pyrosequencing technique. Pyrosequencing AB was listed on the Stockholm Stock Exchange in 1999. It was renamed to Biotage in 2003. The pyrosequencing business line was acquired by Qiagen in 2008. Pyrosequencing technology was further licensed to 454 Life Sciences. 454 developed an array-based pyrosequencing technology which has emerged as a platform for large-scale DNA sequencing. Most notable are the applications for genome sequencing and metagenomics. Roche announced the discontinuation of the 454 sequencing platform in 2013.\n", "id": "1154853", "title": "Pyrosequencing"}
{"url": "https://en.wikipedia.org/wiki?curid=3568175", "text": "Nested polymerase chain reaction\n\nNested polymerase chain reaction (Nested PCR) is a modification of polymerase chain reaction intended to reduce non-specific binding in products due to the amplification of unexpected primer binding sites.\n\nPolymerase chain reaction itself is the process used to amplify DNA samples, via a temperature-mediated DNA polymerase. The products can be used for sequencing or analysis, and this process is a key part of many genetics research laboratories, along with uses in DNA fingerprinting for forensics and other human genetic cases. Conventional PCR requires primers complementary to the termini of the target DNA. The amount of product from the PCR increases with the number of temperature cycles that the reaction is subjected to. A commonly occurring problem is primers binding to incorrect regions of the DNA, giving unexpected products. This problem becomes more likely with an increased number of cycles of PCR.\n\nNested polymerase chain reaction involves two sets of primers, used in two successive runs of polymerase chain reaction, the second set intended to amplify a secondary target within the first run product. This allows amplification for a low number of runs in the first round, limiting non-specific products. The second nested primer set should only amplify the intended product from the first round of amplification and not non-specific product. This allows running more total cycles while minimizing non-specific products. This is useful for very rare templates or PCR with high background.\n\n\nBooks at your local library about nested polymerase chain reactions\nScholarly articles on nested polymerase chain reactions\n", "id": "3568175", "title": "Nested polymerase chain reaction"}
{"url": "https://en.wikipedia.org/wiki?curid=913620", "text": "Touchdown polymerase chain reaction\n\nThe touchdown polymerase chain reaction or touchdown style polymerase chain reaction is a method of polymerase chain reaction by which primers avoid amplifying nonspecific sequences. The annealing temperature during a polymerase chain reaction determines the specificity of primer annealing. The melting point of the primer sets the upper limit on annealing temperature. At temperatures just above this point, only very specific base pairing between the primer and the template will occur. At lower temperatures, the primers bind less specifically. Nonspecific primer binding obscures polymerase chain reaction results, as the nonspecific sequences to which primers anneal in early steps of amplification will \"swamp out\" any specific sequences because of the exponential nature of polymerase amplification.\n\nThe earliest steps of a touchdown polymerase chain reaction cycle have high annealing temperatures. The annealing temperature is decreased in increments for every subsequent set of cycles (the number of individual cycles and increments of temperature decrease is chosen by the experimenter). The primer will anneal at the highest temperature which is least-permissive of nonspecific binding that it is able to tolerate. Thus, the first sequence amplified is the one between the regions of greatest primer specificity; it is most likely that this is the sequence of interest. These fragments will be further amplified during subsequent rounds at lower temperatures, and will outcompete the nonspecific sequences to which the primers may bind at those lower temperatures. If the primer initially (during the higher-temperature phases) binds to the sequence of interest, subsequent rounds of polymerase chain reaction can be performed upon the product to further amplify those fragments. Touchdown increases specificity of the reaction at higher temperatures and increases the efficiency towards the end by lowering the annealing temperature.\n", "id": "913620", "title": "Touchdown polymerase chain reaction"}
{"url": "https://en.wikipedia.org/wiki?curid=617379", "text": "Retrotransposon\n\nRetrotransposons also called transposons via RNA intermediate are genetic elements that can amplify themselves in a genome and are ubiquitous components of the DNA of many eukaryotic organisms. These DNA sequences use a \"copy-and-paste\" mechanism, whereby they are first transcribed into RNA, then converted back into identical DNA sequences using reverse transcription, and these sequences are then inserted into the genome at target sites.\n\nRetrotransposons form one of the two subclasses of transposons, where the others are DNA transposons, which does not involve an RNA intermediate.\n\nRetrotransposons are particularly abundant in plants, where they are often a principal component of nuclear DNA. In maize, 49–78% of the genome is made up of retrotransposons. In wheat, about 90% of the genome consists of repeated sequences and 68% of transposable elements. In mammals, almost half the genome (45% to 48%) is transposons or remnants of transposons. Around 42% of the human genome is made up of retrotransposons, while DNA transposons account for about 2–3%.\n\nThe retrotransposons' replicative mode of transposition by means of an RNA intermediate rapidly increases the copy numbers of elements and thereby can increase genome size. Like DNA transposable elements (class II transposons), retrotransposons can induce mutations by inserting near or within genes. Furthermore, retrotransposon-induced mutations are relatively stable, because the sequence at the insertion site is retained as they transpose via the replication mechanism.\nRetrotransposons copy themselves to RNA and then back to DNA that may integrate back to the genome. The second step of forming DNA may be carried out by a reverse transcriptase, which the retrotransposon encodes. Transposition and survival of retrotransposons within the host genome are possibly regulated both by retrotransposon- and host-encoded factors, to avoid deleterious effects on host and retrotransposon as well. The understanding of how retrotransposons and their hosts' genomes have co-evolved mechanisms to regulate transposition, insertion specificities, and mutational outcomes in order to optimize each other's survival is still in its infancy.\n\nBecause of accumulated mutations, most retrotransposons are no longer able to retrotranspose.\n\nRetrotransposons, also known as class I transposable elements, consist of two subclasses, the long terminal repeat (LTR-retrotransposons) and the non-LTR retrotransposons. Classification into these subclasses is based on the phylogeny of the reverse transcriptase, which goes in line with structural differences, such as presence/absence of long terminal repeats as well as number and types of open reading frames, encoding domains and target site duplication lengths.\nLTR retrotransposons have direct LTRs that range from ~100 bp to over 5 kb in size. LTR retrotransposons are further sub-classified into the Ty1-\"copia\"-like (Pseudoviridae), Ty3-\"gypsy\"-like (Metaviridae), and BEL-Pao-like groups based on both their degree of sequence similarity and the order of encoded gene products. Ty1-\"copia\" and Ty3-\"gypsy\" groups of retrotransposons are commonly found in high copy number (up to a few million copies per haploid nucleus) in animals, fungi, protista, and plants genomes. BEL-Pao like elements have so far only been found in animals. \n\nAlthough retroviruses are often classified separately, they share many features with LTR retrotransposons. A major difference with Ty1-\"copia\" and Ty3-\"gypsy\" retrotransposons is that retroviruses have an envelope protein (ENV). A retrovirus can be transformed into an LTR retrotransposon through inactivation or deletion of the domains that enable extracellular mobility. If such a retrovirus infects and subsequently inserts itself in the genome in germ line cells, it may become transmitted vertically and become an Endogenous Retrovirus (ERV). Endogenous retroviruses make up about 8% of the human genome and approximately 10% of the mouse genome.\n\nIn plant genomes, LTR retrotransposons are the major repetitive sequence class, e.g. able to constitute more than 75% of the maize genome.\n\nEndogenous retroviruses are the most important LTR retrotransposons in mammals, including humans where the Human ERVs make up 8% of the genome.\nNon-LTR retrotransposons consist of two sub-types, long interspersed elements (LINEs) and short interspersed elements (SINEs). They can also be found in high copy numbers, as shown in the plant species. Non-long terminal repeat (LTR) retroposons are widespread in eukaryotic genomes. LINEs possess two ORFs, which encode all the functions needed for retrotransposition. These functions include reverse transcriptase and endonuclease activities, in addition to a nucleic acid-binding property needed to form a ribonucleoprotein particle. SINEs, on the other hand, co-opt the LINE machinery and function as nonautonomous retroelements. While historically viewed as \"junk DNA\", recent research suggests that, in some rare cases, both LINEs and SINEs were incorporated into novel genes so as to evolve new functionality.\n\nLong INterspersed Elements (LINE) are a group of genetic elements that are found in large numbers in eukaryotic genomes, comprising 17% of the human genome (99.9% of which is no longer capable of retrotransposition, and therefore considered \"dead\" or inactive). Among the LINE, there are several subgroups, such as L1, L2 and L3. Human coding L1 begin with an untranslated region (UTR) that includes an RNA polymerase II promoter, two non-overlapping open reading frames (ORF1 and ORF2), and ends with another UTR. Recently, a new open reading frame in the 5' end of the LINE elements has been identified in the reverse strand. It is shown to be transcribed and endogenous proteins are observed. The name ORF0 is coined due to its position with respect to ORF1 and ORF2. ORF1 encodes an RNA binding protein and ORF2 encodes a protein having an endonuclease (e.g. RNase H) as well as a reverse transcriptase. The reverse transcriptase has a higher specificity for the LINE RNA than other RNA, and makes a DNA copy of the RNA that can be integrated into the genome at a new site. The endonuclease encoded by non-LTR retroposons may be AP (Apurinic/Pyrimidinic) type or REL (Restriction Endonuclease Like) type. Elements in the R2 group have REL type endonuclease, which shows site specificity in insertion.\n\nThe 5' UTR contains the promoter sequence, while the 3' UTR contains a polyadenylation signal (AATAAA) and a poly-A tail. Because LINEs (and other class I transposons, e.g. LTR retrotransposons and SINEs) move by copying themselves (instead of moving by a cut and paste like mechanism, as class II transposons do), they enlarge the genome. The human genome, for example, contains about 500,000 LINEs, which is roughly 17% of the genome. Of these, approximately 7,000 are full-length, a small subset of which are capable of retrotransposition.\n\nInterestingly, it was recently found that specific LINE-1 retroposons in the human genome are actively transcribed and the associated LINE-1 RNAs are tightly bound to nucleosomes and essential in the establishment of local chromatin environment.\n\nShort interspersed nuclear elements (SINEs) are a group of non-LTR (long terminal repeat) and non-autonomous retrotransposons.\n\nSINEs are the only TEs that are non- autonomous by nature, meaning that they did not evolve from autonomous elements. They are small (80- 500 bases)) and rely \"in trans\" on functional LINEs for their replication, but their evolutionary origin is very distinct. SINEs can be found in very diverse eukaryotes, but they have only accumulated to impressive amount in mammals, where they represent between 5 and 15% of the genome with millions of copies.\n\nSINEs typically possess a “head” with an RNA pol III promoter that enables autonomous transcription, and a body of various composition. SINEs are postulated to originate from the accidental retrotransposition of various RNA pol III transcripts, and have appeared separately numerous times in evolution history. The type of RNA pol III promoter defines the different superfamilies and reveal their origin: tRNA, 5S ribosomal RNA or signal recognition particle 7SL RNA.\n\nSINEs do not encode a functional reverse transcriptase protein and rely on other mobile elements for the transposition, especially LINEs. SINE RNAs form a complex with LINE ORF2 proteins and are inserted into the genome by target primed reverse transcription, creating short TSDs upon insertion. Some SINE families are thought to rely on specific LINEs for their replication, while others seem to be more generalist.\n\nAlu and B1 elements, with their 1.1 million and 650,000 copies in the human and mouse genomes, respectively, harbor a 7SL promoter. The 350,000 copies of B2 SINEs in the mouse are on the other hand tRNA-related.\n\nAlu and B1 elements, with their 1.1 million and 650,000 copies in the human and mouse genomes, respectively, harbor a 7SL promoter.\n\nThe 350,000 copies of B2 SINEs in the mouse are on the other hand tRNA- related.\n\nThe most common SINE in primates is \"Alu\". \"Alu\" elements are approximately 350 base pairs long, do not contain any coding sequences, and can be recognized by the restriction enzyme (hence the name). The distribution of these elements has been implicated in some genetic diseases and cancers.\n\nHominid genomes contain also original elements termed SVA. They are composite transposons formed by the fusion of a SINE-R and an Alu, separated by a variable number of tandems repeats. Less than 3kb in length and apparently mobilized using LINE1 machinery, they are around 2500-3000 copies in human or gorilla genomes, and less than 1000 in orangutan. SVA are one of the youngest transposable element in great apes genome and among the most active and polymorphic in the human population. \n\n", "id": "617379", "title": "Retrotransposon"}
{"url": "https://en.wikipedia.org/wiki?curid=4529808", "text": "T7 DNA helicase\n\nT7 DNA helicase is a hexameric motor protein that uses energy from dTTP hydrolysis to process unidirectionally along single stranded DNA, separating the two strands as it progresses.\n\nThe crystal structure was solved to 3.0 Å resolution in 2000, as shown in the figure. In (A), notice that the separate subunits appear to be anchored through interactions between an alpha helix and an adjacent subunit. In (B), there are six sets of three loops. The red loop, known as loop II, contains three lysine residues and is thought to be involved in binding the ssDNA that is fed through the center of the enzyme.\n\n\"Crampton et al.\" have proposed a mechanism for the ssDNA-dependent hydrolysis of dTTP by T7 DNA helicase as shown in the figure below. In their model, protein loops located on each hexameric subunit, each of which contain three lysine residues, sequentially interact with the negatively charged phosphate backbone of ssDNA. This interaction presumably causes a conformational change in the actively bound subunit, providing for the efficient release of dTDP from its dTTP binding site. In the process of dTDP release, the ssDNA is transferred to the neighboring subunit, which undergoes a similar process. Previous studies have already suggested that ssDNA is able to bind to two hexameric subunits simultaneously.\n\n", "id": "4529808", "title": "T7 DNA helicase"}
{"url": "https://en.wikipedia.org/wiki?curid=331121", "text": "Contig\n\nA contig (from \"contiguous\") is a set of overlapping DNA segments that together represent a consensus region of DNA.\nIn bottom-up sequencing projects, a contig refers to overlapping sequence data (reads); in top-down sequencing projects, contig refers to the overlapping clones that form a physical map of the genome that is used to guide sequencing and assembly. Contigs can thus refer both to overlapping DNA sequence and to overlapping physical segments (fragments) contained in clones depending on the context.\n\nIn 1780, Staden wrote: \"In order to make it easier to talk about our data gained by the shotgun method of sequencing we have invented the word \"contig\". A contig is a set of gel readings that are related to one another by overlap of their sequences. All gel readings belong to one and only one contig, and each contig contains at least one gel reading. The gel readings in a contig can be summed to form a contiguous consensus sequence and the length of this sequence is the length of the contig.\"\n\nA sequence contig is a continuous (not contiguous) sequence resulting from the reassembly of the small DNA fragments generated by bottom-up sequencing strategies. This meaning of contig is consistent with the original definition by Rodger Staden (1979). The bottom-up DNA sequencing strategy involves shearing genomic DNA into many small fragments (\"bottom\"), sequencing these fragments, reassembling them back into contigs and eventually the entire genome (\"up\"). Because current technology allows for the direct sequencing of only relatively short DNA fragments (300–1000 nucleotides), genomic DNA must be fragmented into small pieces prior to sequencing. In bottom-up sequencing projects, amplified DNA is sheared randomly into fragments appropriately sized for sequencing. The subsequent sequence reads, which are the data that contain the sequences of the small fragments, are put into a database. The assembly software then searches this database for pairs of overlapping reads. Assembling the reads from such a pair (including, of course, only one copy of the identical sequence) produces a longer contiguous read (contig) of sequenced DNA. By repeating this process many times, at first with the initial short pairs of reads but then using increasingly longer pairs that are the result of previous assembly, the DNA sequence of an entire chromosome can be determined.\n\nToday, it is common to use paired-end sequencing technology where both ends of \"consistently sized\" longer DNA fragments are sequenced. Here, a contig still refers to any contiguous stretch of sequence data created by read overlap. Because the fragments are of known length, the distance between the two end reads from each fragment is known. This gives additional information about the orientation of contigs constructed from these reads and allows for their assembly into scaffolds. Scaffolds consist of overlapping contigs separated by gaps of known length. The new constraints placed on the orientation of the contigs allows for the placement of highly repeated sequences in the genome. If one end read has a repetitive sequence, as long as its mate pair is located within a contig, its placement is known. The remaining gaps between the contigs in the scaffolds can then be sequenced by a variety of methods, including PCR amplification followed by sequencing (for smaller gaps) and BAC cloning methods followed by sequencing for larger gaps.\n\nContig can also refer to the overlapping clones that form a physical map of a chromosome when the top-down or hierarchical sequencing strategy is used. In this sequencing method, a low-resolution map is made prior to sequencing in order to provide a framework to guide the later assembly of the sequence reads of the genome. This map identifies the relative positions and overlap of the clones used for sequencing. Sets of overlapping clones that form a contiguous stretch of DNA are called contigs; the minimum number of clones that form a contig that covers the entire chromosome comprise the tiling path that is used for sequencing. Once a tiling path has been selected, its component BACs are sheared into smaller fragments and sequenced. Contigs therefore provide the framework for hierarchical sequencing.\nThe assembly of a contig map involves several steps. First, DNA is sheared into larger (50–200kb) pieces, which are cloned into BACs or PACs to form a BAC library. Since these clones should cover the entire genome/chromosome, it is theoretically possible to assemble a contig of BACs that covers the entire chromosome. Reality, however, is not always ideal. Gaps often remain, and a scaffold—consisting of contigs and gaps—that covers the map region is often the first result. The gaps between contigs can be closed by various methods outlined below.\n\nBAC contigs are constructed by aligning BAC regions of known overlap via a variety of methods. One common strategy is to use sequence-tagged site (STS) content mapping to detect unique DNA sites in common between BACs. The degree of overlap is roughly estimated by the number of STS markers in common between two clones, with more markers in common signifying a greater overlap. Because this strategy provides only a very rough estimate of overlap, restriction digest fragment analysis, which provides a more precise measurement of clone overlap, is often used. In this strategy, clones are treated with one or two restriction enzymes and the resulting fragments separated by gel electrophoresis. If two clones, they will likely have restriction sites in common, and will thus share several fragments. Because the number of fragments in common and the length of these fragments is known (the length is judged by comparison to a size standard), the degree of overlap can be deduced to a high degree of precision.\n\nGaps often remain after initial BAC contig construction. These gaps occur if the Bacterial Artificial Chromosome (BAC) library screened has low complexity, meaning it does not contain a high number of STS or restriction sites, or if certain regions were less stable in cloning hosts and thus underrepresented in the library. If gaps between contigs remain after STS landmark mapping and restriction fingerprinting have been performed, the sequencing of contig ends can be used to close these gaps. This end-sequencing strategy essentially creates a novel STS with which to screen the other contigs. Alternatively, the end sequence of a contig can be used as a primer to primer walk across the gap.\n\n\n", "id": "331121", "title": "Contig"}
{"url": "https://en.wikipedia.org/wiki?curid=4929821", "text": "Radial spoke\n\nThe radial spoke is a multi-unit protein structure found in the axonemes of eukaryotic cilia and flagella. Although experiments have determined the importance of the radial spoke in the proper function of these organelles, its structure and mode of action remain poorly understood.\n\nRadial spokes are T-shaped structures present inside the axoneme. Each spoke consists of a \"head\" and a \"stalk,\" while each of these sub-structures is itself made up of many protein subunits. In all, the radial spoke is known to contain at least 17 different proteins, with 5 located in the head and at least 12 making up the stalk. The spoke stalk binds to the A-tubule of each microtubule outer doublet, and the spoke head faces in towards the center of the axoneme (see illustration at right).\n\nThe radial spoke is known to play a role in the mechanical movement of the flagellum/cilium. For example, mutant organisms lacking properly functioning radial spokes have flagella and cilia that are immotile. Radial spokes also influence the cilium \"waveform\"; that is, the exact bending pattern the cilium repeats.\n\nHow the radial spoke carries out this function is poorly understood. Radial spokes are believed to interact with both the central pair microtubules and the dynein arms, perhaps in a way that maintains the rhythmic activation of the dynein motors. For example, one of the radial spoke subunits, RSP3, is an anchor protein predicted to hold another protein called protein kinase A (PKA). PKA would theoretically then be able to activate/inactivate the adjacent dynein arms via its kinase activity.\n\nHowever, the identities and functions of the many radial spoke subunits are just beginning to be elucidated.\n", "id": "4929821", "title": "Radial spoke"}
{"url": "https://en.wikipedia.org/wiki?curid=1997415", "text": "Microtubule-associated protein\n\nIn cell biology, microtubule-associated proteins (MAPs) are proteins that interact with the microtubules of the cellular cytoskeleton.\n\nMAPs bind to the tubulin subunits that make up microtubules to regulate their stability. A large variety of MAPs have been identified in many different cell types, and they have been found to carry out a wide range of functions. These include both stabilizing and destabilizing microtubules, guiding microtubules towards specific cellular locations, cross-linking microtubules and mediating the interactions of microtubules with other proteins in the cell.\n\nWithin the cell, MAPs bind directly to the tubulin dimers of microtubules. This binding can occur with either polymerized or depolymerized tubulin, and in most cases leads to the stabilization of microtubule structure, further encouraging polymerization. Usually, it is the C-terminal domain of the MAP that interacts with tubulin, while the N-terminal domain can bind with cellular vesicles, intermediate filaments or other microtubules. MAP-microtubule binding is regulated through MAP phosphorylation. This is accomplished through the function of the microtubule-affinity-regulating-kinase (MARK) protein. Phosphorylation of the MAP by the MARK causes the MAP to detach from any bound microtubules. This detachment is usually associated with a destabilization of the microtubule causing it to fall apart. In this way the stabilization of microtubules by MAPs is regulated within the cell through phosphorylation.\n\nThe numerous identified MAPs have been largely divided into two categories: Type I including MAP1 proteins and type II including MAP2, MAP4 and tau proteins.\n\nMAP1a (MAP1A) and MAP1b (MAP1B) are the two major members of the MAP1 family. They bind to microtubules through charge interactions, a different mechanism to many other MAPs. While the C termini of these MAPs bind the microtubules, the N termini bind other parts of the cytoskeleton or the plasma membrane to control spacing of the microtubule within the cell. Members of the MAP1 family are found in the axons and dendrites of nerve cells.\n\nType II MAPs are found exclusively in nerve cells in mammals. These are the most well studied MAPs—MAP2 and tau (MAPT)—which participate in determining the structure of different parts of nerve cells, with MAP2 being found mostly in dendrites and tau in the axon. These proteins have a conserved C-terminal microtubule-binding domain and variable N-terminal domains projecting outwards, probably interacting with other proteins. MAP2 and tau stabilize microtubules, and thus shift the reaction kinetics in favor of addition of new subunits, accelerating microtubule growth. Both MAP2 and tau have been shown to stabilize microtubules by binding to the outer surface of the microtubule protofilaments. A single study has suggested that MAP2 and tau bind on the inner microtubule surface on the same site in tubulin monomers as the drug Taxol, which is used in treating cancer, but this study has not been confirmed. MAP2 binds in a cooperative manner, with many MAP2 proteins binding a single microtubule to promote stabilization. Tau has the additional function of facilitating bundling of microtubules within the nerve cell.\n\nThe function of tau has been linked to the neurological condition Alzheimer's disease. In the nervous tissue of Alzheimer's patients, tau forms abnormal aggregates. This aggregated tau is often severely modified, most commonly through hyperphosphorylation. As described above, phosphorylation of MAPs causes them to detach from microtubules. Thus, the hyperphosphorylation of tau leads to massive detachment, which in turn greatly reduces the stability of microtubules in nerve cells. This increase in microtubule instability may be one of the main causes of the symptoms of Alzheimer's disease.\n\nIn contrast to the MAPs described above, MAP4 (MAP4) is not confined to just nerve cells, but rather can be found in nearly all types of cells. Like MAP2 and tau, MAP4 is responsible for stabilization of microtubules. MAP4 has also been linked to the process of cell division.\n\nBesides the classic MAP groups, novel MAPs have been identified that bind the length of the microtubules. These include STOP (also known as MAP6), and ensconsin (also known as MAP7).\n\nIn addition, plus end tracking proteins, which bind to the very tip of growing microtubules, have also been identified. These include EB1, EB2, EB3, p150Glued, Dynamitin, Lis1, CLIP170, CLIP115, CLASP1, and CLASP2.\n\nAnother MAP whose function has been investigated during cell division is known as XMAP215 (the \"X\" stands for Xenopus). XMAP215 has generally been linked to microtubule stabilization. During mitosis the dynamic instability of microtubules has been observed to rise approximately tenfold. This is partly due to phosphorylation of XMAP215, which makes catastrophes (rapid depolymerization of microtubules) more likely. In this way the phosphorylation of MAPs plays a role in mitosis.\n\nThere are many other proteins which affect microtubule behavior, such as catastrophin, which destabilizes microtubules, katanin, which severs them, and a number of motor proteins that transport vesicles along them. Certain motor proteins were originally designated as MAPs before it was found that they utilized ATP hydrolysis to transport cargo. In general, all these proteins are not considered \"MAPs\" because they do not bind directly to tubulin monomers, a defining characteristic of MAPs. MAPs bind directly to microtubules to stabilize or destabilize them and link them to various cellular components including other microtubules.\n\n\n", "id": "1997415", "title": "Microtubule-associated protein"}
{"url": "https://en.wikipedia.org/wiki?curid=4914941", "text": "Sulfur assimilation\n\nSulfur is an essential element for growth and physiological functioning of plants. However, its content strongly varies between plant species and it ranges from 0.1 to 6% of the plants' dry weight.\n\nSulfates taken up by the roots are the major sulfur source for growth, though it has to be reduced to sulfide before it is further metabolized. Root plastids contain all sulfate reduction enzymes, but the reduction of sulfate to sulfide and its subsequent incorporation into cysteine predominantly takes place in the shoot, in the chloroplasts.\n\nCysteine is the precursor or reduced sulfur donor of most other organic sulfur compounds in plants. The predominant proportion of the organic sulfur is present in the protein fraction (up to 70% of total sulfur), as cysteine and methionine (two amino acids) residues.\n\nCysteine and methionine are highly significant in the structure, conformation and function of proteins. Plants contain a large variety of other organic sulfur compounds, as thiols (glutathione), sulfolipids and secondary sulfur compounds (alliins, glucosinolates, phytochelatins), which play an important role in physiology and protection against environmental stress and pests.\n\nSulfur compounds are also of great importance for food quality and for the production of phyto-pharmaceutics. Sulfur deficiency will result in the loss of plant production, fitness and resistance to environmental stress and pests.\n\nSulfate is taken up by the roots that have high affinity. The maximal sulfate uptake rate is generally already reached at sulfate levels of 0.1 mM and lower. The uptake of sulfate by the roots and its transport to the shoot is strictly controlled and it appears to be one of the primary regulatory sites of sulfur assimilation.\n\nSulfate is actively taken up across the plasma membrane of the root cells, subsequently loaded into the xylem vessels and transported to the shoot by the transpiration stream. The uptake and transport of sulfate is energy dependent (driven by a proton gradient generated by ATPases) through a proton/sulfate co-transport. In the shoot the sulfate is unloaded and transported to the chloroplasts where it is reduced. The remaining sulfate in plant tissue is predominantly present in the vacuole, since the concentration of sulfate in the cytoplasm is kept rather constant.\n\nDistinct sulfate transporter proteins mediate the uptake, transport and subcellular distribution of sulfate. According to their cellular and subcellular gene expression, and possible functioning the sulfate transporters gene family has been classified in up to 5 different groups. Some groups are expressed exclusively in the roots or shoots or expressed both in the roots and shoots.\n\n\nRegulation and expression of the majority of sulfate transporters are controlled by the sulfur nutritional status of the plants. Upon sulfate deprivation, the rapid decrease in root sulfate is regularly accompanied by a strongly enhanced expression of most sulfate transporter genes (up to 100-fold), accompanied by a substantially enhanced sulfate uptake capacity. The nature of these transporters is not yet fully solved, whether sulfate itself or metabolic products of the sulfur assimilation (O-acetylserine, cysteine, glutathione) act as signals in the regulation of sulfate uptake by the root and its transport to the shoot, and in the expression of the sulfate transporters involved.\n\nEven though root plastids contain all sulfate reduction enzymes, sulfate reduction predominantly takes place in the leaf chloroplasts. The reduction of sulfate to sulfide occurs in three steps. Sulfate needs to be activated to adenosine 5'-phosphosulfate (APS) prior to its reduction to sulfite.\n\nThe activation of sulfate is catalyzed by ATP sulfurylase, which affinity for sulfate is rather low (Km approximately 1 mM) and the in situ sulfate concentration in the chloroplast is most likely one of the limiting/regulatory steps in sulfur reduction. Subsequently, APS is reduced to sulfite, catalyzed by APS reductase with likely glutathione as reductant.\n\nThe latter reaction is assumed to be one of the primary regulation points in the sulfate reduction, since the activity of APS reductase is the lowest of the enzymes of the sulfate reduction pathway and it has a fast turnover rate. Sulfite is with high affinity reduced by sulfite reductase to sulfide with ferredoxin as a reductant. The remaining sulfate in plant tissue is transferred into the vacuole. The remobilization and redistribution of the vacuolar sulfate reserves appear to be rather slow and sulfur-deficient plants may still contain detectable levels of sulfate.\n\nSulfide is incorporated into cysteine, catalyzed by O-acetylserine (thiol)lyase, with O-acetylserine as substrate. The synthesis of O-acetylserine is catalyzed by serine acetyltransferase and together with O-acetylserine (thiol)lyase it is associated as enzyme complex named cysteine synthase.\n\nThe formation of cysteine is the direct coupling step between sulfur (sulfur metabolism) and nitrogen assimilation in plants. This differs from the process in yeast, where sulfide must be incorporated first in homocysteine then converted in two steps to cysteine.\n\nCysteine is sulfur donor for the synthesis of methionine, the major other sulfur-containing amino acid present in plants. This happens through the transsulfuration pathway and the methylation of homocysteine.\n\nBoth cysteine and methionine are sulfur-containing amino acids and are of great significance in the structure, conformation and function of proteins and enzymes, but high levels of these amino acids may also be present in seed storage proteins. The thiol groups of the cysteine residues in proteins can be oxidized resulting in disulfide bridges with other cysteine side chains (and form cystine) and/or linkage of polypeptides.\n\nDisulfide bridges (disulfide bonds) make an important contribution to the structure of proteins. The thiol groups are also of great importance in substrate binding of enzymes, in metal-sulfur clusters in proteins (e.g. ferredoxins) and in regulatory proteins (e.g. thioredoxins).\n\nGlutathione or its homologues, e.g. homoglutathione in Fabaceae; hydroxymethylglutathione in Poaceae are the major water-soluble non-protein thiol compounds present in plant tissue and account for 1-2% of the total sulfur. The content of glutathione in plant tissue ranges from 0.1 - 3 mM. Cysteine is the direct precursor for the synthesis of glutathione (and its homologues). First, γ-glutamylcysteine is synthesized from cysteine and glutamate catalyzed by gamma-glutamylcysteine synthetase. Second, glutathione is synthesized from γ-glutamylcysteine and glycine (in glutathione homologues, β-alanine or serine) catalyzed by glutathione synthetase. Both steps of the synthesis of glutathione are ATP dependent reactions. Glutathione is maintained in the reduced form by an NADPH-dependent glutathione reductase and the ratio of reduced glutathione (GSH) to oxidized glutathione (GSSG) generally exceeds a value of 7.\nGlutathione fulfils various roles in plant functioning. In sulfur metabolism it functions as reductant in the reduction of APS to sulfite. It is also the major transport form of reduced sulfur in plants. Roots likely largely depend for their reduced sulfur supply on shoot/root transfer of glutathione via the phloem, since the reduction of sulfur occurs predominantly in the chloroplast. Glutathione is directly involved in the reduction and assimilation of selenite into selenocysteine. Furthermore, glutathione is of great significance in the protection of plants against oxidative and environmental stress and it depresses/scavenges the formation of toxic reactive oxygen species, e.g. superoxide, hydrogen peroxide and lipid hydroperoxides. Glutathione functions as reductant in the enzymatic detoxification of reactive oxygen species in the glutathione-ascorbate cycle and as thiol buffer in the protection of proteins via direct reaction with reactive oxygen species or by the formation of mixed disulfides. The potential of glutathione as protectant is related to the pool size of glutathione, its redox state (GSH/GSSG ratio) and the activity of glutathione reductase. Glutathione is the precursor for the synthesis of phytochelatins, which are synthesized enzymatically by a constitutive phytochelatin synthase. The number of γ-glutamyl-cysteine residues in the phytochelatins may range from 2 - 5, sometimes up to 11. Despite the fact that the phytochelatins form complexes which a few heavy metals, viz. cadmium, it is assumed that these compounds play a role in heavy metal homeostasis and detoxification by buffering of the cytoplasmatic concentration of essential heavy metals. Glutathione is also involved in the detoxification of xenobiotics, compounds without direct nutritional value or significance in metabolism, which at too high levels may negatively affect plant functioning. Xenobiotics may be detoxified in conjugation reactions with glutathione catalyzed by glutathione S-transferase, which activity is constitutive; different xenobiotics may induce distinct isoforms of the enzyme. Glutathione S-transferases have great significance in herbicide detoxification and tolerance in agriculture and their induction by herbicide antidotes ('safeners') is the decisive step for the induction of herbicide tolerance in many crop plants. Under natural conditions glutathione S-transferases are assumed to have significance in the detoxification of lipid hydroperoxides, in the conjugation of endogenous metabolites, hormones and DNA degradation products, and in the transport of flavonoids.\n\nSulfolipids are sulfur containing lipids. Sulfoquinovosyl diacylglycerols are the predominant sulfolipids present in plants. In leaves its content comprises up to 3 - 6% of the total sulfur present. This sulfolipid is present in plastid membranes and likely is involved in chloroplast functioning. The route of biosynthesis and physiological function of sulfoquinovosyl diacylglycerol is still under investigation. From recent studies it is evident that sulfite it the likely sulfur precursor for the formation of the sulfoquinovose group of this lipid.\n\nBrassica species contain glucosinolates, which are sulfur-containing secondary compounds. Glucosinolates are composed of a β-thioglucose moiety, a sulfonated oxime and a side chain. The synthesis of glucosinolates starts with the oxidation of the parent amino acid to an aldoxime, followed by the addition of a thiol group (through conjugation with glutathione) to produce thiohydroximate. The transfer of a glucose and a sulfate moiety completes the formation of the glucosinolates.\n\nThe physiological significance of glucosinolates is still ambiguous, though they are considered to function as sink compounds in situations of sulfur excess. Upon tissue disruption glucosinolates are enzymatically degraded by myrosinase and may yield a variety of biologically active products such as isothiocyanates, thiocyanates, nitriles and oxazolidine-2-thiones. The glucosinolate-myrosinase system is assumed to play a role in plant-herbivore and plant-pathogen interactions.\n\nFurthermore, glucosinolates are responsible for the flavor properties of Brassicaceae and recently \nhave received attention in view of their potential anti-carcinogenic properties.\nAllium species contain γ-glutamylpeptides and alliins (S-alk(en)yl cysteine sulfoxides). The content of these sulfur-containing secondary compounds strongly depends on stage of development of the plant, temperature, water availability and the level of nitrogen and sulfur nutrition. In onion bulbs their content may account for up to 80% of the organic sulfur fraction. Less is known about the content of secondary sulfur compounds in the seedling stage of the plant.\n\nIt is assumed that alliins are predominantly synthesized in the leaves, from where they are subsequently transferred to the attached bulb scale. The biosynthetic pathways of synthesis of γ-glutamylpeptides and alliins are still ambiguous. γ-Glutamylpeptides can be formed from cysteine (via γ-glutamylcysteine or glutathione) and can be metabolized into the corresponding alliins via oxidation and subsequent hydrolyzation by γ-glutamyl transpeptidases.\n\nHowever, other possible routes of the synthesis of γ-glutamylpeptides and alliins may not be excluded. Alliins and γ-glutamylpeptides are known to have therapeutic utility and might have potential value as phytopharmaceutics. The alliins and their breakdown products (e.g. allicin) are the flavor precursors for the odor and taste of species. Flavor is only released when plant cells are disrupted and the enzyme alliinase from the vacuole is able to degrade the alliins, yielding a wide variety of volatile and non-volatile sulfur-containing compounds. The physiological function of γ-glutamylpeptides and alliins is rather unclear.\n\nThe rapid economic growth, industrialization and urbanization are associated with a strong increase in energy demand and emissions of air pollutants including sulfur dioxide (see also acid rain) and hydrogen sulfide, which may affect plant metabolism. Sulfur gases are potentially phytotoxic, however, they may also be metabolized and used as sulfur source and even be beneficial if the sulfur fertilization of the roots is not sufficient.\n\nPlant shoots form a sink for atmospheric sulfur gases, which can directly be taken up by the foliage (dry deposition). The foliar uptake of sulfur dioxide is generally directly dependent on the degree of opening of the stomates, since the internal resistance to this gas is low. Sulfur is highly soluble in the apoplastic water of the mesophyll, where it dissociates under formation of bisulfite and sulfite.\n\nSulfite may directly enter the sulfur reduction pathway and be reduced to sulfide, incorporated into cysteine, and subsequently into other sulfur compounds. Sulfite may also be oxidized to sulfate, extra- and intracellularly by peroxidases or non-enzymatically catalyzed by metal ions or superoxide radicals and subsequently reduced and assimilated again. Excessive sulfate is transferred into the vacuole; enhanced foliar sulfate levels are characteristic for exposed plants.\nThe foliar uptake of hydrogen sulfide appears to be directly dependent on the rate of its metabolism into cysteine and subsequently into other sulfur compounds. There is strong evidence that O-acetyl-serine (thiol)lyase is directly responsible for the active fixation of atmospheric hydrogen sulfide by plants.\n\nPlants are able to transfer from sulfate to foliar absorbed atmospheric sulfur as sulfur source and levels of 60 ppb or higher appear to be sufficient to cover the sulfur requirement of plants. There is an interaction between atmospheric and pedospheric sulfur utilization. For instance, hydrogen sulfide exposure may result in a decreased activity of APS reductase and a depressed sulfate uptake.\n\n\n", "id": "4914941", "title": "Sulfur assimilation"}
{"url": "https://en.wikipedia.org/wiki?curid=5286471", "text": "Treadmilling\n\nTreadmilling is a phenomenon observed in many cellular cytoskeletal filaments, especially in actin filaments and microtubules. It occurs when one end of a filament grows in length while the other end shrinks resulting in a section of filament seemingly \"moving\" across a stratum or the cytosol. This is due to the constant removal of the protein subunits from these filaments at one end of the filament while protein subunits are constantly added at the other end.\n\nThe cytoskeleton is a highly dynamic part of a cell and cytoskeletal filaments constantly grow and shrink through addition and removal of subunits. Directed crawling motion of cells such as macrophages relies on directed growth of actin filaments at the cell front (\"leading edge\").\n\nThe two ends of an actin filament differ in their dynamics of subunit addition and removal. They are thus referred to as the \"plus end\" (with faster dynamics, also called barbed end) and the \"minus end\" (with slower dynamics, also called pointed end). This difference results from the fact that subunit addition at the minus end requires a conformational change of the subunits. Note that each subunit is structurally polar and has to attach to the filament in a particular orientation. As a consequence, the actin filaments are also structurally polar.\n\nElongating the actin filament occurs when free-actin (G-actin) bound to ATP associates with the filament. Under physiological conditions, it is easier for G-actin to associate at the positive end of the filament, and harder at the negative end. However, it is possible to elongate the filament at either end. Association of G-actin into F-actin is regulated by the critical concentration outlined below. Actin polymerization can further be regulated by profilin and cofilin. Cofilin functions by binding to ADP-actin on the negative end of the filament, destabilizing it, and inducing depolymerization. Profilin induces ATP binding to G-actin so that it can be incorporated onto the positive end of the filament.\n\nTwo main theories exist on microtubule movement within the cell: dynamic instability and treadmilling. Dynamic instability occurs when the microtubule assembles and disassembles at one end only, while treadmilling occurs when one end polymerizes while the other end disassembles. However, the biological significance of treadmilling \"in vivo\" is not well characterized. This is due to the fact that within a living cell, many microtubules are tightly anchored at one end of the filament. Some research has suggested that the differences in critical concentration between the positive and the negative end may be a way for the cell to prevent unwanted polymerization events.\n\nThe critical concentration is the concentration of either G-actin (actin) or the alpha,beta- tubulin complex (microtubules) at which the end will remain in an equilibrium state with no net growth or shrinkage. What determines whether the ends grow or shrink is entirely dependent on the cytosolic concentration of available monomer subunits in the surrounding area. Critical concentration differs from the positive (C) and the negative end (C), and under normal physiological conditions, the critical concentration is lower at the positive end than the negative end. Examples in how the cytosolic concentration relates to the critical concentration and polymerization are as follows: \n\nNote that the cytosolic concentration of the monomer subunit between the C and C ends is what is defined as treadmilling in which there is growth at the plus end, and shrinking on the minus end.\n\nWhile treadmilling may occur at different speeds at both ends, there is a concentration at which the speed of growth at the (+) end is equal to the rate of shrinkage at the (-) end. This is deemed steady-state treadmilling in which the net length of the treadmilling filament remains unchanged.\n\n", "id": "5286471", "title": "Treadmilling"}
{"url": "https://en.wikipedia.org/wiki?curid=5347841", "text": "GUS reporter system\n\nThe GUS reporter system (\"GUS\": β-glucuronidase) is a reporter gene system, particularly useful in plant molecular biology and microbiology. Several kinds of GUS reporter gene assay are available, depending on the substrate used. The term GUS staining refers to the most common of these, a histochemical technique.\n\nThe purpose of this technique is to analyze the activity of a promoter (in terms of expression of a gene under that promoter) either in a quantitative way or through visualization of its activity in different tissues. The technique is based on β-glucuronidase, an enzyme from the bacterium \"Escherichia coli\"; this enzyme, when incubated with some specific colorless or non-fluorescent substrates, can transform them into coloured or fluorescent products.\n\nThere are different possible glucuronides that can be used as substrates for the β-glucuronidase, depending on the type of detection needed (histochemical, spectrophotometrical, fluorimetrical). The most common substrate for GUS histochemical staining is 5-bromo-4-chloro-3-indolyl glucuronide (X-Gluc): the product of the reaction is in this case a clear blue color. Other common substrates are p-nitrophenyl β-D-glucuronide for the spectrophotometric assay and 4-methylumbelliferyl-beta-D-glucuronide (MUG) for the fluorimetrical assay.\n\nThe system was originally developed by Richard Anthony Jefferson during his Ph.D. at the University of Colorado at Boulder. He adapted the technique for the use with plants as he worked in the \"Plant Breeding Institute\" of Cambridge, between 1985 and 1987. Since then thousands of labs have used the system, making it one of the most widely used tools in plant molecular biology, as underlined by over 6000 citations in scientific literature.\n\nAn organism is suitable for a GUS assay if it has no β-glucuronidase or if the activity is very low (\"background\" activity). For this reason the assay is not useful in most vertebrates and many molluscs. Since there is no detectable GUS activity in higher plants, mosses, algae, ferns, fungi and most bacteria, the assay is perfectly suited for these organisms. \nThus it is used widely in plant science.\n\nThe GUS system is not the only available gene reporter system for the analysis of promoter activity. Other competing systems are based on e.g. luciferase, GFP, beta-galactosidase, chloramphenicol acetyltransferase (CAT), alkaline phosphatase. The use of one or the other system is mainly dependent on the organism of interest.\n\nThe GUS assay, as well as other reporter gene systems, can be used for other kinds of studies other than the classical promoter activity assay. Reporter systems have been used for the determination of the efficiency of gene delivery systems, the intracellular localization of a gene product, the detection of protein-protein or protein-DNA interactions, the efficiency of translation initiation signals and the success of molecular cloning efforts.\n", "id": "5347841", "title": "GUS reporter system"}
{"url": "https://en.wikipedia.org/wiki?curid=1498669", "text": "Zinc finger inhibitor\n\nZinc finger inhibitors, or zinc ejectors, are substances or compounds that interact adversely with zinc fingers and cause them to release their zinc from its binding site, disrupting the conformation of the polypeptide chain and rendering the zinc fingers ineffective, thereby preventing them from performing their associated cellular functions. This is typically accomplished through chelation of the zinc binding site. As zinc fingers are known to be involved in m-RNA regulation, reverse transcription, protection of synthesized viral DNA, transcription inhibition, and initial integration processes, prevention of zinc finger function can have drastic effects on the function of the cell or virus.\n\nZinc finger inhibitors are typically used to combat HIV. HIV treatments usually rely on targeting reverse transcriptases and proteases. However, these methods are proving to be ineffective due to the development of resistant strains of the virus or due to the stoppage of the treatment. This method of using zinc finger inhibitors to target and destabilize zinc fingers represents a new method of fighting HIV. Other viruses such as SARS, polio, Ebola, measles, human coxsackie, Dengue, rabies, human hepatitis, human parainfluenza and human respiratory syncytical have similar zinc finger motifs and could potentially benefit from zinc finger inhibitor technology.\n\nZinc ejectors were patented in 2008 and some have entered Phase I/II trials as a HIV drug.\n\nThe HIV-1 nucleocapsid protein 7 (NCp7) is the protein targeted by zinc ejectors. NCp7 is initially formed as part of the gag polypeptide and follows a gag-knuckle zinc finger conformation. In its lifetime, NCp7 facilitates the unwinding of tRNA, acts as a primer for reverse transcription, chaperones nucleic acids within the capsid of HIV-1, helps integrate the viral RNA into budding virions, and is intimately involved in the replication of HIV-1 in both the early phase and late phase. These processes play critical roles in the replication of HIV-1 thus making NCp7 a prime target for drugs seeking to contravene the replication process.\n\nNCp7 is a 55-amino acid protein that is highly basic and consists of two gag-knuckle motifs. These motifs contain two peptide units of Cys-X2-Cys-X4-His-X4-Cys (CCHC), where the X represents a substituted amino acid, that make up the zinc (II) ion binding sites. The binding of zinc (II) in the CCHC binding site is necessary for the domain to be functional and for the stabilization of the conformation of the structure, allowing the NCp7 to carry out the processes required for HIV replication. Since the CCHC binding site is mutation resistant and involved in the replication of HIV-1, it makes a prime candidate for the prevention of HIV through zinc ejectors. By inhibiting the function of NCp7, the viral replication is affected and a non-functional virus that is unable to infect its host is produced.\n\nAzodicarbonamide (ADA) was the first zinc ejector to go into clinical trial for treatment of HIV. ADA inhibits HIV by electophilically attacking the sulfur atoms of the zinc coordinated cysteine. This electrophilic interaction destabilizes the zinc binding site making it easier for the zinc ion to be withdrawn due to the new arrangement of bonds. The binding site then performs a disulfide exchange, forming new intermolecular disulfide bonds, and rearrangement occurs placing the zinc finger in a conformation that inhibits its function.\n\n3-nitrosobenzamide (NOBA) and 6-nitroso-1,2-benzopyrone (NOBP) were the first compounds to demonstrate an ability to inhibit infection of HIV by ejecting zinc from NCp7. In the same manner as ADA, the compounds interact with an 18-residue polypeptide on the N terminal zinc knuckle region of the HIV nucleocapsid protein which causes ejection of the zinc from the region by covalently modifying the cysteine residues. Studies suggest that NOBA and NOBP were able to inhibit HIV-1 infection by inhibiting reverse transcription without an apparent impairment of reverse transcriptase. This reiterates the role of NCp7 in reverse transcription.\n\nDIBAs act similarly to ADA, NOBA and NOBP. They react with the cysteine residues on the zinc finger of the NCp7 and cause a covalent conformation change which ejects the zinc from the zinc finger domain. Though DIBAs initially seemed to be promising antiviral candidates, there were clinical issues with their stability. DIBAs tend to cyclize into benzisothiazolones which do not have the same potency when used to combat retroviruses as the original compound. Additionally, glutathione can reduce the disulfide bonds in DIBA thereby restricting its function \"in vitro\".\n\nMercaptobenzamide prodrugs transfer acyl group to 36th cysteine residue of NCp7 and the acyl group then migrates to neighboring lysine residue that triggers the ejection of Zn. The mercaptobenzamide and its corresponding prodrug yielded additivity to synergy when combined with known HIV drugs. Moreover they showed no cytotoxicity.\n\nN-[2-(5-pyridiniovaleroylthio)benzoyl]sulfacetamide bromide (referred to as compound 45) is a pyridinioalkanoyl thiolester that can function as a zinc ejector. Once activated with silver, compound 45 uses its pyridinioalkanoyl groups to covalently modify NCp7, specifically altering cysteines 36 and 49 on the carboxyl-terminal zinc finger. It ejects the zinc from the zinc binding sites in a two steps. The zinc in the carboxyl-terminal zinc finger is released first, followed by the ejection of zinc from the amino-terminal zinc finger.\n\nBis-Thiadizolbenzene-1,2-diamine (NV038) is one of the newer zinc ejectors. NV038 is found to effect the function of the zinc finger after the virus has entered the cell but before reverse transcription is completed. NV038, like other zinc ejector compounds, chelates zinc to remove it from its binding site. However, it is thought to act through a different mechanism than many of the other zinc ejectors due to its structural features. Its structure would not readily allow thiol-disulfide interchange or acyl transfer to cysteine. Instead, NV038 is believed to react with the zinc using its two carbonyl oxygens found in the esters.\n\nThere was concern over whether these zinc ejectors were safe to use due to the uncertainty as to whether the zinc ejectors had sufficient selectivity to target only the CCHC binding sites of the zinc fingers in NCp7. Zinc finger domains are not unique to HIV but rather are ubiquitous in cell biology, and play important roles in many processes such as cellular replication, protein-protein interactions, and DNA replication. If these zinc ejectors unintentionally bind to the wrong zinc finger domains they have the potential to adversely affect other cellular functions that could be essential for proper bodily functions.\n\nExperimentation and modeling of the selectivity of DIBA-1, ADA, and other zinc ejectors found that this safety concern was essentially unfounded. All zinc ejectors were found to effectively combat the replication of HIV at concentrations that did not exhibit cytotoxic effects denoting that they specifically targeted NCp7 and did not target other zinc fingers. It is thought that factors such as ligand binding affinity, ligand reactive proximity, and the general saddleshape necessary for the compound to fit into the binding pocket all play a role in the selectivity shown by zinc ejectors.\n", "id": "1498669", "title": "Zinc finger inhibitor"}
{"url": "https://en.wikipedia.org/wiki?curid=1075046", "text": "Interactome\n\nIn molecular biology, an interactome is the whole set of molecular interactions in a particular cell. The term specifically refers to physical interactions among molecules (such as those among proteins, also known as protein–protein interactions, PPIs) but can also describe sets of indirect interactions among genes (genetic interactions). The interactomes based on PPIs should be associated to the proteome of the corresponding species in order to provide a global view (\"omic\") of all the possible molecular interactions that a protein can present. A recent compendium of interactomes can be obtained in the resource: APID interactomes.\nThe word \"interactome\" was originally coined in 1999 by a group of French scientists headed by Bernard Jacq. Mathematically, interactomes are generally displayed as graphs. Though interactomes may be described as biological networks, they should not be confused with other networks such as neural networks or food webs.\n\nMolecular interactions can occur between molecules belonging to different biochemical families (proteins, nucleic acids, lipids, carbohydrates, etc.) and also within a given family. Whenever such molecules are connected by physical interactions, they form molecular interaction networks that are generally classified by the nature of the compounds involved. Most commonly, \"interactome\" refers to \"protein–protein interaction\" (PPI) network (PIN) or subsets thereof. For instance, the Sirt-1 protein interactome and Sirt family second order interactome is the network involving Sirt-1 and its directly interacting proteins where as second order interactome illustrates interactions up to second order of neighbors (Neighbors of neighbors). Another extensively studied type of interactome is the protein–DNA interactome, also called a \"gene-regulatory network\", a network formed by transcription factors, chromatin regulatory proteins, and their target genes. Even \"metabolic networks\" can be considered as molecular interaction networks: metabolites, i.e. chemical compounds in a cell, are converted into each other by enzymes, which have to bind their substrates physically.\n\nIn fact, all interactome types are interconnected. For instance, protein interactomes contain many enzymes which in turn form biochemical networks. Similarly, gene regulatory networks overlap substantially with protein interaction networks and signaling networks.\n\nIt has been suggested that the size of an organism's interactome correlates better than genome size with the biological complexity of the organism. Although protein–protein interaction maps containing several thousand binary interactions are now available for several species, none of them is presently complete and the size of interactomes is still a matter of debate.\n\nThe yeast interactome, i.e. all protein–protein interactions among proteins of \"Saccharomyces cerevisiae\", has been estimated to contain between 10,000 and 30,000 interactions. A reasonable estimate may be on the order of 20,000 interactions. Larger estimates often include indirect or predicted interactions, often from affinity purification/mass spectrometry (AP/MS) studies.\n\nGenes interact in the sense that they affect each other's function. For instance, a mutation may be harmless, but when it is combined with another mutation, the combination may turn out to be lethal. Such genes are said to \"interact genetically\". Genes that are connected in such a way form \"genetic interaction networks\". Some of the goals of these networks are: develop a functional map of a cell's processes, drug target identification, and to predict the function of uncharacterized genes.\n\nIn 2010, the most \"complete\" gene interactome produced to date was compiled from about 5.4 million two-gene comparisons to describe \"the interaction profiles for ~75% of all genes in the budding yeast\", with ~170,000 gene interactions. The genes were grouped based on similar function so as to build a functional map of the cell's processes. Using this method the study was able to predict known gene functions better than any other genome-scale data set as well as adding functional information for genes that hadn't been previously described. From this model genetic interactions can be observed at multiple scales which will assist in the study of concepts such as gene conservation. Some of the observations made from this study are that there were twice as many negative as positive interactions, negative interactions were more informative than positive interactions, and genes with more connections were more likely to result in lethality when disrupted.\n\nInteractomics is a discipline at the intersection of bioinformatics and biology that deals with studying both the interactions and the consequences of those interactions between and among proteins, and other molecules within a cell. Interactomics thus aims to compare such networks of interactions (i.e., interactomes) between and within species in order to find how the traits of such networks are either preserved or varied.\n\nInteractomics is an example of \"top-down\" systems biology, which takes an overhead, as well as overall, view of a biosystem or organism. Large sets of genome-wide and proteomic data are collected, and correlations between different molecules are inferred. From the data new hypotheses are formulated about feedbacks between these molecules. These hypotheses can then be tested by new experiments.\n\nThe study of interactomes is called interactomics. The basic unit of a protein network is the protein–protein interaction (PPI). While there are numerous methods to study PPIs, there are relatively few that have been used on a large scale to map whole interactomes.\n\nThe yeast two hybrid system (Y2H) is suited to explore the binary interactions among two proteins at a time. Affinity purification and subsequent mass spectrometry is suited to identify a protein complex. Both methods can be used in a high-throughput (HTP) fashion. Yeast two hybrid screens allow false positive interactions between proteins that are never expressed in the same time and place; affinity capture mass spectrometry does not have this drawback, and is the current gold standard. Yeast two-hybrid data better indicates non-specific tendencies towards sticky interactions rather while affinity capture mass spectrometry better indicates functional in vivo protein–protein interactions.<ref name=\"10.3389/fnmol.2014.00058\"></ref>\n\nOnce an interactome has been created, there are numerous ways to analyze its properties. However, there are two important goals of such analyses. First, scientists try to elucidate the systems properties of interactomes, e.g. the topology of its interactions. Second, studies may focus on individual proteins and their role in the network. Such analyses are mainly carried out using bioinformatics methods and include the following, among many others:\n\nFirst, the coverage and quality of an interactome has to be evaluated. Interactomes are never complete, given the limitations of experimental methods. For instance, it has been estimated that typical Y2H screens detect only 25% or so of all interactions in an interactome. The coverage of an interactome can be assessed by comparing it to benchmarks of well-known interactions that have been found and validated by independent assays. Other methods filter out false positives calculating the similarity of known annotations of the proteins involved or define a likelihood of interaction using the subcellular localization of these proteins.\n\nUsing experimental data as a starting point, \"homology transfer\" is one way to predict interactomes. Here, PPIs from one organism are used to predict interactions among homologous proteins in another organism (\"\"interologs\"\"). However, this approach has certain limitations, primarily because the source data may not be reliable (e.g. contain false positives and false negatives). In addition, proteins and their interactions change during evolution and thus may have been lost or gained. Nevertheless, numerous interactomes have been predicted, e.g. that of \"Bacillus licheniformis\".\n\nSome algorithms use experimental evidence on structural complexes, the atomic details of binding interfaces and produce detailed atomic models of protein–protein complexes as well as other protein–molecule interactions. Other algorithms use only sequence information, thereby creating unbiased complete networks of interaction with many mistakes.\n\nSome methods use machine learning to distinguish how interacting protein pairs differ from non-interacting protein pairs in terms of pairwise features such as cellular colocalization, gene co-expression, how closely located on a DNA are the genes that encode the two proteins, and so on. Random Forest has been found to be most-effective machine learning method for protein interaction prediction. Such methods have been applied for discovering protein interactions on human interactome, specifically the interactome of Membrane proteins and the interactome of Schizophrenia-associated proteins.\n\nSome efforts have been made to extract systematically interaction networks directly from the scientific literature. Such approaches range in terms of complexity from simple co-occurrence statistics of entities that are mentioned together in the same context (e.g. sentence) to sophisticated natural language processing and machine learning methods for detecting interaction relationships.\n\nProtein interaction networks have been used to predict the function of proteins of unknown functions. This is usually based on the assumption that uncharacterized proteins have similar functions as their interacting proteins (\"guilt by association\"). For example, YbeB, a protein of unknown function was found to interact with ribosomal proteins and later shown to be involved in translation. Although such predictions may be based on single interactions, usually several interactions are found. Thus, the whole network of interactions can be used to predict protein functions, given that certain functions are usually enriched among the interactors.\n\nThe \"topology\" of an interactome makes certain predictions how a network reacts to the perturbation (e.g. removal) of nodes (proteins) or edges (interactions). Such perturbations can be caused by mutations of genes, and thus their proteins, and a network reaction can manifest as a disease. A network analysis can identified drug targets and biomarkers of diseases.\n\nInteraction networks can be analyzed using the tools of graph theory. Network properties include the degree distribution, clustering coefficients, betweenness centrality, and many others. The distribution of properties among the proteins of an interactome has revealed that the interactome networks often have scale-free topology where functional modules within a network indicate specialized subnetworks. Such modules can be functional, as in a signaling pathway, or structural, as in a protein complex. In fact, it is a formidable task to identify protein complexes in an interactome, given that a network on its own does not directly reveal the presence of a stable complex.\n\nViral protein interactomes consist of interactions among viral or phage proteins. They were among the first interactome projects as their genomes are small and all proteins can be analyzed with limited resources. Viral interactomes are connected to their host interactomes, forming virus-host interaction networks. Some published virus interactomes include\n\nBacteriophage\n\nThe lambda and VZV interactomes are not only relevant for the biology of these viruses but also for technical reasons: they were the first interactomes that were mapped with multiple Y2H vectors, proving an improved strategy to investigate interactomes more completely than previous attempts have shown.\n\nHuman (mammalian) viruses\n\nRelatively few bacteria have been comprehensively studied for their protein–protein interactions. However, none of these interactomes are complete in the sense that they captured all interactions. In fact, it has been estimated that none of them covers more than 20% or 30% of all interactions, primarily because most of these studies have only employed a single method, all of which discover only a subset of interactions. Among the published bacterial interactomes (including partial ones) are\n\nThe \"E. coli\" and \"Mycoplasma\" interactomes have been analyzed using large-scale protein complex affinity purification and mass spectrometry (AP/MS), hence it is not easily possible to infer direct interactions. The others have used extensive yeast two-hybrid (Y2H) screens. The \"Mycobacterium tuberculosis\" interactome has been analyzed using a bacterial two-hybrid screen (B2H).\n\nNote that numerous additional interactomes have been predicted using computational methods (see section above).\n\nThere have been several efforts to map eukaryotic interactomes through HTP methods. While no biological interactomes have been fully characterized, over 90% of proteins in \"Saccharomyces cerevisiae\" have been screened and their interactions characterized, making it the best-characterized interactome. Species whose interactomes have been studied in some detail include \nRecently, the pathogen-host interactomes of Hepatitis C Virus/Human (2008), Epstein Barr virus/Human (2008), Influenza virus/Human (2009) were delineated through HTP to identify essential molecular components for pathogens and for their host's immune system.\n\nAs described above, PPIs and thus whole interactomes can be predicted. While the reliability of these predictions is debatable, they are providing hypotheses that can be tested experimentally. Interactomes have been predicted for a number of species, e.g.\n\nProtein interaction networks can be analyzed with the same tool as other networks. In fact, they share many properties with biological or social networks. Some of the main characteristics are as follows.\n\nThe degree distribution describes the number of proteins that have a certain number of connections. Most protein interaction networks show a scale-free (power law) degree distribution where the connectivity distribution P(k) ~ k with k being the degree. This relationship can also be seen as a straight line on a log-log plot since, the above equation is equal to log(P(k)) ~ —y•log(k). One characteristic of such distributions is that there are many proteins with few interactions and few proteins that have many interactions, the latter being called \"hubs\".\n\nHighly connected nodes (proteins) are called hubs. Han et al. have coined the term \"party hub\" for hubs whose expression is correlated with its interaction partners. Party hubs also connect proteins within functional modules such as protein complexes. In contrast, \"date hubs\" do not exhibit such a correlation and appear to connect different functional modules. Party hubs are found predominantly in AP/MS data sets, whereas date hubs are found predominantly in binary interactome network maps. Note that the validity of the date hub/party hub distinction was disputed. Party hubs generally consist of multi-interface proteins whereas date hubs are more frequently single-interaction interface proteins. Consistent with a role for date-hubs in connecting different processes, in yeast the number of binary interactions of a given protein is correlated to the number of phenotypes observed for the corresponding mutant gene in different physiological conditions.\n\nNodes involved in the same biochemical process are highly interconnected.\n\nThe evolution of interactome complexity is delineated in a study published in \"Nature\". In this study it is first noted that the boundaries between prokaryotes, unicellular eukaryotes and multicellular eukaryotes are accompanied by orders-of-magnitude reductions in effective population size, with concurrent amplifications of the effects of random genetic drift. The resultant decline in the efficiency of selection seems to be sufficient to influence a wide range of attributes at the genomic level in a nonadaptive manner. The Nature study shows that the variation in the power of random genetic drift is also capable of influencing phylogenetic diversity at the subcellular and cellular levels. Thus, population size would have to be considered as a potential determinant of the mechanistic pathways underlying long-term phenotypic evolution. In the study it is further shown that a phylogenetically broad inverse relation exists between the power of drift and the structural integrity of protein subunits. Thus, the accumulation of mildly deleterious mutations in populations of small size induces secondary selection for protein–protein interactions that stabilize key gene functions, mitigating the structural degradation promoted by inefficient selection. By this means, the complex protein architectures and interactions essential to the genesis of phenotypic diversity may initially emerge by non-adaptive mechanisms.\n\nKiemer and Cesareni raise the following concerns with the state (circa 2007) of the field especially with the comparative interactomic: The experimental procedures associated with the field are error prone leading to \"noisy results\". This leads to 30% of all reported interactions being artifacts. In fact, two groups using the same techniques on the same organism found less than 30% interactions in common. However, some authors have argued that such non-reproducibility results from the extraordinary sensitivity of various methods to small experimental variation. For instance, identical conditions in Y2H assays result in very different interactions when different Y2H vectors are used.\n\nTechniques may be biased, i.e. the technique determines which interactions are found. In fact, any method has built in biases, especially protein methods. Because every protein is different no method can capture the properties of each protein. For instance, most analytical methods that work fine with soluble proteins deal poorly with membrane proteins. This is also true for Y2H and AP/MS technologies.\n\nInteractomes are not nearly complete with perhaps the exception of \"S. cerevisiae.\" This is not really a criticism as any scientific area is \"incomplete\" initially until the methodologies have been improved. Interactomics in 2015 is where genome sequencing was in the late 1990s, given that only a few interactome datasets are available (see table above).\n\nWhile genomes are stable, interactomes may vary between tissues, cell types, and developmental stages. Again, this is not a criticism, but rather a description of the challenges in the field.\n\nIt is difficult to match evolutionarily related proteins in distantly related species. While homologous DNA sequences can be found relatively easily, it is much more difficult to predict homologous interactions (\"interologs\") because the homologs of two interacting proteins do not need to interact. For instance, even within a proteome two proteins may interact but their paralogs may not.\n\nEach protein–protein interactome may represent only a partial sample of potential interactions, even when a supposedly definitive version is published in a scientific journal. Additional factors may have roles in protein interactions that have yet to be incorporated in interactomes. The binding strength of the various protein interactors, microenvironmental factors, sensitivity to various procedures, and the physiological state of the cell all impact protein–protein interactions, yet are usually not accounted for in interactome studies.\n\n\n\n\n\n", "id": "1075046", "title": "Interactome"}
{"url": "https://en.wikipedia.org/wiki?curid=2129346", "text": "TILLING (molecular biology)\n\nTILLING (Targeting Induced Local Lesions in Genomes) is a method in molecular biology that allows directed identification of mutations in a specific gene. TILLING was introduced in 2000, using the model plant \"Arabidopsis thaliana\". TILLING has since been used as a reverse genetics method in other organisms such as zebrafish, corn, wheat, rice, soybean, tomato and lettuce.\n\nThe method combines a standard and efficient technique of mutagenesis using a chemical mutagen such as ethyl methanesulfonate (EMS) with a sensitive DNA screening-technique that identifies single base mutations (also called point mutations) in a target gene. The TILLING method relies on the formation of DNA heteroduplexes that are formed when multiple alleles are amplified by PCR and are then heated and slowly cooled. A “bubble” forms at the mismatch of the two DNA strands, which is then cleaved by a single stranded nucleases. The products are then separated by size on several different platforms (see below).\n\nMismatches may be due to induced mutation, heterozygosity within an individual, or natural variation between individuals.\n\nEcoTILLING is a method that uses TILLING techniques to look for natural mutations in individuals, usually for population genetics analysis. DEcoTILLING is a modification of TILLING and EcoTILLING which uses an inexpensive method to identify fragments. Since the advent of NGS sequencing technologies, TILLING-by-sequencing has been developed based on Illumina sequencing of target genes amplified from multidimensionally pooled templates to identify possible single-nucleotide changes.\n\nThere are several sources for single strand nucleases. The first widely used enzyme was mung bean nuclease, but this nuclease has been shown to have high non-specific activity, and only works at low pH, which can degrade PCR products and dye labeled primers. The original source for single strand nuclease was from CEL1, or CJE (celery juice extract), but other products have entered the market including Frontier Genomics’ SNiPerase enzymes, which have been optimized for use on platforms that use labeled and unlabeled PCR products (see next section). Transgenomic isolated the single strand nuclease protein and sells it as a recombinant form. The advantage of the recombinant form is that unlike the enzyme mixtures, it does not contain non-specific nuclease activity, which can degrade the dyes on the PCR primers. The disadvantage is a substantially higher cost.\n\nThe first paper describing TILLING used HPLC to identify mutations (McCallum et al., 2000a). The method was made more high throughput by using the restriction enzyme Cel-I combined with the LICOR gel based system to identify mutations (Colbert et al.,2001). Advantages to using this system are that mutation sites can be easily confirmed and differentiated from noise. This is because different colored dyes can be used for the forward and reverse primers. Once the cleavage products have been run on a gel, it can be viewed in separate channels, and much like an RFLP, the fragment sizes within a lane in each channel should add up to the full length product size. Advantages to the LICOR system are separation of large fragments (~ 2kb), high sample throughput (96 samples loaded on paper combs), and freeware to identify the mutations (GelBuddy). Drawbacks to the LICOR system is having to pour slab gels and long run times (~4 hours). TILLING and EcoTILLING methods are now being used on capillary systems from, Advanced Analytical Technologies, ABI and Beckman.\n\nSeveral systems can be used to separate PCR products that are not labeled with dyes. Simple agarose electrophoresis systems will separate cleavage products inexpensively and with standard lab equipment. This was used to discover SNPs in chum salmon and was referred to as DEcoTILLING. The disadvantage of this system is reduced resolution compared to polyacrylamide systems. Elchrom Scientific sells Spreadex gels which are precast, can be high throughput and are more sensitive than standard polyacrylamide gels. Advanced Analytical Technologies Inc sells the AdvanCE FS96 dsDNA Fluorescent System which is a 96 capillary electrophoresis system that has several advantages over traditional methods; including ability to separate large fragments (up to 40kb), no desalting or precipitation step required, short run times (~30 minutes), sensitivity to 5pg/ul and no need for fluorescent labeled primers.\n\nSeveral TILLING centers exist over the world that focus on agriculturally important species:\n\n\n\n", "id": "2129346", "title": "TILLING (molecular biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=3172558", "text": "Selectable marker\n\nA selectable marker is a gene introduced into a cell, especially a bacterium or to cells in culture, that confers a trait suitable for artificial selection. They are a type of reporter gene used in laboratory microbiology, molecular biology, and genetic engineering to indicate the success of a transfection or other procedure meant to introduce foreign DNA into a cell. Selectable markers are often antibiotic resistance genes; bacteria that have been subjected to a procedure to introduce foreign DNA are grown on a medium containing an antibiotic, and those bacterial colonies that can grow have successfully taken up and expressed the introduced genetic material.\nNormally the genes encoding resistance to antibiotics such as ampicillin, chloroamphenicol, tetracycline or kanamycin, etc., are considered useful selectable markers for \"E. coli\".\n\nThe non-recombinants are separated from recombinants; i.e., a r-DNA is introduced in bacteria, some bacteria are successfully transformed some remain non-transformed. When grown on medium containing ampicillin bacteria die due to lack of ampicillin resistance. The position is later noted on nitrocellulose paper and separated out to move them to nutrient medium for mass production of required product.\nAn alternative to a selectable marker is a screenable marker, which allows the researcher to distinguish between wanted and unwanted cells, e.g. between blue and white colonies.\n\nFor molecular biology research different types of markers may be used based on the selection sought. These include:\n\nExamples of selectable markers include:\n", "id": "3172558", "title": "Selectable marker"}
{"url": "https://en.wikipedia.org/wiki?curid=3737422", "text": "Ethanol precipitation\n\nEthanol precipitation is a method used to purify and/or concentrate RNA, DNA, and polysaccharides such as pectin and xyloglucan from aqueous solutions by adding ethanol as an antisolvent.\n\nDNA is polar due to its highly charged phosphate backbone. Its polarity makes it water-soluble (water is polar) according to the principle \"like dissolves like\".\n\nBecause of the high polarity of water, illustrated by its high dielectric constant of 80.1 (at 20 °C), electrostatic forces between charged particles are considerably lower in aqueous solution than they are in a vacuum or in air.\n\nThis relation is reflected in Coulomb's law, which can be used to calculate the force acting on two charges formula_1 and formula_2 separated by a distance formula_3 by using the dielectric constant formula_4 (also called relative static permittivity) of the medium in the denominator of the equation (formula_5 is an electric constant):\n\nformula_6\n\nAt an atomic level, the reduction in the force acting on a charge results from water molecules forming a hydration shell around it. This fact makes water a very good solvent for charged compounds like salts. Electric force which normally holds salt crystals together by way of ionic bonds is weakened in the presence of water allowing ions to separate from the crystal and spread through solution.\n\nThe same mechanism operates in the case of negatively charged phosphate groups on a DNA backbone: even though positive ions are present in solution, the relatively weak net electrostatic force prevents them from forming stable ionic bonds with phosphates and precipitating out of solution.\n\nEthanol is much less polar than water, with a dielectric constant of 24.3 (at 25 °C). This means that adding ethanol to solution disrupts the screening of charges by water. If enough ethanol is added, the electrical attraction between phosphate groups and any positive ions present in solution becomes strong enough to form stable ionic bonds and DNA precipitation. This usually happens when ethanol composes over 64% of the solution. As the mechanism suggests, the solution has to contain positive ions for precipitation to occur; usually Na, NH or Li plays this role\n\nDNA is precipitated by first ensuring that the correct concentration of positive ions is present in solution (too much will result in a lot of salt co-precipitating with DNA, too little will result in incomplete DNA recovery) and then adding two to three volumes of at least 95% ethanol. Many protocols advise storing DNA at low temperature at this point, but there are also observation that it may not improve DNA recovery, and may even lower precipitation efficiency while using over-night incubation time. Therefore, good efficiency can be achieved at room temperature, but when possible degradation is taken into account, it is probably better to incubate DNA on wet ice. Optimal incubation time depends on the length and concentration of DNA. Smaller fragments and lower concentrations will require longer times to achieve acceptable recovery. For very small lengths and low concentrations over-night incubation is recommended. In such cases use of carriers like tRNA, glycogen or linear polyacrylamide can greatly improve recovery.\n\nDuring incubation DNA and some salts will precipitate from solution, in the next step this precipitate is collected by centrifugation in a microcentrifuge tube at high speeds (~12,000g). Time and speed of centrifugation has the biggest effect on DNA recovery rates. Again smaller fragments and higher dilutions require longer and faster centrifugation. Centrifugation can be done either at room temperature or in 4 °C or 0 °C.\nDuring centrifugation precipitated DNA has to move through ethanol solution to the bottom of the tube, lower temperatures increase viscosity of the solution and larger volumes make the distance longer, so both those factors lower efficiency of this process requiring longer centrifugation for the same effect.\nAfter centrifugation the supernatant solution is removed, leaving a \"pellet\" of crude DNA. Whether the pellet is visible depends on the amount of DNA and on its purity (dirtier pellets are easier to see) or the use of co-precipitants.\n\nIn the next step, 70% ethanol is added to the pellet, and it is gently mixed to break the pellet loose and wash it. This removes some of the salts present in the leftover supernatant and bound to DNA pellet making the final DNA cleaner. This suspension is centrifuged again to once again pellet DNA and the supernatant solution is removed. This step is repeated once.\n\nFinally, the pellet is air-dried and the DNA is resuspended in water or other desired buffer. It is important not to over-dry the pellet as it may lead to denaturation of DNA and make it harder to resuspend.\n\nIsopropanol can also be used instead of ethanol; the precipitation efficiency of the isopropanol is higher making one volume enough for precipitation. However, isopropanol is less volatile than ethanol and needs more time to air-dry in the final step. The pellet might also adhere less tightly to the tube when using isopropanol.\n\n\n", "id": "3737422", "title": "Ethanol precipitation"}
{"url": "https://en.wikipedia.org/wiki?curid=6885971", "text": "Dilution cloning\n\nDilution cloning or cloning by limiting dilution describes a procedure to obtain a monoclonal cell population starting from a polyclonal mass of cells.\nThis is achieved by setting up a series of increasing dilutions of the parent (polyclonal) cell culture. A suspension of the parent cells is made. Appropriate dilutions are then made, depending on cell number in the starting population, as well as the viability and characteristics of the cells being cloned.\nAfter the final dilutions are produced, aliquots of the suspension are plated or placed in wells and incubated. If all works correctly, a monoclonal cell colony will be produced. Applications for the procedure include cloning of parasites, T cells, transgenic cells, and macrophages.\n\n", "id": "6885971", "title": "Dilution cloning"}
{"url": "https://en.wikipedia.org/wiki?curid=620353", "text": "RecBCD\n\nRecBCD (, \"Exonuclease V\", \"Escherichia coli exonuclease V\", \"E. coli exonuclease V\", \"gene recBC endoenzyme\", \"RecBC deoxyribonuclease\", \"gene recBC DNase\", \"gene recBCD enzymes\") is an enzyme of the \"E. coli\" bacterium that initiates recombinational repair from potentially lethal double strand breaks in DNA which may result from ionizing radiation, replication errors, endonucleases, oxidative damage, and a host of other factors. The RecBCD enzyme is both a helicase that unwinds, or separates the strands of DNA, and a nuclease that makes single-stranded nicks in DNA.\n\nThe enzyme complex is composed of three different subunits called RecB, RecC, and RecD and hence the complex is named RecBCD (Figure 1). Before the discovery of the \"recD\" gene, the enzyme was known as “RecBC.” Each subunit is encoded by a separate gene:\n\nBoth the RecD and RecB subunits are helicases, \"i.e.\", energy-dependent molecular motors that unwind DNA (or RNA in the case of other proteins). The RecB subunit in addition has a nuclease function. Finally, RecBCD enzyme (perhaps the RecC subunit) recognizes a specific sequence in DNA, 5'-GCTGGTGG-3', known as Chi (sometimes designated with the Greek letter χ).\n\nRecBCD is unusual amongst helicases because it has two helicases that travel with different rates and because it can recognize and be altered by the Chi DNA sequence. RecBCD avidly binds an end of linear double-stranded (ds) DNA. The RecD helicase travels on the strand with a 5' end at which the enzyme initiates unwinding, and RecB on the strand with a 3' end. RecB is slower than RecD, so that a single-stranded (ss) DNA loop accumulates ahead of RecB (Figure 2). This produces DNA structures with two ss tails (a shorter 3’ ended tail and a longer 5’ ended tail) and one ss loop (on the 3' ended strand) observed by electron microscopy. The ss tails can anneal to produce a second ss loop complementary to the first one; such twin-loop structures were initially referred to as “rabbit ears.”\nDuring unwinding the nuclease in RecB can act in different ways depending on the reaction conditions, notably the ratio of the concentrations of Mg ions and ATP. (1) If ATP is in excess, the enzyme simply nicks the strand with Chi (the strand with the initial 3' end) (Figure 2). Unwinding continues and produces a 3' ss tail with Chi near its terminus. This tail can be bound by RecA protein, which promotes strand exchange with an intact homologous DNA duplex. When RecBCD reaches the end of the DNA, all three subunits disassemble and the enzyme remains inactive for an hour or more; a RecBCD molecule that acted at Chi does not attack another DNA molecule. (2) If Mg ions are in excess, RecBCD cleaves both DNA strands endonucleolytically, although the 5' tail is cleaved less often (Figure 3). When RecBCD encounters a Chi site on the 3' ended strand, unwinding pauses and digestion of the 3' tail is reduced. When RecBCD resumes unwinding, it now cleaves the opposite strand (\"i.e.\", the 5' tail) and loads RecA protein onto the 3’-ended strand. After completing reaction on one DNA molecule, the enzyme quickly attacks a second DNA, on which the same reactions occur as on the first DNA.\n\nAlthough neither reaction has been verified by analysis of intracellular DNA, due to their transient nature, genetic evidence indicates that the first reaction more nearly mimics that in cells. For example, RecBCD mutants lacking detectable exonuclease activity retain high Chi hotspot activity in cells and nicking at Chi outside cells. A Chi site on one DNA molecule in cells reduces or eliminates Chi activity on another DNA, perhaps reflecting the Chi-dependent disassembly of RecBCD observed in vitro under conditions of excess ATP and nicking of DNA at Chi.\n\nUnder both reaction conditions, the 3' strand remains intact downstream of Chi. The RecA protein is then actively loaded onto the 3' tail by RecBCD. At some undetermined point RecBCD dissociates from the DNA, although RecBCD can unwind at least 60 kb of DNA without falling off. RecA initiates exchange of the DNA strand to which it is bound with the identical, or nearly identical, strand in an intact DNA duplex; this strand exchange generates a joint DNA molecule, such as a D-loop (Figure 2). The joint DNA molecule is thought to be resolved either by replication primed by the invading 3’ ended strand containing Chi or by cleavage of the D-loop and formation of a Holliday junction. The Holliday junction can be resolved into linear DNA by the RuvABC complex or dissociated by the RecG protein. Each of these events can generate intact DNA with new combinations of genetic markers by which the parental DNAs may differ. This process, homologous recombination, completes the repair of the double-stranded DNA break.\n\nRecBCD is a model enzyme for the use of single molecule fluorescence as an experimental technique used to better understand the function of protein-DNA interactions. The enzyme is also useful in removing linear DNA, either single- or double-stranded, from preparations of circular double-stranded DNA, since it requires a DNA end for activity.\n", "id": "620353", "title": "RecBCD"}
{"url": "https://en.wikipedia.org/wiki?curid=7215078", "text": "CD90\n\nThy-1 or CD90 (Cluster of Differentiation 90) is a 25–37 kDa heavily N-glycosylated, glycophosphatidylinositol (GPI) anchored conserved cell surface protein with a single V-like immunoglobulin domain, originally discovered as a thymocyte antigen. Thy-1 can be used as a marker for a variety of stem cells and for the axonal processes of mature neurons. Structural study of Thy-1 led to the foundation of the Immunoglobulin superfamily, of which it is the smallest member, and led to some of the initial biochemical description and characterization of a vertebrate GPI anchor and also the first demonstration of tissue specific differential glycosylation.\n\nThe antigen Thy-1 was the first T cell marker to be identified. Thy-1 was discovered by Reif and Allen in 1964 during a search for heterologous antisera against mouse leukemia cells, and was demonstrated by them to be present on murine thymocytes, on T lymphocytes, and on neuronal cells. It was originally named theta (θ) antigen, then Thy-1 (THYmocyte differentiation antigen 1) due to its prior identification in thymocytes (precursors of T cells in the thymus). The human homolog was isolated in 1980 as a 25kDa protein (p25) of T-lymphoblastoid cell line MOLT-3 binding with anti-monkey-thymocyte antisera. The discovery of Thy-1 in mice and humans led to the subsequent discovery of many other T cell markers, which is very significant to the field of immunology since T cells (along with B cells) are the major cellular components of the adaptive immune response.\n\nThy-1 has been conserved throughout vertebrate evolution and even in some invertebrates, with homologs described in many species like squid, frogs, chickens, mice, rats, dogs, and humans.\n\nThe Thy-1 gene is located at human chromosome 11q22.3 (mouse chromosome 9qA5.1). In AceView, it covers 6.82 kb, from 119294854 to 119288036 (NCBI 37, August 2010), on the reverse strand. This locus is very close to CD3 & CD56/NCAM genes. Some believe that there may be a functional significance of both this gene and CD3 delta subunit (T3D) mapping to chromosome 11q in man and chromosome 9 in mouse, though there is no homology (in fact this speculation lead to its localization in chromosome 11q - the human chromosome region syntenic to mouse chromosome 9 which harbored T3D). In mice, there are two alleles: Thy1.1 (Thy 1a, CD90.1) and Thy1.2 (Thy 1b, CD90.2). They differ by only one amino acid at position 108; an arginine in Thy-1.1 and a glutamine in Thy-1.2. Thy 1.2 is expressed by most strains of mice, whereas Thy1.1 is expressed by some like AKR/J and PL mouse strains.\n\nThe 25-kDa core protein (excluding the heavy glycosylation) of rodent Thy-1 is 111 or 112 amino acids in length, and is N-glycosylated at three sites (In contrast to only two glycosylation sites for human Thy-1). The 162aa (murine, 161 for human) Thy1 precursor has 19 amino acid (aa 1-19) signal sequence and 31 amino acid (aa 132-162) C-terminal transmembrane domain that is present in pro form but removed when transferring the 112 amino acid (aa 20-131) mature peptide to GPI anchor which would attach through the aa 131.\n\nSome of the common monoclonal antibodies used to detect this protein are clones OX7, 5E10, K117 and L127.\nThere have been some reports of Thy1 monoclonal antibodies cross reacting with some cytoskeletal elements: anti Thy-1.2 with actin in marsupial, murine, and human cells and anti Thy-1.1 with vimentin, and were suggested to be due to sequence homology by studies done more than 20 years back.\n\nThy-1, like many other GPI anchored proteins can be shed by special types of Phospholipase C e.g. PI-PLC (phosphatidyl-Inositol Phospholipase C, or PLC β). it can also be involved in cell to cell transfer of GPI anchored proteins like CD55 and CD59.\n\nThy-1 is one of the most heavily glycosylated membrane proteins with a carbohydrate content up to 30% of its molecular mass. Thy1 in most species has 3 N-glycosylation sites (Asn 23, 74 and 98) but no O-glycosylation. The composition of Thy-1 carbohydrate moieties varies considerably between different tissues or even among cells of the same lineage at different stages of differentiation: e.g., galactosamine only in brain Thy-1, sialic acid in thymic Thy-1 in far excess than brain Thy-1, that too increasing in parallel with T cell maturation. In this regard it has yet another historic association: Thy1 happens to be the first glycoprotein in which cell type specificity of variant glycosylation on an invariant protein was demonstrated. Analysis of Differencial glycosylation of Thy-1 from brain and thymus showed that all the complex N-linked structures differed between the two forms, superimposed upon a site specific common core. In case of Thy1 this core pattern was constituted by Asn23 carrying mostly oligomannose structures, Asn74 carrying the most extended complex structures, and Asn98 carrying smaller complex structure. The structure of the sugar residues in the GPI anchor and their associated esterified structures (e.g. additional fatty acids and alcohols) also can be cell type and species specific.\n\nThy1 expression varies between species. Amongst the cells reported to generally express Thy-1 are thymocytes (precursor of T cells in the thymus) & CD34(+) prothymocytes; neurons, mesenchymal stem cells, hematopoietic stem cells, NK cells, murine T-cells, endothelium (mainly in high endothelial venules or HEVs where diapedesis takes place), renal glomerular mesangial cells, circulating metastatic melanoma cells, follicular dendritic cells (FDC), a fraction of fibroblasts and myofibroblasts.\n\n\n\nAs a GPI-anchored protein, Thy-1 is present in the outer leaflet of lipid rafts in the cell membrane. In case of neurons it is known to be expressed strongly in the mature axon. The axon hillock can act as a barrier for its lateral spread even though it has no transmembrane segment. Thy-1 has been suggested to interact with G inhibitory proteins, the Src family kinase (SFK) member c-fyn, and tubulin within lipid rafts. In rats and mice, Thy-1 protein is present on the soma (cell body) and dendrites of neurons but is not expressed on axons until axonal growth is complete, and is again temporarily suppressed during axonal injury. HIV-1 Matrix co-localizes with Thy-1 in lipid rafts, the site of virus particle budding from cells, and Thy-1 is incorporated into virus particles as a result of this process.\n\nThe function of Thy-1 has not yet been fully elucidated. It has speculated roles in cell-cell and cell-matrix interactions, with implication in neurite outgrowth, nerve regeneration, apoptosis, metastasis, inflammation, and fibrosis.\n\nThe Thy-1 knockout (KO) mice are viable and appear grossly normal. They display normal social interactions and normal learning in a maze, but fail to learn from social cues (e.g. learning from other mice which foods are safe to eat as compared to wild-type mice). This failure can be rescued by the transgenic expression of Thy-1 or pharmacologic treatment with a GABA (A) receptor antagonists. This suggests that Thy-1 KO mice have excessive GABAergic inhibition in the dentate gyrus and regional inhibition of long-term potentiation.\n\nCrosslinking anti-Thy-1 Ab can promote neurite outgrowth which is dependent on G{alpha}i and L- and N-type calcium channel activation. The ligand for promotion of neurite outgrowth on astrocytes is not yet identified, but the inhibitory ligand has been suggested to be integrins. Thy1 is one of the known ligands of beta 3 integrins. Interaction of thy1 expressed on maturing axons with beta 3 integrins expressed on mature astrocytes may be the cause of halting of axon growth.\n\nCrosslinking Thy-1 molecules in the membrane raft, in the context of strong costimulatory signaling through CD28 in mouse T cells can act to some extent as a substitute activating signal for T-cell receptor signaling. Conversely it can substitute CD28 costimulation for activation through the TCR.\n\nCross linking antibody induced aggregation of Thy1 cause death of thymocytes and mesangial cells mainly by apoptosis despite Bcl2 upregulation. The death of mesangial cells seems to be apoptosis by TUNEL staining or annexin V staining, but electron microscopy suggest it is necrosis.\n\nSingle tail vein intravenous injection of antibody (OX7 mouse monoclonal IgG) against Thy1.1 in rats is used as a standard animal model to produce experimental mesangioproliferative glomerulonephritis which is popularly known in the field of nephrology as \"antiThy1 GN\".\n\nIt has also been proven to be a tumor suppressor for some tumors. It probably is aided by its action in upregulating thrombospondin, SPARC (osteonectin), and fibronectin. However it has also been speculated to aid in extravasation in circulating melanoma cells. In case of prostate cancer it has been shown to be expressed in cancer associated stroma but not in normal stroma and has been suggested to be of potential help for cancer specific drug targeting .\n\nActing through several integrins and probably a few yet unknown other receptors Thy-1 mediates adhesion of leukocytes and monocytes to endothelial cells and fibroblasts, melanoma cells to endothelium, and thymocytes to thymic epithelium. Thy1 expression comes on when endothelial cells are activated. It has been shown to interact with the leukocyte integrin Mac1 (CD11b/CD18) and may play a role in leukocyte homing and recruitment.\n\nRole of Thy-1 in fibrosis and fibroblast differention may have some tissue variation. In lung fibrosis Thy-1 level is suppressed in stimulated fibroblasts. Thy1 knock out mice have increased fibrosis in the lung. Fibrosis induced by radiation mimicking chemotherapeutic agent Bleomycin is also increased in these mice.\n\nThy-1 knock out mice also show impaired cutaneous immune responses and abnormal retinal development: thinning of the inner nuclear, inner plexiform, ganglion cell, and outer segment layers of the retina.\n\nThy-1 can be considered as a surrogate marker for various kind of stem cells (e.g. hematopoietic stem cells or HSCs). It is one of the popular combinatorial surface markers for FACS for stem cells in combination with other markers like CD34. In humans, Thy-1 is expressed on neurons and HSCs among others. It is considered a major marker of HSC pluripotency in concordance with CD34. In human HSCs, Thy1 cells are all CD34 positive. Thy 1 is also a marker of other kind of stem cells, for example: mesenchymal stem cells, hepatic stem cells (\"oval cells\"), keratinocyte stem cells, putative endometrial progenitor/(?)stem cells.\n", "id": "7215078", "title": "CD90"}
{"url": "https://en.wikipedia.org/wiki?curid=1143734", "text": "Peptidomimetic\n\nA peptidomimetic is a small protein-like chain designed to mimic a peptide. They typically arise either from modification of an existing peptide, or by designing similar systems that mimic peptides, such as peptoids and β-peptides. Irrespective of the approach, the altered chemical structure is designed to advantageously adjust the molecular properties such as, stability or biological activity. This can have a role in the development of drug-like compounds from existing peptides. These modifications involve changes to the peptide that will not occur naturally (such as altered backbones and the incorporation of nonnatural amino acids). Based on their similarity with the precursor peptide, peptidomimetics can be grouped into four classes (A – D) where A features the most and D the least similarities. Classes A and B involve peptide-like scaffolds, while classes C and D include small molecules.\n\nA D-peptide is a small sequence of D-amino acids. Since ribosomes are specific to L-amino acids, D-peptides rarely occur naturally in organisms and are not easily digested or degraded. D-peptide peptidomimetics are D-peptides designed to mimic natural L-peptides that commonly have therapeutic properties.\n\nWhen placed in a nonchiral solvent like water, D-peptides, as well as the larger polypeptide D-proteins, \nhave similar but mirrored properties to the L-peptides and L-proteins\nwith identical sequences. If an L-protein does not require a Chaperone or a structural cofactor to fold, its\nD-enantiomer protein should have a mirror image conformation with respect to the L-protein (Figure 1). A D-enzyme should act on substrates of reverse chirality compared to the L-enzyme with the same sequence. Similarly, if an L-peptide binds to an L-protein, their D-peptide and D-protein counterparts\nshould bind together in a mirrored way.\n\nD-peptides also have properties that make them attractive as drugs. D-peptides are less susceptible to be degraded in the stomach or inside cells by \nproteolysis. D-peptide drugs can therefore be taken orally and are effective for a longer period of time. D-peptides are\neasy to synthesize, when compared to many other drugs. In some cases, D-peptides can have a low immunogenic response.\n\nAn L-peptide has three analogue sequences (Figure 2) built from L and D amino acids: the D-enantiomer or inverso-peptide with the same sequence, but composed of D-amino\nacids and a mirror conformation; the retro-peptide, consisting of the same sequence of L amino acids but in reverse order; and the \nretro-inverso or D-retro-enantiomer peptide, consisting of D-amino acids in the reversed sequence.\nWhile the L-peptide and its D-enantiomer are mirror structures of each other, the L-retro-peptide is the mirror image of the D-retro-inverso-peptide.\nOn the other hand, the L-peptide and the D-retro-inverso-peptide share a similar arrangement of side-chains, although their carboxyl and amino groups\npoint in opposing directions. For small peptides that do not depend on a secondary structure for binding, an L-peptide and its D-retro-inverso-peptide is likely to have\na similar binding affinity with a target L-protein.\n\nPhage display is a technique to screen large libraries of peptides for binding to a target protein. In phage display,\nthe DNA sequence that codes the potential drug-peptide is fusioned to the gene of the protein coat of bacteriophages and introduced into a vector. Diversity can be introduced to the peptide by mutagenesis. The protein coats peptides are \nthen expressed and purified, and applied to a surface of immobilized protein targets. The surface is then washed away to remove non-binding \npeptides, while the remaining binding peptides are eluted.\nMirror-image phage display is a similar method that can be used to screen large libraries of D-peptides that bind to target L-proteins. More precisely,\nsince D-peptides can not be expressed in bacteriophages, mirror-image phage display screens L-peptides that bind to immobilized D-proteins that are \npreviously chemically synthesized. Because of the mirror properties of D-peptides, the D-enantiomer of an L-peptide that binds to a D-protein \nwill bind to the L-protein.\n\nMirror-image phage display, however, has two disadvantages when compared to phage display. \nTarget D-proteins must be chemically synthesized, which is normally an expensive and time consuming process. Also, \nthe target protein must not require a cofactor or a chaperone to fold, otherwise the chemically synthesized D-protein\nwill not fold to the target, mirror structure.\n\nPeptidomimetic approaches have been utilized to design small molecules that selectively kill cancer cells, an approach known as targeted chemotherapy, by inducing programmed cell death by a process called apoptosis. The following two examples mimic proteins involved in key Protein–protein interactions that reactivate the apoptotic pathway in cancer, but do so by distinct mechanisms.\n\nIn 2004, Walensky and co-workers reported a stabilized alpha helical peptide that mimics pro-apoptotic BH3-only proteins, such as BID and BAD. This molecule was designed to stabilize the native helical structure by forming a macrocycle between side chains that are \"not\" involved in binding. This process, referred to as peptide stapling, uses non-natural amino acids to facilitate macrocyclization by ring-closing olefin metathesis. In this case, a stapled BH3 helix was identified which specifically activates the mitochondrial apoptotic pathway by antagonizing the sequestration of BH3-only proteins by anti-apoptotic proteins (e.g. Bcl-2, see also intrinsic and extrinsic inducers of the apoptosis). This molecule suppressed growth of human leukemia in a mouse xenograft model.\n\nAlso in 2004, Harran and co-workers reported a dimeric small molecule that mimics the proapoptotic protein Smac (see mitochondrial regulation in apoptosis). This molecule mimics the N-terminal linear motif Ala-Val-Pro-Ile. Uniquely, the dimeric structure of this peptidomimetic led to a marked increase in activity over an analogous monomer. This binding cooperativity results from the molecule's ability to also mimic the homodimeric structure of Smac, which is functionally important for reactivating caspases. Smac mimetics of this type can sensitize an array of non-small-cell lung cancer cells to conventional chemotherapeutics (e.g. Gemcitabine, Vinorelbine) both in vitro and in mouse xenograft models.\n\nHeterocycles are often used to mimic the amide bond of peptides. Thiazoles, for example, are found in naturally occurring peptides and used by researchers to mimic the amide bond of peptide.\n\n", "id": "1143734", "title": "Peptidomimetic"}
{"url": "https://en.wikipedia.org/wiki?curid=3960674", "text": "GeneRIF\n\nA GeneRIF or Gene Reference Into Function is a short (255 characters or fewer) statement about the function of a gene. GeneRIFs provide a simple mechanism for allowing scientists to add to the functional annotation of genes described in the Entrez Gene database. In practice, \"function\" is construed quite broadly. For example, there are GeneRIFs that discuss the role of a gene in a disease, GeneRIFs that point the viewer towards a review article about the gene, and GeneRIFs that discuss the structure of a gene. However, the stated intent is for GeneRIFs to be about gene function. Currently over half a million geneRIFs have been created for genes from almost 1000 different species.\n\nGeneRIFs are always associated with specific entries in the Entrez Gene database. Each GeneRIF has a pointer to the PubMed ID (a type of document identifier) of a scientific publication that provides evidence for the statement made by the GeneRIF. GeneRIFs are often extracted directly from the document that is identified by the PubMed ID, very frequently from its title or from its final sentence.\n\nGeneRIFs are usually produced by NCBI indexers, but anyone may submit a GeneRIF.\nTo be processed, a valid Gene ID must exist for the specific gene, or the Gene staff must have assigned an overall Gene ID to the species. The latter case is implemented via records in Gene with the symbol NEWENTRY. Once the Gene ID is identified, only three types of information are required to complete a submission:\n\nHere are some GeneRIFs taken from Entrez Gene for GeneID 7157, the human gene TP53.\nThe PubMed document identifiers have been omitted from the examples. Note the wide variability with respect to the presence or absence of punctuation and of sentence-initial capital letters.\n\nGeneRIFs are an unusual type of textual genre, and they have recently been the subject of a number of articles from the natural language processing community.\n\n\n", "id": "3960674", "title": "GeneRIF"}
{"url": "https://en.wikipedia.org/wiki?curid=1127940", "text": "Isoschizomer\n\nIsoschizomers are pairs of restriction enzymes specific to the same recognition sequence. For example, SphI (CGTAC/G) and BbuI (CGTAC/G) are isoschizomers of each other. The first enzyme discovered which recognizes a given sequence is known as the prototype; all subsequently identified enzymes that recognize that sequence are isoschizomers. Isoschizomers are isolated from different strains of bacteria and therefore may require different reaction conditions.\n\nAn enzyme that recognizes the same sequence but cuts it differently is a neoschizomer. Neoschizomers are a specific type (subset) of isoschizomer. For example, SmaI (CCC/GGG) and XmaI (C/CCGGG) are neoschizomers of each other. Similarly Kpn1 (GGTAC/C) and Acc651 (G/GTACC) are neoschizomers of each other. \nAn enzyme that recognizes a slightly different sequence, but produces the same ends is an isocaudomer.\n\nIn some cases, only one out of a pair of isoschizomers can recognize both the methylated as well as unmethylated forms of restriction sites. In contrast, the other restriction enzyme can recognize only the unmethylated form of the restriction site.\nThis property of some isoschizomers allows identification of methylation state of the restriction site while isolating it from a bacterial strain.\nFor example, the restriction enzymes HpaII and MspI are isoschizomers, as they both recognize the sequence 5'-CCGG-3' when it is unmethylated. But when the second C of the sequence is methylated, only MspI can recognize it while HpaII cannot.\n", "id": "1127940", "title": "Isoschizomer"}
{"url": "https://en.wikipedia.org/wiki?curid=5061860", "text": "Initiation factor\n\nInitiation factors are proteins that bind to the small subunit of the ribosome during the initiation of translation, a part of protein biosynthesis.\n\nThey are divided into three major groups: \n", "id": "5061860", "title": "Initiation factor"}
{"url": "https://en.wikipedia.org/wiki?curid=7528959", "text": "Peptide computing\n\nPeptide computing is a form of computing which uses peptides and molecular biology, instead of traditional silicon-based computer technologies. The basis of this computational model is the affinity of antibodies towards peptide sequences. Similar to DNA computing, the parallel interactions of peptide sequences and antibodies have been used by this model to solve a few NP-complete problems. Specifically, the hamiltonian path problem (HPP) and some versions of the set cover problem are a few NP-complete problems which have been solved using this computational model so far. This model of computation has also been shown to be computationally universal (or Turing complete).\n\nThis model of computation has some critical advantages over DNA computing. For instance, while DNA is made of four building blocks, peptides are made of twenty building blocks. The peptide-antibody interactions are also more flexible with respect to recognition and affinity than an interaction between a DNA strand and its reverse complement. However, unlike DNA computing, this model is yet to be practically realized. The main limitation is the availability of specific monoclonal antibodies required by the model.\n\n", "id": "7528959", "title": "Peptide computing"}
{"url": "https://en.wikipedia.org/wiki?curid=5293306", "text": "SNP array\n\nIn molecular biology, SNP array is a type of DNA microarray which is used to detect polymorphisms within a population. A single nucleotide polymorphism (SNP), a variation at a single site in DNA, is the most frequent type of variation in the genome. Around 325 million SNPs have been identified in the human genome, 15 million of which are present at frequencies of 1% or higher across different populations worldwide.\n\nThe basic principles of SNP array are the same as the DNA microarray. These are the convergence of DNA hybridization, fluorescence microscopy, and solid surface DNA capture. The three mandatory components of the SNP arrays are:\n\nThe ASO probes are often chosen based on sequencing of a representative panel of individuals: positions found to vary in the panel at a specified frequency are used as the basis for probes. SNP chips are generally described by the number of SNP positions they assay. Two probes must be used for each SNP position to detect both alleles; if only one probe were used, experimental failure would be indistinguishable from homozygosity of the non-probed allele.\n\nAn SNP array is a useful tool for studying slight variations between whole genomes. The most important clinical applications of SNP arrays are for determining disease susceptibility and for measuring the efficacy of drug therapies designed specifically for individuals. In research, SNP arrays are most frequently used for genome-wide association studies. Each individual has many SNPs. SNP-based genetic linkage analysis can be used to map disease loci, and determine disease susceptibility genes in individuals. The combination of SNP maps and high density SNP arrays allows SNPs to be used as markers for genetic diseases that have complex traits. For example, genome-wide association studies have identified SNPs associated with diseases such as rheumatoid arthritis, prostate cancer, and type 2 diabetes. A SNP array can also be used to generate a virtual karyotype using software to determine the copy number of each SNP on the array and then align the SNPs in chromosomal order.\n\nSNPs can also be used to study genetic abnormalities in cancer. For example, SNP arrays can be used to study loss of heterozygosity (LOH). LOH occurs when one allele of a gene is mutated in a deleterious way and the normally-functioning allele is lost. LOH occurs commonly in oncogenesis. For example, tumor suppressor genes help keep cancer from developing. If a person has one mutated and dysfunctional copy of a tumor suppressor gene and his second, functional copy of the gene gets damaged, they may become more likely to develop cancer.\n\nOther chip-based methods such as comparative genomic hybridization can detect genomic gains or deletions leading to LOH. SNP arrays, however, have an additional advantage of being able to detect copy-neutral LOH (also called uniparental disomy or gene conversion). Copy-neutral LOH is a form of allelic imbalance. In copy-neutral LOH, one allele or whole chromosome from a parent is missing. This problem leads to duplication of the other parental allele. Copy-neutral LOH may be pathological. For example, say that the mother's allele is wild-type and fully functional, and the fathers's allele is mutated. If the mother's allele is missing and the child has two copies of the father's mutant allele, disease can occur.\n\nHigh density SNP arrays help scientists identify patterns of allelic imbalance. These studies have potential prognostic and diagnostic uses. Because LOH is so common in many human cancers, SNP arrays have great potential in cancer diagnostics. For example, recent SNP array studies have shown that solid tumors such as gastric cancer and liver cancer show LOH, as do non-solid malignancies such as hematologic malignancies, ALL, MDS, CML and others. These studies may provide insights into how these diseases develop, as well as information about how to create therapies for them.\n\nBreeding in a number of animal and plant species has been revolutionized by the emergence of SNP arrays. The method is based on the prediction of genetic merit by incorporating relationships among individuals based on SNP array data. This process is known as genomic selection.\n\n", "id": "5293306", "title": "SNP array"}
{"url": "https://en.wikipedia.org/wiki?curid=7705107", "text": "DNA Data Bank of Japan\n\nThe DNA Data Bank of Japan (DDBJ) is a biological database that collects DNA sequences. It is located at the National Institute of Genetics (NIG) in the Shizuoka prefecture of Japan. It is also a member of the International Nucleotide Sequence Database Collaboration or INSDC. It exchanges its data with European Molecular Biology Laboratory at the European Bioinformatics Institute and with GenBank at the National Center for Biotechnology Information on a daily basis. Thus these three databanks contain the same data at any given time.\n\nDDBJ began data bank activities in 1986 at NIG and remains the only nucleotide sequence data bank in Asia. Although DDBJ mainly receives its data from Japanese researchers, it can accept data from contributors from any other country. DDBJ is primarily funded by the Japanese Ministry of Education, Culture, Sports, Science and Technology (MEXT). DDBJ has an international advisory committee which consists of nine members, 3 members each from Europe, US, and Japan. This committee advises DDBJ about its maintenance, management and future plans once a year. Apart from this DDBJ also has an international collaborative committee which advises on various technical issues related to international collaboration and consists of working-level participants.\n\n\n", "id": "7705107", "title": "DNA Data Bank of Japan"}
{"url": "https://en.wikipedia.org/wiki?curid=7792469", "text": "MRNA display\n\nmRNA display is a display technique used for \"in vitro\" protein, and/or peptide evolution to create molecules that can bind to a desired target. The process results in translated peptides or proteins that are associated with their mRNA progenitor via a puromycin linkage. The complex then binds to an immobilized target in a selection step (affinity chromatography). The mRNA-protein fusions that bind well are then reverse transcribed to cDNA and their sequence amplified via a polymerase chain reaction. The result is a nucleotide sequence that encodes a peptide with high affinity for the molecule of interest.\n\nPuromycin is an analogue of the 3’ end of a tyrosyl-tRNA with a part of its structure mimics a molecule of adenosine, and the other part mimics a molecule of tyrosine. Compared to the cleavable ester bond in a tyrosyl-tRNA, puromycin has a non-hydrolysable amide bond. As a result, puromycin interferes with translation, and causes premature release of translation products.\n\nTo synthesize an mRNA-polypeptide fusion, the fused puromycin is not the only modification to the mRNA template. Oligonucleotides and other spacers need to be recruited along with the puromycin to provide flexibility and proper length for the puromycin to enter the A site. Ideally, the linker between the 3’ end of an mRNA and the puromycin has to be flexible and long enough to allow the puromycin to enter the A site upon translation of the last codon. This enables the efficient production of high-quality, full-length mRNA-polypeptide fusion. Rihe Liu \"et al.\" optimized the 3’-puromycin oligonucleotide spacer. They reported that dA25 in combination with a Spacer 9 (Glen Research), and dAdCdCP at the 5’ terminus worked the best for the fusion reaction. They found that linkers longer than 40 nucleotides and shorter than 16 nucleotides showed greatly reduced efficiency of fusion formation. Also, when the sequence rUrUP presented adjacent to the puromycin, fusion did not form efficiently.\n\nIn addition to providing flexibility and length, the poly dA portion of the linker also allows further purification of the mRNA-polypeptide fusion due to its high affinity for dT cellulose resin. The mRNA-polypeptide fusions can be selected over immobilized selection targets for several rounds with increasing stringency. After each round of selection, those library members that stay bound to the immobilized target are PCR amplified, and non-binders are washed off.\n\nThe synthesis of an mRNA display library starts from the synthesis of a DNA library. A DNA library for any protein or small peptide of interest can be synthesized by solid-phase synthesis followed by PCR amplification. Usually, each member of this DNA library has a T7 RNA polymerase transcription site and a ribosomal binding site at the 5’ end. The T7 promoter region allows large-scale \"in vitro\" T7 transcription to transcribe the DNA library into an mRNA library, which provides templates for the \"in vitro\" translation reaction later. The ribosomal binding site in the 5’-untranslated region (5’ UTR) is designed according to the \"in vitro\" translation system to be used. There are two popular commercially available \"in vitro\" translation systems. One is \"E. Coli\" S30 Extract System (Promega) that requires a Shine-Dalgarno sequence in the 5’ UTR as a ribosomal binding site; the other one is Red Nova Lysate (Novagen), which needs a ΔTMV ribosomal binding site.\n\nOnce the mRNA library is generated, it will be Urea-PAGE purified and ligated using T4 DNA ligase to the DNA spacer linker containing puromycin at the 3’ end. In this ligation step, a piece of mRNA is ligated with a single stranded DNA with the help from T4 DNA ligase. This is not a standard T4 DNA ligase ligation reaction, where two pieces of double stranded DNA are ligated together. To increase the yield of this special ligation, a single stranded DNA splint may be used to aid the ligation reaction. The 5’ terminus of the splint is designed to be complementary to the 3’ end of the mRNA, and the 3’ terminus of the splint is designed to be complementary to the 5’ end of the DNA spacer linker, which usually consists of poly dA nucleotides (Figure 2).\n\nAfter translation, the single-stranded mRNA portions of the fusions will be converted to heteroduplex of RNA/DNA by reverse transcriptase to eliminate any unwanted RNA secondary structures, and render the nucleic acid portion of the fusion more stable. This step is a standard reverse transcription reaction. For instance, it can be done by using Superscript II (GIBCO-BRL) following the manufacturer’s protocol.\n\nThe mRNA/DNA-polypeptide fusions can be selected over immobilized selection targets for several rounds (Figure 3). There might be a relatively high background for the first few rounds of selection, and this can be minimized by increasing selection stringency, such as adjusting salt concentration, amount of detergent, and/or temperature during the target/fusion binding period. Following binding selection, those library members that stay bound to the immobilized target are PCR amplified. The PCR amplification step will enrich the population from the mRNA-display library that has higher affinity for the immobilized target. Error-prone PCR can also be done in between each round of selection to further increase the diversity of the mRNA-display library and reduce background in selection.\n\nA less time-consuming protocol for mRNA display was recently published.\n\nAlthough there are many other molecular display technologies, such as phage display, bacterial display, yeast display, and ribosome display, mRNA display technology has many advantages over the others. The first three biological display libraries listed have polypeptides or proteins expressed on the respective microorganism’s surface and the accompanying coding information for each polypeptide or protein is retrievable from the microorganism’s genome. However, the library size for these three \"in vivo\" display systems is limited by the transformation efficiency of each organism. For example, the library size for phage and bacterial display is limited to 1-10 × 10^9 different members. The library size for yeast display is even smaller. Moreover, these cell-based display system only allow the screening and enrichment of peptides/proteins containing natural amino acids. In contrast, mRNA display and ribosome display are \"in vitro\" selection methods. They allow a library size as large as 10^15 different members. The large library size increases the probability to select very rare sequences, and also improves the diversity of the selected sequences. In addition, \"in vitro\" selection methods remove unwanted selection pressure, such as poor protein expression, and rapid protein degradation, which may reduce the diversity of the selected sequences. Finally, \"in vitro\" selection methods allow the application of \"in vitro\" mutagenesis and recombination techniques throughout the selection process.\n\nAlthough both ribosome display and mRNA display are \"in vitro\" selection methods, mRNA display has some advantage over the ribosome display technology. mRNA display utilizes covalent mRNA-polypeptide complexes linked through puromycin; whereas, ribosome display utilizes stalled, noncovalent ribosome-mRNA-polypeptide complexes. For ribosome display, selection stringency is limited to keep ribosome-mRNA-polypeptide in a complex because of the noncovalent ribosome-mRNA-polypeptide complexes. This may cause difficulties in reducing background binding during the selection cycle. Also, the polypeptides under selection in a ribosome display system are attached to an enormous rRNA-protein complex, a ribosome, which has a molecular weight of more than 2,000,000 Da. There might be some unpredictable interaction between the selection target and the ribosome, and this may lead to a loss of potential binders during the selection cycle. In contrast, the puromycin DNA spacer linker used in mRNA display technology is much smaller comparing to a ribosome. This linker may have less chance to interact with an immobilized selection target. Thus, mRNA display technology is more likely to give less biased results.\n\nIn 1997, Roberts and Szostak showed that fusions between a synthetic mRNA and its encoded \"myc\" epitope could be enriched from a pool of random sequence mRNA-polypeptide fusions by immunoprecipitation.\n\nNine years later, Fukuda and colleagues chose mRNA display method for \"in vitro\" evolution of single-chain Fv (scFv) antibody fragments. They selected six different scFv mutants with five consensus mutations. However, kinetic analysis of these mutants showed that their antigen-specificity remained similar to that of the wild type. However, they have demonstrated that two of the five consensus mutations were within the complementarity determining regions (CDRs). And they concluded that mRNA display has the potential for rapid artificial evolution of high-affinity diagnostic and therapeutic antibodies by optimizing their CDRs.\n\nRoberts and coworkers have demonstrated that unnatural peptide oligomers consisting of an N-substituted amino acid can be synthesized as mRNA-polypeptide fusions. N-substituted amino acid-containing peptides have been associated with good proteolytic stability and improved pharmacokinetic properties. This work indicates that mRNA display technology has the potential for selecting drug-like peptides for therapeutic usage resistant to proteolysis.\n\n", "id": "7792469", "title": "MRNA display"}
{"url": "https://en.wikipedia.org/wiki?curid=1812008", "text": "RAPD\n\nRAPD (pronounced \"rapid\") stands for 'Random Amplification of Polymorphic DNA'. It is a type of PCR reaction, but the segments of DNA that are amplified are random. The scientist performing RAPD creates several arbitrary, short primers (8–12 nucleotides), then proceeds with the PCR using a large template of genomic DNA, hoping that fragments will amplify. By resolving the resulting patterns, a semi-unique profile can be gleaned from an RAPD reaction.\n\nNo knowledge of the DNA sequence of the targeted genome is required, as the primers will bind somewhere in the sequence, but it is not certain exactly where. This makes the method popular for comparing the DNA of biological systems that have not had the attention of the scientific community, or in a system in which relatively few DNA sequences are compared (it is not suitable for forming a cDNA databank). Because it relies on a large, intact DNA template sequence, it has some limitations in the use of degraded DNA samples. Its resolving power is much lower than targeted, species-specific DNA comparison methods, such as short tandem repeats. In recent years, RAPD has been used to characterize, and trace, the phylogeny of diverse plant and animal species.\n\nRAPD markers are decamer (10 nucleotide length) DNA fragments from PCR amplification of random segments of genomic DNA with single primer of arbitrary nucleotide sequence and which are able to differentiate between genetically distinct individuals, although not necessarily in a reproducible way.\nIt is used to analyse the genetic diversity of an individual by using random primers. Due to problems in experiment reproducibility, many scientific journals do not accept experiments merely based on RAPDs anymore.\nRAPD requires only one primer for amplification.\n\nUnlike traditional PCR analysis, RAPD does not require any specific knowledge of the DNA sequence of the target organism: the identical 10-mer primers will or will not amplify a segment of DNA, depending on positions that are complementary to the primers' sequence. For example, no fragment is produced if primers annealed too far apart or 3' ends of the primers are not facing each other. Therefore, if a mutation has occurred in the template DNA at the site that was previously complementary to the primer, a PCR product will not be produced, resulting in a different pattern of amplified DNA segments on the gel.\n\nRAPD is an inexpensive yet powerful typing method for many bacterial species. The image visible at the link is a silver-stained polyacrylamide gel showing three distinct RAPD profiles generated by primer OPE15 for \"Haemophilus ducreyi\" isolates from Tanzania, Senegal, Thailand, Europe, and North America.\n\nSelecting the right sequence for the primer is very important because different sequences will produce different band patterns and possibly allow for a more specific recognition of individual strains.\n\n\n\n", "id": "1812008", "title": "RAPD"}
{"url": "https://en.wikipedia.org/wiki?curid=7856433", "text": "Phosphodiesterase 3\n\nPDE3 is a phosphodiesterase. The PDEs belong to at least eleven related gene families, which are different in their primary structure, substrate affinity, responses to effectors, and regulation mechanism.\nMost of the PDE families are composed of more than one gene. PDE3 is clinically significant because of its role in regulating heart muscle, vascular smooth muscle and platelet aggregation. PDE3 inhibitors have been developed as pharmaceuticals, but their use is limited by arrhythmic effects and they can increase mortality in some applications.\n\nThe mammalian PDEs share a common structural organization and contain three functional domains, which include the conserved catalytic core, a regulatory N-terminus, and the C-terminus. The conserved catalytic core is much more similar within PDE families, with about 80% amino acid identity, than between different families. It is believed that the core contains common structural elements that are important for the hydrolysis of cAMP and cGMP phosphodiester bonds. It is also believed that it contains family-specific determinants for differences in affinity for substrates and sensitivity for inhibitors.\nThe catalytic domain of PDE3 is characterized by a 44-amino acid insert, but this insert is unique to the PDE3 family, and is a factor when determining a structure for a potent and selective PDE3 inhibitor.\nThe crystal structure of the catalytic domains of several PDEs, including PDE3B, have shown that they contain three helical subdomains : \n\nAt the interface of these domains a deep hydrophobic pocket is formed by residues that are highly conserved among all PDEs. This pocket is the active site and is composed of four subsites :\nThe M site is at the bottom of the hydrophobic binding pocket and contains two divalent metal binding sites. The metal ions that can bind to these sites are either zinc or magnesium. The zinc binding site has two histidine and two aspartic acid residues that are absolutely conserved among those PDE's studied to date.\nThe N-terminal portions of PDEs are widely divergent and contain determinants that are associated with regulatory properties specific to different gene families. For PDE3, those determinants are the hydrophobic membrane association domains and cAMP-dependent protein kinase phosphorylation sites.\n\nAt first, the PDE3s were purified and described as enzymes that hydrolyse both cGMP and cAMP with K values between 0.1 – 0.8 µM. However the V for cAMP hydrolysis is 4 – 10 times higher than V for cGMP hydrolysis.\nWhen different PDEs were first identified, two types of PDEs (PDE3 and PDE4) that exhibited high affinities for cAMP were isolated. PDE3 exhibited high affinity for both cGMP and cAMP, but PDE4 had high affinity for only cAMP. For that reason, the PDE3 was called the \"cGMP-inhibited PDE\" to distinguish it from PDE4.\nThe 44-amino acid insertion in the catalytic domain of PDE3s is believed to be involved in PDE3's interaction with its substrate and inhibitors, but that remains to be established.\nThe proposed molecular mechanism of cyclic nucleotide specificity of PDEs is the so-called glutamine switch mechanism.\nIn the PDEs that have had their structure solved, there seems to be an invariant glutamine residue that stabilizes the binding of the purine ring in the active site (binding pocket). The g-amino group of the glutamine residue can alternatively adopt two different orientations:\nIn PDEs that can hydrolyse both cGMP and cAMP (PDE3s), the glutamine can rotate freely and therefore switch between orientations.\n\nThe PDE3 family in mammals consists of two members, PDE3A and PDE3B. The PDE3 isoforms are structurally similar, containing an N-terminal domain important for the localization and a C-terminus end. The 44-amino acid insertion in the catalytic domain differs in the PDE3 isoforms, and the N-terminal portions of the isoforms are quite divergent. PDE3A and PDE3B have strikingly similar pharmacological and kinetic properties, but the distinction is in expression profiles and affinity for cGMP.\n\nPDE3A is mainly implicated in cardiovascular function and fertility but PDE3B is mainly implicated in lipolysis. Table 1 is an overview of localization of the PDE3 isoforms.\nIn general, PDE3 can be either cytosolic or membrane-bound and has been associated to plasma membrane, sarcoplasmic reticulum, golgi, and nucleus envelope.\nPDE3B is predominantly membrane-associated, and is localized to endoplasmic reticulum and microsomal fractions.\nPDE3A can be either membrane-associated or cytosolic, depending on the variant and the cell type it is expressed in.\n\nThe PDE3 family is composed of two genes, \"PDE3A\" and \"PDE3B\". In cells expressing both genes, \"PDE3A\" is usually dominant. Three different variants of PDE3A (PDE3A1-3) are products of alternate startcodon usage of the \"PDE3A\" gene. The \"PDE3B\" encodes a single isoform only.\n\nIn their full-length both PDE3A and PDE3B contain two N-terminal hydrophobic membrane association regions, NHR1 and NHR2 (figure 2). The difference of the PDE3A1-3 variants lies in whether they include:\nThe last can be predicted to be exclusively on soluble/cytosolic form.\nPDE3A and PDE3B activity is regulated by several phosphorylation pathways. Protein kinase A and Protein kinase B both activate PDE3A and PDE3B via phosphorylation at two different phosphorylation sites (P1 and P2) between NHR1 and NHR2 (figure 2). Hydrolysis of cAMP by PDE3 isoforms is also directly inhibited by cGMP, although PDE3B is only ≈10% as sensitive to cGMP inhibition as PDE3A.\nThe PDE3B has been extensively studied for its importance in mediating the antilipolytic and antiglycogenlytic effect of insulin in adipose and liver tissues. The activation of PDE3B in adipocytes is associated with phosphorylation of serine residue by an insulin-stimulated protein serine kinase (PDE3IK). By blocking insulin activation of PDE3IK, and in turn phosphorylation/activation of PDE3B, the antilipolytic effect of insulin can be antagonized. Activation of PDE3B decreases concentrations of cAMP, which in turn reduces Protein kinase A activity. Protein kinase A is responsible for activation of lipase, which induces lipolysis as well as other physiological pathways.\nWhether phosphorylation pathways, which regulate activity of PDE3A or PDE3B, could serve as potential drug targets rather than the catalytic domain of the PDE3 enzyme itself is unclear and beyond the scope of this text.\n\nPDE3 enzymes are involved in regulation of cardiac and vascular smooth muscle contractility. Molecules that inhibit PDE3 were originally investigated for the treatment of heart failure, but, because of unwanted arrhythmic side-effects, they are not studied for that indication any longer. Nonetheless, the PDE3 inhibitor milrinone is approved for use in heart failure in intravenous form.\nBoth PDE3A and PDE3B are expressed in vascular smooth muscle cells and are likely to modulate contraction. Their expression in vascular smooth muscle is altered under specific conditions such as elevated cAMP and hypoxia.\n\nPDE3 inhibitors:\n\nIt has been demonstrated that PDE3A inhibition prevents oocyte maturation \"in vitro\" and \"in vivo\". For example, when mice are made completely deficient of PDE3A, they become infertile.\nAggregation of platelets is highly regulated by cyclic nucleotides. PDE3A is a regulator of this process, and PDE3 inhibitors effectively prevent aggregation of platelets. Cilostazol is approved for treatment of intermittent claudication and is thought to involve inhibition of platelet aggregation and also inhibition of smooth muscle proliferation and vasodilation.\nThe most studied roles of PDE3B have been in the areas of insulin, IGF1, and leptin signaling. When PDE3B is overexpressed in β-cells in mice, it causes impaired insulin secretion and glucose intolerance.\nThe involvement of PDE3B in regulation of these important pathways has inspired researchers to begin studying the possible roles of this enzyme in disorders such as obesity and diabetes.\n\n\nFrom early studies an initial model of PDE, active site topography was derived. This early model can be summarized into the following steps concerning cAMP active site topography:\n", "id": "7856433", "title": "Phosphodiesterase 3"}
{"url": "https://en.wikipedia.org/wiki?curid=7967411", "text": "HaeIII\n\nHaeIII is one of many restriction enzymes (endonucleases) discovered since 1970. It was the third endonuclease to be isolated from the \"Haemophilus aegyptius\" bacteria, and has a molecular weight of 37126. The enzyme's recognition site—the place where it cuts DNA molecules—is the GGCC nucleotide sequence. This enzyme's gene has been sequenced and cloned.\n\nThe enzyme cleaves the DNA at the positions where the GGCC sequence is found. The cleavage occurs between the second and the third nucleotides (G and C). The resulting DNA fragments are known as restriction fragments. HaeIII cuts both strands of DNA in the same location, yielding restriction fragments with blunt ends. Heat denaturation occurs at 80°C after 20 minutes \n\n", "id": "7967411", "title": "HaeIII"}
{"url": "https://en.wikipedia.org/wiki?curid=2317437", "text": "Synonymous substitution\n\nA synonymous substitution (often called a \"silent\" substitution though they are not always silent) is the evolutionary substitution of one base for another in an exon of a gene coding for a protein, such that the produced amino acid sequence is not modified. This is possible because the genetic code is \"degenerate\", meaning that some amino acids are coded for by more than one three-base-pair codon; since some of the codons for a given amino acid differ by just one base pair from others coding for the same amino acid, a mutation that replaces the \"normal\" base by one of the alternatives will result in incorporation of the same amino acid into the growing polypeptide chain when the gene is translated. Synonymous substitutions and mutations affecting noncoding DNA are often considered silent mutations; however, it is not always the case that the mutation is silent. Synonymous mutations can affect transcription, splicing, mRNA transport, and translation, any of which could alter phenotype, rendering the synonymous mutation non-silent. The substrate specificity of the tRNA to the rare codon can affect the timing of translation, and in turn the co-translational folding of the protein. This is reflected in the codon usage bias that is observed in many species. A nonsynonymous substitution results in a change in amino acid that may be arbitrarily further classified as conservative (change to an amino acid with similar physiochemical properties), semi-conservative (e.g. negative to positively charged amino acid), or radical (vastly different amino acid).\n\nProtein translation involves a set of twenty amino acids. Each of these amino acids is coded for by a sequence of three DNA base pairs called a \"codon\". Because there are 64 possible codons, but only 20 amino acids (as well as a stop signal [i.e. the three codons that do not code for any amino acid and are known as stop codons], indicating that translation should stop), some amino acids are coded for by 2, 3, 4, or 6 different codons. For example, the codons TTT and TTC both code for the amino acid phenylalanine. This is often referred to as \"redundancy\" of the genetic code. There are two mechanisms for redundancy: several different transfer RNAs can deliver the same amino acid, or one tRNA can have a non-standard \"wobbly\" base in position three of the anti-codon, which recognises more than one base in the codon.\n\nIn the above phenylalanine example, suppose that the base in position 3 of a TTT codon got substituted to a C, leaving the codon TTC. The amino acid at that position in the protein will remain a phenylalanine. Hence, the substitution is a synonymous one.\n\nWhen a synonymous or silent mutation occurs, the change is often assumed to be neutral, meaning that it does not affect the fitness of the individual carrying the new gene to survive and reproduce.\n\nSynonymous changes may not be neutral because certain codons are translated more efficiently (faster and/or more accurately) than others. For example, when a handful of synonymous changes in the fruit fly alcohol dehydrogenase gene were introduced, changing several codons to sub-optimal synonyms, production of the encoded enzyme was reduced and the adult flies showed lower ethanol tolerance.\nMany organisms, from bacteria through animals, display biased use of certain synonymous codons. Such codon usage bias may arise for different reasons, some selective, and some neutral. In \"Saccharomyces cerevisiae\" synonymous codon usage has been shown to influence mRNA folding stability, with mRNA encoding different protein secondary structure preferring different codons.\n\nAnother reason why synonymous changes are not always neutral is the fact that exon sequences close to exon-intron borders function as RNA splicing signals. When the splicing signal is destroyed by a synonymous mutation, the exon does not appear in the final protein. This results in a truncated protein. One study found that about a quarter of synonymous variations affecting exon 12 of the cystic fibrosis transmembrane conductance regulator gene result in that exon being skipped.\n\n", "id": "2317437", "title": "Synonymous substitution"}
{"url": "https://en.wikipedia.org/wiki?curid=8036048", "text": "Protofection\n\nProtofection is the transfection of foreign mitochondrial DNA into the mitochondria of all cells in a tissue to supplement or replace the native mitochondrial DNA already present. As mitochondrial DNA becomes progressively more damaged with age, this may provide a method of at least partially rejuvenating mitochondria in old tissue, restoring them to their original, youthful function. It is thought that mitochondrial damage and dysfunction play an important role in aging: see the mitochondrial free radical theory of aging.\n\nProtofection is also a basis for constructing gene therapies for inherited genetic diseases such as Leber's hereditary optic neuropathy in which mitochondrial DNA is mutated.\n\nThis technology could similarly be applied to modified or artificial mitochondria. The intent being designing ones that do not produce as many (preferably zero) free radicals while staying as, or more efficient in generating energy in the cell. While not invulnerable to free radical damage, having less free radicals would also make such generators have longer lifespans if they could still renew at an identical rate, or at least enough to keep more healthy ones at a given time.\n\n", "id": "8036048", "title": "Protofection"}
{"url": "https://en.wikipedia.org/wiki?curid=553659", "text": "Chromatosome\n\nA chromatosome is a result of histone H1 binding to a nucleosome, which contains a histone octamer and DNA. The chromatosome contains 166 base pairs of DNA. 146 base pairs are from the DNA wrapped around the histone core of the nucleosome. The remaining 20 base pairs are from the DNA of histone H1 binding to the nucleosome. Histone H1, and its other variants, are referred to as linker histones. Protruding from the linker histone, are linker DNA. Chromatosomes are connected to each other when the linker DNA, of one chromatosome, binds to the linker histone of another chromatosome. Human genes are made up of thousands to millions of base pairs. A lot of chromatosomes are required to make up a single gene, and even more to make up the entire genome.\n\nhttps://www.rcsb.org/pdb/explore.do?structureId=4QLC\n", "id": "553659", "title": "Chromatosome"}
{"url": "https://en.wikipedia.org/wiki?curid=871418", "text": "Scleroprotein\n\nScleroproteins or fibrous proteins constitute one of the three main types of proteins (alongside globular and membrane proteins). There are many scleroprotein superfamilies including keratin, collagen, elastin, and fibroin. The roles of such proteins include protection and support, forming connective tissue, tendons, bone matrices, and muscle fiber.\n\nA scleroprotein forms long protein filaments, which are shaped like rods or wires. Scleroproteins are structural proteins or storage proteins that are typically inert and water-insoluble. A scleroprotein occurs as an aggregate due to hydrophobic side chains that protrude from the molecule.\n\nA scleroprotein's peptide sequence often has limited residues with repeats; these can form unusual secondary structures, such as a collagen helix. The structures often feature cross-links between chains (e.g., cys-cys disulfide bonds between keratin chains).\n\nScleroproteins tend not to denature as easily as globular proteins.\n\nMiroshnikov et al. (1998) are among the researchers who have attempted to synthesize fibrous proteins.\n", "id": "871418", "title": "Scleroprotein"}
{"url": "https://en.wikipedia.org/wiki?curid=8380924", "text": "Glycoprotein 130\n\nGlycoprotein 130 (also known as gp130, IL6ST, IL6-beta or CD130) is a transmembrane protein which is the founding member of the class of all cytokine receptors. It forms one subunit of the type I cytokine receptor within the IL-6 receptor family. It is often referred to as the common gp130 subunit, and is important for signal transduction following cytokine engagement. As with other type I cytokine receptors, gp130 possesses a WSXWS amino acid motif that ensures correct protein folding and ligand binding. It interacts with Janus kinases to elicit an intracellular signal following receptor interaction with its ligand. Structurally, gp130 is composed of five fibronectin type-III domains and one immunoglobulin-like C2-type (immunoglobulin-like) domain in its extracellular portion.\n\nThe members of the IL-6 receptor family all complex with gp130 for signal transduction. For example, IL-6 binds to the IL-6 Receptor. The complex of these two proteins then associates with gp130. This complex of 3 proteins then homodimerizes to form a hexameric complex which can produce downstream signals. There are many other proteins which associate with gp130, such as cardiotrophin 1 (CT-1), leukemia inhibitory factor (LIF), ciliary neurotrophic factor (CNTF), oncostatin M (OSM), and IL-11. There are also several other proteins which have structural similarity to gp130 and contain the WSXWS motif and preserved cysteine residues. Members of this group include LIF-R, OSM-R, and G-CSF-R.\n\ngp130 is an important part of many different types of signaling complexes. Inactivation of gp130 is lethal to mice. Homozygous mice who are born show a number of defects including impaired development of the ventricular myocardium. Haematopoietic effects included reduced numbers of stem cells in the spleen and liver.\n\ngp130 has no intrinsic tyrosine kinase activity. Instead, it is phosphorylated on tyrosine residues after complexing with other proteins. The phosphorylation leads to association with JAK/Tyk tyrosine kinases and STAT protein transcription factors. In particular, STAT-3 is activated which leads to the activation of many downstream genes. Other pathways activated include RAS and MAPK signaling.\n\nGlycoprotein 130 has been shown to interact with:\n", "id": "8380924", "title": "Glycoprotein 130"}
{"url": "https://en.wikipedia.org/wiki?curid=1484989", "text": "Primer walking\n\nPrimer walking is a sequencing method of choice for sequencing DNA fragments between 1.3 and 7 kilobases. Such fragments are too long to be sequenced in a single sequence read using the chain termination method. This method works by dividing the long sequence into several consecutive short ones. The DNA of interest may be a plasmid insert, a PCR product or a fragment representing a gap when sequencing a genome. The term \"primer walking\" is used where the main aim is to sequence the genome. The term \"chromosome walking\" is used instead when the sequence is known but there is no clone of a gene. For example, the gene for a disease may be located near a specific marker such as an RFLP on the sequence.\n\nThe fragment is first sequenced as if it were a shorter fragment. Sequencing is performed from each end using either universal primers or specifically designed ones. This should identify the first 1000 or so bases. In order to completely sequence the region of interest, design and synthesis of new primers (complementary to the final 20 bases of the known sequence) is necessary to obtain contiguous sequence information.\n\nThe overall process is as follows:\n\nThe method can be used to sequence entire chromosomes (hence \"chromosome walking\"). Primer walking was also the basis for the development of shotgun sequencing, which uses random primers instead of specifically chosen ones. \n", "id": "1484989", "title": "Primer walking"}
{"url": "https://en.wikipedia.org/wiki?curid=8483791", "text": "EcoRV\n\nEcoRV (pronounced \"eco R five\") is a type II restriction endonuclease isolated from certain strains of \"Escherichia coli\". It has the alternative name Eco32I.\n\nIn molecular biology, it is a commonly used restriction enzyme. It creates blunt ends. The enzyme recognizes the palindromic 6-base DNA sequence 5'-GAT|ATC-3' and makes a cut at the vertical line. The complementary sequence is then 3'-CTA|TAG-5'. The ends are blunt and can be ligated into a blunt cloning site easily but with lower efficiency than sticky ends.\n\nThe structure of this enzyme, and several mutants, in complex with the DNA sequence which it cuts has been solved by X-ray crystallography.\n\nThe core of the enzyme consists of a five-stranded mixed β-sheet flanked by α-helices. The core is conserved in all other type II restriction endonucleases. It also has an N-terminal dimerization subdomain formed by a short α-helix, a two-stranded antiparallel -sheet, and a long α-helix. This subdomain is found only in EcoRV and PvuII.\n\nLike EcoRI, EcoRV forms a homodimer in solution before binding and acting on its recognition sequence. Initially the enzyme binds weakly to a non-specific site on the DNA. It randomly walks along the molecule until the specific recognition site is found. EcoRV has a high specificity for its target DNA sequence.\n\nBinding of the enzyme induces a conformational change in the DNA, bending it by about 50°. DNA bending results in the unstacking of the bases, widening of the minor groove, and compression of the major groove. This brings the phosphodiester linkage to be broken closer to the active site of the enzyme, where it can be cleaved. Cleavage occurs within the recognition sequence, and does \"not\" require ATP hydrolysis.\n\nEcoRV is the only type II restriction endonuclease known to cause a major protein-induced conformational change in the DNA.\n\nEcoRV is often used to cut open a plasmid vector to insert a gene-of-interest during gene cloning. The enzyme is supplied by many manufacturers and requires bovine serum albumin to work properly.\n\n", "id": "8483791", "title": "EcoRV"}
{"url": "https://en.wikipedia.org/wiki?curid=28615", "text": "Sequencing\n\nIn genetics and biochemistry, sequencing means to determine the primary structure (sometimes falsely called primary sequence) of an unbranched biopolymer. Sequencing results in a symbolic linear depiction known as a sequence which succinctly summarizes much of the atomic-level structure of the sequenced molecule.\n\nDNA sequencing is the process of determining the nucleotide order of a given DNA fragment. So far, most DNA sequencing has been performed using the chain termination method developed by Frederick Sanger. This technique uses sequence-specific termination of a DNA synthesis reaction using modified nucleotide substrates. However, new sequencing technologies such as pyrosequencing are gaining an increasing share of the sequencing market. More genome data are now being produced by pyrosequencing than Sanger DNA sequencing. Pyrosequencing has enabled rapid genome sequencing. Bacterial genomes can be sequenced in a single run with several times coverage with this technique. This technique was also used to sequence the genome of James Watson recently.\n\nThe sequence of DNA encodes the necessary information for living things to survive and reproduce. Determining the sequence is therefore useful in fundamental research into why and how organisms live, as well as in applied subjects. Because of the key importance DNA has to living things, knowledge of DNA sequences is useful in practically any area of biological research. For example, in medicine it can be used to identify, diagnose, and potentially develop treatments for genetic diseases. Similarly, research into pathogens may lead to treatments for contagious diseases. Biotechnology is a burgeoning discipline, with the potential for many useful products and services.\n\nThe Carlson curve is a term coined by \"The Economist\" to describe the biotechnological equivalent of Moore's law, and is named after author Rob Carlson. Carlson accurately predicted the doubling time of DNA sequencing technologies (measured by cost and performance) would be at least as fast as Moore's law. Carlson curves illustrate the rapid (in some cases hyperexponential) decreases in cost, and increases in performance, of a variety of technologies, including DNA sequencing, DNA synthesis, and a range of physical and computational tools used in protein expression and in determining protein structures.\n\nIn chain terminator sequencing (Sanger sequencing), extension is initiated at a specific site on the template DNA by using a short oligonucleotide 'primer' complementary to the template at that region. The oligonucleotide primer is extended using a DNA polymerase, an enzyme that replicates DNA. Included with the primer and DNA polymerase are the four deoxynucleotide bases (DNA building blocks), along with a low concentration of a chain terminating nucleotide (most commonly a di-deoxynucleotide). Limited incorporation of the chain terminating nucleotide by the DNA polymerase results in a series of related DNA fragments that are terminated only at positions where that particular nucleotide is used. The fragments are then size-separated by electrophoresis in a slab polyacrylamide gel, or more commonly now, in a narrow glass tube (capillary) filled with a viscous polymer.\n\nAn alternative to the labelling of the primer is to label the terminators instead, commonly called 'dye terminator sequencing'. The major advantage of this approach is the complete sequencing set can be performed in a single reaction, rather than the four needed with the labeled-primer approach. This is accomplished by labelling each of the dideoxynucleotide chain-terminators with a separate fluorescent dye, which fluoresces at a different wavelength. This method is easier and quicker than the dye primer approach, but may produce more uneven data peaks (different heights), due to a template dependent difference in the incorporation of the large dye chain-terminators. This problem has been significantly reduced with the introduction of new enzymes and dyes that minimize incorporation variability.\nThis method is now used for the vast majority of sequencing reactions as it is both simpler and cheaper. The major reason for this is that the primers do not have to be separately labelled (which can be a significant expense for a single-use custom primer), although this is less of a concern with frequently used 'universal' primers. This is changing rapidly due to the increasing cost-effectiveness of second- and third-generation systems from Illumina, 454, ABI, Helicos, and Dover.\n\nPyrosequencing, which was developed by Pål Nyrén and Mostafa Ronaghi DNA , has been commercialized by Biotage (for low-throughput sequencing) and 454 Life Sciences (for high-throughput sequencing). The latter platform sequences roughly 100 megabases [now up to 400 megabases] in a seven-hour run with a single machine. In the array-based method (commercialized by 454 Life Sciences), single-stranded DNA is annealed to beads and amplified via EmPCR. These DNA-bound beads are then placed into wells on a fiber-optic chip along with enzymes which produce light in the presence of ATP. When free nucleotides are washed over this chip, light is produced as ATP is generated when nucleotides join with their complementary base pairs. Addition of one (or more) nucleotide(s) results in a reaction that generates a light signal that is recorded by the CCD camera in the instrument. The signal strength is proportional to the number of nucleotides, for example, homopolymer stretches, incorporated in a single nucleotide flow. \n\nWhereas the methods above describe various sequencing methods, separate related terms are used when a large portion of a genome is sequenced. Several platforms were developed to perform exome sequencing (a subset of all DNA across all chromosomes that encode genes) or whole genome sequencing (sequencing of the all nuclear DNA of a human).\n\nRNA is less stable in the cell, and also more prone to nuclease attack experimentally. As RNA is generated by transcription from DNA, the information is already present in the cell's DNA. However, it is sometimes desirable to sequence RNA molecules. While sequencing DNA gives a genetic profile of an organism, sequencing RNA reflects only the sequences that are actively expressed in the cells. To sequence RNA, the usual method is first to reverse transcribe the RNA extracted from the sample to generate cDNA fragments. This can then be sequenced as described above.\nThe bulk of RNA expressed in cells are ribosomal RNAs or small RNAs, detrimental for cellular translation, but often not the focus of a study. This fraction can fortunately be removed \"in vitro\", however, to enrich for the messenger RNA, also included, that usually is of interest. Derived from the exons these mRNAs are to be later translated to proteins that support particular cellular functions. The expression profile therefore indicates cellular activity, particularly desired in the studies of diseases, cellular behaviour, responses to reagents or stimuli. Eukaryotic RNA molecules are not necessarily co-linear with their DNA template, as introns are excised. This gives a certain complexity to map the read sequences back to the genome and thereby identify their origin.\nFor more information on the capabilities of next-generation sequencing applied to whole transcriptomes see: RNA-Seq and MicroRNA Sequencing.\n\nMethods for performing protein sequencing\ninclude:\n\nIf the gene encoding the protein is known, it is currently much easier to sequence the DNA and infer the protein sequence. Determining part of a protein's amino-acid sequence (often one end) by one of the above methods may be sufficient to identify a clone carrying this gene.\n\nThough polysaccharides are also biopolymers, it is not so common to talk of 'sequencing' a polysaccharide, for several reasons. Although many polysaccharides are linear, many have branches. Many different units (individual monosaccharides) can be used, and bonded in different ways. However, the main theoretical reason is that whereas the other polymers listed here are primarily generated in a 'template-dependent' manner by one processive enzyme, each individual join in a polysaccharide may be formed by a different enzyme. In many cases the assembly is not uniquely specified; depending on which enzyme acts, one of several different units may be incorporated. This can lead to a family of similar molecules being formed. This is particularly true for plant polysaccharides. Methods for the structure determination of oligosaccharides and polysaccharides include NMR spectroscopy and methylation analysis.\n\n", "id": "28615", "title": "Sequencing"}
{"url": "https://en.wikipedia.org/wiki?curid=1635294", "text": "Multilocus sequence typing\n\nMultilocus sequence typing (MLST) is a technique in molecular biology for the typing of multiple loci. The procedure characterizes isolates of microbial species using the DNA sequences of internal fragments of multiple housekeeping genes. Approximately 450-500 bp internal fragments of each gene are used, as these can be accurately sequenced on both strands using an automated DNA sequencer. For each housekeeping gene, the different sequences present within a bacterial species are assigned as distinct alleles and, for each isolate, the alleles at each of the loci define the allelic profile or sequence type (ST).\n\nThe first MLST scheme to be developed was for \"Neisseria meningitidis\", the causative agent of meningococcal meningitis and septicaemia. Since its introduction for the research of evolutionary history, MLST has been used not only for human pathogens but also for plant pathogens.\n\nMLST directly measures the DNA sequence variations in a set of housekeeping genes and characterizes strains by their unique allelic profiles. The principle of MLST is simple: the technique involves PCR amplification followed by DNA sequencing. Nucleotide differences between strains can be checked at a variable number of genes depending on the degree of discrimination desired.\n\nThe workflow of MLST involves: 1) data collection, 2) data analysis and 3) multilocus sequence analysis. In the data collection step, definitive identification of variation is obtained by nucleotide sequence determination of gene fragments. In the data analysis step, all unique sequences are assigned allele numbers and combined into an allelic profile and assigned a sequence type (ST). If new alleles and STs are found, they are stored in the database after verification. In the final analysis step of MLST, the relatedness of isolates are made by comparing allelic profiles. Researchers do epidemiological and phylogenetical studies by comparing STs of different clonal complexes. A huge set of data is produced during the sequencing and identification process so bioinformatic techniques are used to arrange, manage, analyze and merge all of the biological data.\n\nTo strike the balance between the acceptable identification power, time and cost for the strain typing, about seven to eight house-keeping genes are commonly used in the laboratories. Quoting \"Staphylococcus aureus\" as an example, seven housekeeping genes are used in MLST typing. These genes include carbamate kinase (\"arcC\"), shikimate dehydrogenase (\"aroE\"), glycerol kinase (\"glpF\"), guanylate kinase (\"gmk\"), phosphate acetyltransferase (\"pta\"), triosephosphate isomerase (\"tpi\") and acetyl coenzyme A acetyltransferase (\"yqiL\") as specified by the MLST website. However, it is not uncommon for up to ten housekeeping genes to be used. For \"Vibrio vulnificus\", the housekeeping genes used are glucose-6-phosphate isomerase (\"glp\"), DNA gyrase, subunit B (\"gyrB\"), malate-lactate dehydrogenase (\"mdh\"), methionyl-tRNA synthetase (\"metG\"), phosphoribosylaminoimidazole synthetase (\"purM\"), threonine dehyrogenase (\"dtdS\"), diaminopimelate decarboxylase (\"lysA\"), transhydrogenase alpha subunit (\"pntA\"), dihydroorotase (\"pyrC\") and tryptophanase (\"tnaA\"). Thus both the number and type of housekeeping genes interrogated by MLST may differ from species to species.\n\nFor each of these housekeeping genes, the different sequences are assigned as alleles and the alleles at the loci provide an allelic profile. A series of profiles can then be the identification marker for strain typing. Sequences that differ at even a single nucleotide are assigned as different alleles and no weighting is given to take into account the number of nucleotide differences between alleles, as we cannot distinguish whether differences at multiple nucleotide sites are a result of multiple point mutations or a single recombinational exchange. The large number of potential alleles at each of the loci provides the ability to distinguish billions of different allelic profiles, and a strain with the most common allele at each locus would only be expected to occur by chance approximately once in 10,000 isolates. Despite MLST providing high discriminatory power, the accumulation of nucleotide changes in housekeeping genes is a relatively slow process and the allelic profile of a bacterial isolate is sufficiently stable over time for the method to be ideal for global epidemiology.\n\nThe relatedness of isolates is displayed as a dendrogram constructed using the matrix of pairwise differences between their allelic profiles, eBURST or a minimum spanning tree (MST). The dendrogram is only a convenient way of displaying those isolates that have identical or very similar allelic profiles that can be assumed to be derived from a common ancestor; the relationships between isolates that differ at more than three out of seven loci are likely to be unreliable and should not be taken to infer their phylogeny. The MST connects all samples in such a way that the summed distance of all branches of the tree is minimal.\n\nAlternatively, the relatedness of isolates can also be analysed with MultiLocus Sequence Analysis (MLSA). This does not use the assigned alleles, but instead concatenates the sequences of the gene fragments of the housekeeping genes and uses this concatenated sequence to determine phylogenetic relationships. In contrast to MLST, this analysis does assign a higher similarity between sequences differing only a single nucleotide and a lower similarity between sequences with multiple nucleotide differences. As a result, this analysis is more suitable for organisms with a clonal evolution and less suitable for organisms in which recombinational events occur very often. It can also be used to determine phylogenetic relationships between closely related species. The terms MLST and MLSA are very often considered interchangeable. This is however not correct as each analysis method has its distinctive features and uses. Care should be taken to use the correct term.\n\nEarlier serological typing approaches had been established for differentiating bacterial isolates, but immunological typing has drawbacks such as reliance on few antigenic loci and unpredictable reactivities of antibodies with different antigenic variants. Several molecular typing schemes have been proposed to determine the relatedness of pathogens such as pulsed-field gel electrophoresis (PFGE), ribotyping, and PCR-based fingerprinting. But these DNA banding-based subtyping methods do not provide meaningful evolutionary analyses. Despite PFGE being considered by many researchers as the “gold standard”, many strains are not typable by this technique due to the degradation of the DNA during the process (gel smears).\n\nThe approach of MLST is distinct from Multi locus enzyme electrophoresis (MLEE), which is based on different electrophoretic mobilities (EM) of multiple core metabolic enzymes. The alleles at each locus define the EM of their products, as different amino acid sequences between enzymes result in different mobilities and distinct bands when run on a gel. The relatedness of isolates can then be visualized with a dendrogram generated from the matrix of pairwise differences between the electrophoretic types. This method has a lower resolution than MLST for several reasons, all arising from the fact that enzymatic phenotype diversity is merely a proxy for DNA sequence diversity. First, enzymes may have different amino acid sequences without having sufficiently different EM to give distinct bands. Second, \"silent mutations\" may alter the DNA sequence of a gene without altering the encoded amino acids. Thirdly, the phenotype of the enzyme can easily be altered in response to environmental conditions and badly affect the reproducibility of MLEE results - common modifications of enzymes are phosphorylation, cofactor binding and cleavage of transport sequences. This also limits comparability of MLEE data obtained by different laboratories, whereas MLST provides portable and comparable DNA sequence data and has great potential for automation and standardization.\n\nMLST should not be confused with DNA barcoding. The latter is a taxonomic method that uses short genetic markers to recognize particular species of eukaryotes. It is based on the fact that mitochondrial DNA (mtDNA) or some parts of the ribosomal DNA cistron have relatively fast mutation rates, which give significant variation in sequences between species. mtDNA methods are only possible in eukaryotes (as prokaryotes lack mitochondria), whereas MLST, although initially developed for prokaryotes, is now finding application in eukaryotes and in principle could be applied to any kingdom.\n\nMLST is highly unambiguous and portable. Materials required for ST determination can be exchanged between laboratories. Primer sequences and protocols can be accessed electronically. It is reproducible and scalable. MLST is automated, combines advances in high throughput sequencing and bioinformatics with established population genetics techniques. MLST data can be used to investigate evolutionary relationships among bacteria. MLST provides good discriminatory power to differentiate isolates.\n\nThe application of MLST is huge, and provides a resource for the scientific, public health, and veterinary communities as well as the food industry. The following are examples of MLST applications.\n\n\"Campylobacter\" is the common causative agent for bacterial infectious intestinal diseases, usually arising from undercooked poultry or unpasteurised milk. However, its epidemiology is poorly understood since outbreaks are rarely detected, so that the sources and transmission routes of outbreak are not easily traced. In addition, \"Campylobacter\" genomes are genetically diverse and unstable with frequent inter- and intragenomic recombination, together with phase variation, which complicates the interpretation of data from many typing methods. Until recently, with the application of MLST technique, \"Campylobacter\" typing has achieved a great success and added onto the MLST database. As at 1 May 2008, the \"Campylobacter\" MLST database contains 3516 isolates and about 30 publications that use or mention MLST in research on \"Campylobacter\" (http://pubmlst.org/campylobacter/).\n\nMLST has provided a more richly textured picture of bacteria within human populations and on strain variants that may be pathogenic to human, plants and animals. MLST technique was first used by Maiden et al. (1) to characterize \"Neisseria meningitidis\" using six loci. The application of MLST has clearly resolved the major meningococcal lineages known to be responsible for invasive disease around the world. To improve the level of discriminatory power between the major invasive lineages, seven loci are now being used and have been accepted by many laboratories as the method of choice for characterizing meningococcal isolates. It is a well known fact that recombinational exchanges commonly occur in \"N. meningitidis\", leading to rapid diversification of meningococcal clones. MLST has successfully provided a reliable method for characterization of clones within other bacterial species in which the rates of clonal diversification are generally lower.\n\n\"S. aureus\" causes a number of diseases. Methicillin-resistant \"S. aureus\" (MRSA) has generated growing concerns over its resistance to almost all antibiotics except vancomycin. However, most serious \"S. aureus\" infections in the community, and many in hospitals, are caused by methicillin-susceptible isolates (MSSA) and there have been few attempts to identify the hypervirulent MSSA clones associated with serious disease. MLST was therefore developed to provide an unambiguous method of characterizing MRSA clones and for the identification of the MSSA clones associated with serious disease.\n\n\"S. pyogenes\" causes diseases ranging from pharyngitis to life-threatening impetigo including necrotizing fasciitis. An MLST scheme for \"S. pyogenes\" has been developed. At present, the database (mlst.net) contains the allelic profiles of isolates that represent the worldwide diversity of the organism and isolates from serious invasive disease.\n\n\"C. albicans\" is a fungal pathogen of humans and is responsible for hospital-acquired bloodstream infections. MLST technique has used to characterize \"C. albicans\" isolates. Combination of the alleles at the different loci results in unique diploid sequence types that can be used to discriminate strains. MLST has been shown successfully applied to study the epidemiology of \"C. albicans\" in the hospital as well as the diversity of \"C. albicans\" isolates obtained from diverse ecological niches including human and animal hosts.\n\nThe \"Cronobacter\" genus is composed of 7 species. Before 2007, the single species name \"Enterobacter sakazakii\" was applied to these organisms. The \"Cronobacter\" MLST was initially applied to distinguish between \"C. sakazakii\" and \"C. malonaticus\" because 16S rDNA sequencing is not always accurate enough, and biotyping is too subjective. The \"Cronobacter\" MLST scheme uses 7 alleles; \"atpD\", \"fusA\", \"glnS\", \"gltB\", \"gyrB\", \"infB\" and \"ppsA\" giving a concatenated sequence of 3036 bp for phylogenetic analysis (MLSA) and comparative genomics. MLST has also been used in the formal recognition of new \"Cronobacter\" species. The method has revealed a strong association between one genetic lineage, sequence type 4 (ST4), and cases of neonatal meningitis., The \"Cronobacter\" MLST site is at http://www.pubMLST.org/cronobacter.\n\nMLST appears best in population genetic study but it is expensive. Due to the sequence conservation in housekeeping genes, MLST sometimes lacks the discriminatory power to differentiate bacterial strains, which limits its use in epidemiological investigations. To improve the discriminatory power of MLST, a multi-virulence-locus sequence typing (MVLST) approach has been developed using \"Listeria monocytogenes\" . MVLST broadens the benefits of MLST but targets virulence genes, which may be more polymorphic than housekeeping genes. Population genetics is not the only relevant factor in an epidemic. Virulence factors are also important in causing disease, and population genetic studies struggle to monitor these. This is because the genes involved are often highly recombining and mobile between strains in comparison with the population genetic framework. Thus, for example in \"Escherichia coli\", identifying strains carrying toxin genes is more important than having a population genetics-based evaluation of prevalent strains.\n\nThe advent of second-generation sequencing technologies has made it possible to obtain sequence information across the entire bacterial genome at relatively modest cost and effort, and MLST can now be assigned from whole-genome sequence information, rather than sequencing each locus separately as was the practice when MLST was first developed. Whole-genome sequencing provides richer information for differentiating bacterial strains (MLST uses approximately 0.1% of the genomic sequence to assign type while disregarding the rest of the bacterial genome). For example, whole-genome sequencing of numerous isolates has revealed the single MLST lineage ST258 of \"Klebsiella pneumoniae\" comprises two distinct genetic clades, providing additional information about the evolution and spread of these multi-drug resistant organisms, and disproving the previous hypothesis of a single clonal origin for ST258.\n\nMLST databases contain the reference allele sequences and sequence types for each organism, and also isolate epidemiological data. The websites contain interrogation and analysis software which allow users to query their allele sequences and sequence types. MLST is widely used as a tool for researchers and public healthcare workers.\n\nThe majority of MLST databases are hosted at 2 web servers currently located at Imperial College, London (mlst.net) and in Oxford University (pubmlst.org).\n\nThe databases hosted at each site are different and hold the organism specific reference allele sequences and lists of STs for individual organisms.\n\nTo assist the gathering and formatting of the utilized sequences a simple and free plug-in for Firefox has been developed (link).\n\n\n", "id": "1635294", "title": "Multilocus sequence typing"}
{"url": "https://en.wikipedia.org/wiki?curid=188183", "text": "Transactivation\n\nIn the context of gene regulation: transactivation is the increased rate of gene expression triggered either by biological processes or by artificial means, through the expression of an intermediate transactivator protein. \n\nIn the context of receptor signaling, transactivation occurs when one or more receptors activate yet another; receptor transactivation may result from the crosstalk of signaling cascades.\n\nTransactivation can be triggered either by endogenous cellular or viral proteins, also called transactivators. These protein factors act in trans (\"i.e.\", intermolecularly). HIV and HTLV are just two of the many viruses that encode transactivators to enhance viral gene expression. These transactivators can also be linked to cancer if they start interacting with, and increasing expression of, a cellular proto-oncogene. HTLV, for instance, has been associated with causing leukemia primarily through this process. Its transactivator, \"Tax\"\",\" can interact with p40, inducing overexpression of interleukin 2, interleukin receptors, GM-CSF and the transcription factor c-Fos. HTLV infects T-cells and via the increased expression of these stimulatory cytokines and transcription factors, leads to uncontrolled proliferation of T-cells and hence lymphoma.\n\nArtificial transactivation of a gene is achieved by inserting it into the genome at the appropriate area as transactivator gene adjoined to special promoter regions of DNA. The transactivator gene expresses a transcription factor that binds to specific promoter region of DNA. By binding to the promoter region of a gene, the transcription factor causes that gene to be expressed. The expression of one transactivator gene can activate multiple genes, as long as they have the same, specific promoter region attached. Because the expression of the transactivator gene can be controlled, transactivation can be used to turn genes on and off. If this specific promoter region is also attached to a reporter gene, we can measure when the transactivator is being expressed.\n\n", "id": "188183", "title": "Transactivation"}
{"url": "https://en.wikipedia.org/wiki?curid=3953835", "text": "FLAG-tag\n\nFLAG-tag, or FLAG octapeptide, or FLAG epitope, is a polypeptide protein tag that can be added to a protein using recombinant DNA technology, having the sequence motif DYKDDDDK (where D=aspartic acid, Y=tyrosine, and K=lysine). It is an artificial antigen to which specific, high affinity monoclonal antibodies have been developed and hence can be used for protein purification by affinity chromatography and also can be used for locating proteins with in living cells. It has been used to separate recombinant, overexpressed protein from wild-type protein expressed by the host organism. It can also be used in the isolation of protein complexes with multiple subunits, because its mild purification procedure tends not to disrupt such complexes. It has been used to obtain proteins of sufficient purity and quality to carry out 3D structure determination by x-ray crystallography.\n\nA FLAG-tag can be used in many different assays that require recognition by an antibody. If there is no antibody against a given protein, adding a FLAG-tag to a protein allows the protein to be studied with an antibody against the FLAG sequence. Examples are cellular localization studies by immunofluorescence or detection by SDS PAGE protein electrophoresis and Western blotting.\n\nThe peptide sequence of the FLAG-tag from the N-terminus to the C-terminus is: DYKDDDDK (1012 Da). Additionally, it may be used in tandem, commonly the 3xFLAG peptide: DYKDHD-G-DYKDHD-I-DYKDDDDK (with the final tag encoding an enterokinase cleavage site). It can be fused to the C-terminus or the N-terminus of a protein, or inserted within a protein. Some commercially available antibodies (e.g., M1/4E11) recognize the epitope only when it is present at the N-terminus. However, other available antibodies (e.g., M2) are position-insensitive. The tyrosine residue in the FLAG-tag can be sulfated, which can affect antibody recognition of the FLAG epitope. The FLAG-tag can be used in conjunction with other affinity tags, for example a polyhistidine tag (His-tag), HA-tag or myc-tag.\n\nThe first use of epitope tagging was described by Munro and Pelham in 1984. The FLAG-tag was the second example of a fully functional, improved epitope tag, published in the scientific literature. and was the only epitope tag to be patented. It has since become the most commonly used protein tag in laboratories worldwide. Unlike some other tags (e.g. myc, HA), where a monoclonal antibody was first isolated against an existing protein, then the epitope was characterized and used as a tag, the FLAG epitope was an idealized, artificial design, to which monoclonal antibodies were raised. The FLAG tag's structure was optimized for compatibility with proteins it is attached to, in that it is more hydrophilic than other common epitope tags and therefore less likely to denature or inactivate proteins to which it is appended. In addition, N-terminal FLAG tags can be removed readily from proteins once they have been isolated, by treatment with the specific protease, enterokinase (enteropeptidase).\n\nThe third report of epitope tagging, (HA-tag), appeared about one year after the Flag system had been first shipped.\n", "id": "3953835", "title": "FLAG-tag"}
{"url": "https://en.wikipedia.org/wiki?curid=3229799", "text": "Alpha globulin\n\nAlpha globulins are a group of globular proteins in plasma that are highly mobile in alkaline or electrically charged solutions. They inhibit certain blood proteases and show significant inhibitor activity.\n\nThe alpha globulins typically have molecular weights of around 93 kDa.\n\n", "id": "3229799", "title": "Alpha globulin"}
{"url": "https://en.wikipedia.org/wiki?curid=7362120", "text": "Orosomucoid\n\nOrosomucoid (\"ORM\") or alpha-1-acid glycoprotein (\"αAGp\", \"AGP\" or \"AAG\") is an acute phase (acute phase protein) plasma alpha-globulin glycoprotein and is modulated by two polymorphic genes. It is synthesized primarily in hepatocytes and has a normal plasma concentration between 0.6-1.2 mg/mL (1-3% plasma protein). Plasma levels are affected by pregnancy, burns, certain drugs, and certain diseases, particularly HIV.\n\nThe only established function of ORM is to act as a carrier of basic and neutrally charged lipophilic compounds. In medicine, it is known as the primary carrier of basic (positively charged) drugs (whereas albumin carries acidic (negatively charged) and neutral drugs), steroids, and protease inhibitors. Aging causes a small decrease in plasma albumin levels; if anything, there is a small increase in alpha-1-acid glycoprotein. The effect of these changes on drug protein binding and drug delivery, however, appear to be minimal. AGP shows a complex interaction with thyroid homeostasis: ORM in low concentrations was observed to stimulate the thyrotropin (TSH) receptor and intracellular accumulation of cyclic AMP. High AGP concentrations, however, inhibited TSH signalling.\n\nAlpha-1-acid glycoprotein has been identified as one of four potentially useful circulating biomarkers for estimating the five-year risk of all-cause mortality (the other three are albumin, very low-density lipoprotein particle size, and citrate).\n\nOrosomucoid increases in amount in obstructive jaundices while diminishes in hepatocellular jaundice and in intestinal infections.\n", "id": "7362120", "title": "Orosomucoid"}
{"url": "https://en.wikipedia.org/wiki?curid=9313361", "text": "NASBA (molecular biology)\n\nNucleic acid sequence based amplification (NASBA) is a method in molecular biology which is used to amplify RNA sequences.\n\nNASBA was developed by J Compton in 1991, who defined it as \"a primer-dependent technology that can be used for the continuous amplification of nucleic acids in a single mixture at one temperature\". Immediately after the invention of NASBA it was used for the rapid diagnosis and quantification of HIV-1 in patient sera. Although RNA can also be amplified by PCR using a reverse transcriptase (in order to synthesize a complementary DNA strand as a template), NASBA's main advantage is that it works at isothermic conditions – usually at a constant temperature of 41 °C. NASBA can be used in medical diagnostics as an alternative to PCR that is quicker and more sensitive in some circumstances.\n\nExplained briefly, NASBA works as follows:\n\nThe NASBA technique has been used to develop rapid diagnostic tests for several pathogenic viruses with single-stranded RNA genomes, e.g. influenza A, foot-and-mouth disease virus, severe acute respiratory syndrome (SARS)-associated coronavirus, human bocavirus (HBoV) and also parasites like \"Trypanosoma brucei\".\n", "id": "9313361", "title": "NASBA (molecular biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=378917", "text": "Intracellular\n\nIn cell biology, molecular biology and related fields, the word intracellular means \"inside the cell\".\n\nIt is used in contrast to extracellular (outside the cell). The cell membrane (and, in many organisms, the cell wall) is the barrier between the two, and chemical composition of intra- and extracellular \"milieu\" (\"Milieu intérieur\") can be radically different. In most organisms, for example, a Na+/K+ ATPase maintains a high potassium level inside cells while keeping sodium low, leading to chemical excitability. \n\n", "id": "378917", "title": "Intracellular"}
{"url": "https://en.wikipedia.org/wiki?curid=9505399", "text": "Electrochromatography\n\nElectrochromatography is a chemical separation technique in analytical chemistry, biochemistry and molecular biology used to resolve and separate mostly large biomolecules such as proteins. It is a combination of size exclusion chromatography (gel filtration chromatography) and gel electrophoresis. These separation mechanisms operate essentially in superposition along the length of a gel filtration column to which an axial electric field gradient has been added. The molecules are separated by size due to the gel filtration mechanism and by electrophoretic mobility due to the gel electrophoresis mechanism. Additionally there are secondary chromatographic solute retention mechanisms.\n\nCapillary electrochromatography (CEC) is an electrochromatography technique in which the liquid mobile phase is driven through a capillary containing the chromatographic stationary phase by electroosmosis. It is a combination of high-performance liquid chromatography and capillary electrophoresis. The capillaries is packed with HPLC stationary phase and a high voltage is applied to achieve separation is achieved by electrophoretic migration of the analyte and differential partitioning in the stationary phase.\n\n", "id": "9505399", "title": "Electrochromatography"}
{"url": "https://en.wikipedia.org/wiki?curid=9508543", "text": "Prokaryotic initiation factor\n\nProkaryotes require the use of three initiation factors: IF1, IF2, and IF3, for translation.\n\nProkaryotic initiation factor-1 associates with the 30S ribosomal subunit in the A site and prevents an aminoacyl-tRNA from entering. It modulates IF2 binding to the ribosome by increasing its affinity. It may also prevent the 50S subunit from binding, stopping the formation of the 70S subunit. It also contains a β-domain fold common for nucleic acid binding proteins .\n\nProkaryotic initiation factor-2 binds to an initiator tRNA and controls the entry of that tRNA into the ribosome. IF2, bound to GTP, binds to the 30S P site. After associating with the 30S subunit, fMet-tRNA binds to the IF2 then IF2 transfers the tRNA into the partial P site. When the 50S subunit joins, it hydrolyzes GTP to GDP and P, causing a conformational change in the IF2 that causes IF2 to release and allow the 70S subunit to form.\n\nProkaryotic initiation factor-3 is not universally found in all bacterial species but in \"E. coli\" it is required for the 30S subunit to bind to the initiation site in mRNA. In addition, it has several other jobs including stabilization of free 30S subunits, facilitation of 30S subunits binding to mRNA and checking for accuracy against the first aminoacyl-tRNA. It also allows for rapid codon-anticodon pairing for the initiator tRNA to bind quickly to. IF3 is required by the small subunit to form initiation complexes, but has to be released to allow the 50S subunit to bind.\n", "id": "9508543", "title": "Prokaryotic initiation factor"}
{"url": "https://en.wikipedia.org/wiki?curid=9508538", "text": "Eukaryotic initiation factor\n\nEukaryotic initiation factors (eIFs) are proteins or protein complexes involved in the initiation phase of eukaryotic translation. These proteins help stabilize the formation of ribosomal preinitiation complexes around the start codon and are an important input for post-transcription gene regulation. Several initiation factors form a complex with the small 40S ribosomal subunit and Met-tRNA called the 43S preinitiation complex (43S PIC). Additional factors of the eIF4F complex (eIF4A, E, and G) recruit the 43S PIC to the five-prime cap structure of the mRNA, from which the 43S particle scans 5'-->3' along the mRNA to reach an AUG start codon. Recognition of the start codon by the Met-tRNA promotes gated phosphate and eIF1 release to form the 48S preinitiation complex (48S PIC), followed by large 60S ribosomal subunit recruitment to form the 80S ribosome. There exist many more eukaryotic initiation factors than prokaryotic initiation factors, reflecting the greater biological complexity of eukaryotic translation. There are at least twelve eukaryotic initiation factors, composed of many more polypeptides, and these are described below.\n\neIF1 and eIF1A both bind to the 40S ribosome subunit-mRNA complex. Together they induce an \"open\" conformation of the mRNA binding channel, which is crucial for scanning, tRNA delivery, and start codon recognition. In particular, eIF1 dissociation from the 40S subunit is considered to be a key step in start codon recognition.\n\neIF1 and eIF1A are small proteins (13 and 16 kDa, respectively in humans) and are both components of the 43S PIC. eIF1 binds near the ribosomal P-site, while eIF1A binds near the A-site, in a manner similar to the structurally and functionally related bacterial counterparts IF3 and IF1, respectively.\n\neIF2 is the main protein complex responsible for delivering the initiator tRNA to the P-site of the preinitiation complex, as a ternary complex containing Met-tRNA and GTP (the eIF2-TC). eIF2 has specificity for the methionine-charged initiator tRNA, which is distinct from other methionine-charged tRNAs used for elongation of the polypeptide chain. Following placement of the initiator tRNA on the AUG start codon in the P-site, eIF1 dissociates and eIF2 switches to the GDP-bound form via gated phosphate release. This hydrolysis also signals for the dissociation of eIF3, eIF1, and eIF1A, and allows the large subunit to bind. This signals the beginning of elongation.\n\neIF2 has three subunits, eIF2-α, β, and γ. The former α-subunit is a target of regulatory phosphorylation and is of particular importance for cells that may need to turn off protein synthesis globally as a response to cell signaling events. When phosphorylated, it sequesters eIF2B (not to be confused with eIF2β), a GEF. Without this GEF, GDP cannot be exchanged for GTP, and translation is repressed. One example of this is the eIF2α-induced translation repression that occurs in reticulocytes when starved for iron. In the case of viral infection, protein kinase R (PKR) phosphorylates eIF2α when dsRNA is detected in many multicellular organisms, leading to cell death.\n\nThe proteins eIF2A and eIF2D are both technically named 'eIF2' but neither are part of the eIF2 heterotrimer and they seem to play unique functions in translation. Instead, they appear to be involved in specialized pathways, such as 'eIF2-independent' translation initiation or re-initiation, respectively.\n\neIF3 independently binds the 40S ribosomal subunit, multiple initiation factors, and cellular and viral mRNA.\n\nIn mammals, eIF3 is the largest initiation factor, made up of 13 subunits (a-m). It has a molecular weight of ~800 kDa and controls the assembly of the 40S ribosomal subunit on mRNA that have a 5' cap or an IRES. eIF3 may use the eIF4F complex, or alternatively during internal initiation, an IRES, to position the mRNA strand near the exit site of the 40S ribosomal subunit, thus promoting the assembly of a functional pre-initiation complex.\n\nIn many human cancers, eIF3 subunits are overexpressed (subunits a, b, c, h, i, and m) and underexpressed (subunits e and f). One potential mechanism to explain this disregulation comes from the finding that eIF3 binds a specific set of cell proliferation regulator mRNA transcripts and regulates their translation. eIF3 also mediates cellular signaling through S6K1 and mTOR/Raptor to effect translational regulation.\n\nThe eIF4F complex is composed of three subunits: eIF4A, eIF4E, and eIF4G. Each subunit has multiple human isoforms and there exist additional eIF4 proteins: eIF4B and eIF4H.\n\neIF4G is a 175.5-kDa scaffolding protein that interacts with eIF3 and the Poly(A)-binding protein (PABP), as well as the other members of the eIF4F complex. eIF4E recognizes and binds to the 5' cap structure of mRNA, while eIF4G binds PABP, which binds the poly(A) tail, potentially circularizing and activating the bound mRNA. eIF4Aa DEAD box RNA helicaseis important for resolving mRNA secondary structures.\n\neIF4B contains two RNA-binding domainsone non-specifically interacts with mRNA, whereas the second specifically binds the 18S portion of the small ribosomal subunit. It acts as an anchor, as well as a critical co-factor for eIF4A. It is also a substrate of S6K, and when phosphorylated, it promotes the formation of the pre-initiation complex. In vertebrates, eIF4H is an additional initiation factor with similar function to eIF4B.\n\neIF5 is a GTPase-activating protein, which helps the large ribosomal subunit associate with the small subunit. It is required for GTP-hydrolysis by eIF2 and contains the unusual amino acid hypusine.\n\neIF5B is a GTPase, and is involved in assembly of the full ribosome. It is the functional eukaryotic analog of bacterial IF2.\n\neIF6 performs the same inhibition of ribosome assembly as eIF3, but binds with the large subunit.\n\n", "id": "9508538", "title": "Eukaryotic initiation factor"}
{"url": "https://en.wikipedia.org/wiki?curid=9361398", "text": "Lipofectamine\n\nLipofectamine or Lipofectamine 2000 is a common transfection reagent, produced and sold by Invitrogen, used in molecular and cellular biology. It is used to increase the transfection efficiency of RNA (including mRNA and siRNA) or plasmid DNA into in vitro cell cultures by lipofection. Lipofectamine reagent contains lipid subunits that can form liposomes in an aqueous environment, which entrap the transfection payload, i.e. DNA plasmids. \n\nLipofectamine is a cationic liposome formulation, which complexes with negatively charged nucleic acid molecules to allow them to overcome the electrostatic repulsion of the cell membrane. Lipofectamine's cationic lipid molecules are formulated with a neutral co-lipid (helper lipid). The DNA-containing liposomes (with positive charge on their surfaces) can fuse with the negatively charged plasma membrane of living cells, due to the neutral co-lipid mediating fusion of the liposome with the cell membrane, allowing nucleic acid to cross into the cytoplasm and contents to be available to the cell for replication or expression. \n\nIn order for a cell to express this transgene, the nucleic acid must reach the nucleus of the cell to begin transcription. This process involves many risks. The transfected genetic material may never reach the nucleus in the first place, instead being disrupted somewhere along the delivery process. In dividing cells, the material may reach the nucleus by being trapped in the reassembling nuclear envelope following mitosis. But also in non-dividing cells, research has shown that Lipofectamine improves the efficiency of transfection, which suggests that it additionally helps the transfected genetic material penetrate the intact nuclear envelope.\n\nThis method of transfection was invented by Dr. Yongliang Chu at Life Technologies, Inc.\n\n", "id": "9361398", "title": "Lipofectamine"}
{"url": "https://en.wikipedia.org/wiki?curid=9059630", "text": "Zoo blot\n\nA zoo blot or garden blot is a type of Southern blot that demonstrates the similarity between specific, usually protein-coding, DNA sequences of different species. A zoo blot compares animal species while a garden blot compares plant species. The purpose of the zoo blot is to detect the conservation of the gene(s) of interest throughout the evolution of different species.\n\nIn order to understand the degree to which a particular gene is similar from species to species, DNA extracts from a set of species are isolated and spread over a surface. Then, a gene probe specific to one of the species is labeled and allowed to hybridize to the prepared DNA. Usually, the probe is marked with a radioactive isotope of phosphorus. Following the hybridization, autoradiography or other imaging techniques are used to identify successfully hybridized probes, proof of similarity between species' genomes. \n\nThe hybridization between a probe and a segment of DNA will happen even when the strands are similar but not identical. As a result, zoo blotting is used to detect similar or exact relationships between the DNA in question and other organisms. It can also help establish the locations of introns and exons, as the latter will be far more conserved than the former.\n\n", "id": "9059630", "title": "Zoo blot"}
{"url": "https://en.wikipedia.org/wiki?curid=3284977", "text": "P element\n\nP elements are transposable elements that were discovered in Drosophila as the causative agents of genetic traits called hybrid dysgenesis. The transposon is responsible for P trait of P element and it is found only in wild flies.\n\nAll P elements have a canonical structure contain 31 bp terminal inverted repeats and 11 bp internal inverted repeats located at THAP domain of the transposase. The shorter and longest P elements are nonautonomous elements. The longest \"P\" elements encode transposase needed for transposition.\n\nIn hybrid dysgenesis, one strain of \"Drosophila\" mates with another to produce hybrid offspring cause chromosomal damage known to be dysgenic. Hybrid dysgenesis requires a contribution from both parents. For example, in the \"P-M system,\" P strain contributing paternal \"and\" M strain contributing maternal. The reverse cross, with M father and P mother, produces normal offspring, as it crosses P x P or M x M manner. P male chromosome can cause dysgenesis when cross with an M female.\n\nP element also encodes a suppressor of transposition, which accumulates in the cytoplasm during the development of cells. Thus, in a cross of a P or M male with a P female, the female cytoplasm contains the suppressor, which binds to any P elements and prevents their transposition.\n\nP elements are commonly used as mutagenic agents in genetic experiments with \"Drosophila.\" One advantage of this approach is that the mutations are easy to locate .\n\nThe P element encodes for the protein P transposase and is flanked by terminal inverted repeats which are important for its mobility. Unlike laboratory strain females, wild type females are thought to express an inhibitor to P transposase function. This inhibitor reduces the disruption to the genome caused by the P elements, allowing fertile progeny. Evidence for this comes from crosses of laboratory females (which lack P transposase inhibitor) with wild type males (which have P elements). In the absence of the inhibitor, the P elements can proliferate throughout the genome, disrupting many genes and killing progeny.\n\nThe P element is a class II transposon, and moves by a DNA-based \"cut and paste\" mechanism. The sequence comprises 4 exons with 3 introns. Complete splicing of the introns produces the transposase enzyme, while alternative partial splicing of intron 1 and 2 leaving in only intron 3 encodes the P element repressor. The complete, autonomous P element encodes a transposase enzyme, which recognizes the 31 bp terminal inverted repeats of the P element and catalyzes P element excision and re-insertion. The complete element is 2907 bp; non-autonomous P elements contain an internal deletion of varying length which abolishes transposase production, but such elements can still be mobilized if transposase is encoded elsewhere in the genome. P element insertion and subsequent excision results in the production of 8 bp direct repeats, and the presence of such repeats is indicative of previous P element activity.\n\nHybrid dysgenesis refers to the high rate of mutation in germ line cells of \"Drosophila\" strains resulting from a cross of males with autonomous P elements (P Strain/P cytotype) and females that lack P elements (M Strain/M cytotype). The hybrid dysgenesis syndrome is marked by temperature-dependent sterility, elevated mutation rates, and increased chromosome rearrangement and recombination.\n\nThe hybrid dysgenesis phenotype is affected by the transposition of P elements within the germ-line cells of offspring of \"P strain\" males with \"M strain\" females. Transposition only occurs in germ-line cells, because a splicing event needed to make transposase mRNA does not occur in somatic cells.\n\nHybrid dysgenesis manifests when crossing P strain males with M strain females and not when crossing P strain females (females with autonomous P elements) with M strain males. The eggs of P strain females contain high amounts of a repressor protein that prevents transcription of the transposase gene. The eggs of M strain mothers, which do not contain the repressor protein, allow for transposition of P elements from the sperm of fathers. In P strain females, the repressors are found in the cytoplasm. Hence, when P strain males fertilize M strain females (whose cytoplasm contain no repressor), the male contributes its genome with the P element but not the male cytoplasm leading to P strain progeny.\n\nThis effect contributes to piRNAs being inherited only in the maternal line, which provides a defence mechanism against P elements.\nThe P element has found wide use in \"Drosophila\" research as a mutagen. The mutagenesis system typically uses an autonomous but immobile element, and a mobile nonautonomous element. Flies from subsequent generations can then be screened by phenotype or PCR.\n\nNaturally-occurring P elements contain:\n\nTransposase is an enzyme that regulates and catalyzes the excision of a P element from the host DNA, cutting at two recognition sites, and then reinserting randomly. It is the random insertion that may interfere with existing genes, or carry an additional gene, that can be used for genetic research.\n\nTo use this as a useful and controllable genetic tool, the two parts of the P element must be separated to prevent uncontrolled transposition. The normal genetic tools are therefore:\n\n\"P Plasmids always contain:\"\n\"And may contain:\"\n\nThere are two main ways to utilise these tools:\n\n\nThe inserted gene may have damaged the function of one of the host's genes. Several lines of flies are required so comparison can take place and ensure that no additional genes have been knocked out.\n\n\n\"Possible mutations:\"\n\nThe hijack of an enhancer from another gene allows the analysis of the function of that enhancer. This, especially if the reporter gene is for a fluorescent protein, can be used to help map expression of the mutated gene through the organism, and is a very powerful tool. It is a useful tool for looking at gene expression patterns (temporally and spatially).\n\nThese methods are referred to as reverse genetics. Reverse genetics is an approach to discover the function of a gene by analyzing the phenotypic effects of specific gene sequences obtained by DNA sequencing\n\nOnce the function of the mutated protein has been determined it is possible to sequence/purify/clone the regions flanking the insertion by the following methods:\n\n\nThe process of cutting, self ligation and re cutting allows the amplification of the flanking regions of DNA without knowing the sequence. The point at which the ligation occurred can be seen by identifying the cut site of [enzyme 1].\n\n\n\n", "id": "3284977", "title": "P element"}
{"url": "https://en.wikipedia.org/wiki?curid=9782034", "text": "CD133\n\nCD133 antigen also known as prominin-1 is a glycoprotein that in humans is encoded by the \"PROM1\" gene. It is a member of pentaspan transmembrane glycoproteins (5-transmembrane, 5-TM), which specifically localize to cellular protrusions. While the precise function of CD133 remains unknown, it has been proposed to act as an organizer of cell membrane topology.\n\nCD133 is expressed in hematopoietic stem cells, endothelial progenitor cells, glioblastoma, neuronal and glial stem cells, various pediatric brain tumors, as well as adult kidney, mammary glands, trachea, salivary glands, placenta, digestive tract, testes, and some other cell types.\n\nToday CD133 is the most commonly used marker for isolation of cancer stem cell (CSC) population from different tumors, mainly from various gliomas and carcinomas. Initial studies that showed ability of CD133-positive population to efficiently propagate tumor when injected into immune-compromised mice firstly were perfomed on brain tumors. However, subsequent studies have indicated the difficulty in isolating pure CSC populations. CD133 melanoma cells are considered a subpopulation of CSC a critical role in recurrence. Moreover, CD133 melanoma cells are immunogenic and can be used as an antimelanoma vaccination. In mice the vaccination with CD133 melanoma cells mediated strong anti-tumor activity that resulted in the eradication of parental melanoma cells. In addition, it has also been shown that CD133 melanoma cells preferentially express the RNA helicase DDX3X . As DDX3X also is an immunogenic protein, the same anti-melanoma vaccination strategy can be employed to give therapeutic antitumor immunity in mice.\n\n", "id": "9782034", "title": "CD133"}
{"url": "https://en.wikipedia.org/wiki?curid=9773858", "text": "Bisulfite sequencing\n\nBisulfite sequencing (also known as bisulphite sequencing) is the use of bisulfite treatment of DNA to determine its pattern of methylation. DNA methylation was the first discovered epigenetic mark, and remains the most studied. In animals it predominantly involves the addition of a methyl group to the carbon-5 position of cytosine residues of the dinucleotide CpG, and is implicated in repression of transcriptional activity.\n\nTreatment of DNA with bisulfite converts cytosine residues to uracil, but leaves 5-methylcytosine residues unaffected. Therefore, DNA that has been treated with bisulfite retains only methylated cytosines. Thus, bisulfite treatment introduces specific changes in the DNA sequence that depend on the methylation status of individual cytosine residues, yielding single-nucleotide resolution information about the methylation status of a segment of DNA. Various analyses can be performed on the altered sequence to retrieve this information. The objective of this analysis is therefore reduced to differentiating between single nucleotide polymorphisms (cytosines and thymidine) resulting from bisulfite conversion (Figure 1).\n\nBisulfite sequencing applies routine sequencing methods on bisulfite-treated genomic DNA to determine methylation status at CpG dinucleotides. Other non-sequencing strategies are also employed to interrogate the methylation at specific loci or at a genome-wide level. All strategies assume that bisulfite-induced conversion of unmethylated cytosines to uracil is complete, and this serves as the basis of all subsequent techniques. Ideally, the method used would determine the methylation status separately for each allele. Alternative methods to bisulfite sequencing include Combined Bisulphite Restriction Analysis and methylated DNA immunoprecipitation (MeDIP).\n\nMethodologies to analyze bisulfite-treated DNA are continuously being developed. To summarize these rapidly evolving methodologies, numerous review articles have been written.\n\nThe methodologies can be generally divided into strategies based on methylation-specific PCR (MSP) (Figure 4), and strategies employing polymerase chain reaction (PCR) performed under non-methylation-specific conditions (Figure 3). Microarray-based methods use PCR based on non-methylation-specific conditions also.\n\nThe first reported method of methylation analysis using bisulfite-treated DNA utilized PCR and standard dideoxynucleotide DNA sequencing to directly determine the nucleotides resistant to bisulfite conversion. Primers are designed to be strand-specific as well as bisulfite-specific (i.e., primers containing non-CpG cytosines such that they are not complementary to non-bisulfite-treated DNA), flanking (but not involving) the methylation site of interest. Therefore, it will amplify both methylated and unmethylated sequences, in contrast to methylation-specific PCR. All sites of unmethylated cytosines are displayed as thymines in the resulting amplified sequence of the sense strand, and as adenines in the amplified antisense strand. This technique required cloning of the PCR product prior to sequencing for adequate sensitivity, and therefore was a very labour-intensive method unsuitable for higher throughput. Alternatively, nested PCR methods can be used to enhance the product for sequencing.\n\nAll subsequent DNA methylation analysis techniques using bisulfite-treated DNA is based on this report by Frommer et al. (Figure 2) (). Although most other modalities are not true sequencing-based techniques, the term \"bisulfite sequencing\" is often used to describe bisulfite-conversion DNA methylation analysis techniques in general.\n\nPyrosequencing has also been used to analyze bisulfite-treated DNA without using methylation-specific PCR. Following PCR amplification of the region of interest, pyrosequencing is used to determine the bisulfite-converted sequence of specific CpG sites in the region. The ratio of C-to-T at individual sites can be determined quantitatively based on the amount of C and T incorporation during the sequence extension. The main limitation of this method is the cost of the technology. However, Pyrosequencing does well allow for extension to high-throughput screening methods.\n\nA further improvement to this technique was recently described by Wong et al., which uses allele-specific primers that incorporate single-nucleotide polymorphisms into the sequence of the sequencing primer, thus allowing for separate analysis of maternal and paternal alleles. This technique is of particular usefulness for genomic imprinting analysis.\n\nThis method is based on the single-strand conformation polymorphism analysis (SSCA) method developed for single-nucleotide polymorphism (SNP) analysis. SSCA differentiates between single-stranded DNA fragments of identical size but distinct sequence based on differential migration in non-denaturating electrophoresis. In MS-SSCA, this is used to distinguish between bisulfite-treated, PCR-amplified regions containing the CpG sites of interest. Although SSCA lacks sensitivity when only a single nucleotide difference is present, bisulfite treatment frequently makes a number of C-to-T conversions in most regions of interest, and the resulting sensitivity approaches 100%. MS-SSCA also provides semi-quantitative analysis of the degree of DNA methylation based on the ratio of band intensities. However, this method is designed to assess all CpG sites as a whole in the region of interest rather than individual methylation sites.\n\nA further method to differentiate converted from unconverted bisulfite-treated DNA is using high-resolution melting analysis (HRM), a quantitative PCR-based technique initially designed to distinguish SNPs. The PCR amplicons are analyzed directly by temperature ramping and resulting liberation of an intercalating fluorescent dye during melting. The degree of methylation, as represented by the C-to-T content in the amplicon, determines the rapidity of melting and consequent release of the dye. This method allows direct quantitation in a single-tube assay, but assesses methylation in the amplified region as a whole rather than at specific CpG sites.\n\nMS-SnuPE employs the primer extension method initially designed for analyzing single-nucleotide polymorphisms. DNA is bisulfite-converted, and bisulfite-specific primers are annealed to the sequence up to the base pair immediately before the CpG of interest. The primer is allowed to extend one base pair into the C (or T) using DNA polymerase terminating dideoxynucleotides, and the ratio of C to T is determined quantitatively.\n\nA number of methods can be used to determine this C:T ratio. At the beginning, MS-SnuPE relied on radioactive ddNTPs as the reporter of the primer extension. Fluorescence-based methods or Pyrosequencing can also be used. However, matrix-assisted laser desorption ionization/time-of-flight (MALDI-TOF) mass spectrometry analysis to differentiate between the two polymorphic primer extension products can be used, in essence, based on the GOOD assay designed for SNP genotyping. Ion pair reverse-phase high-performance liquid chromatography (IP-RP-HPLC) has also been used to distinguish primer extension products.\n\nA recently described method by Ehrich et al. further takes advantage of bisulfite-conversions by adding a base-specific cleavage step to enhance the information gained from the nucleotide changes. By first using in vitro transcription of the region of interest into RNA (by adding an RNA polymerase promoter site to the PCR primer in the initial amplification), RNase A can be used to cleave the RNA transcript at base-specific sites. As RNase A cleaves RNA specifically at cytosine and uracil ribonucleotides, base-specificity is achieved by adding incorporating cleavage-resistant dTTP when cytosine-specific (C-specific) cleavage is desired, and incorporating dCTP when uracil-specific (U-specific) cleavage is desired. The cleaved fragments can then be analyzed by MALDI-TOF. Bisulfite treatment results in either introduction/removal of cleavage sites by C-to-U conversions or shift in fragment mass by G-to-A conversions in the amplified reverse strand. C-specific cleavage will cut specifically at all methylated CpG sites. By analyzing the sizes of the resulting fragments, it is possible to determine the specific pattern of DNA methylation of CpG sites within the region, rather than determining the extent of methylation of the region as a whole. This method demonstrated efficacy for high-throughput screening, allowing for interrogation of numerous CpG sites in multiple tissues in a cost-efficient manner.\n\nThis alternative method of methylation analysis also uses bisulfite-treated DNA but avoids the need to sequence the area of interest. Instead, primer pairs are designed themselves to be \"methylated-specific\" by including sequences complementing only unconverted 5-methylcytosines, or, on the converse, \"unmethylated-specific\", complementing thymines converted from unmethylated cytosines. Methylation is determined by the ability of the specific primer to achieve amplification. This method is particularly useful to interrogate CpG islands with possibly high methylation density, as increased numbers of CpG pairs in the primer increase the specificity of the assay. Placing the CpG pair at the 3'-end of the primer also improves the sensitivity. The initial report using MSP described sufficient sensitivity to detect methylation of 0.1% of alleles. In general, MSP and its related protocols are considered to be the most sensitive when interrogating the methylation status at a specific locus.\n\nThe MethyLight method is based on MSP, but provides a quantitative analysis using quantitative PCR. Methylated-specific primers are used, and a methylated-specific fluorescence reporter probe is also used that anneals to the amplified region. In alternative fashion, the primers or probe can be designed without methylation specificity if discrimination is needed between the CpG pairs within the involved sequences. Quantitation is made in reference to a methylated reference DNA. A modification to this protocol to increase the specificity of the PCR for successfully bisulfite-converted DNA (ConLight-MSP) uses an additional probe to bisulfite-unconverted DNA to quantify this non-specific amplification.\n\nFurther methodology using MSP-amplified DNA analyzes the products using melting curve analysis (Mc-MSP). This method amplifies bisulfite-converted DNA with both methylated-specific and unmethylated-specific primers, and determines the quantitative ratio of the two products by comparing the differential peaks generated in a melting curve analysis. A high-resolution melting analysis method that uses both quantitative PCR and melting analysis has been introduced, in particular, for sensitive detection of low-level methylation\n\nMicroarray-based methods are a logical extension of the technologies available to analyze bisulfite-treated DNA to allow for genome-wide analysis of methylation. Oligonucleotide microarrays are designed using pairs of oligonucleotide hybridization probes targeting CpG sites of interest. One is complementary to the unaltered methylated sequence, and the other is complementary to the C-to-U-converted unmethylated sequence. The probes are also bisulfite-specific to prevent binding to DNA incompletely converted by bisulfite. The Illumina Methylation Assay is one such assay that applies the bisulfite sequencing technology on a microarray level to generate genome-wide methylation data.\n\nBisulfite sequencing is used widely across mammalian genomes, however complications have arisen with the discovery of a new mammalian DNA modification 5-hydroxymethylcytosine. 5-Hydroxymethylcytosine converts to cytosine-5-methylsulfonate upon bisulfite treatment, which then reads as a C when sequenced. Therefore, bisulfite sequencing cannot discriminate between 5-methylcytosine and 5-hydroxymethylcytosine. This means that the output from bisulfite sequencing can no longer be defined as solely DNA methylation, as it is the composite of 5-methylcytosine and 5-hydroxymethylcytosine. The development of Tet-assisted bisulfite sequencing by Chuan He at the University of Chicago is now able to distinguish between the two modifications at single base resolution.\n\nBisulfite sequencing relies on the conversion of every single unmethylated cytosine residue to uracil. If conversion is incomplete, the subsequent analysis will incorrectly interpret the unconverted unmethylated cytosines as methylated cytosines, resulting in false positive results for methylation. Only cytosines in single-stranded DNA are susceptible to attack by bisulfite, therefore denaturation of the DNA undergoing analysis is critical. It is important to ensure that reaction parameters such as temperature and salt concentration are suitable to maintain the DNA in a single-stranded conformation and allow for complete conversion. Embedding the DNA in agarose gel has been reported to improve the rate of conversion by keeping strands of DNA physically separate.\n\nA major challenge in bisulfite sequencing is the degradation of DNA that takes place concurrently with the conversion. The conditions necessary for complete conversion, such as long incubation times, elevated temperature, and high bisulfite concentration, can lead to the degradation of about 90% of the incubated DNA. Given that the starting amount of DNA is often limited, such extensive degradation can be problematic. The degradation occurs as depurinations resulting in random strand breaks. Therefore, the longer the desired PCR amplicon, the more limited the number of intact template molecules will likely be. This could lead to the failure of the PCR amplification, or the loss of quantitatively accurate information on methylation levels resulting from the limited sampling of template molecules. Thus, it is important to assess the amount of DNA degradation resulting from the reaction conditions employed, and consider how this will affect the desired amplicon. Techniques can also be used to minimize DNA degradation, such as cycling the incubation temperature.\n\nA potentially significant problem following bisulfite treatment is incomplete desulfonation of pyrimidine residues due to inadequate alkalization of the solution. This may inhibit some DNA polymerases, rendering subsequent PCR difficult. However, this situation can be avoided by monitoring the pH of the solution to ensure that desulfonation will be complete.\n\nA final concern is that bisulfite treatment greatly reduces the level of complexity in the sample, which can be problematic if multiple PCR reactions are to be performed (2006). Primer design is more difficult, and inappropriate cross-hybridization is more frequent.\n\nThe advances in bisulfite sequencing have led to the possibility of applying them at a genome-wide scale, where, previously, global measure of DNA methylation was feasible only using other techniques, such as Restriction landmark genomic scanning. The mapping of the human epigenome is seen by many scientists as the logical follow-up to the completion of the Human Genome Project. This epigenomic information will be important in understanding how the function of the genetic sequence is implemented and regulated. Since the epigenome is less stable than the genome, it is thought to be important in gene-environment interactions.\n\nEpigenomic mapping is inherently more complex than genome sequencing, however, since the epigenome is much more variable than the genome. One’s epigenome varies with age, differs between tissues, is altered by environmental factors, and shows aberrations in diseases. Such rich epigenomic mapping, however, representing different ages, tissue types, and disease states, would yield valuable information on the normal function of epigenetic marks as well as the mechanisms leading to aging and disease.\n\nDirect benefits of epigenomic mapping include probable advances in cloning technology. It is believed that failures to produce cloned animals with normal viability and lifespan result from inappropriate patterns of epigenetic marks. Also, aberrant methylation patterns are well characterized in many cancers. Global hypomethylation results in decreased genomic stability, while local hypermethylation of tumour suppressor gene promoters often accounts for their loss of function. Specific patterns of methylation are indicative of specific cancer types, have prognostic value, and can help to guide the best course of treatment.\n\nLarge-scale epigenome mapping efforts are under way around the world and have been organized under the Human Epigenome Project. This is based on a multi-tiered strategy, whereby bisulfite sequencing is used to obtain high-resolution methylation profiles for a limited number of reference epigenomes, while less thorough analysis is performed on a wider spectrum of samples. This approach is intended to maximize the insight gained from a given amount of resources, as high-resolution genome-wide mapping remains a costly undertaking.\n\nGene-set analysis (for example using tools like DAVID and GoSeq) has been shown to be severely biased when applied to high-throughput methylation data (e.g. genome-wide bisulfite sequencing); it has been suggested that this can be corrected using sample label permutations or using a statistical model to control for differences in the numberes of CpG probes / CpG sites that target each gene.\n\n5-Methylcytosine and 5-hydroxymethylcytosine both read as a C in bisulfite sequencing. Oxidative bisulfite sequencing is a method to discriminate between 5-methylcytosine and 5-hydroxymethylcytosine at single base resolution. The method employs a specific chemical oxidation of 5-hydroxymethylcytosine to 5-formylcytosine, which subsequently converts to uracil during bisulfite treatment. The only base that then reads as a C is 5‑methylcytosine, giving a map of the true methylation status in the DNA sample. Levels of 5‑hydroxymethylcytosine can also be quantified by measuring the difference between bisulfite and oxidative bisulfite sequencing.\n\n\n", "id": "9773858", "title": "Bisulfite sequencing"}
{"url": "https://en.wikipedia.org/wiki?curid=9807237", "text": "Rfam\n\nRfam is a database containing information about non-coding RNA (ncRNA) families and other structured RNA elements. It is an annotated, open access database originally developed at the Wellcome Trust Sanger Institute in collaboration with Janelia Farm, and currently hosted at the European Bioinformatics Institute. Rfam is designed to be similar to the Pfam database for annotating protein families.\n\nUnlike proteins, ncRNAs often have similar secondary structure without sharing much similarity in the primary sequence. Rfam divides ncRNAs into families based on evolution from a common ancestor. Producing multiple sequence alignments (MSA) of these families can provide insight into their structure and function, similar to the case of protein families. These MSAs become more useful with the addition of secondary structure information. Rfam researchers also contribute to Wikipedia's .\n\nThe Rfam database can be used for a variety of functions. For each ncRNA family, the interface allows users to: view and download multiple sequence alignments; read annotation; and examine species distribution of family members. There are also links provided to literature references and other RNA databases.\nRfam also provides links to Wikipedia so that entries can be created or edited by users.\n\nThe interface at the Rfam website allows users to search ncRNAs by keyword, family name, or genome as well as to search by ncRNA sequence or EMBL accession number. \nThe database information is also available for download, installation and use using the INFERNAL software package. The INFERNAL package can also be used with Rfam to annotate sequences (including complete genomes) for homologues to known ncRNAs.\n\nIn the database, the information of the secondary structure and the primary sequence, represented by the MSA, is combined in statistical models called profile stochastic context-free grammars (SCFGs), also known as covariance models. These are analogous to hidden Markov models used for protein family annotation in the Pfam database. Each family in the database is represented by two multiple sequence alignments in Stockholm format and a SCFG.\n\nThe first MSA is the \"seed\" alignment. It is a hand-curated alignment that contains representative members of the ncRNA family and is annotated with structural information. This seed alignment is used to create the SCFG, which is used with the Rfam software INFERNAL to identify additional family members and add them to the alignment. A family-specific threshold value is chosen to avoid false positives.\n\nUntil release 12, Rfam used an initial BLAST filtering step because profile SCFGs were too computationally expensive. However, the latest versions of INFERNAL are fast enough so that the BLAST step is no longer necessary.\n\nThe second MSA is the “full” alignment, and is created as a result of a search using the covariance model against the sequence database. All detected homologs are aligned to the model, giving the automatically produced full alignment.\n\nVersion 1.0 of Rfam was launched in 2003 and contained 25 ncRNA families and annotated about 50 000 ncRNA genes. In 2005, version 6.1 was released and contained 379 families annotating over 280 000 genes. In August 2012, version 11.0 contained 2208 RNA families, while the current version (12.3) annotates 2687 families.\n\n\n", "id": "9807237", "title": "Rfam"}
{"url": "https://en.wikipedia.org/wiki?curid=3742825", "text": "Heat map\n\nA heat map (or heatmap) is a graphical representation of data where the individual values contained in a matrix are represented as colors. The term 'heat map' was originally coined and trademarked by software designer Cormac Kinney in 1991, to describe a 2D display depicting financial market information, though similar plots such as shading matrices have existed for over a century.\n\nThe company that acquired Kinney's invention in 2003 unintentionally allowed the trademark to lapse.\n\nHeat maps originated in 2D displays of the values in a data matrix. Larger values were represented by small dark gray or black squares (pixels) and smaller values by lighter squares. Loua (1873) used a shading matrix to visualize social statistics across the districts of Paris. Sneath (1957) displayed the results of a cluster analysis by permuting the rows and the columns of a matrix to place similar values near each other according to the clustering. Jacques Bertin used a similar representation to display data that conformed to a Guttman scale. The idea for joining cluster trees to the rows and columns of the data matrix originated with Robert Ling in 1973. Ling used overstruck printer characters to represent different shades of gray, one character-width per pixel. Leland Wilkinson developed the first computer program in 1994 (SYSTAT) to produce cluster heat maps with high-resolution color graphics. The Eisen et al. display shown in the figure is a replication of the earlier SYSTAT design.\n\nThere are different kinds of heat maps:\n\nThere are many different color schemes that can be used to illustrate the heatmap, with perceptual advantages and disadvantages for each. Rainbow colormaps are often used, as humans can perceive more shades of color than they can of gray, and this would purportedly increase the amount of detail perceivable in the image. However, this is discouraged by many in the scientific community, for the following reasons:\n\nChoropleth maps are sometimes incorrectly referred to as heat maps. A choropleth map features different shading or patterns within geographic boundaries to show the proportion of a variable of interest, whereas the coloration a heat map (in a map context) does not correspond to geographic boundaries.\n\nSeveral heat map software implementations are listed here (the list is not complete):\n\n\n\n", "id": "3742825", "title": "Heat map"}
{"url": "https://en.wikipedia.org/wiki?curid=6148256", "text": "Alkaline lysis\n\nAlkaline lysis or alkaline extraction is a method used in molecular biology to isolate plasmid DNA from bacteria.\n\nBacteria containing the plasmid of interest are first cultured, then a sample is centrifuged in order to concentrate cellular material (including DNA) into a pellet at the bottom of the containing vessel. The supernatant is discarded, and the pellet is then re-suspended in an EDTA-containing physiological buffer. The purpose of the EDTA is to chelate divalent metal cations such as Mg and Ca, which are required for the function of DNA degrading enzymes (DNAses) and also serve to stabilise the DNA phosphate backbone and cell wall. Glucose in the buffer will maintain the osmotic pressure of the cell in order to prevent the cell from bursting. Tris in the buffer will retain the pH of the cell with 8.0 and RNase will remove the RNA which will disrupt the experiment.\n\nSeparately, a strong alkaline solution consisting of the detergent sodium dodecyl sulfate (SDS) and a strong base such as sodium hydroxide (NaOH) is prepared and then added. The resulting mixture is incubated for a few minutes. During this time, the detergent disrupts cell membranes and allows the alkali to contact and denature both chromosomal and plasmid DNA.\nOne thing which should be noted is that, after tearing apart the cell membrane by SDS, the cell content will neutralize the NaOH; this is why the pH of the lysis goes down from 12.8 to 12.3. So if there are not enough bacterial cells, the extra NaOH will function to generate small DNA fragment. But 0.5 M L-arginine, which can supply a stable pH, can used to replace 0.1 M sodium hydroxide.\n\nFinally, potassium acetate is added. This acidifies the solution and allows the renaturing of plasmid DNA, but not chromosomal DNA, which is precipitated out of solution. Another function of the potassium is to cause the precipitation of sodium dodecyl sulfate and thus removal of the detergent. A final centrifugation is carried out, and this time the pellet contains only debris and can be discarded. The plasmid-containing supernatant is carefully removed and can be further purified or used for analysis, such as gel electrophoresis.\n\nAlso, alkaline lysis is sometimes used to extract plant genetic material. The plant cells are subjected to a strongly alkaline solution containing a detergent (usually a zwitterionic or nonionic detergent such as Tween 20), and the mixture is incubated at high temperature. This method is not used as often due to the sodium hydroxide's tendency to damage genetic material, reducing DNA fragment size.\n\n", "id": "6148256", "title": "Alkaline lysis"}
{"url": "https://en.wikipedia.org/wiki?curid=7702597", "text": "Emerin\n\nEmerin is a protein that in humans is encoded by the \"EMD\" gene, also known as the \"STA\" gene. Emerin, together with MAN1, is a LEM domain-containing integral protein of the inner nuclear membrane in vertebrates. Emerin is highly expressed in cardiac and skeletal muscle. In cardiac muscle, emerin localizes to adherens junctions within intercalated discs where it appears to function in mechanotransduction of cellular strain and in beta-catenin signaling. Mutations in emerin cause X-linked recessive Emery–Dreifuss muscular dystrophy, cardiac conduction abnormalities and dilated cardiomyopathy.\n\nIt is named after Alan Emery.\n\nEmerin is a 29.0 kDa (34 kDa observed MW) protein composed of 254 amino acids. Emerin is a serine-rich protein with an N-terminal 20-amino acid hydrophobic region that is flanked by charged residues; the hydrophobic region may be important for anchoring the protein to the membrane, with the charged terminal tails being cytosolic. In cardiac, skeletal, and smooth muscle, emerin localizes to the inner nuclear membrane; expression of emerin is highest in skeletal and cardiac muscle. In cardiac muscle specifically, emerin also resides at adherens junctions within intercalated discs.\n\nEmerin is a serine-rich nuclear membrane protein and a member of the nuclear lamina-associated protein family. It mediates membrane anchorage to the cytoskeleton. Emery–Dreifuss muscular dystrophy is an X-linked inherited degenerative myopathy resulting from mutation in the \"EMD\" (also known clinically as \"STA\") gene. Emerin appears to be involved in mechanotransduction, as emerin-deficient mouse fibroblasts failed to transduce normal mechanosensitive gene expression responses to strain stimuli. In cardiac muscle, emerin is also found complexed to beta-catenin at adherens junctions of intercalated discs, and cardiomyocytes from hearts lacking emerin showed beta-catenin redistribution as well as perturbed intercalated disc architecture and myocyte shape. This interaction appears to be regulated by glycogen synthase kinase 3 beta.\n\nMutations in emerin cause X-linked recessive Emery–Dreifuss muscular dystrophy, which is characterized by early contractures in the Achilles tendons, elbows and post-cervical muscles; muscle weakness proximal in the upper limbs and distal in lower limbs; along with cardiac conduction defects that range from sinus bradycardia, PR prolongation to complete heart block. In these patients, immunostaining of emerin is lost in various tissues, including muscle, skin fibroblasts, and leukocytes, however diagnostic protocols involve mutational analysis rather than protein staining. In nearly all cases, mutations result in a complete deletion, or undetectable levels, of emerin protein. Approximately 20% of cases have X chromosomes with an inversion within the Xq28 region.\n\nMoreover, recent research have found that the absence of functional emerin may decrease the infectivity of HIV-1. Thus, it is speculated that patients suffering from Emery–Dreifuss muscular dystrophy may have immunity to or show an irregular infection pattern to HIV-1.\n\nEmerin has been shown to interact with:\n\n\n", "id": "7702597", "title": "Emerin"}
{"url": "https://en.wikipedia.org/wiki?curid=9955145", "text": "Eukaryotic transcription\n\nEukaryotic transcription is the elaborate process that eukaryotic cells use to copy genetic information stored in DNA into units of RNA replica. Gene transcription occurs in both eukaryotic and prokaryotic cells. Unlike prokaryotic RNA polymerase that initiates the transcription of all different types of RNA, RNA polymerase in eukaryotes (including humans) comes in three variations, each encoding a different type of gene. A eukaryotic cell has a nucleus that separates the processes of transcription and translation. Eukaryotic transcription occurs within the nucleus where DNA is packaged into nucleosomes and higher order chromatin structures. The complexity of the eukaryotic genome necessitates a great variety and complexity of gene expression control.\n\nTranscription is the process of copying genetic information stored in a DNA strand into a transportable complementary strand of RNA. Eukaryotic transcription takes place in the nucleus of the cell and proceeds in three sequential stages: initiation, elongation, and termination. The transcriptional machinery that catalyzes this complex reaction has at its core three multi-subunit RNA polymerases. RNA polymerase I is responsible for transcribing RNA that codes for genes that become structural components of the ribosome.\nProtein coding genes are transcribed into messenger RNAs (mRNAs) that carry the information from DNA to the site of protein synthesis. Although mRNAs possess great diversity, they are not the most abundant RNA species made in the cell. The so-called non-coding RNAs account for the large majority of the transcriptional output of a cell. These non-coding RNAs perform a variety of important cellular functions.\n\nEukaryotes have three nuclear RNA polymerases, each with distinct roles and properties\n\nRNA polymerase I (Pol I) catalyses the transcription of all rRNA genes except 5S. These rRNA genes are organised into a single transcriptional unit and are transcribed into a continuous transcript. This precursor is then processed into three rRNAs: 18S, 5.8S, and 28S. The transcription of rRNA genes takes place in a specialised structure of the nucleus called the nucleolus, where the transcribed rRNAs are combined with proteins to form ribosomes.\n\nRNA polymerase II (Pol II) is responsible for the transcription of all mRNAs, some snRNAs, siRNAs, and all miRNAs. Many Pol II transcripts exist transiently as single strand precursor RNAs (pre-RNAs) that are further processed to generate mature RNAs. For example, precursor mRNAs (pre-mRNAs)are extensively processed before exiting into the cytoplasm through the nuclear pore for protein translation.\n\nRNA polymerase III (Pol III) transcribes small non-coding RNAs, including tRNAs, 5S rRNA, U6 snRNA, SRP RNA, and other stable short RNAs such as ribonuclease P RNA.\n\nRNA Polymerases I, II, and III contain 14, 12, and 17 subunits, respectively. All three eukaryotic polymerases have five core subunits that exhibit homology with the β, β’, α, α, and ω subunits of E. coli RNA polymerase. An identical ω-like subunit (RBP6) is used by all three eukaryotic polymerases, while the same α-like subunits are used by Pol I and III. The three eukaryotic polymerases share four other common subunits among themselves. The remaining subunits are unique to each RNA polymerase. The additional subunits found in Pol I and Pol III relative to Pol II, are homologous to Pol II transcription factors.\n\nCrystal structures of RNA polymerases I and II provide an opportunity to understand the interactions among the subunits and the molecular mechanism of eukaryotic transcription in atomic detail.\n\nThe carboxyl terminal domain (CTD) of RPB1, the largest subunit of RNA polymerase II, plays an important role in bringing together the machinery necessary for the synthesis and processing of Pol II transcripts. Long and structurally disordered, the CTD contains multiple repeats of heptapeptide sequence YSPTSPS that are subject to phosphorylation and other posttranslational modifications during the transcription cycle. These modifications and their regulation constitute the operational code for the CTD to control transcription initiation, elongation and termination and to couple transcription and RNA processing.\n\nThe initiation of gene transcription in eukaryotes occurs in specific steps. First, an RNA polymerase along with general transcription factors binds to the promoter region of the gene to form a closed complex called the preinitiation complex. The subsequent transition of the complex from the closed state to the open state results in the melting or separation of the two DNA strands and the positioning of the template strand to the active site of the RNA polymerase. Without the need of a primer, RNA polymerase can initiate the synthesis of a new RNA chain using the template DNA strand to guide ribonucleotide selection and polymerization chemistry. However, many of the initiated syntheses are aborted before the transcripts reach a significant length (~10 nucleotides). During these abortive cycles, the polymerase keeps making and releasing short transcripts until it is able to produce a transcript that surpasses ten nucleotides in length. Once this threshold is attained, RNA polymerase passes the promoter and transcription proceeds to the elongation phase.\n\nPol II-transcribed genes contain a region in the immediate vicinity of the transcription start site (TSS) that binds and positions the preinitiation complex. This region is called the core promoter because of its essential role in transcription initiation. Different classes of sequence elements are found in the promoters. For example, the TATA box is the highly conserved DNA recognition sequence for the TATA box binding protein, TBP, whose binding initiates transcription complex assembly at many genes.\n\nEukaryotic genes also contain regulatory sequences beyond the core promoter. These cis-acting control elements bind transcriptional activators or repressors to increase or decrease transcription from the core promoter. Well-characterized regulatory elements include enhancers, silencers, and insulators. These regulatory sequences can be spread over a large genomic distance, sometimes located hundreds of kilobases from the core promoters.\n\nGeneral transcription factors are a group of proteins involved in transcription initiation and regulation. These factors typically have DNA-binding domains that bind specific sequence elements of the core promoter and help recruit RNA polymerase to the transcriptional start site.\nGeneral transcription factors for RNA polymerase II include TFIID, TFIIA, TFIIB, TFIIF, TFIIE, and TFIIH.\n\nTo prepare for transcription, a complete set of general transcription factors and RNA polymerase need to be assembled at the core promoter to form the ~2 million dalton preinitiation complex. For example, for promoters that contain a TATA box near the TSS, the recognition of TATA box by the TBP subunit of TFIID initiates the assembly of a transcription complex. The next proteins to enter are TFIIA and TFIIB, which stabilize the DNA-TFIID complex and recruit Pol II in association with TFIIF and additional transcription factors. TFIIF serves as the bridge between the TATA-bound TBP and polymerase. One of the last transcription factors to be recruited to the preinitiation complex is TFIIH, which plays an important role in promoter melting and escape.\n\nFor pol II-transcribed genes, and unlike bacterial RNA polymerase, promoter melting requires hydrolysis of ATP and is mediated by TFIIH. TFIIH is a ten-subunit protein, including both ATPase and protein kinase activities. While the upstream promoter DNA is held in a fixed position by TFIID, TFIIH pulls downstream double-stranded DNA into the cleft of the polymerase, driving the separation of DNA strands and the transition of the preinitiation complex from the closed to open state. TFIIB aids in open complex formation by binding the melted DNA and stabilizing the transcription bubble.\n\nOnce the initiation complex is open, the first ribonucleotide is brought into the active site to initiate the polymerization reaction in the absence of a primer. This generates a nascent RNA chain that forms a hetero-duplex with the template DNA strand. However, before entering the elongation phase, polymerase may terminate prematurely and release a short, truncated transcript. This process is called abortive initiation. Many cycles of abortive initiation may occur before the transcript grows to sufficient length to promote polymerase escape from the promoter. Throughout abortive initiation cycles, RNA polymerase remains bound to the promoter and pulls downstream DNA into its catalytic cleft in a scrunching-kind of motion.\n\nWhen a transcript attains the threshold length of ten nucleotides, it enters the RNA exit channel. The polymerase breaks its interactions with the promoter elements and any regulatory proteins associated with the initiation complex that it no longer needs. Promoter escape in eukaryotes requires ATP hydrolysis and, in the case of Pol II-phosphorylation of the CTD. Meanwhile, the transcription bubble collapses down to 12-14 nucleotides, providing kinetic energy required for the escape.\n\nAfter escaping the promoter and shedding most of the transcription factors for initiation, the polymerase acquires new factors for the next phase of transcription: elongation. Transcription elongation is a processive process. Double stranded DNA that enters from the front of the enzyme is unzipped to avail the template strand for RNA synthesis. For every DNA base pair separated by the advancing polymerase, one hybrid RNA:DNA base pair is immediately formed. DNA strands and nascent RNA chain exit from separate channels; the two DNA strands reunite at the trailing end of the transcription bubble while the single strand RNA emerges alone.\n\nAmong the proteins recruited to polymerase are elongation factors, thus called because they stimulate transcription elongation. There are different classes of elongation factors. Some factors can increase the overall rate of transcribing, some can help the polymerase through transient pausing sites, and some can assist the polymerase to transcribe through chromatin. One of the elongation factors, P-TEFb, is particularly important. P-TEFb phosphorylates the second residue (Ser-2) of the CTD repeats (YSPTSPS) of the bound Pol II. P-TEFb also phosphorylates and activates SPT5 and TAT-SF1. SPT5 is a universal transcription factor that helps recruit 5'-capping enzyme to Pol II with a CTD phosphorylated at Ser-5. TAF-SF1 recruits components of the RNA splicing machinery to the Ser-2 phosphorylated CTD. P-TEFb also helps suppress transient pausing of polymerase when it encounters certain sequences immediately following initiation.\n\nTranscription fidelity is achieved through multiple mechanisms. RNA polymerases select correct nucleoside triphosphate (NTP) substrate to prevent transcription errors. Only the NTP which correctly base pairs with the coding base in the DNA is admitted to the active center. RNA polymerase performs two known proof reading functions to detect and remove misincorporated nucleotides: pyrophosphorylytic editing and hydrolytic editing. The former removes the incorrectly inserted ribonucleotide by a simple reversal of the polymerization reaction, while the latter involves backtracking of the polymerase and cleaving of a segment of error-containing RNA product. Elongation factor TFIIS stimulates an inherent ribonuclease activity in the polymerase, allowing the removal of misincorporated bases through limited local RNA degradation. Note that all reactions (phosphodiester bond synthesis, pyrophosphorolysis, phosphodiester bond hydrolysis) are performed by RNA polymerase by using a single active center.\n\nTranscription elongation is not a smooth ride along the DNA railway. For proofreading, the polymerase is made to back-up, erase some of the RNA it has already made and have another go at transcription. In general, RNA polymerase does not transcribe through a gene at a constant pace. Rather it pauses periodically at certain sequences, sometimes for long periods of time before resuming transcription. In extreme cases, for example, when the polymerase encounters a damaged nucleotide, it comes to a complete halt. More often, an elongating polymerase is stalled near the promoter. Promoter-proximal pausing during early elongation is a commonly used mechanism for regulating genes poised to be expressed rapidly or in a coordinated fashion. Pausing is mediated by a complex called NELF (negative elongation factor) in collaboration with DSIF (DRB-sensitivity-inducing factor containing SPT4/SPT5). The blockage is released once the polymerase receives an activation signal, such as the phosphorylation of Ser-2 of CTD tail by P-TEFb. Other elongation factors such as ELL and TFIIS stimulate the rate of elongation by limiting the length of time that polymerase pauses.\n\nElongating polymerase is associated with a set of protein factors required for various types of RNA processing. mRNA is capped as soon as it emerges from the RNA-exit channel of the polymerase. After capping, dephosphorylation of Ser-5 within the CTD repeats may be responsible for dissociation of the capping machinery. Further phosphorylation of Ser-2 causes recruitment of the RNA splicing machinery that catalyzes the removal of non-coding introns to generate mature mRNA. Alternative splicing expands the protein complements in eukaryotes. Just as with 5’-capping and splicing, the CTD tail is involved in recruiting enzymes responsible for 3’-polyadenylation, the final RNA processing event that is coupled with the termination of transcription.\n\nThe last stage of transcription is termination, which leads to the dissociation of the complete transcript and the release of RNA polymerase from the template DNA.The process differs for each of the three RNA polymerases. \nThe mechanism of termination is the least understood of the three transcription stages.\n\nThe termination of transcription of pre-rRNA genes by polymerase Pol I is performed by a system that needs a specific transcription termination factor. The mechanism used bears some resemblance to the rho-dependent termination in prokaryotes. Eukaryotic cells contain hundreds of ribosomal DNA repeats, sometimes distributed over multiple chromosomes. Termination of transcription occurs in the ribosomal intergenic spacer region that contains several transcription termination sites upstream of a Pol I pausing site. Through a yet unknown mechanism, the 3’-end of the transcript is cleaved, generating a large primary rRNA molecule that is further processed into the mature 18S, 5.8S and 28S rRNAs.\n\nAs Pol II reaches the end of a gene, two protein complexes carried by the CTD, CPSF (cleavage and polyadenylation specificity factor) and CSTF (cleavage stimulation factor), recognize the poly-A signal in the transcribed RNA. Poly-A-bound CPSF and CSTF recruit other proteins to carry out RNA cleavage and then polyadenylation. Poly-A polymerase adds approximately 200 adenines to the cleaved 3’ end of the RNA without a template. The long poly-A tail is unique to transcripts made by Pol II.\nIn the process of terminating transcription by Pol I and Pol II, the elongation complex does not dissolve immediately after the RNA is cleaved. The polymerase continues to move along the template, generating a second RNA molecule associated with the elongation complex. Two models have been proposed to explain how termination is achieved at last. The allosteric model states that when transcription proceeds through the termination sequence, it causes disassembly of elongation factors and/or an assembly of termination factors that cause conformational changes of the elongation complex. The torpedo model suggests that a 5' to 3' exonuclease degrades the second RNA as it emerges from the elongation complex. Polymerase is released as the highly processive exonuclease overtakes it. It is proposed that an emerging view will express a merge of these two models.\n\nRNA polymerase III can terminate transcription efficiently without the involvement of additional factors. The Pol III termination signal consists of a stretch of thymines (on the nontemplate strand) located within 40bp downstream from the 3' end of mature RNAs. The poly-T termination signal pauses Pol III and causes it to backtrack to the nearest RNA hairpin to become a “dead-end” complex. Consistent with the allosteric mechanism of termination, the RNA hairpin allosterically opens Pol III and causes the elongation complex to disintegrate. The extensive structure embedded in the Pol III-transcript thus is responsible for the factor-independent release of Pol III at the end of a gene. RNA-duplex-dependent termination is an ancient mechanism that dates back to the last universal common ancestor.\n\nThe regulation of gene expression in eukaryotes is achieved through the interaction of several levels of control that acts both locally to turn on or off individual genes in response to a specific cellular need and globally to maintain a chromatin-wide gene expression pattern that shapes cell identity. Because eukaryotic genome is wrapped around histones to form nucelosomes and higher-order chromatin structures, the substrates for transcriptional machinery are in general partially concealed. Without regulatory proteins, many genes are expressed at low level or not expressed at all. Transcription requires displacement of the positioned nucleosomes to enable the transcriptional machinery to gain access of the DNA.\n\nAll steps in the transcription are subject to some degree of regulation. Transcription initiation in particular is the primary level at which gene expression is regulated. Targeting the rate-limiting initial step is the most efficient in terms of energy costs for the cell. Transcription initiation is regulated by cis-acting elements (enhancers, silencers, isolators) within the regulatory regions of the DNA, and sequence-specific trans-acting factors that act as activators or repressors. Gene transcription can also be regulated post-initiation by targeting the movement of the elongating polymerase.\n\nThe eukaryotic genome is organized into a compact chromatin structure that allows only regulated access to DNA. The chromatin structure can be globally \"open\" and more transcriptionally permissive, or globally \"condensed\" and transcriptionally inactive. The former (euchromatin) is lightly packed and rich in genes under active transcription. The latter (heterochromatin) includes gene-poor regions such as telomeres and centromeres but also regions with normal gene density but transcriptionally silenced. Transcription can be silenced by histone modification (deaceltylation and methylation), RNA interference, and/or DNA methylation.\n\nThe gene expression patterns that define cell identity are inherited through cell division. This process is called epigenetic regulation. DNA methylation is reliably inherited through the action of maintenance methylases that modify the nascent DNA strand generated by replication. In mammalian cells, DNA methylation is the primary marker of transcriptionally silenced regions. Specialized proteins can recognize the marker and recruit histone deacetylases and methylases to re-establish the silencing. Nucleosome histone modifications could also be inherited during cell division, however, it is not clear whether it can work independently without the direction by DNA methylation.\n\nThe two main tasks of transcription initiation are to provide RNA polymerase with an access to the promoter and to assemble general transcription factors with polymerase into a transcription initiation complex. Diverse mechanisms of initiating transcription by overriding inhibitory signals at the gene promoter have been identified. Eukaryotic genes have acquired extensive regulatory sequences that encompass a large number of regulator-binding sites and spread overall kilobases (sometimes hundreds of kilobases) from the promoter–-both upstream and downstream. The regulator binding sites are often clustered together into units called enhancers. Enhancers can facilitate highly cooperative action of several transcription factors (which constitute enhanceosomes). Remote enhancers allow transcription regulation at a distance. Insulators situated between enhancers and promoters help define the genes that an enhancer can or cannot influence.\n\nEukaryotic transcriptional activators have separate DNA-binding and activating functions. Upon binding to its cis-element, an activator can recruit polymerase directly or recruit other factors needed by the transcriptional machinery. An activator can also recruit nucleosome modifiers that alter chromatin in the vicinity of the promoter and thereby help initiation. Multiple activators can work together, either by recruiting a common or two mutually dependent components of the transcriptional machinery, or by helping each other bind to their DNA sites. These interactions can synergize multiple signaling inputs and produce intricate transcriptional responses to address cellular needs.\n\nEukaryotic transcription repressors share some of the mechanisms used by their prokaryotic counterparts. For example, by binding to a site on DNA that overlaps with the binding site of an activator, a repressor can inhibit binding of the activator. But more frequently, eukaryotic repressors inhibit the function of an activator by masking its activating domain, preventing its nuclear localization, promoting its degradation, or inactivating it through chemical modifications. Repressors can directly inhibit transcription initiation by binding to a site upstream of a promoter and interacting with the transcriptional machinery. Repressors can indirectly repress transcription by recruiting histone modifiers (deacetylases and methylases) or nucelosome remodeling enzymes that affect the accessibility of the DNA. Repressing histone and DNA modifications are also the basis of transcriptional silencing that can spread along the chromatin and switch off multiple genes.\n\nThe elongation phase starts once assembly of the elongation complex has been completed, and progresses until a termination sequence is encountered. The post-initiation movement of RNA polymerase is the target of another class of important regulatory mechanisms. For example, the transcriptional activator Tat affects elongation rather than initiation during its regulation of HIV transcription. In fact, many eukaryotic genes are regulated by releasing a block to transcription elongation called promoter-proximal pausing. Pausing can influence chromatin structure at promoters to facilitate gene activity and lead to rapid or synchronous transcriptional responses when cells are exposed to an activation signal. Pausing is associated with the binding of two negative elongation factors, DSIF (SPT4/SPT5) and NELF, to the elongation complex. Other factors can also influence the stability and duration of the paused polymerase. Pause release is triggered by the recruitment of the P-TEFb kinase.\n\nTranscription termination has also emerged as an important area of transcriptional regulation. Termination is coupled with the efficient recycling of polymerase. The factors associated with transcription termination can also mediate gene looping and thereby determine the efficiency of re-initiation.\n\nWhen transcription is arrested by the presence of a lesion in the transcribed strand of a gene, DNA repair proteins are recruited to the stalled RNA polymerase to initiate a process called transcription-coupled repair. Central to this process is the general transcription factor TFIIH that has ATPase activity. TFIIH causes a conformational change in the polymerase, to expose the transcription bubble trapped inside, in order for the DNA repair enzymes to gain access to the lesion. Thus, RNA polymerase serves as damage-sensing protein in the cell to target repair enzymes to genes that are being actively transcribed.\n\nEukaryotic transcription is more complex than prokaryotic transcription. For instance, in eukaryotes the genetic material (DNA), and therefore transcription, is primarily localized to the nucleus, where it is separated from the cytoplasm (in which translation occurs) by the nuclear membrane. This allows for the temporal regulation of gene expression through the sequestration of the RNA in the nucleus, and allows for selective transport of mature RNAs to the cytoplasm. Bacteria do not have a distinct nucleus that separates DNA from ribosome and mRNA is translated into protein as soon as it is transcribed. The coupling between the two processes provides an important mechanism for prokaryotic gene regulation.\n\nAt the level of initiation, RNA polymerase in prokaryotes (bacteria in particular) binds strongly to the promoter region and initiates a high basal rate of transcription. No ATP hydrolysis is needed for the close-to-open transition, promoter melting is driven by binding reactions that favor the melted conformation. Chromatin greatly impedes transcription in eukaryotes. Assembly of large multi-protein preinitiation complex is required for promoter-specific initiation. Promoter melting in eukaryotes requires hydrolysis of ATP. As a result, eukaryotic RNA polymerases exhibit a low basal rate of transcription initiation.\n\nIn vertebrates, the majority of gene promoters contain a CpG island with numerous CpG sites. When many of a gene's promoter CpG sites are methylated the gene becomes silenced. Colorectal cancers typically have 3 to 6 driver mutations and 33 to 66 hitchhiker or passenger mutations. However, transcriptional silencing may be of more importance than mutation in causing progression to cancer. For example, in colorectal cancers about 600 to 800 genes are transcriptionally silenced by CpG island methylation (see regulation of transcription in cancer). Transcriptional repression in cancer can also occur by other epigenetic mechanisms, such as altered expression of microRNAs. In breast cancer, transcriptional repression of BRCA1 may occur more frequently by over-expressed microRNA-182 than by hypermethylation of the BRCA1 promoter (see Low expression of BRCA1 in breast and ovarian cancers).\n", "id": "9955145", "title": "Eukaryotic transcription"}
{"url": "https://en.wikipedia.org/wiki?curid=8559750", "text": "Hin recombinase\n\nHin recombinase is a 21kD protein composed of 198 amino acids that is found in the bacteria Salmonella. Hin belongs to the serine recombinase family of DNA invertases in which it relies on the active site serine to initiate DNA cleavage and recombination. The related protein, gamma-delta resolvase shares high similarity to Hin, of which much structural work has been done, including structures bound to DNA and reaction intermediates. Hin functions to invert a 900 base pair (bp) DNA segment within the salmonella genome that contains a promoter for downstream flagellar genes, fljA and fljB. Inversion of the intervening DNA alternates the direction of the promoter and thereby alternates expression of the flagellar genes. This is advantageous to the bacterium as a means of escape from the host immune response.\n\nHin functions by binding to two 26bp imperfect inverted repeat sequences as a homodimer. These hin binding sites flank the invertible segment which not only encodes the Hin gene itself, but also contains an enhancer element to which the bacterial Fis proteins binds with nanomolar affinity. Four molecules of Fis bind to this site as a homodimers and are required for the recombination reaction to proceed.\n\nThe initial reaction requires binding of Hin and Fis to their respective DNA sequences and assemble into a higher-order nucleoprotein complex with branched plectonemic supercoils with the aid of the DNA bending protein HU. At this point, it is believed that the Fis protein modulates subtle contacts to activate the reaction, possibly through direct interactions with the Hin protein. Activation of the 4 catalytic serine residues within the Hin tetramer make a 2-bp double stranded DNA break and forms a covalent reaction intermediate. The DNA cleavage event also requires the divalent metal cation magnesium. A large conformational change reveals a large hydrophobic interface that allows for subunit rotation which may be driven by superhelical torsion within the protein-DNA complex. After this 180° rotation, Hin returns to its native conformation and re-ligates the cleaved DNA, without the aid of high energy cofactors and without the loss of any DNA.\n\n", "id": "8559750", "title": "Hin recombinase"}
{"url": "https://en.wikipedia.org/wiki?curid=10096234", "text": "Eyespot apparatus\n\nThe eyespot apparatus (or \"stigma\") is a photoreceptive organelle found in the flagellate or (motile) cells of green algae and other unicellular photosynthetic organisms such as euglenids. It allows the cells to sense light direction and intensity and respond to it by swimming either towards the light (positive phototaxis) or away from the light (negative phototaxis). A related response (\"photoshock\" or photophobic response) occurs when cells are briefly exposed to high light intensity, causing the cell to stop, briefly swim backwards, then change swimming direction. Eyespot-mediated light perception helps the cells in finding an environment with optimal light conditions for photosynthesis. Eyespots are the simplest and most common \"eyes\" found in nature, composed of photoreceptors and areas of bright orange-red pigment granules. Signals relayed from the eyespot photoreceptors result in alteration of the beating pattern of the flagella, generating a phototactic response.\n\nUnder the light microscope, eyespots appear as dark, orange-reddish spots or \"stigmata\". They get their color from carotenoid pigments contained in bodies called pigment granules. The photoreceptors are found in the plasma membrane overlaying the pigmented bodies.\n\nThe eyespot apparatus of \"Euglena\" comprises the paraflagellar body connecting the eyespot to the flagellum. In electron microscopy, the eyespot apparatus appears as a highly ordered lamellar structure formed by membranous rods in a helical arrangement.\n\nIn \"Chlamydomonas\", the eyespot is part of the chloroplast and takes on the appearance of a membranous sandwich structure. It is assembled from chloroplast membranes (outer, inner, and thylakoid membranes) and carotenoid-filled granules overlaid by plasma membrane. The stacks of granules act as a quarter-wave plate, reflecting incoming photons back to the overlying photoreceptors, while shielding the photoreceptors from light coming from other directions. It disassembles during cell division and reforms in the daughter cells in an asymmetric fashion in relation to the cytoskeleton. This asymmetric positioning of the eyespot in the cell is essential for proper phototaxis.\n\nThe most critical eyespot proteins are the photoreceptor proteins that sense light. The photoreceptors found in unicellular organisms fall into two main groups: flavoproteins and retinylidene proteins (rhodopsins). Flavoproteins are characterized by containing flavin molecules as chromophores, whereas retinylidene proteins contain retinal. The photoreceptor protein in \"Euglena\" is likely a flavoprotein. In contrast, \"Chlamydomonas\" phototaxis is mediated by archaeal-type rhodopsins.\n\nBesides photoreceptor proteins, eyespots contain a large number of structural, metabolic and signaling proteins. The eyespot proteome of \"Chlamydomonas\" cells consists of roughly 200 different proteins.\n\nThe \"Euglena\" photoreceptor was identified as a blue-light-activated adenylyl cyclase. Excitation of this receptor protein results in the formation of cyclic adenosine monophosphate (cAMP) as a second messenger. Chemical signal transduction ultimately triggers changes in flagellar beat patterns and cell movement.\n\nThe archaeal-type rhodopsins of \"Chlamydomonas\" contain an all-\"trans\" retinylidene chromatophore which undergoes photoisomerization to a 13-\"cis\" isomer. This activates a photoreceptor channel, leading to a change in membrane potential and cellular calcium ion concentration. Photoelectric signal transduction ultimately triggers changes in flagellar strokes and thus cell movement.\n\n", "id": "10096234", "title": "Eyespot apparatus"}
{"url": "https://en.wikipedia.org/wiki?curid=1233025", "text": "Photopigment\n\nPhotopigments are unstable pigments that undergo a chemical change when they absorb light. The term is generally applied to the non-protein chromophore moiety of photosensitive chromoproteins, such as the pigments involved in photosynthesis and photoreception. In medical terminology, \"photopigment\" commonly refers to the photoreceptor proteins of the retina.\n\nPhotosynthetic pigment (converting light into biochemical energy). Examples for photosynthetic pigments are chlorophyll, carotenoids and phycobilins. These pigments enter a high-energy state upon absorbing a photon which they can release in the form of chemical energy. This can occur via light-driven pumping of ions across a biological membrane (e.g. in the case of the proton pump bacteriorhodopsin) or via excitation and transfer of electrons released by photolysis (e.g. in the photosystems of the thylakoid membranes of plant chloroplasts). In chloroplasts, the light-driven electron transfer chain in turn drives the pumping of protons across the membrane.\n\nThe pigments in photoreceptor proteins either change their conformation or undergo photoreduction when they absorb a photon. This change in the conformation or redox state of the chromophore then affects the protein conformation or activity and triggers a signal transduction cascade. Examples for photoreceptor pigments include retinal (for example in rhodopsin), flavin (for example in cryptochrome), and bilin (for example in phytochrome).\n\nIn medical terminology, the term photopigment is applied to opsin-type photoreceptor proteins, specifically rhodopsin and photopsins, the photoreceptor proteins in the retinal rods and cones of vertebrates that are responsible for visual perception, but also melanopsin and others.\n\n", "id": "1233025", "title": "Photopigment"}
{"url": "https://en.wikipedia.org/wiki?curid=10226901", "text": "Tn3 transposon\n\nThe Tn3 transposon is a 4957 base pair mobile genetic element, found in prokaryotes. \nIt encodes three proteins:\n\n\nInitially discovered as a repressor of transposase, resolvase also plays a role in facilitating Tn3 replication (Sherratt 1989).\n\nThe transposon is flanked by a pair of 38bp inverted repeats.\n\nThis first stage is catalysed by transposase.\n\nThe plasmid containing the transposon (the donor plasmid) fuses with a host plasmid (the target plasmid). In the process, the transposon and a short section of host DNA are replicated. The end product is a 'cointegrate' plasmid containing two copies of the transposon.\n\nShapiro (1978) proposed the following mechanism for this process: \n\nThe diagrams on the right illustrate the way in which the positions of the cleavages lead to the replication of certain regions once the plasmids have fused.\n\nTo separate the host and target molecules Tn3 resolvase executes site-specific recombination between the old and new copy of transposon at a specific site called \"res\", which is present in each copy of the transposon. \"Res\" is 114 bp long and it consists of 3 sub-sites, namely sites I, II and III. Each of these sites is of different lengths (28, 34 and 25bp, respectively) and they are unevenly spaced with 22bp separating sites I and II and only 5bp between sites II and III. The sites consist of 6bp inverted repeat motifs flanking a central sequence of variable length. These motifs act as binding sites for resolvase, so that each site binds a resolvase dimer but with varying affinity and probably a slightly different protein-DNA complex architecture. All three sub-sites are essential for recombination.\n\nAt recombination, two directly repeated res sites with resolvase dimers bound to each sub-site, come together to form a large complex structure called the synaptosome. Resolvase bound to sites II and III initiates the assembly of this complex. In this structure, exact architecture of which is still unclear, two res sites are intertwined in such a way as to juxtapose two copies of site I, allowing resolvase dimers bound to each site to form a tetramer. Again, it is the interaction between the resolvase dimers bound at accessory sites (sites II and III) and resolvase at site I that causes the two dimers to synapse and form a tetramer. After the tetramer is formed it becomes activated and the top and bottom DNA strands are simultaneously cleaved in the middle of the site I with a 2bp overhang. The strand exchange ensues by as yet unknown mechanism with a resulting net rotation of 180°. The strand exchange is then followed by the religation (Stark et al., 1992).\nRecombination between two directly repeated res sites separates, or resolves, the \"cointegrate\" into two original molecules, each one now containing a copy of the Tn3 transposon. After resolution these two molecules remain linked as a simple two-noded catenane which can be easily separated \"in vivo\" by a type II topoisomerase (Grindley 2002). \nWild type resolvase system absolutely requires a supercoiled substrate and that the recombination sites are oriented in a direct repeat on the same DNA molecule. \nHowever, a number of \"deregulated\" or \"hyperactive\" mutants that have lost the requirement for the accessory sites have been isolated. These mutants are capable of catalysing recombination between two copies of site I only, which basically reduces the recombination site size from 114bp to only 28bp. Furthermore, these mutants have no supercoiling or connectivity requirements (Arnold et al., 1999) and have been shown to work in mammalian cells. Hyperactive resolvase mutants have so far proven useful in creating resolvases with altered sequence specificity but also in structural work.\n\nThe entire resolvase recombination reaction can be reproduced \"in vitro\", requiring only resolvase, a substrate DNA and multivalent cations, using either wild type protein or hyperactive mutants.\n\nHyperactive resolvase mutants, if further developed, could become an alternative to Cre and FLP, the most commonly used recombination systems in molecular biology to date.\n\n1.Sherratt, D. J. (1989). Tn3 and related transposable elements: site-specific recombination and transposition. In Berg, D. E., Howe, M. (eds) Mobile DNA. American Society for Microbiology, Washinghton, DC pp. 163–184\n\n4.Grindley, N.D.F. (2002). The movement of Tn3-like elements: transposition and cointegrate resolution. In Mobile DNA II, Craig, N., Craigie, R., Gellert, M. and Lambowitz, A. (ed.), pp272–302. ASM Press, Washington, DC, USA\n", "id": "10226901", "title": "Tn3 transposon"}
{"url": "https://en.wikipedia.org/wiki?curid=9681395", "text": "Discrete nanoscale transport\n\nDiscrete Nanoscale Transport refers to a class of transport phenomena inside cells. The word “nanoscale” refers to the nano-morphological features of the discrete entities being transported and their relevant interactions, whose length scale is from a few to several hundreds nanometers. This distinguishes nanoscale transport from transport of molecular-scale entities such as ions, solutes, metabolites, lipids, proteins, etc.\n\nTransport of nanoscale entities is determined not only by thermal mobility, but also by their interactions with cellular structures (e.g. cytoskeletal filaments), with molecular-scale entities (e.g. motor proteins, signaling molecules), and with other nanoscale entities. In most cases, due to the discrete nature of the transport events and their strong dependence on the local structural and architectural properties of the media, it cannot be described by continuum approaches that are routinely used to describe macroscale transport.\n", "id": "9681395", "title": "Discrete nanoscale transport"}
{"url": "https://en.wikipedia.org/wiki?curid=10359432", "text": "Missense mRNA\n\nMissense mRNA molecules have one or more mutated codons that yield polypeptides with an amino acid sequence different from the wild-type or naturally occurring polypeptide. Missense mRNA molecules are created when template DNA strands or the mRNA strands themselves undergo a missense mutation in which a protein coding sequence is mutated and an altered amino acid sequence is coded for.\n", "id": "10359432", "title": "Missense mRNA"}
{"url": "https://en.wikipedia.org/wiki?curid=11390412", "text": "Electroblotting\n\nElectroblotting is a method in molecular biology/biochemistry/immunogenetics to transfer proteins or nucleic acids onto a membrane by using PVDF or nitrocellulose, after gel electrophoresis. The protein or nucleic acid can then be further analyzed using probes such as specific antibodies, ligands like lectins, or stains. This method can be used with all polyacrylamide and agarose gels. An alternative technique for transferring proteins from a gel is capillary blotting.\n\nThis technique was patented in 1989 by William J. Littlehales under the title \"Electroblotting technique for transferring specimens from a polyacrylamide electrophoresis or like gel onto a membrane.\n\nThis technique relies upon current and a transfer buffer solution to drive proteins or nucleic acids onto a membrane. Following electrophoresis, a standard tank or semi-dry blotting transfer system is set up. A stack is put together in the following order from cathode to anode: sponge | three sheets of filter paper soaked in transfer buffer | gel | PVDF or nitrocellulose membrane | three sheets of filter paper soaked in transfer buffer | sponge. It is a necessity that the membrane is located between the gel and the positively charged anode, as the current and sample will be moving in that direction. Once the stack is prepared, it is placed in the transfer system, and a current of suitable magnitude is applied for a suitable period of time according to the materials being used.\n\nTypically the electrophoresis gel is stained with Coomassie Brilliant Blue following the transfer to ensure that a sufficient quantity of material has been transferred. Because the proteins may retain or regain part of their structure during blotting they may react with specific antibodies giving rise to the term immunoblotting. Alternatively the proteins may react with ligands like lectins giving rise to the term affinity blotting.\n\n\n", "id": "11390412", "title": "Electroblotting"}
{"url": "https://en.wikipedia.org/wiki?curid=9804129", "text": "Tree puzzle\n\nTREE-PUZZLE is a computer program used to construct phylogenetic trees from sequence data by maximum likelihood analysis. Branch lengths can be calculated with and without the molecular clock hypothesis.\n\n\n", "id": "9804129", "title": "Tree puzzle"}
{"url": "https://en.wikipedia.org/wiki?curid=11521413", "text": "Adaptor hypothesis\n\nThe adaptor hypothesis is part of a scheme to explain how information encoded in DNA is used to specify the amino acid sequence of proteins. It was formulated by Francis Crick in the mid-1950s, together with the central dogma of molecular biology and the sequence hypothesis. It first appeared in an informal publication of the RNA Tie Club in 1955 and was formally published in an article “On Protein Synthesis” in 1958.\n\n\nThe adaptor hypothesis was framed to explain how information could be extracted from a nucleic acid and used to put together a string of amino acids in a specific sequence, that sequence being determined by the nucleotide sequence of the nucleic acid (DNA or RNA) template. Crick proposed that each amino acid is first attached to its own specific “adaptor” piece of nucleic acid (in an enzyme-catalysed reaction). The order of assembly of the amino acids is then determined by a specific recognition between the adaptor and the nucleic acid which is serving as the informational template. In this way the amino acids could be lined up by the template in a specific order. Coupling between adjacent amino acids would then lead to the synthesis of a polypeptide whose sequence is determined by the template nucleic acid.\n\n\nCrick’s thinking behind this proposal was based on a general consideration of the chemical properties of the two classes of molecule — nucleic acids and proteins. The amino acids are characterised by having a variety of side chains which vary from being hydrophilic to hydrophobic: their individual characters reside in the very different properties these side chains have. By contrast, a nucleic acid is composed of a string of nucleotides whose sequence presents a geometrically defined surface for hydrogen bonding. This makes nucleic acids good at recognising each other, but poor at distinguishing the varied side chains of amino acids. It was this apparent lack of any possibility of specific recognition of amino acid side chains by a nucleotide sequence which led Crick to conclude that amino acids would first become attached to a small nucleic acid — the adaptor — and that this, by base-pairing with the template (presumably as occurs between DNA strands in the double helix), would carry the amino acids to be lined up on the template.\n\n\nThat such adaptors do exist was discovered by Mahlon Hoagland and Paul Zamecnik in 1958. These “soluble RNAs” are now called transfer RNAs and mediate the translation of messenger RNAs on ribosomes according to the rules contained in the genetic code. Crick imagined that his adaptors would be small, perhaps 5-10 nucleotides long. In fact, they are much larger, having a more complex role to play in protein synthesis, and are closer to 100 nucleotides in length.\n\n", "id": "11521413", "title": "Adaptor hypothesis"}
{"url": "https://en.wikipedia.org/wiki?curid=7973428", "text": "Anfinsen's dogma\n\nAnfinsen's dogma (also known as the thermodynamic hypothesis) is a postulate in molecular biology that, at least for small globular proteins, the native structure is determined only by the protein's amino acid sequence. The dogma was championed by the Nobel Prize Laureate Christian B. Anfinsen from his research on the folding of ribonuclease A. The postulate amounts to saying that, at the environmental conditions (temperature, solvent concentration and composition, etc.) at which folding occurs, the native structure is a unique, stable and kinetically accessible minimum of the free energy.\nThe three conditions:\n\nHow the protein reaches this structure is the subject of the field of protein folding, which has a related concept called Levinthal's paradox. The Levinthal paradox states that the number of possible conformations available to a given protein is astronomically large, such that even a small protein of 100 residues would require more time than the universe has existed (10 seconds) to explore all possible conformations and choose the appropriate one, it would also arguably make computational prediction of protein structures under the same basis unfeasible if not impossible.\n\nAlso, some proteins need the assistance of another protein called a chaperone protein to fold properly. It has been suggested that this disproves Anfinsen's dogma. However, the chaperones do not appear to affect the final state of the protein; they seem to work primarily by preventing aggregation of several protein molecules before the protein is folded.\n\nPrions are an exception to Anfinsen's dogma. Prions are stable conformations of proteins which differ from the native folding state. In Bovine spongiform encephalopathy (Mad Cow Disease), native proteins re-fold into a different stable conformation, which causes fatal amyloid buildup. Other amyloid diseases, including Alzheimer's disease and Parkinson's disease, are also exceptions to Anfinsen's dogma.\n\n", "id": "7973428", "title": "Anfinsen's dogma"}
{"url": "https://en.wikipedia.org/wiki?curid=8786437", "text": "Derepression\n\nIn genetics and cell biology, repression is a mechanism often used to decrease or inhibit the expression of a gene. Removal of repression is called derepression. This mechanism may occur at different stages in the expression of a gene, with the result of increasing the overall RNA or protein products. Dysregulation of derepression mechanisms can result in altered gene expression patterns, which may lead to negative phenotypic consequences such as disease. \n\nTranscription can be repressed in a variety of ways, and therefore can be derepressed in different ways as well. A common mechanism is allosteric regulation. This is when a substrate binds a repressor protein and causes it to undergo a conformational change. If the repressor is bound upstream of a gene, such as in an operator sequence, then it would be repressing the gene's expression. This conformational change would take away the repressor’s ability to bind DNA, thus removing its repressive effect on transcription.\n\nAnother form of transcriptional derepression uses chromatin remodeling complexes. For transcription to occur, RNA polymerase needs to have access to the promoter sequence of the gene or it cannot bind the DNA. Sometimes these sequences are wrapped around nucleosomes or are in condensed heterochromatin regions, and are therefore inaccessible. Through different chromatin remodeling mechanisms these promoter sequences can become accessible to the RNA polymerase, and transcription becomes derepressed.\n\nTranscriptional derepression may also occur at the level of transcription factor activation. Certain families of transcription factors are non-functional on their own because their active domains are blocked by another part of the protein. Substrate binding to this second, regulatory domain causes a conformational change in the protein to allows access to the active domain. This lets the transcription factor bind to DNA and serve its function, thus derepressing the transcription factor.\n\nDerepression of translation increases protein production without altering the levels of mRNA in the cell. miRNAs are a common mechanism of translation repression, binding to the mRNA through complimentary base pairing to silence them. Certain RNA binding proteins have been shown to target untranslated regions of the mRNAs and upregulate the translation initiation rates by alleviating the repressive miRNA effects.\n\nAn example is the auxin mediated derepression of the auxin response factor family of transcription factors in plants. These auxin response factors are repressed by Aux/IAA repressors. In the presence of auxin, these Aux/AII proteins undergo ubiquitination and are then degraded. This derepresses the auxin response factors so they may carry out their functions in the cell.\n\nAlzheimer’s is a neurodegenerative disease involving progressive memory loss and other declines in brain function. One common cause of familial Alzheimer’s is mutation in the \"PSEN1\" gene. This gene encodes a protein that cleaves certain intracellular peptides which, once free in the cytoplasm, promote CBPdegradation. Mutations in \"PSEN1\" decrease its production or ability to cleave proteins. This derepresses the CBP proteins, and allows them to perform their function of upregulating transcription of their target genes.\n\nRhett syndrome is a neurodevelopmental disorder involving deterioration of learned language and motor skills, autism, and seizures starting in infancy. Many cases of Rhett syndrome are associated with mutations in \"MECP2\", a gene encoding a transcriptional repressor. Mutations in this gene decrease the levels of MeCP2 binding to different promoter sequences, resulting in their overall derepression. The increased expression of these MeCP2 regulated genes in neurons contribute to the Rhett syndrome phenotype.\n\nThis syndrome is associated with an increased susceptibility of tumors and growth abnormalities in children. A common cause of this syndrome is a mutation in an imprint control region near the \"Igf2\" gene. This imprint control region is normally bound by an insulator on the maternal allele, which represses an enhancer from acting on the \"Igf2\" gene. This insulator is absent on the paternal allele and allows it access to the gene. Mutations in this imprint control region inhibit the insulator from binding, which derepresses enhancer activity on the maternal Igf2 gene. This abnormal derepression and increase in gene expression can result in Beckwith-Wiedemann syndrome.\n", "id": "8786437", "title": "Derepression"}
{"url": "https://en.wikipedia.org/wiki?curid=11314351", "text": "Digital polymerase chain reaction\n\nDigital polymerase chain reaction (digital PCR, DigitalPCR, dPCR, or dePCR) is a biotechnological refinement of conventional polymerase chain reaction methods that can be used to directly quantify and clonally amplify nucleic acids strands including DNA, cDNA or RNA. The key difference between dPCR and traditional PCR lies in the method of measuring nucleic acids amounts, with the former being a more precise method than PCR, though also more prone to error in the hands of inexperienced users. A “digital” measurement quantitatively and discretely measures a certain variable, whereas an “analog” measurement extrapolates certain measurements based on measured patterns. PCR carries out one reaction per single sample. dPCR also carries out a single reaction within a sample, however the sample is separated into a large number of partitions and the reaction is carried out in each partition individually. This separation allows a more reliable collection and sensitive measurement of nucleic acid amounts. The method has been demonstrated as useful for studying variations in gene sequences — such as copy number variants and point mutations — and it is routinely used for clonal amplification of samples for next-generation sequencing.\n\nThe polymerase chain reaction method is used to quantify nucleic acids by amplifying a nucleic acid molecule with the enzyme DNA polymerase. Conventional PCR is based on the theory that amplification is exponential. Therefore, nucleic acids may be quantified by comparing the number of amplification cycles and amount of PCR end-product to those of a reference sample. However, many factors complicate this calculation, creating uncertainties and inaccuracies. These factors include the following: initial amplification cycles may not be exponential; PCR amplification eventually plateaus after an uncertain number of cycles; and low initial concentrations of target nucleic acid molecules may not amplify to detectable levels. However, the most significant limitation of PCR is that PCR amplification efficiency in a sample of interest may be different from that of reference samples. Since PCR is an exponential process, only twofold differences in amplification can be observed, greatly impacting the validity and precision of the results.\n\ndPCR improves upon the current PCR practices by dividing up the reaction into multiple, smaller reactions. A sample is partitioned so that individual nucleic acid molecules within the sample are localized and concentrated within many separate regions. Micro well plates, capillaries, oil emulsion, and arrays of miniaturized chambers with nucleic acid binding surfaces can be used to partition the samples. A PCR solution is made similarly to a TaqMan assay, which consists of template DNA (or RNA), fluorescence-quencher probes, primers, and a PCR master mix, which contains DNA polymerase, dNTPs, MgCl, and reaction buffers at optimal concentrations. The PCR solution is divided into smaller reactions and are then made to run PCR individually. After multiple PCR amplification cycles, the samples are checked for fluorescence with a binary readout of “0” or “1”. The fraction of fluorescing droplets is recorded. The partitioning of the sample allows one to estimate the number of different molecules by assuming that the molecule population follows the Poisson distribution, thus accounting for the possibility of multiple target molecules inhabiting a single molecule. Using Poisson's law of small numbers, the distribution of target molecule within the sample can be accurately approximated allowing for a quantification of the target strand in the PCR product. Figure 2 shows the Poisson distribution of the copies of target molecule per droplet (CPD) based on the fraction of fluorescent droplets (p), represented by the function CPD=-ln(1-p). This model simply predicts that as the number of samples containing at least one target molecule increases, the probability of the samples containing more than one target molecule increases. In conventional PCR, the number of PCR amplification cycles is proportional to the starting copy number. dPCR, however, is not dependent on the number of amplification cycles to determine the initial sample amount, eliminating the reliance on uncertain exponential data to quantify target nucleic acids and therefore provides absolute quantification.\n\nThe benefits of dPCR include increased precision through massive sample partitioning, which ensures reliable measurements in the desired DNA sequence due to reproducibility. With basic PCR, error rates are larger when detecting small fold-change differences, however, with dPCR the error rate decreases because smaller fold-change differences can be detected in DNA sequence. The technique itself reduces the use of a larger volume of reagent needed, which inevitably will lower experiment cost. Also, dPCR is highly quantitative as it does not rely on relative fluorescence of the solution to determine the amount of amplified target DNA.\n\nIn Digital Droplet PCR (ddPCR) the PCR solution is divided into smaller reactions through a water oil emulsion technique, which are then made to run PCR individually. As shown in Figure 3, the PCR sample is partitioned into nanoliter-size samples and encapsulated into oil droplets. The oil droplets are made using a droplet generator that applies a vacuum to each of the wells. Approximately 20,000 oil droplets are made from each 20 μL sample.\n\ndPCR measures the actual number of molecules (target DNA) as each molecule is in one droplet, thus making it a discrete “digital” measurement. It provides absolute quantification because dPCR measures the positive fraction of samples, which is the number of droplets that are fluorescing due to proper amplification. This positive fraction accurately indicates the initial amount of template nucleic acid. Similarly, qPCR utilizes fluorescence; however, it measures the intensity of fluorescence at specific times (generally after every amplification cycle) to determine the relative amount of target molecule (DNA), but cannot specify the exact amount without constructing a standard curve using different amounts of a defined standard. It gives the threshold per cycle (CT) and the difference in CT is used to calculate the amount of initial nucleic acid. As such, qPCR is an analog measurement, which may not be as precise due to the extrapolation required to attain a measurement. It should be noted that the previous reference was authored by employees of Bio-Rad, which provides commercial kits and reagents for dPCR, and therefore stands to profit from their promotion and sale.\n\ndPCR measures the amount of DNA after amplification is complete and then determines the fraction of replicates. This is representative of an endpoint measurement as it requires the observation of the data after the experiment is completed. In contrast, qPCR records the relative fluorescence of the DNA at specific points during the amplification process, which requires stops in the experimental process. This “real-time” aspect of qPCR may theoretically affect results due to the stopping of the experiment. In practice, however, most qPCR thermal cyclers read each sample's fluorescence very quickly at the end of the annealing/extension step before proceeding to the next melting step, meaning this hypothetical concern is not actually relevant or applicable for the vast majority of researchers.\n\nqPCR is unable to distinguish differences in gene expression or copy number variations that are smaller than twofold. It is difficult to identify alleles with frequencies of less than 1% because highly abundant, common alleles would be matched with similar sequences. On the other hand, dPCR has been shown to detect a differences of less than 30% in gene expression, distinguish between copy number variations of that differ by only 1 copy, and identify alleles that occur at frequencies less than 0.1%.\n\ndPCR applications largely include, detection of mutant alleles; for example, dPCR can be used to quantitate mutant alleles in circulating tumor DNA. This technique can also be used for genetic prenatal diagnostics. In a research paper conducted by White et al., a dPCR assay was able to detect an L1 insertion event in as few as 0.01% to 0.1% of cells, which indicates the precision of this device. dPCR has proved useful for quantitation of DNA methylation including the analysis of heterogeneous methylation.\n\ndPCR provides quantification of gene expression levels, especially with low-abundance miRNA due to the technique's sensitivity and precision. dPCR can be used for amplification of different types of RNA such as siRNA, mRNA, etc. Due to the high degree of cell-cell variation in gene expression, dPCR enable low copy number quantification in order to perform single cell analysis. Following variations among cells, dPCR enables an increased detection of differences in gene copy number in order to analyze complex behavioral traits, phenotypic variability, and diseases.\n\nThe first paper on dPCR was published by Dr Alec Morley and Pamela Sykes in 1992. The purpose was to quantify PCR targets in an attempt to track and measure the absolute lowest number of leukemic cells in a patient with leukemia. The purpose was to monitor residual disease in leukemia patients, and thereby treat the patients at the earliest possible moment of detection of disease recurrence. Further evolutions of the technology allowed for more widespread distribution of this technique, with small partitions created by emulsion droplets and/or microfluidics.\n\nIn 1995, Brown at Cytonix and Silver National Institutes of Health coinvented single-step quantitization and sequencing methods employing nano-scale physical containment arrays (Brown, Silver), and open chambers (Brown) using localized clonal colonies in 1D and 2D capillaries, macro volumes, gels, free chambers, and affinity surfaces/particles. resulting in a 1997 U. S. Patent, and subsequent divisional and continuation patents. The concepts of electrowetting and digital microfluidics were further introduced (Brown) as one means of manipulating nano fluid volumes.\n\nDigital PCR has been shown to be a possible surveillance tool for illnesses such as cancer.\n\nVogelstein and Kinzler developed a technology called BEAMing based on digital PCR (Beads, Emulsion, Amplification, Magnetics) and quantified KRAS mutations in stool DNA from colorectal cancer patients.\n\nDressman, et al., began using emulsion beads for digital PCR.\n\nIn 2006 Fluidigm introduced the first commercial system for digital PCR based on integrated fluidic circuits (chips) having integrated chambers and valves for partitioning samples.\n\nIn 2008, Inostics started to provide BEAMing digital PCR services for the detection of mutations in plasma/serum and tissue.\n\nQuantaLife developed a different method of partitioning, called the Droplet Digital PCR (ddPCR) technology, which partitions a sample into 20,000 droplets and digitally counts nucleic acid targets. In 2011, Bio-Rad Laboratories acquired Quantalife.\n\nIn 2013, RainDance Technologies launched a digital PCR platform based on its picoliter-scale droplet technology, which generates up to 10 million picoliter-sized droplets per lane. The technology was first demonstrated in a paper published in \"Lab on a Chip\" by scientists from Université de Strasbourg and Université Paris Descartes. Later that year, RainDance Technologies announced a partnership with Integrated DNA Technologies to develop reagents for the digital PCR platform.\n\nDigital PCR has many potential applications, including the detection and quantification of low-level pathogens, rare genetic sequences, copy number variations, and relative gene expression in single cells. This method provides the information with accuracy and precision. Clonal amplification enabled by single-step digital PCR is a key factor in reducing the time and cost of many of the \"next-generation sequencing\" methods and hence enabling personal genomics.\n\nThe \"Minimum Information for Publication of Quantitative Digital PCR Experiments\" or \"digital MIQE\" guidelines are a comprehensive set of best practices which aim to increase the validity and comparability of digital PCR experiments reported in published literature. The guide was published in 2013 and followed publication in 2009 of \"MIQE\", a comparable guide for quantitative real-time PCR. In the two years following publication of the \"digital MIQE\", less than 20% of published digital PCR papers have cited the guideline.\n\n\n", "id": "11314351", "title": "Digital polymerase chain reaction"}
{"url": "https://en.wikipedia.org/wiki?curid=1972580", "text": "Phototropin\n\nPhototropins are photoreceptor proteins (more specifically, flavoproteins) that mediate phototropism responses in higher plants. Along with cryptochromes and phytochromes they allow plants to respond and alter their growth in response to the light environment. Phototropins may also be important for the opening of stomata and the movement of chloroplasts.\n\nPhototropins are part of the phototropic sensory system in plants that causes various environmental responses in plants. Phototropins specifically will cause stems to bend towards light and stomata to open. Phototropins have been shown to impact the movement of chloroplast inside the cell. In addition phototropins mediate the first changes in stem elongation in blue light prior to cryptochrome activation. Phototropin is also required for blue light mediated transcript destabilization of specific mRNAs in the cell.\n\n", "id": "1972580", "title": "Phototropin"}
{"url": "https://en.wikipedia.org/wiki?curid=3272302", "text": "Complement component 5a\n\nC5a is a protein fragment released from cleavage of complement component C5 by protease C5-convertase into C5a and C5b fragments. C5b is important in late events of the complement cascade, an orderly series of reactions which coordinates several basic defense mechanisms, including formation of the Membrane Attack Complex (MAC), one of the most basic weapons of the innate immune system, formed as an automatic response to intrusions from foreign particles and microbial invaders. It essentially pokes microscopic pinholes in these foreign objects, causing loss of water and sometimes death. C5a, the other cleavage product of C5, acts as a highly inflammatory peptide, encouraging complement activation, formation of the MAC, attraction of innate immune cells, and histamine release involved in allergic responses. The origin of C5 is in the hepatocyte, but its synthesis can also be found in macrophages, where it may cause local increase of C5a. C5a is a chemotactic agent and an anaphylatoxin; it is essential in the innate immunity but it is also linked with the adaptive immunity. The increased production of C5a is connected with a number of inflammatory diseases.\n\nHuman polypeptide C5a contains 74 amino acids and has 11kDa. NMR spectroscopy proved that the molecule is composed of four helices and connected by peptide loops with three disulphide bonds between helix IV and II, III. There is a short 1.5 turn helix on N terminus but all agonist activity take place in the C terminus. C5a is rapidly metabolised by a serum enzyme carboxypeptidase B to a 72 amino acid form C5a des-Arg without C terminal arginine.\n\nC5a is an anaphylatoxin, causing increased expression of adhesion molecules on endothelium, contraction of smooth muscle, and increased vascular permeability. C5a des-Arg is a much less potent anaphylatoxin. Both C5a and C5a des-Arg can trigger mast cell degranulation, releasing proinflammatory molecules histamine and TNF-α. C5a is also an effective chemoattractant, initiating accumulation of complement and phagocytic cells at sites of infection or recruitment of antigen-presenting cells to lymph nodes.\nC5a plays a key role in increasing migration and adherence of neutrophils and monocytes to vessel walls. White blood cells are activated by upregulation of integrin avidity, the lipoxygenase pathway and arachidonic acid metabolism.\nC5a also modulates the balance between activating versus inhibitory IgG Fc receptors on leukocytes, thereby enhancing the autoimmune response.\n\nC5a interact with receptor protein C5aR (CD88) on the surface of target cells such as macrophages, neutrophils and endothelial cells. C5aR is a member of the G-protein-coupled receptor superfamily of proteins, predicted to have seven transmembrane helical domains of largely hydrophobic amino acid residues, forming three intra- and three extra-cellular loops, with an extracellular N-terminus and an intracellular C-terminus.\n\nC5a binding to the receptor is a two-stage process: an interaction between basic residues in the helical core of C5a and acidic residues in the extracellular N-terminal domain allows the C-terminus of C5a to bind to residues in the receptor transmembrane domains. The latter interaction leads to receptor activation, and the transduction of the ligand binding signal across the cell plasma membrane to the cytoplasmic G protein G type GNAI2.\n\nSensitivity of C5aR to C5a stimulation is enhanced by Lipopolysaccharides exposure, yet this is not due to C5aR upregulation. C5L2 is another C5a receptor that is thought to regulate the C5a-C5aR effects. There is apparently contradictory evidence showing decoy receptor activity conferring anti-inflammatory properties and also signalling activity conferring pro-inflammatory properties.\n\nC5a is a powerful inflammatory mediator, and seems to be a key factor in the development of pathology of many inflammatory diseases involving the complement system such as sepsis, rheumatoid arthritis, inflammatory bowel disease, systemic lupus erythemotosis, psoriasis. The inhibitor of C5a that can block its effects would be helpful in medical applications. One of inhibition substances is analogue of compstatin or monoclonal antibody against C5 called eculizumab however they are not specific only for C5a and interfere with others.\nAnother candidate is PMX53 or PMX205 that is highly specific for CD88 and effectively reduces inflammatory response. C5a has been identified as a key mediator of neutrophil dysfunction in sepsis, with antibody blockade of C5a improving outcomes in experimental models. This has also been shown in humans, with C5a-mediated neutrophil dysfunction predicting subsequent nosocomial infection and death from sepsis.\n", "id": "3272302", "title": "Complement component 5a"}
{"url": "https://en.wikipedia.org/wiki?curid=1840155", "text": "Biodistribution\n\nBiodistribution is a method of tracking where compounds of interest travel in an experimental animal or human subject. For example, in the development of new compounds for PET (positron emission tomography) scanning, a radioactive isotope is chemically joined with a peptide (subunit of a protein). This particular class of isotopes emits positrons (which are antimatter particles, equal in mass to the electron, but with a positive charge). When ejected from the nucleus, positrons encounter an electron, and undergo annihilation which produces two gamma rays travelling in opposite directions. These gamma rays can be measured, and when compared to a standard, quantified.\n\nFor example, a new compound would be injected intravenously into a group of 16-20 rodents (typically mice or rats). At intervals of 1, 2, 4, and 24 hours, smaller groups (4-5) of the animals are euthanized, then dissected. The organs of interest (usually: blood, liver, spleen, kidney, muscle, fat, adrenals, pancreas, brain, bone, stomach, small intestine, and upper and lower large intestine, etc.) are placed in pre-weighed containers, then into a device that measures gamma radiation. The results give a dynamic view of how the compound moves through the animal.\nA useful compound is one that is used either for the medical imaging of certain body parts or tumors (at low doses of radioactivity) or treating tumors (at high doses of radioactivity).\n\nIn gene therapy, gene delivery vectors, such as viruses, can be imaged according either to their particle biodistribution or their transduction pattern. The former means labeling the viruses with a contrast agent, being visible in some imaging modality, such as MRI or SPECT/PET and latter means visualising the marker gene of gene delivery vector to be visible by the means of immunohistochemical methods, optical imaging or even by PCR. Non-invasive imaging has gained popularity as the imaging equipment has become available for research use from clinics.\n\nFor example, avidin-displaying baculoviruses could be imaged in rat brain by coating them with biotinylated iron particles, rendering them visible in MR imaging. The biodistribution of the iron-virus particles was seen to concentrate on the choroid plexus cells of lateral ventricles.\n", "id": "1840155", "title": "Biodistribution"}
{"url": "https://en.wikipedia.org/wiki?curid=6732328", "text": "Target-site overlap\n\nIn a zinc finger protein, certain sequences of amino acid residues are able to recognise and bind to an extended target-site of four or even five nucleotides When this occurs in a ZFP in which the three-nucleotide subsites are contiguous, one zinc finger interferes with the target-site of the zinc finger adjacent to it, a situation known as target-site overlap. For example, a zinc finger containing arginine at position -1 and aspartic acid at position 2 along its alpha-helix will recognise an extended sequence of four nucleotides of the sequence 5'-NNG(G/T)-3'. The hydrogen bond between Asp and the N4 of either a cytosine or adenine base paired to the guanine or thymine, respectively defines these two nucleotides at the 3' position, defining a sequence that overlaps into the subsite of any zinc finger that may be attached N-terminally.\n\nTarget-site overlap limits the modularity of those zinc fingers which exhibit it, by restricting the number of situations to which they can be applied. If some of the zinc fingers are restricted in this way, then a larger repertoire is required to address the situations in which those zinc fingers cannot be used. Target-site overlap may also affect the selection of zinc fingers during by display, in cases where amino acids on a non-randomised finger, and the bases of its associated subsite, influence the binding of residues on the adjacent finger which contains the randomised residues. Indeed, attempts to derive zinc finger proteins targeting the 5'-(A/T)NN-3' family of sequences by site-directed mutagenesis of finger two of the C7 protein were unsuccessful due to the Asp of the third finger of said protein.\n\nThe extent to which target-site overlap occurs is largely unknown, with a variety of amino acids having shown involvement in such interactions. When interpreting the zinc finger repertoires presented by investigations using ZFP phage display, it is important to appreciate the effects that the rest of the zinc finger framework may have had in these selections. Since the problem only appears to occur in a limited number of cases, the issue is nullified in most situations in which there are a variety of suitable targets to choose from and only becomes a real issue if binding to a specific DNA sequence is required (e.g. blocking binding by endogenous DNA-binding proteins).\n\n", "id": "6732328", "title": "Target-site overlap"}
{"url": "https://en.wikipedia.org/wiki?curid=8942120", "text": "Vertical resistance\n\nThe term vertical resistance was first used by J.E. Vanderplank to describe single-gene resistance. This contrasted the term horizontal resistance which was used to describe many-gene resistance. Raoul A. Robinson further refined the definition of vertical resistance, emphasizing that in vertical resistance there are single genes for resistance in the host plant, and there are also single genes for parasitic ability in the parasite. This phenomenon is known as the gene-for-gene relationship, and it is the defining character of vertical resistance.\n", "id": "8942120", "title": "Vertical resistance"}
{"url": "https://en.wikipedia.org/wiki?curid=8942272", "text": "Horizontal resistance\n\nIn genetics, the term horizontal resistance was first used by J.E. Vanderplank to describe many-gene resistance, which is sometimes also called generalized resistance. This contrasts with the term vertical resistance which was used to describe single-gene resistance. Raoul A. Robinson further refined the definition of horizontal resistance. Unlike vertical resistance and parasitic ability, horizontal resistance and horizontal parasitic ability are entirely independent of each other in genetic terms.\n\nIn the first round of breeding for horizontal resistance, plants are exposed to pathogens and selected for partial resistance. Those with no resistance die, and plants unaffected by the pathogen have vertical resistance and are removed. The remaining plants have partial resistance and their seed is stored and bred back up to sufficient volume for further testing. The hope is that in these remaining plants are multiple types of partial-resistance genes, and by crossbreeding this pool back on itself, multiple partial resistance genes will come together and provide resistance to a larger variety of pathogens.\n\nSuccessive rounds of breeding for horizontal resistance proceed in a more traditional fashion, selecting plants for disease resistance as measured by yield. These plants are exposed to native regional pathogens, and given minimal assistance in fighting them.\n", "id": "8942272", "title": "Horizontal resistance"}
{"url": "https://en.wikipedia.org/wiki?curid=11502870", "text": "Arthur M. Lesk\n\nArthur Mallay Lesk, is a protein science researcher, who is a professor of biochemistry and molecular biology at the Pennsylvania State University in University Park.\n\nLesk received a bachelor's degree, magna cum laude, from Harvard University in 1961. He received his doctoral degree from Princeton University in 1966. He also received a master's degree from the University of Cambridge in the United Kingdom in 1999.\n\nLesk has made significant contributions to the study of protein evolution. He and Cyrus Chothia, working at the Medical Research Council (UK) Laboratory of Molecular Biology in Cambridge, United Kingdom, discovered the relationship between changes in amino-acid sequence and changes in protein structure by analyzing the mechanism of evolution in protein families. This discovery has provided the quantitative basis for the most successful and widely used method of structure prediction, known as homology modelling.\n\nLesk and Chothia also studied the conformations of antigen-binding sites of immunoglobulins. They discovered the “canonical-structure model” for the conformation of the complementarity-determining regions of antibodies, and they applied this model to the analysis of antibody-germ-line genes, including the prediction of the structure of the corresponding proteins. This work has supported the “humanization” of antibodies for therapy in the treatment of cancer. “This approach to cancer therapy is based on the observation of H. Waldmann that rats can raise antibodies against human cancers, but that the rat antibodies lead to immune responses, similar to allergies, in human patients,” Lesk explains. “Humanization of these antibodies is the formation of hybrid molecules that are more human than rat, but that retain the therapeutic activity while reducing the patient’s immune response.”\n\nLesk’s work also involves the detailed comparison of proteins in different structural states as a means for understanding the mechanisms that enable the proteins to change conformation, both as part of their normal activity and in disease. The discovery and analysis of these mechanisms was the key to understanding conformation changes in serine protease inhibitors, also known as serpins, mutations of which are an important cause of several diseases, including emphysema and certain types of inherited mental illness.\n\nLesk used a systematic analysis of protein-folding patterns to develop a mathematical representation that aids in the recognition and classification of these patterns. He also wrote the first computer program to generate schematic diagrams of proteins using molecular graphics, and he developed many algorithms now used by other researchers to analyze the structures of proteins.\n\nLesk used to be chair of the Task Group on Biological Macromolecules for the Committee on Data for Science and Technology (CODATA), which aimed to foster worldwide coordination of databases in molecular biology to enhance their quality and utility. He has given invited lectures and presentations related to his research at universities and professional conferences worldwide.\n\nLesk is a member of the American Physical Society. He has published 189 scientific articles and 10 books related to his research.\n\nPrior to joining Penn State during the fall semester of 2003, Lesk was on the faculty of the clinical school at the University of Cambridge from 1990 to 2003. He was a group leader in the biocomputing program at the European Molecular Biology Laboratory in Heidelberg, Germany, from 1987 to 1990; a visiting scientist at MRC Laboratory of Molecular Biology in Cambridge, United Kingdom, between 1977 and 1990; and a professor of chemistry at Fairleigh Dickinson University in New Jersey from 1971 to 1987. He has held visiting fellowships at the University of Otago in New Zealand and Monash University in Australia. He also is a Life Member of Clare Hall, Cambridge in the United Kingdom.\n\nAlong with Karl D. Hardman, Lesk wrote one of the first computer programs for generating the schematic diagram of protein structure. It is known to produce one of the most effective representations of the protein structures and employs the classification scheme for Ribbon Diagrams created by Jane Richardson. Although these schematic diagrams are less detailed compared to the other representations, such as, picture stimulating wire models or space-filling models, it is more effective in presenting the topological relationships among elements of secondary structure and protein, due to its simplistic structural expression. This was then further improved by creating a program to produce stereoscopic pairs of diagrams. As a result, the viewer’s ability to perceive spatial relationship in complex molecules was enhanced.\n\nThe basic operation of the program begins with the execution of line drawing. There are four phases involved in this program:\n\n\nArthur Lesk's son, Victor Lesk, followed his father into the field of structural biology and bioinformatics, and has held a post-doctoral research position with Michael Sternberg at Imperial College London.\n", "id": "11502870", "title": "Arthur M. Lesk"}
{"url": "https://en.wikipedia.org/wiki?curid=12151116", "text": "C-myc mRNA\n\nC-myc mRNA is a type of mRNA that serves as a template for the MYC protein which is implicated in the rapid growth of cancer cells. This mRNA is a topic of ongoing research to investigate the viability of preventing cancer growth by cleaving or degrading the c-myc mRNA.\n\n", "id": "12151116", "title": "C-myc mRNA"}
{"url": "https://en.wikipedia.org/wiki?curid=12234905", "text": "Ribosome-binding site\n\nA ribosome binding site, or ribosomal binding site, (RBS) is a sequence of nucleotides upstream of the start codon of an mRNA transcript that is responsible for the recruitment of a ribosome during the initiation of protein translation. Mostly, RBS refers to bacterial sequences, although internal ribosome entry sites (IRES) have been described in mRNAs of eukaryotic cells or viruses that infect eukaryotes. Ribosome recruitment in eukaryotes is generally mediated by the 5' cap present on eukaryotic mRNAs.\n\nThe RBS in prokaryotes is a region upstream of the start codon. This region of the mRNA has the consensus AGGAGG, also called the Shine-Dalgarno (SD) sequence. The complementary sequence (CCUCCU), called the anti-Shine-Dalgarno (ASD) is contained in the 3’ end of the 16S region of the smaller (30S) ribosomal subunit. Upon encountering the Shine-Dalgarno sequence, the ASD of the ribosome base pairs with it, after which translation is initiated.\n\nProkaryotic ribosomes begin translation of the mRNA transcript while DNA is still being transcribed. Thus translation and transcription are parallel processes. Bacterial mRNA are usually polycistronic and contain multiple ribosome binding sites. Translation initiation is the most highly regulated step of protein synthesis in prokaryotes.\n\nThe rate of translation depends on two factors: \nThe RBS sequence affects both of these factors.\n\nThe ribosomal protein S1 binds to adenine sequences upstream of the RBS. Increasing the concentration of adenine upstream of the RBS will increase the rate of ribosome recruitment.\n\nThe level of complementarity of the mRNA SD sequence to the ribosomal ASD greatly affects the efficiency of translation initiation. Richer complementarity results in higher initiation efficiency. It is worth noting that this only holds up to a certain point - having too rich of a complementarity is known to paradoxically decrease the rate of translation as the ribosome then happens to be bound too tightly to proceed downstream.\n\nThe optimal distance between the RBS and the start codon is variable - it depends on the portion of the SD sequence encoded in the actual RBS and its distance to the start site of a consensus SD sequence. Optimal spacing increases the rate of translation initiation once a ribosome has been bound. The composition of nucleotides in the spacer region itself was also found to affect the rate of translation initiation in one study.\n\nSecondary structures formed by the RBS can affect the translational efficiency of mRNA, generally inhibiting translation. These secondary structures are formed by H-bonding of the mRNA base pairs and are sensitive to temperature. At a higher-than-usual temperature (~42 °C), the RBS secondary structure of heat shock proteins becomes undone thus allowing ribosomes to bind and initiate translation. This mechanism allows a cell to quickly respond to an increase in temperature.\n\nRibosome recruitment in eukaryotes happens when eukaryote initiation factors elF4F and poly(A)-binding protein (PABP) recognize the 5' capped mRNA and recruit the 43S ribosome complex at that location.\n\nTranslation initiation happens following recruitment of the ribosome, at the start codon (underlined) found within the Kozak consensus sequence ACCAUGG. Since the Kozak sequence itself is not involved in the recruitment of the ribosome, it is not considered a ribosome binding site.\n\nEukaryotic ribosomes are known to bind to transcripts in a mechanism unlike the one involving the 5' cap, at a sequence called the internal ribosome entry site. This process is not dependent on the full set of translation initiation factors (although this depends on the specific IRES) and is commonly found in the translation of viral mRNA.\n\nThe identification of RBSs is used to determine the site of translation initiation in an unannotated sequence. This is referred to as N-terminal prediction. This is especially useful when multiple start codons are situated around the potential start site of the protein coding sequence.\n\nIdentification of RBSs is particularly difficult, because they tend to be highly degenerated. One approach to identifying RBS in E.coli is using neural networks. Another approach is using the Gibbs sampling method.\n\nThe Shine-Dalgarno sequence, of the prokaryotic RBS, was discovered by John Shine and Lynne Dalgarno in 1975. The Kozak consensus sequence was first identified by Marilyn Kozak in 1984 while she was in the Department of Biological Sciences at the University of Pittsburgh.\n\n", "id": "12234905", "title": "Ribosome-binding site"}
{"url": "https://en.wikipedia.org/wiki?curid=12280976", "text": "RNA integrity number\n\nThe RNA integrity number (RIN) is an algorithm for assigning integrity values to RNA measurements.\n\nThe integrity of RNA is a major concern for gene expression studies and traditionally has been evaluated using the 28S to 18S rRNA ratio, a method that has been shown to be inconsistent. This inconsistency arises because subjective, human interpretation is necessary to compare the 28S and 18S gel images. The RIN algorithm was devised to overcome this issue. The RIN algorithm is applied to electrophoretic RNA measurements, typically obtained using capillary gel electrophoresis, and based on a combination of different features that contribute information about the RNA integrity to provide a more universal measure. RIN has been demonstrated to be robust and reproducible in studies comparing it to other RNA integrity calculation algorithms, cementing its position as a preferred method of determining the quality of RNA to be analyzed.\n\nA major criticism to RIN is when using with plants or in studies of eukaryotic-prokaryotic cells interactions. The RIN algorithm is unable to differentiate eukaryotic/prokaryotic/chloroplastic ribosomal RNA, creating serious quality index underestimation in such situations.\n\nElectrophoresis is the process of separating nucleic acid species based on their length by applying an electric field to them. As nucleic acids are negatively charged, they are pushed by an electric field through a matrix, usually an agarose gel, with the smaller molecules being pushed farther, faster. Capillary electrophoresis is a technique whereby small amounts of a nucleic acid sample can be run on a gel in a very thin tube. There is a detector in the machine that can tell when nucleic acid samples pass through a detector, with smaller samples passing through first. This can produce an electropherogram such as the one in Figure 1, where length is related to time at which the samples pass this detector.\n\nA marker is a sample of known size run along with the sample so that the actual size of the rest of the sample can be known by comparing their running distance/time to be relative to this marker.\n\nRNA is a biological macromolecule made of sugars and nitrogenous bases that plays a number of crucial roles in all living cells. There are several subtypes of RNA, with the most prominent in the cell being tRNA (transfer RNA), rRNA (ribosomal RNA), and mRNA (messenger RNA). All three of these are involved in the process of translation, with the most prominent species (~85%) of cellular RNA being rRNA. As a result, this is the most immediately visible species when RNA is analyzed via electrophoresis and is thus used for determining RNA quality (see Computation, below). rRNA comes in various sizes, with those in mammals belonging to the sizes 5S, 18S, and 28S. The 28S and 5S rRNAs form the large subunit and the 18S forms the small subunit of the ribosome, the molecular machinery responsible for synthesizing proteins.\n\nRNases are ubiquitous and can often contaminate and subsequently degrade RNA samples in the laboratory, so RNA integrity can very easily be compromised, leading to a number of laboratory techniques designed to eliminate their impact. However, these methods are not fool-proof, and so samples can still be degraded, necessitating a method of measuring RNA integrity to ensure the trustworthiness and reproducibility of molecular assays, as RNA integrity is critical for proper results in gene expression studies, such as microarray analysis, Northern blots, or quantitative real-time PCR (qPCR). RNA that has been degraded has a direct impact on calculated expression levels, often leading to significantly decreased apparent expression.\n\nqPCR and similar techniques are very expensive, taking a good deal of both time and money, so continuing research being undertaken to decrease the cost while maintaining qPCR's accuracy and reproducibility for gene expression and other applications. RIN assessment allows a scientist to evaluate an experiment’s trustworthiness and reproducibility before incurring substantial costs in performing the gene expression studies.\n\nRIN has become a standard method of measuring integrity, even in meta-analyses such as \"Efficient extraction of small and large RNAs in bacteria for excellent total RNA sequencing and comprehensive transcriptome analysis\", wherein the authors analyzed a number of different RNA extraction methods, using the resultant RIN as a measure of their efficiency at producing high-quality RNA. RIN has also been used to test the efficiency of other techniques, such as microdissections, on maintaining RNA integrity in human cells, so it has a wide variety of uses in testing the quality of new techniques. RIN has become an industry standard to test various methods for the quality of their produced RNA, and it is also used in a wide variety of non-meta analyses so that the scientists are able to ensure that the quality of their RNA is trustworthy and their results are thus sound, often with a benchmark minimal RIN being used as a standard to ensure all of their RNA is high enough quality to yield reliable findings.\n\nAs RNA integrity has long been known to be a problem in molecular biology studies, there are a few methods that have been used historically to determine the integrity of RNA. The most popular has long been agarose gel electrophoresis with ethidium bromide staining, allowing one to visualize the bands from the rRNA peaks. The height of the 28S and 18S bands can be compared to each other, with a 2:1 ratio indicating non-degraded RNA. While this method is very cheap and easy, there are several issues with this method, primarily its subjectivity, leading to inconsistent, non-standardized RNA quality assessments, and the large amounts of RNA that are needed to visualize it on an agarose gel, which can be problematic if there is not much RNA to work with. There are also a number of different problems that can arise from agarose gel electrophoresis, such as poor loading, uneven running, and uneven staining that lead to increased variability in the accuracy of using agarose gel electrophoresis to determine RNA integrity. Clearly, a standardized, repeatable, objective method of determining RNA integrity was needed.\n\nAgilent Technologies sought to create one, and so created the RNA Integrity Number. The algorithm was generated by taking hundreds of samples and having specialists manually assign them all a value of 1 to 10 based on their integrity, with 10 being the highest. Adaptive learning tools using a Bayesian learning technique were used to generate an algorithm that could predict the RIN, predominantly by using the features listed below under \"Computation\". This allows for all Agilent software to produce the same RIN for a given RNA sample, standardizing the measurement and making it much less subjective than earlier methods.\n\nRIN for a sample is computed using several characteristics of an RNA electropherogram trace, with the first two listed below being most significant. RIN assigns an electropherogram a value of 1 to 10, with 10 being the least degraded. All the following descriptions apply to mammalian RNA because RNAs in other species have different rRNA sizes:\nThe total RNA ratio is calculated by taking the ratio of the area under the 18S and 28S rRNA peaks to the total area under the graph, a large number here is desired, indicating much of the rRNA is still at these sizes and thus little to no degradation has occurred. An ideal ratio can be seen in figure 1, where almost all of the RNA is in the 18S and 28S RNA peaks.\n\nFor the height of 28S peak, a large value is desired. 28S, the most prominent rRNA species, is used in RIN calculation as it is typically degraded more quickly than 18S rRNA, and so measuring its peak height allows for detection of the early stages of degradation. Again, this is seen in figure 1, where the 28S peak is the largest, and so this is good.\n\nThe fast region is the area between the 18S and 5S rRNA peaks on an electropherogram. Initially, as the fast area ratio value increases, it indicates degradation of the 18S and 28S rRNA to an intermediate size, though the ratio subsequently decreases as RNA degrades further, to even smaller sizes. Thus, a low value doesn't necessarily indicate either good or bad RNA integrity.\n\nA small marker height is desired, indicating only small amounts of RNA have been degraded and proceeded to the smallest lengths, indicated by the short marker. If a large number is found here, that indicates that large amounts of the rRNAs have been degraded to small pieces that would be found closer to this marker. This situation can be seen in the 'poor quality' RNA electropherogram found in figure 2, where the height of the peak over the marker (far left) is very large, so the RNA has been greatly degraded.\nIn prokaryotic samples, the algorithm is somewhat different, but the Agilent 2100 Bioanalyzer Expert software is able to calculate RIN for prokaryotic samples now as well. The difference likely arises from the fact that, while mammalian samples have 28S and 18S ribosomal RNAs as their predominant species, prokaryotic RNAs have the sizes shifted slightly smaller, to 23S and 16S, so the algorithm must be shifted to accommodate that. Another crucial fact about calculating prokaryotic RNA integrity numbers is that RIN has not been validated to the extent that it has for eukaryotic RNA. It has been shown that higher RIN values correlate with better downstream results in eukaryotes, but this hasn't been done as extensively for prokaryotes, so it may mean less in prokaryotes.\n\nThese electropherograms for calculating RIN are done using the Agilent Bioanalyzer machine, which is capable of performing electrophoresis and generating the electropherograms. The Agilent 2100 software is uniquely able to perform the RIN software, as the exact algorithm is proprietary, so there are additional important RNA electropherogram features that are used in its calculation that are not publicly available.\n\n", "id": "12280976", "title": "RNA integrity number"}
{"url": "https://en.wikipedia.org/wiki?curid=11660481", "text": "Fibrillarin\n\nrRNA 2'-O-methyltransferase fibrillarin is an enzyme that in humans is encoded by the \"FBL\" gene.\n\nThis gene product is a component of a nucleolar small nuclear ribonucleoprotein (snRNP) particle thought to participate in the first step in processing pre-ribosomal (r)RNA. It is associated with the U3, U8, and U13 small nucleolar RNAs and is located in the dense fibrillar component (DFC) of the nucleolus. The encoded protein contains an N-terminal repetitive domain that is rich in glycine and arginine residues, like fibrillarins in other species. Its central region resembles an RNA-binding domain and contains an RNP consensus sequence. Antisera from approximately 8% of humans with the autoimmune disease scleroderma recognize fibrillarin.\n\nFibrillarin is a component of several ribonucleoproteins including a nucleolar small nuclear ribonucleoprotein (SnRNP) and one of the two classes of small nucleolar ribonucleoproteins (snoRNPs). SnRNAs function in RNA splicing while snoRNPs function in ribosomal RNA processing. \n\nFibrillarin is associated with U3, U8 and U13 small nuclear RNAs in mammals and is similar to the yeast NOP1 protein. Fibrillarin has a well conserved sequence of around 320 amino acids, and contains 3 domains, an N-terminal Gly/Arg-rich region; a central domain resembling other RNA-binding proteins and containing an RNP-2-like consensus sequence; and a C-terminal alpha-helical domain. An evolutionarily related pre-rRNA processing protein, which lacks the Gly/Arg-rich domain, has been found in various archaebacteria.\n\nA study by Schultz et al. indicated that the K-turn binding 15.5-kDa protein (called Snu13 in yeast) interacts with spliceosome proteins hPRP31, hPRP3, hPRP4, CYPH and the small nucleolar ribonucleoproteins NOP56, NOP58, and fibrillarin. The 15.5-kDa protein has sequence similarity to other RNA-binding proteins such as ribosomal proteins S12, L7a, and L30 and the snoRNP protein NHP2. The U4/U6 snRNP contains 15.5-kDa protein. The 15.5-kDa protein also exists in a ribonucleoprotein complex that binds the U3 box B/C motif. The 15.5-kDa protein also exists as one of the four core proteins of the C/D small nucleolar ribonucleoprotein that mediates methylation of pre-ribosomal RNAs.\n\nStructural evidence supporting the idea that fibrillarin is the snoRNA methyltransferase has been reviewed.\n\nFibrillarin has been shown to interact with DDX5 and SMN1.\n\n", "id": "11660481", "title": "Fibrillarin"}
{"url": "https://en.wikipedia.org/wiki?curid=3004242", "text": "Virokine\n\nVirokines are proteins encoded by some large DNA viruses that are secreted by the host cell and serve to evade the host's immune system. Such proteins are referred to as virokines if they resemble cytokines, growth factors, or complement regulators; the term viroceptor is sometimes used if the proteins resemble cellular receptors. A third class of virally encoded immunomodulatory proteins consists of proteins that bind directly to cytokines. Due to the immunomodulatory properties of these proteins, they have been proposed as potentially therapeutically relevant to autoimmune diseases.\n\nThe primary mechanism of virokine interference with immune signaling is thought to be competitive inhibition of the binding of host signaling molecules to their target receptors. Virokines occupy binding sites on host receptors, thereby inhibiting access by signaling molecules. Viroceptors mimic host receptors and thus divert signaling molecules from finding their targets. Cytokine-binding proteins bind to and sequester cytokines, occluding the binding surface through which they interact with receptors. The effect is to attenuate and subvert host immune response.\n\nThe term \"virokine\" was coined by National Institutes of Health virologist Bernard Moss. The early 1990s saw several reports of virally encoded proteins with sequence homology to immune proteins, followed by reports of the cowpox and vaccinia viruses directly interfering with key immune regulator IL1B. The first identified virokine was an epidermal growth factor-like protein found in myxoma viruses.\n\nMuch of the early work on virokines involved vaccinia virus, which was discovered to secrete proteins that promote proliferation of neighboring cells and block complement immune activity leading to inflammation.\n\nThe immunomodulatory proteins, including virokines, in the poxvirus family have been extensively studied in the context of the evolution of the family. Virokines in this family are thought to have been acquired from host genes and from other viruses through horizontal gene transfer. Similar observations have been made in the herpesvirus family; for example, Epstein-Barr virus encodes an interleukin protein with high sequence identity to the human interleukin-10, suggesting a recent evolutionary origin.\n\n", "id": "3004242", "title": "Virokine"}
{"url": "https://en.wikipedia.org/wiki?curid=11664690", "text": "Magnetofection\n\nMagnetofection is a simple and highly efficient transfection method that uses magnetic fields to concentrate particles containing nucleic acid into the target cells. This method attempts to unite the advantages of the popular biochemical (cationic lipids or polymers) and physical (electroporation, gene gun) transfection methods in one system while excluding their inconveniences (low efficiency, toxicity). Magnetofection is commercialized by OZ Biosciences and is registered as a trademark.\n\nThe magnetofection principle is to associate nucleic acids with cationic magnetic nanoparticles: these molecular complexes are then concentrated and transported into cells supported by an appropriate magnetic field. In this way, the magnetic force allows a very rapid concentration of the entire applied vector dose onto cells, so that 100% of the cells get in contact with a significant vector dose.\n\nMagnetofection has been adapted to all types of nucleic acids (DNA, siRNA, dsRNA, shRNA, mRNA, ODN), non viral transfection systems (transfection reagents) and viruses. It has been successfully tested on a broad range of cell lines, hard-to-transfect and primary cells. Several optimized and efficient magnetic nanopartciles formulations have been specifically developed for several types applications such as DNA, siRNA, and primary neuron transfection as well as viral applications.\n\nThe magnetic nanoparticles are made of iron oxide, which is fully biodegradable, coated with specific cationic proprietary molecules varying upon the applications. Their association with the gene vectors (DNA, siRNA, ODN, virus, etc.) is achieved by salt-induced colloidal aggregation and electrostatic interaction. The magnetic particles are then concentrated on the target cells by the influence of an external magnetic field generated by magnets. The cellular uptake of the genetic material is accomplished by endocytosis and pinocytosis, two natural biological processes. Consequently, membrane architecture & structure stays intact, in contrast to other physical transfection methods that damage the cell membrane.\n\nThe nucleic acids are then released into the cytoplasm by different mechanisms depending upon the formulation used: 1) is the proton sponge effect caused by cationic polymers coated on the nanoparticles that promote endosome osmotic swelling, disruption of the endosome membrane and intracellular release of DNA form, 2) is the destabilization of endosome by cationic lipids coated on the particles that release the nucleic acid into cells by flip-flop of cell negative lipids and charge neutralization and 3) is the usual viral infection mechanism when virus is used. Magnetofection works for primary cells and hard to transfect cells that are not dividing or slowly dividing, meaning that the genetic materials can go to the cell nucleus without cell division. Coupling magnetic nanoparticles to gene vectors of any kind results in a dramatic increase of the uptake of these vectors and consequently high transfection efficiency.\n\nThe biodegradable cationic magnetic nanoparticles are not toxic at the recommended doses and even higher doses. Gene vectors / magnetic nanoparticles complexes are seen into cells after 10–15 minutes that is much faster than any other transfection method. After 24, 48 or 72 hours, most of the particles are localized in the cytoplasm, in vacuoles (membranes surrounded structure into cells) and occasionally in the nucleus.\n\nhttp://www.ozbiosciences.com/magnetofection.html\n", "id": "11664690", "title": "Magnetofection"}
{"url": "https://en.wikipedia.org/wiki?curid=12707594", "text": "Magnetic-activated cell sorting\n\nMagnetic-activated cell sorting (MACS) is a method for separation of various cell populations depending on their surface antigens (CD molecules) invented by Miltenyi Biotec. The name MACS is a registered trademark of the company.\n\nThe method is performed using Miltenyi Biotec's MACS Technology, which uses superparamagnetic nanoparticles and columns. The superparamagnetic nanoparticles are of the order of 100 nm. They are used to tag the targeted cells in order to capture them inside the column. The column is placed between permanent magnets so that when the magnetic particle-cell complex passes through it, the tagged cells can be captured. The column consists of steel wool which increases the magnetic field gradient to maximize separation efficiency when the column is placed between the permanent magnets.\n\nMagnetic-activated cell sorting is a commonly used method in areas like immunology, cancer research, neuroscience, and stem cell research. Miltenyi Biotec itself has recently developed MicroBeads which are magnetic nanoparticles conjugated to antibodies which can be used to target specific cells.\n\nThe MACS method allows cells to be separated by incubating with magnetic nanoparticles coated with antibodies against a particular surface antigen. This causes the cells expressing this antigen to attach to the magnetic nanoparticles. Afterwards the cell solution is transferred on a column placed in a strong magnetic field. In this step, the cells attached to the nanoparticles (expressing the antigen) stay on the column, while other cells (not expressing the antigen) flow through. With this method, the cells can be separated positively or negatively with respect to the particular antigen(s).\n\nIn positive selection the cells expressing the antigen(s) of interest, which attached to the magnetic column, are washed out to a separate vessel, after removing the column from the magnetic field. This method is useful for isolation of a particular cell type, for instance CD4 lymphocytes.\n\nIn negative selection the antibody used is against surface antigen(s) which are known to be present on cells that are not of interest. After administration of the cells/magnetic nanoparticles solution onto the column the cells expressing these antigens bind to the column and the fraction that goes through is collected, as it contains almost no cells with these undesired antigens.\n\nMagnetic nanoparticles conjugated to an antibody against an antigen of interest are not always available, but there is a way to circumvent it. Since fluorophore-conjugated antibodies are much more prevalent, it is possible to use magnetic nanoparticles coated with anti-fluorochrome antibodies. They are incubated with the fluorescent-labelled antibodies against the antigen of interest and may thus serve for cell separation with respect to the antigen.\n\n\n", "id": "12707594", "title": "Magnetic-activated cell sorting"}
{"url": "https://en.wikipedia.org/wiki?curid=7150276", "text": "Hydrophilic interaction chromatography\n\nHydrophilic interaction chromatography (or hydrophilic interaction liquid chromatography, HILIC) is a variant of normal phase liquid chromatography that partly overlaps with other chromatographic applications such as ion chromatography and reversed phase liquid chromatography. HILIC uses hydrophilic stationary phases with reversed-phase type eluents. The name was suggested by Dr. Andrew Alpert in his 1990 paper on the subject. He described the chromatographic mechanism for it as liquid-liquid partition chromatography where analytes elute in order of increasing polarity, a conclusion supported by a review and re-evaluation of published data. \n\nAny polar chromatographic surface can be used for HILIC separations. Even non-polar bonded silicas have been used with extremely high organic solvent composition, when the silica used for the chromatographic media was particularly polar. With that exception, HILIC phases can be grouped into five categories of neutral polar or ionic surfaces: \n\nA typical mobile phase for HILIC chromatography includes acetonitrile (\"MeCN\", also designated as \"ACN\") with a small amount of water. However, any aprotic solvent miscible with water (e.g. THF or dioxane) can be used. Alcohols can also be used, however, their concentration must be higher to achieve the same degree of retention for an analyte relative to an aprotic solvent - water combination. See also Aqueous Normal Phase Chromatography\n\nIt is commonly believed that in HILIC, the mobile phase forms a water-rich layer on the surface of the polar stationary phase vs. the water-deficient mobile phase, creating a liquid/liquid extraction system. The analyte is distributed between these two layers. However, HILIC is more than just simple partitioning and includes hydrogen donor interactions between neutral polar species as well as weak electrostatic mechanisms under the high organic solvent conditions used for retention. This distinguishes HILIC as a mechanism distinct from ion exchange chromatography. The more polar compounds will have a stronger interaction with the stationary aqueous layer than the less polar compounds. Thus, a separation based on a compound's polarity and degree of solvation takes place.\n\nIonic additives, such as ammonium acetate and ammonium formate, are usually used to control the mobile phase pH and ion strength. In HILIC they can also contribute to the polarity of the analyte, resulting in differential changes in retention. For extremely polar analytes (e.g. aminoglycoside antibiotics (gentamicin) or Adenosine triphosphate), higher concentrations of buffer (ca. 100mM) are required to assure that the analyte will be in a single ionic form. Otherwise asymmetric peak shape, chromatographic tailing, and/or poor recovery from the stationary phase will be observed. For the separation of neutral polar analytes (e.g. carbohydrates), no buffer is necessary.\n\nUse of other salts such as 100-300mM sodium perchlorate, which are soluble in high-organic solvent mixtures (ca. 70%-90% acetonitrile), can be used to increase the mobile phase polarity to effect elution. These salts are not volatile, so this technique is less useful with a mass spectrometer as the detector. Usually a gradient (to increasing amounts of water) is enough to promote elution.\n\nAll ions partition into the stationary phase to some degree, so an occasional \"wash\" with water is required to ensure a reproducible stationary phase.\n\nThe HILIC mode of separation is used extensively for separation of some biomolecules, organic and some inorganic molecules by differences in polarity. Its utility has increased due to the simplified sample preparation for biological samples, when analyzing for metabolites, since the metabolic process generally results in the addition of polar groups to enhance elimination from the cellular tissue. This separation technique is also particularly suitable for glycosylation analysis and quality assurance of glycoproteins and glycoforms in biologic medical products. For the detection of polar compounds with the use of electrospray-ionization mass spectrometry as a chromatographic detector, HILIC can offer a ten fold increase in sensitivity over reversed-phase chromatography because the organic solvent is much more volatile.\n\nWith surface chemistries that are weakly ionic, the choice of pH can affect the ionic nature of the column chemistry. Properly adjusted, the pH can be set to reduce the selectivity toward functional groups with the same charge as the column, or enhance it for oppositely charged functional groups. Similarly, the choice of pH affects the polarity of the solutes. However, for column surface chemistries that are strongly ionic, and thus resistant to pH values in the mid-range of the pH scale (pH 3.5-8.5), these separations will be reflective of the polarity of the analytes alone, and thus might be easier to understand when doing methods development.\n\nIn 2008, Alpert coined the term, ERLIC (electrostatic repulsion hydrophilic interaction chromatography), for HILIC separations where an ionic column surface chemistry is used to repel a common ionic polar group on an analyte or within a set of analytes, to facilitate separation by the remaining polar groups. Electrostatic effects have an order of magnitude stronger chemical potential than neutral polar effects. This allows one to minimize the influence of a common, ionic group within a set of analyte molecules; or to reduce the degree of retention from these more polar functional groups, even enabling isocratic separations in lieu of a gradient in some situations. His subsequent publication further described orientation effects which others have also called ion-pair normal phase or e-HILIC, reflecting retention mechanisms sensitive to a particular ionic portion of the analyte, either attractive or repulsive. ERLIC (eHILIC) separations need not be isocratic, but the net effect is the reduction of the attraction of a particularly strong polar group, which then requires less strong elution conditions, and the enhanced interaction of the remaining polar (opposite charged ionic, or non-ionic) functional groups of the analyte(s).\n\nFor example, one could use a cation exchange (negatively charged) surface chemistry for ERLIC separations to reduce the influence on retention of anionic (negatively charged) groups (the phosphates of nucleotides or of phosphonyl antibiotic mixtures; or sialic acid groups of modified carbohydrates) to now allow separation based more on the basic and/or neutral functional groups of these molecules. Modifying the polarity of a weakly ionic group (e.g. carboxyl) on the surface is easily accomplished by adjusting the pH to be within two pH units of that group's pKa. For strongly ionic functional groups of the surface (i.e. sulfates or phosphates) one could instead use a lower amount of buffer so the residual charge is not completely ion paired. An example of this would be the use of a 12.5mM (rather than the recommended >20mM buffer), pH 9.2 mobile phase on a polymeric, zwitterionic, betaine-sulfonate surface to separate phosphonyl antibiotic mixtures (each containing a phosphate group). This enhances the influence of the column's sulfonic acid functional groups of its surface chemistry over its, slightly diminished (by pH), quaternary amine. Commensurate with this, these analytes will show a reduced retention on the column eluting earlier, and in higher amounts of organic solvent, than if a neutral polar HILIC surface were used. This also increases their detection sensitivity by negative ion mass spectrometry.\n\nBy analogy to the above, one can use an anion exchange (positively charged) column surface chemistry to reduce the influence on retention of cationic (positively charged) functional groups for a set of analytes, such as when selectively isolating phosphorylated peptides or sulfated polysaccharide molecules. Use of a pH between 1 and 2 pH units will reduce the polarity of two of the three ionizable oxygens of the phosphate group, and thus will allow easy desorption from the (oppositely charged) surface chemistry. It will also reduce the influence of negatively charged carboxyls in the analytes, since they will be protonated at this low a pH value, and thus contribute less overall polarity to the molecule. Any common, positively charged amino groups will be repelled from the column surface chemistry and thus these conditions enhance the role of the phosphate's polarity (as well as other neutral polar groups) in the separation.\n", "id": "7150276", "title": "Hydrophilic interaction chromatography"}
{"url": "https://en.wikipedia.org/wiki?curid=11395514", "text": "MEROPS\n\nMEROPS is an on-line database for peptidases (also known as proteases, proteinases and proteolytic enzymes) and their inhibitors. The classification scheme for peptidases was published by Rawlings & Barrett in 1993, and that for protein inhibitors by Rawlings \"et al.\" in 2004.. The most recent version, MEROPS 12.0, was released in September 2017 \n\nThe classification is based on similarities at the tertiary and primary structural levels. Comparisons are restricted to that part of the sequence directly involved in the reaction, which in the case of a peptidase must include the active site, and for a protein inhibitor the reactive site. The classification is hierarchical: sequences are assembled into families, and families are assembled into clans. Each peptidase, family, and clan has a unique identifier.\n\nThe families of peptidases are constructed by comparisons of amino acid sequences. A family is assembled around a \"type example\", the sequence of a well-characterized peptidase or inhibitor. All other sequences in the family must be related to the family type example, either directly or through a transitive relationship involving one or more sequences already shown to be family members. Typically, FastA or BlastP is used to establish sequence relationships, with an expect value of 0.001 or lower taken to be statistically significant. HMMER or psi-blast searches are used for adding sequences which are distantly related to a family. Each family is identified by a letter representing the catalytic type of the peptidases it contains followed by an arbitrary unique number. \n\nSome families are divided into subfamilies due to evidence of very ancient divergence within the family. The divergence corresponds to more than 150 accepted point mutations per 100 amino acid residues.\n\nThe similarity in three-dimensional structures supports the evidence that many of the families do share common ancestry with others. \"Clan\" is used to describe such a group of families. A clan is also assembled around a type example, this being the structure of a well-characterized peptidase or inhibitor. A family is included in a clan if the tertiary structure of a family member can be shown to be related to that of the clan type example. Typically, DALI is used to establish clan membership, with a z score of 6.00 standard deviation units or above considered to be statistically significant. For peptidases, other evidence to indicate that families are related when a tertiary structure is absent includes the same order of catalytic residues in the sequences.\n\nEach family, clan, peptidase, and inhibitor has a unique identifier. Description and example of identifiers are shown in the table below.\n\n\n", "id": "11395514", "title": "MEROPS"}
{"url": "https://en.wikipedia.org/wiki?curid=7658869", "text": "MICAD\n\nThe Molecular Imaging and Contrast Agent Database (MICAD) is a freely accessible online source of information on \"in vivo\" molecular imaging agents. It was established as a key component of the \"Molecular Libraries and Imaging\" program of the NIH Roadmap, a set of major inter-agency initiatives accelerating medical research and the development of new, more specific therapies for a wide range of diseases.\n\nMICAD includes agents developed for imaging modalities such as positron emission tomography (PET), single photon emission computed tomography (SPECT), magnetic resonance imaging (MRI), ultrasound, computed tomography, optical imaging, and planar gamma imaging. It contains textual information, references, numerous links to MEDLINE and to other relevant resources from the National Center for Biotechnology Information (NCBI).\n\nMICAD is edited by a team of scientific editors and curators at the National Library of Medicine, NIH. It is being developed under the guidance of a trans-NIH panel of experts in the field. Members of the imaging community are invited to contribute to the MICAD database by writing and submitting entries (chapters) on agents of their choice for online publication. The MICAD staff will work with individual guest authors to prepare the chapters. Interested members of the imaging community should contact the MICAD staff at micad@ncbi.nlm.nih.gov.\n", "id": "7658869", "title": "MICAD"}
{"url": "https://en.wikipedia.org/wiki?curid=5847984", "text": "Fragment molecular orbital\n\nThe fragment molecular orbital method (FMO) is a computational method that can compute very large molecular systems with thousands of atoms using ab initio quantum-chemical wave functions.\n\nThe fragment molecular orbital method (FMO) was developed by K. Kitaura and coworkers in 1999. FMO is deeply interconnected with the energy decomposition analysis (EDA) by Kitaura and Morokuma, developed in 1976. The main use of FMO is to compute very large molecular systems by dividing them into fragments and performing ab initio or density functional quantum-mechanical calculations of fragments and their dimers, whereby the Coulomb field from the whole system is included. The latter feature allows fragment calculations without using caps.\n\nThe mutually consistent field (MCF) method had introduced the idea of self-consistent fragment calculations in their embedding potential, which was later used with some modifications in various methods including FMO. There had been other methods related to FMO including the incremental correlation method by H. Stoll (1992). Also FMO bears some similarity to the method by J. Gao (1997), the applicability of which for condensed phase systems was subsequently demonstrated by carrying out a statistical mechanical Monte Carlo simulation of liquid water in 1998; this method was later renamed as the explicit polarization (X-Pol) theory. The incremental method uses formally the same many-body expansion of properties as FMO, although the exact meaning of terms is different. The difference between X-Pol and FMO is in the approximation for estimating the pair interactions between fragments. X-Pol is closely related to the one-body expansion used in FMO (FMO1) in terms of the electrostatics, but other interactions are treated differently.\n\nLater, other methods closely related to FMO were proposed including the kernel energy method of L. Huang and the electrostatically embedded many-body expansion by E. Dahlke,\nS. Hirata and later M. Kamiya suggested approaches also very closely related to FMO. Effective fragment molecular orbital (EFMO) method combines some features of the effective fragment potentials (EFP) and FMO. A detailed perspective on the fragment-based method development can be found in a recent review.\n\nIn addition to the calculation of the total properties, such as the energy,\nenergy gradient, dipole moment etc., the pair interaction is obtained for\neach pair of fragments. This pair interaction energy can be further\ndecomposed into electrostatic, exchange, charge transfer and dispersion\ncontributions. This analysis is known as the pair interaction energy\ndecomposition analysis (PIEDA) and it can be thought of as FMO-based EDA.\nAlternatively, configuration analysis for fragment interaction (CAFI) and fragment interaction analysis based on local MP2 (FILM) were suggested within the FMO framework.\n\nIn FMO, various wave functions can be used for ab initio calculations of fragments and their dimers, such as Hartree–Fock, Density functional theory (DFT), Multi-configurational self-consistent field (MCSCF), time-dependent DFT (TDDFT), configuration interaction (CI), second order Møller–Plesset perturbation theory (MP2), and coupled cluster (CC). The solvent effects can be treated with the Polarizable continuum model (PCM). The FMO code is very efficiently parallelized utilising the generalized distributed data interface (GDDI) and hundreds of CPUs can be used with nearly perfect scaling.\n\nIn the FMO book published in 2009, one can find 10 illustrated chapters written by the experts in the FMO development and applications, as well as a CDROM with\nannotated samples of input and output files, Facio modelling software and video tutorials (AppliGuide movies, showing mouse clicks) for treating difficult PDB files with Facio. In addition to this book, there are several chapters published in other books.\n\nThere are three general reviews of FMO published.\n\nIn 2013-2014, a Japanese journal, CICSJ Bulletin, published a series of FMO papers in Japanese (about 100 pages in total), which give a representative summary of the recent FMO development and applications done in Japan, including papers on the GAMESS/FMO interface in Facio and developing an OpenMP version of GAMESS/FMO on the K computer.\n\nThe largest system size computed with FMO so far is a slab of fullerite surface, containing 1,030,440 atoms, whose geometry was fully optimized using FMO-DFTB recently implemented in GAMESS.\n\nThere are two main application fields of FMO: biochemistry and molecular dynamics of chemical reactions in solution. In addition, there is an emerging field of inorganic applications. \nIn 2005, an application of FMO to the calculation of the ground electronic state of photosynthetic protein with more than 20,000 atoms was distinguished with the best technical paper award at Supercomputing 2005.\nA number of applications of FMO to biochemical problems has been published, for instance, to Drug design , quantitative structure-activity relationship (QSAR) as well as the studies of excited states and chemical reactions of biological systems. In the recent development (2008), the adaptive frozen orbital (AFO) treatment of the detached bonds was suggested for FMO, making it possible to study solids, surfaces and nano systems, such as silicon nanowrires. FMO-TDDFT was also applied to the excited states of molecular crystals (quinacridone).\n\nAmong inorganic systems, silica-related materials (zeolites, mesoporous nanoparticles and silica surfaces) were studied with FMO,\nas well as ionic liquids and boron nitride ribbons.\n\nThe FMO method is implemented in GAMESS (US), ABINIT-MP and PAICS software packages, distributed free of charge.\n\nIn the earlier stage, the preparation of the GAMESS input files was facilitated with the FMOutil software. Later, various parts of FMOutil were\nincorporated in the new graphical user interface called fu. \nFu is a general open-source GUI not limited to FMO or GAMESS. It is written mainly in Python and some critical modules\nare in FORTRAN. Fu is distributed under BSD license so anybody can modify it and redistributed freely. \nIn addition, another graphical user interface Facio developed by M. Suenaga has a very convenient specialised support of FMO (in addition to other features), with which an automatic fragmentation of molecular clusters, proteins, nucleotides, saccharides and any combination thereof (e.g., DNA and protein complexes in explicit solvent) can be done in a few minutes, and a manual fragmentation of solids and surfaces can be accomplished by clicking the bonds to be detached. Facio can also visualise results of FMO calculations, such as the pair interactions.\n\n\n", "id": "5847984", "title": "Fragment molecular orbital"}
{"url": "https://en.wikipedia.org/wiki?curid=12830500", "text": "Restriction landmark genomic scanning\n\nRestriction landmark genomic scanning (RLGS) is a genome analysis method for rapid simultaneous visualization of thousands of landmarks, or restriction sites. Using a combination of restriction enzymes some of which are specific to DNA modifications, the technique can be used to visualize differences in methylation levels across the genome of a given organism. RLGS employs direct labeling of DNA, which is first cut by a specific series of restriction enzymes, and then labeled by a radioactive isotope (usually phosphorus-32). A two-dimensional electrophoresis process is then employed, yielding high-resolution results. The radioactive second-dimension gel is then allowed to expose a large sheet of film. The radiation produced by the radioactive labeling will cause the film to be exposed wherever the restriction fragments have migrated during electrophoresis. The film is then developed, yielding a visual representation of the results in the form of an autoradiograph. The same combination of restriction enzymes will produce the same pattern of 'spots' from samples from the same organisms, but different patterns for different types of organism. For example, human and mouse DNA will produce distinctly different patterns when treated with the same combination of enzymes. These finished auto-rads can be examined against each other, revealing any changes in gene expression that lead to visual differences in the film. Each autoradiograph contains thousands of spots, each corresponding to a labeled DNA restriction landmark.\n\nRLGS becomes useful when doing whole-genome scans, and can effectively do the work of thousands of polymerase chain reactions at once. It readily detects alterations deviating from normal, and thus is exceptionally effective in identifying hyper/hypomethylation in tumors, deletions or amplifications of genes, or simply changes in gene expression throughout the development of an organism.\n\n", "id": "12830500", "title": "Restriction landmark genomic scanning"}
{"url": "https://en.wikipedia.org/wiki?curid=6997526", "text": "Terminal restriction fragment length polymorphism\n\nTerminal restriction fragment length polymorphism (TRFLP or sometimes T-RFLP) is a molecular biology technique for profiling of microbial communities based on the position of a restriction site closest to a labelled end of an amplified gene. The method is based on digesting a mixture of PCR amplified variants of a single gene using one or more restriction enzymes and detecting the size of each of the individual resulting terminal fragments using a DNA sequencer. The result is a graph image where the x-axis represents the sizes of the fragment and the y-axis represents their fluorescence intensity.\n\nTRFLP is one of several molecular methods aimed to generate a fingerprint of an unknown microbial community. Other similar methods include DGGE, TGGE, ARISA, ARDRA, PLFA, etc.\n<br>These relatively high throughput methods were developed in order to reduce the cost and effort in analyzing microbial communities using a clone library. The method was first described by Liu and colleagues in 1997 which employed the amplification of the 16S rDNA target gene from the DNA of several isolated bacteria as well as environmental samples.\n<br>Since then the method has been applied for the use of other marker genes such as the functional marker gene pmoA to analyze methanotrophic communities.\n\nLike most other community analysis methods, TRFLP is also based on PCR amplification of a target gene. In the case of TRFLP, the amplification is performed with one or both the primers having their 5’ end labeled with a fluorescent molecule. In case both primers are labeled, different fluorescent dyes are required. While several common fluorescent dyes can be used for the purpose of tagging such as 6-carboxyfluorescein (6-FAM), ROX, carboxytetramethylrhodamine (TAMRA, a rhodamine-based dye), and hexachlorofluorescein (HEX), the most widely used dye is 6-FAM. The mixture of amplicons is then subjected to a restriction reaction, normally using a four-cutter restriction enzyme. Following the restriction reaction, the mixture of fragments is separated using either capillary or polyacrylamide electrophoresis in a DNA sequencer and the sizes of the different terminal fragments are determined by the fluorescence detector. Because the excised mixture of amplicons is analyzed in a sequencer, only the terminal fragments (i.e. the labeled end or ends of the amplicon) are read while all other fragments are ignored. Thus, T-RFLP is different from ARDRA and RFLP in which all restriction fragments are visualized. In addition to these steps the TRFLP protocol often includes a cleanup of the PCR products prior to the restriction and in case a capillary electrophoresis is used a desalting stage is also performed prior to running the sample.\n\nThe result of a T-RFLP profiling is a graph called electropherogram which is an intensity plot representation of an electrophoresis experiment (gel or capillary). In an electropherogram the X-axis marks the sizes of the fragments while the Y-axis marks the fluorescence intensity of each fragment. Thus, what appears on an electrophoresis gel as a band appears as a peak on the electropherogram whose integral is its total fluorescence. In a T–RFLP profile each peak assumingly corresponds to one genetic variant in the original sample while its height or area corresponds to its relative abundance in the specific community. Both assumptions listed above, however, are not always met. Often, several different bacteria in a population might give a single peak on the electropherogram due to the presence of a restriction site for the particular restriction enzyme used in the experiment at the same position. To overcome this problem and to increase the resolving power of this technique a single sample can be digested in parallel by several enzymes (often three) resulting in three T-RFLP profiles per sample each resolving some variants while missing others. Another modification which is sometimes used is to fluorescently label the reverse primer as well using a different dye, again resulting in two parallel profiles per sample each resolving a different number of variants.\n\nIn addition to convergence of two distinct genetic variants into a single peak artifacts might also appear, mainly in the form of false peaks. False peaks are generally of two types: background “noises” and “pseudo” TRFs. Background (noise) peaks are peaks resulting from the sensitivity of the detector in use. These peaks are often small in their intensity and usually form a problem in case the total intensity of the profile is low (i.e. low concentration of DNA). Because these peaks result from background noise they are normally irreproducible in replicate profiles, thus the problem can be tackled by producing a consensus profile from several replicates or by eliminating peaks below a certain threshold. Several other computational techniques were also introduced in order to deal with this problem. Pseudo TRFs, on the other hand, are reproducible peaks and are linear to the amount of DNA loaded. These peaks are thought to be the result of ssDNA annealing on to itself and creating double stranded random restriction sites which are later recognized by the restriction enzyme resulting in a terminal fragment which does not represent any genuine genetic variant. It has been suggested that applying a DNA exonuclease such as the Mung bean exonuclease prior to the digestion stage might eliminate such artifact.\n\nThe data resulting from the electropherogram is normally interpreted in one of the following ways.\n\nIn pattern comparison the general shapes of electropherograms of different samples are compared for changes such as presence-absence of peaks between treatments, their relative size, etc.\n\nIf a clone library is constructed in parallel to the T-RFLP analysis then the clones can be used to assess and interpret the T-RFLP profile. In this method the TRF of each clone is determined either directly (i.e. performing T-RFLP analysis on each single clone) or by \"in silico\" analysis of that clone’s sequence. By comparing the T-RFLP profile to a clone library it is possible to validate each of the peaks as genuine as well as to assess the relative abundance of each variant in the library.\n\nSeveral computer applications attempt to relate the peaks in an electropherogram to specific bacteria in a database. Normally this type of analysis is done by simultaneously resolving several profiles of a single sample obtained with different restriction enzymes. The software then resolves the profile by attempting to maximize the matches between the peaks in the profiles and the entries in the database so that the number of peaks left without a matching sequence is minimal. The software withdraws from the database only those sequences which have their TRFs in all analyzed profiles.\n\nA recently growing way to analyze T-RFLP profiles is use multivariate statistical methods to interpret the T-RFLP data. Usually the methods applied are those commonly used in ecology and especially in the study of biodiversity. Among them ordinations and cluster analysis are the most widely used.\nIn order to perform multivariate statistical analysis on T-RFLP data, the data must first be converted to table known as a “sample by species table“ which depicts the different samples (T-RFLP profiles) versus the species (T-RFS) with the height or area of the peaks as values.\n\nAs T-RFLP is a fingerprinting technique its advantages and drawbacks are often discussed in comparison with other similar techniques, mostly DGGE.\n\nThe major advantage of T-RFLP is the use of an automated sequencer which gives highly reproducible results for repeated samples. Although the genetic profiles are not completely reproducible and several minor peaks which appear are irreproducible the overall shape of the electropherogram and the ratios of the major peaks are considered reproducible. The use of an automated sequencer which outputs the results in a digital numerical format also enables an easy way to store the data and compare different samples and experiments. The numerical format of the data can and has been used for relative (though not absolute) quantification and statistical analysis. Although sequence data cannot be definitively inferred directly from the T-RFLP profile, ‘’in-silico’’ assignment of the peaks to existing sequences is possible to a certain extent.\n\nBecause T-RFLP relies on DNA extraction methods and PCR, the biases inherent to both will affect the results of the analysis. Also, the fact that only the terminal fragments are being read means that any two distinct sequences which share a terminal restriction site will result in one peak only on the electropherogram and will be indistinguishable. Indeed, when T-RFLP is applied on a complex microbial community the result is often a compression of the total diversity to normally 20-50 distinct peaks only representing each an unknown number of distinct sequences. Although this phenomenon makes the T-RFLP results easier to handle, it naturally introduces biases and oversimplification of the real diversity. Attempts to minimize (but not overcome) this problem are often done by applying several restriction enzymes and/ or labeling both primers with a different fluorescent dye. The inability to retrieve sequences from T-RFLP often leads to the need to construct and analyze one or more clone libraries in parallel to the T-RFLP analysis which adds to the effort and complicates analysis. The possible appearance of false (pseudo) T-RFs, as discussed above, is yet another drawback. To handle this researchers often only consider peaks which can be affiliated to sequences in a clone library.\n\n\n\n\n\n\n", "id": "6997526", "title": "Terminal restriction fragment length polymorphism"}
{"url": "https://en.wikipedia.org/wiki?curid=8255258", "text": "DNA separation by silica adsorption\n\nDNA separation by silica adsorption is a method of DNA separation that is based on DNA molecules binding to silica surfaces in the presence of certain salts and under certain pH conditions.\n\nConventional methods for DNA extraction, such as ethanol precipitation or preparations using commercial purification kits, cannot be integrated onto microchips because they require multiple hands-on processing steps. In addition, they also require large equipment and high volumes of reagents and samples. DNA extraction on microchips provides a fast, cost effective, and effective for high-throughput screening, which also has a very small footprint. This new method has useful applications for biosensors, lab on a chip devices, and other new technologies that require rapid, high quality DNA at minimal cost. \n\nA\nIn the actual operations, a sample (this may be anything from purified cells to a tissue specimen) is placed into the chip and lysed. The resultant mix of proteins, DNA, phospholipids, etc., is then run through the channel where the DNA is adsorbed by silica surface in the presence of solutions with high ionic strength. The highest DNA adsorption efficiencies occur in the presence of buffer solution with a pH at or below the pKa of the surface silanol groups.\n\nAlthough the mechanism is not fully understood, one possible explanation involves reduction of the silica’s surface’s negative charge due to the high ionic strength of the buffer. This decrease in surface charge leads to a decrease in the electrostatic repulsion between the negatively charged DNA and the negatively charged silica. Meanwhile, the buffer also reduces the activity of water by formatting hydrated ions. This leads to the silica surface and DNA becoming dehydrated. These conditions lead to an energetically favorable situation for DNA to adsorb to the silica surface. \n\nA further explanation of how DNA binds to silica is based on the action of guanidium HCl (GuHCl), which acts as a chaotrope. A chaotrope denatures biomolecules by disrupting the shell of hydration around them. This allows positively charged ions to form a salt bridge between the negatively charged silica and the negatively charged DNA backbone in high salt concentration. The DNA can then be washed with high salt and ethanol, and ultimately eluted with low salt. \n\nAfter the DNA is adsorbed to the silica surface, all other molecules pass through the column. Most likely, these molecules are sent to a waste section on the chip, which can then be closed off using a gated channel or a pressure- or voltage-controlled chamber. The DNA is then washed to remove any excess waste particles from the sample and then eluted from the channel using an elution buffer for further downstream processing. \n\nThe following solutions have been proposed and validated for use in this process DNA binding: GuHCl- based loading buffer; Channel Wash: 80% isopropanol; DNA elution: TE at pH 8.4.\n\nMethods using silica beads and silica resins have been created that can isolate DNA molecules for subsequent PCR amplification. However, these methods have associated problems. First, beads and resins are highly variable depending on how well they are packed and are thus hard to reproduce. Each loading of a micro-channel can result in a different amount of packing and thus change the amount of DNA that adsorbed to the channel. Furthermore, these methods result in a two step manufacturing process.\n\nSilica structures are a much more effective method of packing material because they are etched into the channel during its fabrication and is thus the result of a one step manufacturing processes via soft lithography. Silica structures are therefore easier to use in highly parallelized designs than beads or resins.\n\n\n", "id": "8255258", "title": "DNA separation by silica adsorption"}
{"url": "https://en.wikipedia.org/wiki?curid=5649173", "text": "Multicopy single-stranded DNA\n\nBefore the discovery of msDNA in myxobacteria, a group of swarming, soil-dwelling bacteria, it was thought that the enzymes known as reverse transcriptases (RT) existed only in eukaryotes and viruses. The discovery led to an increase in research of the area. As a result, msDNA has been found to be widely distributed among bacteria, including various strains of \"Escherichia coli\" and pathogenic bacteria. Further research discovered similarities between HIV-encoded reverse transcriptase and an open reading frame (ORF) found in the msDNA coding region. Tests confirmed the presence of reverse transcriptase activity in crude lysates of retron-containing strains. Although an RNase H domain was tentatively identified in the retron ORF, it was later found that the RNase H activity required for msDNA synthesis is actually supplied by the host.\n\nThe discovery of msDNA has led to broader questions regarding where reverse transcriptase originated, as genes encoding for reverse transcriptase (not necessarily associated with msDNA) have been found in prokaryotes, eukaryotes, viruses and even archaea. After a DNA fragment coding for the production of msDNA in \"E. coli\" was discovered, it was conjectured that bacteriophages might have been responsible for the introduction of the RT gene into \"E. coli\". These discoveries suggest that reverse transcriptase played a role in the evolution of viruses from bacteria, with one hypothesis stating that, with the help of reverse transcriptase, viruses may have arisen as a breakaway msDNA gene that acquired a protein coat. Since nearly all RT genes function in retrovirus replication and/or the movement of transposable elements, it is reasonable to imagine that retrons might be mobile genetic elements, but there has been little supporting evidence for such a hypothesis, save for the observed fact that msDNA is widely yet sporadically dispersed among bacterial species in a manner suggestive of both horizontal and vertical transfer. Since it is not known whether retron sequences \"per se\" represent mobile elements, retrons are functionally defined by their ability to produce msDNA while deliberately avoiding speculation about other possible activities.\n\nThe function of msDNA remains unknown even though many copies are present within cells. Knockout mutations that do not express msDNA are viable, so the production of msDNA is not essential to life under laboratory conditions. Over-expression of msDNA is mutagenic, apparently as a result of titrating out repair proteins by the mismatched base pairs that are typical of their structure. It has been suggested that msDNA may have some role in pathogenicity or the adaptation to stressful conditions. Sequence comparison of msDNAs from \"Myxococcus xanthus\", \"Stigmatella aurantiaca\", and many other bacteria reveal conserved and hypervariable domains reminiscent of conserved and hypervariable sequences found in allorecognition molecules. The major msDNAs of \"M. xanthus\" and \"S. aurantiaca\", for instance, share 94% sequence homology except within a 19 base-pair domain that shares sequence homology of only 42%. The presence of such domains is significant because myxobacteria exhibit complex cooperative social behaviors including swarming and formation of fruiting bodies, while \"E. coli\" and other pathogenic bacteria form biofilms that exhibit enhanced antibiotic and detergent resistance. The sustainability of social assemblies that require significant individual investment of energy is generally dependent on the evolution of allorecognition mechanisms that enable groups to distinguish self versus non-self.\n\n Biosynthesis of msDNA is purported to follow a unique pathway found nowhere else in DNA/RNA biochemistry. Because of the similarity of the 2'-5' branch junction to the branch junctions found in RNA splicing intermediates, it might at first have been expected that branch formation would be via spliceosome- or ribozyme-mediated ligation. Surprisingly, however, experiments in cell-free systems using purified retron reverse transcriptase indicate that cDNA synthesis is directly primed from the 2'-OH group of the specific internal G residue of the primer RNA. The RT recognizes specific stem-loop structures in the precursor RNA, rendering synthesis of msDNA by the RT highly specific to its own retron. The priming of msDNA synthesis offers a fascinating challenge to our understanding of DNA synthesis. DNA polymerases (which include RT) share highly conserved structural features, which means that their active catalytic sites vary little from species to species, or even between DNA polymerases using DNA as a template, versus DNA polymerases using RNA as a template. The catalytic region of eukaryotic reverse transcriptase comprises three domains termed the \"fingers\", \"palm\", and \"thumb\" which hold the double-stranded primer-template in a right-hand grip with the 3'-OH of the primer buried in the active site of the polymerase, a cluster of highly conserved acidic and polar residues situated on the palm between what would be the index and middle fingers. In eukaryotic RTs, the RNase H domain lies on the wrist below the base of the thumb, but retron RTs lack RNase H activity. The nucleic acid binding cleft, extending from the polymerase active site to the RNase H active site, is about 60 Å in length in eukaryotic RTs, corresponding to nearly two helical turns. When eukaryotic RT extends a conventional primer, the growing DNA/RNA double helix spirals along the cleft, and as the double helix passes the RNase H domain, the template RNA is digested to release the nascent strand of cDNA. In the case of msDNA primer extension, however, a long strand of RNA remains attached to the 3'-OH of the priming G. Although it is possible to model an RT-primer template complex which would make the 2'-OH accessible for the priming reaction, further extension of the DNA strand presents a problem: as DNA synthesis progresses, the bulky RNA strand extending from the 3'-OH needs somehow to spiral down the binding cleft without being blocked by steric hindrance. To overcome this issue, the msDNA reverse transcriptase clearly would require special features not shared by other RTs.\n\n", "id": "5649173", "title": "Multicopy single-stranded DNA"}
{"url": "https://en.wikipedia.org/wiki?curid=10449512", "text": "Shuttle vector\n\nA shuttle vector is a vector (usually a plasmid) constructed so that it can propagate in two different host species . Therefore, DNA inserted into a shuttle vector can be tested or manipulated in two different cell types. The main advantage of these vectors is they can be manipulated in \"E. coli\", then used in a system which is more difficult or slower to use (e.g. yeast).\n\nShuttle vectors include plasmids that can propagate in eukaryotes and prokaryotes (e.g. both \"Saccharomyces cerevisiae\" and \"Escherichia coli\") or in different species of bacteria (e.g. both \"E. coli\" and \"Rhodococcus erythropolis\"). There are also adenovirus shuttle vectors, which can propagate in \"E. coli\" and mammals. \n\nShuttle vectors are frequently used to quickly make multiple copies of the gene in \"E. coli\" (amplification). They can also be used for \"in vitro\" experiments and modifications (e.g. mutagenesis, PCR) \n\nOne of the most common types of shuttle vectors is the yeast shuttle vector . Almost all commonly used \"S. cerevisiae\" vectors are shuttle vectors. Yeast shuttle vectors have components that allow for replication and selection in both \"E. coli\" cells and yeast cells. The \"E. coli\" component of a yeast shuttle vector includes an origin of replication and a selectable marker, e.g. antibiotic resistance, beta lactamase, beta galactosidase. The yeast component of a yeast shuttle vector includes an autonomously replicating sequence (ARS), a yeast centromere (CEN), and a yeast selectable marker (e.g. URA3, a gene that encodes an enzyme for uracil synthesis, Lodish et al. 2007).\n\n", "id": "10449512", "title": "Shuttle vector"}
{"url": "https://en.wikipedia.org/wiki?curid=13515967", "text": "Relaxosome\n\n'The \"Relaxosome\"' is the complex of proteins that facilitates plasmid transfer during bacterial conjugation. The proteins are encoded by the tra operon on a fertility plasmid in the region near the origin of transfer, oriT. The most important of these proteins is relaxase, which is responsible for beginning the conjugation process by cutting at the \"nic\" site via transesterification. This nicking results in a DNA-Protein complex with the relaxosome bound to a single strand of the plasmid DNA and an exposed 3' hydroxyl group. Relaxase also unwinds the plasmid being conjugated with its helicase properties. The relaxosome interacts with integration host factors within the oriT.\n\nOther genes that code for relaxosome components include TraH, which stabilizes the relaxosome's structural formation, TraI, which encodes for the relaxase protein, TraJ, which recruits the complex to the oriT site, TraK, which increases the 'nicked' state of the target plasmid, and TraY, which imparts single-stranded DNA character on the oriT site. TraM plays a particularly important role in relaxase interaction by stimulating 'relaxed' DNA formation.\n", "id": "13515967", "title": "Relaxosome"}
{"url": "https://en.wikipedia.org/wiki?curid=13564754", "text": "Veterinary virology\n\nVeterinary virology is the study of viruses in non-human animals. It is an important branch of veterinary medicine.\n\nRhabdoviruses are a diverse family of single stranded, negative sense RNA viruses that infect a wide range of hosts, from plants and insects, to fish and mammals. The \"Rhaboviridae\" family consists of six genera, two of which, cytorhabdoviruses and nucleorhabdoviruses, only infect plants. Novirhabdoviruses infect fish, and vesiculovirus, lyssavirus and ephemerovirus infect mammals, fish and invertebrates. The family includes pathogens such as rabies virus, vesicular stomatitis virus and potato yellow dwarf virus that are of public health, veterinary, and agricultural significance.\n\nFoot-and-mouth disease virus (FMDV) is a member of the Aphthovirus genus in the Picornaviridae family and is the cause of foot-and-mouth disease in pigs, cattle, sheep and goats. It is a non-enveloped, positive strand, RNA virus. FMDV is a highly contagious virus. It enters the body through inhalation.\n\nPestiviruses have a single stranded, positive-sense RNA genomes. They cause Classical swine fever (CSF) and Bovine viral diarrhea(BVD). Mucosal disease is a distinct, chronic persistent infection, whereas BVD is an acute infection.\n\nArteriviruses are small, enveloped, animal viruses with an icosahedral core containing a positive-sense RNA genome. The family includes equine arteritis virus (EAV), porcine reproductive and respiratory syndrome virus (PRRSV), lactate dehydrogenase elevating virus (LDV) of mice and simian haemorrhagic fever virus (SHFV).\n\nCoronaviruses are enveloped viruses with a positive-sense RNA genome and with a nucleocapsid of helical symmetry. They infect the upper respiratory and gastrointestinal tract of mammals and birds. They are the cause of a wide range of diseases in cats, dog, pigs, rodents, cattle and humans. Transmission is by the faecal-oral route.\n\nTorovirus is a genus of viruses within the family \"Coronaviridae\", subfamily \"Torovirinae\" that primarily infect vertebrates and include Berne virus of horses and Breda virus of cattle. They cause gastroenteritis in mammals, including humans but rarely.\n\nInfluenza is caused by RNA viruses of the family Orthomyxoviridae and affects birds and mammals.\n\nWild aquatic birds are the natural hosts for a large variety of influenza A viruses. Occasionally viruses are transmitted from this reservoir to other species and may then cause devastating outbreaks in domestic poultry or give rise to human influenza pandemics. \n\nBluetongue virus (BTV), a member of Orbivirus genus within the Reoviridae family causes serious disease in livestock (sheep, goat, cattle). It is non-enveloped, double-stranded RNA virus. The genome is segmented.\n\nCircoviruses are small single-stranded DNA viruses. There are to genera: gyrovirus, with one species called chicken anemia virus; and circovirus, which includes porcine circovirus types 1 and 2, psittacine beak and feather disease virus, pigeon circovirus, canary circovirus goose circovirus.\n\nHerpesviruses are ubiquitous pathogens infecting animals and humans.\n\nAfrican swine fever virus (ASFV) is a large double-stranded DNA virus which replicates in the cytoplasm of infected cells and is the only member of the Asfarviridae family. The virus causes a lethal haemorraghic disease in domestic pigs. Some strains can cause death of animals within as little as a week after infection. In other species, the virus causes no obvious disease. ASFV is endemic to sub-Saharan Africa and exists in the wild through a cycle of infection between ticks and wild pigs, bushpigs and warthogs.\n\nRetroviruses are established pathogens of veterinary importance. They are generally a cause of cancer or immune deficiency.\n\nFlaviviruses constitute a family of linear, single-stranded RNA(+) viruses. Flaviviruses include the West Nile virus, dengue virus, Tick-borne Encephalitis Virus, Yellow Fever Virus, and several other viruses. Many flavivirus species can replicate in both mammalian and insect cells. Most flaviviruses are arthropod borne and multiply in both vertebrates and arthropods. The viruses in this family that are of veterinary importance include Japanese encephalitis virus, St. Louis encephalitis virus, West Nile virus, Israel turkey meningoencephalomyelitis virus, Sitiawan virus, Wesselsbron virus, yellow fever virus and the tick-borne flaviviruses e.g. louping ill virus.\n\nParamyxoviruses are a diverse family of non-segmented negative strand RNA viruses that include many highly pathogenic viruses affecting humans, animals, and birds. These include canine distemper virus (dogs), phocine distemper virus (seals), cetacean morbillivirus (dolphins and porpoises) Newcastle disease virus (birds) and rinderpest virus (cattle). Some paramyxoviruses such as the henipaviruses are zoonotic pathogens, occurring primarility in an animal hosts, but also able to infect humans.\n\nParvoviruses are linear, non-segmented single-stranded DNA viruses, with an average genome size of 5000 nucleotides. They are classified as group II viruses in Baltimore classification of viruses. Parvoviruses are among the smallest viruses (hence the name, from Latin \"parvus\" meaning \"small\") and are 18–28 nm in diameter.\nParvoviruses can cause disease in some animals, including starfish and humans. Because the viruses require actively dividing cells to replicate, the type of tissue infected varies with the age of the animal. The gastrointestinal tract and lymphatic system can be affected at any age, leading to vomiting, diarrhea and immunosuppression but cerebellar hypoplasia is only seen in cats that were infected in the womb or at less than two weeks of age, and disease of the myocardium is seen in puppies infected between the ages of three and eight weeks.\n\n", "id": "13564754", "title": "Veterinary virology"}
{"url": "https://en.wikipedia.org/wiki?curid=13642373", "text": "Upstream and downstream (DNA)\n\nIn molecular biology and genetics, upstream and downstream both refer to relative positions in DNA or RNA. Each strand of DNA or RNA has a 5' end and a 3' end, so named for the carbon position on the deoxyribose (or ribose) ring. By convention, upstream and downstream relate to the 5' to 3' direction in which RNA transcription takes place. Upstream is toward the 5' end of the RNA molecule and downstream is toward the 3' end. When considering double-stranded DNA, upstream is toward the 5' end of the coding strand for the gene in question and downstream is toward the 3' end. Due to the anti-parallel nature of DNA, this means the 3' end of the template strand is upstream of the gene and the 5' end is downstream.\n\nSome genes on the same DNA molecule may be transcribed in opposite directions. This means the upstream and downstream areas of the molecule may change depending on which gene is used as the reference.\n\n", "id": "13642373", "title": "Upstream and downstream (DNA)"}
{"url": "https://en.wikipedia.org/wiki?curid=13642379", "text": "Upstream and downstream (transduction)\n\nIn molecular biology, the terms upstream and downstream can refer to the temporal and mechanistic order of cellular and molecular events. For example, in signal transduction, the second messenger acts downstream tothat is to say, temporally \"after\"activation of cell membrane receptors. The other way around, activation of cell membrane receptors occurs upstream ofthat is to say, prior tothe production of second messengers.\n\n", "id": "13642379", "title": "Upstream and downstream (transduction)"}
{"url": "https://en.wikipedia.org/wiki?curid=13500312", "text": "Ideotype\n\nIn systematics, an ideotype is a specimen identified as belonging to a specific taxon by the author of that taxon, but collected from somewhere other than the type locality.\n\nThe concept of ideotype in plant breeding was introduced by Donald in 1968 to describe the idealized appearance of a plant variety. It literally means 'a form denoting an idea'. According to Donald, ideotype is a biological model which is expected to perform or behave in a particular manner within a defined environment: \"a crop ideotype is a plant model, which is expected to yield a greater quantity or quality of grain, oil or other useful product when developed as a cultivar.\" Donald and Hamblin (1976) proposed the concepts of isolation, competition and crop ideotypes. Market ideotype, climatic ideotype, edaphic ideotype, stress ideotype and disease/pest ideotypes are its other concepts. The term ideotype has the following synonyms: model plant type, ideal model plant type and ideal plan type.\n\nThe term is also used in cognitive science and cognitive psychology, where Ronaldo Vigo (2011, 2013, 2014) introduced it to refer to a type of concept metarepresentation that is a compound memory trace consisting of the structural information detected by humans in categorical stimuli.\n", "id": "13500312", "title": "Ideotype"}
{"url": "https://en.wikipedia.org/wiki?curid=23650", "text": "Primer (molecular biology)\n\nA primer is a short strand of RNA or DNA (generally about 18-22 bases) that serves as a starting point for DNA synthesis. It is required for DNA replication because the enzymes that catalyze this process, DNA polymerases, can only add new nucleotides to an existing strand of DNA. The polymerase starts replication at the 3'-end of the primer, and copies the opposite strand.\n\nIn vivo DNA replication utilizes short strands of RNA called RNA primers to initiate DNA synthesis on both the leading and lagging strands — DNA primers are not seen in vivo in humans. These RNA primers can be made \"de novo\".\n\nOn the other hand, many of the in vitro laboratory techniques that involve DNA polymerase in biochemistry and molecular biology (such as DNA sequencing and the polymerase chain reaction), use DNA primers because they are more temperature stable. In experiments, it is often important to use a primer with a similar Tm (melting temperature) to the template strand it will be hybridizing to. A primer with a Tm significantly higher than the reaction's annealing temperature may mishybridize and extend at an incorrect location along the DNA sequence, while one with a Tm significantly lower than the annealing temperature may fail to anneal and extend at all. These primers are usually short, chemically synthesized oligonucleotides, with a length of about twenty bases. They are hybridized to a target DNA, which is then copied by the polymerase.\n\nThe lagging strand of DNA is that strand of the DNA double helix that is orientated in a 5' to 3' manner. Therefore, its complement must be synthesized in a 3'→5' manner. Because DNA polymerase III cannot synthesize in the 5'→3' direction, the lagging strand is synthesized in short segments known as Okazaki fragments. Along the lagging strand's template, primase builds RNA primers in short bursts. DNA polymerases are then able to use the free 3'-OH groups on the RNA primers to synthesize DNA in the 5'→3' direction.\n\nThe RNA fragments are then removed by DNA polymerase I for prokaryotes or DNA polymerase δ for eukaryotes (different mechanisms are used in eukaryotes and prokaryotes) and new deoxyribonucleotides are added to fill the gaps where the RNA was present. DNA ligase then joins the deoxyribonucleotides together, completing the synthesis of the lagging strand.\n\nIn eukaryotic primer removal, DNA polymerase δ extends the Okazaki fragment in 5' to 3' direction, and when it encounters the RNA primer from the previous Okazaki fragment, it displaces the 5′ end of the primer into a single-stranded RNA flap, which is removed by nuclease cleavage. Cleavage of the RNA flaps involves either flap structure-specific endonuclease 1 (FEN1) cleavage of short flaps, or coating of long flaps by the single-stranded DNA binding protein replication protein A (RPA) and sequential cleavage by Dna2 nuclease and FEN1.\n\nThis mechanism is a potential explanation of how the HIV virus can transform its genome into double-stranded DNA from the RNA-DNA formed after reverse transcription of its RNA. However, the HIV-encoded reverse transcriptase has its own ribonuclease activity that degrades the viral RNA during the synthesis of cDNA, as well as DNA-dependent DNA polymerase activity that copies the sense cDNA strand into \"antisense\" DNA to form a double-stranded DNA intermediate.\n\nDNA sequencing is used to determine the nucleotides in a DNA strand. The Sanger chain termination method of sequencing uses a primer to start the chain reaction.\n\nIn PCR, primers are used to determine the DNA fragment to be amplified by the PCR process. The length of primers is usually not more than 30 (usually 18–24) nucleotides, and they need to match the beginning and the end of the DNA fragment to be amplified. They direct replication towards each other – the extension of one primer by polymerase then becomes the template for the other, leading to an exponential increase in the target segment.\n\nIt is worth noting that primers are not always for DNA synthesis, but can in fact be used by viral polymerases, e.g. influenza, for RNA synthesis.\n\nPairs of primers should have similar melting temperatures since annealing in a PCR occurs for both simultaneously. A primer with a \"T\" (melting temperature) significantly higher than the reaction's annealing temperature may mishybridize and extend at an incorrect location along the DNA sequence, while \"T\" significantly lower than the annealing temperature may fail to anneal and extend at all.\n\nPrimer sequences need to be chosen to uniquely select for a region of DNA, avoiding the possibility of mishybridization to a similar sequence nearby. A commonly used method is BLAST search whereby all the possible regions to which a primer may bind can be seen. Both the nucleotide sequence as well as the primer itself can be BLAST searched. The free NCBI tool Primer-BLAST integrates primer design and BLAST search into one application, as do commercial software products such as ePrime and Beacon Designer. Computer simulations of theoretical PCR results (Electronic PCR) may be performed to assist in primer design.\n\nMany online tools are freely available for primer design, some of which focus on specific applications of PCR. The popular tools Primer3Plus and PrimerQuest can be used to find primers matching a wide variety of specifications. Highly degenerate primers for targeting a wide variety of DNA templates can be interactively designed using GeneFISHER. Primers with high specificity for a subset of DNA templates in the presence of many similar variants can be designed using DECIPHER. Primer design aims to generate a balance between specificity and efficiency of amplification.\n\nMononucleotide and dinucleotide repeats should be avoided, as loop formation can occur and contribute to mishybridization. Primers should not easily anneal with other primers in the mixture (either other copies of same or the reverse direction primer); this phenomenon can lead to the production of 'primer dimer' products contaminating the mixture. Primers should also not anneal strongly to themselves, as internal hairpins and loops could hinder the annealing with the template DNA.\n\nWhen designing a primer for use in TA cloning, efficiency can be increased by adding AG tails to the 5' and the 3' end.\n\nThe reverse primer has to be the reverse complement of the given cDNA sequence. The reverse complement can be easily determined, e.g. with online calculators.\n\nSometimes \"degenerate primers\" are used. These are actually mixtures of similar, but not identical primers. They may be convenient if the same gene is to be amplified from different organisms, as the genes themselves are probably similar but not identical. The other use for degenerate primers is when primer design is based on protein sequence. As several different codons can code for one amino acid, it is often difficult to deduce which codon is used in a particular case. Therefore, primer sequence corresponding to the amino acid isoleucine might be \"ATH\", where A stands for adenine, T for thymine, and H for adenine, thymine, or cytosine, according to the genetic code for each codon, using the IUPAC symbols for degenerate bases. Use of degenerate primers can greatly reduce the specificity of the PCR amplification. The problem can be partly solved by using touchdown PCR.\n\n\"Degenerate primers\" are widely used and extremely useful in the field of microbial ecology. They allow for the amplification of genes from thus far uncultivated microorganisms or allow the recovery of genes from organisms where genomic information is not available. Usually, degenerate primers are designed by aligning gene sequencing found in GenBank. Differences among sequences are accounted for by using IUPAC degeneracies for individual bases. PCR primers are then synthesized as a mixture of primers corresponding to all permutations.\n\nThere are a number of programs available to perform these primer predictions; \n\n\n", "id": "23650", "title": "Primer (molecular biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=5781055", "text": "Mitointeractome\n\nMitointeractome is a mitochondrial protein interactome database. \n\n", "id": "5781055", "title": "Mitointeractome"}
{"url": "https://en.wikipedia.org/wiki?curid=12279693", "text": "Intrinsic termination\n\nIntrinsic termination (also called Rho-independent termination) is a mechanism in prokaryotes that causes RNA transcription to stop and release the newly made RNA. In this mechanism, the mRNA contains a sequence that can base pair with itself to form a stem-loop structure 7-20 base pairs in length that is also rich in cytosine-guanine base pairs. C-G base pairs have significant base-stacking interactions (especially repeated G-C pairs) and can form three hydrogen bonds between each other, resulting in a stable RNA duplex. Following the stem-loop structure is a chain of uracil residues. The bonds between uracil and adenine are very weak. A protein bound to RNA polymerase (\"nusA\") binds to the stem-loop structure tightly enough to cause the polymerase to temporarily stall. This pausing of the polymerase coincides with transcription of the poly-uracil sequence. The weak Adenine-Uracil bonds lower the energy of destabilization for the RNA-DNA duplex, allowing it to unwind and dissociate from the RNA polymerase.\n\nStem-loop structures that are not followed by a poly-Uracil sequence cause the RNA polymerase to pause, but it will typically continue transcription after a brief time because the duplex is too stable to unwind far enough to cause termination.\n\nRho-independent transcription termination is a frequent mechanism underlying the activity of \"cis\"-acting RNA regulatory elements, such as riboswitches.\n\n", "id": "12279693", "title": "Intrinsic termination"}
{"url": "https://en.wikipedia.org/wiki?curid=9663941", "text": "Shq1\n\nShq1p is a protein involved in the rRNA processing pathway. It was discovered by Pok Yang in the Chanfreau laboratory at UCLA. Depletion of Shq1p has led to decreased level of various H/ACA box snoRNAs (H/ACA box snoRNAs are responsible for pseuduridylation of pre-rRNA) and certain pre-rRNA intermediates.\n\nDuring the synthesis of eukaryotic ribosomes, four mature ribosomal RNAs (the 5S, 5.8S, 18S, and 25S) must be synthesized. Three of these rRNAs (5.8S, 18S, and 25S) come from a single pre-rRNA known as the 35S. Although many of the intermediates in this rRNA processing pathway have been identified in the last thirty years, there are still a number of proteins involved in this process whose specific function is unknown.\n\nShq1, a protein thought to play a role in the stabilization and/or production of box H/ACA snoRNA, is still uncharacterized. It has been proposed that Shq1, along with Naf1p, is involved in the initial steps of the biogenesis of H/ACA box snoRNPs (box H/ACA snoRNAs form complexes with proteins, thereby forming snoRNPs) because of its association with certain snoRNP proteins during the snoRNP’s maturation, while showing very little association with the mature snoRNP complex. Despite the known involvement of Shq1 in H/ACA box snoRNP's production, the exact function of this protein in the overall rRNA processing pathway is still unknown.\n\n\n", "id": "9663941", "title": "Shq1"}
{"url": "https://en.wikipedia.org/wiki?curid=14086111", "text": "Bis-peptide\n\nBis-peptides are analogues of peptides, but consist of bis-amino acids, which bear two carboxyl groups and two amino groups. The connection of specific bis-amino acids leads to the formation of bis-peptides with well-defined molecular shape, which is of great interest for designing nano-structures.\n\nPossible applications that are currently investigated include the binding and inactivation of cholera toxin and the cross linkage of surface proteins of various viruses (HIV, Ebola virus). Further the group of Christian Schafmeister developed molecular hinges, which can be used for the construction of molecular machines, such as nano-valves or data storage systems.\n\n\nLevins CG, Schafmeister CE. \"The synthesis of curved and linear structures from a minimal set of monomers.\" Journal of Organic Chemistry, 70, p. 9002, 2005. \n", "id": "14086111", "title": "Bis-peptide"}
{"url": "https://en.wikipedia.org/wiki?curid=3403985", "text": "Phenol extraction\n\nPhenol extraction is a processing technology used to prepare phenols as raw materials, compounds or additives for industrial wood processing and for chemical industries. Phenol extraction also is a laboratory process to purify DNA samples.\n\nIn this method, a mixture of TE (or Tris-Ethylenediaminetetraacetic acid) and phenol is combined with an equal volume of an aqueous DNA sample. After agitation and centrifugal separation, the aqueous layer is extracted, and further processed with ether. Then the DNA is concentrated by ethanol precipitation.\n\nThe phenol extraction technique is often used to purify samples of nucleic acids taken from cells. To obtain nucleic acid samples, the cell must be lysed and the nucleic acids separated from all other cell materials. Phenol is a useful compound for breaking down superfluous cell materials that would otherwise contaminate the nucleic acid sample.\n\nThere are two reasons why phenol makes such an effective purifier for nucleic acid samples. The first is that it is a non-polar compound. Because nucleic acids are highly polar, they do not dissolve in the presence of phenol. The second is that phenol has a density of 1.07 g/cm, which is higher than the density of water (1.00 g/cm). Thus, when phenol is added to a cell sample solution the water and phenol remain separate. Two “phases” form when phenol is added to the solution and centrifuged. There is an aqueous, polar phase at the top of the solution containing nucleic acids and water, and an organic phase containing denatured proteins and other cell components at the bottom of the solution. The aqueous phase is always on top of the organic because, as mentioned above, phenol is denser than water. Nucleic acids are polar, and therefore stay in the aqueous phase, whereas non-polar cellular components move into the organic phase.\nAfter phenol has been added to the sample it is centrifuged and the aqueous and organic (“Phenol”) phases form.\n\nPhenol is often used in combination with chloroform. The purpose of adding chloroform along with phenol is to ensure a clear separation between the aqueous and organic phases. Chloroform and phenol mix well together, unlike phenol and water. The density of chloroform is 1.47 g/cm, higher than that of water and phenol. Mixing chloroform and phenol creates a denser solution than phenol alone, and therefore the separation of the organic from the aqueous phase is even clearer than if only phenol was added to a cell sample. There is less cross-contamination from the organic phase in the aqueous phase. This is useful for when the aqueous phase is removed from the solution in order to obtain a pure nucleic acid sample.\n\npH is an important factor to consider in the phenol extraction technique. For phenol to be effective the pH of the solution must vary according to what is being extracted. In the case of DNA purification a pH of 7.0–8.0 is used. If the aim of an experiment is to obtain samples of purified RNA, a pH of around 4.5 is used. Because of the negative charge on the backbone of DNA from phosphates, decreasing the pH of a solution will lead to neutralization. A pH of 4.5 has a higher concentration of H+ ions that would neutralize the negative phosphate charges and cause DNA to dissolve in the organic phase, while RNA has additional hydroxyl group in pentose sugar which allows the RNA to remain in water phase.\n\n\n", "id": "3403985", "title": "Phenol extraction"}
{"url": "https://en.wikipedia.org/wiki?curid=3682580", "text": "Plasmid preparation\n\nA plasmid preparation is a method of DNA extraction and purification for plasmid DNA. Many methods have been developed to purify plasmid DNA from bacteria. These methods invariably involve three steps: \n\nPlasmids are almost always purified from liquid bacteria cultures, usually \"E. coli\", which have been transformed and isolated. Virtually all plasmid vectors in common use encode one or more antibiotic resistance genes as a selectable marker, for example a gene encoding ampicillin or kanamycin resistance, which allows bacteria that have been successfully transformed to multiply uninhibited. Bacteria that have not taken up the plasmid vector are assumed to lack the resistance gene, and thus only colonies representing successful transformations are expected to grow.\nBacteria are grown under favourable conditions.\n\nWhen bacteria are lysed under alkaline conditions (pH 12.0–12.5) both chromosomal DNA and protein are denatured; the plasmid DNA however, remains stable. Some scientists reduce the concentration of NaOH used to 0.1M in order to reduce the occurrence of ssDNA. After the addition of acetate-containing neutralization buffer the large and less supercoiled chromosomal DNA and proteins precipitate, but the small bacterial DNA plasmids stay in solution.\n\nKits are available from varying manufacturers to purify plasmid DNA, which are named by size of bacterial culture and corresponding plasmid yield. In increasing order, these are the miniprep, midiprep, maxiprep, megaprep, and gigaprep. The plasmid DNA yield will vary depending on the plasmid copy number, type and size, the bacterial strain, the growth conditions, and the kit.\n\nMinipreparation of plasmid DNA is a rapid, small-scale isolation of plasmid DNA from bacteria. It is based on the alkaline lysis method. The extracted plasmid DNA resulting from performing a miniprep is itself often called a \"miniprep\".\nMinipreps are used in the process of molecular cloning to analyze bacterial clones. A typical plasmid DNA yield of a miniprep is 50 to 100 µg depending on the cell strain.\nMiniprep of large number of plasmids can also be done conveniently on filter paper by lysing the cell and eluting the plasmid on to filter paper.\n\nThe starting E. coli culture volume is 15-25 mL of lysogeny broth (LB) and the expected DNA yield is 100-350 µg.\n\nThe starting E. coli culture volume is 100-200 mL of LB and the expected DNA yield is 500-850 µg.\n\nThe starting E. coli culture volume is 500 mL – 2.5 L of LB and the expected DNA yield is 1.5-2.5 mg.\n\nThe starting E. coli culture volume is 2.5-5 L of LB and the expected DNA yield is 7.5–10 mg.\n\nMultiple methods of nucleic acid purification exist. All work on the principle of generating conditions where either only the nucleic acid precipitates, or only other biomolecules precipitate, allowing the nucleic acid to be separated.\n\nEthanol precipitation works by using ethanol as an antisolvent of DNA, causing it to precipitate out of solution. The soluble fraction is discarded to remove other biomolecules.\n\nSpin column-based nucleic acid purification precipitates nucleic acid such that it binds a solid matrix and other components flow through. The conditions are then changed to elute the purified nucleic acid.\n\nIn a phenol–chloroform extraction, addition of a phenol/chloroform mixture will dissolve protein and lipid contaminants, leaving the nucleic acids in the aqueous phase. It also denatures proteins, like DNase, which is especially important if the plasmids are to be used for enzyme digestion. Otherwise, smearing may occur in enzyme restricted form of plasmid DNA.\n\n\n", "id": "3682580", "title": "Plasmid preparation"}
{"url": "https://en.wikipedia.org/wiki?curid=14570400", "text": "Rare-cutter enzyme\n\nA rare-cutter enzyme is a restriction enzyme with a recognition sequence which occurs only rarely in a genome. An example is NotI, which cuts after the first GC of a 5'-GCGGCCGC-3' sequence; restriction enzymes with seven and eight base pair recognition sequences are often also called rare-cutter enzymes (six bp recognition sequences are much more common).\n\nFor example, rare-cutter enzymes with 7-nucleotide recognition sites cut once every 4 bp (16,384 bp), and those with 8-nucleotide recognition sites cut every 4 bp (65,536 bp) respectively. They are used in top-down mapping to cut a chromosome into chunks of these sizes on average.\n\n", "id": "14570400", "title": "Rare-cutter enzyme"}
{"url": "https://en.wikipedia.org/wiki?curid=10714473", "text": "Polymerase chain reaction optimization\n\nThe polymerase chain reaction (PCR) is a commonly used molecular biology tool for amplifying DNA, and various techniques for PCR optimization which have been developed by molecular biologists to improve PCR performance and minimize failure.\n\nThe PCR method is extremely sensitive, requiring only a few DNA molecules in a single reaction for amplification across several orders of magnitude. Therefore, adequate measures to avoid contamination from any DNA present in the lab environment (bacteria, viruses, or human sources) are required. Because products from previous PCR amplifications are a common source of contamination, many molecular biology labs have implemented procedures that involve dividing the lab into separate areas. One lab area is dedicated to preparation and handling of pre-PCR reagents and the setup of the PCR reaction, and another area to post-PCR processing, such as gel electrophoresis or PCR product purification. For the setup of PCR reactions, many standard operating procedures involve using pipettes with filter tips and wearing fresh laboratory gloves, and in some cases a laminar flow cabinet with UV lamp as a work station (to destroy any extraneomultimer formation). PCR is routinely assessed against a negative control reaction that is set up identically to the experimental PCR, but without template DNA, and performed alongside the experimental PCR.\n\nSecondary structures in the DNA can result in folding or knotting of DNA template or primers, leading to decreased product yield or failure of the reaction. Hairpins, which consist of internal folds caused by base-pairing between nucleotides in inverted repeats within single-stranded DNA, are common secondary structures and may result in failed PCRs.\n\nTypically, primer design that includes a check for potential secondary structures in the primers, or addition of DMSO or glycerol to the PCR to minimize secondary structures in the DNA template , are used in the optimization of PCRs that have a history of failure due to suspected DNA hairpins.\n\nTaq polymerase lacks a 3' to 5' exonuclease activity. Thus, Taq has no error-proof-reading activity, which consists of excision of any newly misincorporated nucleotide base from the nascent (i.e., extending) DNA strand that does not match with its opposite base in the complementary DNA strand. The lack in 3' to 5' proofreading of the Taq enzyme results in a high error rate (mutations per nucleotide per cycle) of approximately 1 in 10,000 bases, which affects the fidelity of the PCR, especially if errors occur early in the PCR with low amounts of starting material, causing accumulation of a large proportion of amplified DNA with incorrect sequence in the final product.\n\nSeveral \"high-fidelity\" DNA polymerases, having engineered 3' to 5' exonuclease activity, have become available that permit more accurate amplification for use in PCRs for sequencing or cloning of products. Examples of polymerases with 3' to 5' exonuclease activity include: KOD DNA polymerase, a recombinant form of \"Thermococcus kodakaraensis\" KOD1; Vent, which is extracted from \"Thermococcus litoralis\"; Pfu DNA polymerase, which is extracted from \"Pyrococcus furiosus\"; and Pwo, which is extracted from \"Pyrococcus woesii\".\n\nMagnesium is required as a co-factor for thermostable DNA polymerase. Taq polymerase is a magnesium-dependent enzyme and determining the optimum concentration to use is critical to the success of the PCR reaction. Some of the components of the reaction mixture such as template concentration, dNTPs and the presence of chelating agents (EDTA) or proteins can reduce the amount of free magnesium present thus reducing the activity of the enzyme. Primers which bind to incorrect template sites are stabilized in the presence of excessive magnesium concentrations and so results in decreased specificity of the reaction. Excessive magnesium concentrations also stabilize double stranded DNA and prevent complete denaturation of the DNA during PCR reducing the product yield. Inadequate thawing of MgCl may result in the formation of concentration gradients within the magnesium chloride solution supplied with the DNA polymerase and also contributes to many failed experiments .\n\nPCR works readily with a DNA template of up to two to three thousand base pairs in length. However, above this size, product yields often decrease, as with increasing length stochastic effects such as premature termination by the polymerase begin to affect the efficiency of the PCR. It is possible to amplify larger pieces of up to 50,000 base pairs with a slower heating cycle and special polymerases. These are polymerases fused to a processivity-enhancing DNA-binding protein, enhancing adherence of the polymerase to the DNA.\n\nOther valuable properties of the chimeric polymerases TopoTaq and PfuC2 include enhanced thermostability, specificity and resistance to contaminants and inhibitors. They were engineered using the unique helix-hairpin-helix (HhH) DNA binding domains of topoisomerase V from hyperthermophile \"Methanopyrus kandleri\". Chimeric polymerases overcome many limitations of native enzymes and are used in direct PCR amplification from cell cultures and even food samples, thus by-passing laborious DNA isolation steps. A robust strand-displacement activity of the hybrid TopoTaq polymerase helps solve PCR problems that can be caused by hairpins and G-loaded double helices. Helices with a high G-C content possess a higher melting temperature, often impairing PCR, depending on the conditions.\n\nNon-specific binding of primers frequently occurs and may occur for several reasons. These include repeat sequences in the DNA template, non-specific binding between primer and template, high or low G-C content in the template, or incomplete primer binding, leaving the 5' end of the primer unattached to the template. Non-specific binding of degenerate primers is also common. Manipulation of annealing temperature and magnesium ion concentration may be used to increase specificity. For example, lower concentrations of magnesium or other cations may prevent non-specific primer interactions, thus enabling successful PCR. A \"hot-start\" polymerase enzyme whose activity is blocked unless it is heated to high temperature (e.g., 90–98˚C) during the denaturation step of the first cycle, is commonly used to prevent non-specific priming during reaction preparation at lower temperatures. Chemically mediated hot-start PCRs require higher temperatures and longer incubation times for polymerase activation, compared with antibody or aptamer-based hot-start PCRs.\n\nOther methods to increase specificity include Nested PCR and Touchdown PCR.\n\nComputer simulations of theoretical PCR results (Electronic PCR) may be performed to assist in primer design.\n\nTouchdown polymerase chain reaction or touchdown style polymerase chain reaction is a method of polymerase chain reaction by which primers will avoid amplifying nonspecific sequences. The annealing temperature during a polymerase chain reaction determines the specificity of primer annealing. The melting point of the primer sets the upper limit on annealing temperature. At temperatures just below this point, only very specific base pairing between the primer and the template will occur. At lower temperatures, the primers bind less specifically. Nonspecific primer binding obscures polymerase chain reaction results, as the nonspecific sequences to which primers anneal in early steps of amplification will \"swamp out\" any specific sequences because of the exponential nature of polymerase amplification.\n\nThe earliest steps of a touchdown polymerase chain reaction cycle have high annealing temperatures. The annealing temperature is decreased in increments for every subsequent set of cycles (the number of individual cycles and increments of temperature decrease is chosen by the experimenter). The primer will anneal at the highest temperature which is least-permissive of nonspecific binding that it is able to tolerate. Thus, the first sequence amplified is the one between the regions of greatest primer specificity; it is most likely that this is the sequence of interest. These fragments will be further amplified during subsequent rounds at lower temperatures, and will out compete the nonspecific sequences to which the primers may bind at those lower temperatures. If the primer initially (during the higher-temperature phases) binds to the sequence of interest, subsequent rounds of polymerase chain reaction can be performed upon the product to further amplify those fragments.\n\nAnnealing of the 3' end of one primer to itself or the second primer may cause primer extension, resulting in the formation of so-called primer dimers, visible as low-molecular-weight bands on PCR gels. Primer dimer formation often competes with formation of the DNA fragment of interest, and may be avoided using primers that are designed such that they lack complementarity—especially at the 3' ends—to itself or the other primer used in the reaction. If primer design is constrained by other factors and if primer-dimers do occur, methods to limit their formation may include optimisation of the MgCl concentration or increasing the annealing temperature in the PCR.\n\nDeoxynucleotides (dNTPs) may bind Mg ions and thus affect the concentration of free magnesium ions in the reaction. In addition, excessive amounts of dNTPs can increase the error rate of DNA polymerase and even inhibit the reaction. An imbalance in the proportion of the four dNTPs can result in misincorporation into the newly formed DNA strand and contribute to a decrease in the fidelity of DNA polymerase.\n", "id": "10714473", "title": "Polymerase chain reaction optimization"}
{"url": "https://en.wikipedia.org/wiki?curid=14766447", "text": "Dojindo\n\n, otherwise known as Dojindo, is composed of a group of chemical companies that produce various reagents for use in research. These products are used by many organizations such as universities, medical institutions, research institutes, and factories around the world. The main branch is located in Kumamoto, Japan. There are branches in Europe, China, and the United States (Dojindo Molecular Technologies, Inc.).\n\nDojindo's history can be traced back to the establishment of Dojindo Pharmacy by Keiji Ueno in 1910. It became incorporated in 1978. The current chairman is Keisyou Ueno, and the current president is Eiji Noda.\n\n", "id": "14766447", "title": "Dojindo"}
{"url": "https://en.wikipedia.org/wiki?curid=14677231", "text": "Insert (molecular biology)\n\nIn Molecular biology, an insert is a piece of DNA that is inserted into a larger DNA vector by a recombinant DNA technique, such as ligation or recombination. This allows it to be multiplied, selected, further manipulated or expressed in a host organism.\n", "id": "14677231", "title": "Insert (molecular biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=12185751", "text": "International Max Planck Research School for Molecular and Cellular Biology\n\nThe International Max Planck Research School for Molecular and Cellular Biology (IMPRS-MCB) is an international PhD program in molecular biology and cellular biology founded in 2006 by the Max Planck Institute of Immunobiology and Epigenetics and the University of Freiburg.\n\nThe Max Planck Society (MPG) started in 2000 an initiative to attract more international students to Germany to pursue their PhD studies. Therefore, International-Max-Planck-Research-Schools (IMPRS) were established. The number of IMPRS has ever been increasing since then in all three sections of research of the MPG.\n\n", "id": "12185751", "title": "International Max Planck Research School for Molecular and Cellular Biology"}
{"url": "https://en.wikipedia.org/wiki?curid=15250377", "text": "NdeI\n\n\"Nde\"I is an endonuclease isolated from \"Neisseria denitrificans\".\n\nIn molecular biology, it is commonly used as a restriction enzyme.\n\nRecognition sequence of \"Nde\"I:\n\nThe ends generated by \"Nde\"I digest:\n\n\"Nde\"I is a specific Type II restriction enzyme that cuts open specific target sequences, unlike exonucleases. This enzyme is used in gene cloning to cut open reading frames in the plasmid of certain bacteria such as E. coli and insert a foreign gene, such as the gfpuv gene that codes for bio fluorescence of the jelly fish \"Aequorea victoria\".\n\n\"Nde\"I is useful in generating heterologous DNA construct because it contains the start codon ATG. It therefore can be in used in some expression vectors, such as those in the pET series of vectors, for the ligation of the start of a gene when making an expression construct. Note however that \"Nde\"I generates only a two-base overhang and therefore has a lower melting temperature than other restriction enzymes that generate a four-base overhang. It has a lower ligation efficiency, since ligation is affected by the ability of the ends to anneal and a two-base overhang has a significantly lower melting temperature compared to a 4-base overhang. Ligation of \"Nde\"I-generated ends is therefore best performed with higher ligase concentration with a longer ligation time, whether at room temperature, 14-16C, or at 4C.\n", "id": "15250377", "title": "NdeI"}
{"url": "https://en.wikipedia.org/wiki?curid=15386743", "text": "Vector (molecular biology)\n\nIn molecular cloning, a vector is a DNA molecule used as a vehicle to artificially carry foreign genetic material into another cell, where it can be replicated and/or expressed (e.g.- plasmid, cosmid, Lambda phages). A vector containing foreign DNA is termed recombinant DNA. The four major types of vectors are plasmids, viral vectors, cosmids, and artificial chromosomes. Of these, the most commonly used vectors are plasmids. Common to all engineered vectors are an origin of replication, a multicloning site, and a selectable marker.\n\nThe vector itself is generally a DNA sequence that consists of an insert (transgene) and a larger sequence that serves as the \"backbone\" of the vector. The purpose of a vector which transfers genetic information to another cell is typically to isolate, multiply, or express the insert in the target cell. All vectors may be used for cloning and are therefore cloning vectors, but there are also vectors designed specially for cloning, while others may be designed specifically for other purposes, such as transcription and protein expression. Vectors designed specifically for the expression of the transgene in the target cell are called expression vectors, and generally have a promoter sequence that drives expression of the transgene. Simpler vectors called transcription vectors are only capable of being transcribed but not translated: they can be replicated in a target cell but not expressed, unlike expression vectors. Transcription vectors are used to amplify their insert.\n\nThe manipulation of DNA is normally conducted on \"E. coli\" vectors, which contain elements necessary for their maintenance in \"E. coli\". However, vectors may also have elements that allow them to be maintained in another organism such as yeast, plant or mammalian cells, and these vectors are called shuttle vectors. Such vectors have bacterial or viral elements which may be transferred to the non-bacterial host organism, however other vectors termed intragenic vectors have also been developed to avoid the transfer of any genetic material from an alien species.\n\nInsertion of a vector into the target cell is usually called transformation for bacterial cells, transfection for eukaryotic cells, although insertion of a viral vector is often called transduction.\n\nPlasmids are double-stranded and generally circular DNA sequences that are capable of automatically replicating in a host cell. Plasmid vectors minimalistically consist of an origin of replication that allows for semi-independent replication of the plasmid in the host. Plasmids are found widely in many bacteria, for example in \"Escherichia coli\", but may also be found in a few eukaryotes, for example in yeast such as \"Saccharomyces cerevisiae\". Bacterial plasmids may be conjugative/transmissible and non-conjugative:\n\nPlasmids with specially-constructed features are commonly used in laboratory for cloning purposes. These plasmid are generally non-conjugative but may have many more features, notably a \"multiple cloning site\" where multiple restriction enzyme cleavage sites allow for the insertion of a transgene insert. The bacteria containing the plasmids can generate millions of copies of the vector within the bacteria in hours, and the amplified vectors can be extracted from the bacteria for further manipulation. Plasmids may be used specifically as transcription vectors and such plasmids may lack crucial sequences for protein expression. Plasmids used for protein expression, called expression vectors, would include elements for translation of protein, such as a ribosome binding site, start and stop codons.\n\nViral vectors are generally genetically engineered viruses carrying modified viral DNA or RNA that has been rendered noninfectious, but still contain viral promoters and also the transgene, thus allowing for translation of the transgene through a viral promoter. However, because viral vectors frequently are lacking infectious sequences, they require helper viruses or packaging lines for large-scale transfection. Viral vectors are often designed for permanent incorporation of the insert into the host genome, and thus leave distinct genetic markers in the host genome after incorporating the transgene. For example, retroviruses leave a characteristic retroviral integration pattern after insertion that is detectable and indicates that the viral vector has incorporated into the host genome.\n\nTranscription of the cloned gene is a necessary component of the vector when expression of the gene is required: one gene may be amplified through transcription to generate multiple copies of mRNAs, the template on which protein may be produced through translation. A larger number of mRNAs would express a greater amount of protein, and how many copies of mRNA are generated depends on the promoter used in the vector. The expression may be constitutive, meaning that the protein is produced constantly in the background, or it may be inducible whereby the protein is expressed only under certain condition, for example when a chemical inducer is added. These two different types of expression depend on the types of promoter and operator used. \n\nViral promoters are often used for constitutive expression in plasmids and in viral vectors because they normally force constant transcription in many cell lines and types reliably. Inducible expression depends on promoters that respond to the induction conditions: for example, the murine mammary tumor virus promoter only initiates transcription after dexamethasone application and the \"Drosophilia\" heat shock promoter only initiates after high temperatures. \n\nSome vectors are designed for transcription only, for example for \"in vitro\" mRNA production. These vectors are called transcription vectors. They may lack the sequences necessary for polyadenylation and termination, therefore may not be used for protein production.\n\nExpression vectors produce proteins through the transcription of the vector's insert followed by translation of the mRNA produced, they therefore require more components than the simpler transcription-only vectors. Expression in different host organism would require different elements, although they share similar requirements, for example a promoter for initiation of transcription, a ribosomal binding site for translation initiation, and termination signals.\n\n\nEukaryote expression vectors require sequences that encode for:\n\nModern artificially-constructed vectors contain essential components found in all vectors, and may contain other additional features found only in some vectors:\n\n\n\n\n", "id": "15386743", "title": "Vector (molecular biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=13535930", "text": "Iteron\n\nIterons are directly repeated DNA sequences which play an important role in regulation of plasmid copy number in bacterial cells. It is one among the three negative regulatory elements found in plasmids which control its copy number. The others include antisense RNAs and ctRNAs. Iterons complex with cognate replication (Rep) initiator proteins to achieve the required regulatory effect.\n\nIterons have an important role in plasmid replication. An iteron-containing plasmid origin of replication can be found containing about five iterons about 20 base pairs in length total. These iterons provide a saturation site for initiator receptor proteins and promote replication thus increasing plasmid copy number in a given cell.\n\nThere are 4 main limiting factors leading to no initiation of replication in iterons:\n\n\nTranscriptional auto-repression is thought to reduce initiator synthesis by repressing the formation of the Rep proteins. Since these proteins work to promote binding of replication machinery, replication can be halted in this form. Another factor used to stop replication is known as dimerization. It works to dimerize these Rep proteins and as a result monomers of these proteins are no longer in a high enough concentration to initiate replication. Another limiting factor, titration, occurs after replication and works to prevent saturation by distributing monomers to daughter origins so that no are fully saturated. Finally, handcuffing refers to pairing origins leading to inactivation. This is mediated by monomers and inactivation is due to steric hindrance between the origins.\n\nAnother less prevalent limitation thought to be present in these iterons is the presence of extra repeats. If a plasmid contains an extra supply of iterons outside of the saturation site it has been shown this can decrease plasmid copy number. In contrast, removing these extra iterons will increase copy number.\n\nPlasmids are known to have very similar structure when under control of Iterons. This structure consists of an origin of replication upstream of a gene that codes for a replication initiator protein. The iterons themselves are known to cover about half of the origin of replication. Usually, iterons on the same plasmid are highly conserved, whereas comparing iterons on different plasmids still exhibit homology yet are not as highly conserved. This suggests that iterons could be evolutionarily related.\n\nThe replication initiator protein (Rep) plays a key role in initiation of replication in plasmids. In its monomer form, Rep binds an iteron and promotes replication. The protein itself is known to contain two independent N-terminal and C-terminal globular domains that subsequently bind to two domains of the iteron. The dimer version of the protein is generally inactive in iteron binding, however it is known to bind to the repE operator. This operator contains half of the iteron sequence making it able to bind the dimer and promote gene expression.\n\nPlasmids containing iterons are all organized very similarly in structure. The gene for Rep proteins is usually found directly downstream of the origin of replication. This means that the iterons themselves are known to regulate the synthesis of the rep proteins.\n", "id": "13535930", "title": "Iteron"}
{"url": "https://en.wikipedia.org/wiki?curid=15428737", "text": "Molecular probe\n\nA molecular probe is a group of atoms or molecules used in molecular biology or chemistry to study the properties of other molecules or structures. If some measurable property of the molecular probe used changes when it interacts with the analyte (such as a change in absorbance), the interactions between the probe and the analyte can be studied. This makes it possible to indirectly study the properties of compounds and structures which may be hard to study directly. The choice of molecular probe will depend on which compound or structure is being studied as well as on what property is of interest. Radioactive DNA or RNA sequences are used in molecular genetics to detect the presence of a complementary sequence by molecular hybridization.\n\n\n", "id": "15428737", "title": "Molecular probe"}
{"url": "https://en.wikipedia.org/wiki?curid=1258217", "text": "Transposase\n\nTransposase is an enzyme that binds to the end of a transposon and catalyzes the movement of the transposon to another part of the genome by a cut and paste mechanism or a replicative transposition mechanism. \nThe word \"transposase\" was first coined by the individuals who cloned the enzyme required for transposition of the Tn3 transposon. The existence of transposons was postulated in the late 1940s by Barbara McClintock, who was studying the inheritance of maize, but the actual molecular basis for transposition was described by later groups. McClintock discovered that pieces of the chromosomes changed their position, jumping from one chromosome to another. The repositioning of these transposons (which coded for color) allowed other genes for pigment to be expressed. Transposition in maize causes changes in color; however, in other organisms, such as bacteria, it can cause antibiotic resistance. Transposition is also important in creating genetic diversity within species and adaptability to changing living conditions. During the course of human evolution, as much as 40% of the human genome has moved around via methods such as transposition of transposons.\n\nTransposases are classified under EC number EC 2.7.7.\n\nGenes encoding transposases are widespread in the genomes of most organisms and are the most abundant genes known.\n\nTransposase (Tnp) Tn5 is a member of the RNase superfamily of proteins which includes retroviral integrases. Tn5 can be found in \"Shewanella\" and \"Escherichia\" bacteria. The transposon codes for antibiotic resistance to kanamycin and other aminoglycoside antibiotics.\n\nTn5 and other transposases are notably inactive. Because DNA transposition events are inherently mutagenic, the low activity of transposases is necessary to reduce the risk of causing a fatal mutation in the host, and thus eliminating the transposable element. One of the reasons Tn5 is so unreactive is because the N- and C-termini are located in relatively close proximity to one another and tend to inhibit each other. This was elucidated by the characterization of several mutations which resulted in hyperactive forms of transposases. One such mutation, L372P, is a mutation of amino acid 372 in the Tn5 transposase. This amino acid is generally a leucine residue in the middle of an alpha helix. When this leucine is replaced with a proline residue the alpha helix is broken, introducing a conformational change to the C-Terminal domain, separating it from the N-Terminal domain enough to promote higher activity of the protein. The transposition of a transposon often needs only three pieces: the transposon, the transposase enzyme, and the target DNA for the insertion of the transposon. This is the case with Tn5, which uses a cut-and-paste mechanism for moving around transposons.\n\nTn5 and most other transposases contain a DDE motif, which is the active site that catalyzes the movement of the transposon. Aspartate-97, Aspartate-188, and Glutamate-326 make up the active site, which is a triad of acidic residues. The DDE motif is said to coordinate divalent metal ions, most often magnesium and manganese, which are important in the catalytic reaction. Because transposase is incredibly inactive, the DDE region is mutated so that the transposase becomes hyperactive and catalyzes the movement of the transposon. The glutamate is transformed into an aspartate and the two asparates into glutamates. Through this mutation, the study of Tn5 becomes possible, but some steps in the catalytic process are lost as a result.\n\nThere are several steps which catalyze the movement of the transposon, including Tnp binding, synapsis (the creation of a synaptic complex), cleavage, target capture, and strand transfer. Transposase then binds to the DNA strand and creates a clamp over the transposon end of the DNA and inserts into the active site. Once the transposase binds to the transposon, it produces a synaptic complex in which two transposases are bound in a cis/trans relationship with the transposon.\n\nIn cleavage, the magnesium ions activate oxygen from water molecules and expose them to nucleophilic attack. This allows the water molecules to nick the 3' strands on both ends and create a hairpin formation, which separates the transposon from the donor DNA. Next, the transposase moves the transposon to a suitable location. Not much is known about the target capture, although there is a sequence bias which has not yet been determined. After target capture, the transposase attacks the target DNA nine base pairs apart, resulting in the integration of the transposon into the target DNA.\n\nAs mentioned before, due to the mutations of the DDE, some steps of the process are lost—for example, when this experiment is performed \"in vitro\", and SDS heat treatment denatures the transposase. However, it is still uncertain what happens to the transposase \"in vivo\".\n\nThe study of transposase Tn5 is of general importance because of its similarities to HIV-1 and other retroviral diseases. By studying Tn5, much can also be discovered about other transposases and their activities.\n\nTn5 is utilized in genome sequencing for fragmentation of the DNA, in the technique called ATAC-seq.\n\nThe Sleeping Beauty (SB) transposase is the recombinase that drives the Sleeping Beauty transposon system. SB transposase belongs to the DD[E/D] family of transposases, which in turn belong to a large superfamily of polynucleotidyl transferases that includes RNase H, RuvC Holliday resolvase, RAG proteins, and retroviral integrases. The SB system is used primarily in vertebrate animals for gene transfer, including gene therapy, and gene discovery. The engineered SB100X is an enzyme that directs the high levels of transposon integration.\n", "id": "1258217", "title": "Transposase"}
{"url": "https://en.wikipedia.org/wiki?curid=15586765", "text": "IMAGE cDNA clones\n\nIMAGE cDNA clones are a collection of DNA vectors containing cDNAs from various organisms including human, mouse, rat, non-human primates, zebrafish, pufferfish, Xenopus (frogs), and cow. Together they represent a more or less complete set of expressed genes from these organisms. IMAGE stands for \"integrated molecular analysis of genomes and their expression\".\n\nFrom 1993 to 2007, the cDNA library was maintained by the \"IMAGE Consortium\", a joint effort of four academic groups. At the end of 2007, the consortium handed over operations and stocks to a company associated with Open Biosystems.\n\n\n", "id": "15586765", "title": "IMAGE cDNA clones"}
{"url": "https://en.wikipedia.org/wiki?curid=15659584", "text": "Host-Cell Reactivation\n\nThe term host cell reactivation HCR was first used to describe the survival of UV-irradiated bacteriophages, that were trasfected to UV-pretreated cells. This phenomenon was first thought to be the result of homologous recombination between both bacteria and phage, but later recognized as enzymatic repair. Modifications of the assay were later developed, using transient expression plasmid DNA vectors on immortalized fibroblasts, and lately on human lymphocytes.\n\nThe HCR assay known also as plasmid reactivation assay, indirectly monitors cellular transcriptional repair system, that is activated by the transcriptional-inhibited damage inflected by UV-Radiation into the plasmid. Given that UV-induced DNA damage is used as mutagen, the cell uses nucleotide excision repair NER pathway, that is activated by distortion of de DNA helix \n\nThe Host-Cell Reactivation Assay or HCR is a technique used to measure the DNA repair capacity of cell of a particular DNA alteration. In the HCR assay the ability of an intact cell to repair exogenous DNA is measured The host cell is transfected with a damaged plasmid containing a reporter gene, usually luciferase, which has been deactivated due to the damage. The ability of the cell to repair the damage in the plasmid, after it has been introduced to the cell, allows the reporter gene to be reactivated. Earlier versions of this assay were based on the chloramphenicol acetyltransferase (CAT) gene, but the version of the assay using luciferase as reporter gene is as much as 100-fold more sensitive.\n\n<div style=\"-moz-column-count:3; column-count:3;\">\n", "id": "15659584", "title": "Host-Cell Reactivation"}
{"url": "https://en.wikipedia.org/wiki?curid=7687444", "text": "Cot analysis\n\nCt analysis, a technique based on the principles of DNA reassociation kinetics, is a biochemical technique that measures how much repetitive DNA is in a DNA sample such as a genome. It is used to study genome structure and organization and has also been used to simplify the sequencing of genomes that contain large amounts of repetitive sequence.\n\nThe procedure involves heating a sample of genomic DNA until it denatures into the single stranded-form, and then slowly cooling it, so the strands can pair back together. While the sample is cooling, measurements are taken of how much of the DNA is base paired at each temperature. \n\nThe amount of single and double-stranded DNA is measured by rapidly diluting the sample, which slows reassociation, and then binding the DNA to a hydroxylapatite column. The column is first washed with a low concentration of sodium phosphate buffer, which elutes the single-stranded DNA, and then with high concentrations of phosphate, which elutes the double stranded DNA. The amount of DNA in these two solutions is then measured using a spectrophotometer.\n\nSince a sequence of single-stranded DNA needs to find its complementary strand to reform a double helix, common sequences renature more rapidly than rare sequences. Indeed, the rate at which a sequence will reassociate is proportional to the number of copies of that sequence in the DNA sample. A sample with a highly-repetitive sequence will renature rapidly, while complex sequences will renature slowly. \n\nHowever, instead of simply measuring the percentage of double-stranded DNA versus time, the amount of renaturation is measured relative to a Ct value. The Ct value is the product of C (the initial concentration of DNA), t (time in seconds), and a constant that depends on the concentration of cations in the buffer. Repetitive DNA will renature at low Ct values, while complex and unique DNA sequences will renature at high Ct values. The fast renaturation of the repetitive DNA is because of the availability of numerous complementary sequences.\n\nCt filtration is a technique that uses the principles of DNA renaturation kinetics to separate the repetitive DNA sequences that dominate many eukaryotic genomes from \"gene-rich\" single/low-copy sequences. This allows DNA sequencing to concentrate on the parts of the genome that are most informative and interesting, which will speed up the discovery of new genes and make the process more efficient.\n\nIt was first developed and utilized by Roy Britten and his colleagues at the Carnegie Institution of Washington in the 1960s. Of particular note, it was through Ct analysis that the redundant (repetitive) nature of eukaryotic genomes was first discovered. However, it wasn't until the breakthrough DNA reassociation kinetics experiments of Britten and his colleagues that it was shown that not all DNA coded for genes. In fact, their experiments demonstrated that the majority of eukaryotic genomic DNA is composed of repetitive, non-coding elements.\n\n", "id": "7687444", "title": "Cot analysis"}
{"url": "https://en.wikipedia.org/wiki?curid=7684354", "text": "Cot filtration\n\nCt filtration, or CF, is a technique that uses the principles of DNA renaturation kinetics (\"i\".\"e\". Cot analysis) to separate the repetitive DNA sequences that dominate many eukaryotic genomes from \"gene-rich\" single/low-copy sequences. This allows DNA sequencing to concentrate on the parts of the genome that are most informative and interesting.\n\nBriefly, when sheared genomic DNA in solution is heated to near boiling temperature, the molecular forces holding complementary base pairs together are disrupted, and the two strands of each double-helix dissociate or ‘denature.’ If the denatured DNA is then slowly returned to a cooler temperature, sequences will begin to ‘reassociate’ (renature) with complementary strands. \n\nThe temperature at which renaturation occurs can be regulated so that little or no sequence mismatch is tolerated. The rate at which a sequence finds a complementary strand with which to hybridize is directly related to how common that sequence is in the genome. In other words, those sequences that are extremely abundant (on average) find complementary strands with which to pair relatively quickly while single-copy sequences take much longer to find complements. \n\nIn CF, genomic DNA is heat-denatured and allowed to renature to a Cot value (Cot = DNA concentration x time x a factor based on the cation concentration of the buffer) at which the majority of repetitive elements have reassociated but single and low-copy elements remain single stranded. Double-stranded, repetitive DNA is separated from single-stranded, low-copy DNA by hydroxyapatite chromatography or other means.\n\nCF allows the single/low copy sequences and the repetitive sequences of a genome to be studied independently of each other. It can also be used to fractionate highly repetitive DNA from moderately repetitive sequences or to further fractionate isolated kinetic components. CF is most accurately performed if fractionation is based upon the results of a Cot analysis.\n\n\n", "id": "7684354", "title": "Cot filtration"}
{"url": "https://en.wikipedia.org/wiki?curid=3772981", "text": "EMBO Reports\n\nEMBO Reports is a peer-reviewed scientific journal covering research related to biology at a molecular level. It publishes primary research papers, reviews, and essays and opinion. It also features commentaries on the social impact of advances in the life sciences and the converse influence of society on science. A sister journal to \"The EMBO Journal\", \"EMBO Reports\" was established in 2000 and was published on behalf of the European Molecular Biology Organization by Nature Publishing Group since 2003. It is now published by \"EMBO Press\", which also publishes \"The EMBO Journal\" and \"Molecular Systems Biology\".\n", "id": "3772981", "title": "EMBO Reports"}
{"url": "https://en.wikipedia.org/wiki?curid=509959", "text": "Coding strand\n\nWhen referring to DNA transcription, the coding strand is the DNA strand whose base sequence corresponds to the base sequence of the RNA transcript produced (although with thymine replaced by uracil). It is this strand which contains codons, while the non-coding strand contains anticodons. During transcription, RNA Pol II binds the non-coding strand, reads the anti-codons, and transcribes their sequence to synthesize an RNA transcript with complementary bases.\n\nBy convention, the coding strand is the strand used when displaying a DNA sequence. It is presented in the 5' to 3' direction.\n\nWherever a gene exists on a DNA molecule, one strand is the coding strand (or sense strand or non-template strand), and the other is the noncoding strand (also called the antisense strand, anticoding strand, template strand, or transcribed strand).\n\nDuring transcription, RNA polymerase unwinds a short section of the DNA double helix near the start of the gene (the transcription start site). This unwound section is known as the transcription bubble. The RNA polymerase, and with it the transcription bubble, travels along the noncoding strand in the opposite, 3' to 5', direction, as well as polymerizing a newly synthesized strand in 5' to 3' or downstream direction. The DNA double helix is rewound by RNA polymerase at the rear of the transcription bubble (Lewin, pp 235 ). Like how two adjacent zippers work, when pulled together, they unzip and rezip as they proceed in a particular direction. Various factors can cause double-stranded DNA to break; thus, reorder genes or cause cell death. \n\nWhere the helix is unwound, the coding strand consists of unpaired bases, while the template strand consists of an RNA:DNA composite, followed by a number of unpaired bases at the rear. This hybrid consists of the most recently added nucleotides of the RNA transcript, complementary base-paired to the template strand. The number of base-pairs in the hybrid is under investigation, but it has been suggested that the hybrid is formed from the last 10 nucleotides added. \n\n\n", "id": "509959", "title": "Coding strand"}
{"url": "https://en.wikipedia.org/wiki?curid=967164", "text": "Edman degradation\n\nEdman degradation, developed by Pehr Edman, is a method of sequencing amino acids in a peptide. In this method, the amino-terminal residue is labeled and cleaved from the peptide without disrupting the peptide bonds between other amino acid residues.\n\nPhenyl isothiocyanate is reacted with an uncharged N-terminal amino group, under mildly alkaline conditions, to form a cyclical \"phenylthiocarbamoyl\" derivative. Then, under acidic conditions, this derivative of the terminal amino acid is cleaved as a thiazolinone derivative. The thiazolinone amino acid is then selectively extracted into an organic solvent and treated with acid to form the more stable phenylthiohydantoin (PTH)- amino acid derivative that can be identified by using chromatography or electrophoresis. This procedure can then be repeated again to identify the next amino acid. A major drawback to this technique is that the peptides being sequenced in this manner cannot have more than 50 to 60 residues (and in practice, under 30). The peptide length is limited due to the cyclical derivatization not always going to completion. The derivatization problem can be resolved by cleaving large peptides into smaller peptides before proceeding with the reaction. It is able to accurately sequence up to 30 amino acids with modern machines capable of over 99% efficiency per amino acid. An advantage of the Edman degradation is that it only uses 10 - 100 pico-moles of peptide for the sequencing process. The Edman degradation reaction was automated in 1967 by Edman and Beggs to speed up the process and 100 automated devices were in use worldwide by 1973.\n\nBecause the Edman degradation proceeds from the N-terminus of the protein, it will not work if the N-terminus has been chemically modified (e.g. by acetylation or formation of pyroglutamic acid). Sequencing will stop if a non-α-amino acid is encountered (e.g. isoaspartic acid), since the favored five-membered ring intermediate is unable to be formed. Edman degradation is generally not useful to determine the positions of disulfide bridges. It also requires peptide amounts of 1 picomole or above for discernible results.\n\nFollowing 2D SDS PAGE the proteins can be transferred to a polyvinylidene difluoride (PVDF) blotting membrane for further analysis. Edman degradations can be performed directly from a PVDF membrane. N-terminal residue sequencing resulting in five to ten amino acid may be sufficient to identify a Protein of Interest (POI).\n\n", "id": "967164", "title": "Edman degradation"}
{"url": "https://en.wikipedia.org/wiki?curid=16784189", "text": "Oligomer restriction\n\nOligomer Restriction (abbreviated OR) is a procedure to detect an altered DNA sequence in a genome. A labeled oligonucleotide probe is hybridized to a target DNA, and then treated with a restriction enzyme. If the probe exactly matches the target, the restriction enzyme will cleave the probe, changing its size. If, however, the target DNA does not exactly match the probe, the restriction enzyme will have no effect on the length of the probe. The OR technique, now rarely performed, was closely associated with the development of the popular polymerase chain reaction (PCR) method.\n\nIn part 1a of the schematic the oligonucleotide probe, labeled on its left end (asterisk), is shown on the top line. It is fully complementary to its target DNA (here taken from the human β-hemoglobin gene), as shown on the next line. Part of the probe includes the Recognition site for the restriction enzyme Dde I (underlined).\n\nIn part 1b, the restriction enzyme has cleaved the probe and its target (Dde I leaves three bases unpaired at each end). The labeled end of the probe is now just 8 bases in length, and is easily separated by Gel electrophoresis from the uncut probe, which was 40 bases long.\n\nIn part 2, the same probe is shown hybridized to a target DNA which includes a single base mutation (here the mutation responsible for Sickle Cell Anemia, or SCA). The mismatched hybrid no longer acts as a recognition site for the restriction enzyme, and the probe remains at its original length.\n\nThe Oligomer Restriction technique was developed as a variation of the Restriction Fragment Length Polymorphism (RFLP) assay method, with the hope of avoiding the laborious Southern blotting step used in RFLP analysis. OR was conceived by Randall Saiki and Henry Erlich in the early 1980s, working at Cetus Corporation in Emeryville, California. It was patented in 1984 and published in 1985, having been applied to the genomic mutation responsible for Sickle Cell Anemia. OR was soon replaced by the more general technique of Allele Specific Oligonucleotide (ASO) probes.\n\nThe Oligomer Restriction method was beset by a number of problems:\n\n\nDespite its limitations, the OR technique benefited from its close association with the development of the polymerase chain reaction. Kary Mullis, who also worked at Cetus, had synthesized the oligonucleotide probes being tested by Saiki and Erlich. Aware of the problems they were encountering, he envisioned an alternative method for analyzing the SCA mutation that would use components of the Sanger DNA sequencing technique. Realizing the difficulty of hybridizing an oligonucleotide primer to a single location in the genome, he considered using a second primer on the opposite strand. He then generalized that process and realized that repeated extensions of the two primers would lead to an exponential increase in the segment of DNA between the primers - a chain reaction of replication catalyzed by DNA polymerase.\n\nAs Mullis encountered his own difficulties in demonstrating PCR, he joined an existing group of researchers that were addressing the problems with OR. Together, they developed the combined PCR-OR assay. Thus, OR became the first method used to analyze PCR-amplified genomic DNA.\n\nMullis also encountered difficulties in publishing the basic idea of PCR (scientific journals rarely publish concepts without accompanying results). When his manuscript for the journal Nature was rejected, the basic description of PCR was hurriedly added to the paper originally intended to report the OR method (Mullis was also a co-author there). This OR paper thus became the first publication of PCR, and for several years would become the report most cited by other researchers.\n", "id": "16784189", "title": "Oligomer restriction"}
{"url": "https://en.wikipedia.org/wiki?curid=5650682", "text": "The Regenerative Medicine Institute\n\nThe Regenerative Medicine Institute (REMEDI), was established in 2003 as a Centre for Science, Technology & Engineering in collaboration with National University of Ireland, Galway. It obtained an award of €14.9 million from Science Foundation Ireland over five years.\n\nIt conducts basic research and applied research in regenerative medicine, an emerging field that combines the technologies of gene therapy and adult stem cell therapy. The goal is to use cells and genes to regenerate healthy tissues that can be used to repair or replace other tissues and organs in a minimally invasive approach.\n\nCentres for Science, Engineering & Technology help link scientists and engineers in partnerships across academia and industry to address crucial research questions, foster the development of new and existing Irish-based technology companies, attract industry that could make an important contribution to Ireland and its economy, and expand educational and career opportunities in Ireland in science and engineering. CSETs must exhibit outstanding research quality, intellectual breadth, active collaboration, flexibility in responding to new research opportunities, and integration of research and education in the fields that SFI supports.\n\n", "id": "5650682", "title": "The Regenerative Medicine Institute"}
{"url": "https://en.wikipedia.org/wiki?curid=15668437", "text": "Gartons Agricultural Plant Breeders\n\nDr John Garton, of the firm of Garton Brothers of Newton-le-Willows in the United Kingdom was the Originator of Scientific Farm Plant Breeding. He is credited as the first scientist to show that the common grain crops and many other plants are self-fertilizing. He also invented the process of multiple cross-fertilization of crop plants.\n\nIn 1898 the business became known as Gartons Limited and, under the inspired commercial leadership of George Peddie Miln, was to become the British Empire's largest plant breeding and seed company. \n\nA public company from the start, its shares were traded on the London Stock Exchange from 1947.\n\nJohn Garton and his two brothers, Robert and Thomas, were in business with their father, Peter, in Golborne and Newton-le-Willows in Lancashire, England, as corn and agricultural merchants.\n\nAs a young man, John Garton (1863–1922), was the first to understand that whilst some agricultural plants were self-pollinating, others were cross-pollinating. He began experimenting with the artificial cross pollination firstly of cereal plants, then herbage species and root crops. \n\nHe attracted the friendship and encouragement of a young Scottish seedsman, George Peddie Miln (1861–1928) who had trained in Dundee and was seed manager of Dicksons Limited of Chester.\n\nKnowing he had developed a far reaching new technique in plant breeding John Garton began to carry out many thousands of controlled crosses on fields at the family farm in Newton-le-Willows. He and his colleagues tried in 1889 to interest the UK Government’s new Board of Agriculture in the invention they called Scientific Farm Plant Breeding. But this was to no avail.\n\nRobert and John Garton made a commercial start as R. & J. Garton. They launched their first variety, 'Abundance' oat, in 1892.\n\nGenerous publicity followed in the press, together with the publication of articles by botanists in the Journal of the Royal Agricultural Society of England, and in the Transactions of the Royal Highland and Agricultural Society of Scotland in 1894 and 1898. Professor Robert Wallace (1853–1939) of the University of Edinburgh said 'Under the system originated by Mr John Garton an infinite number of new and distinct breeds of oats, barleys, wheats, clovers and grasses have been produced'.\n\nIn 1898 a public company was launched, Gartons Limited. It was based in Warrington. Many of the 600 or so subscribers for £50,000 cumulative preference shares of 6% rising to 10% were farmers. \n\nGeorge Peddie Miln joined the company as Managing Director, together with Robert Garton, Thomas R. Garton, Thomas Baxter and Arthur Smith as directors. Robert and John Garton agreed to continue to work for the Company for five years for £500 and to receive the entire ordinary share capital of the new company of £50,000.\n\nIt rapidly became the United Kingdom's best known plant breeding and seed company, and also exported seeds widely.\n\nIn 1900 an endowment was made to found the Garton Lectureship in Indian and Colonial Agriculture at the University of Edinburgh. The Garton Lectureship still exists as a biennial award to promising young lecturers in the School of Agriculture but is without emoluments and is no longer tied to colonial agriculture.\n\nNoel Kingsbury writes:From the late nineteenth century on, seed companies began to play an increasingly important, if not dominant, role in breeding non-cereal crops and a major role in producing varieties for market gardening and for private growers. The production of new cereals was a somewhat different matter - the fact that they were so vitally important for national food supplies and involved large-scale and long-term work made it more likely that they would be the concern of government. \n\nThere were exceptions though, one being the family firm of Gartons of Warrington, Lancashire, in the north of England. Their production of cereals - oats in particular - was appreciated as internationally important during the latter quarter of the nineteenth century and the early years of the twentieth; (The American wheat breeder) Mark Carleton visited them in 1898 and was reportedly astonished at their work, Garton varieties were widely exported throughout the British Empire (then by far and away the world's largest political unit) and the United States. \n\n\"That private companies could be so effective in breeding cereal grains indicated that there was no link of necessity between their improvement and the publicly funded research that was to so dominate this sector over so much of the next century.\" \n\nIn 1903 Professor Willet M. Hays (1859–1927) of the Agricultural Experiment Station in Minnesota, USA said 'No one has done more brilliant work in Agricultural Plant Breeding than Messrs. Garton, and this is from now on to be recognised.'\n\nThe introduction to their 1899 Spring Catalogue reads: \n\n\"Our original idea for the dissemination of the seed of these new breeds as the stocks became sufficiently large for the purpose, was through some public body as in the form of an annual free seed distribution upon similar lines to the free seed distributions carried out by the Governments of the United States, Canada, and several of the British Colonies. \n\n\"On three successive occasions we approached Her Majesty's Government with this object in view, the first occasion being on the formation of the Board of Agriculture, in 1889, when we offered to hand over the whole of the valuable results, providing that body would undertake their dissemination and the continuance of the work, either in the form of an annual free seed distribution or at current market price.\"\n\n\"Upon the last occasion our offer was accompanied by letters and reports from all the leading Agricultural Professors, Botanists, and Scientists in the Kingdom, setting forth the national benefit which would accrue from the dissemination of the results in the form we had suggested. The final reply of Her Majesty's Government, however, was that whilst fully recognising the value of the work, owing to there being no precedent upon which to act in such a matter, they were unable to avail themselves of the offer. This was much to be regretted for had our ideas been carried into effect the British farmer would have been placed in immediate possession of important results, which in the hands of a Public Company would not reach him for many years.\"\n\n\"Our efforts in this direction not having been successful, and as we were not in a position to undertake the work of distribution ourselves, we have placed it in the lands of a Public Company, and we trust that the continued efforts made by us on behalf of the British farmer will be fully appreciated by him, through his support of the Company responsible for the distribution of the seed of our new breeds of agricultural plants\"\n\n\"R. & J. Garton\"\"\n\nThe firm's first historic introduction was 'Abundance' oat, the world's first agricultural plant variety bred from a \"controlled\" cross, introduced to commerce in 1892.\n\nAmong the other 170 crop varieties Gartons Limited bred and introduced to commerce were:\n\n\nOn 23 March 1922 the Senatus Academicus of the University of Edinburgh offered to confer the honorary doctorate of LL.D on John Garton shortly before his death, which he duly accepted. At its meeting on 6 July 1922 the Senatus Academicus learned that John Garton had died.\n\nThe programme and report of the Graduation Ceremony held on 21 July 1922 reads The Senatus Academicus recently conferred the Honorary Degree of Doctor of Laws upon the late John Garton, who duly accepted it. The Degree would have been formally conferred on the present occasion but for his lamented death. Mr Garton invented the process of multiple cross fertilisation of crop plants and has been the means in a great measure of revolutionising field culture by producing hundreds of new and improved varieties which have greatly increased the yields of all the common crops of the farm. The achievement proved to be of immense national importance during the War.\n\nMr Garton first showed that the common grain crops and many other crop plants are self-fertilising. Up till that time they were generally believed to be fertilised by wind or insects.\nMr Garton’s results got in crossing different species of grasses helped to develop the modern conception of species.\n\nTwenty-two tears ago Mr Garton provided the means to establish the Garton Lectures on Colonial and Indian Agriculture, and subsequently he permanently endowed them as an integral part of the work of the Chair of Agriculture.\n\nThe plant breeding grounds were initially at Newton-le-Willows but moved to Acton Grange, two and a half miles south west of Warrington before settling in about 1930 at Little Leigh near Northwich in Cheshire. A seed development farm was located in Essex, and root crop trials were located on farms in the north of England and in Scotland. Traditionally groups of farmers were invited in mid-summer to inspect the plant breeding grounds and be entertained by the Company.\n\nInitially the Seed Warehouse for cleaning and distributing seed was in Newton-le-Willows but moved to Friars Green in Warrington in 1899 by which time the offices were at Thynne Street, Warrington. A purpose built seven story Seed Warehouse and separate Head Office were built at Arpley, Warrington in 1910. There was an L. M. S. railway siding into the Seed Warehouse. On 25 April 1912 the Seed Warehouse burned down but quickly rebuilt largely by the same builders. Seed cleaning machinery, some unique to the Company, ensured the purity of the product. As time went by fewer seeds were ‘picked’ or cleaned by hand by upwards of one hundred staff as machinery became more sophisticated. Across the top of roof of the warehouse was the company’s name which had to be disguised during wartime.\n\nFrom the beginning Gartons Limited tested its seeds for purity and germination at its own seed testing laboratories in Warrington. The 1920 Seeds Act, for the first time, made testing and declaring for purity and germination a legal requirement for all seed companies. The Official Seed Testing Station was created in 1917, firstly in Victoria Street in Westminster, London and then in 1921 within the newly formed National Institute of Agricultural Botany in Cambridge. Larger seed companies including Gartons Limited were licensed to carry out their own purity and germination testing. \n\nGartons Limited was the United Kingdom’s only major agricultural plant breeding company. But this caused them difficulties as early as in their Spring 1900 seed catalogue where a paragraph of the introduction reads: It has come to our knowledge that nearly all the New Breeds introduced by us up to the present time have been renamed by various dealers and are being offered by them under different names. Although the honesty of this conduct is more than questionable, we are resigned for the present to regard it as a novel form of flattery, but we strongly recommend all those who wish to secure our Seeds to order them direct from us, as they cannot be procured from any other genuine source. \n\nAfter the Great War (1914–1918) the United Kingdom government funded cereal breeding at the Plant Breeding Institute at Cambridge which had been founded in 1912, and funded the setting up of plant breeding stations in Edinburgh (1921), Aberystwyth (1919) and in Glasnevin, Northern Ireland in competition with Gartons Limited.\n\nThe 1960 Report of the Committee on Transactions in Seeds set up by Parliament entitled Plant Breeders' Rights stated that whilst two thirds of breeding work was by then carried out by government organisations, one third was in the hands of private breeders. And yet the only non-government funded agricultural crop plant breeding, research and testing establishment visited by the Committee was to Gartons Limited. The United Kingdom's Plant Varieties and Seeds Act 1964 allowed plant breeders to fully protect and be rewarded for their introductions. The last variety bred by Gartons, 'Apex' wheat, was the first British bred wheat to be awarded plant breeders rights in 1967 under this legislation.\n\nThe eldest of the three Garton brothers, Robert Garton, was the first chairman of Gartons Limited. He died in February 1950, aged 91, a widower with no children. He was succeeded by the youngest brother, Thomas R. Garton, who died in May 1956. His son, John, was chairman from August 1963 until September 1965. Dr John Garton, the middle of the three brothers, was never, a director.\n\nNine members of the Miln family were involved with the business over a period of seventy five years.\n\nBorn at Linlathen, Broughty Ferry in 1861, George Peddie Miln trained in a Dundee seed warehouse, the traditional Scottish training for a young man with ambition in the seed trade. He moved to Chester and ran one of its old established seed merchants before joining Gartons Limited as its first Managing Director in 1898. He was a member of Her Majesty's Board of Agriculture Seeds Advisory Council during the first World War. Both the Seeds Act 1920 and the formation of the National Institute of Agricultural Botany came about with his considerable encouragement during his three-year presidency of the Seed Trade Association of the United Kingdom. Of his eleven children, five trained in the seed trade. He was a Justice of the Peace in the City of Chester, a Fellow of the Linnean Society and a member of the Council of the Royal Agricultural Society of England. He died aged 68 following an unsuccessful operation.\n\nWhen George P. Miln died in 1928 he was succeeded as managing director by his eldest son, Thomas Edward Miln (1890–1963) who for over twenty five years was chairman of the Retail Committee of the Seed Trade Association of the United Kingdom which proudly kept its independence from government control during World War II. A plant breeder as well as a businessman he is credited with the introduction of the sugar beet crop to UK agriculture. \n\nWhen Gartons Limited became a public quoted company on the London Stock Exchange in 1947 T. E. Miln entered into a further ten year employment contract as Managing Director, even though he was already 59, such was his reputation. Both his sons, Wallace and George, joined him in the business.\n\nT. E. Miln was succeeded in 1961 by his elder son, Wallace Miln (1919–1994) also a skilled plant breeder and seed analyst. Wallace Miln was one of the three founders of the British Association of Plant Breeders at the time of the introduction of the United Kingdom's Plant Varieties and Seeds Act 1964. He was twice President of the Seed Trade Association of United Kingdom. He left Gartons Limited in 1973 to join his elder son, Barnaby Miln, who had trained at Gartons and later with Northrup-King in Minneapolis before setting up his own seed business at Bodenham in Herefordshire.\n\nFrom 1947 Gartons Limited's shares were quoted on the London Stock Exchange. The Company's profits for the previous seven years had averaged £48,940. In 1949 the profit of £75,340 was the highest ever recorded by the Company.\n\nIn 1965 Peter Darlington became chairman of Gartons Limited. In a dramatic change of direction in 1967 Gartons Limited ceased retailing seeds directly to farmers. Instead a new brand was created, Gartons GROplan, and marketing became wholesale through agricultural merchants throughout the United Kingdom. However Gartons GROplan was sold to Agricultural Holdings Company Limited in 1971, but Gartons Limited continued as a plant breeding company.\n\nGartons plc, as it became known, ceased trading in 1983. The seed warehouse at Bridge Foot, Warrington, which had dominated the town centre skyline since 1910, was demolished in October 1986. A hotel was built on the site.\n\nThese Explanatory Notes come from the Gartons Seed Catalogue for Spring 1900:\n\nTo those who are not acquainted with the botanical construction of plants it may be well to explain that plants possess generative organs, which correspond to those of the male and female in the animal kingdom. In the animal kingdom, progeny is derived from the mating of different animals of the same breed; in the vegetable kingdom the rule is that the seed is produced through the agency of the generative organs growing together on the same plant. Prior to the commencement of the work initiated by us and carried on during the past 20 years, which has led to the production of our New and Improved Breeds of agricultural plants, it was a recognised belief that many farm plants in the production of their seed were more or less cross-fertilised. The results of our experiments however have proved that such was not the case but that constant in-and-in breeding was the rule. \n\nWhere such in-and-in breeding takes place the results are governed by the same natural laws as the in-and-in breeding of animals. In the production of New Breeds of animals the rule followed is to mate two animals of distinct breeds. The progeny, when fixity of type has been secured, constitutes a New Breed. \n\nUnder our system of plant breeding we carry the mating of varieties or breeds far beyond what is practised in the animal kingdom. In the first instance we mate varieties and also what were formerly regarded as distinct species of the same genera, and after fixation, the progeny by these combinations are further mated together. \n\nA further extension of our system which is in itself unique and instructive, is the mating of uncultivated indigenous plants of the same Natural Order as the cultivated varieties. From such combinations most valuable results have been obtained. \n\nFor example In Southern Asia there exists a species of wheat botanically known as Triticum spelta. In some districts it is looked upon as an indigenous weed infesting the cultivated crops of wheat. \n\nUnder no climatic conditions does the grain of this species shed its seed when ripe, and even in threshing it is not possible to separate the grains, as the spikelets break off at the bases of the glumes, the grains remaining firmly enclosed between the chaff scales. \n\nBy mating the varieties of this species with cultivated varieties, new breeds have been produced which will under no conditions shed their seed when ripe, but which thresh out a perfectly clean sample with a much heavier yield per acre than common wheat. \n\nIn China there is an indigenous species of oat botanically known as Avena nuda or naked oat. The peculiar feature of this species is that the grains (which are very small) grow without any husk, being protected only by the chaff. The habit of the plant is likewise quite unique, four or five grains being suspended upon a thread-like filament about half an inch long. The mating of this species with cultivated varieties has produced new breeds giving yields 50 to 100 per cent. heavier than the original cultivated parents, with a corresponding decrease in the thickness of the skin. \n\nThe wild or land oat, Avena fatua, of Great Britain has likewise been used with marked success in the production of new breeds in conjunction with the cultivated varieties. In the wild oat there is hardiness of constitution, vigour, strength of straw, and remarkable fertility. All these qualities have been retained in the new breeds produced. \n\nAnother part of our system is the improvement of existing varieties of agricultural plants. The method is similar to that adopted by the breeder of stock for the improvement of his animals, when fresh blood of the same breed is introduced from some other herd. \n\nBy crossing two distinct plants of the same variety the resulting progeny is more vigorous and robust in constitution, whilst the habit and individual character of the variety is maintained. \n\nA year later, these Explanatory Notes come from the Gartons Seed Catalogue for Spring 1901:\n\nFOR over 20 years the work of cross-fertilising crop plants, with the object of producing New and Improved Breeds, has been carried on at Newton-le-Willows in Lancashire. It has there for the first time been demonstrated to Scientific Botanists as well as to Agriculturists that all the corn crops (cereals) and nearly all the other common crops of the farm are self fertilising. In other words, each individual plant provides the pollen which is required in the process of producing seed, to fertilise the female organs of its own flowers. This natural process results in a perfect system of in-breeding which has been going on for an indefinite period, making it possible to grow the different varieties of crops of the same kind in close proximity to each other, and even as mixed crops without any danger of crosses being produced. \n\nIf crossing could have occurred in nature it would have been quite impossible to maintain the purity of any variety of crop plant for more than a year or two. As in the Animal Kingdom, the in-breeding of plants tends to the decrease of constitutional vigour, consequently when cross-fertilisation is practised the size and vigour of the selected progeny are increased in a remarkable degree.\nAlthough the natural laws which govern the Animal and the Vegetable Kingdoms bear a very strong resemblance to each other, further points can be realised and greater progress can be made in a limited time with plants than with animals under a system of cross breeding. \n\nNot only have varieties of a given species, but what were formerly regarded as distinct species belonging to the same genus, been successfully mated. \nThe tendency to sterility in their progeny is overcome by introducing pollen from one or other '01' the original plants, it being the male organs of reproduction that are liable to be absent or defective in the progeny of two extremely divergent parent plants. Many varieties as well as species can thus be blended in the formation of a new breed, but as it is necessary to secure fixity of type in every cross bred plant before it is again used for crossing, the labour and care involved are very considerable. \n\nAttempts at the production of first crosses are not new, as these have been practised for many years by experimenters in the same field, who however stopped short of the point at which the Garton System achieved its greatest results, viz. by compound or multiple crossing. This further stage of the work of cross fertilisation leads to a thorough dislocation of the usual course of the law of inheritance by which \"like produces like.\" In the wilderness of uncertainty and confusion which follows and in which the great majority of the progeny are found to be abortive or inferior, a few choice specimens appear which are grown for a number of years until fixity of type has been secured. These superior and selected specimens are adopted as suitable for cultivation, and a number of them are described in this Catalogue and offered to the public as much superior to the old varieties from which they were derived by the Garton System of Plant Breeding. In making the selections the large quantity and superior quality of the grain, together with great standing power in the straw have been the chief characteristics aimed at, and if these desiderata have been secured in a few of the new breeds to the detriment of the habit of \"tillering,\" the difficulty is readily overcome by providing a liberal seeding.\nSome of the most striking and valuable results have been achieved by introducing as progenitors, certain weeds belonging to the same natural order of plants as the cultivated parents. For example, an inferior variety of \"spelt\" wheat Triticum spelta from Southern Asia, has been employed with excellent results to introduce strength of gluten to the grain, and large yielding and standing power to the crop with immunity from shedding its seed during harvest.\nA wild naked Oat, Avena nuda, indigenous to China has been used to produce new breeds which yield in some instances 100 per cent. more than their cultivated parents. Four or five grains are suspended in each spikelet by a thread like filament about half-an-inch long. This peculiar habit of the plant 'has been extended in the progeny and an' accompanying illustration shows a spikelet with no fewer than 14 grains in it.\nThe hardiness of constitution, standing power of straw, and remarkable fertility of the wild or land oat, Avena fatua, of Great Britain have been successfully introduced, but not without many difficulties, into some of the new breeds.\nSome progress has also been made with the improvement of existing varieties of Agricultural plants by introducing pollen from plants of the same variety to increase the vigour of the plant without materially altering its general characteristics.\n\nBarley Varieties:\n\nBarley varieties bred and introduced to UK agriculture include Standwell in 1898, Invincible (1899), Zero (1900), Brewer’s Favourite (1901), The Maltster (1903), Eclipse (1904), Ideal (1906), 1917 (1918), Admiral Beatty (1920), Triumphant (1927).\n\nOat Varieties:\n\nOat varieties bred and introduced to UK agriculture include Abundance in 1892, Pioneer (1899), Tartar King (1899), Waverley (1900), Goldfinder (1901), Storm King (1902), Excelsior (1903), Colossal (1904), Rival (1906), Unversed (1907), Bountiful (1908), The Yielder (1909), The Record (1911), The Leader (1913), Supreme (1915), The Hero (1916), The Captain (1919), Sir Douglas Haig (1920), Marvellous (1921), Superb (1923), Earl Haig (1925), Cropwell (1926), Plentiful (1927), Black Prince (1929), Progress (1930), Unique (1931), Onward (1935), Jubilee (1936), Royal Scot (1940), Spitfire (1945), Early Grey (1946), Forward (1953), Angus (1959).\n\nWheat Varieties:\n\nWheat varieties bred and introduced to UK agriculture include White Monarch in 1899, White Pearl (1900), Red King (1900), New Era (1903), Reliance (1909), Victor (1910), Benefactor (1914), Early Cone (1918), The Hawk (1918), Marshal Foch (1919), Rector (1923), Benefactress (1925), Renown (1926), Wilhelmina Regenerated (1928), Gartons No 60 (1932), Gartons Q3 (1933), Redman (1934), Little Tich (1935), Wilma (1936), Warden (1938), Meteor (1941), Pilot (1945), Welcome (1950), Masterpiece (1951), Alpha (1952), Victor II (1953), Ritchie (1957), Apex (1965).\n\nSwede Varieties:\n\nSwede varieties bred and introduced to UK agriculture include Zero in 1900, Lord Derby (1900), Perfection (1900), Monarch (1900), Model (1900), Green Tankard (1901), Keepwell (1902), Cropwell (1903), Superlative (1905), Victory (1907), Incomparable (1907), Warrington (1914), Acme (1914), Magnificent (1917), Viking (1918), Feedwell (1922), White Fleshed (1933), Parkside (1951), Townhead (1951).\n\nTurnip Varieties:\n\nTurnip varieties bred and introduced to UK agriculture include Mammoth Purpletop in 1900, Greentop Scotch Yellow (1900), Hardy Green Globe (1900), Pioneer (1903), Purpletop Long Keeping (1912), Deep Golden Yellow Long Keeping (1912), The Bruce (1917), The Grampian (1920), The Wallace (1935).\n\nSugar Beet Varieties:\n\nSugar Beet varieties bred and introduced to UK agriculture include Gartons in 1909, Gartons C (1941) and Gartons Number 632 (1962).\n\nKale and Kail Varieties:\n\nKale varieties bred and introduced to UK agriculture include Thousand Headed in 1902, Marrow Stem Kail (1912), Gartons Hybrid (1937) and Hungry Gap (1941).\n\nKohl Rabi Varieties:\n\nKohl Rabi varieties bred and introduced to UK agriculture include Large Green in 1902 and Improved Short Top in 1904.\n\nMangel Varieties:\n\nMangel varieties bred and introduced to UK agriculture include Large Yellow Intermediate in 1900, Mammoth Long Red (1900), Golden Tankard (1900), Large Yellow Globe (1900), Select Golden Globe (1900), Sugar (1905), Red Intermediate (1905), Devon Yellow Intermediate (1907), Golden Gatepost (1909), Large Red Globe (1910), Large Golden Globe (1910), Nonsuch (1917), Sunrise (1919), White Knight (1922), New Combination (1924), Lemon Globe (1927), Gartons Number 432 (1928), Gartons Number 47 (1931), White Chief (1935), Gartons Number 601 (1960).\n\nRape Varieties:\n\nRape varieties bred and introduced to UK agriculture include Broadleaved in 1906, Early Giant (1947) and Late Dwarf (1947).\n\nHerbage Grass Varieties:\n\nHerbage grass varieties bred and introduced to UK agriculture include Hatchmere Perennial Ryegrass in 1899, Ellesmere Perennialized Italian Ryegrass (1907), Pickmere Perennial Ryegrass (1932), Delamere Cocksfoot (1936), Oakmere Timothy (1940), Flaxmere (1952), Gartons Tall Fescue (1955), Marbury Meadow Fescue (1957), Barmere Timothy (1958).\n\nClover Varieties:\n\nClover varieties bred and introduced to UK agriculture include Giant Cowgrass in 1898, Perennial Cowgrass (1898), Perennialized Broad Red Clover (1898), Gartons White Clover (1898) and Broad Red Clover (1907).\n\nField Cabbage Varieties:\n\nField Cabbage varieties bred and introduced to UK agriculture include Early Ox Heart in 1900, Extra Early Express (1900), Early Drumhead (1900), Selected Drumhead Savoy (1902), Selected Ormskirk Savoy (1902), Gartons Cattle Drumhead (1904), Giant Purple Flat Poll (1917), Utility (1924), Intermediate Drumhead (1924), Gartons Primo (1939).\n\nField Carrot Varieties:\n\nField Carrot varieties bred and introduced to UK agriculture include Scarlet Intermediate in 1900, Mid Season Scarlet (1911), Mammoth White (1924), Intermediate Stump Rooted (1935), Red Cored Early Market (1935), Short Stump Rooted (1938), Giant White (1939).\n\nLupin, Parsnip, Potato, Sprouting Broccoli, Winter Beans and Winter Rye Varieties:\n\nOther crop varieties bred and introduced to UK agriculture include Gartons Lupin in 1922, Gartons Field Parsnips (1902), Gartons Number 12 Potato (1912), Gartons Purple Sprouting Broccoli (1903), Gartons Giant Winter Bean (1922), GS Giant Winter Bean (1950), P/L 14 Giant Winter Bean (1954), Gartons Giant Large Grained Winter Rye (1922).\n", "id": "15668437", "title": "Gartons Agricultural Plant Breeders"}
{"url": "https://en.wikipedia.org/wiki?curid=16908428", "text": "Post-transcriptional regulation\n\nPost-transcriptional regulation is the control of gene expression at the RNA level, therefore between the transcription and the translation of the gene. It contributes substantially to gene expression regulation across human tissues.\n\nAfter being produced, the stability and distribution of the different transcripts is regulated (post-transcriptional regulation) by means of RNA binding protein (RBP) that control the various steps and rates cripts: events such as alternative splicing, nuclear degradation (exosome), processing, nuclear export (three alternative pathways), sequestration in P-bodies for storage or degradation and ultimately translation. These proteins achieve these events thanks to a RNA recognition motif (RRM) that binds a specific sequence or secondary structure of the transcripts, typically at the 5’ and 3’ UTR of the transcript.\n\nModulating the capping, splicing, addition of a Poly(A) tail, the sequence-specific nuclear export rates and in several contexts sequestration of the RNA transcript occurs in eukaryotes but not in prokaryotes. This modulation is a result of a protein or transcript which in turn is regulated and may have an affinity for certain sequences.\n\n\nMicroRNAs (miRNAs) appear to regulate the expression of more than 60% of protein coding genes of the human genome. If an miRNA is abundant it can behave as a \"switch\", turning some genes on or off. However, altered expression of many miRNAs only leads to a modest 1.5- to 4-fold change in protein expression of their target genes. Individual miRNAs often repress several hundred target genes. Repression usually occurs either through translational silencing of the mRNA or through degradation of the mRNA, via complementary binding, mostly to specific sequences in the 3' untranslated region of the target gene's mRNA. The mechanism of translational silencing or degradation of mRNA is implemented through the RNA-induced silencing complex (RISC).\n\nIn metazoans and bacteria, many genes involved in post-post transcriptional regulation are regulated post transcriptionally. For Drosophila RBPs associated with splicing or nonsense mediated decay, analyses of protein-protein and protein-RNA interaction profiles have revealed ubiquitous interactions with RNA and protein products of the same gene. It remains unclear whether these observations are driven by ribosome proximal or ribosome mediated contacts, or if some protein complexes, particularly RNPs, undergo co-translational assembly.\n\nThis area of study has recently gained more importance due to the increasing evidence that post-transcriptional regulation plays a larger role than previously expected. Even though protein with DNA binding domains are more abundant than protein with RNA binding domains, a recent study by Cheadle et al. (2005) showed that during T-cell activation 55% of significant changes at the steady-state level had no corresponding changes at the transcriptional level, meaning they were a result of stability regulation alone.\n\nFurthermore, RNA found in the nucleus is more complex than that found in the cytoplasm: more than 95% (bases) of the RNA synthesized by RNA polymerase II never reaches the cytoplasm. The main reason for this is due to the removal of introns which account for 80% of the total bases. Some studies have shown that even after processing the levels of mRNA between the cytoplasm and the nucleus differ greatly.\n\nDevelopmental biology is a good source of models of regulation, but due to the technical difficulties it was easier to determine the transcription factor cascades than regulation at the RNA level. In fact several key genes such as nanos are known to bind RNA but often their targets are unknown. Although RNA binding proteins may regulate post transcriptionally large amount of the transcriptome, the targeting of a single gene is of interest to the scientific community for medical reasons, this is RNA interference and microRNAs which are both examples of posttranscriptional regulation, which regulate the destruction of RNA and change the chromatin structure. To study post-transcriptional regulation several techniques are used, such as RIP-Chip (RNA immunoprecipitation on chip).\n\nDeficiency of expression of a DNA repair gene occurs in many cancers (see DNA repair defect and cancer risk and microRNA and DNA repair). Altered microRNA (miRNA) expression that either decreases accurate DNA repair or increases inaccurate microhomology-mediated end joining (MMEJ) DNA repair is often observed in cancers. Deficiency of accurate DNA repair may be a major source of the high frequency of mutations in cancer (see mutation frequencies in cancers). Repression of DNA repair genes in cancers by changes in the levels of microRNAs may be a more frequent cause of repression than mutation or epigenetic methylation of DNA repair genes.\n\nFor instance, BRCA1 is employed in the accurate homologous recombinational repair (HR) pathway. Deficiency of BRCA1 can cause breast cancer. Down-regulation of BRCA1 due to mutation occurs in about 3% of breast cancers. Down-regulation of BRCA1 due to methylation of its promoter occurs in about 14% of breast cancers. However, increased expression of miR-182 down-regulates BRCA1 mRNA and protein expression, and increased miR-182 is found in 80% of breast cancers.\n\nIn another example, a mutated constitutively (persistently) expressed version of the oncogene c-Myc is found in many cancers. Among many functions, c-Myc negatively regulates microRNAs miR-150 and miR-22. These microRNAs normally repress expression of two genes essential for MMEJ, Lig3 and Parp1, thereby inhibiting this inaccurate, mutagenic DNA repair pathway. Muvarak et al. showed, in leukemias, that constitutive expression of c-Myc, leading to down-regulation of miR-150 and miR-22, allowed increased expression of Lig3 and Parp1. This generates genomic instability through increased inaccurate MMEJ DNA repair, and likely contributes to progression to leukemia.\n\nTo show the frequent ability of microRNAs to alter DNA repair expression, Hatano et al. performed a large screening study, in which 810 microRNAs were transfected into cells that were then subjected to ionizing radiation (IR). For 324 of these microRNAs, DNA repair was reduced (cells were killed more efficiently by IR) after transfection. For a further 75 microRNAs, DNA repair was increased, with less cell death after IR. This indicates that alterations in microRNAs may often down-regulate DNA repair, a likely important early step in progression to cancer.\n\n\n", "id": "16908428", "title": "Post-transcriptional regulation"}
{"url": "https://en.wikipedia.org/wiki?curid=17060990", "text": "Nucleic acid methods\n\nNucleic acid methods are the techniques used to study nucleic acids: DNA and RNA.\n\n\n\n\n\n\n\n\n", "id": "17060990", "title": "Nucleic acid methods"}
{"url": "https://en.wikipedia.org/wiki?curid=15846772", "text": "Nucleofection\n\nNucleofection is an electroporation-based transfection method which enables transfer of nucleic acids such as DNA and RNA into cells by applying a specific voltage and reagents. Nucleofection, also referred to as nucleofector technology, was invented by the biotechnology company Amaxa. \"Nucleofector\" and \"nucleofection\" are trademarks owned by Lonza Cologne AG, part of the Lonza Group.\n\nNucleofection is a method to transfer substrates into mammalian cells so far considered difficult or even impossible to transfect. Examples for such substrates are nucleic acids, like the DNA of an isolated gene cloned into a plasmid, or small interfering RNA (siRNA) for knocking down expression of a specific endogenous gene.\n\nPrimary cells, for example stem cells, especially fall into this category, although many other cell lines are also difficult to transfect. Primary cells are freshly isolated from body tissue and thus cells are unchanged, closely resembling the in-vivo situation, and are therefore of particular relevance for medical research purposes. In contrast, cell lines have often been cultured for decades and may significantly differ from their origin.\n\nBased on the physical method of electroporation, nucleofection uses a combination of electrical parameters, generated by a device called Nucleofector, with cell-type specific reagents. The substrate is transferred directly into the cell nucleus and the cytoplasm. In contrast, other commonly used non-viral transfection methods rely on cell division for the transfer of DNA into the nucleus. Thus, nucleofection provides the ability to transfect even non-dividing cells, such as neuron and resting blood cells. Before the introduction of the Nucleofector Technology, efficient gene transfer into primary cells had been restricted to the use of viral vectors, which typically involve disadvantages such as safety risks, lack of reliability, and high cost. The non-viral gene transfer methods available were not suitable for the efficient transfection of primary cells. Non-viral delivery methods may require cell division for completion of transfection, since the DNA enters the nucleus during breakdown of the nuclear envelope upon cell division or by a specific localization sequence.\nOptimal nucleofection conditions depend upon the individual cell type, not on the substrate being transfected. This means that identical conditions are used for the nucleofection of DNA, RNA, siRNAs, shRNAs, mRNAs and pre-mRNAs, BACs, peptides, morpholinos, PNA, or other biologically active molecules.\n\n\n", "id": "15846772", "title": "Nucleofection"}
{"url": "https://en.wikipedia.org/wiki?curid=17314163", "text": "Spin column-based nucleic acid purification\n\nSpin column-based nucleic acid purification is a solid phase extraction method to quickly purify nucleic acids. This method relies on the fact that nucleic acid will bind to the solid phase of silica under certain conditions.\n\nThe stages of the method are lyse, bind, wash and elute, i.e. lysis of cells, binding of nucleic acid to silica gel membrane, washing the nucleic acid bound to the silica gel membrane and elution of the nucleic acid. \n\nTo lyse, the cells of a sample are broken open with a lysis procedure which free breaks the cell membrane and the nucleus to release the nucleic acid. \n\nFor binding, a buffer solution is then added to the sample along with ethanol or isopropanol. This forms the binding solution. The binding solution is transferred to a spin column and the column is put in a centrifuge. The centrifuge forces the binding solution through a silica gel membrane that is inside the spin column. If the pH and salt concentration of the binding solution are optimal, the nucleic acid will bind to the silica gel membrane as the solution passes through.\n\nTo wash, the flow-through is removed and a wash buffer is added to the column. The column is put in a centrifuge again, forcing the wash buffer through the membrane. This removes any remaining impurities from the membrane, leaving only the nucleic acid bound to the silica gel. \n\nTo elute, the wash buffer is removed and an elution buffer (or simply water) is added to the column. The column is put in a centrifuge again, forcing the elution buffer through the membrane. The elution buffer removes the nucleic acid from the membrane and the nucleic acid is collected from the bottom of the column.\n\nEven prior to the nucleic acid methods employed today it was known that in the presence of chaotropic agents, such as sodium iodide or sodium perchlorate, DNA binds to silica, glass particles or to unicellular algae called diatoms which shield their cell walls with silica. This property was used to purify nucleic acid using glass powder or silica beads under alkaline conditions. This was later improved used guanidinium thiocyanate or guanidinium hydrochloride as the chaotropic agent. The use of glass beads was later changed to silica gel.\n\n", "id": "17314163", "title": "Spin column-based nucleic acid purification"}
{"url": "https://en.wikipedia.org/wiki?curid=7564872", "text": "HindIII\n\n\"Hin\"dIII (pronounced \"Hin D Three\") is a type II site-specific deoxyribonuclease restriction enzyme isolated from \"Haemophilus influenzae\" that cleaves the DNA palindromic sequence AAGCTT in the presence of the cofactor Mg via hydrolysis.\n\nThe cleavage of this sequence between the AA's results in 5' overhangs on the DNA called sticky ends:\n\n5'-A |A G C T T-3'\n\n3'-T T C G A| A-5'\nRestriction endonucleases are used as defense mechanisms in prokaryotic organisms in the restriction modification system. Their primary function is to protect the host genome against invasion by foreign DNA, primarily bacteriophage DNA. There is also evidence that suggests the restriction enzymes may act alongside modification enzymes as selfish elements, or may be involved in genetic recombination and transposition.\n\nThe structure of HindIII is complex, and consists of a homodimer. Like other type II restriction endonucleases, it is believed to contain a common structural core comprising four β-sheets and a single α-helix. Each subunit contains 300 amino acids and the predicted molecular mass is 34,950 Da. Despite the importance of this enzyme in molecular biology and DNA technology, little information is available concerning the mechanism of DNA recognition and phosphodiester bond cleavage. However, it is believed that HindIII utilizes a common mechanism of recognition and catalysis of DNA found in other type II enzymes such as \"Eco\"RI, \"Bam\"HI, and \"Bgl\"II. These enzymes contain the amino acid sequence motif PD-(D/E)XK to coordinate Mg, a cation required to cleave DNA in most type II restriction endonucleases. The cofactor Mg is believed to bind water molecules and carry them to the catalytic sites of the enzymes, among other cations. Unlike most documented type II restriction endonucleases, HindIII is unique in that it has little to no catalytic activity when Mg is substituted for other cofactors, such as Mn.\n\nDespite the uncertainty concerning the structure-catalysis relationship of type II endonucleases, site-directed mutagenesis of the restriction endonuclease HindIII has provided much insight into the key amino acid residues involved. In particular, substitutions of Asn for Lys at residue 125 and Leu for Asp at residue 108 significantly decreased DNA binding and the catalytic function of \"Hin\"dIII. In a separate mutagenesis study it was shown that a mutation at residue 123 from Asp to Asn reduced enzymatic activity. Despite the fact that this residue is most likely responsible for the unwinding of DNA and coordination to water rather than direct interaction with the attacking nucleophile, its specific function is unknown.\n\nWhile restriction enzymes cleave at specific DNA sequences, they are first required to bind non-specifically with the DNA backbone before localizing to the restriction site. On average, the restriction enzyme will form 15-20 hydrogen bonds with the bases of the recognition sequence. With the aid of other van der Waals interactions, this bonding facilitates a conformational change of the DNA-enzyme complex which leads to the activation of catalytic centers.\n\nDespite the lack of evidence suggesting an exact mechanism for the cleavage of DNA by HindIII, site-mutagenesis analysis coupled with more detailed studies of metal ion-mediated catalysis in \"Eco\"RV have led to the following proposed catalytic mechanism. It has been suggested that during the hydrolysis of DNA by EcoRV the catalytic residue Lys-92 stabilizes and orients the attacking water nucleophile, while the carboxylate of Asp-90 stabilizes the leaving hydroxide anion through to coordination of Mg. Furthermore, enzymatic function is dependent upon the correct position of the Asp-74 residue, suggesting has a role in increasing the nucleophilicity of the attacking water molecule.\n\nAs a result of the site-mutagenesis experiments previously outlined, it is thus proposed that Lys-125, Asp-123, and Asp-108 of HindIII function similarly to Lys-92, Asp-90, and Asp-74 in EcoRV, respectively. Lys-125 positions the attacking water molecule while Asp-108 improves its nucleophilicity. Asp-123 coordinates to Mg2+ which in turn stabilizes the leaving hydroxide ion.\n\nHindIII as well as other type II restriction endonucleases are very useful in modern science, particularly in DNA sequencing and mapping. Unlike type I restriction enzymes, type II restriction endonucleases perform very specific cleaving of DNA. Type I restriction enzymes recognize specific sequences, but cleave DNA randomly at sites other than their recognition site whereas type II restriction enzymes cleave only at their specific recognition site. Since their discovery in the early 1970s, type II restriction enzymes have revolutionized the way scientists work with DNA, particularly in genetic engineering and molecular biology.\n\nMajor uses of type II restriction enzymes include gene analysis and cloning. They have proven to be ideal modeling systems for the study of protein-nucleic acid interactions, structure-function relationships, and the mechanism of evolution. They make good assays for the study of genetic mutations by their ability to specifically cleave DNA to allow the removal or insertion of DNA. Through the use of restriction enzymes, scientists are able to modify, insert, or remove specific genes, a very powerful tool especially when it comes to modifying an organism's genome.\n", "id": "7564872", "title": "HindIII"}
{"url": "https://en.wikipedia.org/wiki?curid=2015367", "text": "Two-hybrid screening\n\nTwo-hybrid screening (originally known as yeast two-hybrid system or Y2H) is a molecular biology technique used to discover protein–protein interactions (PPIs) and protein–DNA interactions by testing for physical interactions (such as binding) between two proteins or a single protein and a DNA molecule, respectively.\n\nThe premise behind the test is the activation of downstream reporter gene(s) by the binding of a transcription factor onto an upstream activating sequence (UAS). For two-hybrid screening, the transcription factor is split into two separate fragments, called the DNA-binding domain (DBD or often also abbreviated as BD) and activating domain (AD). The BD is the domain responsible for binding to the UAS and the AD is the domain responsible for the activation of transcription. The Y2H is thus a protein-fragment complementation assay.\n\nPioneered by Stanley Fields and Ok-Kyu Song in 1989, the technique was originally designed to detect protein–protein interactions using the Gal4 transcriptional activator of the yeast \"Saccharomyces cerevisiae\". The Gal4 protein activated transcription of a protein involved in galactose utilization, which formed the basis of selection. Since then, the same principle has been adapted to describe many alternative methods, including some that detect protein–DNA interactions or DNA-DNA interactions, as well as methods that use different host organisms such as \"Escherichia coli\" or mammalian cells instead of yeast.\n\nThe key to the two-hybrid screen is that in most eukaryotic transcription factors, the activating and binding domains are modular and can function in proximity to each other without direct binding. This means that even though the transcription factor is split into two fragments, it can still activate transcription when the two fragments are indirectly connected.\n\nThe most common screening approach is the yeast two-hybrid assay. In this approach the researcher knows where each prey is located on the used medium (agar plates). Millions of potential interactions in several organisms have been screened in the latest decade using high-throughput screening systems (often using robots) and over thousands of interactions have been detected and categorized in databases as BioGRID. This system often utilizes a genetically engineered strain of yeast in which the biosynthesis of certain nutrients (usually amino acids or nucleic acids) is lacking. When grown on media that lacks these nutrients, the yeast fail to survive. This mutant yeast strain can be made to incorporate foreign DNA in the form of plasmids. In yeast two-hybrid screening, separate bait and prey plasmids are simultaneously introduced into the mutant yeast strain or a mating strategy is used to get both plasmids in one host cell.\n\nThe second high-throughput approach is the library screening approach. In this set up the bait and prey harboring cells are mated in a random order. After mating and selecting surviving cells on selective medium the scientist will sequence the isolated plasmids to see which prey (DNA sequence) is interacting with the used bait. This approach has a lower rate of reproducibility and tends to yield higher amounts of false positives compared to the matrix approach.\n\nPlasmids are engineered to produce a protein product in which the DNA-binding domain (BD) fragment is fused onto a protein while another plasmid is engineered to produce a protein product in which the activation domain (AD) fragment is fused onto another protein. The protein fused to the BD may be referred to as the bait protein, and is typically a known protein the investigator is using to identify new binding partners. The protein fused to the AD may be referred to as the prey protein and can be either a single known protein or a library of known or unknown proteins. In this context, a library may consist of a collection of protein-encoding sequences that represent all the proteins expressed in a particular organism or tissue, or may be generated by synthesising random DNA sequences. Regardless of the source, they are subsequently incorporated into the protein-encoding sequence of a plasmid, which is then transfected into the cells chosen for the screening method. This technique, when using a library, assumes that each cell is transfected with no more than a single plasmid and that, therefore, each cell ultimately expresses no more than a single member from the protein library.\n\nIf the bait and prey proteins interact (i.e., bind), then the AD and BD of the transcription factor are indirectly connected, bringing the AD in proximity to the transcription start site and transcription of reporter gene(s) can occur. If the two proteins do not interact, there is no transcription of the reporter gene. In this way, a successful interaction between the fused protein is linked to a change in the cell phenotype.\n\nThe challenge of separating cells that express proteins that happen to interact with their counterpart fusion proteins from those that do not, is addressed in the following section.\n\nIn any study, some of the protein domains, those under investigation, will be varied according to the goals of the study whereas other domains, those that are not themselves being investigated, will be kept constant. For example, in a two-hybrid study to select DNA-binding domains, the DNA-binding domain, BD, will be varied while the two interacting proteins, the bait and prey, must be kept constant to maintain a strong binding between the BD and AD. There are a number of domains from which to choose the BD, bait and prey and AD, if these are to remain constant. In protein–protein interaction investigations, the BD may be chosen from any of many strong DNA-binding domains such as Zif268. A frequent choice of bait and prey domains are residues 263–352 of yeast Gal11P with a N342V mutation and residues 58–97 of yeast Gal4, respectively. These domains can be used in both yeast- and bacterial-based selection techniques and are known to bind together strongly.\n\nThe AD chosen must be able to activate transcription of the reporter gene, using the cell's own transcription machinery. Thus, the variety of ADs available for use in yeast-based techniques may not be suited to use in their bacterial-based analogues. The herpes simplex virus-derived AD, VP16 and yeast Gal4 AD have been used with success in yeast whilst a portion of the α-subunit of \"E. coli\" RNA polymerase has been utilised in \"E. coli\"-based methods.\n\nWhilst powerfully activating domains may allow greater sensitivity towards weaker interactions, conversely, a weaker AD may provide greater stringency.\n\nA number of engineered genetic sequences must be incorporated into the host cell to perform two-hybrid analysis or one of its derivative techniques. The considerations and methods used in the construction and delivery of these sequences differ according to the needs of the assay and the organism chosen as the experimental background.\n\nThere are two broad categories of hybrid library: random libraries and cDNA-based libraries. A cDNA library is constituted by the cDNA produced through reverse transcription of mRNA collected from specific cells of types of cell. This library can be ligated into a construct so that it is attached to the BD or AD being used in the assay. A random library uses lengths of DNA of random sequence in place of these cDNA sections. A number of methods exist for the production of these random sequences, including cassette mutagenesis. Regardless of the source of the DNA library, it is ligated into the appropriate place in the relevant plasmid/phagemid using the appropriate restriction endonucleases.\n\nBy placing the hybrid proteins under the control of IPTG-inducible \"lac\" promoters, they are expressed only on media supplemented with IPTG. Further, by including different antibiotic resistance genes in each genetic construct, the growth of non-transformed cells is easily prevented through culture on media containing the corresponding antibiotics. This is particularly important for counter selection methods in which a \"lack\" of interaction is needed for cell survival.\n\nThe reporter gene may be inserted into the \"E. coli\" genome by first inserting it into an episome, a type of plasmid with the ability to incorporate itself into the bacterial cell genome with a copy number of approximately one per cell.\n\nThe hybrid expression phagemids can be electroporated into \"E. coli\" XL-1 Blue cells which after amplification and infection with VCS-M13 helper phage, will yield a stock of library phage. These phage will each contain one single-stranded member of the phagemid library.\n\nOnce the selection has been performed, the primary structure of the proteins which display the appropriate characteristics must be determined. This is achieved by retrieval of the protein-encoding sequences (as originally inserted) from the cells showing the appropriate phenotype.\n\nThe phagemid used to transform \"E. coli\" cells may be \"rescued\" from the selected cells by infecting them with VCS-M13 helper phage. The resulting phage particles that are produced contain the single-stranded phagemids and are used to infect XL-1 Blue cells. The double-stranded phagemids are subsequently collected from these XL-1 Blue cells, essentially reversing the process used to produce the original library phage. Finally, the DNA sequences are determined through dideoxy sequencing.\n\nThe \"Escherichia coli\"-derived Tet-R repressor can be used in line with a conventional reporter gene and can be controlled by tetracycline or doxicycline (Tet-R inhibitors). Thus the expression of Tet-R is controlled by the standard two-hybrid system but the Tet-R in turn controls (represses) the expression of a previously mentioned reporter such as \"HIS3\", through its Tet-R promoter. Tetracycline or its derivatives can then be used to regulate the sensitivity of a system utilising Tet-R.\n\nSensitivity may also be controlled by varying the dependency of the cells on their reporter genes. For example, this may be affected by altering the concentration of histidine in the growth medium for \"his3\"-dependent cells and altering the concentration of streptomycin for \"aadA\" dependent cells. Selection-gene-dependency may also be controlled by applying an inhibitor of the selection gene at a suitable concentration. 3-Amino-1,2,4-triazole (3-AT) for example, is a competitive inhibitor of the \"HIS3\"-gene product and may be used to titrate the minimum level of \"HIS3\" expression required for growth on histidine-deficient media.\n\nSensitivity may also be modulated by varying the number of operator sequences in the reporter DNA.\n\nA third, non-fusion protein may be co-expressed with two fusion proteins. Depending on the investigation, the third protein may modify one of the fusion proteins or mediate or interfere with their interaction.\n\nCo-expression of the third protein may be necessary for modification or activation of one or both of the fusion proteins. For example, \"S. cerevisiae\" possesses no endogenous tyrosine kinase. If an investigation involves a protein that requires tyrosine phosphorylation, the kinase must be supplied in the form of a tyrosine kinase gene.\n\nThe non-fusion protein may mediate the interaction by binding both fusion proteins simultaneously, as in the case of ligand-dependent receptor dimerization.\n\nFor a protein with an interacting partner, its functional homology to other proteins may be assessed by supplying the third protein in non-fusion form, which then may or may not compete with the fusion-protein for its binding partner. Binding between the third protein and the other fusion protein will interrupt the formation of the reporter expression activation complex and thus reduce reporter expression, leading to the distinguishing change in phenotype.\n\nOne limitation of classic yeast two-hybrid screens is that they are limited to soluble proteins. It is therefore impossible to use them to study the protein–protein interactions between insoluble integral membrane proteins. The split-ubiquitin system provides a method for overcoming this limitation. In the split-ubiquitin system, two integral membrane proteins to be studied are fused to two different ubiquitin moieties: a C-terminal ubiquitin moiety (\"Cub\", residues 35–76) and an N-terminal ubiquitin moiety (\"Nub\", residues 1–34). These fused proteins are called the bait and prey, respectively. In addition to being fused to an integral membrane protein, the Cub moiety is also fused to a transcription factor (TF) that can be cleaved off by ubiquitin specific proteases. Upon bait–prey interaction, Nub and Cub-moieties assemble, reconstituting the split-ubiquitin. The reconstituted split-ubiquitin molecule is recognized by ubiquitin specific proteases, which cleave off the transcription factor, allowing it to induce the transcription of reporter genes.\n\nZolghadr and co-workers presented a fluorescent two-hybrid system that uses two hybrid proteins that are fused to different fluorescent proteins as well as LacI, the lac repressor. The structure of the fusion proteins looks like this: FP2-LacI-bait and FP1-prey where the bait and prey proteins interact and bring the fluorescent proteins (FP1 = GFP, FP2=mCherry) in close proximity at the binding site of the LacI protein in the host cell genome. The system can also be used to screen for inhibitors of protein–protein interactions.\n\nWhile the original Y2H system used a reconstituted transcription factor, other systems create enzymatic activities to detect PPIs. For instance, the KInase Substrate Sensor (\"KISS\"), is a mammalian two-hybrid approach has been designed to map intracellular PPIs. Here, a bait protein is fused to a kinase-containing portion of TYK2 and a prey is coupled to a gp130 cytokine receptor fragment. When bait and prey interact, TYK2 phosphorylates STAT3 docking sites on the prey chimera, which ultimately leads to activation of a reporter gene.\n\nThe one-hybrid variation of this technique is designed to investigate protein–DNA interactions and uses a single fusion protein in which the AD is linked directly to the binding domain. The binding domain in this case however is not necessarily of fixed sequence as in two-hybrid protein–protein analysis but may be constituted by a library. This library can be selected against the desired target sequence, which is inserted in the promoter region of the reporter gene construct. In a positive-selection system, a binding domain that successfully binds the UAS and allows transcription is thus selected.\n\nNote that selection of DNA-binding domains is not necessarily performed using a one-hybrid system, but may also be performed using a two-hybrid system in which the binding domain is varied and the bait and prey proteins are kept constant.\n\nRNA-protein interactions have been investigated through a three-hybrid variation of the two-hybrid technique. In this case, a hybrid RNA molecule serves to adjoin together the two protein fusion domains—which are not intended to interact with each other but rather the intermediary RNA molecule (through their RNA-binding domains). Techniques involving non-fusion proteins that perform a similar function, as described in the 'non-fusion proteins' section above, may also be referred to as three-hybrid methods.\n\nSimultaneous use of the one- and two-hybrid methods (that is, simultaneous protein–protein and protein–DNA interaction) is known as a one-two-hybrid approach and expected to increase the stringency of the screen.\n\nAlthough theoretically, any living cell might be used as the background to a two-hybrid analysis, there are practical considerations that dictate which is chosen. The chosen cell line should be relatively cheap and easy to culture and sufficiently robust to withstand application of the investigative methods and reagents. The latter is especially important for doing high-throughput studies. Therefore the yeast \"S. cerevisiae\" has been the main host organism for two-hybrid studies. However it is not always the ideal system to study interacting proteins from other organisms. Yeast cells often do not have the same post translational modifications, have a different codon use or lack certain proteins that are important for the correct expression of the proteins. To cope with these problems several novel two-hybrid systems have been developed. Depending on the system used agar plates or specific growth medium is used to grow the cells and allow selection for interaction. The most common used method is the agar plating one where cells are plated on selective medium to see of interaction takes place. Cells that have no interaction proteins should not survive on this selective medium.\n\nThe yeast \"S. cerevisiae\" was the model organism used during the two-hybrid technique's inception. It is commonly known as the Y2H system. It has several characteristics that make it a robust organism to host the interaction, including the ability to form tertiary protein structures, neutral internal pH, enhanced ability to form disulfide bonds and reduced-state glutathione among other cytosolic buffer factors, to maintain a hospitable internal environment. The yeast model can be manipulated through non-molecular techniques and its complete genome sequence is known. Yeast systems are tolerant of diverse culture conditions and harsh chemicals that could not be applied to mammalian tissue cultures.\n\nA number of yeast strains have been created specifically for Y2H screens, e.g. Y187 and AH109, both produced by Clontech. Yeast strains R2HMet and BK100 have also been used.\n\n\"C. albicans\" is a yeast with a particular feature: it translates the CUG codon into serine rather than leucine. Due to this different codon usage it is difficult to use the model system \"S. cerevisiae\" as a Y2H to check for protein-protein interactions using \"C. albicans\" genes. To provide a more native environment a \"C. albicans\" two-hybrid (C2H) system was developed. With this system protein-protein interactions can be studied in \"C. albicans\" itself.\n\nBacterial \"E. coli\"-based two hybrid methods (abbreviated as B2H) have several characteristics that may make them preferable to yeast-based homologues. The higher transformation efficiency and faster rate of growth lends \"E. coli\" to the use of larger libraries (in excess of 10). A low false positive rate of approximately 3x10, the absence of requirement for a nuclear localisation signal to be included in the protein sequence and the ability to study proteins that would be toxic to yeast may also be major factors to consider when choosing an experimental background organism.\n\nIt may be of note that the methylation activity of certain \"E. coli\" DNA methyltransferase proteins may interfere with some DNA-binding protein selections. If this is anticipated, the use of an \"E. coli\" strain that is defective for a particular methyltransferase may be an obvious solution. Important to mention is that bacteria are prokaryotic organisms and when studying eukaryotic protein-protein interactions (e.g. human proteins) the results need to be carefully approached.\n\nIn recent years a mammalian two hybrid (M2H) system has been designed to study mammalian protein-protein interactions in a cellular environment that closely mimics the native protein environment Transiently transfected mammalian cells are used in this system to find protein-protein interactions.\nUsing a mammalian cell line to study mammalian protein-protein interactions gives the advantage of working in a more native context.\nThe post-translational modifications, phosphorylation, acylation and glycosylation are similar. The intracellular localization of the proteins is also more correct compared to using a yeast two hybrid system.\n\nIt is also possible with the mammalian two-hybrid system to study signal inputs.\nAnother big advantage is that results can be obtained within 48 hours after transfection.\n\nIn 2005 a two hybrid system in plants was developed. Using protoplasts of \"A. thaliana\" protein-protein interactions can be studied in plants. This way the interactions can be studied in their native context. In this system the GAL4 AD and BD are under the control of the strong 35S promoter. Interaction is measured using a GUS reporter. In order to enable a high-throughput screening the vectors were made gateway compatible.\nThe system is known as the protoplast two hybrid (P2H) system.\n\nThe sea hare \"A californica\" is a model organism in neurobiology to study among others the molecular mechanisms of long-term memory. To study interactions, important in neurology, in a more native environment a two-hybrid system has been developed in \"A californica\" neurons. A GAL4 AD and BD are used in this system.\n\nAn insect two-hybrid (I2H) system was developed in a silkworm cell line from the larva or caterpillar of the domesticated silk moth, \"Bombyx mori\" (BmN4 cells). This system uses the GAL4 BD and the activation domain of mouse NF-κB P65. Both are under the control of the OpIE2 promoter.\n\nBy changing specific amino acids by mutating the corresponding DNA base-pairs in the plasmids used, the importance of those amino acid residues in maintaining the interaction can be determined.\n\nAfter using bacterial cell-based method to select DNA-binding proteins, it is necessary to check the specificity of these domains as there is a limit to the extent to which the bacterial cell genome can act as a sink for domains with an affinity for other sequences (or indeed, a general affinity for DNA).\n\nProtein–protein signalling interactions pose suitable therapeutic targets due to their specificity and pervasiveness. The random drug discovery approach uses compound banks that comprise random chemical structures, and requires a high-throughput method to test these structures in their intended target.\n\nThe cell chosen for the investigation can be specifically engineered to mirror the molecular aspect that the investigator intends to study and then used to identify new human or animal therapeutics or anti-pest agents.\n\nBy determination of the interaction partners of unknown proteins, the possible functions of these new proteins may be inferred. This can be done using a single known protein against a library of unknown proteins or conversely, by selecting from a library of known proteins using a single protein of unknown function.\n\nTo select zinc finger proteins (ZFPs) for protein engineering, methods adapted from the two-hybrid screening technique have been used with success. A ZFP is itself a DNA-binding protein used in the construction of custom DNA-binding domains that bind to a desired DNA sequence.\n\nBy using a selection gene with the desired target sequence included in the UAS, and randomising the relevant amino acid sequences to produce a ZFP library, cells that host a DNA-ZFP interaction with the required characteristics can be selected. Each ZFP typically recognises only 3–4 base pairs, so to prevent recognition of sites outside the UAS, the randomised ZFP is engineered into a 'scaffold' consisting of another two ZFPs of constant sequence. The UAS is thus designed to include the target sequence of the constant scaffold in addition to the sequence for which a ZFP is selected.\n\nA number of other DNA-binding domains may also be investigated using this system.\n\n\nThe reason for this high error rate lies in the characteristics of the screen:\n\nEach of these points alone can give rise to false results. Due to the combined effects of all error sources yeast two-hybrid have to be interpreted with caution. The probability of generating false positives means that all interactions should be confirmed by a high confidence assay, for example co-immunoprecipitation of the endogenous proteins, which is difficult for large scale protein–protein interaction data. Alternatively, Y2H data can be verified using multiple Y2H variants or bioinformatics techniques. The latter test whether interacting proteins are expressed at the same time, share some common features (such as gene ontology annotations or certain network topologies), have homologous interactions in other species.\n\n\n", "id": "2015367", "title": "Two-hybrid screening"}
{"url": "https://en.wikipedia.org/wiki?curid=2655094", "text": "Southwestern blot\n\nSouthwestern blotting, based along the lines of Southern blotting (which was created by Edwin Southern) and first described by B. Bowen, J. Steinberg and colleagues in 1980, is a lab technique which involves identifying and characterizing DNA-binding proteins (proteins that bind to DNA) by their ability to bind to specific oligonucleotide probes. The proteins are separated by gel electrophoresis and are subsequently transferred to nitrocellulose membranes similar to other types of blotting.\n\nThe name \"southwestern blotting\" is based on the fact that this technique detects DNA-binding proteins, since DNA detection is by Southern blotting and protein detection is by western blotting.\n\nHowever, since the first southwestern blottings, many more have been proposed and discovered. The former protocols were hampered by the need for large amounts of proteins and their susceptibility to degradation while being isolated.\n\n\"Southwestern blot mapping\" is performed for rapid characterization of both DNA-binding proteins and their specific sites on genomic DNA. Proteins are separated on a polyacrylamide gel (PAGE) containing sodium dodecyl sulfate (SDS), renatured by removing SDS in the presence of urea, and blotted onto nitrocellulose by diffusion. The genomic DNA region of interest is digested by restriction enzymes selected to produce fragments of appropriate but different sizes, which are subsequently end-labeled and allowed to bind to the separated proteins. The specifically bound DNA is eluted from each individual protein-DNA complex and analyzed by polyacrylamide gel electrophoresis. Evidence that tissue-specific DNA binding proteins may be detected by this technique has been presented. Moreover, their sequence-specific binding allows the purification of the corresponding selectively bound DNA fragments and may improve protein-mediated cloning of DNA regulatory sequences.\n", "id": "2655094", "title": "Southwestern blot"}
{"url": "https://en.wikipedia.org/wiki?curid=2848926", "text": "Micrococcal nuclease\n\nMicrococcal nuclease (, \"S7 Nuclease\", \"MNase\", \"spleen endonuclease\", \"thermonuclease\", \"nuclease T\", \"micrococcal endonuclease\", \"nuclease T\"', \"staphylococcal nuclease\", \"spleen phosphodiesterase\", \"Staphylococcus aureus nuclease\", \"Staphylococcus aureus nuclease B\", \"ribonucleate (deoxynucleate) 3'-nucleotidohydrolase\") is an endo-exonuclease that preferentially digests single-stranded nucleic acids. The rate of cleavage is 30 times greater at the 5' side of A or T than at G or C and results in the production of mononucleotides and oligonucleotides with terminal 3'-phosphates. The enzyme is also active against double-stranded DNA and RNA and all sequences will be ultimately cleaved.\n\nThe enzyme has a molecular weight of 16.9kDa.\n\nThe pH optimum is reported as 9.2. The enzyme activity is strictly dependent on Ca and the pH optimum varies according to Ca concentration. The enzyme is therefore easily inactivated by EGTA.\n\nThis enzyme is the extracellular nuclease of Staphylococcus aureus. Two strains, V8 and Foggi, yield almost identical enzymes. A common source is \"E.coli\" cells carrying a cloned nuc gene encoding Staphylococcus aureus extracellular nuclease (micrococcal nuclease).\n\nThe 3-dimensional structure of micrococcal nuclease (then called Staphyloccal nuclease) was solved very early in the history of protein crystallography, in 1969, deposited as now-obsolete Protein Data Bank file 1SNS. Higher-resolution, more recent crystal structures are available for the apo form as Protein Data Bank file 1SNO: and for the thymidine-diphosphate-inhibited form as Protein Data Bank file 3H6M: or 1SNC: . As seen in the ribbon diagram above, the nuclease molecule has 3 long alpha helices and a 5-stranded, barrel-shaped beta sheet, in an arrangement known as the OB-fold (for oligonucleotide-binding fold) as classified in the SCOP database.\n\n\n\n", "id": "2848926", "title": "Micrococcal nuclease"}
{"url": "https://en.wikipedia.org/wiki?curid=17404900", "text": "UGGT\n\nUGGT, or UDP-glucose:glycoprotein glucosyltransferase, is a soluble enzyme resident in the lumen of the endoplasmic reticulum (ER) [1]. UGGT is about 170 kDa consisting of two structurally independent portions: a variable N-terminal portion of ~1200 amino acids, which in turn comprises 4 thioredoxin-like domains and two beta-sandwich domains, and senses glycoprotein misfolding; and a highly conserved C-terminal catalytic portion of ~300 amino acids, folding as a glucosyltransferase domain belonging to fold family GT24. The main function of UGGT is to recognize misfolded glycoproteins and transfer a glucose (Glc) monomer (monoglucosylate) to the terminal mannose of the A-branch of the glycan on the glycoprotein. It uses UDP-glucose (UDP-Glc) as the glucosyl donor and requires calcium ions for its activity.\n\nUGGT is part of the ER quality control system of glycoprotein folding and its activity increases the potential for correctly folded glycoproteins [2]. The main proteins involved in the ER quality control system are UGGT, the ER lectin chaperones (calnexin and calreticulin), and glucosidase II. UGGT first recognizes the incompletely folded glycoprotein and monoglucosylates it. The lectins, calnexin and calreticulin, have high affinities for monoglucosylated proteins and the ER chaperones that associate with these lectins assist the folding of the misfolded glycoprotein. Subsequently, glucosidase II will deglucosylate the glycoprotein. If the glycoprotein is still misfolded, UGGT will re-glucosylate it and allow it to go through the cycle again. \n\nCurrently, it is unclear how UGGT recognize misfolded glycoprotein. It has been proposed that UGGT may bind to exposed hydrophobic stretches, a characteristic feature of misfolded proteins. UGGT crystal structures [3] suggest marked conformational mobility, which could explain the ability of the protein to recognise a wide variety of client glycoproteins of different shapes and forms. The same conformational mobility could account for the ability of the protein to re-glucosylate N-linked glycans at different distances from the misfold site. See for example the picture in which glycoproteins are symbolized by nuts and UGGT by an adjustable wrench.\n\n[1] Parodi DAJ, Caramelo JJ, D’Alessio C (2014) UDP-Glucose: Glycoprotein Glucosyltransferase\n1,2 (UGGT1,2). Handbook of Glycosyltransferases and Related Genes (Springer Japan,\nTokyo), pp 15–30.\n\n[2] Dejgaard, S.; Nicolay, J.; Taheri, M.; Thomas, D. Y.; Bergeron, J. J. The ER glycoprotein quality control system. Curr. Issues Mol. Biol. 2004;6:29–42.\n\n[3] Roversi, P.; Marti, L.; Caputo, A.T.; Alonzi, D.S.; Hill, J.C.; Dent, K.C.; Kumar, A.; Levasseur, M.D.; Lia, A.; Waksman, T.; Basu, S.; Soto Albrecht, Y.; Qian, K.; McIvor, J.P; Lipp, C.B.; Siliqi, D.; Vasiljević, S.; Mohammed, S.; Lukacik, P.; Walsh, M. A.; Santino, A. and Zitzmann N. Interdomain conformational flexibility underpins the activity of UGGT, the eukaryotic glycoprotein secretion checkpoint\nProc Natl Acad Sci U S A. 2017 Aug 8;114(32):8544-8549. doi: 10.1073/pnas.1703682114. Epub 2017 Jul 24. \n", "id": "17404900", "title": "UGGT"}
{"url": "https://en.wikipedia.org/wiki?curid=4228914", "text": "Lydia Fairchild\n\nLydia Fairchild is an American woman who exhibits chimerism, in having two distinct populations of DNA among the cells of her body. She was pregnant with her third child when she and the father of her children, Jamie Townsend, separated. When Fairchild applied for enforcement of child support in 2002, providing DNA evidence of Townsend's paternity was a routine requirement. While the results showed Townsend to be certainly their father, they seemed to rule out her being their mother.\n\nFairchild's stood accused of fraud by either claiming benefits for other people's children, or taking part in a surrogacy scam, and records of her prior births were put similarly in doubt. Prosecutors called for her two children to be taken away from her. As time came for her to give birth to her third child, the judge ordered that an observer be present at the birth, ensure that blood samples were immediately taken from both the child and Fairchild, and be available to testify. Two weeks later, DNA tests seemed to indicate that she was also not the mother of that child.\n\nA breakthrough came when a lawyer for the prosecution knew of Karen Keegan, a chimeric woman in Boston, and suggested the similar possibility to Fairchild's lawyer, Alan Tindell, who then introduced an article in the \"New England Journal of Medicine\" about Keegan. He realised that Fairchild's case might also be caused by chimerism. As in Keegan's case, DNA samples were taken from members of the extended family. The DNA of Fairchild's children matched that of Fairchild's mother to the extent expected of a grandmother. They also found that, although the DNA in Fairchild's skin and hair did not match her children's, the DNA from a cervical smear test did match. Fairchild was carrying two different sets of DNA, the defining characteristic of chimerism.\n\n\n", "id": "4228914", "title": "Lydia Fairchild"}
{"url": "https://en.wikipedia.org/wiki?curid=18267886", "text": "Meganuclease I-SceI\n\nMeganuclease I-Sce I is a homing endonuclease. It recognises an 18-base pair sequence TAGGGATAACAGGGTAAT and leaves a 4 base pair 3' hydroxyl overhang. It is a rare cutting endonuclease. Statistically an 18-bp sequence will occur once in every 6.9*10 base pairs (a frequency of 1 in 4). This sequence does not normally occur in a human or mouse genome.\n\nI-SceI is coded by introns. It is present in the mitochondria of yeast Saccharomyces cerevisiae.\n", "id": "18267886", "title": "Meganuclease I-SceI"}
{"url": "https://en.wikipedia.org/wiki?curid=3974", "text": "Biopolymer\n\nBiopolymers are polymers produced by living organisms; in other words, they are polymeric biomolecules. Since they are polymers, biopolymers contain monomeric units that are covalently bonded to form larger structures. There are three main classes of biopolymers, classified according to the monomeric units used and the structure of the biopolymer formed: polynucleotides (RNA and DNA), which are long polymers composed of 13 or more nucleotide monomers; polypeptides, which are short polymers of amino acids; and polysaccharides, which are often linear bonded polymeric carbohydrate structures.\nOther examples of biopolymers include rubber, suberin, melanin and lignin.\n\nCellulose is the most common organic compound and biopolymer on Earth. About 33 percent of all plant matter is cellulose. The cellulose content of cotton is 90 percent, for wood it is 50 percent.\n\nA major defining difference between biopolymers and synthetic polymers can be found in their structures. All polymers are made of repetitive units called monomers. Biopolymers often have a well-defined structure, though this is not a defining characteristic (example: lignocellulose): \nThe exact chemical composition and the sequence in which these units are arranged is called the primary structure, in the case of proteins. Many biopolymers spontaneously fold into characteristic compact shapes (see also \"protein folding\" as well as secondary structure and tertiary structure), which determine their biological functions and depend in a complicated way on their primary structures. Structural biology is the study of the structural properties of the biopolymers.\nIn contrast, most synthetic polymers have much simpler and more random (or stochastic) structures. This fact leads to a molecular mass distribution that is missing in biopolymers.\nIn fact, as their synthesis is controlled by a template-directed process in most \"in vivo\" systems, all biopolymers of a type (say one specific protein) are all alike: they all contain the similar sequences and numbers of monomers and thus all have the same mass. This phenomenon is called monodispersity in contrast to the polydispersity encountered in synthetic polymers. As a result, biopolymers have a polydispersity index of 1.\n\nThe convention for a polypeptide is to list its constituent amino acid residues as they occur from the amino terminus to the carboxylic acid terminus. The amino acid residues are always joined by peptide bonds. Protein, though used colloquially to refer to any polypeptide, refers to larger or fully functional forms and can consist of several polypeptide chains as well as single chains. Proteins can also be modified to include non-peptide components, such as saccharide chains and lipids.\n\nThe convention for a nucleic acid sequence is to list the nucleotides as they occur from the 5' end to the 3' end of the polymer chain, where 5' and 3' refer to the numbering of carbons around the ribose ring which participate in forming the phosphate diester linkages of the chain. Such a sequence is called the primary structure of the biopolymer.\n\nSugar-based biopolymers are often difficult with regards to convention. Sugar polymers can be linear or branched and are typically joined with glycosidic bonds. The exact placement of the linkage can vary, and the orientation of the linking functional groups is also important, resulting in α- and β-glycosidic bonds with numbering definitive of the linking carbons' location in the ring. In addition, many saccharide units can undergo various chemical modifications, such as amination, and can even form parts of other molecules, such as glycoproteins.\n\nThere are a number of biophysical techniques for determining sequence information. Protein sequence can be determined by Edman degradation, in which the N-terminal residues are hydrolyzed from the chain one at a time, derivatized, and then identified. Mass spectrometer techniques can also be used. Nucleic acid sequence can be determined using gel electrophoresis and capillary electrophoresis. Lastly, mechanical properties of these biopolymers can often be measured using optical tweezers or atomic-force microscopy. Dual polarization interferometry can be used to measure the conformational changes or self-assembly of these materials when stimulated by pH, temperature, ionic strength or other binding partners.\n\nSome biopolymers- such as PLA, naturally occurring zein, and poly-3-hydroxybutyrate can be used as plastics, replacing the need for polystyrene or polyethylene based plastics.\n\nSome plastics are now referred to as being 'degradable', 'oxy-degradable' or 'UV-degradable'. This means that they break down when exposed to light or air, but these plastics are still primarily (as much as 98 per cent) oil-based and are not currently certified as 'biodegradable' under the European Union directive on Packaging and Packaging Waste (94/62/EC). Biopolymers will break down, and some are suitable for domestic composting.\n\nBiopolymers (also called renewable polymers) are produced from biomass for use in the packaging industry. Biomass comes from crops such as sugar beet, potatoes or wheat: when used to produce biopolymers, these are classified as non food crops. These can be converted in the following pathways:\n\nSugar beet > Glyconic acid > Polyglyconic acid\n\nStarch > (fermentation) > Lactic acid > Polylactic acid (PLA)\n\nBiomass > (fermentation) > Bioethanol > Ethene > Polyethylene\n\nMany types of packaging can be made from biopolymers: food trays, blown starch pellets for shipping fragile goods, thin films for wrapping.\n\nBiopolymers can be sustainable, carbon neutral and are always renewable, because they are made from plant materials which can be grown indefinitely. These plant materials come from agricultural non food crops. Therefore, the use of biopolymers would create a sustainable industry. In contrast, the feedstocks for polymers derived from petrochemicals will eventually deplete. In addition, biopolymers have the potential to cut carbon emissions and reduce CO quantities in the atmosphere: this is because the CO released when they degrade can be reabsorbed by crops grown to replace them: this makes them close to carbon neutral.\n\nBiopolymers are biodegradable, and some are also compostable. Some biopolymers are biodegradable: they are broken down into CO and water by microorganisms. Some of these biodegradable biopolymers are compostable: they can be put into an industrial composting process and will break down by 90% within six months. Biopolymers that do this can be marked with a 'compostable' symbol, under European Standard EN 13432 (2000). Packaging marked with this symbol can be put into industrial composting processes and will break down within six months or less. An example of a compostable polymer is PLA film under 20μm thick: films which are thicker than that do not qualify as compostable, even though they are biodegradable. In Europe there is a home composting standard and associated logo that enables consumers to identify and dispose of packaging in their compost heap.\n\n\n", "id": "3974", "title": "Biopolymer"}
{"url": "https://en.wikipedia.org/wiki?curid=219277", "text": "Codon usage bias\n\nCodon usage bias refers to differences in the frequency of occurrence of synonymous codons in coding DNA. A codon is a series of three nucleotides (a triplet) that encodes a specific amino acid residue in a polypeptide chain or for the termination of translation (stop codons).\n\nThere are 64 different codons (61 codons encoding for amino acids plus 3 stop codons) but only 20 different translated amino acids. The overabundance in the number of codons allows many amino acids to be encoded by more than one codon. Because of such redundancy it is said that the genetic code is degenerate. The genetic codes of different organisms are often biased towards using one of the several codons that encode the same amino acid over the others—that is, a greater frequency of one will be found than expected by chance. How such biases arise is a much debated area of molecular evolution. Codon usage tables detailing genomic codon usage bias for most organisms in GenBank and RefSeq can be found in the HIVE-Codon Usage Table database.\n\nIt is generally acknowledged that codon biases reflect a balance between mutational biases and natural selection for translational optimization. Optimal codons in fast-growing microorganisms, like \"Escherichia coli\" or \"Saccharomyces cerevisiae\" (baker's yeast), reflect the composition of their respective genomic tRNA pool. It is thought that optimal codons help to achieve faster translation rates and high accuracy. As a result of these factors, translational selection is expected to be stronger in highly expressed genes, as is indeed the case for the above-mentioned organisms. In other organisms that do not show high growing rates or that present small genomes, codon usage optimization is normally absent, and codon preferences are determined by the characteristic mutational biases seen in that particular genome. Examples of this are \"Homo sapiens\" (human) and \"Helicobacter pylori\". Organisms that show an intermediate level of codon usage optimization include \"Drosophila melanogaster\" (fruit fly), \"Caenorhabditis elegans\" (nematode worm), \"Strongylocentrotus purpuratus\" (sea urchin) or \"Arabidopsis thaliana\" (thale cress). Several viral families (herpesvirus, lentivirus, papillomavirus, polyomavirus, adenovirus, and parvovirus) are known to encode structural proteins that display heavily skewed codon usage compared to the host cell. The suggestion has been made that these codon biases play a role in the temporal regulation of their late proteins.\n\nThe nature of the codon usage-tRNA optimization has been fiercely debated. It is not clear whether codon usage drives tRNA evolution or vice versa. At least one mathematical model has been developed where both codon usage and tRNA expression co-evolve in feedback fashion (\"i.e.\", codons already present in high frequencies drive up the expression of their corresponding tRNAs, and tRNAs normally expressed at high levels drive up the frequency of their corresponding codons). However, this model does not seem to yet have experimental confirmation. Another problem is that the evolution of tRNA genes has been a very inactive area of research.\n\nDifferent factors have been proposed to be related to codon usage bias, including gene expression level (reflecting selection for optimizing translation process by tRNA abundance), %G+C composition (reflecting horizontal gene transfer or mutational bias), GC skew (reflecting strand-specific mutational bias), amino acid conservation, protein hydropathy, transcriptional selection, RNA stability, optimal growth temperature, hypersaline adaptation and dietary nitrogen.\n\nAlthough the mechanism of codon bias selection remains controversial, possible explanations for this bias fall into two general categories. One explanation revolves around the selectionist theory, in which codon bias contributes to the efficiency and/or accuracy of protein expression and therefore undergoes positive selection. The selectionist model also explains why more frequent codons are recognized by more abundant tRNA molecules, as well as the correlation between preferred codons, tRNA levels and gene copy numbers. Although it has been shown that the rate of amino acid incorporation at more frequent codons occurs at a much higher rate than that of rare codons, the speed of translation has not been shown to be directly affected and therefore the bias towards more frequent codons may not be directly advantageous. However, the increase in translation elongation speed may still be indirectly advantageous by increasing the cellular concentration of free ribosomes and potentially the rate of initiation for messenger RNAs.\n\nThe second explanation for codon usage can be explained by mutational bias, a theory which posits that codon bias exists because of nonrandomness in the mutational patterns. In other words, some codons can undergo more changes and therefore result in lower equilibrium frequencies, also known as “rare” codons. Different organisms also exhibit different mutational biases, and there is growing evidence that the level of genome-wide GC content is the most significant parameter in explaining codon bias differences between organisms. Additional studies have demonstrated that codon biases can be statistically predicted in prokaryotes using only intergenic sequences, arguing against the idea of selective forces on coding regions and further supporting the mutation bias model. However, this model alone cannot fully explain why preferred codons are recognized by more abundant tRNAs.\n\nTo reconcile the evidence from both mutational pressures and selection, the prevailing hypothesis for codon bias can be explained by the mutation-selection-drift balance model. This hypothesis states that selection favors major codons over minor codons, but minor codons are able to persist due to mutation pressure and genetic drift. It also suggests that selection is generally weak, but that selection intensity scales to higher expression and more functional constraints of coding sequences.\n\nBecause secondary structure of the 5’ end of mRNA influences translational efficiency, synonymous changes at this region on the mRNA can result in profound effects on gene expression. Codon usage in noncoding DNA regions can therefore play a major role in RNA secondary structure and downstream protein expression, which can undergo further selective pressures. In particular, strong secondary structure at the ribosome-binding site or initiation codon can inhibit translation, and mRNA folding at the 5’ end generates a large amount of variation in protein levels.\n\nHeterologous gene expression is used in many biotechnological applications, including protein production and metabolic engineering. Because tRNA pools vary between different organisms, the rate of transcription and translation of a particular coding sequence can be less efficient when placed in a non-native context. For an overexpressed transgene, the corresponding mRNA makes a large percent of total cellular RNA, and the presence of rare codons along the transcript can lead to inefficient use and depletion of ribosomes and ultimately reduce levels of heterologous protein production. However, using codons that are optimized for tRNA pools in a particular host to overexpress a heterologous gene may also cause amino acid starvation and alter the equilibrium of tRNA pools. This method of adjusting codons to match host tRNA abundances, called codon optimization, has traditionally been used for expression of a heterologous gene. However, new strategies for optimization of heterologous expression consider global nucleotide content such as local mRNA folding, codon pair bias, a codon ramp or codon correlations.\n\nSpecialized codon bias is further seen in some endogenous genes such as those involved in amino acid starvation. For example, amino acid biosynthetic enzymes preferentially use codons that are poorly adapted to normal tRNA abundances, but have codons that are adapted to tRNA pools under starvation conditions. Thus, codon usage can introduce an additional level of transcriptional regulation for appropriate gene expression under specific cellular conditions.\n\nGenerally speaking for highly expressed genes, translation elongation rates are faster along transcripts with higher codon adaptation to tRNA pools, and slower along transcripts with rare codons. This correlation between codon translation rates and cognate tRNA concentrations provides additional modulation of translation elongation rates, which can provide several advantages to the organism. Specifically, codon usage can allow for global regulation of these rates, and rare codons may contribute to the accuracy of translation at the expense of speed.\n\nProtein folding \"in vivo\" is vectorial, such that the N-terminus of a protein exits the translating ribosome and becomes solvent-exposed before its more C-terminal regions. As a result, co-translational protein folding introduces several spatial and temporal constraints on the nascent polypeptide chain in its folding trajectory. Because mRNA translation rates are coupled to protein folding, and codon adaption is linked to translation elongation, it has been hypothesized that manipulation at the sequence level may be an effective strategy to regulate or improve protein folding. Several studies have shown that pausing of translation as a result of local mRNA structure occurs for certain proteins, which may be necessary for proper folding. Furthermore, synonymous mutations have been shown to have significant consequences in the folding process of the nascent protein and can even change substrate specificity of enzymes. These studies suggest that codon usage influences the speed at which polypeptides emerge vectorially from the ribosome, which may further impact protein folding pathways throughout the available structural space.\n\nIn the field of bioinformatics and computational biology, many statistical methods have been proposed and used to analyze codon usage bias. Methods such as the 'frequency of optimal codons' (Fop), the Relative Codon Adaptation (RCA) or the 'Codon Adaptation Index' (CAI) are used to predict gene expression levels, while methods such as the 'effective number of codons' (Nc) and Shannon entropy from information theory are used to measure codon usage evenness. Multivariate statistical methods, such as correspondence analysis and principal component analysis, are widely used to analyze variations in codon usage among genes. There are many computer programs to implement the statistical analyses enumerated above, including CodonW, GCUA, INCA, etc. Codon optimization has applications in designing synthetic genes and DNA vaccines. Several software packages are available online for this purpose (refer to external links).\n\n", "id": "219277", "title": "Codon usage bias"}
{"url": "https://en.wikipedia.org/wiki?curid=18744044", "text": "Chromosome landing\n\nChromosomal landing is a genetic technique used to identify and isolate clones in a genetic library. Chromosomal landing reduces the problem of analyzing large, and/or highly repetitive genomes by minimizing the need for chromosome walking. It is based on the principle that the expected average between-marker distances can be smaller than the average insert length of a clone library containing the gene of interest. \n\nFrom the abstract of :\n\n", "id": "18744044", "title": "Chromosome landing"}
{"url": "https://en.wikipedia.org/wiki?curid=19057218", "text": "Cleaved amplified polymorphic sequence\n\nThe cleaved amplified polymorphic sequence or CAPS method is a technique in molecular biology for the analysis of genetic markers. It is an extension to the Restriction Fragment Length Polymorphism (RFLP) method, using polymerase chain reaction (PCR) to more quickly analyse the results.\n\nLike RFLP, CAPS works on the principle that genetic differences between individuals can create or abolish restriction endonuclease restriction sites, and that these differences can be detected in the resulting DNA fragment length after digestion. \n\nIn the CAPS method, PCR amplification is directed across the altered restriction site, and the products digested with the restriction enzyme. When fractionated by agarose or acrylamide gel electrophoresis, the digested PCR products will give readily distinguishable patterns of bands. Alternatively, the amplified segment can be analyzed by Allele specific oligonucleotide (ASO) probes, a process that can often be done by a simple dot blot.\n\n\n", "id": "19057218", "title": "Cleaved amplified polymorphic sequence"}
{"url": "https://en.wikipedia.org/wiki?curid=5921769", "text": "Human artificial chromosome\n\nA human artificial chromosome (HAC) is a microchromosome that can act as a new chromosome in a population of human cells. That is, instead of 46 chromosomes, the cell could have 47 with the 47th being very small, roughly 6–10megabases (Mb) in size instead of 50–250Mb for natural chromosomes, and able to carry new genes introduced by human researchers. Ideally, researchers could integrate different genes that perform a variety of functions, including disease defense.\n\nAlternative methods of creating transgenes, such as utilizing yeast artificial chromosomes and bacterial artificial chromosomes, lead to unpredictable problems. The genetic material introduced by these vectors not only leads to different expression levels, but the inserts also disrupt the original genome. HACs differ in this regard, as they are entirely separate chromosomes. This separation from existing genetic material assumes that no insertional mutants would arise. This stability and accuracy makes HACs preferable to other methods such as viral vectors, YACs and BACs. HACs allow for delivery of more DNA (including promoters and copy-number variation) than is possible with viral vectors.\n\nYeast artificial chromosomes and bacterial artificial chromosomes were created before human artificial chromosomes, which first appeared in 1997. HACs are useful in expression studies as gene transfer vectors, as a tool for elucidating human chromosome function, and as a method for actively annotating the human genome.\n\nHACs were first constructed \"de novo\" in 1997 by adding alpha-satellite DNA to telomeric and genomic DNA in human HT1080 cells. This resulted in an entirely new microchromosome that contained DNA of interest, as well as elements allowing it to be structurally and mitotically stable, such as telomeric and centromeric sequences. Due to the difficulty of \"de novo\" HAC formation, this method has been largely abandoned.\n\nThere are currently two accepted models for the creation of human artificial chromosome vectors. The first is to create a small minichromosome by altering a natural human chromosome. This is accomplished by truncating the natural chromosome, followed by the introduction of unique genetic material via the Cre-Lox system of recombination. The second method involves the literal creation of a novel chromosome \"de novo\". Progress regarding \"de novo\" HAC formation has been limited, as many large genomic fragments will not successfully integrate into \"de novo\" vectors. Another factor limiting \"de novo\" vector formation is limited knowledge of what elements are required for construction, specifically centromeric sequences.\n\nA 2009 study has shown additional benefits of HACs, namely their ability to stably contain extremely large genomic fragments. Researchers incorporated the 2.4Mb dystrophin gene, in which a mutation is a key causal element of Duchenne muscular dystrophy. The resulting HAC was mitotically stable, and correctly expressed dystrophin in chimeric mice. Previous attempts at correctly expressing dystrophin have failed. Due to its large size, it has never before been successfully integrated into a vector.\n\nIn 2010, a refined human artificial chromosome called 21HAC was reported. 21HAC is based on a stripped copy of human chromosome 21, producing a chromosome 5Mb in length. Truncation of chromosome 21 resulted in a human artificial chromosome that is mitotically stable. 21HAC was also able to be transferred into cells from a variety of species (mice, chickens, humans). Using 21HAC, researchers were able to insert a herpes simplex virus- thymidine kinase coding gene into tumor cells. This \"suicide gene\" is required to activate many antiviral medications. These targeted tumor cells were successfully, and selectively, terminated by the antiviral drug ganciclovir in a population including healthy cells. This research opens a variety of opportunities for using HACs in gene therapy.\n\nIn 2011, researchers formed a human artificial chromosome by truncating chromosome 14. Genetic material was then introduced as mentioned above, using the Cre-Lox recombination system. This particular study focused on changes in expression levels by leaving portions of the existing genomic DNA. By leaving existing telomeric and sub-telomeric sequences, they were able to amplify expression levels of genes coding for erythropoietin production over 1000-fold. This work also has large gene therapy implications, as erythropoietin controls red blood cell formation.\n\nHACs have been used to create transgenic animals for use as animal models of human disease and for production of therapeutic products.\n\n", "id": "5921769", "title": "Human artificial chromosome"}
{"url": "https://en.wikipedia.org/wiki?curid=19564541", "text": "Glutamate-glutamine cycle\n\nIn biochemistry, the glutamate-glutamine cycle is a sequence of events by which an adequate supply of the neurotransmitter glutamate is maintained in the central nervous system. Neurons are unable to synthesize either the neurotransmitter glutamate or γ-aminobutyric acid (GABA) from glucose. Discoveries of glutamine and glutamate pools within intercellular compartments led to suggestions of the glutamate-glutamine cycle working between neurons and astrocytes. The glutamate/GABA-glutamine cycle is a metabolic pathway that describes the release of glutamate or GABA from neurons which are then taken up into astrocytes (star-shaped glial cells). In return, astrocytes release glutamine to be taken up into neurons for use as a precursor to the synthesis of glutamate or GABA.\n\nInitially, in a glutamatergic synapse, the neurotransmitter glutamate is released from the neurons and is taken up into the synaptic cleft. Glutamate residing in the synapse must be rapidly removed in one of three ways: \nPostsynaptic neurons remove little glutamate from the synapse. There is active reuptake into presynaptic neurons, but this mechanism appears to be less important than astrocytic transport. Astrocytes could dispose of transported glutamate in two ways. They could export it to blood capillaries, which abut the astrocyte foot processes. However, this strategy would result in a net loss of carbon and nitrogen from the system. An alternate approach would be to convert glutamate into another compound, preferably a non-neuroactive species. The advantage of this approach is that neuronal glutamate could be restored without the risk of trafficking the transmitter through extracellular fluid, where glutamate would cause neuronal depolarization. Astrocytes readily convert glutamate to glutamine via the glutamine synthetase pathway and released into the extracellular space. The glutamine is taken into the presynaptic terminals and metabolized into glutamate by the phosphate-activated glutaminase (a mitochondrial enzyme). The glutamate that is synthesized in the presynaptic terminal is packaged into synaptic vesicles by the glutamate transporter, VGLUT. Once the vesicle is released, glutamate is removed from the synaptic cleft by excitatory amino-acid transporters (EAATs). This allows synaptic terminals and glial cells to work together to maintain a proper supply of glutamate, which can also be produced by transamination of 2-oxoglutarate, an intermediate in the Citric acid cycle. Recent electrophysiological evidence suggests that active synapses require presynaptically localized glutamine glutamate cycle to maintain excitatory neurotransmission in specific circumstances. In other systems, it has been suggested that neurons have alternate mechanisms to cope with compromised glutamate-glutamine cycling.\n\nAt GABAergic synapses, the cycle is called the GABA-glutamine cycle. Here the glutamine taken up by neurons is converted to glutamate, which is then metabolized into GABA by glutamate decarboxylase (GAD). Upon release, GABA is taken up into astrocytes via GABA transporters and then catabolized into succinate by the joint actions of GABA transaminase and succinate-semialdehyde dehydrogenase. Glutamine is synthesized from succinate via the TCA cycle, which includes a condensation reaction of oxaloacetate and acetyl-CoA-forming citrate. Then the synthesis of α-ketoglutarate and glutamate occurs, after which glutamate is again metabolized into GABA by GAD. The supply of glutamine to GABAergic neurons is less significant, because these neurons exhibit a larger proportion of reuptake of the released neurotransmitter compared to their glutamatergic counterparts \n\nOne of the problems of both the glutamate-glutamine cycle and the GABA-glutamine cycle is ammonia homeostasis. When one molecule of glutamate or GABA is converted to glutamine in the astrocytes, one molecule of ammonia is absorbed. Also, for each molecule of glutamate or GABA cycled into the astrocytes from the synapse, one molecule of ammonia will be produced in the neurons. This ammonia will obviously have to be transported out of the neurons and back into the astrocytes for detoxification, as an elevated ammonia concentration has detrimental effects on a number of cellular functions and can cause a spectrum of neuropsychiatric and neurological symptoms (impaired memory, shortened attention span, sleep-wake inversions, brain edema, intracranial hypertension, seizures, ataxia and coma).\n\nThis could happen in two different ways: ammonia itself might simply diffuse (as NH3) or be transported (as NH4+) across the cell membranes in and out of the extracellular space, or a shuttle system involving carrier molecules (amino acids) might be employed. Certainly, ammonia can diffuse across lipid membranes, and it has been shown that ammonia can be transported by K+/CI– co-transporters.\n\nSince diffusion and transport of free ammonia across the cell membrane will affect the pH level of the cell, the more attractive and regulated way of transporting ammonia between the neuronal and the astrocytic compartment is via an amino-acid shuttle, of which there are two: leucine and alanine. The amino acid moves in the opposite direction of glutamine. In the opposite direction of the amino acid, a corresponding molecule is transported; for alanine this molecule is lactate; for leucine, α-ketoisocaproate.\n\nThe ammonia fixed as part of the glutamate dehydrogenase enzyme reaction in the neurons is transaminated into α-ketoisocaproate to form the branched-chain amino acid leucine, which is exported to the astrocytes, where the process is reversed. α-ketoisocaproate is transported in the other direction.\n\nThe ammonia produced in neurons is fixed into α-ketoglutarate by the glutamate-dehydrogenase reaction to form glutamate, then transaminated by alanine aminotransferase into lactate-derived pyruvate to form alanine, which is exported to astrocytes. In the astrocytes, this process is then reversed, and lactate is transported in the other direction.\n\nNumerous reports have been published indicating that the glutamate/GABA-glutamine cycle is compromised in a variety of neurological disorders and conditions. Biopsies of sclerotic hippocampus tissue from human subjects suffering from epilepsy have shown decreased glutamate-glutamine cycling. Another pathology in which the glutamate/GABA-glutamine cycle might be compromised is Alzheimer's disease; NMR spectroscopy showed decreased glutamate neurotransmission activity and TCA cycling rate in patients suffering from Alzheimer's disease. Hyperammonemia in the brain, typically occurring as a secondary complication of primary liver disease and known as hepatic encephalopathy, is a condition that affects glutamate/GABA-glutamine cycling in the brain. Current research into autism also indicates potential roles for glutamate, glutamine, and/or GABA in autistic spectrum disorders.\n\nIn the treatment of epilepsy, drugs targeting both GABA transporters and the GABA metabolizing enzyme GABA-transaminase (vigabatrin) have been marketed, providing proof of principle for the neurotransmitter cycling systems as pharmacological targets. However, with regard to glutamate transport and metabolism, no such drugs have been developed, because glutamatergic synapses are abundant, and the neurotransmitter glutamate is an important metabolite in metabolism, making interference capable of adverse effects. So far, most of the drug development directed at the glutamatergic system seems to have been focused on ionotropic glutamate receptors as pharmacological targets, although G-protein coupled receptors have been attracting increased attention over the years.\n", "id": "19564541", "title": "Glutamate-glutamine cycle"}
{"url": "https://en.wikipedia.org/wiki?curid=17030615", "text": "Transposon mutagenesis\n\nTransposon mutagenesis, or transposition mutagenesis, is a biological process that allows genes to be transferred to a host organism's chromosome, interrupting or modifying the function of an extant gene on the chromosome and causing mutation. Transposon mutagenesis is much more effective than chemical mutagenesis, with a higher mutation frequency and a lower chance of killing the organism. Other advantages include being able to induce single hit mutations, being able to incorporate selectable markers in strain construction, and being able to recover genes after mutagenesis. Disadvantages include the low frequency of transposition in living systems, and the inaccuracy of most transposition systems.\n\nTransposon mutagenesis was first studied by Barbara McClintock in the mid-20th century during her Nobel Prize-winning work with corn. McClintock received her BSc in 1923 from Cornell’s College of Agriculture. By 1927 she had her PhD in botany, and she immediately began working on the topic of maize chromosomes. In the early 1940s, McClintock was studying the progeny of self-pollinated maize plants which resulted from crosses having a broken chromosome 9. These plants were missing their telomeres. This research prompted the first discovery of a transposable element, and from there transposon mutagenesis have been exploited as a biological tool.\n\nIn the case of bacteria, transposition mutagenesis is usually accomplished by way of a plasmid from which a transposon is extracted and inserted into the host chromosome. This usually requires a set of enzymes including transposase to be translated. The transposase can be expressed either on a separate plasmid, or on the plasmid containing the gene to be integrated. Alternatively, an injection of transposase mRNA into the host cell can induce translation and expression. Early transposon mutagenesis experiments relied on bacteriophages and conjugative bacterial plasmids for the insertion of sequences. These were very non-specific, and made it difficult to incorporate specific genes. A newer technique called shuttle mutagenesis uses specific cloned genes from the host species to incorporate genetic elements. Another effective approach is to deliver transposons through viral capsids. This facilitates integration into the chromosome and long-term transgene expression.\n\nThe Tn5 transposon system is a model system for the study of transposition and for the application of transposon mutagenesis. Tn5 is a bacterial composite transposon in which genes (the original system containing antibiotic resistance genes) are flanked by two nearly identical insertion sequences, named IS50R and IS50L corresponding to the right and left sides of the transposon respectively. The IS50R sequence codes for two proteins, Tnp and Inh. These two proteins are identical in sequence, save for the fact that Inh is lacking the 55 N-terminal amino acids. Tnp codes for a transposase for the entire system, and Inh encodes an inhibitor of transposase. The DNA-binding domain of Tnp resides in the 55 N-terminal amino acids, and so these residues are essential for function. The IS50R and IS50L sequences are both flanked by 19-base pair elements on the inside and outside ends of the transposon, labelled IE and OE respectively. Mutation of these regions results in an inability of transposase genes to bind to the sequences. The binding interactions between transposase and these sequences is very complicated, and is affected by DNA methylation and other epigenetic marks. In addition, other proteins seem to be able to bind with and affect the transposition of the IS50 elements, such as DnaA.\n\nThe most likely pathway of Tn 5 transposition is the common pathway for all transposon systems. It begins with Tnp binding the OE and IE sequences of each IS50 sequence. The two ends are brought together, and through oligomerization of DNA, the sequence is cut out of the chromosome. After introducing 9-base pair 5' ends in target DNA, the transposon and its incorporated genes are inserted into the target DNA, duplicating the regions on either end of the transposon. Genes of interest can be genetically engineered into the transposon system between the IS50 sequences. By placing the transposon under the control of a host promoter, the genes will be expressed. Incorporated genes usually include, in addition to the gene of interest, a selectable marker to identify transformants, a eukaryotic promoter/terminator (if expressing in a eukaryote), and 3' UTR sequences to separate genes in a polycistronic stretch of sequence.\n\nThe Sleeping Beauty transposon system (SBTS) is the first successful non-viral vector for incorporation of a gene cassette into a vertebrate genome. Up until the development of this system, the major problems with non-viral gene therapy have been the intracellular breakdown of the transgene due to it being recognized as prokaryotic and the inefficient delivery of the transgene into organ systems. The SBTS revolutionized these issues by combining the advantages of viruses and naked DNA. It consists of a transposon containing the cassette of genes to be expressed, as well as its own transposase enzyme. By transposing the cassette directly into the genome of the organism from the plasmid, sustained expression of the transgene can be attained. This can be further refined by enhancing the transposon sequences and the transposase enzymes used. SB100X is a hyperactive mammalian transposase which is roughly 100x more efficient than the typical first-generation transposase. Incorporation of this enzyme into the cassette results in even more sustained transgene expression (over one year). Additionally, transgenesis frequencies can be as high as 45% when using pronuclear injection into mouse zygotes.\nThe mechanism of the SBTS is similar to the Tn5 transposon system, however the enzyme and gene sequences are eukaryotic in nature as opposed to prokaryotic. The system's tranposase can act in \"trans\" as well as in \"cis\", allowing a diverse collection of transposon structures. The transposon itself is flanked by inverted repeat sequences, which are each repeated twice in a direct fashion, designated IR/DR sequences. The internal region consists of the gene or sequence to be transposed, and could also contain the transposase gene. Alternatively, the transposase can be encoded on a separate plasmid or injected in its protein form. Yet another approach is to incorporate both the transposon and the transposase genes into a viral vector, which can target a cell or tissue of choice. The transposase protein is extremely specific in the sequences that it binds, and is able to discern its IR/DR sequences from a similar sequence by three base pairs. Once the enzyme is bound to both ends of the transposon, the IR/DR sequences are brought together and held by the transposase in a Synaptic Complex Formation (SCF). The formation of the SCF is a checkpoint ensuring proper cleavage. HMGB1 is a non-histone protein from the host which is associated with eukaryotic chromatin. It enhances the preferential binding of the transposase to the IR/DR sequences and is likely essential for SCF complex formation/stability. Transposase cleaves the DNA at the target sites, generating 3' overhangs. The enzyme then targets TA dinucleotides in the host genome as target sites for integration. The same enzymatic catalytic site which cleaved the DNA is responsible for integrating the DNA into the genome, duplicating the region of the genome in the process. Although transposase is specific for TA dinucleotides, the high frequency of these pairs in the genome indicates that the transposon undergoes fairly random integration.\n\nAs a result of the capacity of transposon mutagenesis to incorporate genes into most areas of target chromosomes, there are a number of functions associated with the process.\n\nIn 1999, the virulence genes associated with \"Mycobacterium tuberculosis\" were identified through transposon mutagenesis-mediated gene knockout. A plasmid named pCG113 containing kanamycin resistance genes and the IS\"1096\" insertion sequence was engineered to contain variable 80-base pair tags. The plasmids were then transformed into \"M. tuberculosis\" cells by electroporation. Colonies were plated on kanamycin to select for resistant cells. Colonies that underwent random transposition events were identified by \"Bam\"HI digestion and Southern blotting using an internal IS\"1096\" DNA probe. Colonies were screened for attenuated multiplication to identify colonies with mutations in candidate virulence genes. Mutations leading to an attenuated phenotype were mapped by amplification of adjacent regions to the IS\"1096\" sequences and compared with the published \"M. tuberculosis\" genome. In this instance transposon mutagenesis identified 13 pathogenic loci in the \"M. tuberculosis\" genome which were not previously associated with disease. This is essential information in understanding the infectious cycle of the bacterium.\n\nThe \"PiggyBac\" (PB) transposon from the cabbage looper moth \"Trichoplusia\" \"ni\" was engineered to be highly active in mammalian cells, and is capable of genome-wide mutagenesis. Transposons contained both \"PB\" and \"Sleeping Beauty\" inverted repeats, in order to be recognized by both transposases and increase the frequency of transposition. In addition, the transposon contained promoter and enhancer elements, a splice donor and acceptors to allow gain- or loss-of-function mutations depending on the transposon's orientation, and bidirectional polyadenylation signals. The transposons were transformed into mouse cells \"in vitro\" and mutants containing tumours were analyzed. The mechanism of the mutation leading to tumour formation determined if the gene was classified as an oncogene or a tumour-suppressor gene. Oncogenes tended to be characterized by insertions in regions leading to overexpression of a gene, whereas tumour-suppressor genes were classified as such based on loss-of-function mutations. Since the mouse is a model organism for the study of human physiology and disease, this research will help lead to an increased understanding of cancer-causing genes and potential therapeutic targets.\n\n\n", "id": "17030615", "title": "Transposon mutagenesis"}
{"url": "https://en.wikipedia.org/wiki?curid=2941387", "text": "Restriction fragment\n\nA restriction fragment is a DNA fragment resulting from the cutting of a DNA strand by a restriction enzyme (restriction endonucleases), a process called restriction. Each restriction enzyme is highly specific, recognising a particular short DNA sequence, or restriction site, and cutting both DNA strands at specific points within this site. Most restriction sites are palindromic, (the sequence of nucleotides is the same on both strands when read in the 5' to 3' direction), and are four to eight nucleotides long. Many cuts are made by one restriction enzyme because of the chance repetition of these sequences in a long DNA molecule, yielding a set of restriction fragments. A particular DNA molecule will always yield the same set of restriction fragments when exposed to the same restriction enzyme. Restriction fragments can be analyzed using techniques such as gel electrophoresis or used in recombinant DNA technology.\n\nIn recombinant DNA technology specific restriction endonucleases are used that will isolate a particular gene and cleave the sugar phosphate backbones at different points (retaining symmetry), so that the double stranded restriction fragments have single stranded ends. These short extensions, called \"sticky ends\" can form hydrogen bonded base pairs with complementary sticky ends on any other DNA cut with the same enzyme (such as a bacterial plasmid).\n\nIn agarose gel electrophoresis, the restriction fragments yield a band pattern characteristic of the original DNA molecule and restriction enzyme used, for example the relatively small DNA molecules of viruses and plasmids can be identified simply by their restriction fragment patterns. If the nucleotide differences of two different alleles occur within the restriction site of a particular restriction enzyme, digestion of segments of DNA from individuals with different alleles for that particular gene with that enzyme would produce different fragments and that will each yield different band patterns in gel electrophoresis.\n", "id": "2941387", "title": "Restriction fragment"}
{"url": "https://en.wikipedia.org/wiki?curid=20497041", "text": "Sonoporation\n\nSonoporation, or \"cellular sonication\", is the use of sound (typically ultrasonic frequencies) for modifying the permeability of the cell plasma membrane. This technique is usually used in molecular biology and non-viral gene therapy in order to allow uptake of large molecules such as DNA into the cell, in a cell disruption process called transfection or transformation. Sonoporation employs the acoustic cavitation of microbubbles to enhance delivery of these large molecules. The bioactivity of this technique is similar to, and in some cases found superior to, electroporation. Extended exposure to low-frequency (<MHz) ultrasound has been demonstrated to result in complete cellular death (rupturing), thus cellular viability must also be accounted for when employing this technique.\n\nSonoporation is under active study for the introduction of foreign genes in tissue culture cells, especially mammalian cells. Sonoporation is also being studied for use in targeted Gene therapy in vivo, in a medical treatment scenario whereby a patient is given modified DNA, and an ultrasonic transducer might target this modified DNA into specific regions of the patient's body.\n\nSonoporation is performed with a dedicated sonoporator. Sonoporation may also be performed with custom-built piezoelectric transducers connected to bench-top function generators and acoustic amplifiers. Standard ultrasound medical devices may also be used in some applications.\n\nMeasurement of the acoustics used in sonoporation is listed in terms of mechanical index, which quantifies the likelihood that exposure to diagnostic ultrasound will produce an adverse biological effect by a non-thermal action based on pressure.\n\nSonoporation uses microbubbles for significantly enhancing transfection, and in some cases is required for DNA uptake. These microbubble agents include Optison, manufactured by General Electric Healthcare.\n", "id": "20497041", "title": "Sonoporation"}
{"url": "https://en.wikipedia.org/wiki?curid=20044943", "text": "Christoph Cremer\n\nChristoph Cremer (born in Freiburg im Breisgau, Germany) is a German physicist and professor at the Ruprecht-Karls-University Heidelberg, honorary professor at the University of Mainz and group leader at the Institute of Molecular Biology (IMB) a newly established research centre on the campus of the Johannes Gutenberg University of Mainz, Germany, who has successfully overcome the conventional limit of resolution that applies to light based investigations (the Abbe limit) by a range of different methods (1971/1978 development of the concept of 4Pi-microscopy; 1996 localization microscopy SPDM; 1997 spatially structured illumination SMI). In September 2014 he founded the NPO LuciaOptics to support the use of Super-resolution microscopy in the fields of molecular biology, biomedicine, microbiology, virology, pharmaceutical sciences and diagnosis\n\nHis actual microscope Vertico-SMI is the world’s fastest nano light microscope that allows large scale investigation of supramolecular complexes including living cell conditions. It allows 3 D imaging of biological preparations marked with conventional fluorescent dyes and reaches a resolution of 10 nm in 2D and 40 nm in 3D.\n\nThis nanoscope has therefore the potential to add substantially to the current revolution in optical imaging which will affect the entire molecular biology, medical and pharmaceutical research. The technology allows the development of new strategies for the prevention, the lowering of risk and therapeutic treatment of diseases.\n\nFollowing a few semesters studying philosophy and history at Freiburg University and Munich University, Cremer studied physics in Munich (with financial support from the Studienstiftung des Deutschen Volkes) and completed his Ph.D. in genetics and biophysics in Freiburg. This was followed by post-doctoral studies at the Institute for Human Genetics at Freiburg University, several years in the United States at the University of California, and his \"Habilitation\" in general human genetics and experimental cytogenetics at Freiburg University. Since 1983, he is teaching as a professor (chair since 2004) for \"applied optics and information processing\" at the Kirchhoff Institute for Physics at the University of Heidelberg. In addition, he is a member of the Interdisciplinary Center for Scientific Computing of the Institute for Pharmacy and Molecular Biotechnology, as well as of the University’s \"Bioquant\" Center. Cremer is a participant in three current \"Projects of Excellence\" of the University of Heidelberg (2007–2012), and is also a partner in the Biotechnology Cluster for cell-based and molecular medicine, one of five clusters selected in 2008 as German BMBF Clusters of Excellence. Elected as Second Speaker of the Senate of the University of Heidelberg (since 2006), Cremer is also involved in university governance and politics. In his function as adjunct professor at the University of Maine and as member of the Jackson Laboratory (Bar Harbor, Maine), where he undertakes research for several weeks each year during the semester breaks, he is involved in the establishment of the biophysics center (Institute for Molecular Biophysics), which is linked with the University of Heidelberg through a \"Global Network\" collaboration.\n\nCremer is married to architect and artist Letizia Mancino-Cremer.\n\nCremer was involved early in the further development of laser based light microscopy approaches. First ideas had their origin in his graduate student years in the 1970s. Jointly with his brother Thomas Cremer, now professor (chair) of Anthropology and Human Genetics at the Ludwigs-Maximilian University in Munich, Christoph Cremer proposed the development of a hologram-based laser scanning 4Pi microscope. The basic idea was to focus laser light from all sides (space angle 4Pi) in a spot with a diameter smaller than the conventional laser focus and to scan the object by means of this spot. In this manner, it should be possible to achieve an improved optical resolution beyond the conventional limit of approx. 200 nm lateral, 600 nm axial. Since 1992, 4Pi microscopy has been developed by Stefan Hell (Max-Planck Institute for Biophysical Chemistry, Göttingen) into a highly efficient, high-resolution imaging process, using two microscope objective lenses of high numeric aperture opposing each other.\n\nIn the early 1970s, the brothers realized a UV laser micro irradiation instrument which for the first time made it possible to irradiate in a controlled manner only a tiny part of a living cell at the absorption maximum for DNA (257 nm). This replaced the conventional UV partial irradiation practiced for over 60 years. In this way, it was possible for the first time to induce alterations in the DNA in a focused manner (i.e. at predetermined places in the cell nucleus of living cells) without compromising the cells ability to divide and to survive. Specific very small cell regions could be irradiated and thus the dynamics of macromolecules (DNA) contained there quantitatively estimated. Furthermore, due to the high speed of the process using irradiation times of fractions of a second, it became possible to irradiate even moving cell organelles. This development provided the basis for important experiments in the area of genome structure research (establishing the existence of so-called chromosome territories in living mammalian cells) and led, a few years later (1979/1980) to a successful collaboration with the biologist Christiane Nüsslein-Volhard (Max Planck Institute for Developmental Biology, Tübingen). In this collaboration Cremer used his UV laser micro irradiation equipment to elicit cellular changes in the early larval stages of the fruit fly \"Drosophila melanogaster\".\n\nOn the basis of experience gained in the construction and application of the UV laser micro irradiation instrument, the Cremer brothers designed in 1978 a laser scanning process which scans point-by-point the three-dimensional surface of an object by means of a focused laser beam and creates the over-all picture by electronic means similar to those used in scanning electron microscopes. It is this plan for the construction of a confocal laser scanning microscope (CSLM), which for the first time combined the laser scanning method with the 3D detection of biological objects labeled with fluorescent markers that earned Cremer his professorial position at the University of Heidelberg. During the next decade, the confocal fluorescence microscopy was developed into a technically fully matured state in particular by groups working at the University of Amsterdam and the European Molecular Biology Laboratory (EMBL) in Heidelberg and their industry partners. In later years, this technology was adopted widely by biomolecular and biomedical laboratories and remains to this day the gold standard as far as three-dimensional light microscopy with conventional resolution is concerned.\n\nThe goal of microscopy is in many cases to determine the size of individual, small objects. Conventional fluorescence microscopy can only establish sizes to around the conventional optical resolution limit of approximately 200 nm (lateral). More than 20 years after submitting the 4 pi patent application, Christoph Cremer returned to the problem of the diffraction limit. \nWith the Vertico SMI microscope he could realize his various super resolution techniques including SMI,SPDM, SPDMphymod and LIMON. These methods are mainly used for biomedical applications \n\nAround 1995, he commenced with the development of a light microscopic process, which achieved a substantially improved size resolution of cellular nanostructures stained with a fluorescent marker. This time he employed the principle of wide field microscopy combined with structured laser illumination (spatially modulated illumination, SMI) Currently, a size resolution of 30 – 40 nm (approximately 1/16 – 1/13 of the wavelength used) is being achieved. In addition, this technology is no longer subjected to the speed limitations of the focusing microscopy so that it becomes possible to undertake 3D analyses of whole cells within short observation times (at the moment around a few seconds). Disambiguation SMI: S = spatially, M = Modulated I= Illumination.\n\nAlso around 1995, Cremer developed and realized new fluorescence based wide field microscopy approaches which had as their goal the improvement of the effective optical resolution (in terms of the smallest detectable distance between two localized objects) down to a fraction of the conventional resolution (spectral precision distance/position determination microscopy, SPDM; Disambiguation SPDM: S = Spectral, P = Precision, D = Distance, M = Microscopy).\n\nWith this method, it is possible to use conventional, well established and inexpensive fluorescent dyes, standard like GFP, RFP, YFP, Alexa 488, Alexa 568, Alexa 647, Cy2, Cy3, Atto 488 and fluorescein.\n\nDisambiguation SPDMphymod: S = Spectral, P = Precision D = Distance, M = Microscopy, phy = physically, mod = modifiable\n\nCombining SPDM and SMI, known as LIMON microscopy. Christoph Cremer can currently achieve a resolution of approx. 10 nm in 2D and 40 nm in 3D in wide field images of whole living cells. Widefield 3D \"nanoimages\" of whole living cells currently still take about two minutes, but work to reduce this further is currently under way. Vertico-SMI is currently the fastest optical 3D nanoscope for the three-dimensional structural analysis of whole cells worldwide \n\n", "id": "20044943", "title": "Christoph Cremer"}
{"url": "https://en.wikipedia.org/wiki?curid=20579517", "text": "Gateway Technology\n\nThe Gateway cloning System, invented and commercialized by Invitrogen since the late 1990s, is a molecular biology method that enables researchers to efficiently transfer DNA-fragments between plasmids using a proprietary set of recombination sequences, the \"Gateway att\" sites, and two proprietary enzyme mixes, called \"LR Clonase\", and \"BP Clonase\". Gateway Cloning Technique allows transfer of DNA fragments between different cloning vectors while maintaining the reading frame. Using Gateway, one can clone subclone DNA segments for functional analysis. The system requires the initial insertion of a DNA fragment into a plasmid with two flanking recombination sequences called “att L 1” and “att L 2”, to develop a “Gateway Entry clone” (special Invitrogen nomenclature).\n\nLarge archives of Gateway Entry clones, containing the vast majority of human, mouse and rat ORFs (open reading frames) have been cloned from human cDNA libraries or chemically synthesized to support the research community using NIH (National Institutes of Health) funding (e.g., Mammalian Gene Collection, http://mgc.nci.nih.gov/). The availability of these gene cassettes in a standard Gateway cloning plasmid helps researchers quickly transfer these cassettes into plasmids that facilitate the analysis of gene function. Gateway cloning does take more time for initial set-up, and is more expensive than traditional restriction enzyme and ligase-based cloning methods, but it saves time, and offers simpler and highly efficient cloning for down-stream applications.\n\nThe technology has been widely adopted by the life science research community especially for applications that require the transfer of thousands of DNA fragments into one type of plasmid (e.g., one containing a CMV promoter for protein expression in mammalian cells), or for the transfer of one DNA fragment into many different types of plasmids (e.g., for bacterial, insect and mammalian protein expression).\n\nThe first step in Gateway cloning is the preparation of a Gateway Entry clone. Entry clones are often made in two steps:\n\n1) “Gateway attB1, and attB2” sequences are added to the 5’, and 3’ end of a gene fragment, respectively, using gene specific PCR primers and PCR-amplification;\n\n2) the PCR amplification products are then mixed with special plasmids called Gateway “Donor vectors” (Invitrogen nomenclature) and the proprietary “BP Clonase” enzyme mix. The enzyme mix catalyzes the recombination and insertion of the att-B-sequence-containing PCR product into the att P recombination sites in the Gateway Donor vector. Once the cassette is part of the target plasmid, it is called an \"Entry clone\" in the Gateway nomenclature, and recombination sequences are referred to as the Gateway “att L” type.\n\nThe gene cassette in the Gateway Entry clone can then be simply and efficiently transferred into any Gateway Destination vector (Invitrogen nomenclature for any Gateway plasmid that contains Gateway “att R” recombination sequences and elements such as promoters and epitope tags, but not ORFs) using the proprietary enzyme mix, “LR Clonase”. Thousands of Gateway Destination plasmids have been made and are freely shared amongst researchers across the world. Gateway Destination vectors are similar to classical expression vectors containing multiple cloning sites, before the insertion of a gene of interest, using restriction enzyme digestion and ligation. Gateway Destination vectors are commercially available from Invitrogen, EMD (Novagen) and Covalys.\n\nSince Gateway cloning uses patented recombination sequences, and proprietary enzyme mixes available only from Invitrogen, the technology does not allow researchers to switch vendors and contributes to the lock-in effect of all such patented procedures.\n\nTo summarize the different steps involved in Gateway cloning:\n\n\n", "id": "20579517", "title": "Gateway Technology"}
{"url": "https://en.wikipedia.org/wiki?curid=11058303", "text": "Trizol\n\nTRIzol is a chemical solution used in RNA/DNA/protein extraction, by the reference paper from Piotr Chomczyński and Sacchi, N. in 1987.\n\nTRIzol is the brand name of the product from the Ambion part of Life Technologies, and Tri-Reagent is the brand name from MRC, which was founded by Chomczynski.\n\nThe correct name of the method is guanidinium thiocyanate-phenol-chloroform extraction. The use of TRIzol can result in DNA and RNA yields comparable to other extraction methods. An alternative method for RNA extraction is phenol extraction and TCA/acetone precipitation. Chloroform should be exchanged with 1-bromo-3-chloropropane when using the new generation TRI Reagent.\n\nTRIzol is light sensitive and is often stored in a dark-colored, glass container covered in foil. It must be kept below room temperature.\n\nWhen used, it resembles cough syrup, bright pink. The smell of the phenol is extremely strong. TRIzol works by maintaining RNA integrity during tissue homogenization, while at the same time disrupting and breaking down cells and cell components.\n\nCaution should be taken while using TRIzol (due to the phenol and chloroform).\n\nExposure to TRIzol can be a serious health hazard. Exposure can lead to serious chemical burns and permanent scarring. A lab coat, gloves and a plastic apron are recommended.\n\n", "id": "11058303", "title": "Trizol"}
{"url": "https://en.wikipedia.org/wiki?curid=20727645", "text": "Somatic fusion\n\nSomatic fusion, also called protoplast fusion, is a type of genetic modification in plants by which two distinct species of plants are fused together to form a new hybrid plant with the characteristics of both, a somatic hybrid. Hybrids have been produced either between different varieties of the same species (e.g. between non-flowering potato plants and flowering potato plants) or between two different species (e.g. between wheat \"Triticum\" and rye \"Secale\" to produce Triticale).\n\nUses of somatic fusion include making potato plants resistant to potato leaf roll disease. Through somatic fusion, the crop potato plant \"Solanum tuberosum\" – the yield of which is severely reduced by a viral disease transmitted on by the aphid vector – is fused with the wild, non-tuber-bearing potato \"Solanum brevidens\", which is resistant to the disease. The resulting hybrid has the chromosomes of both plants and is thus similar to polyploid plants.\nSomatic hybridization was first introduced by Carlson in \"Nicotiana glauca\".\n\nThe somatic fusion process occurs in four steps:\n\nDifferent from the procedure for seed plants describe above, fusion of moss protoplasts can be initiated without electric shock but by the use of polyethylene glycol (PEG). Further, moss protoplasts do not need phytohormones for regeneration, and they do not form a callus. Instead, regenerating moss protoplasts behave like germinating moss spores. Of further note sodium nitrate and calcium ion at high pH can be used, although results are variable depending on the organism.\n\nSomatic cells of different types can be fused to obtain hybrid cells. Hybrid cells are useful in a variety of ways, e.g.,\n\n(i) to study the control of cell division and gene expression,\n\n(ii) to investigate malignant transformations,\n\n(iii) to obtain viral replication,\n\n(iv) for gene or chromosome mapping and for\n\n(v) production of monoclonal antibodies by producing hybridoma (hybrid cells between an immortalised cell and an antibody producing lymphocyte), etc.\n\nChromosome mapping through somatic cell hybridization is essentially based on fusion of human and mouse somatic cells. Generally, human fibrocytes or leucocytes are fused with mouse continuous cell lines.\n\nWhen human and mouse cells (or cells of any two mammalian species or of the same species) are mixed, spontaneous cell fusion occurs at a very low rate (10-6). Cell fusion is enhanced 100 to 1000 times by the addition of ultraviolet inactivated Sendai (parainfluenza) virus or polyethylene glycol (PEG).\n\nThese agents adhere to the plasma membranes of cells and alter their properties in such a way that facilitates their fusion. Fusion of two cells produces a heterokaryon, i.e., a single hybrid cell with two nuclei, one from each of the cells entering fusion. Subsequently, the two nuclei also fuse to yield a hybrid cell with a single nucleus.\n\nA generalized scheme for somatic cell hybridization may be described as follows. Appropriate human and mouse cells are selected and mixed together in the presence of inactivated Sendai virus or PEG to promote cell fusion. After a period of time, the cells (a mixture of man, mouse and 'hybrid' cells) are plated on a selective medium, e.g., HAT medium, which allows the multiplication of hybrid cells only.\n\nSeveral clones (each derived from a single hybrid cell) of the hybrid cells are thus isolated and subjected to both cytogenetic and appropriate biochemical analyses for the detection of enzyme/ protein/trait under investigation. An attempt is now made to correlate the presence and absence of the trait with the presence and absence of a human chromosome in the hybrid clones.\n\nIf there is a perfect correlation between the presence and absence of a human chromosome and that of a trait in the hybrid clones, the gene governing the trait is taken to be located in the concerned chromosome.\n\nThe HAT medium is one of the several selective media used for the selection of hybrid cells. This medium is supplemented with hypoxanthine, aminopterin and thymidine, hence the name HAT medium. Antimetabolite aminopterin blocks the cellular biosynthesis of purines and pyrimidines from simple sugars and amino acids.\n\nHowever, normal human and mouse cells can still multiply as they can utilize hypoxanthine and thymidine present in the medium through a salvage pathway, which ordinarily recycles the purines and pyrimidines produced from degradation of nucleic acids.\n\nHypoxanthine is converted into guanine by the enzyme hypoxanthine-guanine phosphoribosyltransferase (HGPRT), while thymidine is phosphorylated by thymidine kinase (TK); both HGPRT and TK are enzymes of the salvage pathway.\n\nOn a HAT medium, only those cells that have active HGPRT (HGPRT+) and TK (TK+) enzymes can proliferate, while those deficient in these enzymes (HGPRr- and/or TK-) can not divide (since they cannot produce purines and pyrimidines due to the aminopterin present in the HAT medium).\n\nFor using HAT medium as a selective agent, human cells used for fusion must be deficient for either the enzyme HGPRT or TK, while mouse cells must be deficient for the other enzyme of this pair. Thus, one may fuse HGPRT deficient human cells (designated as TK+ HGPRr-) with TK deficient mouse cells (denoted as TK- HGPRT+).\n\nTheir fusion products (hybrid cells) will be TK+ (due to the human gene) and HGPRT+ (due to the mouse gene) and will multiply on the HAT medium, while the man and mouse cells will fail to do so. Experiments with other selective media can be planned in a similar fashion.\n\n\nTable: Reference #5\nNote: The table only lists a few examples, there are many more crosses. The possibilities of this technology are great; however, not all species are easily put into protoplast culture.\n", "id": "20727645", "title": "Somatic fusion"}
{"url": "https://en.wikipedia.org/wiki?curid=68206", "text": "Central dogma of molecular biology\n\nThe central dogma of molecular biology is an explanation of the flow of genetic information within a biological system. It is often stated as \"DNA makes RNA and RNA makes protein,\" although this is an oversimplification. It was first stated by Francis Crick in 1958:\n\nand re-stated in a \"Nature\" paper published in 1970:\n\nA second version of the central dogma is popular but incorrect. This is the simplistic DNA → RNA → protein pathway published by James Watson in the first edition of \"The Molecular Biology of the Gene\" (1965). Watson's version differs from Crick's because Watson describes the two-step (DNA → RNA and RNA → protein) pathway as the central dogma. While the dogma, as originally stated by Crick, remains valid today, Watson's version does not. \n\nThe dogma is a framework for understanding the transfer of sequence information between information-carrying biopolymers, in the most common or general case, in living organisms. There are 3 major classes of such biopolymers: DNA and RNA (both nucleic acids), and protein. There are 3×3=9 conceivable direct transfers of information that can occur between these. The dogma classes these into 3 groups of 3: three general transfers (believed to occur normally in most cells), three special transfers (known to occur, but only under specific conditions in case of some viruses or in a laboratory), and three unknown transfers (believed never to occur). The general transfers describe the normal flow of biological information: DNA can be copied to DNA (DNA replication), DNA information can be copied into mRNA (transcription), and proteins can be synthesized using the information in mRNA as a template (translation). The special transfers describe: RNA being copied from RNA (RNA replication), DNA being synthesised using an RNA template (reverse transcription), and proteins being synthesised directly from a DNA template without the use of mRNA. The unknown transfers describe: a protein being copied from a protein, synthesis of RNA using the primary structure of a protein as a template, and DNA synthesis using the primary structure of a protein as a template - these are not thought to naturally occur.\n\nThe biopolymers that comprise DNA, RNA and (poly)peptides are linear polymers (i.e.: each monomer is connected to at most two other monomers). The sequence of their monomers effectively encodes information. The transfers of information described by the central dogma ideally are faithful, deterministic transfers, wherein one biopolymer's sequence is used as a template for the construction of another biopolymer with a sequence that is entirely dependent on the original biopolymer's sequence.\n\nIn the sense that DNA replication must occur if genetic material is to be provided for the progeny of any cell, whether somatic or reproductive, the copying from DNA to DNA arguably is the fundamental step in the central dogma. A complex group of proteins called the replisome performs the replication of the information from the parent strand to the complementary daughter strand.\n\nThe replisome comprises:\n\nThis process typically takes place during S phase of the cell cycle.\n\nTranscription is the process by which the information contained in a section of DNA is replicated in the form of a newly assembled piece of messenger RNA (mRNA). Enzymes facilitating the process include RNA polymerase and transcription factors. In eukaryotic cells the primary transcript is pre-mRNA. Pre-mRNA must be processed for translation to proceed. Processing includes the addition of a 5' cap and a poly-A tail to the pre-mRNA chain, followed by splicing. Alternative splicing occurs when appropriate, increasing the diversity of the proteins that any single mRNA can produce. The product of the entire transcription process (that began with the production of the pre-mRNA chain) is a mature mRNA chain.\n\nThe mature mRNA finds its way to a ribosome, where it gets translated. In prokaryotic cells, which have no nuclear compartment, the processes of transcription and translation may be linked together without clear separation. In eukaryotic cells, the site of transcription (the cell nucleus) is usually separated from the site of translation (the cytoplasm), so the mRNA must be transported out of the nucleus into the cytoplasm, where it can be bound by ribosomes. The ribosome reads the mRNA triplet codons, usually beginning with an AUG (adenine−uracil−guanine), or initiator methionine codon downstream of the ribosome binding site. Complexes of initiation factors and elongation factors bring aminoacylated transfer RNAs (tRNAs) into the ribosome-mRNA complex, matching the codon in the mRNA to the anti-codon on the tRNA. Each tRNA bears the appropriate amino acid residue to add to the polypeptide chain being synthesised. As the amino acids get linked into the growing peptide chain, the chain begins folding into the correct conformation. Translation ends with a stop codon which may be a UAA, UGA, or UAG triplet.\n\nThe mRNA does not contain all the information for specifying the nature of the mature protein. The nascent polypeptide chain released from the ribosome commonly requires additional processing before the final product emerges. For one thing, the correct folding process is complex and vitally important. For most proteins it requires other chaperone proteins to control the form of the product. Some proteins then excise internal segments from their own peptide chains, splicing the free ends that border the gap; in such processes the inside \"discarded\" sections are called inteins. Other proteins must be split into multiple sections without splicing. Some polypeptide chains need to be cross-linked, and others must be attached to cofactors such as haem (heme) before they become functional.\n\n<br>\n\nReverse transcription is the transfer of information from RNA to DNA (the reverse of normal transcription). This is known to occur in the case of retroviruses, such as HIV, as well as in eukaryotes, in the case of retrotransposons and telomere synthesis.\nIt is the process by which genetic information from RNA gets transcribed into new DNA.\n\nRNA replication is the copying of one RNA to another. Many viruses replicate this way. The enzymes that copy RNA to new RNA, called RNA-dependent RNA polymerases, are also found in many eukaryotes where they are involved in RNA silencing.\n\nRNA editing, in which an RNA sequence is altered by a complex of proteins and a \"guide RNA\", could also be seen as an RNA-to-RNA transfer.\n\nDirect translation from DNA to protein has been demonstrated in a cell-free system (i.e. in a test tube), using extracts from \"E. coli\" that contained ribosomes, but not intact cells. These cell fragments could synthesize proteins from single-stranded DNA templates isolated from other organisms (e,g., mouse or toad), and neomycin was found to enhance this effect. However, it was unclear whether this mechanism of translation corresponded specifically to the genetic code.\n\nAfter protein amino acid sequences have been translated from nucleic acid chains, they can be edited by appropriate enzymes. Although this is a form of protein affecting protein sequence, not explicitly covered by the central dogma, there are not many clear examples where the associated concepts of the two fields have much to do with each other.\n\nAn intein is a \"parasitic\" segment of a protein that is able to excise itself from the chain of amino acids as they emerge from the ribosome and rejoin the remaining portions with a peptide bond in such a manner that the main protein \"backbone\" does not fall apart. This is a case of a protein changing its own primary sequence from the sequence originally encoded by the DNA of a gene. Additionally, most inteins contain a homing endonuclease or HEG domain which is capable of finding a copy of the parent gene that does not include the intein nucleotide sequence. On contact with the intein-free copy, the HEG domain initiates the DNA double-stranded break repair mechanism. This process causes the intein sequence to be copied from the original source gene to the intein-free gene. This is an example of protein directly editing DNA sequence, as well as increasing the sequence's heritable propagation.\n\nVariation in methylation states of DNA can alter gene expression levels significantly. Methylation variation usually occurs through the action of DNA methylases. When the change is heritable, it is considered epigenetic. When the change in information status is not heritable, it would be a somatic epitype. The effective information content has been changed by means of the actions of a protein or proteins on DNA, but the primary DNA sequence is not altered.\n\nPrions are proteins of particular amino acid sequences in particular conformations. They propagate themselves in host cells by making conformational changes in other molecules of protein with the same amino acid sequence, but with a different conformation that is functionally important or detrimental to the organism. Once the protein has been transconformed to the prion folding it changes function. In turn it can convey information into new cells and reconfigure more functional molecules of that sequence into the alternate prion form. In some types of prion in fungi this change is continuous and direct; the information flow is Protein → Protein.\n\nSome scientists such as Alain E. Bussard and Eugene Koonin have argued that prion-mediated inheritance violates the central dogma of molecular biology. However, Rosalind Ridley in \"Molecular Pathology of the Prions\" (2001) has written that \"The prion hypothesis is not heretical to the central dogma of molecular biology—that the information necessary to manufacture proteins is encoded in the nucleotide sequence of nucleic acid—because it does not claim that proteins replicate. Rather, it claims that there is a source of information within protein molecules that contributes to their biological function, and that this information can be passed on to other molecules.\"\n\nJames A. Shapiro argues that a superset of these examples should be classified as natural genetic engineering and are sufficient to falsify the central dogma. While Shapiro has received a respectful hearing for his view, his critics have not been convinced that his reading of the central dogma is in line with what Crick intended.\nIn his autobiography, \"\", Crick wrote about his choice of the word \"dogma\" and some of the problems it caused him:\n\"I called this idea the central dogma, for two reasons, I suspect. I had already used the obvious word hypothesis in the sequence hypothesis, and in addition I wanted to suggest that this new assumption was more central and more powerful. ... As it turned out, the use of the word dogma caused almost more trouble than it was worth. Many years later Jacques Monod pointed out to me that I did not appear to understand the correct use of the word dogma, which is a belief \"that cannot be doubted\". I did apprehend this in a vague sort of way but since I thought that \"all\" religious beliefs were without foundation, I used the word the way I myself thought about it, not as most of the world does, and simply applied it to a grand hypothesis that, however plausible, had little direct experimental support.\"\nSimilarly, Horace Freeland Judson records in \"The Eighth Day of Creation\":\n\n\"My mind was, that a dogma was an idea for which there was \"no reasonable evidence\". You see?!\" And Crick gave a roar of delight. \"I just didn't \"know\" what dogma \"meant\". And I could just as well have called it the 'Central Hypothesis,' or — you know. Which is what I meant to say. Dogma was just a catch phrase.\"\n\nThe Weismann barrier, proposed by August Weismann in 1892, distinguishes between the \"immortal\" germ cell lineages (the germ plasm) which produce gametes and the \"disposable\" somatic cells. Hereditary information moves only from germline cells to somatic cells (that is, somatic mutations are not inherited). This, before the discovery of the role or structure of DNA, does not predict the central dogma, but does anticipate its gene-centric view of life, albeit in non-molecular terms.\n\n\n\n", "id": "68206", "title": "Central dogma of molecular biology"}
{"url": "https://en.wikipedia.org/wiki?curid=964229", "text": "Protoplast\n\nProtoplast, from ancient Greek (\"prōtóplastos\", \"first-formed\"), in biology, it was proposed by Hanstein (1880) to refer to the entire cell, excluding the cell wall, but currently has several definitions:\n\nCell walls are made of a variety of polysaccharides. Protoplasts can be made by degrading cell walls with a mixture of the appropriate polysaccharide-degrading enzymes:\n\nDuring and subsequent to digestion of the cell wall, the protoplast becomes very sensitive to osmotic stress. This means cell wall digestion and protoplast storage must be done in an isotonic solution to prevent rupture of the plasma membrane.\n\nProtoplasts can be used to study membrane biology, including the uptake of macromolecules and viruses . These are also used in somaclonal variation.\n\nProtoplasts are widely used for DNA transformation (for making genetically modified organisms), since the cell wall would otherwise block the passage of DNA into the cell. In the case of plant cells, protoplasts may be regenerated into whole plants first by growing into a group of plant cells that develops into a callus and then by regeneration of shoots (caulogenesis) from the callus using plant tissue culture methods. Growth of protoplasts into callus and regeneration of shoots requires the proper balance of plant growth regulators in the tissue culture medium that must be customized for each species of plant. Unlike protoplasts from vascular plants, protoplasts from mosses, such as \"Physcomitrella patens\", do not need phytohormones for regeneration, nor do they form a callus during regeneration. Instead, they regenerate directly into the filamentous protonema, mimicking a germinating moss spore.\n\nProtoplasts may also be used for plant breeding, using a technique called protoplast fusion. Protoplasts from different species are induced to fuse by using an electric field or a solution of polyethylene glycol. This technique may be used to generate somatic hybrids in tissue culture.\n\nAdditionally, protoplasts of plants expressing fluorescent proteins in certain cells may be used for Fluorescence Activated Cell Sorting (FACS), where only cells fluorescing a selected wavelength are retained. Among other things, this technique is used to isolate specific cell types (e.g., guard cells from leaves, pericycle cells from roots) for further investigations, such as transcriptomics.\n\n", "id": "964229", "title": "Protoplast"}
{"url": "https://en.wikipedia.org/wiki?curid=859981", "text": "Biochip\n\nIn molecular biology, biochips are essentially miniaturized laboratories that can perform hundreds or thousands of simultaneous biochemical reactions. Biochips enable researchers to quickly screen large numbers of biological analytes for a variety of purposes, from disease diagnosis to detection of bioterrorism agents. Digital microfluidic biochips have become one of the most promising technologies in many biomedical fields. In a digital microfluidic biochip, a group of (adjacent) cells in the microfluidic array can be configured to work as storage, functional operations, as well as for transporting fluid droplets dynamically.\n\nThe development started with early work on the underlying sensor technology. One of the first portable, chemistry-based sensors was the glass pH electrode, invented in 1922 by Hughes. In subsequent years. For example, a K sensor was produced by incorporating valinomycin into a thin membrane.\n\nIn 1953, Watson and Crick announced their discovery of the now familiar double helix structure of DNA molecules and set the stage for genetics research that continues to the present day. The development of sequencing techniques in 1977 by Gilbert and Sanger (working separately) enabled researchers to directly read the genetic codes that provide instructions for protein synthesis. This research showed how hybridization of complementary single oligonucleotide strands could be used as a basis for DNA sensing. Two additional developments enabled the technology used in modern DNA-based. First, in 1983 Kary Mullis invented the polymerase chain reaction (PCR) technique, a method for amplifying DNA concentrations. This discovery made possible the detection of extremely small quantities of DNA in samples. Secondly in 1986 Hood and co-workers devised a method to label DNA molecules with fluorescent tags instead of radiolabels, thus enabling hybridization experiments to be observed optically.\nFigure 1 shows the make up of a typical biochip platform. The actual sensing component (or \"chip\") is just one piece of a complete analysis system Transduction must be done to translate the actual sensing event (DNA binding, oxidation/reduction, \"etc.\") into a format understandable by a computer (voltage, light intensity, mass, \"etc.\"), which then enables additional analysis and processing to produce a final, human-readable output. The multiple technologies needed to make a successful biochip — from sensing chemistry, to microarraying, to signal processing — require a true multidisciplinary approach, making the barrier to entry steep. One of the first commercial biochips was introduced by Affymetrix. Their \"GeneChip\" products contain thousands of individual DNA sensors for use in sensing defects, or single nucleotide polymorphisms (SNPs), in genes such as p53 (a tumor suppressor) and BRCA1 and BRCA2 (related to breast cancer). The chips are produced using microlithography techniques traditionally used to fabricate integrated circuits (see below).\n\nThe microarray — the dense, two-dimensional grid of biosensors — is the critical component of a biochip platform. Typically, the sensors are deposited on a flat substrate, which may either be passive (\"e.g.\" silicon or glass) or active, the latter\nconsisting of integrated electronics or micromechanical devices that perform or assist signal transduction. Surface chemistry is used to covalently bind the sensor molecules to the substrate medium. The fabrication of microarrays is non-trivial and is a major economic and technological hurdle that may\nultimately decide the success of future biochip platforms. The primary manufacturing challenge is the process of placing each sensor at a specific position (typically on a Cartesian grid) on the substrate. Various means exist to achieve the placement, but typically robotic micro-pipetting or micro-printing systems are used to place tiny spots of sensor material on the chip surface. Because each sensor is unique, only a few spots can be placed at a time. The low-throughput nature of this\nprocess results in high manufacturing costs.\n\nFodor and colleagues developed a unique fabrication process (later used by Affymetrix) in which a series of microlithography steps is used to combinatorially synthesize hundreds of thousands of unique, single-stranded DNA sensors on a substrate one nucleotide at a time. One lithography step is needed per base type; thus, a total of four steps is required per nucleotide level. Although this technique is very powerful in that many sensors can be created simultaneously, it is currently only feasible for creating short DNA strands (15–25 nucleotides). Reliability and cost factors limit the number of photolithography steps that can be done. Furthermore, light-directed combinatorial synthesis techniques are not currently possible for proteins or other sensing molecules.\n\nAs noted above, most microarrays consist of a Cartesian grid of sensors. This approach is used chiefly to map or \"encode\" the coordinate of each sensor to its function. Sensors in these arrays typically use a universal signalling technique (\"e.g.\" fluorescence), thus making coordinates their only identifying feature. These arrays must be made using a serial process (\"i.e.\" requiring multiple, sequential steps) to ensure that each sensor is placed at the correct position.\n\n\"Random\" fabrication, in which the sensors are placed at arbitrary positions on the chip, is an alternative to the serial method. The tedious and expensive positioning process is\nnot required, enabling the use of parallelized self-assembly techniques. In this approach, large batches of identical sensors can be produced; sensors from each batch are then combined and assembled into an array. A non-coordinate based encoding scheme must be used to identify each sensor. As the figure shows, such a design was first demonstrated (and later commercialized by Illumina) using functionalized beads placed randomly in the wells of an etched fiber optic cable. Each bead was uniquely encoded with a fluorescent signature. However, this encoding scheme is limited in the number of unique dye combinations that can be used and successfully differentiated.\n\nMicroarrays are not limited to DNA analysis; protein microarrays, antibody microarray, chemical compound microarray can also be produced using biochips. Randox Laboratories Ltd. launched Evidence, the first protein Biochip Array Technology analyzer in 2003. In protein Biochip Array Technology, the biochip replaces the ELISA plate or cuvette as the reaction platform. The biochip is used to simultaneously analyze a panel of related tests in a single sample, producing a patient profile. The patient profile can be used in disease screening, diagnosis, monitoring disease progression or monitoring treatment. Performing multiple analyses simultaneously, described as multiplexing, allows a significant reduction in processing time and the amount of patient sample required. Biochip Array Technology is a novel application of a familiar methodology, using sandwich, competitive and antibody-capture immunoassays. The difference from conventional immunoassays is that, the capture ligands are covalently attached to the surface of the biochip in an ordered array rather than in solution.\n\nIn sandwich assays an enzyme-labelled antibody is used; in competitive assays an enzyme-labelled antigen is used. On antibody-antigen binding a chemiluminescence reaction produces light. Detection is by a charge-coupled device (CCD) camera. The CCD camera is a sensitive and high-resolution sensor able to accurately detect and quantify very low levels of light. The test regions are located using a grid pattern then the chemiluminescence signals are analysed by imaging software to rapidly and simultaneously quantify the individual analytes.\n\nDetails about other array technologies can be found in the following page: Antibody microarray\n\n", "id": "859981", "title": "Biochip"}
{"url": "https://en.wikipedia.org/wiki?curid=21620243", "text": "Affinity electrophoresis\n\nAffinity electrophoresis is a general name for many analytical methods used in biochemistry and biotechnology. Both qualitative and quantitative information may be obtained through affinity electrophoresis. The methods include the so-called mobility shift electrophoresis, charge shift electrophoresis and affinity capillary electrophoresis. The methods are based on changes in the electrophoretic pattern of molecules (mainly macromolecules) through biospecific interaction or complex formation. The interaction or binding of a molecule, charged or uncharged, will normally change the electrophoretic properties of a molecule. Membrane proteins may be identified by a shift in mobility induced by a charged detergent. Nucleic acids or nucleic acid fragments may be characterized by their affinity to other molecules. The methods have been used for estimation of binding constants, as for instance in lectin affinity electrophoresis or characterization of molecules with specific features like glycan content or ligand binding. For enzymes and other ligand-binding proteins, one-dimensional electrophoresis similar to counter electrophoresis or to \"rocket immunoelectrophoresis\", affinity electrophoresis may be used as an alternative quantification of the protein. Some of the methods are similar to affinity chromatography by use of immobilized ligands.\n\nCurrently, there is ongoing research in developing new ways of utilizing the knowledge already associated with affinity electrophoresis to improve its functionality and speed, as well as attempts to improve already established methods and tailor them towards performing specific tasks.\n\nA type of electrophoretic mobility shift assay (AMSA), agarose gel electrophoresis is used to separate protein-bound amino acid complexes from free amino acids. Using a low voltage (~10 V/cm) to minimize the risk for heat damage, electricity is run across an agarose gel.\n\nThis technique utilizes a high voltage () with a 0.5× Tris-borate buffer run across an agarose gel. This method differs from the traditional agarose gel electrophoresis by utilizing a higher voltage to facilitate a shorter run time as well as yield a higher band resolution. Other factors included in developing the technique of rapid agarose gel electrophoresis are gel thickness, and the percentage of agarose within the gel.\n\nBoronate affinity electrophoresis utilizes boronic acid infused acrylimide gels to purify NAD-RNA. This purification allows for researchers to easily measure the kinetic activity of NAD-RNA decapping enzymes.\n\nAffinity capillary electrophoresis utilizes a formulary approach in accordance with the theory of electromigration. This method utilizes the inter-molecular interactions found in a free solution. \"Affinity probes\" consisting of fluorophore-labeled molecules that will bind to target molecules are mixed with the sample being tested. This mixture and its subsequent complexes are then separated through capillary electrophoresis. The principle behind this type of electrophoresis is the mobility of the target molecules being altered by inter-molecular interactions.\n\nAffinity-trap polyacrylamide gel electrophoresis (PAGE) has become one of the most popular methods of protein separation. This is not only due to its separation qualities, but also because it can be used in conjunction with a variety of other analytic methods, such as mass spectrometry, and western blotting. This method utilizes a two-step approach. First, a protein sample is run through a polyacrylamide gel using electrophoresis. Then, the sample is transferred to a different polyacrylamide gel (the affinity-trap gel) where affinity probes are immobilized. The proteins that do not have affinity for the affinity probes pass through the affinity-trap gel, and proteins with affinity for the probes will be \"trapped\" by the immobile affinity probes. These trapped proteins are then visualized and identified using mass spectrometry after in-gel digestion.\n\nPhosphate affinity electrophoresis utilizes an affinity probe which consists of a molecule that binds specifically to divalent phosphate ions in neutral aqueous solution, known as a \"Phos-Tag\". This methods also utilizes a separation gel made of an acrylamide-pendent Phos-Tag monomer that is copolymerized. Phosphorylated proteins migrate slowly in the gel compared to non-phosphorylated proteins. This technique gives the researcher the ability to observe the differences in the phosphorylation states of any given protein.\n\n\n", "id": "21620243", "title": "Affinity electrophoresis"}
{"url": "https://en.wikipedia.org/wiki?curid=2428570", "text": "Immunoelectrophoresis\n\nImmunoelectrophoresis is a general name for a number of biochemical methods for separation and characterization of proteins based on electrophoresis and reaction with antibodies. All variants of immunoelectrophoresis require immunoglobulins, also known as antibodies, reacting with the proteins to be separated or characterized. The methods were developed and used extensively during the second half of the 20th century. In somewhat chronological order: Immunoelectrophoretic analysis (one-dimensional immunoelectrophoresis \"ad modum\" Grabar), crossed immunoelectrophoresis (two-dimensional quantitative immunoelectrophoresis \"ad modum\" Clarke and Freeman or \"ad modum\" Laurell), rocket-immunoelectrophoresis (one-dimensional quantitative immunoelectrophoresis \"ad modum\" Laurell), fused rocket immunoelectrophoresis \"ad modum\" Svendsen and Harboe, affinity immunoelectrophoresis \"ad modum\" Bøg-Hansen.\n\nAgarose as 1% gel slabs of about 1 mm thickness buffered at high pH (around 8.6) is traditionally preferred for the electrophoresis as well as the reaction with antibodies. The agarose was chosen as the gel matrix because it has large pores allowing free passage and separation of proteins, but provides an anchor for the immunoprecipitates of protein and specific antibodies. The high pH was chosen because antibodies are practically immobile at high pH. An electrophoresis equipment with a horizontal cooling plate was normally recommended for the electrophoresis.\n\nImmunoprecipitates may be seen in the wet agarose gel, but are stained with protein stains like Coomassie Brilliant Blue in the dried gel. In contrast to SDS-gel electrophoresis, the electrophoresis in agarose allows native conditions, preserving the native structure and activities of the proteins under investigation, therefore immunoelectrophoresis allows characterization of enzyme activities and ligand binding etc. in addition to electrophoretic separation.\n\nThe immunoelectrophoretic analysis \"ad modum\" Grabar is the classical method of immunoelectrophoresis. Proteins are separated by electrophoresis, then antibodies are applied in a trough next to the separated proteins and immunoprecipitates are formed after a period of diffusion of the separated proteins and antibodies against each other. The introduction of the immunoelectrophoretic analysis gave a great boost to protein chemistry, some of the very first results were the resolution of proteins in biological fluids and biological extracts. Among the important observations made were the great number of different proteins in serum, the existence of several immunoglobulin classes and their electrophoretic heterogeneity.\n\nCrossed immunoelectrophoresis is also called two-dimensional quantitative immunoelectrophoresis \"ad modum\" Clarke and Freeman or \"ad modum\" Laurell. In this method the proteins are first separated during the first dimension electrophoresis, then instead of the diffusion towards the antibodies, the proteins are electrophoresed into an antibody-containing gel in the second dimension. Immunoprecipitation will take place during the second dimension electrophorsis and\nthe immunoprecipitates have a characteristic bell-shape, each precipitate representing one antigen, the position of the precipitate being dependent on the amount of protein as well as the amount of specific antibody in the gel, so relative quantification can be performed. The sensitivity and resolving power of crossed immunoelectrophoresis is than that of the classical immunoelectrophoretic analysis and there are multiple variations of the technique useful for various purposes. Crossed immunoelectrophoresis has been used for studies of proteins in biological fluids, particularly human serum, and biological extracts.\n\nRocket immunoelectrophoresis is one-dimensional quantitative immunoelectrophoresis. The method has been used for quantitation of human serum proteins before automated methods became available.\n\nFused rocket immunoelectrophoresis is a modification of one-dimensional quantitative immunoelectrophorsis used for detailed measurement of proteins in fractions from protein separation experiments.\n\nAffinity immunoelectrophoresis is based on changes in the electrophoretic pattern of proteins through specific interaction or complex formation with other macromolecules or ligands. Affinity immunoelectrophoresis has been used for estimation of binding constants, as for instance with lectins or for characterization of proteins with specific features like glycan content or ligand binding. Some variants of affinity immunoelectrophoresis are similar to affinity chromatography by use of immobilized ligands.\n\nThe open structure of the immunoprecipitate in the agarose gel will allow additional binding of radioactively labeled antibodies to reveal specific proteins. This variation has been used for identification of allergens through reaction with IgE.\n\nTwo factors determine that immunoelectrophoretic methods are not widely used. First they are rather work intensive and require some manual expertise. Second they require rather large amounts of polyclonal antibodies. Today gel electrophoresis followed by electroblotting is the preferred method for protein characterization because its ease of operation, its high sensitivity, and its low requirement for specific antibodies. In addition proteins are separated by gel electrophoresis on the basis of their apparent molecular weight, which is not accomplished by immunoelectrophoresis, but nevertheless immunoelectrophoretic methods are still useful when non-reducing conditions are needed.\n\n", "id": "2428570", "title": "Immunoelectrophoresis"}
{"url": "https://en.wikipedia.org/wiki?curid=1053500", "text": "DNA extraction\n\nDNA isolation is a process of purification of DNA from sample using a combination of physical and chemical methods. The first isolation of DNA was done in 1869 by Friedrich Miescher. Currently it is a routine procedure in molecular biology or forensic analyses. For the chemical method, there are many different kits used for extraction, and selecting the correct one will save time on kit optimization and extraction procedures. PCR sensitivity detection is considered to show the variation between the commercial kits.\n\nThere are three basic and two optional steps in a DNA extraction:\n\nCellular and histone proteins bound to the DNA can be removed either by adding a protease or by having precipitated the proteins with sodium or ammonium acetate, or extracted them with a phenol-chloroform mixture prior to the DNA-precipitation.\n\nAfter isolation, the DNA is dissolved in slightly alkaline buffer, usually in the TE buffer, or in ultra-pure water.\n\nSpecific techniques must be chosen for isolation of DNA from some samples. Typical samples with complicated DNA isolation are:\n\nExtrachromosomal DNA is generally easy to isolate, especially plasmids may be easily isolated by cell lysis followed by precipitation of proteins, which traps chromosomal DNA in insoluble fraction and after centrifugation, plasmid DNA can be purified from soluble fraction.\n\nA Hirt DNA Extraction is an isolation of all extrachromosomal DNA in a mammalian cell. The Hirt extraction process gets rid of the high molecular weight nuclear DNA, leaving only low molecular weight mitochondrial DNA and any viral episomes present in the cell.\n\nA diphenylamine (DPA) indicator will confirm the presence of DNA. This procedure involves chemical hydrolysis of DNA: when heated (e.g. ≥95 °C) in acid, the reaction requires a deoxyribose sugar and therefore is specific for DNA. Under these conditions, the 2-deoxyribose is converted to w-hydroxylevulinyl aldehyde, which reacts with the compound, diphenylamine, to produce a blue-colored compound. DNA concentration can be determined measuring the intensity of absorbance of the solution at the 600 nm with a spectrophotometer and comparing to a standard curve of known DNA concentrations.\n\nMeasuring the intensity of absorbance of the DNA solution at wavelengths 260 nm and 280 nm is used as a measure of DNA purity. DNA absorbs UV light at 260 and 280 nanometres, and aromatic proteins absorb UV light at 280 nm; a pure sample of DNA has a ratio of 1.8 at 260/280 and is relatively free from protein contamination. A DNA preparation that is contaminated with protein will have a 260/280 ratio lower than 1.8.\n\nDNA can be quantified by cutting the DNA with a restriction enzyme, running it on an agarose gel, staining with ethidium bromide or a different stain and comparing the intensity of the DNA with a DNA marker of known concentration.\n\nUsing the Southern blot technique, this quantified DNA can be isolated and examined further using PCR and RFLP analysis. These procedures allow differentiation of the repeated sequences within the genome. It is these techniques which forensic scientists use for comparison, identification, and analysis.\n\n\n\n", "id": "1053500", "title": "DNA extraction"}
{"url": "https://en.wikipedia.org/wiki?curid=21923868", "text": "Oscillating gene\n\nIn molecular biology, an oscillating gene is a gene that is expressed in a rhythmic pattern or in periodic cycles. Oscillating genes are usually circadian and can be identified by periodic changes in the state of an organism. Circadian rhythms, controlled by oscillating genes, have a period of approximately 24 hours. For example, plant leaves opening and closing at different times of the day or the sleep-wake schedule of animals can all include circadian rhythms. Other periods are also possible, such as 29.5 days resulting from circalunar rhythms or 12.4 hours resulting from circatidal rhythms. Oscillating genes include both core clock component genes and output genes. A core clock component gene is a gene necessary for to the pacemaker. However, an output oscillating gene, such as the AVP gene, is rhythmic but not necessary to the pacemaker.\n\nThe first recorded observations of oscillating genes come from the marches of Alexander the Great in the fourth century B.C. At this time, one of Alexander's generals, Androsthenes, wrote that the tamarind tree would open its leaves during the day and close them at nightfall. Until 1729, the rhythms associated with oscillating genes were assumed to be \"passive responses to a cyclic environment\". In 1729, Jean-Jacques d'Ortous de Mairan demonstrated that the rhythms of a plant opening and closing its leaves continued even when placed somewhere where sunlight could not reach it. This was one of the first indications that there was an active element to the oscillations. In 1923, Ingeborg Beling published her paper \"Über das Zeitgedächtnis der Bienen\" (\"On the Time Memory of Bees\") which extended oscillations to animals, specifically bees In 1971, Ronald Konopka and Seymour Benzer discovered that mutations of the PERIOD gene caused changes in the circadian rhythm of flies under constant conditions. They hypothesized that the mutation of the gene was affecting the basic oscillator mechanism. Paul Hardin, Jeffrey Hall, and Michael Rosbash demonstrated that relationship by discovering that within the PERIOD gene, there was a feedback mechanism that controlled the oscillation. The mid-1990s saw an outpouring of discoveries, with CLOCK, CRY, and others being added to the growing list of oscillating genes.\n\nThe primary molecular mechanism behind an oscillating gene is best described as a transcription/translation feedback loop. This loop contains both positive regulators, which increase gene expression, and negative regulators, which decrease gene expression. The fundamental elements of these loops are found across different phyla. In the mammalian circadian clock, for example, transcription factors CLOCK and BMAL1 are the positive regulators. CLOCK and BMAL1 bind to the E-box of oscillating genes, such as Per1, Per2, and Per3 and Cry1 and Cry2, and upregulate their transcription. When the PERs and CRYs form a heterocomplex in the cytoplasm and enter the nucleus again, they inhibit their own transcription. This means that over time the mRNA and protein levels of PERs and CRYs, or any other oscillating gene under this mechanism, will oscillate.\n\nThere also exists a secondary feedback loop, or 'stabilizing loop', which regulates the cyclic expression of Bmal1. This is caused by two nuclear receptors, REV-ERB and ROR, which suppresses and activates Bmal1 transcription, respectively.\n\nIn addition to these feedback loops, post-translational modifications also play a role in changing the characteristics of the circadian clock, such as its period. Without any type of feedback repression, the molecular clock would have a period of just a few hours. Casein kinase members CK1ε and CK1δ were both found to be mammalian protein kinases involved in circadian regulation. Mutations in these kinases are associated with familial advanced sleep phase syndrome (FASPS). In general, phosphorylation is necessary for the degradation of PERs via ubiquitin ligases. In contrast, phosphorylation of BMAL1 via CK2 is important for accumulation of BMAL1.\n\nThe genes provided in this section are only a small number of the vast amount of oscillating genes found in the world. These genes were selected because they were determined to be the some of most important genes in regulating the circadian rhythm of their respective classification.\n\n\n\n\n\n\n", "id": "21923868", "title": "Oscillating gene"}
{"url": "https://en.wikipedia.org/wiki?curid=544641", "text": "CDNA library\n\nA cDNA library is a combination of cloned cDNA (complementary DNA) fragments inserted into a collection of host cells, which together constitute some portion of the transcriptome of the organism and are stored as a \"library\". cDNA is produced from fully transcribed mRNA found in the nucleus and therefore contains only the expressed genes of an organism. Similarly, tissue-specific cDNA libraries can be produced. In eukaryotic cells the mature mRNA is already spliced, hence the cDNA produced lacks introns and can be readily expressed in a bacterial cell. While information in cDNA libraries is a powerful and useful tool since gene products are easily identified, the libraries lack information about enhancers, introns, and other regulatory elements found in a genomic DNA library.\n\ncDNA is created from a mature mRNA from a eukaryotic cell with the use of reverse transcriptase. In eukaryotes, a poly-(A) tail (consisting of a long sequence of adenine nucleotides) distinguishes mRNA from tRNA and rRNA and can therefore be used as a primer site for reverse transcription. This has the problem that not all transcripts, such as those for the histone, encode a poly-A tail.\n\nFirstly, the mRNA is obtained and purified from the rest of the RNAs. Several methods exist for purifying RNA such as trizol extraction and column purification. Column purification is done by using oligomeric dT nucleotide coated resins where only the mRNA having the poly-A tail will bind. The rest of the RNAs are eluted out. The mRNA is eluted by using eluting buffer and some heat to separate the mRNA strands from oligo-dT.\n\nOnce mRNA is purified, \"oligo-dT\" (a short sequence of deoxy-thymidine nucleotides) is tagged as a complementary primer which binds to the poly-A tail providing a free 3'-OH end that can be extended by reverse transcriptase to create the complementary DNA strand. Now, the mRNA is removed by using a RNAse enzyme leaving a single stranded cDNA (sscDNA). This sscDNA is converted into a double stranded DNA with the help of DNA polymerase. However, for DNA polymerase to synthesize a complementary strand a free 3'-OH end is needed. This is provided by the sscDNA itself by generating a \"hairpin loop\" at the 3' end by coiling on itself. The polymerase extends the 3'-OH end and later the loop at 3' end is opened by the scissoring action of \"S nuclease\". Restriction endonucleases and DNA ligase are then used to clone the sequences into bacterial plasmids.\n\nThe cloned bacteria are then selected, commonly through the use of antibiotic selection. Once selected, stocks of the bacteria are created which can later be grown and sequenced to compile the cDNA library.\n\ncDNA libraries are commonly used when reproducing eukaryotic genomes, as the amount of information is reduced to remove the large numbers of non-coding regions from the library. cDNA libraries are used to express eukaryotic genes in prokaryotes. Prokaryotes do not have introns in their DNA and therefore do not possess any enzymes that can cut it out during transcription process. cDNA does not have introns and therefore can be expressed in prokaryotic cells. cDNA libraries are most useful in reverse genetics where the additional genomic information is of less use. Also, it is useful for subsequently isolating the gene that codes for that mRNA.\n\ncDNA library lacks the non-coding and regulatory elements found in genomic DNA. Genomic DNA libraries provide more detailed information about the organism, but are more resource-intensive to generate and maintain.\n\ncDNA molecules can be cloned by using restriction site linkers. Linkers are short, double stranded pieces of DNA (oligodeoxyribonucleotide) about 8 to 12 nucleotide pairs long that include a restriction endonuclease cleavage site e.g. BamHI.\nBoth the cDNA and the linker have blunt ends which can be ligated together using a high concentration of T4 DNA ligase. Then sticky ends are produced in the cDNA molecule by cleaving the cDNA ends (which now have linkers with an incorporated site) with the appropriate endonuclease. A cloning vector (plasmid) is then also cleaved with the appropriate endonuclease. Following \"sticky end\" ligation of the insert into the vector the resulting recombinant DNA molecule is transferred into \"E. coli\" host cell for cloning.\n\n", "id": "544641", "title": "CDNA library"}
{"url": "https://en.wikipedia.org/wiki?curid=21932806", "text": "Prokaryotic initiation factor-1\n\nProkaryotic initiation factor-1 is a prokaryotic initiation factor.\n\nIF1 associates with the 30S ribosomal subunit in the A site and prevents an aminoacyl-tRNA from entering. It modulates IF2 binding to the ribosome by increasing its affinity. It may also prevent the 50S subunit from binding, stopping the formation of the 70S subunit. It also contains a β-domain fold common for nucleic acid binding proteins.\n", "id": "21932806", "title": "Prokaryotic initiation factor-1"}
{"url": "https://en.wikipedia.org/wiki?curid=21932809", "text": "Prokaryotic initiation factor-2\n\nProkaryotic initiation factor-2 is a prokaryotic initiation factor.\n\nIF2 binds to an initiator tRNA and controls the entry of tRNA onto the ribosome. IF2, bound to GTP, binds to the 30S P site. After associating with the 30S subunit, fMet-tRNA binds to the IF2 then IF2 transfers the tRNA into the partial P site. When the 50S subunit joins, it hydrolyzes GTP to GDP and P, causing a conformational change in the IF2 that causes IF2 to release and allow the 70S ribosome to form.\n", "id": "21932809", "title": "Prokaryotic initiation factor-2"}
{"url": "https://en.wikipedia.org/wiki?curid=21932814", "text": "Prokaryotic initiation factor-3\n\nProkaryotic initiation factor-3 is a prokaryotic initiation factor.\n\nIF3 is not universally found in all bacterial species but in \"E. coli\" it is required for the 30S subunit to bind to the initiation site in mRNA. In addition, it has several other jobs including the stabilization of free 30S subunits, enables 30S subunits to bind to mRNA and checks for accuracy against the first aminoacyl-tRNA. It also allows for rapid codon-anticodon pairing for the initiator tRNA to bind quickly to. IF3 is required by the small subunit to form initiation complexes, but has to be released to allow the 50S subunit to bind.\n", "id": "21932814", "title": "Prokaryotic initiation factor-3"}
{"url": "https://en.wikipedia.org/wiki?curid=22275501", "text": "Lexitropsin\n\nLexitropsins are members of a family of semi-synthetic DNA-binding ligands. They are structural analogs of the natural antibiotics netropsin and distamycin. Antibiotics of this group can bind in the minor groove of DNA with different sequence-selectivity. Lexitropsins form a complexes with DNA with stoichiometry 1:1 and 2:1. Based on the 2:1 complexes were obtained ligands with high sequence-selectivity.\n\n", "id": "22275501", "title": "Lexitropsin"}
{"url": "https://en.wikipedia.org/wiki?curid=16018002", "text": "Trans-Spliced Exon Coupled RNA End Determination\n\nTrans-Spliced Exon Coupled RNA End Determination (TEC-RED) is a transcriptomic technique that, like SAGE, allows for the digital detection of messenger RNA sequences. Unlike SAGE, detection and purification of transcripts from the 5’ end of the messenger RNA require the presence of a trans-spliced leader sequence.\n\nSpliced leader sequences are short sequences of non coding RNA, not found within a gene itself, that are attached to the 5’ end of all, or a portion of, mRNAs transcribed in an organism. They have been found in several species to be responsible for separating polycistronic transcripts into single gene mRNAs, and in others to splice onto monocistronic transcripts. The major role of trans-splicing on monocistronic transcripts is largely unknown. It has been proposed that they may act as an independent promoter that aids in tissue specific expression of independent protein isoforms. Spliced leaders have been seen in trypanosomatids, Euglena, flatworms, Caenorhabditis. Some species contain only one spliced leader sequence found on all mRNAs. In C. elegans two are seen and are labeled SL1 and SL2.\n\nTotal RNA is purified from the specimen of interest. Poly A messenger RNA is then purified from total RNA and subsequently translated into cDNA using a reverse transcription reaction. The cDNA produced from the mRNA is labeled using primers homologous to the spliced leader sequences of the organism. In a nine step PCR reaction the cDNAs are concurrently embedded with the BpmI restriction endonuclease site (though any class IIs restriction endonuclease may work) and a biotin label which are present in the primers. These tagged cDNAs are then cleaved 14 bp downstream from the recognition site using BpmI restriction endonuclease and blunt ended with T4 DNA polymerase. The fragments are further purified away from extraneous DNA material by using the biotin labels to bind them to a strepdavidin matrix. They are then ligated to adapter DNA, in six separate reactions, containing six different restriction endonuclease recognition sites. These tags are then amplified by PCR with primers containing a mismatch changing the Bpm1 site to a Xho1 site. The amplicons are concatenated and ligated into a plasmid vector. The clonal vectors are then sequenced and mapped to the genome.\n\nConcatenation of the tags, as developed in 2004, is different from that seen in SAGE. The cleavage of the tags with Xho1 and mixture of the different samples, followed by ligation, form the first concatenation step. The second step uses one of the restriction endonucleases with consensus to the adapter molecule attached to the 3’ end. They are again ligated, and PCR is performed to purify samples for the next joining. The concatenation is continued with the second restriction endonuclease, followed by the third and finally the fourth. This results in the concatamer formed by the six endonuclease ligations containing 32 tags, arranged 5’ to 5’ around the Xho1 site. In SAGE, concatenation takes place after ditags are formed and amplified by PCR. The linkers on the outside of the ditags are cleaved with the enzyme that provided their binding and these sticky end ditags are concatenated randomly and placed into a cloning vector.\n\nThe advantage of TEC-RED over SAGE is that no restriction endonuclease is needed for the initial linker binding. This prevents bias associated with restriction site sequences that will be missing from some genes, as is seen in SAGE. The ability to have a snapshot of specific RNA isoforms allows the deduction of differential regulation of isoforms through alternative selection of promoters. This may also aid in the discernment of expression patterns unique to the SL1 or SL2 sequence. TEC-RED also allows characterization of the 5’ ends of RNA produced and therefore of isoforms that differ by the amino terminal splicing. The technology permits the determination and verification of all known and unknown genes that may be predicted as well as the 5’ splice isoforms or 5’ RNA ends that may be produced. Using TEC-RED in conjunction with SAGE or a modified protocol will allow discernment of the 5’ and 3’ ends of transcripts, respectively. The identification of alternative splice variants, and possibly the relative quantities, containing a trans-spliced leader sequence is therefore possible.\n\nTwo alternate techniques have been described that allow for 5’ tag analysis in organisms that do not have trans-spliced leader sequences. The techniques presented by Toshiyuki et al. and Shin-ichi et al. are called CAGE and 5’ SAGE respectively. CAGE utilizes biotinylated cap-trapper technology to maintain mRNA signal long enough to create and select full length cDNAs, which have adapter sequences ligated on the 5‘ end. 5’ SAGE utilizes oligo-capping technology. Both use their adapter sequence to prime from after the cDNA is created. Both of these methods have disadvantages though. CAGE has shown tags with addition of a guanine on the first position and oligo-capping may lead to sequence bias due to the use of RNA ligase.\n\n\n\n", "id": "16018002", "title": "Trans-Spliced Exon Coupled RNA End Determination"}
{"url": "https://en.wikipedia.org/wiki?curid=22269613", "text": "Prp24\n\nPrp24 (precursor RNA processing, gene 24) is a protein part of the pre-messenger RNA splicing process and aids the binding of U6 snRNA to U4 snRNA during the formation of spliceosomes. Found in eukaryotes from yeast to \"E. coli\", fungi, and humans, Prp24 was initially discovered to be an important element of RNA splicing in 1989. Mutations in Prp24 were later discovered in 1991 to suppress mutations in U4 that resulted in cold-sensitive strains of yeast, indicating its involvement in the reformation of the U4/U6 duplex after the catalytic steps of splicing.\nThe process of spliceosome formation involves the U4 and U6 snRNPs associating and forming a di-snRNP in the cell nucleus. This di-snRNP then recruits another member (U5) to become a tri-snRNP. U6 must then dissociate from U4 to bond with U2 and become catalytically active. Once splicing has been done, U6 must dissociate from the spliceosome and bond back with U4 to restart the cycle.\n\nPrp24 has been shown to promote the binding of U4 and U6 snRNPs. Removing Prp24 results in the accumulation of free U4 and U6, and the subsequent addition of Prp24 regenerates U4/U6 and reduces the amount of free U4 and U6. Naked U6 snRNA is very compact and has little room to form base pairs with other RNA. However, when U6 snRNP associates with proteins such as Prp24, the structure is much more open, thus facilitating the binding to U4. Prp24 is not present in the U6/U4 duplex itself, and it has been suggested that Prp24 must leave the complex in order for proper base pairs to be formed. It has also been suggested that Prp24 may play a role in destabilizing U4/U6 in order for U6 to pair bases with U2.\n\nPrp24 has a molecular weight of 50 kDa and has been shown to contain four RNA recognition motifs (RRMs) and a conserved 12-amino acid sequence at the C-terminus. RRMs 1 and 2 have been shown to be important for high-affinity binding of U6, while RRMs 3 and 4 bind at lower affinity sites on U6. The first three RRMs interact extensively with each other and contain canonical folds that contain a four-stranded beta-sheet and two alpha-helices. The electropositive surface of RRMs 1 and 2 is a RNA annealing domain while the cleft between RRMs 1 and 2 including the beta-sheet face of RRM2 is a sequence-specific RNA binding site. The C-terminal motif is required for association with LSm proteins and contributes to substrate (U6) binding and not the catalytic rate of splicing.\n\nPrp24 interacts with the U6 snRNA via its RRMs. It has been shown through chemical modification testing that nucleotides 39–57 of U6 (40–43 in particular) are involved in binding Prp24.\n\nThe LSm proteins are in a consistent configuration on the U6 RNA. It has been proposed that the LSm proteins and Prp24 interact both physically and functionally and the C-terminal motif of Prp24 is important for this interaction. The binding of Prp24 to U6 is enhanced by the binding of Lsm proteins to U6, as is binding of U4 and U6. It was revealed by electron microscopy that Prp24 may interact with the LSm protein ring at LSm2.\n\nPrp24 has a human homolog, SART3. SART3 is a tumor rejection antigen (SART3 stands for \"squamous cell carcinoma antigen recognized by T cells, gene 3). The RRMs 1 and 2 in yeast are similar to RRMs in human SART3. The C-terminal domain is also highly conserved from yeast to humans. This protein, like Prp24, interacts with the LSm proteins for the recycling of U6 into the U4/U6 snRNP. It has been proposed that SART3 target U6 to a Cajal body or a nuclear inclusion as the site of assembly of the U4/U6 snRNP. SART3 is located on chromosome 12, and a mutation is likely the cause of disseminated superficial actinic porokeratosis.\n\n", "id": "22269613", "title": "Prp24"}
{"url": "https://en.wikipedia.org/wiki?curid=22438135", "text": "Transgenic hydra\n\nCnidarians such as Hydra have become attractive model organisms to study the evolution of immunity. However, despite long-term efforts, stably transgenic animals could not be generated, severely limiting the functional analysis of genes. For analytical purposes, therefore, an important technical breakthrough in the field was the development of a transgenic procedure for generation of stably transgenic lines by embryo microinjection.\n\nHydra polyps are small and transparent which makes it possible to trace single cells in vivo. In addition, transgenic Hydra provide a ready system for generating gain-of-function phenotypes. With the use of transgenes producing dominant-negative versions of proteins, one should be able to obtain loss-of-function phenotypes as well. \nCurrent technology allows generation of reporter constructs using promoters of various Hydra genes fused to fluorescent proteins.\n\nSince transgenic Hydra lines have become an important tool to dissect molecular mechanisms of development, a “Hydra Transgenic Facility” has been established at the Christian-Albrechts-University of Kiel (Germany).\n\n\n", "id": "22438135", "title": "Transgenic hydra"}
{"url": "https://en.wikipedia.org/wiki?curid=8031240", "text": "Relaxase\n\nA relaxase is a single-strand DNA transesterase enzyme produced by some prokaryotes and viruses. Relaxases are responsible for site- and strand-specific nicks in double-stranded DNA. Known relaxases belong to the Rolling Circle Replication (RCR) initiator superfamily of enzymes and fall into two broad classes: replicative (Rep) and mobilization (Mob). The nicks produced by Rep relaxases initiate plasmid or virus RCR. Mob relaxases nick at origin of transfer (oriT) to initiate the process of DNA mobilization and transfer known as bacterial conjugation. Relaxases are so named because the single-stranded DNA nick that they catalyze lead to relaxation of helical tension.\n\nKnown relaxases are metal ion dependent tyrosine transesterases. This means that they use a metal ion to aid the transfer of an ester bond from the DNA phosphodiester backbone to a catalytic tyrosine side chain, resulting in a long-lived covalent phosphotyrosine intermediate that essentially unified the nicked DNA strand and the enzyme as one molecule. Preliminary reports of relaxase inhibition by small molecules that mimic intermediates of this reaction were first reported in 2007. Such inhibition has implications related to preventing the propagation of antibiotic resistance in clinical settings.\n\nThe first relaxase x-ray crystal and NMR structures - of Rep relaxases from tomato yellow leaf curl virus (TYLCV) and adeno associated virus serotype 5 (AAV-5) - were solved in 2002. These revealed compact molecules composed of five-stranded, antiparallel beta sheet cores and peripheral alpha helices. A histidine-rich motif, previously identified by sequence conservation, was shown to be a metal ion binding site located on the beta sheet core, nearby the carboxy-terminal catalytic tyrosine residue. Later structures of the Mob relaxases TrwC from plasmid R388 and TraI from the F-plasmid confirmed that the Mob and Rep classes are evolutionarily related to one another through circular permutation. This means that they share a general fold, but the amino-terminal sequence of one is homologous to the C-terminus of the other, and vice versa. Thus the Catalytic tyrosines of TraI and TrwC are amino-terminal rather than carboxy-terminal.\n\nRelaxase nomenclature is varied. In conjugative bacterial plasmids, Mob-class relaxases go by names such as TraI (in plasmid RP4), VirD2 (pTi), TrwC (R388), TraI (F-plasmid), MobB (CloDF13), or TrsK (pGO1).\n\n", "id": "8031240", "title": "Relaxase"}
{"url": "https://en.wikipedia.org/wiki?curid=22555068", "text": "PstI\n\nPstI is a type II restriction endonuclease isolated from the Gram negative species, \"Providencia stuartii\".\n\nPstI cleaves DNA at the recognition sequence 5′-CTGCA/G-3′ generating fragments with 3′-cohesive termini. This cleavage yields sticky ends 4 base pairs long. PstI is catalytically active as a dimer. The two subunits are related by a 2-fold symmetry axis which in the complex with the substrate coincides with the dyad axis of the recognition sequence. It has a molecular weight of 69,500 and contains 54 positive and 41 negatively charged residues.\n\nThe PstI restriction/modification (R/M) system has two components: a restriction enzyme that cleaves foreign DNA, and a methyltransferase which protect native DNA strands via histone methylation. The combination of both provide a defense mechanism against invading viruses. The methyltransferase and endonuclease are encoded as two separate proteins and act independently. In the PstI system, the genes are encoded on opposite strands and hence must be transcribed divergently from separate promoters. The transcription initiation sites are separated by only 70 base pairs. A delay in the expression of the endonuclease relative to methylase is due to the inherent differences of the two proteins. The endonuclease is a dimer, requiring a second step for assembly, whereas the methylase is a monomer.\n\nPstI is functionally equivalent to BsuBI. Both enzymes recognize the target sequence 5'CTGCAG. The enzyme systems have similar methyltransferases (41% amino acid identity), restriction endonucleases (46% amino acid identity), and genetic makeup (58% nucleotide identity). These observations suggest a shared evolutionary history.\n\nWhen examining the preferential double strand cleavage of DNA, the restriction endonuclease PstI bind to pSM1 plasmid DNA.\n\nPstI is a useful enzyme for DNA cloning as it provides a selective system for generating hybrid DNA molecules. These hybrid DNA molecules can be then cleaved at the regenerated PstI sites. Its use is not limited to molecular cloning; it is also used in restriction site mapping, genotyping, southern blotting, restriction fragment length polymorphism (RFLP) and SNP. It is also an isoschizomer restriction enzyme SalPI from \"Streptomyces albus P\".\n\nPstI preferentially cleaves purified pSM1 DNA without being influenced by the superhelicity of the substrate. However, it is not known whether the effects of this cleavage occurs upon binding to the recognition site or DNA scission. Its differential cleavage rates at different restriction sites is due to the five features of duplex structure. The proximity to the ends in linear DNA molecule, variation in DNA sequence within the recognition sites for enzymes, short distance between regions of unusual DNA sequences and recognition sites, and lastly the special structures such as loops and hairpins. The collective effect of these five factors could affect the accessibility of the restriction enzyme to its recognition sites.\n", "id": "22555068", "title": "PstI"}
