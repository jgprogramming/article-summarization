{"url": "https://en.wikipedia.org/wiki?curid=50773876", "text": "Algorithm selection\n\nAlgorithm selection (sometimes also called per-instance algorithm selection or offline algorithm selection) is a meta-algorithmic technique to choose an algorithm from a portfolio on an instance-by-instance basis. It is motivated by the observation that on many practical problems, algorithms have different performances. That is, while one algorithm performs well on some instances, it performs poorly on others and vice versa for another algorithm. If we can identify when to use which algorithm, we can get the best of both worlds and improve overall performance. This is what algorithm selection aims to do. The only prerequisite for applying algorithm selection techniques is that there exists (or that there can be constructed) a set of complementary algorithms.\n\nGiven a portfolio formula_1 of algorithms formula_2, a set of instances formula_3 and a cost metric formula_4, the algorithm selection problem consists of finding a mapping formula_5 from instances formula_6 to algorithms formula_1 such that the cost formula_8 across all instances is optimized.\n\nA well-known application of algorithm selection is the Boolean satisfiability problem. Here, the portfolio of algorithms is a set of (complementary) SAT solvers, the instances are Boolean formulas, the cost metric is for example average runtime or number of unsolved instances. So, the goal is to select a well-performing SAT solver for each individual instance. In the same way, algorithm selection can be applied to many other formula_9-hard problems (such as mixed integer programming, CSP, AI planning, TSP, MAXSAT, QBF and answer set programming). Competition-winning systems in SAT are SATzilla, 3S and CSHC\n\nIn machine learning, algorithm selection is better known as meta-learning. The portfolio of algorithms consists of machine learning algorithms (e.g., Random Forest, SVM, DNN), the instances are data sets and the cost metric is for example the error rate. So, the goal is to predict which machine learning algorithm will have a small error on each data set.\n\nThe algorithm selection problem is mainly solved with machine learning techniques. By representing the problem instances by numerical features formula_10, algorithm selection can be seen as a multi-class classification problem by learning a mapping formula_11 for a given instance formula_12.\n\nInstance features are numerical representations of instances. For example, we can count the number of variables, clauses, average clause length for Boolean formulas, or number of samples, features, class balance for ML data sets to get an impression about their characteristics.\n\nWe distinguish between two kinds of features: \n\nDepending on the used performance metric formula_13, feature computation can be associated with costs.\nFor example, if we use running time as performance metric, we include the time to compute our instance features into the performance of an algorithm selection system.\nSAT solving is a concrete example, where such feature costs cannot be neglected, since instance features for CNF formulas can be either very cheap (e.g., to get the number of variables can be done in constant time for CNFs in the DIMACs format) or very expensive (e.g., graph features which can cost tens or hundreds of seconds).\n\nIt is important to take the overhead of feature computation into account in practice in such scenarios; otherwise a misleading impression of the performance of the algorithm selection approach is created. For example, if the decision which algorithm to choose can be made with prefect accuracy, but the features are the running time of the portfolio algorithms, there is no benefit to the portfolio approach. This would not be obvious if feature costs were omitted.\n\nOne of the first successful algorithm selection approaches predicted the performance of each algorithm formula_14 and selecting the algorithm with the best predicted performance formula_15 for a new instance formula_12.\n\nA common assumption is that the given set of instances formula_6 can be clustered into homogeneous subsets \nand for each of these subsets, there is one well-performing algorithm for all instances in there.\nSo, the training consists of identifying the homogeneous clusters via an unsupervised clustering approach and associating an algorithm with each cluster.\nA new instance is assigned to a cluster and the associated algorithm selected.\n\nA more modern approach is cost-sensitive hierarchical clustering using supervised learning to identify the homogeneous instance subsets.\n\nA common approach for multi-class classification is to learn pairwise models between every pair of classes (here algorithms) \nand choose the class that was predicted most often by the pairwise models.\nWe can weight the instances of the pairwise prediction problem by the performance difference between the two algorithms.\nThis is motivated by the fact that we care most about getting predictions with large differences correct, but the penalty for an incorrect prediction is small if there is almost no performance difference.\nTherefore, each instance formula_12 for training a classification model formula_19 vs formula_20 is associated with a cost formula_21.\n\nThe algorithm selection problem can be effectively applied under the following assumptions:\n\nAlgorithm selection is not limited to single domains but can be applied to any kind of algorithm if the above requirements are satisfied.\nApplication domains include:\n\n\nFor an extensive list of literature about algorithm selection, we refer to a literature overview.\n\nOnline algorithm selection in Hyper-heuristic refers to switching between different algorithms during the solving process. In contrast, (offline) algorithm selection is an one-shot game where we select an algorithm for a given instance only once.\n\nAn extension of algorithm selection is the per-instance algorithm scheduling problem, in which we do not select only one solver, but we select a time budget for each algorithm on a per-instance base. This approach improves the performance of selection systems in particular if the instance features are not very informative and a wrong selection of a single solver is likely.\n\nGiven the increasing importance of parallel computation,\nan extension of algorithm selection for parallel computation is parallel portfolio selection,\nin which we select a subset of the algorithms to simultaneously run in a parallel portfolio.\n\n", "id": "50773876", "title": "Algorithm selection"}
{"url": "https://en.wikipedia.org/wiki?curid=3920550", "text": "Transfer learning\n\nTransfer learning or inductive transfer is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks. This area of research bears some relation to the long history of psychological literature on transfer of learning, although formal ties between the two fields are limited.\n\nThe earliest cited work on transfer in machine learning is attributed to Lorien Pratt, who formulated the discriminability-based transfer (DBT) algorithm in 1993. \n\nIn 1997, the journal \"Machine Learning\" published a special issue devoted to transfer learning, and by 1998, the field had advanced to include multi-task learning, along with a more formal analysis of its theoretical foundations. \"Learning to Learn\", edited by Pratt and Sebastian Thrun, is a 2012 review of the subject.\n\nTransfer learning has also been applied in cognitive science, with the journal \"Connection Science\"\npublishing a special issue on reuse of neural networks through transfer in 1996.\n\nNotably, scientists have developed algorithms for transfer learning in Markov logic networks and Bayesian networks. Researchers have also applied techniques for transfer to problems in text classification, and spam filtering.\n\n", "id": "3920550", "title": "Transfer learning"}
{"url": "https://en.wikipedia.org/wiki?curid=460689", "text": "Evolutionary programming\n\nEvolutionary programming is one of the four major evolutionary algorithm paradigms. It is similar to genetic programming, but the structure of the program to be optimized is fixed, while its numerical parameters are allowed to evolve.\n\nIt was first used by Lawrence J. Fogel in the US in 1960 in order to use simulated evolution as a learning process aiming to generate artificial intelligence. Fogel used finite-state machines as predictors and evolved them.\nCurrently evolutionary programming is a wide evolutionary computing dialect with no fixed structure or (representation), in contrast with some of the other dialects. It is becoming harder to distinguish from evolutionary strategies.\n\nIts main variation operator is mutation; members of the population are viewed as part of a specific species rather than members of the same species therefore each parent generates an offspring, using a (μ + μ) survivor selection.\n\n\n\n", "id": "460689", "title": "Evolutionary programming"}
{"url": "https://en.wikipedia.org/wiki?curid=40254", "text": "Genetic algorithm\n\nIn computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on bio-inspired operators such as mutation, crossover and selection.\n\nIn a genetic algorithm, a population of candidate solutions (called individuals, creatures, or phenotypes) to an optimization problem is evolved toward better solutions. Each candidate solution has a set of properties (its chromosomes or genotype) which can be mutated and altered; traditionally, solutions are represented in binary as strings of 0s and 1s, but other encodings are also possible.\n\nThe evolution usually starts from a population of randomly generated individuals, and is an iterative process, with the population in each iteration called a \"generation\". In each generation, the fitness of every individual in the population is evaluated; the fitness is usually the value of the objective function in the optimization problem being solved. The more fit individuals are stochastically selected from the current population, and each individual's genome is modified (recombined and possibly randomly mutated) to form a new generation. The new generation of candidate solutions is then used in the next iteration of the algorithm. Commonly, the algorithm terminates when either a maximum number of generations has been produced, or a satisfactory fitness level has been reached for the population.\n\nA typical genetic algorithm requires:\n\n\nA standard representation of each candidate solution is as an array of bits. Arrays of other types and structures can be used in essentially the same way. The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size, which facilitates simple crossover operations. Variable length representations may also be used, but crossover implementation is more complex in this case. Tree-like representations are explored in genetic programming and graph-form representations are explored in evolutionary programming; a mix of both linear chromosomes and trees is explored in gene expression programming.\n\nOnce the genetic representation and the fitness function are defined, a GA proceeds to initialize a population of solutions and then to improve it through repetitive application of the mutation, crossover, inversion and selection operators.\n\nThe population size depends on the nature of the problem, but typically contains several hundreds or thousands of possible solutions. Often, the initial population is generated randomly, allowing the entire range of possible solutions (the \"search space\"). Occasionally, the solutions may be \"seeded\" in areas where optimal solutions are likely to be found.\n\nDuring each successive generation, a portion of the existing population is selected to breed a new generation. Individual solutions are selected through a \"fitness-based\" process, where fitter solutions (as measured by a fitness function) are typically more likely to be selected. Certain selection methods rate the fitness of each solution and preferentially select the best solutions. Other methods rate only a random sample of the population, as the former process may be very time-consuming.\n\nThe fitness function is defined over the genetic representation and measures the \"quality\" of the represented solution. The fitness function is always problem dependent. For instance, in the knapsack problem one wants to maximize the total value of objects that can be put in a knapsack of some fixed capacity. A representation of a solution might be an array of bits, where each bit represents a different object, and the value of the bit (0 or 1) represents whether or not the object is in the knapsack. Not every such representation is valid, as the size of objects may exceed the capacity of the knapsack. The \"fitness\" of the solution is the sum of values of all objects in the knapsack if the representation is valid, or 0 otherwise.\n\nIn some problems, it is hard or even impossible to define the fitness expression; in these cases, a simulation may be used to determine the fitness function value of a phenotype (e.g. computational fluid dynamics is used to determine the air resistance of a vehicle whose shape is encoded as the phenotype), or even interactive genetic algorithms are used.\n\nThe next step is to generate a second generation population of solutions from those selected through a combination of genetic operators: crossover (also called recombination), and mutation.\n\nFor each new solution to be produced, a pair of \"parent\" solutions is selected for breeding from the pool selected previously. By producing a \"child\" solution using the above methods of crossover and mutation, a new solution is created which typically shares many of the characteristics of its \"parents\". New parents are selected for each new child, and the process continues until a new population of solutions of appropriate size is generated.\nAlthough reproduction methods that are based on the use of two parents are more \"biology inspired\", some research suggests that more than two \"parents\" generate higher quality chromosomes.\n\nThese processes ultimately result in the next generation population of chromosomes that is different from the initial generation. Generally the average fitness will have increased by this procedure for the population, since only the best organisms from the first generation are selected for breeding, along with a small proportion of less fit solutions. These less fit solutions ensure genetic diversity within the genetic pool of the parents and therefore ensure the genetic diversity of the subsequent generation of children.\n\nOpinion is divided over the importance of crossover versus mutation. There are many references in Fogel (2006) that support the importance of mutation-based search.\n\nAlthough crossover and mutation are known as the main genetic operators, it is possible to use other operators such as regrouping, colonization-extinction, or migration in genetic algorithms.\n\nIt is worth tuning parameters such as the mutation probability, crossover probability and population size to find reasonable settings for the problem class being worked on. A very small mutation rate may lead to genetic drift (which is non-ergodic in nature). A recombination rate that is too high may lead to premature convergence of the genetic algorithm. A mutation rate that is too high may lead to loss of good solutions, unless elitist selection is employed.\n\nIn addition to the main operators above, other heuristics may be employed to make the calculation faster or more robust. The \"speciation\" heuristic penalizes crossover between candidate solutions that are too similar; this encourages population diversity and helps prevent premature convergence to a less optimal solution. \n\nThis generational process is repeated until a termination condition has been reached. Common terminating conditions are:\n\n\nGenetic algorithms are simple to implement, but their behavior is difficult to understand. In particular it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems. The building block hypothesis (BBH) consists of:\n\n\nGoldberg describes the heuristic as follows:\n\nDespite the lack of consensus regarding the validity of the building-block hypothesis, it has been consistently evaluated and used as reference throughout the years. Many estimation of distribution algorithms, for example, have been proposed in an attempt to provide an environment in which the hypothesis would hold. Although good results have been reported for some classes of problems, skepticism concerning the generality and/or practicality of the building-block hypothesis as an explanation for GAs efficiency still remains. Indeed, there is a reasonable amount of work that attempts to understand its limitations from the perspective of estimation of distribution algorithms.\n\nThere are limitations of the use of a genetic algorithm compared to alternative optimization algorithms:\n\n\nThe simplest algorithm represents each chromosome as a bit string. Typically, numeric parameters can be represented by integers, though it is possible to use floating point representations. The floating point representation is natural to evolution strategies and evolutionary programming. The notion of real-valued genetic algorithms has been offered but is really a misnomer because it does not really represent the building block theory that was proposed by John Henry Holland in the 1970s. This theory is not without support though, based on theoretical and experimental results (see below). The basic algorithm performs crossover and mutation at the bit level. Other variants treat the chromosome as a list of numbers which are indexes into an instruction table, nodes in a linked list, hashes, objects, or any other imaginable data structure. Crossover and mutation are performed so as to respect data element boundaries. For most data types, specific variation operators can be designed. Different chromosomal data types seem to work better or worse for different specific problem domains.\n\nWhen bit-string representations of integers are used, Gray coding is often employed. In this way, small changes in the integer can be readily affected through mutations or crossovers. This has been found to help prevent premature convergence at so called \"Hamming walls\", in which too many simultaneous mutations (or crossover events) must occur in order to change the chromosome to a better solution.\n\nOther approaches involve using arrays of real-valued numbers instead of bit strings to represent chromosomes. Results from the theory of schemata suggest that in general the smaller the alphabet, the better the performance, but it was initially surprising to researchers that good results were obtained from using real-valued chromosomes. This was explained as the set of real values in a finite population of chromosomes as forming a \"virtual alphabet\" (when selection and recombination are dominant) with a much lower cardinality than would be expected from a floating point representation.\n\nAn expansion of the Genetic Algorithm accessible problem domain can be obtained through more complex encoding of the solution pools by concatenating several types of heterogenously encoded genes into one chromosome. This particular approach allows for solving optimization problems that require vastly disparate definition domains for the problem parameters. For instance, in problems of cascaded controller tuning, the internal loop controller structure can belong to a conventional regulator of three parameters, whereas the external loop could implement a linguistic controller (such as a fuzzy system) which has an inherently different description. This particular form of encoding requires a specialized crossover mechanism that recombines the chromosome by section, and it is a useful tool for the modelling and simulation of complex adaptive systems, especially evolution processes.\n\nA practical variant of the general process of constructing a new population is to allow the best organism(s) from the current generation to carry over to the next, unaltered. This strategy is known as \"elitist selection\" and guarantees that the solution quality obtained by the GA will not decrease from one generation to the next.\n\nParallel implementations of genetic algorithms come in two flavors. Coarse-grained parallel genetic algorithms assume a population on each of the computer nodes and migration of individuals among the nodes. Fine-grained parallel genetic algorithms assume an individual on each processor node which acts with neighboring individuals for selection and reproduction.\nOther variants, like genetic algorithms for online optimization problems, introduce time-dependence or noise in the fitness function.\n\nGenetic algorithms with adaptive parameters (adaptive genetic algorithms, AGAs) is another significant and promising variant of genetic algorithms. The probabilities of crossover (pc) and mutation (pm) greatly determine the degree of solution accuracy and the convergence speed that genetic algorithms can obtain. Instead of using fixed values of \"pc\" and \"pm\", AGAs utilize the population information in each generation and adaptively adjust the \"pc\" and \"pm\" in order to maintain the population diversity as well as to sustain the convergence capacity. In AGA (adaptive genetic algorithm), the adjustment of \"pc\" and \"pm\" depends on the fitness values of the solutions. In \"CAGA\" (clustering-based adaptive genetic algorithm), through the use of clustering analysis to judge the optimization states of the population, the adjustment of \"pc\" and \"pm\" depends on these optimization states.\nIt can be quite effective to combine GA with other optimization methods. GA tends to be quite good at finding generally good global solutions, but quite inefficient at finding the last few mutations to find the absolute optimum. Other techniques (such as simple hill climbing) are quite efficient at finding absolute optimum in a limited region. Alternating GA and hill climbing can improve the efficiency of GA while overcoming the lack of robustness of hill climbing.\n\nThis means that the rules of genetic variation may have a different meaning in the natural case. For instance – provided that steps are stored in consecutive order – crossing over may sum a number of steps from maternal DNA adding a number of steps from paternal DNA and so on. This is like adding vectors that more probably may follow a ridge in the phenotypic landscape. Thus, the efficiency of the process may be increased by many orders of magnitude. Moreover, the inversion operator has the opportunity to place steps in consecutive order or any other suitable order in favour of survival or efficiency. (See for instance or example in travelling salesman problem, in particular the use of an edge recombination operator.)\n\nA variation, where the population as a whole is evolved rather than its individual members, is known as gene pool recombination.\n\nA number of variations have been developed to attempt to improve performance of GAs on problems with a high degree of fitness epistasis, i.e. where the fitness of a solution consists of interacting subsets of its variables. Such algorithms aim to learn (before exploiting) these beneficial phenotypic interactions. As such, they are aligned with the Building Block Hypothesis in adaptively reducing disruptive recombination. Prominent examples of this approach include the mGA, GEMGA and LLGA.\n\nProblems which appear to be particularly appropriate for solution by genetic algorithms include timetabling and scheduling problems, and many scheduling software packages are based on GAs. GAs have also been applied to engineering. Genetic algorithms are often applied as an approach to solve global optimization problems.\n\nAs a general rule of thumb genetic algorithms might be useful in problem domains that have a complex fitness landscape as mixing, i.e., mutation in combination with crossover, is designed to move the population away from local optima that a traditional hill climbing algorithm might get stuck in. Observe that commonly used crossover operators cannot change any uniform population. Mutation alone can provide ergodicity of the overall genetic algorithm process (seen as a Markov chain).\n\nExamples of problems solved by genetic algorithms include: mirrors designed to funnel sunlight to a solar collector, antennae designed to pick up radio signals in space, and walking methods for computer figures.\n\nIn his \"Algorithm Design Manual\", Skiena advises against genetic algorithms for any task:\n\nIn 1950, Alan Turing proposed a \"learning machine\" which would parallel the principles of evolution. Computer simulation of evolution started as early as in 1954 with the work of Nils Aall Barricelli, who was using the computer at the Institute for Advanced Study in Princeton, New Jersey. His 1954 publication was not widely noticed. Starting in 1957, the Australian quantitative geneticist Alex Fraser published a series of papers on simulation of artificial selection of organisms with multiple loci controlling a measurable trait. From these beginnings, computer simulation of evolution by biologists became more common in the early 1960s, and the methods were described in books by Fraser and Burnell (1970) and Crosby (1973). Fraser's simulations included all of the essential elements of modern genetic algorithms. In addition, Hans-Joachim Bremermann published a series of papers in the 1960s that also adopted a population of solution to optimization problems, undergoing recombination, mutation, and selection. Bremermann's research also included the elements of modern genetic algorithms. Other noteworthy early pioneers include Richard Friedberg, George Friedman, and Michael Conrad. Many early papers are reprinted by Fogel (1998).\n\nAlthough Barricelli, in work he reported in 1963, had simulated the evolution of ability to play a simple game, artificial evolution became a widely recognized optimization method as a result of the work of Ingo Rechenberg and Hans-Paul Schwefel in the 1960s and early 1970s – Rechenberg's group was able to solve complex engineering problems through evolution strategies. Another approach was the evolutionary programming technique of Lawrence J. Fogel, which was proposed for generating artificial intelligence. Evolutionary programming originally used finite state machines for predicting environments, and used variation and selection to optimize the predictive logics. Genetic algorithms in particular became popular through the work of John Holland in the early 1970s, and particularly his book \"Adaptation in Natural and Artificial Systems\" (1975). His work originated with studies of cellular automata, conducted by Holland and his students at the University of Michigan. Holland introduced a formalized framework for predicting the quality of the next generation, known as Holland's Schema Theorem. Research in GAs remained largely theoretical until the mid-1980s, when The First International Conference on Genetic Algorithms was held in Pittsburgh, Pennsylvania.\n\nIn the late 1980s, General Electric started selling the world's first genetic algorithm product, a mainframe-based toolkit designed for industrial processes. \nIn 1989, Axcelis, Inc. released Evolver, the world's first commercial GA product for desktop computers. The New York Times technology writer John Markoff wrote about Evolver in 1990, and it remained the only interactive commercial genetic algorithm until 1995. Evolver was sold to Palisade in 1997, translated into several languages, and is currently in its 6th version.\n\nGenetic algorithms are a sub-field of:\n\nEvolutionary algorithms is a sub-field of evolutionary computing.\n\n\nSwarm intelligence is a sub-field of evolutionary computing.\n\n\nEvolutionary computation is a sub-field of the metaheuristic methods.\n\n\nMetaheuristic methods broadly fall within stochastic optimisation methods.\n\n\n\n\n\n", "id": "40254", "title": "Genetic algorithm"}
{"url": "https://en.wikipedia.org/wiki?curid=52242050", "text": "Multiplicative weight update method\n\nMultiplicative weight update method is a meta-algorithm. It is an algorithmic technique which \"maintains a distribution on a certain set of interest, and updates it iteratively by multiplying the probability mass of elements by suitably chosen factors based on feedback obtained by running another algorithm on the distribution\". It was discovered repeatedly in very diverse fields such as machine learning (AdaBoost, Winnow, Hedge), optimization (solving linear programs), theoretical computer science (devising fast algorithm for LPs and SDPs), and game theory.\n\n\"Multiplicative weights\" implies the iterative rule used in algorithms derived from the multiplicative weight update method. It is given with different names in the different fields where it was discovered or rediscovered.\n\nThe earliest known version of this technique was in an algorithm named \"fictitious play\" which was proposed in game theory in the early 1950s. Grigoriadis and Khachiyan applied a randomized variant of \"fictitious play\" to solve two-player zero-sum games efficiently using the multiplicative weights algorithm. In this case, player allocates higher weight to the actions that had a better outcome and choose his strategy relying on these weights. In machine learning, Littlestone applied the earliest form of the multiplicative weights update rule in his famous winnow algorithm, which is similar to Minsky and Papert's earlier perceptron learning algorithm. Later, he generalized the winnow algorithm to weighted majority algorithm. Freund and Schapire followed his steps and generalized the winnow algorithm in the form of hedge algorithm.\n\nThe multiplicative weights algorithm is also widely applied in computational geometry such as Clarkson's algorithm for linear programming (LP) with a bounded number of variables in linear time. Later, Bronnimann and Goodrich employed analogous methods to find set covers for hypergraphs with small VC dimension.\n\nIn operation research and on-line statistical decision making problem field, the weighted majority algorithm and its more complicated versions have been found independently.\n\nIn computer science field, some researchers have previously observed the close relationships between multiplicative update algorithms used in different contexts. Young discovered the similarities between fast LP algorithms and Raghavan's method of pessimistic estimators for derandomization of randomized rounding algorithms; Klivans and Servedio linked boosting algorithms in learning theory to proofs of Yao's XOR Lemma; Garg and Khandekar defined a common framework for convex optimization problems that contains Garg-Konemann and Plotkin-Shmoys-Tardos as subcases.\n\nA binary decision needs to be made based on n experts’ opinions to attain an associated payoff. In the first round, all experts’ opinions have the same weight. The decision maker will make the first decision based on the majority of the experts' prediction. Then, in each successive round, the decision maker will repeatedly update the weight of each expert's opinion depending on the correctness of his prior predictions. Real life examples includes predicting if it is rainy tomorrow or if the stock market will go up or go down.\n\nGiven a sequential game played between an adversary and an aggregator who is advised by N experts, the goal is for the aggregator to make as few mistakes as possible. Assume there is an expert among the N experts who always gives the correct prediction. In the halving algorithm, only the consistent experts are retained. Experts who make mistake will all be dismissed. For every decision, the aggregator decides by taking a majority vote among the remaining experts. Therefore, every time the aggregator makes a mistake, at least half of the remaining experts are dismissed. The aggregator makes at most mistakes.\n\nUnlike halving algorithm which dismisses experts who have made mistakes, weighted majority algorithm discounts their advice. Given the same \"expert advice\" setup, suppose we have n decisions, and we need to select one decision for each loop. In each loop, every decision incurs a cost. All costs will be revealed after making the choice. The cost is 0 if the expert is correct, and 1 otherwise. this algorithm's goal is to limit its cumulative losses to roughly the same as the best of experts.\nThe very first algorithm that makes choice based on majority vote every iteration does not work since the majority of the experts can be wrong consistently every time. The weighted majority algorithm corrects above trivial algorithm by keeping a weight of experts instead of fixing the cost at either 1 or 0. This would make fewer mistakes compared to halving algorithm.\n\nIf formula_10, the weight of the expert's advice will remain the same. When formula_1 increases, the weight of the expert's advice will decrease. Note that some researchers fix formula_12 in weighted majority algorithm.\n\nAfter formula_6 steps, let formula_14 be the number of mistakes of expert i and formula_15 be the number of mistakes our algorithm has made. Then we have the following bound for every formula_16:\n\nIn particular, this holds for i which is the best expert. Since the best expert will have the least formula_14, it will give the best bound on the number of mistakes made by the algorithm as a whole.\n\nGiven the same setup with N experts. Consider the special situation where the proportions of experts predicting positive and negative, counting the weights, are both close to 50%. Then, there might be a tie. Following the weight update rule in weighted majority algorithm, the predictions made by the algorithm would be randomized. The algorithm calculates the probabilities of experts predicting positive or negatives, and then makes a random decision based on the computed fraction:\n\npredict \n\nwhere \n\nThe number of mistakes made by the randomized weighted majority algorithm is bounded as: \n\nwhere formula_22 and formula_23.\n\nNote that only the learning algorithm is randomized. The underlying assumption is that the examples and experts' predictions are not random. The only randomness is the randomness where the learner makes his own prediction.\nIn this randomized algorithm, formula_24 if formula_25. Compared to weighted algorithm, this randomness halved the number of mistakes the algorithm is going to make. However, it is important to note that in some research, people define formula_12 in weighted majority algorithm and allow formula_27 in randomized weighted majority algorithm.\n\nThe multiplicative weights method is usually used to solve a constrained optimization problem. Let each expert be the constraint in the problem, and the events represent the points in the area of interest. The punishment of the expert corresponds to how well its corresponding constraint is satisfied on the point represented by an event.\n\nSuppose we were given the distribution formula_28 on experts. Let formula_29 = payoff matrix of a finite two-player zero-sum game, with formula_30 rows.\n\nWhen the row player formula_31 uses plan formula_16 and the column player formula_33 uses plan formula_34, the payoff of player formula_33 is formula_36≔formula_37, assuming formula_38.\n\nIf player formula_31 chooses action formula_16 from a distribution formula_28 over the rows, then the expected result for player formula_33 selecting action formula_34 is formula_44.\n\nTo maximize formula_45, player formula_33 is should choose plan formula_34. Similarly, the expected payoff for player formula_48 is formula_49. Choosing plan formula_16 would minimize this payoff. By John Von Neumann's Min-Max Theorem, we obtain:\n\nwhere P and i changes over the distributions over rows, Q and j changes over the columns.\n\nThen, let formula_52 denote the common value of above quantities, also named as the \"value of the game\". Let formula_53 be an error parameter. To solve the zero-sum game bounded by additive error of formula_54,\n\nSo there is an algorithm solving zero-sum game up to an additive factor of δ using O(/formula_57) calls to ORACLE, with an additional processing time of O(n) per call\n\nIn machine learning, Littlestone and Warmuth generalized the winnow algorithm to the weighted majority algorithm. Later, Freund and Schapire generalized it in the form of hedge algorithm. AdaBoost Algorithm formulated by Yoav Freund and Robert Schapire also employed the Multiplicative Weight Update Method.\n\nBased on current knowledge in algorithms, multiplicative weight update method was first used in Littlestone's winnow algorithm. It is used in machine learning to solve a linear program.\n\nGiven formula_58 labeled examples formula_59 where formula_60 are feature vectors, and formula_61 are their labels.\n\nThe aim is to find non-negative weights such that for all examples, the sign of the weighted combination of the features matches its labels. That is, require that formula_62 for all formula_34. Without loss of generality, assume the total weight is 1 so that they form a distribution. Thus, for notational convenience, redefine formula_64 to be formula_65, the problem reduces to finding a solution to the following LP:\n\nThis is general form of LP.\n\nThe hedge algorithm is similar to the weighted majority algorithm. However, their exponential update rules are different.\nIt is generally used to solve the problem of binary allocation in which we need to allocate different portion of resources into N different options. The loss with every option is available at the end of every iteration. The goal is to reduce the total loss suffered for a particular allocation. The allocation for the following iteration is then revised, based on the total loss suffered in the current iteration using multiplicative update.\n\nAssume formula_69 and for formula_70, formula_71 is picked by Hedge. Then for all experts formula_16,\n\nInitialization: Fix an formula_74. For each expert, associate the weight formula_75 ≔1\nFor t=1,2,…,T:\n\nThis algorithm maintains a set of weights formula_81 over the training examples. On every iteration formula_3, a distribution formula_71 is computed by normalizing these weights. This distribution is fed to the weak learner WeakLearn which generates a hypothesis formula_84 that (hopefully) has small error with respect to the distribution. Using the new hypothesis formula_84, AdaBoost generates the next weight vector formula_86. The process repeats. After T such iterations, the final hypothesis formula_87 is the output. The hypothesis formula_87 combines the outputs of the T weak hypotheses using a weighted majority vote.\n\nGiven a formula_110 matrix formula_29 and formula_112, is there a formula_113 such that formula_114?\n\nUsing the oracle algorithm in solving zero-sum problem, with an error parameter formula_116, the output would either be a point formula_113 such that formula_118 or a proof that formula_113 does not exist, i.e., there is no solution to this linear system of inequalities.\n\nGiven vector formula_120, solves the following relaxed problem\n\nIf there exists a x satisfying (1), then x satisfies (2) for all formula_122. The contrapositive of this statement is also true.\nSuppose if oracle returns a feasible solution for a formula_123, the solution formula_113 it returns has bounded width formula_125.\nSo if there is a solution to (1), then there is an algorithm that its output x satisfies the system (2) up to an additive error of formula_126. The algorithm makes at most formula_127 calls to a width-bounded oracle for the problem (2). The contrapositive stands true as well. The multiplicative updates is applied in the algorithm in this case.\n\nIn operations research and on-line statistical decision making problem field, the weighted majority algorithm and its more complicated versions have been found independently.\n\nThe multiplicative weights algorithm is also widely applied in computational geometry, such as Clarkson's algorithm for linear programming (LP) with a bounded number of variables in linear time. Later, Bronnimann and Goodrich employed analogous methods to find Set Covers for hypergraphs with small VC dimension.\n", "id": "52242050", "title": "Multiplicative weight update method"}
{"url": "https://en.wikipedia.org/wiki?curid=938663", "text": "Multi-task learning\n\nMulti-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately. Early versions of MTL were called \"hints\"\n\nIn a widely cited 1997 paper, Rich Caruana gave the following characterization:Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.\n\nIn the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly. One example is a spam-filter, which can be treated as distinct but related classification tasks across different users. To make this more concrete, consider that different people have different distributions of features which distinguish spam emails from legitimate ones, for example an English speaker may find that all emails in Russian are spam, not so for Russian speakers. Yet there is a definite commonality in this classification task across users, for example one common feature might be text related to money transfer. Solving each user's spam classification problem jointly via MTL can let the solutions inform each other and improve performance. Further examples of settings for MTL include multiclass classification and multi-label classification.\n\nMulti-task learning works because regularization induced by requiring an algorithm to perform well on a related task can be superior to regularization that prevents overfitting by penalizing all complexity uniformly. One situation where MTL may be particularly helpful is if the tasks share significant commonalities and are generally slightly under sampled. However, as discussed below, MTL has also been shown to be beneficial for learning unrelated tasks.\n\nWithin the MTL paradigm, information can be shared across some or all of the tasks. Depending on the structure of task relatedness, one may want to share information selectively across the tasks. For example, tasks may be grouped or exist in a hierarchy, or be related according to some general metric. Suppose, as developed more formally below, that the parameter vector modeling each task is a linear combination of some underlying basis. Similarity in terms of this basis can indicate the relatedness of the tasks. For example, with sparsity, overlap of nonzero coefficients across tasks indicates commonality. A task grouping then corresponds to those tasks lying in a subspace generated by some subset of basis elements, where tasks in different groups may be disjoint or overlap arbitrarily in terms of their bases. Task relatedness can be imposed a priori or learned from the data. Hierarchical task relatedness can also be exploited implicitly without assuming a priori knowledge or learning relations explicitly.\n\nOne can attempt learning a group of principal tasks using a group of auxiliary tasks, unrelated to the principal ones. In many applications, joint learning of unrelated tasks which use the same input data can be beneficial. The reason is that prior knowledge about task relatedness can lead to sparser and more informative representations for each task grouping, essentially by screening out idiosyncrasies of the data distribution. Novel methods which builds on a prior multitask methodology by favoring a shared low-dimensional representation within each task grouping have been proposed. The programmer can impose a penalty on tasks from different groups which encourages the two representations to be orthogonal. Experiments on synthetic and real data have indicated that incorporating unrelated tasks can result in significant improvements over standard multi-task learning methods.\n\nRelated to multi-task learning is the concept of knowledge transfer. Whereas traditional multi-task learning implies that a shared representation is developed concurrently across tasks, transfer of knowledge implies a sequentially shared representation. Large scale machine learning projects such as the deep convolutional neural network GoogLeNet, an image-based object classifier, can develop robust representations which may be useful to further algorithms learning related tasks. For example, the pre-trained model can be used as a feature extractor to perform pre-processing for another learning algorithm. Or the pre-trained model can be used to initialize a model with similar architecture which is then fine-tuned to learn a different classification task.\n\nTraditionally Multi-task learning and transfer of knowledge are applied to stationary learning settings. Their extension to non-stationary environments is termed Group online adaptive learning (GOAL). Sharing information could be particularly useful if learners operate in continuously changing environments, because a learner could benefit from previous experience of another learner to quickly adapt to their new environment. Such group-adaptive learning has numerous applications, from predicting financial time-series, through content recommendation systems, to visual understanding for adaptive autonomous agents.\n\nThe MTL problem can be cast within the context of RKHSvv (a complete inner product space of vector-valued functions equipped with a reproducing kernel). In particular, recent focus has been on cases where task structure can be identified via a separable kernel, described below. The presentation here derives from Ciliberto et al., 2015.\n\nSuppose the training data set is formula_1, with formula_2, formula_3, where formula_4 indexes task, and formula_5. Let formula_6. In this setting there is a consistent input and output space and the same loss function formula_7 for each task: . This results in the regularized machine learning problem: \nwhere formula_8 is a vector valued reproducing kernel Hilbert space with functions formula_9 having components formula_10.\n\nThe reproducing kernel for the space formula_8 of functions formula_12 is a symmetric matrix-valued function formula_13 , such that formula_14 and the following reproducing property holds: \nThe form of the kernel formula_15 induces both the representation of the feature space and structures the output across tasks. A natural simplification is to choose a \"separable kernel,\" which factors into separate kernels on the input space formula_16 and on the tasks formula_17. In this case the kernel relating scalar components formula_18 and formula_19 is given by formula_20. For vector valued functions formula_21 we can write formula_22, where formula_23 is a scalar reproducing kernel, and formula_24 is a symmetric positive semi-definite formula_25 matrix. Henceforth denote formula_26 .\n\nThis factorization property, separability, implies the input feature space representation does not vary by task. That is, there is no interaction between the input kernel and the task kernel. The structure on tasks is represented solely by formula_24. Methods for non-separable kernels formula_15 is an current field of research.\n\nFor the separable case, the representation theorem is reduced to formula_29. The model output on the training data is then formula_30 , where formula_31 is the formula_32 empirical kernel matrix with entries formula_33, and formula_34 is the formula_35 matrix of rows formula_36.\n\nWith the separable kernel, equation can be rewritten as\n\nwhere formula_37 is a (weighted) average of formula_38 applied entry-wise to Y and KCA. (The weight is zero if formula_39 is a missing observation).\n\nNote the second term in can be derived as follows:\n\nformula_40\n\nformula_41 (bilinearity)\n\nformula_42 (reproducing property)\n\nformula_43\n\nThere are three largely equivalent ways to represent task structure: through a regularizer; through an output metric, and through an output mapping.\n\nRegularizer - With the separable kernel, it can be shown (below) that formula_44, where formula_45 is the formula_46 element of the pseudoinverse of formula_47, and formula_48 is the RKHS based on the scalar kernel formula_49, and formula_50. This formulation shows that formula_45 controls the weight of the penalty associated with formula_52. (Note that formula_52 arises from formula_54.)\n\nProof:\n\nformula_55\n\nformula_56\n\nformula_57\n\nformula_58\n\nformula_59\n\nformula_60\n\nformula_61\n\nformula_62\n\nformula_63\n\nOutput metric - an alternative output metric on formula_64 can be induced by the inner product formula_65. With the squared loss there is an equivalence between the separable kernels formula_66 under the alternative metric, and formula_67, under the canonical metric.\n\nOutput mapping - Outputs can be mapped as formula_68 to a higher dimensional space to encode complex structures such as trees, graphs and strings. For linear maps formula_69, with appropriate choice of separable kernel, it can be shown that formula_70.\n\nVia the regularizer formulation, one can represent a variety of task structures easily. \n\nLearning problem can be generalized to admit learning task matrix A as follows:\nChoice of formula_91 must be designed to learn matrices \"A\" of a given type. See \"Special cases\" below.\n\nRestricting to the case of convex losses and coercive penalties Ciliberto \"et al.\" have shown that although is not convex jointly in \"C\" and \"A,\" a related problem is jointly convex.\n\nSpecifically on the convex set formula_92, the equivalent problem\n\nis convex with the same minimum value. And if formula_93 is a minimizer for then formula_94 is a minimizer for .\n\nThe perturbation via the barrier formula_95 forces the objective functions to be equal to formula_96 on the boundary of formula_97 .\n\nSpectral penalties - Dinnuzo \"et al\" suggested setting \"F\" as the Frobenius norm formula_100. They optimized directly using block coordinate descent, not accounting for difficulties at the boundary of formula_101.\n\nClustered tasks learning - Jacob \"et al\" suggested to learn \"A\" in the setting where \"T\" tasks are organized in \"R\" disjoint clusters. In this case let formula_102 be the matrix with formula_103. Setting formula_104, and formula_105, the task matrix formula_106 can be parameterized as a function of formula_107: formula_108 , with terms that penalize the average, between clusters variance and within clusters variance respectively of the task predictions. M is not convex, but there is a convex relaxation formula_109. In this formulation, formula_110.\n\nNon-convex penalties - Penalties can be constructed such that A is constrained to be a graph Laplacian, or that A has low rank factorization. However these penalties are not convex, and the analysis of the barrier method proposed by Ciliberto et al. does not go through in these cases.\n\nNon-separable kernels - Separable kernels are limited, in particular they do not account for structures in the interaction space between the input and output domains jointly. Future work is needed to develop models for these kernels.\n\nUsing the principles of MTL, techniques for collaborative spam filtering that facilitates personalization have been proposed. In large scale open membership email systems, most users do not label enough messages for an individual local classifier to be effective, while the data is too noisy to be used for a global filter across all users. A hybrid global/individual classifier can be effective at absorbing the influence of users who label emails very diligently from the general public. This can be accomplished while still providing sufficient quality to users with few labeled instances.\n\nUsing boosted decision trees, one can enable implicit data sharing and regularization. This learning method can be used on web-search ranking data sets. One example is to use ranking data sets from several countries. Here, multitask learning is particularly helpful as data sets from different countries vary largely in size because of the cost of editorial judgments. It has been demonstrated that learning various tasks jointly can lead to significant improvements in performance with surprising reliability.\n\nIn order to facilitate transfer of knowledge, IT infrastructure is being developed. One such project, RoboEarth, aims to set up an open source internet database that can be accessed and continually updated from around the world. The goal is to facilitate a cloud-based interactive knowledge base, accessible to technology companies and academic institutions, which can enhance the sensing, acting and learning capabilities of robots and other artificial intelligence agents.\n\nThe Multi-Task Learning via StructurAl Regularization (MALSAR) Matlab package implements the following multi-task learning algorithms:\n\n\n\n", "id": "938663", "title": "Multi-task learning"}
{"url": "https://en.wikipedia.org/wiki?curid=38782554", "text": "RoboEarth\n\nRoboEarth is a network and database repository where robots can share information and learn from each other and a cloud for outsourcing heavy computation tasks. The project that has been described as a \"World Wide Web for robots\". The project brings together researchers from five major universities in Germany, the Netherlands and Spain and is backed by the European Union.\n\nIt allows robots to:\n\n\nIn addition to the cloud-based infrastructure, RoboEarth offers ROS-compatible, robot-unspecific components for high level control of the robot. See software-components for more details.\n\nRoboEarth offers a Cloud Robotics infrastructure, which includes everything needed to close the loop from robot to the cloud and back to the robot. RoboEarth’s World-Wide-Web style database stores knowledge generated by humans – and robots – in a machine-readable format. Data stored in the RoboEarth knowledge base include software components, maps for navigation (e.g., object locations, world models), task knowledge (e.g., action recipes, manipulation strategies), and object recognition models (e.g., images, object models).\n\nThe RoboEarth Cloud Engine (also called Rapyuta) makes powerful computation available to robots. It allows robots to offload their heavy computation to secure computing environments in the cloud with minimal configuration. The Cloud Engine’s computing environments provide high bandwidth access to the RoboEarth knowledge repository enabling robots to benefit from the experience of other robots\n\nIn late 2009, the RoboEarth project was awarded a 4-year funding grant from the European Commission’s Cognitive Systems and Robotics Initiative in order to develop their networked database platform, Rapyuta, and to develop proof-of-concept systems to demonstrate its use. In January 2014, it was officially announced that 'Wikipedia for Robots' had been launched.\n\nRoboEarth has a spin off called Robohow.\n", "id": "38782554", "title": "RoboEarth"}
{"url": "https://en.wikipedia.org/wiki?curid=52992310", "text": "VGG Image Annotator\n\nVGG Image Annotator (VIA) is an open source project developed at the Visual Geometry Group and released under the BSD-2 clause license. With this standalone application, you can define regions in an image and create a textual description of those regions. Such image regions and descriptions are useful for supervised training of machine learning algorithms.\n\n\n\n", "id": "52992310", "title": "VGG Image Annotator"}
{"url": "https://en.wikipedia.org/wiki?curid=53279262", "text": "Instance selection\n\nInstance selection (or dataset reduction, or dataset condensation) is an important Data pre-processing step that can be applied in many Machine learning (or Data mining) tasks. Approaches for instance selection can be applied for reducing the original dataset to a manageable volume, leading to a reduction of the computational resources that are necessary for performing the learning process. Algorithms of instance selection can also be applied for removing noisy instances, before applying learning algorithms. This step can improve the accuracy in classification problems.\n\nAlgorithm for instance selection should identify a subset of the total available data to achieve the original purpose of the data mining (or machine learning) application as if the whole data had been used. Considering this, the optimal outcome of IS would be the minimum data subset that can accomplish the same task with no performance loss, in comparison with the performance achieved when the task is performed using the whole available data. Therefore, every instance selection strategy should deal with a trade-off between the reduction rate of the dataset and the classification quality.\n\nThe literature provides several different algorithms for instance selection. They can be distinguished from each other according to several different criteria. Considering this, instance selection algorithms can be grouped in two main classes, according to what instances they select: algorithms that preserve the instances at the boundaries of classes and algorithms that preserve the internal instances of the classes. Within the category of algorithms that select instances at the boundaries it is possible to cite DROP3, ICF and LSBo. On the other hand, within the category of algorithms that select internal instances, it is possible to mention ENN and LSSm. In general, algorithm such as ENN and LSSm are used for removing harmful (noisy) instances from the dataset. They do not reduce the data as the algorithms that select border instances, but they remove instances at the boundaries that have a negative impact on the data mining task. They can be used by other instance selection algorithms, as a filtering step. For example, the ENN algorithm is used by DROP3 as the first step, and the LSSm algorithm is used by LSBo.\n\nThere is also another group os algorithms that adopt different selection criteria. For example, the algorithms LDIS and CDIS select the densest instances in a given arbitrary neighborhood. The selected instances can include both, border and internal instances. The LDIS and CDIS algorithms are very simple and select subsets that are very representative of the original dataset. Besides that, since they search by the representative instances in each class separately, they are faster (in terms of time complexity and effective running time) than other algorithms, such as DROP3 and ICF.\n", "id": "53279262", "title": "Instance selection"}
{"url": "https://en.wikipedia.org/wiki?curid=53802271", "text": "Machine learning control\n\nMachine learning control (MLC) is a subfield of machine learning and control theory\nwhich solves optimal control problems with methods of machine learning.\nKey applications are complex nonlinear systems\nfor which linear control theory methods are not applicable.\n\nFour types of problems are commonly encountered.\n\nMLC comprises, for instance, neural network control, \ngenetic algorithm based control, \ngenetic programming control,\nreinforcement learning control, \nand has methodolical overlaps with other data-driven control,\nlike artificial intelligence and robot control.\n\nMLC has been successfully applied\nto many nonlinear control problems,\nexploring unknown and often unexpected actuation mechanisms.\nExample applications include\n\n\nAs for all general nonlinear methods,\nMLC comes with no guaranteed convergence, \noptimality or robustness for a range of operating conditions.\n\n", "id": "53802271", "title": "Machine learning control"}
{"url": "https://en.wikipedia.org/wiki?curid=53970843", "text": "Machine learning in bioinformatics\n\nMachine learning, a subfield of computer science involving the development of algorithms that learn how to make predictions based on data, has a number of emerging applications in the field of bioinformatics. Bioinformatics deals with computational and mathematical approaches for understanding and processing biological data. \n\nPrior to the emergence of machine learning algorithms, bioinformatics algorithms had to be explicitly programmed by hand which, for problems such as protein structure prediction, proves extremely difficult. Machine learning techniques such as deep learning enable the algorithm to make use of automatic feature learning which means that based on the dataset alone, the algorithm can learn how to combine multiple features of the input data into a more abstract set of features from which to conduct further learning. This multi-layered approach to learning patterns in the input data allows such systems to make quite complex predictions when trained on large datasets. In recent years, the size and number of available biological datasets have skyrocketed, enabling bioinformatics researchers to make use of these machine learning systems. Machine learning has been applied to six main subfields of bioinformatics: genomics, proteomics, microarrays, systems biology, evolution, and text mining.\n\nGenomics involves the study of the genome, the complete DNA sequence, of organisms. While genomic sequence data has historically been sparse due to the technical difficulty in sequencing a piece of DNA, the number of available sequences is growing exponentially. However, while raw data is becoming increasingly available and accessible, the biological interpretation of this data is occurring at a much slower pace. Therefore, there is an increasing need for the development of machine learning systems that can automatically determine the location of protein-encoding genes within a given DNA sequence. This is a problem in computational biology known as gene prediction.\n\nGene prediction is commonly performed through a combination of what are known as extrinsic and intrinsic searches. For the extrinsic search, the input DNA sequence is run through a large database of sequences whose genes have been previously discovered and their locations annotated. A number of the sequence's genes can be identified by determining which strings of bases within the sequence are homologous to known gene sequences. However, given the limitation in size of the database of known and annotated gene sequences, not all the genes in a given input sequence can be identified through homology alone. Therefore, an intrinsic search is needed where a gene prediction program attempts to identify the remaining genes from the DNA sequence alone.\n\nMachine learning is also been used for the problem of multiple sequence alignment which involves aligning many DNA or amino acid sequences in order to determine regions of similarity that could indicate a shared evolutionary history.\nIt can also be used to detect and visualize genome rearrangements.\n\nProteins, strings of amino acids, gain much of their function from protein folding in which they conform into a three-dimensional structure. This structure is composed of a number of layers of folding, including the primary structure (i.e. the flat string of amino acids), the secondary structure (alpha helices and beta sheets), the tertiary structure, and the quartenary structure.\n\nProtein secondary structure prediction is a main focus of this subfield as the further protein foldings (tertiary and quartenary structures) are determined based on the secondary structure. Solving the true structure of a protein is an incredibly expensive and time-intensive process, furthering the need for systems that can accurately predict the structure of a protein by analyzing the amino acid sequence directly. Prior to machine learning, researchers needed to conduct this prediction manually. This trend began in 1951 when Pauling and Corey released their work on predicting the hydrogen bond configurations of a protein from a polypeptide chain. Today, through the use of automatic feature learning, the best machine learning techniques are able to achieve an accuracy of 82-84%. The current state-of-the-art in secondary structure prediction uses a system called DeepCNF (deep convolutional neural fields) which relies on the machine learning model of artificial neural networks to achieve an accuracy of approximately 84% when tasked to classify the amino acids of a protein sequence into one of three structural classes (helix, sheet, or coil). The theoretical limit for three-state protein secondary structure is 88–90%.\n\nMachine learning has also been applied to proteomics problems such as protein side-chain prediction, protein loop modeling, and protein contact map prediction.\n\nMicroarrays, a type of lab-on-a-chip, are used for automatically collecting data about large amounts of biological material. Machine learning can aid in the analysis of this data, and it has been applied to expression pattern identification, classification, and genetic network induction.\n\nThis technology is especially useful for monitoring the expression of genes within a genome, aiding in diagnosing different types of cancer based on which genes are expressed. One of the main problems in this field is identifying which genes are expressed based on the collected data. In addition, due to the huge number of genes on which data is collected by the microarray, there is a large amount of irrelevant data to the task of expressed gene identification, further complicating this problem. Machine learning presents a potential solution to this problem as various classification methods can be used to perform this identification. The most commonly used methods are radial basis function networks, deep learning, Bayesian classification, decision trees, and random forest.\n\nSystems biology focuses on the study of the emergent behaviors from complex interactions of simple biological components in a system. Such components can include molecules such as DNA, RNA, proteins, and metabolites.\n\nMachine learning has been used to aid in the modelling of these complex interactions in biological systems in domains such as genetic networks, signal transduction networks, and metabolic pathways. Probabilistic graphical models, a machine learning technique for determining the structure between different variables, are one of the most commonly used methods for modeling genetic networks. In addition, machine learning has been applied to systems biology problems such as identifying transcription factor binding sites using a technique known as Markov chain optimization. Genetic algorithms, machine learning techniques which are based on the natural process of evolution, have been used to model genetic networks and regulatory structures.\n\nOther systems biology applications of machine learning include the task of enzyme function prediction, high throughput microarray data analysis, analysis of genome-wide association studies to better understand markers of Multiple Sclerosis, protein function prediction, and identification of NCR-sensitivity of genes in yeast.\n\nThe increase in available biological publications led to the issue of the increase in difficulty in searching through and compiling all the relevant available information on a given topic across all sources. This task is known as knowledge extraction. This is necessary for biological data collection which can then in turn be fed into machine learning algorithms to generate new biological knowledge. Machine learning can be used for this knowledge extraction task using techniques such as natural language processing to extract the useful information from human-generated reports in a database. Text Nailing, an alternative approach to machine learning, capable of extracting features from clinical narrative notes was introduced in 2017.\n\nThis technique has been applied to the search for novel drug targets, as this task requires the examination of information stored in biological databases and journals. Annotations of proteins in protein databases often do not reflect the complete known set of knowledge of each protein, so additional information must be extracted from biomedical literature. Machine learning has been applied to automatic annotation of the function of genes and proteins, determination of the subcellular localization of a protein, analysis of DNA-expression arrays, large-scale protein interaction analysis, and molecule interaction analysis.\n\nOther application is the detection and visualization of regions that share high degree of similarity or are new according to a reference.\n", "id": "53970843", "title": "Machine learning in bioinformatics"}
{"url": "https://en.wikipedia.org/wiki?curid=54033657", "text": "Labeled data\n\nLabeled data is a group of samples that have been tagged with one or more labels. Labeling typically takes a set of unlabeled data and augments each piece of that unlabeled data with meaningful tags that are informative. For example, labels might be indicate whether a photo contains a horse or a cow, which words were uttered in an audio recording, what type of action is being performed in a video, what the topic of a news article is, what the overall sentiment of a tweet is, whether the dot in an x-ray is a tumor, etc.\n\nLabels can be obtained by asking humans to make judgments about a given piece of unlabeled data (e.g., \"Does this photo contain a horse or a cow?\"), and are significantly more expensive to obtain than the raw unlabeled data.\n\nAfter obtaining a labeled dataset, machine learning models can be applied to the data so that new unlabeled data can be presented to the model and a likely label can be guessed or predicted for that piece of unlabeled data.\n", "id": "54033657", "title": "Labeled data"}
{"url": "https://en.wikipedia.org/wiki?curid=53631046", "text": "Caffe (software)\n\nCAFFE (Convolutional Architecture for Fast Feature Embedding) is a deep learning framework, originally developed at UC Berkeley. It is open source, under a BSD license. It is written in C++, with a Python interface.\n\nYangqing Jia created the caffe project during his PhD at UC Berkeley. Now there are many contributors to the project, and it is hosted at GitHub.\n\nCaffe supports many different types of deep learning architectures geared towards image classification and image segmentation. It supports CNN, RCNN, LSTM and fully connected neural network designs. Caffe supports GPU-based acceleration using CuDNN of Nvidia.\n\nCaffe is being used in academic research projects, startup prototypes, and even large-scale industrial applications in vision, speech, and multimedia. Yahoo! has also integrated caffe with Apache Spark to create CaffeOnSpark, a distributed deep learning framework.\n\nIn April 2017, Facebook announced Caffe2, which includes new features such as Recurrent Neural Networks.\n\n\n", "id": "53631046", "title": "Caffe (software)"}
{"url": "https://en.wikipedia.org/wiki?curid=44577560", "text": "Occam learning\n\nIn computational learning theory, Occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to probably approximately correct (PAC) learning, where the learner is evaluated on its predictive power of a test set.\n\nOccam learnability implies PAC learning, and for a wide variety of concept classes, the converse is also true: PAC learnability implies Occam learnability.\n\nOccam Learning is named after Occam's razor, which is a principle stating that, given all other things being equal, a shorter explanation for observed data should be favored over a lengthier explanation. The theory of Occam learning is a formal and mathematical justification for this principle. It was first shown by Blumer, et al. that Occam learning implies PAC learning, which is the standard model of learning in computational learning theory. In other words, \"parsimony\" (of the output hypothesis) implies \"predictive power\".\n\nThe succinctness of a concept formula_1 in concept class formula_2 can be expressed by the length formula_3 of the shortest bit string that can represent formula_1 in formula_2. Occam learning connects the succinctness of a learning algorithm's output to its predictive power on unseen data.\n\nLet formula_2 and formula_7 be concept classes containing target concepts and hypotheses respectively. Then, for constants formula_8 and formula_9, a learning algorithm formula_10 is an formula_11-Occam algorithm for formula_2 using formula_7 if, given a set formula_14 of formula_15 samples labeled according to a concept formula_16, formula_10 outputs a hypothesis formula_18 such that\nwhere formula_24 is the maximum length of any sample formula_25. An Occam algorithm is called \"efficient\" if it runs in time polynomial in formula_24, formula_15, and formula_3. We say a concept class formula_2 is \"Occam learnable\" with respect to a hypothesis class formula_7 if there exists an efficient Occam algorithm for formula_2 using formula_7.\n\nOccam learnability implies PAC learnability, as the following theorem of Blumer, et al. shows:\n\nLet formula_10 be an efficient formula_11-Occam algorithm for formula_2 using formula_7. Then there exists a constant formula_37 such that for any formula_38, for any distribution formula_39, given formula_40 samples drawn from formula_39 and labelled according to a concept formula_42 of length formula_24 bits each, the algorithm formula_10 will output a hypothesis formula_45 such that formula_46 with probability at least formula_47 .Here, formula_48 is with respect to the concept formula_1 and distribution formula_39. This implies that the algorithm formula_10 is also a PAC learner for the concept class formula_2 using hypothesis class formula_7. A slightly more general formulation is as follows:\n\nLet formula_38. Let formula_10 be an algorithm such that, given formula_15 samples drawn from a fixed but unknown distribution formula_57 and labeled according to a concept formula_42 of length formula_24 bits each, outputs a hypothesis formula_60 that is consistent with the labeled samples. Then, there exists a constant formula_61 such that if formula_62, then formula_10 is guaranteed to output a hypothesis formula_60 such that formula_46 with probability at least formula_47.\n\nWhile the above theorems show that Occam learning is sufficient for PAC learning, it doesn't say anything about \"necessity.\" Board and Pitt show that, for a wide variety of concept classes, Occam learning is in fact necessary for PAC learning. They proved that for any concept class that is \"polynomially closed under exception lists,\" PAC learnability implies the existence of an Occam algorithm for that concept class. Concept classes that are polynomially closed under exception lists include Boolean formulas, circuits, deterministic finite automata, decision-lists, decision-trees, and other geometrically-defined concept classes.\n\nA concept class formula_2 is polynomially closed under exception lists if there exists a polynomial-time algorithm formula_68 such that, when given the representation of a concept formula_42 and a finite list formula_70 of \"exceptions\", outputs a representation of a concept formula_71 such that the concepts formula_1 and formula_73 agree except on the set formula_70.\n\nWe first prove the Cardinality version. Call a hypothesis formula_75 \"bad\" if formula_76, where again formula_48 is with respect to the true concept formula_1 and the underlying distribution formula_57. The probability that a set of samples formula_14 is consistent with formula_19 is at most formula_82, by the independence of the samples. By the union bound, the probability that there exists a bad hypothesis in formula_83 is at most formula_84, which is less than formula_85 if formula_86. This concludes the proof of the second theorem above.\n\nUsing the second theorem, we can prove the first theorem. Since we have a formula_11-Occam algorithm, this means that any hypothesis output by formula_10 can be represented by at most formula_89 bits, and thus formula_90. This is less than formula_91 if we set formula_92 for some constant formula_37. Thus, by the Cardinality version Theorem, formula_10 will output a consistent hypothesis formula_19 with probability at least formula_96. This concludes the proof of the first theorem above.\n\nThough Occam and PAC learnability are equivalent, the Occam framework can be used to produce tighter bounds on the sample complexity of classical problems including conjunctions, conjunctions with few relevant variables, and decision lists.\n\nOccam algorithms have also been shown to be successful for PAC learning in the presence of errors, probabilistic concepts, function learning and Markovian non-independent examples.\n\n", "id": "44577560", "title": "Occam learning"}
{"url": "https://en.wikipedia.org/wiki?curid=54550729", "text": "Connectionist temporal classification\n\nConnectionist temporal classification (CTC) is a type of neural network output and associated scoring function, for training recurrent neural networks (RNNs) such as LSTM networks to tackle sequence problems where the timing is variable. It can be used for tasks like on-line handwriting recognition or recognizing phonemes in speech audio. CTC refers to the outputs and scoring, and is independent of the underlying neural network structure. It was introduced in 2006.\n\nThe input is a sequence of observations, and the outputs are a sequence of labels, which can include blank outputs. The difficulty of training comes from there being many more observations than there are labels. For example in speech audio there can be multiple time slices which correspond to a single phoneme. Since we don't know the alignment of the observed sequence with the target labels we predict a probability distribution at each time step. A CTC network has a continuous output (e.g. softmax), which is fitted through training to model the probability of a label. CTC does not attempt to learn boundaries and timings: Label sequences are considered equivalent if they differ only in alignment, ignoring blanks. Equivalent label sequences can occur in many ways – which makes scoring a non-trivial task. Fortunately there is an efficient forward-backwards algorithm.\n\nCTC scores can then be used with the back-propagation algorithm to update the neural network weights.\n\nAlternative approaches to a CTC-fitted neural network include a hidden Markov model (HMM).\n", "id": "54550729", "title": "Connectionist temporal classification"}
{"url": "https://en.wikipedia.org/wiki?curid=52003586", "text": "End-to-end reinforcement learning\n\nIn end-to-end reinforcement learning, the end-to-end process, in other words, the entire process from sensors to motors in a robot or agent consists of only one layered or recurrent neural network without modularization, and the network is trained comprehensively by reinforcement learning. End-to-end reinforcement learning has been propounded for a long time, and has been sparked by the successful results in learning to play ATARI TV games (2013–15) and AlphaGo (2016) by Google DeepMind. As well as deep learning, by using a neural network, it enables to learn massively parallel processing that humans can hardly design by hand, and to surpass what humans design. Unlike supervised learning, reinforcement learning makes autonomous learning possible. Therefore, it can make the interference by human design minimum, and very flexible and purposive learning on a huge degree of freedom can be realized. That is the reason why it is expected to solve the frame problem or symbol grounding problem and to open up the way to artificial general intelligence (AGI) or strong AI.\n\nIn reinforcement learning research, it has been general that state space and action space are designed in advance and only the mapping from state space to action space is learned. Therefore, reinforcement learning has been limited to learning only for action, and human designers have to design how to construct state space from sensor signals and to give how the motion commands are generated for each action before learning. Neural networks have been often used in reinforcement learning, but that has been for non-linear function approximation to avoid the curse of dimensionality problem that occurs when table-lookup approach is used. Recurrent neural networks have been also used sometimes, but the main purpose of the use is to avoid perceptual aliasing or POMDP (partially observable Markov decision process).\n\nHowever, the end-to-end reinforcement learning extends reinforcement learning from learning only for actions to learning for entire process by extending the learned process to the entire process from sensors to motors. Therefore, not only actions, but also various functions including recognition and memory are expected to emerge. Especially, in higher functions, they do not connect directly with either sensors or motors, and so even deciding either their inputs or outputs is very difficult. Since that has disturbed the understanding or developing of them, the progress in it is expected by this approach.\n\nThe origin of this approach can be seen in TD-Gammon by G. Tesauro (1992). In a popular game named Back Gammon, the evaluation of the game situation during self-play was learned through TD(formula_1) using a layered neural network. 4 inputs were used for the number of men of a given color at a given location on the board and in total there are 198 input signals. With zero knowledge built in, the network was able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, and the internal representation after learning was observed.\n\nK. Shibata's group has persisted in this framework and done so many works since around 1997. Other than Q-learning, they also employed actor-critic for continuous motion tasks, and used a recurrent neural network for memory-required tasks. They also applied this framework to some real robot tasks. They have also shown that various functions emerged in this framework as in the next section.\n\nSince around 2013, as mentioned, Google DeepMind showed very impressive learning results in learning to play TV games and game of Go (AlphaGo). They used a deep convolutional neural network that has shown superior results in image recognition. They also used 4 frames of almost raw RGB pixels (84x84) as inputs of the network, and the network was trained based on reinforcement learning with the reward representing the sign of the change in the game score. All the 49 games could be learned using the same network architecture and Q-learning with the minimal prior knowledge, and it outperformed competing methods in almost all the games and performed at a level that is broadly comparable with or superior to a professional human game tester in the majority of games. It is sometimes called DQN (Deep-Q network). In AlphaGo, deep neural networks are trained not only by reinforcement learning, but also by supervised learning. It was also combined with Monte Carlo tree search.\n\nIt has been shown by K. Shibata's group that various functions emerge in this framework: (1)image recognition, (2)color constancy (optical illusion), (3)sensor motion (active recognition), (4)hand-eye coordination and hand reaching movement, (5)explanation of brain activities, (6)knowledge transfer, (7)memory, (8)selective attention, (9)prediction, (10)exploration and so on. Communications have been also established in this framework. (1)Dynamic communication (negotiation), (2)binalization of signals, and (3)grounded communication using a real robot and camera emerged in their works\n", "id": "52003586", "title": "End-to-end reinforcement learning"}
{"url": "https://en.wikipedia.org/wiki?curid=55075082", "text": "BigDL\n\nBigDL is a distributed deep learning framework for Apache Spark, created by Jason Dai at Intel.\nIt is hosted at GitHub.\n\n", "id": "55075082", "title": "BigDL"}
{"url": "https://en.wikipedia.org/wiki?curid=55375136", "text": "Highway network\n\nIn machine learning, a highway network is an approach to optimizing networks and increasing their depth. Highway networks use learned gating mechanisms to regulate information flow, inspired by Long Short-Term Memory (LSTM) recurrent neural networks. The gating mechanisms allow neural networks to have paths for information to follow across different layers (\"information highways\").\n\nHighway networks have been used as part of text sequence labeling and speech recognition tasks.\n", "id": "55375136", "title": "Highway network"}
{"url": "https://en.wikipedia.org/wiki?curid=54994687", "text": "Documenting Hate\n\nDocumenting Hate is a project of ProPublica, in collaboration with a number of journalistic, academic, and computing organizations, for systematic tracking of hate crimes and bias incidents. It uses an online form to facilitate reporting of incidents by the general public. Since August 2017, it has also used machine learning and natural language processing techniques to monitor and collect news stories about hate crimes and bias incidents. , over 100 news organizations had joined the project.\n\nDocumenting Hate was created in response to ProPublica's dissatisfaction with the quality of reporting and tracking of evidence of hate crimes and bias incidents after the United States presidential election of 2016. The project was launched on 17 January 2017, after the publication on 15 November 2016 of a ProPublica news story about the difficulty of obtaining hard data on hate crimes.\n\nOn 18 August 2017, ProPublica and Google announced the creation of the Documenting Hate News Index, which uses the Google Cloud Natural Language API for automated monitoring and collection of news stories about hate crimes and bias incidents. The API uses machine learning and natural language processing techniques. The findings of the Index are integrated with reports from members of the public. The Index is a joint project of ProPublica, Google News Lab, and the data visualization studio Pitch Interactive.\n\n, thousands of incidents had been reported via Documenting Hate. , over 100 news organizations had joined the project, including the \"Boston Globe\", the \"New York Times\", \"Vox\", and the Georgetown University \"Hoya\".\n\nAn education reporter for the conservative \"Daily Caller\" has criticized the project for ambiguity in the terms it uses to describe hate crimes, and for neglect of hate-crime hoaxes. Another \"Daily Caller\" journalist has likewise criticized the Documenting Hate News Index for underrepresentation of conservative outlets among the news sources it monitors. \n\nA policy analyst for the Center for Data Innovation (an affiliate of the Information Technology and Innovation Foundation), while supporting ProPublica's critique of the present state of hate-crime statistics, and praising ProPublica for drawing attention to the problem, has argued that a nongovernmental project like Documenting Hate cannot solve it unaided; instead, intervention at the federal level is needed.\n\n", "id": "54994687", "title": "Documenting Hate"}
{"url": "https://en.wikipedia.org/wiki?curid=1363880", "text": "Random forest\n\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n\nThe first algorithm for random decision forests was created by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the \"stochastic discrimination\" approach to classification proposed by Eugene Kleinberg.\n\nAn extension of the algorithm was developed by Leo Breiman and Adele Cutler, and \"Random Forests\" is their trademark. The extension combines Breiman's \"bagging\" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.\n\nThe general method of random decision forests was first proposed by Ho in 1995, who established that forests of trees splitting with oblique hyperplanes, if randomly restricted to be sensitive to only selected feature dimensions, can gain accuracy as they grow without suffering from overtraining. A subsequent work along the same lines concluded that other splitting methods, as long as they are randomly forced to be insensitive to some feature dimensions, behave similarly. Note that this observation of a more complex classifier (a larger forest) getting more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level of accuracy before being hurt by overfitting. The explanation of the forest method's resistance to overtraining can be found in Kleinberg's theory of stochastic discrimination.\nThe early development of Breiman's notion of random forests was influenced by the work of Amit and\nGeman who introduced the idea of searching over a random subset of the\navailable decisions when splitting a node, in the context of growing a single\ntree. The idea of random subspace selection from Ho was also influential in the design of random forests. In this method a forest of trees is grown,\nand variation among the trees is introduced by projecting the training data\ninto a randomly chosen subspace before fitting each tree or each node. Finally, the idea of\nrandomized node optimization, where the decision at each node is selected by a\nrandomized procedure, rather than a deterministic optimization was first\nintroduced by Dietterich.\n\nThe introduction of random forests proper was first made in a paper\nby Leo Breiman. This paper describes a method of building a forest of\nuncorrelated trees using a CART like procedure, combined with randomized node\noptimization and bagging. In addition, this paper combines several\ningredients, some previously known and some novel, which form the basis of the\nmodern practice of random forests, in particular:\n\n\nThe report also offers the first theoretical result for random forests in the\nform of a bound on the generalization error which depends on the strength of the\ntrees in the forest and their correlation.\n\nDecision trees are a popular method for various machine learning tasks. Tree learning \"come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining\", say Hastie \"et al.\", \"because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate\".\n\nIn particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, i.e. have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance in the final model.\n\nThe training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set = , ..., with responses = , ..., , bagging repeatedly (\"B\" times) selects a random sample with replacement of the training set and fits trees to these samples:\n\nAfter training, predictions for unseen samples can be made by averaging the predictions from all the individual regression trees on :\n\nor by taking the majority vote in the case of classification trees.\n\nThis bootstrapping procedure leads to better model performance because it decreases the variance of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets.\n\nAdditionally, an estimate of the uncertainty of the prediction can be made as the standard deviation of the predictions from all the individual regression trees on :\n\nThe number of samples/trees, , is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. An optimal number of trees can be found using cross-validation, or by observing the \"out-of-bag error\": the mean prediction error on each training sample , using only the trees that did not have in their bootstrap sample.\nThe training and test error tend to level off after some number of trees have been fit.\n\nThe above procedure describes the original bagging algorithm for trees. Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called \"feature bagging\". The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the trees, causing them to become correlated. An analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by Ho.\n\nTypically, for a classification problem with features, (rounded down) features are used in each split. For regression problems the inventors recommend (rounded down) with a minimum node size of 5 as the default.\n\nAdding one further step of randomization yields \"extremely randomized trees\", or ExtraTrees. These are trained using bagging and the random subspace method, like in an ordinary random forest, but additionally the top-down splitting in the tree learner is randomized. Instead of computing the locally \"optimal\" feature/split combination (based on, e.g., information gain or the Gini impurity), for each feature under consideration, a random value is selected for the split. This value is selected from the feature's empirical range (in the tree's training set, i.e., the bootstrap sample).\n\nRandom forests can be used to rank the importance of variables in a regression or classification problem in a natural way. The following technique was described in Breiman's original paper and is implemented in the R package randomForest.\n\nThe first step in measuring the variable importance in a data set formula_3 is to fit a random forest to the data. During the fitting process the out-of-bag error for each data point is recorded and averaged over the forest (errors on an independent test set can be substituted if bagging is not used during training).\n\nTo measure the importance of the formula_4-th feature after training, the values of the formula_4-th feature are permuted among the training data and the out-of-bag error is again computed on this perturbed data set. The importance score for the formula_4-th feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees. The score is normalized by the standard deviation of these differences.\n\nFeatures which produce large values for this score are ranked as more important than features which produce small values.\n\nThis method of determining variable importance has some drawbacks. For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. Methods such as partial permutations\nand growing unbiased trees can be used to solve the problem. If the data contain groups of correlated features of similar relevance for the output, then smaller groups are favored over larger groups.\n\nA relationship between random forests and the -nearest neighbor algorithm (-NN) was pointed out by Lin and Jeon in 2002. It turns out that both can be viewed as so-called \"weighted neighborhoods schemes\". These are models built from a training set formula_7 that make predictions formula_8 for new points by looking at the \"neighborhood\" of the point, formalized by a weight function :\n\nHere, formula_10 is the non-negative weight of the 'th training point relative to the new point in the same tree. For any particular , the weights for points formula_11 must sum to one. Weight functions are given as follows:\n\n\nSince a forest averages the predictions of a set of trees with individual weight functions formula_14, its predictions are\n\nThis shows that the whole forest is again a weighted neighborhood scheme, with weights that average those of the individual trees. The neighbors of in this interpretation are the points formula_11 sharing the same leaf in any tree formula_4. In this way, the neighborhood of depends in a complex way on the structure of the trees, and thus on the structure of the training set. Lin and Jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature.\n\nAs part of their construction, random forest predictors naturally lead to a dissimilarity measure between the observations. One can also define a random forest dissimilarity measure between unlabeled data: the idea is to construct a random forest predictor that distinguishes the “observed” data from suitably generated synthetic data.\nThe observed data are the original unlabeled data and the synthetic data are drawn from a reference distribution. A random forest dissimilarity can be attractive because it handles mixed variable types well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. The random forest dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the \"Addcl 1\" random forest dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. The random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.\n\nInstead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes classifiers.\n\nIn machine learning, kernel random forests establish the connection between random forests and kernel methods. By slightly modifying their definition, random forests can be rewritten as kernel methods, which are more interpretable and easier to analyze.\n\nLeo Breiman was the first person to notice the link between random forest and kernel methods. He pointed out that random forests which are grown using i.i.d. random vectors in the tree construction are equivalent to a kernel acting on the true margin. Lin and Jeon established the connection between random forests and adaptive nearest neighbor, implying that random forests can be seen as adaptive kernel estimates. Davies and Ghahramani proposed Random Forest Kernel and show that it can empirically outperform state-of-art kernel methods. Scornet first defined KeRF estimates and gave the explicit link between KeRF estimates and random forest. He also gave explicit expressions for kernels based on centered random forest and uniform random forest, two simplified models of random forest. He named these two KeRFs by Centered KeRF and Uniform KeRF, and proved upper bounds on their rates of consistency.\n\nCentered forest is a simplified model for Breiman's original random forest, which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. The algorithm stops when a fully binary tree of level formula_18 is built, where formula_19 is a parameter of the algorithm.\n\nUniform forest is another simplified model for Breiman's original random forest, which uniformly selects a feature among all features and performs splits at a point uniformly drawn on the side of the cell, along the preselected feature.\n\nGiven a training sample formula_20 of formula_21-valued independent random variables distributed as the independent prototype pair formula_22, where formula_23. We aim at predicting the response formula_24, associated with the random variable formula_25, by estimating the regression function formula_26. A random regression forest is an ensemble of formula_27 randomized regression trees. Denote formula_28 the predicted value at point formula_29 by the formula_4-th tree, where formula_31 are independent random variables, distributed as a generic random variable formula_32, independent of the sample formula_33. This random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate formula_34.\nFor regression trees, we have formula_35, where formula_36 is the cell containing formula_29, designed with randomness formula_38 and dataset formula_33, and formula_40.\n\nThus random forest estimates satisfy, for all formula_41, formula_42. Random regression forest has two level of averaging, first over the samples in the target cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the random forest methods and compensate the misestimation, Scornet defined KeRF by\n\nwhich is equal to the mean of the formula_44's falling in the cells containing formula_29 in the forest. If we define the connection function of the formula_27 finite forest as formula_47, i.e. the proportion of cells shared between formula_29 and formula_49, then almost surely we have formula_50, which defines the KeRF.\n\nThe construction of Centered KeRF of level formula_18 is the same as for centered forest, except that predictions are made by formula_52, the corresponding kernel function, or connection function is\n\nUniform KeRF is built in the same way as uniform forest, except that predictions are made by formula_52, the corresponding kernel function, or connection function is\n\nPredictions given by KeRF and random forests are close if the number of points in each cell is controlled:\nAssume that there exist sequences formula_56 such that, almost surely,\nThen almost surely,\nWhen the number of trees formula_27 goes to infinity, then we have infinite random forest and infinite KeRF. Their estimates are close if the number of observations in each cell is bounded:\nAssume that there exist sequences formula_60 such that, almost surely\nThen almost surely,\nAssume that formula_65, where formula_66 is a centered Gaussian noise, independent of formula_25, with finite variance formula_68. Moreover, formula_25 is uniformly distributed on formula_70 and formula_71 is Lipschitz. Scornet proved upper bounds on the rates of consistency for centered KeRF and uniform KeRF.\n\nProviding formula_72 and formula_73, there exists a constant formula_74 such that, for all formula_75,\nformula_76.\n\nProviding formula_72 and formula_73, there exists a constant formula_79 such that,\nformula_80.\n\n\n\n", "id": "1363880", "title": "Random forest"}
{"url": "https://en.wikipedia.org/wiki?curid=55817338", "text": "Algorithmic bias\n\nAlgorithmic bias occurs when data used to teach a machine learning system reflects implicit values of humans involved in that data collection, selection, or use. Algorithmic bias has been identified and critiqued for its impact on search engine results, social media platforms, privacy, and racial profiling. In search results, this bias can create results reflecting racist, sexist, or other social biases, despite the presumed neutrality of the data. The study of algorithmic bias is most concerned with algorithms that reflect \"systematic and unfair\" discrimination.\n\nAs algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways unanticipated output and manipulation can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise, and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Nonetheless, bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or through use in unanticipated contexts or by audiences not considered in their initial design.\n\nAlgorithmic bias has been discovered or theorized in cases ranging from election outcomes to the spread of online hate speech. Problems in understanding, researching, and discovering algorithmic bias may come from the proprietary nature of algorithms, which are typically treated as trade secrets. Even with full transparency, understanding algorithms can be difficult because of their complexity, and because not every permutation of an algorithm's input or output can be anticipated or reproduced. In many cases, even within a single use-case (such as a website or app), there is no single \"algorithm\" to examine, but a vast network of interrelated programs and data inputs, even between users of the same service.\n\nAlgorithms are difficult to define, but may be generally understood as sets of instructions within computer programs that determine how these programs read, collect, process, and analyze data to generate some readable form of analysis or output. Newer computers can process millions of these algorithmic instructions per second, which has boosted the design and adoption of technologies such as machine learning and artificial intelligence. By analyzing and processing data, algorithms drive search engines, social media websites, recommendation engines, online retail, online advertising, and more.\n\nContemporary social scientists are concerned with algorithmic processes embedded into hardware and software applications in order to understand their political effects, and to question the underlying assumptions of their neutrality. The term \"algorithmic bias\" is used to describe systematic and repeatable errors that create unfair outcomes, i.e., generating one result for certain users and another result for others. For example, a credit score algorithm may deny a loan based on certain factors without being unfair if it is consistently weighing relevant financial criteria. If that algorithm allows loans to some, but denies loans to another set of nearly identical users based on arbitrary criteria, and if this behavior can be repeated across multiple occurrences, an algorithm can be described as \"biased\". This bias may be intentional or unintentional.\n\nBias can be introduced to an algorithm in several ways. During the assemblage of a database, data must be collected, digitized, adapted, and entered according to human-assisted cataloging criteria. Next, in the design of the algorithm, programmers assign certain priorities, or hierarchies, in how programs assess and sort that data. This requires human decisions about how data is categorized and which data is discarded. Some algorithms collect their own data based on human-selected criteria, which can reflect the bias of human users. Others may practice reinforcing stereotypes and preferences as they process and display \"relevant\" data for human users, as in selecting information based on previous choices of a user, or group of users.\n\nBeyond assembling the data, bias can emerge as a result of design. Examples may arise: In sorting processes that determine the allocation of resources or scrutiny (as in determining school placements), or classification and identification processes that may inadvertently discriminate against a category when assigning risk (as in credit scores). In processing associations, such as recommendation engines or inferred marketing traits, algorithms may be flawed in ways that reveal personal information. Inclusion and exclusion criteria may have unanticipated outcomes for search results, such as in flight recommendation software omitting flights that don't follow the sponsoring airline's preferred flight paths. Algorithms may also display an \"uncertainty bias\", offering more confident assessments when larger data sets are available. This can skew algorithmic processes toward results that more closely correspond with larger sample populations, which may not align with data from underrepresented populations.\n\nThe earliest computer programs reflected simple, human-derived operations, and were deemed to be functioning when they completed those operations. Artificial Intelligence pioneer Joseph Weizenbaum wrote that such programs are therefore understood to \"embody law\". Weizenbaum describes early, simple computer programs changing perceptions of machines from transferring power to transferring information. However, he noted that machines might transfer information with unintended consequences if there are errors in details provided to the machine, and if users interpret data in intuitive ways that cannot be formally communicated to, or from, a machine. Weizenbaum stated that all data fed to a machine must reflect \"human decisionmaking processes\" which have been translated into rules for the computer to follow. To do this, Weizenbaum asserted that programmers \"legislate the laws for a world one first has to create in imagination,\" and as a result, computer simulations can be built on models with incomplete or incorrect human data. Weizenbaum compared the results of such decisions to a tourist in a country who can make \"correct\" decisions through a coin toss, but has no basis of understanding how or why the decision was made.\n\nThe complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design. Decisions made by one designer, or team of designers, may be obscured among the many pieces of code created for a single program; over time these decisions and their impact may be forgotten and taken as natural results of the program's output. These biases can create new patterns of behavior, or \"scripts,\" in relationship to specific technologies as the code interacts with other elements of society. Biases may also impact how society shapes itself around the data points that algorithms require.\n\nThe decisions of algorithmic programs can be weighed more heavily than the decisions of the human beings they are meant to assist, a process described by author Clay Shirky as \"algorithmic authority\". Shirky uses the term to describe \"the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources,\" such as search results. This neutrality can also be misrepresented through language frames used when results are presented to the public. For example, a list of news items selected and presented as \"trending\" or \"popular\" may be weighed based on significantly wider criteria than their popularity.\n\nBecause of their convenience and authority, algorithms are theorized as a means of delegating responsibility in decision making away from humans. This can have the effect of reducing alternative options, compromises, or flexibility. Sociologist Scott Lash has critiqued algorithms as a new form of \"generative power\" in that they are a virtual means of generating actual ends.\n\nPre-existing bias in an algorithm is a consequence of underlying social and institutional ideologies. Such ideas may reflect personal biases of individual designers or programmers, or can reflect social, institutional, or cultural assumptions. In both cases, such prejudices can be explicit and conscious, or implicit and unconscious. Poorly selected input data will influence the outcomes created by machines. In a critical view, encoding pre-existing bias into software can preserve social and institutional bias, and replicate it into all possible uses of the algorithm into the future.\n\nAn example of this form of bias is the British Nationality Act Program, designed to automate the evaluation of new UK citizens after the 1981 British Nationality Act. The program accurately reflected the tenets of the law, which stated that \"a man is the father of only his legitimate children, whereas a woman is the mother of all her children, legitimate or not.\" By attempting to appropriately articulate this logic into an algorithmic process, the BNAP inscribed the logic of the British Nationality Act into its algorithm.\n\nTechnical bias emerges through limitations of a program, computational power, its design, or other constraint on the system. Such bias can also be a restraint of design, for example, a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three, as in an airline price display. Flaws in random number generation can also introduce bias into results.\n\nA \"decontextualized algorithm\" uses unrelated information to sort results, for example, a flight-pricing algorithm that sorts results by alphabetical order would be biased in favor of American Airlines over United Airlines. The opposite may also apply, in which results are evaluated in different contexts from which they are collected. For example, data may be collected without crucial external context when facial recognition software is used by surveillance cameras, but evaluated by remote staff in another country or region, or evaluated by non-human algorithms with no awareness of what takes place beyond the camera's field of vision.\n\nLastly, technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior will correlate. For example, software that weighs data points to determine whether a defendant should accept a plea bargain, while ignoring the impact of emotion on a jury, is displaying a form of technical bias.\n\nEmergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts. New forms of knowledge, such as drug or medical breakthroughs, new laws, business models, or shifting cultural norms, may be discovered without algorithms being adjusted to consider them. This may exclude groups through technology, without delineating clear outlines of authorship or personal responsibility. Similarly, problems may emerge when training data, i.e., the samples \"fed\" to a machine by which it models certain conclusions, do not align with uses that algorithm encounters in the real world.\n\nIn 1990, an example of emergent bias was identified in the software used to place US medical students into residencies, the National Residency Match Program (NRMP). The algorithm was designed at a time when few married couples would seek residencies together. As more women entered medical schools, more students were likely to request a residency alongside their partners. The process calls for each applicant to provide a list of preferences for placement across the US, which is then sorted and assigned when a hospital and an applicant both agree to a match. In the case of married couples where both sought residencies, the algorithm weighed a \"lead member's\" location choices first. Once it identified an optimum placement for that person, it removed distant locations from their partner's preferences, reducing their list to the preferred locations within the same city as the partner. The result was a frequent assignment of high-rated schools for the first partner and lower-preference schools to the second partner, rather than sorting for compromises in placement preference.\n\nAdditional emergent biases include:\n\nUnpredictable correlations can emerge when large data sets are compared to each other in practice. For example, data collected about web-browsing patterns may align with signals marking sensitive data (such as race or sexual orientation). By \"discrimination\" against certain behavior or browsing patterns, the end effect would be almost identical to discrimination through the use of direct race or orientation data. In other cases, correlations can be inferred for reasons beyond the algorithm's ability to understand them, as when a triage program gave lower priority to asthmatics who had pneumonia. Because asthmatics with pneumonia were at the highest risk, hospitals typically give them the best and most immediate care; the algorithm simply compared survival rates.\n\nEmergent bias can occur when an algorithm is used by unanticipated audiences, such as machines that demand users can read, write, or understand numbers. Certain metaphors may not carry across different populations or skill sets. For example, the British National Act Program was created as a proof-of-concept by computer scientists and immigration lawyers to evaluate suitability for British citizenship. The designers therefore have expertise beyond the user, whose understanding of both software and immigration law would likely be unsophisticated. The agents administering the questions would not be aware of alternative pathways to citizenship outside of the software, and shifting case law and legal interpretations would lead the algorithm to outdated results.\n\nAn area of concern around emergent bias is that it may be compounded as biased technology is more deeply integrated into society. For example, users with vision impairments may not be able to use an ATM, but can easily go to a bank branch. If bank branches begin to close because ATMs replace them, they begin to exclude vision-impaired users from banking, an unintended consequence of a technology.\n\nEmergent bias may also create a feedback loop, or recursion, if data collected for an algorithm results in real-world responses which are fed back into the algorithm. For example, simulations of the predictive policing software, PredPol, deployed in Oakland, California, suggested an increased police presence in black neighborhoods based on crime data reported by the public. The simulation showed that public reports of crime could rise based on the sight of increased police activity, and could be interpreted by the software in modeling predictions of crime, and to encourage a further increase in police presence within the same neighborhoods. The Human Rights Data Analysis Group, which conducted the study, warned that in places where racial discrimination is a factor in arrests, such feedback loops could reinforce and perpetuate racial discrimination in policing.\n\nAn early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to St. George's Hospital Medical School per year from 1982 to 1986, based on implementation of a new computer-guidance assessment system that denied entry to women and men with \"foreign-sounding names\" based on historical trends in admissions. Other examples include the display of higher-paying jobs to male applicants on job search websites.\n\nThe plagiarism-detection software Turnitin compares student-written texts to information found online and returns a probability that the student's work is copied. Because the software compares strings of text, it is more likely to identify non-native speakers of English than native speakers, who might be better able to adapt individual words and break up strings of plagiarized text, or obscure them through synonyms.\n\nSurveillance camera software may be considered inherently political as it requires algorithms to identify and flag normal from abnormal behaviors, and to determine who belongs in certain locations at certain times. A 2002 analysis of facial recognition software used to identify individuals in CCTV images found several examples of bias. Software was assessed as identifying men more frequently than women, older people more frequently than younger people, and identified Asians, African-Americans and other races more often than whites.\n\nIn 2017 a Facebook algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content, according to internal Facebook documents. The algorithm, which is a blend of computer programs and human content reviewers, was created to protect broad categories rather than specific subsets of categories. For example, posts denouncing \"Muslims\" would be blocked, while posts denouncing \"Radical Muslims\" would be allowed. An unanticipated outcome of the algorithm is to allow hate speech against black children, because they denounce the \"children\" subset of blacks, rather than \"all blacks,\" whereas \"all white men\" would trigger a block, because whites and males are not considered subsets. Facebook was also found to allow ad purchasers to target \"Jew haters\" as a category of users, which the company said was an inadvertent outcome of algorithms used in assessing and categorizing data. The company's design also allowed ad buyers to block African-Americans from seeing housing ads.\n\nCorporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies, without the knowledge of a user who may mistake the algorithm as being impartial. For example, American Airlines created a flight-finding algorithm in the 1980s. The software presented a range of flights from various airlines to customers, but weighed factors that boosted its own flights, regardless of price or convenience. In testimony to the United States Congress, the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment.\n\nIn a 1998 paper describing Google, the founders of the company adopted a policy of transparency in search results regarding paid placement, arguing that “advertising-funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers.” This bias would be an \"invisible\" manipulation of the user.\n\nA series of studies of undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about 20%. The researchers concluded that candidates have \"no means of competing\" if an algorithm, with or without intent, boosted page listings for a rival candidate.\n\nFacebook users who saw messages related to voting were more likely to vote themselves. A randomized trial of Facebook users showing an increased effect of 340,000 votes among users, and friends of users, who saw pro-voting messages in 2010. The legal scholar Jonathan Zittrain has warned that this could create a \"digital gerrymandering\" effect in elections, \"the selective presentation of information by an intermediary to meet its agenda, rather than to serve its users,\" if intentionally manipulated.\n\nIn 2016, the professional networking site LinkedIn was discovered to recommend male variations of women's names in response to search queries for women. The site did not make similar recommendations in searches for male names. For example, \"Andrea\" would bring up a prompt asking if users meant \"Andrew,\" but queries for \"Andrew\" did not ask if users meant to find \"Andrea\". The company said this was the result of an analysis of users' interactions with the site.\n\nIn 2012, the department store franchise Target was cited for gathering data points to infer when women customers were pregnant, even if they hadn't announced it, and then sharing that information with marketing partners. Because the data had been predicted, rather than directly observed or reported, the company had no legal obligation to protect the privacy of those customers.\n\nWeb search algorithms have also been accused of bias. Google's results may prioritize pornographic content in search terms related to sexuality, for example, \"lesbian\". This bias extends to the search engine surfacing popular but sexualized content in neutral searches, as in \"Top 25 Sexiest Women Athletes\" articles displayed as first-page results in searches for \"women athletes\". In 2017 Google announced plans to curb search results that surfaced hate groups, racist views, child abuse and pornography, and other upsetting and offensive content.\n\nAlgorithms have been criticized as a method for obscuring racial prejudices in decision-making. Lisa Nakamura has noted that census machines were among the first to adopt the punch-card processes that lead to contemporary computing, and that their use as categorization and sorting machines for race has been long established and socially tolerated.\n\nOne example is the use of risk assessments in criminal sentencing and parole hearings, an algorithmically generated score intended to reflect the risk that a suspect or prisoner will repeat a crime. From 1920 until 1970, the nationality of a suspect's father was a consideration in such risk assessments. Today, these scores are shared with judges in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington, and Wisconsin. An independent investigation by ProPublica found that the scores were inaccurate 80% of the time, and disproportionately skewed to suggest blacks to be at risk of relapse, 77% more often than whites.\n\nIn 2015, Google apologized when black users complained that an image-identification algorithm in its Photos application identified them as gorillas. In 2010, Nikon cameras were criticized when image-recognition algorithms consistently asked Asian users if they were blinking. Such examples are the product of bias in biometric data sets. Biometric data is drawn from aspects of the body, including racial features either observed or inferred, which can then be transferred into data points.\n\nBiometric data about race may also be inferred, rather than observed. For example, a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records, regardless of any police record of that individual's name.\n\nIn 2011, users of the gay hookup app Grindr reported that the app was linked to sex-offender lookup apps in the Android app store's recommendation algorithm. Writer Mike Ananny criticized this association in The Atlantic, arguing that such associations further stigmatized gay men and may discourage closeted men to maintain secrecy. A 2009 incident with online retailer Amazon saw 57,000 books de-listed after a shift in the algorithm expanded its \"adult content\" blacklist for pornographic works to any books addressing sexuality or gay themes, for example, the critically acclaimed novel \"Brokeback Mountain\".\n\nSeveral challenges impede the study of large-scale algorithmic bias, hindering the application of academically rigorous studies and public understanding.\n\nCommercial algorithms are proprietary, and may be treated as trade secrets. This protects companies, such as a search engine, in cases where a transparent algorithm for ranking results would reveal techniques for manipulating the service. This makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function. It can also be used to obscure possible unethical methods used in producing or processing algorithmic output. The closed nature of the code is not the only concern, however; as a certain degree of obscurity is protected by the complexity of contemporary programs, and the inability to know every permutations of a code's input or output.\n\nSocial scientist Bruno Latour has identified this process as blackboxing, a process in which \"scientific and technical work is made invisible by its own success. When a machine runs efficiently, when a matter of fact is settled, one need focus only on its inputs and outputs and not on its internal complexity. Thus, paradoxically, the more science and technology succeed, the more opaque and obscure they become.\" Others have critiqued the black box metaphor, suggesting that current algorithms are not one black box, but a network of interconnected ones.\n\nAlgorithmic processes are complex, often exceeding the understanding of the people who use them. Large-scale operations may not be understood even by those involved in creating them. The social media site Facebook factored in at least 100,000 data points to determine the layout of a user's social media feed in 2013. Furthermore, large teams of programmers may operate in relative isolation from one another, and be unaware of the cumulative effects of small decisions with nested sections of sprawling algorithmic processes. Not all code is original, and may be borrowed from other libraries, creating a complicated set of relationships between data processing and data input systems.\n\nA significant barrier to understanding tackling bias in practice is that categories, such as demographics of individuals protected by anti-discrimination law, are often not explicitly held by those collecting and processing data. In some cases, there is little opportunity to collect this data explicitly, such as in device fingerprinting, ubiquitous computing and the Internet of Things. In other cases, the data controller may not wish to collect such data for reputational reasons, or because it represents a heightened liability and security risk. It may also be the case that, at least in relation to the General Data Protection Regulation, such data falls under the 'special category' provisions (Article 9), and therefore comes with more restrictions on potential collection and processing.\n\nAlgorithmic bias does not only relate to protected categories, but can also concern something less easily observable or codifiable, such as political viewpoints. In these cases, there is rarely an easily accessible or non-controversial ground truth, and 'debiasing' such a system becomes considerably more tricky.\n\nFurthermore, false and accidental correlations can emerge from a lack of understanding of protected categories, for example, insurance rates based on historical data of car accidents which may overlap with residential clusters of ethnic minorities.\n\nPersonalization of algorithms based on user interactions such as clicks, time on site, and other metrics, can confuse attempts to understand them. One unidentified streaming radio service reported it had five unique music-selection algorithms it selected for its users based on behavior. This creates widely disparate experiences of the same streaming product between different users.\nCompanies also run frequent A/B tests to fine-tune algorithms based on user response. For example, the search engine Bing can run up to ten million subtle variations of its service per day, segmenting the experience of an algorithm between users, or among the same users.\n\nComputer programs and systems can quickly spread among users, embedding biased algorithms into broader society before their impact can be recognized or remedied.\n\nThe General Data Protection Regulation (GDPR), the European Union's revised data protection regime that enters force in 2018, addresses \"Automated individual decision-making, including profiling\" in Article 22. These rules prohibit \"solely\" automated decisions which have a \"significant\" or \"legal\" effect on an individual, unless they are explicitly authorised by consent, contract, or member state law. Where they are permitted, there must be safeguards in place, such as a right to a human-in-the-loop, and allegedly (although for political reasons, only in in a non-binding recital) a right to an explanation of decisions reached. While these are commonly considered to be new, it is the case that nearly identical provisions have existed across Europe since 1995 in Article 15 of the Data Protection Directive, with the original automated decision rules and safeguards originating in French law in the later 1970s. They have rarely been used, given the heavy carve-outs that exist, and are not discussed in any case law of the European Court of Justice.\n\nThe GDPR does address algorithmic bias in profiling systems, as well as the statistical approaches possible to clean it, directly in recital 71, noting thatthe controller should use appropriate mathematical or statistical procedures for the profiling, implement technical and organisational measures appropriate [...] that prevents, inter alia, discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation, or that result in measures having such an effect.Like the alleged right to an explanation in recital 71, this suffers from the non-binding nature of recitals compared to the binding articles, and while it has been treated as a requirement by the Article 29 Working Party that advise on the implementation of data protection law, its practical dimensions are unclear. It has been argued that the obligatory Data Protection Impact Assessments for high risk data profiling, in tandem with other pre-emptive measures within data protection, may be a better way to tackle issues of algorithmic discrimination than relying on individual transparency rights, as information rights have traditionally fatigued individuals who are too overwhelmed and overburdened to use them effectively.\n\nThe United States has no overall legislation regulating controls for algorithmic bias, approaching the topic through various state and federal laws that might vary by industries, sectors, and uses. Many policies are self-enforced or controlled by the Federal Trade Commission. In 2016, the Obama administration released the National Artificial Intelligence Research and Development Strategic Plan, which called for a critical assessment of algorithms and for researchers to \"design these systems so that their actions and decision-making are transparent and easily interpretable by humans, and thus can be examined for any bias they may contain, rather than just learning and repeating these biases\".\n", "id": "55817338", "title": "Algorithmic bias"}
{"url": "https://en.wikipedia.org/wiki?curid=30992863", "text": "Proaftn\n\nProaftn is a fuzzy classification method that belongs to the class of supervised learning algorithms. The acronym Proaftn stands for: (PROcédure d'Affectation Floue pour la problématique du Tri Nominal), which means in English: Fuzzy Assignment Procedure for Nominal Sorting.\nThe method enables to determine the fuzzy indifference relations by generalizing the indices (concordance and discordance) used in the ELECTRE III method. To determine the fuzzy indifference relations, PROAFTN uses the general scheme of the discretization technique described in, that establishes a set of pre-classified cases called a training set.\n\nTo resolve the classification problems, Proaftn proceeds by the following stages:\n\nStage 1. Modeling of classes: In this stage, the prototypes of the classes are conceived using the two following steps:\n\nDirect technique: It consists in adjusting the parameters through the training set and with the expert intervention.\n\nIndirect technique: It consists in fitting the parameters without the expert intervention as used in machine learning approaches. \nIn multicriteria classification problem, the indirect technique is known as \"preference disaggregation analysis\". This technique requires less cognitive effort than the former technique; it uses an automatic method to determine the optimal parameters, which minimize the classification errors.\nFurthermore, several heuristics and metaheuristics were used to learn the multicriteria classification method Proaftn.\n\nStage 2. Assignment: After conceiving the prototypes, Proaftn proceeds to assign the new objects to specific classes.\n\n", "id": "30992863", "title": "Proaftn"}
{"url": "https://en.wikipedia.org/wiki?curid=43385931", "text": "Data exploration\n\nData exploration is an approach similar to initial data analysis, whereby a data analyst uses visual exploration to understand what is in a dataset and the characteristics of the data, rather than through traditional data management systems. These characteristics can include size or amount of data, completeness of the data, correctness of the data, possible relationships amongst data elements or files/tables in the data.\n\nData exploration is typically conducted using a combination of automated and manual activities. Automated activities can include data profiling or data visualization or tabular reports to give the analyst an initial view into the data and an understanding of key characteristics.\n\nThis is often followed by manual drill-down or filtering of the data to identify anomalies or patterns identified through the automated actions. Data exploration can also require manual scripting and queries into the data (e.g. using languages such as SQL or R) or using Excel or similar tools to view the raw data.\n\nAll of these activities are aimed at creating a clear mental model and understanding of the data in the mind of the analyst, and defining basic metadata (statistics, structure, relationships) for the data set that can be used in further analysis.\n\nOnce this initial understanding of the data is had, the data can be pruned or refined by removing unusable parts of the data, correcting poorly formatted elements and defining relevant relationships across datasets. This process is also known as determining data quality.\n\nAt this stage, the data can be considered ready for deeper analysis or be handed off to other analysts or users who have specific needs for the data.\n\nData exploration can also refer to the adhoc querying and visualization of data to identify potential relationships or insights that may be hidden in the data. In this scenario, hypotheses may be created and then the data is explored to identify whether those hypotheses are correct. \n\nTraditionally, this had been a key area of focus for statisticians, with John Tukey being a key evangelist in the field. Today, data exploration is more widespread and is the focus of data analysts and data scientists; the latter being a relatively new role within enterprises and larger organizations.\n\nThis area of data exploration has become an area of interest in the field of machine learning. This is a relatively new field and is still evolving. As it’s most basic level, a machine-learning algorithm can be fed a data set and can be used to identify whether a hypothesis is true based on the dataset. Common machine learning algorithms can focus on identifying specific patterns in the data. Common patterns include regression, classification or clustering, but there are many possible patterns and algorithms that can be applied to data via machine learning.\n\nBy employing machine learning, it is possible to find patterns or relationships in the data that would be difficult or impossible to find via manual inspection, trial and error or traditional exploration techniques.\n\n", "id": "43385931", "title": "Data exploration"}
{"url": "https://en.wikipedia.org/wiki?curid=54361643", "text": "Hyperparameter optimization\n\nIn machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm.\n\nThe same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data. The objective function takes a tuple of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance.\n\nThe traditional way of performing hyperparameter optimization has been \"grid search\", or a \"parameter sweep\", which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set\nor evaluation on a held-out validation set.\n\nSince the parameter space of a machine learner may include real-valued or unbounded value spaces for certain parameters, manually set bounds and discretization may be necessary before applying grid search.\n\nFor example, a typical soft-margin SVM classifier equipped with an RBF kernel has at least two hyperparameters that need to be tuned for good performance on unseen data: a regularization constant \"C\" and a kernel hyperparameter γ. Both parameters are continuous, so to perform grid search, one selects a finite set of \"reasonable\" values for each, say\n\nGrid search then trains an SVM with each pair (\"C\", γ) in the Cartesian product of these two sets and evaluates their performance on a held-out validation set (or by internal cross-validation on the training set, in which case multiple SVMs are trained per pair). Finally, the grid search algorithm outputs the settings that achieved the highest score in the validation procedure.\n\nGrid search suffers from the curse of dimensionality, but is often embarrassingly parallel because typically the hyperparameter settings it evaluates are independent of each other.\n\nSince grid searching is an exhaustive and therefore potentially expensive method, several alternatives have been proposed. In particular, a randomized search that simply samples parameter settings a fixed number of times has been found to be more effective in high-dimensional spaces than exhaustive search. This is because oftentimes, it turns out some hyperparameters do not significantly affect the loss. Therefore, having randomly dispersed data gives more \"textured\" data than an exhaustive search over parameters that ultimately do not affect the loss.\n\nBayesian optimization is a methodology for the global optimization of noisy black-box functions. Applied to hyperparameter optimization, Bayesian optimization consists of developing a statistical model of the function from hyperparameter values to the objective evaluated on a validation set. Intuitively, the methodology assumes that there is some smooth but noisy function that acts as a mapping from hyperparameters to the objective. In Bayesian optimization, one aims to gather observations in such a manner as to evaluate the machine learning model the least number of times while revealing as much information as possible about this function and, in particular, the location of the optimum. Bayesian optimization relies on assuming a very general prior over functions which when combined with observed hyperparameter values and corresponding outputs yields a distribution over functions. The methodology proceeds by iteratively picking hyperparameters to observe (experiments to run) in a manner that trades off exploration (hyperparameters for which the outcome is most uncertain) and exploitation (hyperparameters which are expected to have a good outcome). In practice, Bayesian optimization has been shown to obtain better results in fewer experiments than grid search and random search, due to the ability to reason about the quality of experiments before they are run.\n\nFor specific learning algorithms, it is possible to compute the gradient with respect to hyperparameters and then optimize the hyperparameters using gradient descent. The first usage of these techniques was focused on neural networks. Since then, these methods have been extended to other models such as support vector machines or logistic regression.\n\nA different approach in order to obtain a gradient with respect to hyperparameters consists in differentiating the steps of an iterative optimization algorithm using automatic differentiation.\n\nEvolutionary optimization is a methodology for the global optimization of noisy black-box functions. In hyperparameter optimization, evolutionary optimization uses evolutionary algorithms to search the space of hyperparameters for a given algorithm. Evolutionary hyperparameter optimization follows a process inspired by the biological concept of evolution:\n\n\nEvolutionary optimization has been used in hyperparameter optimization for statistical machine learning algorithms, automated machine learning, deep neural network architecture search, as well as training of the weights in deep neural networks.\n\nRBF and spectral approaches have also been developed.\n\n\n\n\n\n\n\n\n", "id": "54361643", "title": "Hyperparameter optimization"}
{"url": "https://en.wikipedia.org/wiki?curid=406624", "text": "Time series\n\nA time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.\n\nTime series are very frequently plotted via line charts. Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.\n\nTime series \"analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series \"forecasting is the use of a model to predict future values based on previously observed values. While regression analysis is often employed in such a way as to test theories that the current values of one or more independent time series affect the current value of another time series, this type of analysis of time series is not called \"time series analysis\", which focuses on comparing values of a single time series or multiple dependent time series at different points in time. Interrupted time series analysis is the analysis of interventions on a single time series\n\nTime series data have a natural temporal ordering. This makes time series analysis distinct from cross-sectional studies, in which there is no natural ordering of the observations (e.g. explaining people's wages by reference to their respective education levels, where the individuals' data could be entered in any order). Time series analysis is also distinct from spatial data analysis where the observations typically relate to geographical locations (e.g. accounting for house prices by the location as well as the intrinsic characteristics of the houses). A stochastic model for a time series will generally reflect the fact that observations close together in time will be more closely related than observations further apart. In addition, time series models will often make use of the natural one-way ordering of time so that values for a given period will be expressed as deriving in some way from past values, rather than from future values (see time reversibility.)\n\nTime series analysis can be applied to real-valued, continuous data, discrete numeric data, or discrete symbolic data (i.e. sequences of characters, such as letters and words in the English language).\n\nMethods for time series analysis may be divided into two classes: frequency-domain methods and time-domain methods. The former include spectral analysis and wavelet analysis; the latter include auto-correlation and cross-correlation analysis. In the time domain, correlation and analysis can be made in a filter-like manner using scaled correlation, thereby mitigating the need to operate in the frequency domain.\n\nAdditionally, time series analysis techniques may be divided into parametric and non-parametric methods. The parametric approaches assume that the underlying stationary stochastic process has a certain structure which can be described using a small number of parameters (for example, using an autoregressive or moving average model). In these approaches, the task is to estimate the parameters of the model that describes the stochastic process. By contrast, non-parametric approaches explicitly estimate the covariance or the spectrum of the process without assuming that the process has any particular structure.\n\nMethods of time series analysis may also be divided into linear and non-linear, and univariate and multivariate.\n\nA time series is one type of panel data. Panel data is the general class, a multidimensional data set, whereas a time series data set is a one-dimensional panel (as is a cross-sectional dataset). A data set may exhibit characteristics of both panel data and time series data. One way to tell is to ask what makes one data record unique from the other records. If the answer is the time data field, then this is a time series data set candidate. If determining a unique record requires a time data field and an additional identifier which is unrelated to time (student ID, stock symbol, country code), then it is panel data candidate. If the differentiation lies on the non-time identifier, then the data set is a cross-sectional data set candidate.\n\nThere are several types of motivation and data analysis available for time series which are appropriate for different purposes and etc.\n\nIn the context of statistics, econometrics, quantitative finance, seismology, meteorology, and geophysics the primary goal of time series analysis is forecasting. In the context of signal processing, control engineering and communication engineering it is used for signal detection and estimation, while in the context of data mining, pattern recognition and machine learning time series analysis can be used for clustering, classification, query by content, anomaly detection as well as forecasting.\n\nThe clearest way to examine a regular time series manually is with a line chart such as the one shown for tuberculosis in the United States, made with a spreadsheet program. The number of cases was standardized to a rate per 100,000 and the percent change per year in this rate was calculated. The nearly steadily dropping line shows that the TB incidence was decreasing in most years, but the percent change in this rate varied by as much as +/- 10%, with 'surges' in 1975 and around the early 1990s. The use of both vertical axes allows the comparison of two time series in one graphic.\n\nOther techniques include:\n\n\nCurve fitting is the process of constructing a curve, or mathematical function, that has the best fit to a series of data points, possibly subject to constraints. Curve fitting can involve either interpolation, where an exact fit to the data is required, or smoothing, in which a \"smooth\" function is constructed that approximately fits the data. A related topic is regression analysis, which focuses more on questions of statistical inference such as how much uncertainty is present in a curve that is fit to data observed with random errors. Fitted curves can be used as an aid for data visualization, to infer values of a function where no data are available, and to summarize the relationships among two or more variables. Extrapolation refers to the use of a fitted curve beyond the range of the observed data, and is subject to a degree of uncertainty since it may reflect the method used to construct the curve as much as it reflects the observed data.\n\nThe construction of economic time series involves the estimation of some components for some dates by interpolation between values (\"benchmarks\") for earlier and later dates. Interpolation is estimation of an unknown quantity between two known quantities (historical data), or drawing conclusions about missing information from the available information (\"reading between the lines\"). Interpolation is useful where the data surrounding the missing data is available and its trend, seasonality, and longer-term cycles are known. This is often done by using a related series known for all relevant dates. Alternatively polynomial interpolation or spline interpolation is used where piecewise polynomial functions are fit into time intervals such that they fit smoothly together. A different problem which is closely related to interpolation is the approximation of a complicated function by a simple function (also called regression).The main difference between regression and interpolation is that polynomial regression gives a single polynomial that models the entire data set. Spline interpolation, however, yield a piecewise continuous function composed of many polynomials to model the data set.\n\nExtrapolation is the process of estimating, beyond the original observation range, the value of a variable on the basis of its relationship with another variable. It is similar to interpolation, which produces estimates between known observations, but extrapolation is subject to greater uncertainty and a higher risk of producing meaningless results.\n\nIn general, a function approximation problem asks us to select a function among a well-defined class that closely matches (\"approximates\") a target function in a task-specific way.\nOne can distinguish two major classes of function approximation problems: First, for known target functions approximation theory is the branch of numerical analysis that investigates how certain known functions (for example, special functions) can be approximated by a specific class of functions (for example, polynomials or rational functions) that often have desirable properties (inexpensive computation, continuity, integral and limit values, etc.).\n\nSecond, the target function, call it \"g\", may be unknown; instead of an explicit formula, only a set of points (a time series) of the form (\"x\", \"g\"(\"x\")) is provided. Depending on the structure of the domain and codomain of \"g\", several techniques for approximating \"g\" may be applicable. For example, if \"g\" is an operation on the real numbers, techniques of interpolation, extrapolation, regression analysis, and curve fitting can be used. If the codomain (range or target set) of \"g\" is a finite set, one is dealing with a classification problem instead. A related problem of \"online\" time series approximation is to summarize the data in one-pass and construct an approximate representation that can support a variety of time series queries with bounds on worst-case error.\n\nTo some extent the different problems (regression, classification, fitness approximation) have received a unified treatment in statistical learning theory, where they are viewed as supervised learning problems.\n\nIn statistics, prediction is a part of statistical inference. One particular approach to such inference is known as predictive inference, but the prediction can be undertaken within any of the several approaches to statistical inference. Indeed, one description of statistics is that it provides a means of transferring knowledge about a sample of a population to the whole population, and to other related populations, which is not necessarily the same as prediction over time. When information is transferred across time, often to specific points in time, the process is known as forecasting.\n\nAssigning time series pattern to a specific category, for example identify a word based on series of hand movements in sign language\n\nThis approach is based on harmonic analysis and filtering of signals in the frequency domain using the Fourier transform, and spectral density estimation, the development of which was significantly accelerated during World War II by mathematician Norbert Wiener, electrical engineers Rudolf E. Kálmán, Dennis Gabor and others for filtering signals from noise and predicting signal values at a certain point in time. See Kalman filter, Estimation theory, and Digital signal processing\n\nSplitting a time-series into a sequence of segments. It is often the case that a time-series can be represented as a sequence of individual segments, each with its own characteristic properties. For example, the audio signal from a conference call can be partitioned into pieces corresponding to the times during which each person was speaking. In time-series segmentation, the goal is to identify the segment boundary points in the time-series, and to characterize the dynamical properties associated with each segment. One can approach this problem using change-point detection, or by modeling the time-series as a more sophisticated system, such as a Markov jump linear system.\n\nModels for time series data can have many forms and represent different stochastic processes. When modeling variations in the level of a process, three broad classes of practical importance are the \"autoregressive\" (AR) models, the \"integrated\" (I) models, and the \"moving average\" (MA) models. These three classes depend linearly on previous data points. Combinations of these ideas produce autoregressive moving average (ARMA) and autoregressive integrated moving average (ARIMA) models. The autoregressive fractionally integrated moving average (ARFIMA) model generalizes the former three. Extensions of these classes to deal with vector-valued data are available under the heading of multivariate time-series models and sometimes the preceding acronyms are extended by including an initial \"V\" for \"vector\", as in VAR for vector autoregression. An additional set of extensions of these models is available for use where the observed time-series is driven by some \"forcing\" time-series (which may not have a causal effect on the observed series): the distinction from the multivariate case is that the forcing series may be deterministic or under the experimenter's control. For these models, the acronyms are extended with a final \"X\" for \"exogenous\".\n\nNon-linear dependence of the level of a series on previous data points is of interest, partly because of the possibility of producing a chaotic time series. However, more importantly, empirical investigations can indicate the advantage of using predictions derived from non-linear models, over those from linear models, as for example in nonlinear autoregressive exogenous models. Further references on nonlinear time series analysis: (Kantz and Schreiber), and (Abarbanel)\n\nAmong other types of non-linear time series models, there are models to represent the changes of variance over time (heteroskedasticity). These models represent autoregressive conditional heteroskedasticity (ARCH) and the collection comprises a wide variety of representation (GARCH, TARCH, EGARCH, FIGARCH, CGARCH, etc.). Here changes in variability are related to, or predicted by, recent past values of the observed series. This is in contrast to other possible representations of locally varying variability, where the variability might be modelled as being driven by a separate time-varying process, as in a doubly stochastic model.\n\nIn recent work on model-free analyses, wavelet transform based methods (for example locally stationary wavelets and wavelet decomposed neural networks) have gained favor. Multiscale (often referred to as multiresolution) techniques decompose a given time series, attempting to illustrate time dependence at multiple scales. See also Markov switching multifractal (MSMF) techniques for modeling volatility evolution.\n\nA Hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states. An HMM can be considered as the simplest dynamic Bayesian network. HMM models are widely used in speech recognition, for translating a time series of spoken words into text.\n\nA number of different notations are in use for time-series analysis. A common notation specifying a time series \"X\" that is indexed by the natural numbers is written\n\nAnother common notation is\nwhere \"T\" is the index set.\n\nThere are two sets of conditions under which much of the theory is built:\n\nHowever, ideas of stationarity must be expanded to consider two important ideas: strict stationarity and second-order stationarity. Both models and applications can be developed under each of these conditions, although the models in the latter case might be considered as only partly specified.\n\nIn addition, time-series analysis can be applied where the series are seasonally stationary or non-stationary. Situations where the amplitudes of frequency components change with time can be dealt with in time-frequency analysis which makes use of a time–frequency representation of a time-series or signal.\n\nTools for investigating time-series data include:\n\n\nTime series metrics or features that can be used for time series classification or regression analysis:\n\n\nTime series can be visualized with two categories of chart: Overlapping Charts and Separated Charts. Overlapping Charts display all-time series on the same layout while Separated Charts presents them on different layouts (but aligned for comparison purpose)\n\n\n\nWorking with Time Series data is a relatively common use for statistical analysis software. As a result of this, there are many offerings both commercial and open source. Some examples include:\n\n\n", "id": "406624", "title": "Time series"}
{"url": "https://en.wikipedia.org/wiki?curid=346382", "text": "Image analysis\n\nImage analysis is the extraction of meaningful information from images; mainly from digital images by means of digital image processing techniques. Image analysis tasks can be as simple as reading bar coded tags or as sophisticated as identifying a person from their face.\n\nComputers are indispensable for the analysis of large amounts of data, for tasks that require complex computation, or for the extraction of quantitative information. On the other hand, the human visual cortex is an excellent image analysis apparatus, especially for extracting higher-level information, and for many applications — including medicine, security, and remote sensing — human analysts still cannot be replaced by computers. For this reason, many important image analysis tools such as edge detectors and neural networks are inspired by human visual perception models.\n\nComputer Image Analysis largely contains the fields of computer or machine vision, and medical imaging, and makes heavy use of pattern recognition, digital geometry, and signal processing. This field of computer science developed in the 1950s at academic institutions such as the MIT A.I. Lab, originally as a branch of artificial intelligence and robotics.\n\nIt is the quantitative or qualitative characterization of two-dimensional (2D) or three-dimensional (3D) digital images. 2D images are, for example, to be analyzed in computer vision, and 3D images in medical imaging. The field was established in the 1950s—1970s, for example with pioneering contributions by Azriel Rosenfeld, Herbert Freeman, Jack E. Bresenham, or King-Sun Fu.\n\nThere are many different techniques used in automatically analysing images. Each technique may be useful for a small range of tasks, however there still aren't any known methods of image analysis that are generic enough for wide ranges of tasks, compared to the abilities of a human's image analysing capabilities. Examples of image analysis techniques in different fields include:\n\nDigital Image Analysis is when a computer or electrical device automatically studies an image to obtain useful information from it. Note that the device is often a computer but may also be an electrical circuit, a digital camera or a mobile phone. The applications of digital image analysis are continuously expanding through all areas of science and industry, including:\n\n\"Object-Based Image Analysis\" (OBIA) employs two main processes, segmentation and classification. Traditional image segmentation is on a per-pixel basis. However, OBIA groups pixels into homogeneous objects. These objects can have different shapes and scale. Objects also have statistics associated with them which can be used to classify objects. Statistics can include geometry, context and texture of image objects. The analyst defines statistics in the classification process to generate for example land cover. The technique is implemented in software such as eCognition or the Orfeo_toolbox.\n\nWhen applied to earth images, OBIA is known as \"Geographic Object-Based Image Analysis\" (GEOBIA), defined as \"a sub-discipline of geoinformation science devoted to (...) partitioning remote sensing (RS) imagery into meaningful image-objects, and assessing their characteristics through spatial, spectral and temporal scale\".\nThe international GEOBIA conference has been held biannually since 2006.\n\nLand cover and land use change detection using remote sensing and geospatial data provides baseline information for assessing the climate change impacts on habitats and biodiversity, as well as natural resources, in the target areas.\n\n\n\n", "id": "346382", "title": "Image analysis"}
{"url": "https://en.wikipedia.org/wiki?curid=1571780", "text": "Condensation algorithm\n\nThe condensation algorithm (Conditional Density Propagation) is a computer vision algorithm. The principal application is to detect and track the contour of objects moving in a cluttered environment. Object tracking is one of the more basic and difficult aspects of computer vision and is generally a prerequisite to object recognition. Being able to identify which pixels in an image make up the contour of an object is a non-trivial problem. Condensation is a probabilistic algorithm that attempts to solve this problem.\n\nThe algorithm itself is described in detail by Isard and Blake in a publication in the International Journal of Computer Vision in 1998. One of the most interesting facets of the algorithm is that it does not compute on every pixel of the image. Rather, pixels to process are chosen at random, and only a subset of the pixels end up being processed. Multiple hypotheses about what is moving are supported naturally by the probabilistic nature of the approach. The evaluation functions come largely from previous work in the area and include many standard statistical approaches. The original part of this work is the application of particle filter estimation techniques.\n\nThe algorithm’s creation was inspired by the inability of Kalman filtering to perform object tracking well in the presence of significant background clutter. The presence of clutter tends to produce probability distributions for the object state which are multi-modal and therefore poorly modeled by the Kalman filter. The Condensation Algorithm in its most general form requires no assumptions about the probability distributions of the object or measurements.\n\nThe Condensation algorithm seeks to solve the problem of estimating the conformation of an object described by a vector formula_1 at time formula_2, given observations formula_3 of the detected features in the images up to and including the current time. The Condensation Algorithm outputs an estimate to the state conditional probability density formula_4 by applying a nonlinear filter based on factored sampling and can be thought of as a development of a Monte-Carlo method. formula_4 is a representation of the probability of possible conformations for the objects based on previous conformations and measurements. The condensation algorithm is a generative model since it models the joint distribution of the object and the observer.\n\nThe conditional density of the object at the current time formula_4 is estimated as a weighted, time-indexed sample set formula_7 with weights formula_8. N is a parameter determining the number of sample sets chosen. A realization of formula_4 is obtained by sampling with replacement from the set formula_10 with probability equal to the corresponding element of formula_11.\n\nThe assumptions that object dynamics form a temporal Markov chain and that observations are independent of each other and the dynamics facilitate the implementation of the Condensation Algorithm. The first assumption allows the dynamics of the object to be entirely determined by the conditional density formula_12. The model of the system dynamics determined by formula_12 must also be selected for the algorithm, and generally includes both deterministic and stochastic dynamics.\n\nThe algorithm can be summarized by initialization at time formula_14 and three steps at each time t:\n\nForm the initial sample set and weights by sampling according to the prior distribution . For example, specify as Gaussian and set the weights equal to each other.\n\n\nThis algorithm outputs the probability distribution formula_4 which can be directly used to calculate the mean position of the tracked object, as well as the other moments of the tracked object.\n\nCumulative weights can instead be used to achieve a more efficient sampling.\n\nSince object-tracking can be a real-time objective, consideration of algorithm efficiency becomes important. The Condensation Algorithm is relatively simple when compared to the computational intensity of the Ricatti equation required for Kalman filtering. The parameter formula_24 which determines the number of samples in the sample set will clearly hold a trade off in efficiency versus performance.\n\nOne way to increase efficiency of the algorithm is by selecting a low degree of freedom model for representing the shape of the object. The model used by Isard 1998 is a linear parameterization of B-splines in which the splines are limited to certain configurations. Suitable configurations were found by analytically determining combinations of contours from multiple views, of the object in different poses, and through principal component analysis (PCA) on the deforming object.\n\nIsard and Blake model the object dynamics formula_12 as a second order difference equation with deterministic and stochastic components: formula_26\n\nwhere formula_27 is the mean value of the state, and formula_28, formula_29 are matrices representing the deterministic and stochastic components of the dynamical model respectively. formula_28, formula_29, and formula_27 are estimated via Maximum Likelihood Estimation while the object performs typical movements.\n\nThe observation model formula_33 cannot be directly estimated from the data, requiring assumptions to be made in order to estimate it. Isard 1998 assumes that the clutter which may make the object not visible is a Poisson random process with spatial density formula_34 and that any true target measurement is unbiased and normally distributed with standard deviation formula_35.\n\nThe basic Condensation algorithm is used to track a single object in time. It is possible to extend the Condensation algorithm using a single probability distribution to describe the likely states of multiple objects to track multiple objects in a scene at the same time.\n\nSince clutter can cause the object probability distribution to split into multiple peaks, each peak represents a hypothesis about the object configuration. Smoothing is a statistical technique of conditioning the distribution based on both past and future measurements once the tracking is complete in order to reduce the effects of multiple peaks. Smoothing cannot be directly done in real-time since it requires information of future measurements.\n\nThe algorithm can be used for vision-based robot localization of mobile robots. Instead of tracking the position of an object in the scene however, the position of the camera platform is tracked. This allows the camera platform to be globally localized given a visual map of the environment.\n\nExtensions of the condensation algorithm have also been used to recognize human gestures in image sequences. This application of the condensation algorithm impacts the range of human-computer interactions possible. It has been used to recognize simple gestures of a user at a whiteboard to control actions such as selecting regions of the boards to print or save them. Other extensions have also been used for tracking multiple cars in the same scene.\n\nThe condensation algorithm has also been used for face recognition in a video sequence.\n\nAn implementation of the Condensation Algorithm in C can be found on Michael Isard’s website.\n\nAn implementation in Matlab can be found on the Mathworks File Exchange.\n\nAn example of implementation using the OpenCV library can be found on the OpenCV forums.\n\n", "id": "1571780", "title": "Condensation algorithm"}
{"url": "https://en.wikipedia.org/wiki?curid=5491440", "text": "Multi-scale approaches\n\nThe scale space representation of a signal obtained by Gaussian smoothing satisfies a number of special properties, scale-space axioms, which make it into a special form of multi-scale representation. There are, however, also other types of \"multi-scale approaches\" in the areas of computer vision, image processing and signal processing, in particular the notion of wavelets. The purpose of this article is to describe a few of these approaches:\n\nFor \"one-dimensional signals\", there exists quite a well-developed theory for continuous and discrete kernels that guarantee that new local extrema or zero-crossings cannot be created by a convolution operation. For \"continuous signals\", it holds that all scale-space kernels can be decomposed into the following sets of primitive smoothing kernels:\n\nFor \"discrete signals\", we can, up to trivial translations and rescalings, decompose any discrete scale-space kernel into the following primitive operations:\n\nFrom this classification, it is apparent that we require a continuous semi-group structure, there are only three classes of scale-space kernels with a continuous scale parameter; the Gaussian kernel which forms the scale-space of continuous signals, the discrete Gaussian kernel which forms the scale-space of discrete signals and the time-causal Poisson kernel that forms a temporal scale-space over discrete time. If we on the other hand sacrifice the continuous semi-group structure, there are more options:\n\nFor discrete signals, the use of generalized binomial kernels provides a formal basis for defining the smoothing operation in a pyramid. For temporal data, the one-sided truncated exponential kernels and the first-order recursive filters provide a way to define \"time-causal scale-spaces\" that allow for efficient numerical implementation and respect causality over time without access to the future. The first-order recursive filters also provide a framework for defining recursive approximations to the Gaussian kernel that in a weaker sense preserve some of the scale-space properties.\n\n", "id": "5491440", "title": "Multi-scale approaches"}
{"url": "https://en.wikipedia.org/wiki?curid=5717580", "text": "Active appearance model\n\nAn active appearance model (AAM) is a computer vision algorithm for matching a statistical model of object shape and appearance to a new image. They are built during a training phase. A set of images, together with coordinates of landmarks that appear in all of the images, is provided to the training supervisor.\n\nThe model was first introduced by Edwards, Cootes and Taylor in the context of face analysis at the 3rd International Conference on Face and Gesture Recognition, 1998. Cootes, Edwards and Taylor further described the approach as a general method in computer vision at the European Conference on Computer Vision in the same year. The approach is widely used for matching and tracking faces and for medical image interpretation.\n\nThe algorithm uses the difference between the current estimate of appearance and the target image to drive an optimization process.\nBy taking advantage of the least squares techniques, it can match to new images very swiftly.\n\nIt is related to the active shape model (ASM). One disadvantage of ASM is that it only uses shape constraints (together with some information about the image structure near the landmarks), and does not take advantage of all the available information – the texture across the target object. This can be modelled using an AAM.\n\n\n", "id": "5717580", "title": "Active appearance model"}
{"url": "https://en.wikipedia.org/wiki?curid=5477059", "text": "Scale space implementation\n\nThe linear scale-space representation of an \"N\"-dimensional continuous signal,\n\nis obtained by convolving \"f\" with an \"N\"-dimensional Gaussian kernel:\n\nIn other words:\n\nHowever, for implementation, this definition is impractical, since it is continuous. When applying the scale space concept to a discrete signal \"f\", different approaches can be taken. This article is a brief summary of some of the most frequently used methods.\n\nUsing the \"separability property\" of the Gaussian kernel\n\nthe \"N\"-dimensional convolution operation can be decomposed into a set of separable smoothing steps with a one-dimensional Gaussian kernel \"G\" along each dimension\n\nwhere\n\nand the standard deviation of the Gaussian σ is related to the scale parameter \"t\" according to \"t\" = σ.\n\nSeparability will be assumed in all that follows, even when the kernel is not exactly Gaussian, since separation of the dimensions is the most practical way to implement multidimensional smoothing, especially at larger scales. Therefore, the rest of the article focuses on the one-dimensional case.\n\nWhen implementing the one-dimensional smoothing step in practice, the presumably simplest approach is to convolve the discrete signal \"f\" with a \"sampled Gaussian kernel\":\n\nwhere\n\n(with \"t\" = σ) which in turn is truncated at the ends to give a filter with finite impulse response\n\nfor \"M\" chosen sufficiently large (see error function) such that\n\nA common choice is to set M to a constant C times the standard deviation of the Gaussian kernel\n\nwhere \"C\" is often chosen somewhere between 3 and 6.\n\nUsing the sampled Gaussian kernel can, however, lead to implementation problems, in particular when computing higher-order derivatives at finer scales by applying sampled derivatives of Gaussian kernels. When accuracy and robustness are primary design criteria, alternative implementation approaches should therefore be considered.\n\nFor small values of ε (10 to 10) the errors introduced by truncating the Gaussian are usually negligible. For larger values of ε, however, there are many better alternatives to a rectangular window function. For example, for a given number of points, a Hamming window, Blackman window, or Kaiser window will do less damage to the spectral and other properties of the Gaussian than a simple truncation will. Nonwithstanding this, since the Gaussian kernel decreases rapidly at the tails, the main recommendation is still to use a sufficiently small value of ε such that the truncation effects are no longer important.\n\nA more refined approach is to convolve the original signal by the \"discrete Gaussian kernel\" \"T\"(\"n\", \"t\")\n\nwhere\n\nand formula_14 denotes the modified Bessel functions of integer order, \"n\". This is the discrete counterpart of the continuous Gaussian in that it is the solution to the discrete diffusion equation (discrete space, continuous time), just as the continuous Gaussian is the solution to the continuous diffusion equation.\n\nThis filter can be truncated in the spatial domain as for the sampled Gaussian\n\nor can be implemented in the Fourier domain using a closed-form expression for its discrete-time Fourier transform:\n\nWith this frequency-domain approach, the scale-space properties transfer \"exactly\" to the discrete domain, or with excellent approximation using periodic extension and a suitably long discrete Fourier transform to approximate the discrete-time Fourier transform of the signal being smoothed. Moreover, higher-order derivative approximations can be computed in a straightforward manner (and preserving scale-space properties) by applying small support central difference operators to the discrete scale space representation.\n\nAs with the sampled Gaussian, a plain truncation of the infinite impulse response will in most cases be a sufficient approximation for small values of ε, while for larger values of ε it is better to use either a decomposition of the discrete Gaussian into a cascade of generalized binomial filters or alternatively to construct a finite approximate kernel by multiplying by a window function. If ε has been chosen too large such that effects of the truncation error begin to appear (for example as spurious extrema or spurious responses to higher-order derivative operators), then the options are to decrease the value of ε such that a larger finite kernel is used, with cutoff where the support is very small, or to use a tapered window.\n\nSince computational efficiency is often important, low-order \"recursive filters\" are often used for scale-space smoothing. For example, Young and van Vliet use a third-order recursive filter with one real pole and a pair of complex poles, applied forward and backward to make a sixth-order symmetric approximation to the Gaussian with low computational complexity for any smoothing scale.\n\nBy relaxing a few of the axioms, Lindeberg concluded that good smoothing filters would be \"normalized Pólya frequency sequences\", a family of discrete kernels that includes all filters with real poles at 0 < \"Z\" < 1 and/or \"Z\" > 1, as well as with real zeros at \"Z\" < 0. For symmetry, which leads to approximate directional homogeneity, these filters must be further restricted to pairs of poles and zeros that lead to zero-phase filters.\n\nTo match the transfer function curvature at zero frequency of the discrete Gaussian, which ensures an approximate semi-group property of additive \"t\", two poles at\n\ncan be applied forward and backwards, for symmetry and stability. This filter is the simplest implementation of a normalized Pólya frequency sequence kernel that works for any smoothing scale, but it is not as excellent an approximation to the Gaussian as Young and van Vliet's filter, which is \"not\" normalized Pólya frequency sequence, due to its complex poles.\n\nThe transfer function, \"H\", of a symmetric pole-pair recursive filter is closely related to the discrete-time Fourier transform of the discrete Gaussian kernel via first-order approximation of the exponential:\n\nwhere the \"t\" parameter here is related to the stable pole position \"Z\" = \"p\" via:\n\nFurthermore, such filters with \"N\" pairs of poles, such as the two pole pairs illustrated in this section, are an even better approximation to the exponential:\n\nwhere the stable pole positions are adjusted by solving:\n\nThe impulse responses of these filters are not very close to gaussian unless more than two pole pairs are used. However, even with only one or two pole pairs per scale, a signal successively smoothed at increasing scales will be very close to a gaussian-smoothed signal. The semi-group property is poorly approximated when too few pole pairs are used.\n\nScale-space axioms that are still satisfied by these filters are:\n\n\nThe following are only approximately satisfied, the approximation being better for larger numbers of pole pairs:\n\n\nThis recursive filter method and variations to compute both the Gaussian smoothing as well as Gaussian derivatives has been described by several authors. Tan \"et al.\" have analyzed and compared some of these approaches, and have pointed out that the Young and van Vliet filters are a cascade (multiplication) of forward and backward filters, while the Deriche and the Jin \"et al.\" filters are sums of forward and backward filters.\n\nAt fine scales, the recursive filtering approach as well as other separable approaches are not guaranteed to give the best possible approximation to rotational symmetry, so non-separable implementations for 2D images may be considered as an alternative.\n\nWhen computing several derivatives in the N-jet simultaneously, discrete scale-space smoothing with the discrete analogue of the Gaussian kernel, or with a recursive filter approximation, followed by small support difference operators, may be both faster and more accurate than computing recursive approximations of each derivative operator.\n\nFor small scales, a low-order FIR filter may be a better smoothing filter than a recursive filter. The symmetric 3-kernel , for \"t\" ≤ 0.5 smooths to a scale of \"t\" using a pair of real zeros at \"Z\" < 0, and approaches the discrete Gaussian in the limit of small \"t\". In fact, with infinitesimal \"t\", either this two-zero filter or the two-pole filter with poles at \"Z\" = \"t\"/2 and \"Z\" = 2/\"t\" can be used as the infinitesimal generator for the discrete Gaussian kernels described above.\n\nThe FIR filter's zeros can be combined with the recursive filter's poles to make a general high-quality smoothing filter. For example, if the smoothing process is to always apply a biquadratic (two-pole, two-zero) filter forward then backwards on each row of data (and on each column in the 2D case), the poles and zeros can each do a part of the smoothing. The zeros limit out at \"t\" = 0.5 per pair (zeros at \"Z\" = –1), so for large scales the poles do most of the work. At finer scales, the combination makes an excellent approximation to the discrete Gaussian if the poles and zeros each do about half the smoothing. The \"t\" values for each portion of the smoothing (poles, zeros, forward and backward multiple applications, etc.) are additive, in accordance with the approximate semi-group property.\n\nThe FIR filter transfer function is closely related to the discrete Gaussian's DTFT, just as was the recursive filter's. For a single pair of zeros, the transfer function is\n\nwhere the \"t\" parameter here is related to the zero positions \"Z\" = \"z\" via:\n\nand we require \"t\" ≤ 0.5 to keep the transfer function non-negative.\n\nFurthermore, such filters with \"N\" pairs of zeros, are an even better approximation to the exponential and extend to higher values of \"t\" :\n\nwhere the stable zero positions are adjusted by solving:\n\nThese FIR and pole-zero filters are valid scale-space kernels, satisfying the same axioms as the all-pole recursive filters.\n\nRegarding the topic of automatic scale selection based on normalized derivatives, pyramid approximations are frequently used to obtain real-time performance. The appropriateness of approximating scale-space operations within a pyramid originates from the fact that repeated cascade smoothing with generalized binomial kernels leads to equivalent smoothing kernels that under reasonable conditions approach the Gaussian. Furthermore, the binomial kernels (or more generally the class of generalized binomial kernels) can be shown to constitute the unique class of finite-support kernels that guarantee non-creation of local extrema or zero-crossings with increasing scale (see the article on multi-scale approaches for details). Special care may, however, need to be taken to avoid discretization artifacts.\n\nFor one-dimensional kernels, there is a well-developed theory of multi-scale approaches, concerning filters that do not create new local extrema or new zero-crossings with increasing scales. For continuous signals, filters with real poles in the \"s\"-plane are within this class, while for discrete signals the above-described recursive and FIR filters satisfy these criteria. Combined with the strict requirement of a continuous semi-group structure, the continuous Gaussian and the discrete Gaussian constitute the unique choice for continuous and discrete signals.\n\nThere are many other multi-scale signal processing, image processing and data compression techniques, using wavelets and a variety of other kernels, that do not exploit or require the same requirements as scale space descriptions do; that is, they do not depend on a coarser scale not generating a new extremum that was not present at a finer scale (in 1D) or non-enhancement of local extrema between adjacent scale levels (in any number of dimensions).\n\n", "id": "5477059", "title": "Scale space implementation"}
{"url": "https://en.wikipedia.org/wiki?curid=6838895", "text": "N-jet\n\nAn \"N\"-jet is the set of (partial) derivatives of a function formula_1 up to order \"N\".\n\nSpecifically, in the area of computer vision, the \"N\"-jet is usually computed from a scale space representation formula_2 of the input image formula_3, and the partial derivatives of formula_2 are used as a basis for expressing various types of visual modules. For example, algorithms for tasks such as feature detection, feature classification, stereo matching, tracking and object recognition can be expressed in terms of \"N\"-jets computed at one or several scales in scale space.\n\n", "id": "6838895", "title": "N-jet"}
{"url": "https://en.wikipedia.org/wiki?curid=7095986", "text": "Neighborhood operation\n\nIn computer vision and image processing a neighborhood operation is a commonly used class of computations on image data which implies that it is processed according to the following pseudo code:\n\nThis general procedure can be applied to image data of arbitrary dimensionality. Also, the image data on which the operation is applied does not have to be defined in terms of intensity or color, it can be any type of information which is organized as a function of spatial (and possibly temporal) variables in p.\nThe result of applying a neighborhood operation on an image is again something which can be interpreted as an image, it has the same dimension as the original data. The value at each image point, however, does not have to be directly related to intensity or color. Instead it is an element in the range of the function f, which can be of arbitrary type.\n\nNormally the neighborhood N is of fixed size and is a square (or a cube, depending on the dimensionality of the image data) centered on the point p. Also the function f is fixed, but may in some cases have parameters which can vary with p, see below.\n\nIn the simplest case, the neighborhood N may be only a single point. This type of operation is often referred to as a point-wise operation.\n\nThe most common examples of a neighborhood operation use a fixed function f which in addition is linear, that is, the computation consists of a linear shift invariant operation. In this case, the neighborhood operation corresponds to the convolution operation. A typical example is convolution with a low-pass filter, where the result can be interpreted in terms of local averages of the image data around each image point. Other examples are computation of local derivatives of the image data.\n\nIt is also rather common to use a fixed but non-linear function f. This includes median filtering, and computation of local variances. The Nagao-Matsuyama filter is an example of a complex local neighbourhood operation that uses variance as an indicator of the uniformity within a pixel group. The result is similar to a convolution with a low-pass filter with the added effect of preserving sharp edges. \n\nThere is also a class of neighborhood operations in which the function f has additional parameters which can vary with p:\n\nThis implies that the result is not shift invariant. Examples are adaptive Wiener filters.\n\nThe pseudo code given above suggests that a neighborhood operation is implemented in terms of an outer loop over all image points. However, since the results are independent, the image points can be visited in arbitrary order, or can even be processed in parallel. Furthermore, in the case of linear shift-invariant operations, the computation of f at each point implies a summation of products between the image data and the filter coefficients. The implementation of this neighborhood operation can then be made by having the summation loop outside the loop over all image points.\n\nAn important issue related to neighborhood operation is how to deal with the fact that the neighborhood N becomes more or less undefined for points p close to the edge or border of the image data. Several strategies have been proposed:\n\n", "id": "7095986", "title": "Neighborhood operation"}
{"url": "https://en.wikipedia.org/wiki?curid=4081616", "text": "Relaxation labelling\n\nRelaxation labelling is an image treatment methodology. Its goal is to associate a label to the pixels of a given image or nodes of a given graph.\n\n", "id": "4081616", "title": "Relaxation labelling"}
{"url": "https://en.wikipedia.org/wiki?curid=8755061", "text": "Landmark point\n\nIn morphometrics, landmark point or shortly landmark is a point in a shape object in which correspondences between and within the populations of the object are preserved. In other disciplines, landmarks may be known as vertices, anchor points, control points, sites, profile points, 'sampling' points, nodes, markers, fiducial markers, etc. Landmarks can be defined either manually by experts or automatically by a computer program. There are three basic types of landmarks: anatomical landmarks, mathematical landmarks or pseudo-landmarks. \n\nAn anatomical landmark is a biologically-meaningful point in an organism. Usually experts define anatomical points to ensure their correspondences within the same species. Examples of anatomical landmark in shape of a skull are the eye corner, tip of the nose, jaw, etc. Anatomical landmarks determine homologous parts of an organism, which share a common ancestry.\n\nMathematical landmarks are points in a shape that are located according to some mathematical or geometrical property, for instance, a high curvature point or an extreme point. A computer program usually determines mathematical landmarks used for an automatic pattern recognition.\n\nPseudo-landmarks are constructed points located between anatomical or mathematical landmarks. A typical example is an equally spaced set of points between two anatomical landmarks to get more sample points from a shape. Pseudo-landmarks are useful during shape matching, when the matching process requires a large number of points.\n\n", "id": "8755061", "title": "Landmark point"}
{"url": "https://en.wikipedia.org/wiki?curid=8707155", "text": "Statistical shape analysis\n\nStatistical shape analysis is an analysis of the geometrical properties of some given set of shapes by statistical methods. For instance, it could be used to quantify differences between male and female Gorilla skull shapes, normal and pathological bone shapes, leaf outlines with and without herbivory by insects, etc. Important aspects of shape analysis are to obtain a measure of distance between shapes, to estimate mean shapes from (possibly random) samples, to estimate shape variability within samples, to perform clustering and to test for differences between shapes. One of the main methods used is principal component analysis (PCA). Statistical shape analysis has applications in various fields, including medical imaging, computer vision, computational anatomy, sensor measurement, and geographical profiling.\n\nIn the point distribution model, a shape is determined by a finite set of coordinate points, known as landmark points. These landmark points often correspond to important identifiable features such as the corners of the eyes. Once the points are collected some form of registration is undertaken. This can be a baseline methods used by Fred Bookstein for geometric morphometrics in anthropology. Or an approach like Procrustes analysis which finds an average shape.\n\nDavid George Kendall investigated the statistical distribution of the shape of triangles, and represented each triangle by a point on a sphere. He used this distribution on the sphere to investigate ley lines and whether three stones were more likely to be co-linear than might be expected. Statistical distribution like the Kent distribution can be used to analyse the distribution of such spaces.\n\nAlternatively, shapes can be represented by curves or surfaces representing their contours, by the spatial region they occupy.\n\nDifferences between shapes can be quantified by investigating deformations transforming one shape into another. In particular a diffeomorphism preserves smoothness in the deformation. This was pioneered in D'Arcy Thompson's On Growth and Form before the advent of computers. Deformations can be interpreted as resulting from a force applied to the shape. Mathematically, a deformation is defined as a mapping from a shape \"x\" to a shape \"y\" by a transformation function formula_1, i.e., formula_2. Given a notion of size of deformations, the distance between two shapes can be defined as the size of the smallest deformation between these shapes.\n\n", "id": "8707155", "title": "Statistical shape analysis"}
{"url": "https://en.wikipedia.org/wiki?curid=3145356", "text": "Pose (computer vision)\n\nIn computer vision and in robotics, a typical task is to identify specific objects in an image and to determine each object's position and orientation relative to some coordinate system. This information can then be used, for example, to allow a robot to manipulate an object or to avoid moving into the object. The combination of \"position\" and \"orientation\" is referred to as the pose of an object, even though this concept is sometimes used only to describe the orientation. \"Exterior orientation\" and \"Translation\" are also used as synonyms to pose.\n\nThe image data from which the pose of an object is determined can be either a single image, a stereo image pair, or an image sequence where, typically, the camera is moving with a known velocity. The objects which are considered can be rather general, including a living being or body parts, e.g., a head or hands. The methods which are used for determining the pose of an object, however, are usually specific for a class of objects and cannot generally be expected to work well for other types of objects.\n\nThe pose can be described by means of a rotation and translation transformation which brings the object from a reference pose to the observed pose. This rotation transformation can be represented in different ways, e.g., as a rotation matrix or a quaternion.\n\nThe specific task of determining the pose of an object in an image (or stereo images, image sequence) is referred to as \"pose estimation\". The pose estimation problem can be solved in different ways depending on the image sensor configuration, and choice of methodology. Three classes of methodologies can be distinguished:\n\n\n", "id": "3145356", "title": "Pose (computer vision)"}
{"url": "https://en.wikipedia.org/wiki?curid=7174467", "text": "Connected-component labeling\n\nConnected-component labeling (alternatively connected-component analysis, blob extraction, region labeling, blob discovery, or region extraction) is an algorithmic application of graph theory, where subsets of \"connected components\" are uniquely \"labeled\" based on a given heuristic. Connected-component labeling is not to be confused with segmentation.\n\nConnected-component labeling is used in computer vision to detect connected regions in binary digital images, although color images and data with higher dimensionality can also be processed. When integrated into an image recognition system or human-computer interaction interface, connected component labeling can operate on a variety of information. Blob extraction is generally performed on the resulting binary image from a thresholding step. Blobs may be counted, filtered, and tracked.\n\nBlob extraction is related to but distinct from blob detection.\n\nA graph, containing vertices and connecting edges, is constructed from relevant input data. The vertices contain information required by the comparison heuristic, while the edges indicate connected 'neighbors'. An algorithm traverses the graph, labeling the vertices based on the connectivity and relative values of their neighbors. Connectivity is determined by the medium; image graphs, for example, can be 4-connected or 8-connected.\n\nFollowing the labeling stage, the graph may be partitioned into subsets, after which the original information can be recovered and processed .\n\nThe usage of the term connected components labeling (CCL) and its definition is quite consistent in the academic literature, whereas connected components analysis (CCA) varies in terms of both, terminology and problem definition.\n\nRosenfeld et al. define connected components labeling as the “[c]reation of a labeled image in which the positions associated with the same connected component of the binary input image have a unique label.” Shapiro et al. define CCL as an operator whose “input is a binary image and [...] output is a symbolic image in which the label assigned to each pixel is an integer uniquely identifying the connected component to which that pixel belongs.”\n\nThere is no consensus on the definition of connected components analysis in the academic literature. It is often used interchangeably with CCL. A more extensive definition is given by Shapiro et al.: “Connected component analysis consists of connected component labeling of the black pixels followed by property measurement of the component regions and decision making.” The definition for connected components analysis presented here is more general, taking the thoughts expressed in into account.\n\n“Connected components analysis derives one feature vector of each connected component in a binary 2-D input image. A feature vector of a connected component is an n-tuple composed of functions of the component’s pattern.”\n\nThe algorithms discussed can be generalized to arbitrary dimensions, albeit with increased time and space complexity.\n\nThis is a fast and very simple method to implement and understand. It is based on graph traversal methods in graph theory. In short, once the first pixel of a connected component is found, all the connected pixels of that connected component are labelled before going onto the next pixel in the image. This algorithm is part of Vincent and Soille's watershed segmentation algorithm, other implementations also exist.\n\nIn order to do that a linked list is formed that will keep the indexes of the pixels that are connected to each other, steps (2) and (3) below. The method of defining the linked list specifies the use of a depth or a breadth first search. For this particular application, there is no difference which strategy to use. The simplest kind of a last in first out queue implemented as a singly linked list will result in a depth first search strategy.\n\nIt is assumed that the input image is a binary image, with pixels being either background or foreground and that the connected components in the foreground pixels are desired. The algorithm steps can be written as:\n\n\nNote that the pixels are labelled before being put into the queue. The queue will only keep a pixel to check its neighbours and add them to the queue if necessary. This algorithm only needs to check the neighbours of each foreground pixel once and doesn't check the neighbours of background pixels.\n\nRelatively simple to implement and understand, the two-pass algorithm, iterates through 2-dimensional binary data. The algorithm makes two passes over the image. The first pass to assign temporary labels and record equivalences and the second pass to replace each temporary label by the smallest label of its equivalence class.\n\nThe input data can be modified \"in situ\" (which carries the risk of data corruption), or labeling information can be maintained in an additional data structure.\n\nConnectivity checks are carried out by checking neighbor pixels' labels (neighbor elements whose labels are not assigned yet are ignored), or say, the North-East, the North, the North-West and the West of the current pixel (assuming 8-connectivity). 4-connectivity uses only North and West neighbors of the current pixel. The following conditions are checked to determine the value of the label to be assigned to the current pixel (4-connectivity is assumed)\n\nConditions to check:\n\n\nThe algorithm continues this way, and creates new region labels whenever necessary. The key to a fast algorithm, however, is how this merging is done. This algorithm uses the union-find data structure which provides excellent performance for keeping track of equivalence relationships. Union-find essentially stores labels which correspond to the same blob in a disjoint-set data structure, making it easy to remember the equivalence of two labels by the use of an interface method E.g.: findSet(l). findSet(l) returns the minimum label value that is equivalent to the function argument 'l'.\n\nOnce the initial labeling and equivalence recording is completed, the second pass merely replaces each pixel label with its equivalent disjoint-set representative element.\n\nA faster-scanning algorithm for connected-region extraction is presented below.\n\nOn the first pass:\n\nOn the second pass:\n\nHere, the background is a classification, specific to the data, used to distinguish salient elements from the foreground. If the background variable is omitted, then the two-pass algorithm will treat the background as another region.\n\n1. The array from which connected regions are to be extracted is given below (8-connectivity based).\n\nWe first assign different binary values to elements in the graph. Attention should be paid that the \"0~1\" values written on the center of the elements in the following graph are elements' values. While, the \"1,2...,7\" values in the next two graphs are the elements' labels. The two concepts should not be confused.\n\n2. After the first pass, the following labels are generated. A total of 7 labels are generated in accordance with the conditions highlighted above.\n\nThe label equivalence relationships generated are,\n\n3. Array generated after the merging of labels is carried out. Here, the label value that was the smallest for a given region \"floods\" throughout the connected region and gives two distinct labels, and hence two distinct labels.\n\n4. Final result in color to clearly see two different regions that have been found in the array.\n\nThe pseudocode is as follows:\n\nThe \"find\" and \"union\" algorithms are implemented as described in union find.\n\nCreate a region counter\n\nScan the image (in the following example, it is assumed that scanning is done from left to right and from top to bottom):\n\n\nSome of the steps present in the two-pass algorithm can be merged for efficiency, allowing for a single sweep through the image. Multi-pass algorithms also exist, some of which run in linear time relative to the number of image pixels.\n\nIn the early 1990s, there was considerable interest in parallelizing connected-component algorithms in image analysis applications, due to the bottleneck of sequentially processing each pixel.\n\nThe interest to the algorithm arises again with an extensive use of CUDA.\n\nA one pass (also referred to as single pass) version of the connected-component-labeling algorithm is given as follows. The algorithm identifies and marks the connected components in a single pass. The run time of the algorithm depends on the size of the image and the number of connected components (which create an overhead). The run time is comparable to the two pass algorithm if there are a lot of small objects distributed over the entire image such that they cover a significant number of pixels from it. Otherwise the algorithm runs fairly fast.\n\nAlgorithm:\n\nThe source code is as follows(4-connectivity based):\n\nIn the last two decades many novel approaches on connected-component labeling have been proposed and almost none of them was compared on the same data. YACCLAB\n\nThe emergence of FPGAs with enough capacity to perform complex image processing tasks also led to high-performance architectures for connected component labeling. Most of these architectures utilize the single pass variant of this algorithm, because of the limited memory resources available on an FPGA. These types of connected component labeling architectures are able to process several image pixels in parallel, thereby enabling a high throughput at low processing latency to be achieved.\n\n\n", "id": "7174467", "title": "Connected-component labeling"}
{"url": "https://en.wikipedia.org/wiki?curid=12265304", "text": "Point distribution model\n\nThe point distribution model is a model for representing the mean geometry of a shape and some statistical modes of geometric variation inferred from a training set of shapes.\n\nIt has been developed by Cootes, Taylor \"et al.\" and became a standard in computer vision for the statistical study of shape and for segmentation of medical images where shape priors really help interpretation of noisy and low-contrasted pixels/voxels. The latter point leads to active shape models (ASM) and active appearance models (AAM).\n\nPoint distribution models rely on landmark points. A landmark is an annotating point posed by an anatomist onto a given locus for every shape instance across the training set population. For instance, the same landmark will designate the tip of the index finger in a training set of 2D hands outlines. Principal component analysis (PCA), for instance, is a relevant tool for studying correlations of movement between groups of landmarks among the training set population. Typically, it might detect that all the landmarks located along the same finger move exactly together across the training set examples showing different finger spacing for a flat-posed hands collection.\n\nFirst, a set of training images are manually landmarked with enough corresponding landmarks to sufficiently approximate the geometry of the original shapes. These landmarks are aligned using the generalized procrustes analysis, which minimizes the least squared error between the points.\n\nformula_1 aligned landmarks in two dimensions are given as\n\nIt's important to note that each landmark formula_3 should represent the same anatomical location. For example, landmark #3, formula_4 might represent the tip of the ring finger across all training images.\n\nNow the shape outlines are reduced to sequences of formula_1 landmarks, so that a given training shape is defined as the vector formula_6. Assuming the scattering is gaussian in this space, PCA is used to compute normalized eigenvectors and eigenvalues of the covariance matrix across all training shapes. The matrix of the top formula_7 eigenvectors is given as formula_8, and each eigenvector describes a principal mode of variation along the set.\n\nFinally, a linear combination of the eigenvectors is used to define a new shape formula_9, mathematically defined as:\n\nwhere formula_11 is defined as the mean shape across all training images, and formula_12 is a vector of scaling values for each principal component. Therefore, by modifying the variable formula_12 an infinite number of shapes can be defined. To ensure that the new shapes are all within the variation seen in the training set, it is common to only allow each element of formula_12 to be within formula_153 standard deviations, where the standard deviation of a given principal component is defined as the square root of its corresponding eigenvalue.\n\nPDM's can be extended to any arbitrary number of dimensions, but are typically used in 2D image and 3D volume applications (where each landmark point is formula_16 or formula_17).\n\nAn eigenvector, interpreted in euclidean space, can be seen as a sequence of formula_1 euclidean vectors associated to corresponding landmark and designating a compound move for the whole shape. Global nonlinear variation is usually well handled provided nonlinear variation is kept to a reasonable level. Typically, a twisting nematode worm is used as an example in the teaching of kernel PCA-based methods.\n\nDue to the PCA properties: eigenvectors are mutually orthogonal, form a basis of the training set cloud in the shape space, and cross at the 0 in this space, which represents the mean shape. Also, PCA is a traditional way of fitting a closed ellipsoid to a Gaussian cloud of points (whatever their dimension): this suggests the concept of bounded variation.\n\nThe idea behind PDM's is that eigenvectors can be linearly combined to create an infinity of new shape instances that will 'look like' the one in the training set. The coefficients are bounded alike the values of the corresponding eigenvalues, so as to ensure the generated 2n/3n-dimensional dot will remain into the hyper-ellipsoidal allowed domain—allowable shape domain (ASD).\n\n\n", "id": "12265304", "title": "Point distribution model"}
{"url": "https://en.wikipedia.org/wiki?curid=13038078", "text": "Phase congruency\n\nPhase congruency is a measure of feature significance in computer images, a method of edge detection that is particularly robust against changes in illumination and contrast.\n\nPhase congruency reflects the behaviour of the image in the frequency domain. It has been noted that edgelike features have many of their frequency components in the same phase. The concept is similar to coherence, except that it applies to functions of different wavelength.\n\nFor example, the Fourier decomposition of a square wave consists of sine functions, whose frequencies are odd multiples of the fundamental frequency. At the rising edges of the square wave, each sinusoidal component has a rising phase; the phases have maximal congruency at the edges. This corresponds to the human-perceived edges in an image where there are sharp changes between light and dark.\n\nPhase congruency compares the weighted alignment of the Fourier components of a signal formula_1 with the sum of the Fourier components.\n\nwhere formula_3 is the local or instantaneous phase as can be calculated using the Hilbert transform and formula_1 are the local amplitude, or energy, of the signal. When all the phases are aligned, this is equal to 1.\n\nThe square-wave example is naive in that most edge detection methods deal with it equally well. For example, the first derivative has a maximal magnitude at the edges. However, there are cases where the perceived edge does not have a sharp step or a large derivative. The method of phase congruency applies to many cases where other methods fail.\n\nA notable example is an image feature consisting of a single line, such as the letter \"l\". Many edge-detection algorithms will pick up two adjacent edges: the transitions from white to black, and black to white. On the other hand, the phase congruency map has a single line. A simple Fourier analogy of this case is a triangle wave. In each of its crests there is a congruency of crests from different sinusoidal functions.\n\nCalculating the phase congruency map of an image is very computationally intensive, and sensitive to image noise. Techniques of noise reduction are usually applied prior to the calculation.\n\n", "id": "13038078", "title": "Phase congruency"}
{"url": "https://en.wikipedia.org/wiki?curid=12989981", "text": "Intrinsic dimension\n\nIn signal processing of multidimensional signals, for example in computer vision, the intrinsic dimension of the signal describes how many variables are needed to represent the signal. For a signal of \"N\" variables, its intrinsic dimension \"M\" satisfies \"0 ≤ M ≤ N\".\n\nUsually the intrinsic dimension of a signal relates to variables defined in a Cartesian coordinate system. In general, however, it is also possible to describe the concept for non-Cartesian coordinates, for example, using polar coordinates.\n\nLet \"f(x, x)\" be a two-variable function (or signal) which is of the form\n\nfor some one-variable function \"g\" which is not constant. This means that \"f\" varies, in accordance to \"g\", with the first variable or along the first coordinate. On the other hand, \"f\" is constant with respect to the second variable or along the second coordinate. It is only necessary to know the value of one, namely the first, variable in order to determine the value of \"f\". Hence, it is a two-variable function but its intrinsic dimension is one.\n\nA slightly more complicated example is\n\n\"f\" is still intrinsic one-dimensional, which can be seen by making a variable transformation\n\nwhich gives\n\nSince the variation in \"f\" can be described by the single variable \"y\" its intrinsic dimension is one.\n\nFor the case that \"f\" is constant, its intrinsic dimension is zero since no variable is needed to describe variation. For the general case, when the intrinsic dimension of the two-variable function \"f\" is neither zero or one, it is two.\n\nIn the literature, functions which are of intrinsic dimension zero, one, or two are sometimes referred to as \"i0D\", \"i1D\" or \"i2D\", respectively.\n\nFor an \"N\"-variable function \"f\", the set of variables can be represented as an \"N\"-dimensional vector x:\n\nIf for some \"M\"-variable function \"g\" and \"M × N\" matrix A is it the case that\n\n\nthen the intrinsic dimension of \"f\" is \"M\".\n\nThe intrinsic dimension is a characterization of \"f\", it is not an unambiguous characterization of \"g\" nor of A. That is, if the above relation is satisfied for some \"f\", \"g\", and A, it must also be satisfied for the same \"f\" and \"g′\" and A′ given by\n\nwhere B is a non-singular \"M × M\" matrix, since\n\nAn \"N\" variable function which has intrinsic dimension \"M < N\" has a characteristic Fourier transform. Intuitively, since this type of function is constant along one or several dimensions its Fourier transform must appear like an impulse (the Fourier transform of a constant) along the same dimension in the frequency space.\n\nLet \"f\" be a two-variable function which is i1D. This means that there exists a normalized vector n in R and a one-variable function \"g\" such that\n\nfor all x in R. If \"F\" is the Fourier transform of \"f\" (both are two-variable functions) it must be the case that\n\nHere \"G\" is the Fourier transform of \"g\" (both are one-variable functions), \"δ\" is the Dirac impulse function and m is a normalized vector in R perpendicular to n. This means that \"F\" vanishes everywhere except on a line which passes through the origin of the frequency domain and is parallel to m. Along this line \"F\" varies according to \"G\".\n\nLet \"f\" be an \"N\"-variable function which has intrinsic dimension \"M\", that is, there exists an \"M\"-variable function \"g\" and \"M × N\" matrix A such that\n\nIts Fourier transform \"F\" can then be described as follows:\n\n\nThe type of intrinsic dimension described above assumes that a linear transformation is applied to the coordinates of the \"N\"-variable function \"f\" to produce the \"M\" variables which are necessary to represent every value of \"f\". This means that \"f\" is constant along lines, planes, or hyperplanes, depending on \"N\" and \"M\".\n\nIn a general case, \"f\" has intrinsic dimension \"M\" is there exist \"M\" functions \"a\", \"a\", ..., \"a\" and an \"M\"-variable function \"g\" such that\n\n\nA simple example is transforming a 2-variable function \"f\" to polar coordinates:\n\n\nFor the general case, a simple description of either the point sets for which \"f\" is constant or its Fourier transform is usually not possible.\n\nThe case of a two-variable signal which is i1D appears frequently in computer vision and image processing and captures the idea of local image regions which contain lines or edges. The analysis of such regions has a long history, but it was not until a more formal and theoretical treatment of such operations began that the concept of intrinsic dimension was established, even though the name has varied.\n\nFor example, the concept which here is referred to as an \"image neighborhood of intrinsic dimension 1\" or \"i1D neighborhood\" is called \"1-dimensional\" by Knutsson (1982), \"linear symmetric\" by Bigün & Granlund (1987) and \"simple neighborhood\" in Granlund & Knutsson (1995).\n\nThe term intrinsic dimension was coined by Bennett (1965).\n\n\n", "id": "12989981", "title": "Intrinsic dimension"}
{"url": "https://en.wikipedia.org/wiki?curid=13274389", "text": "Articulated body pose estimation\n\nArticulated body pose estimation in computer vision is the study of algorithms and systems that recover the pose of an articulated body, which consists of joints and rigid parts using image-based observations. It is one of the longest-lasting problems in computer vision because of the complexity of the models that relate observation with pose, and because of the variety of situations in which it would be useful.\n\nPerception of human beings in their neighboring environment is an important capability that robots must possess. If a person uses gestures to point to a particular object, then the interacting machine should be able to understand the situation in real world context. Thus pose estimation is an important and challenging problem in computer vision, and many algorithms have been deployed in solving this problem over the last two decades. Many solutions involve training complex models with large data sets.\n\nPose estimation is a difficult problem and an active subject of research because the human body has 244 degrees of freedom with 230 joints. Although not all movements between joints are evident, the human body is composed of 10 large parts with 20 degrees of freedom. Algorithms must account for large variability introduced by differences in appearance due to clothing, body shape, size, and hairstyles. Additionally, the results may be ambiguous due to partial occlusions from self-articulation, such as a person's hand covering their face, or occlusions from external objects. Finally, most algorithms estimate pose from monocular (two-dimensional) images, taken from a normal camera. Other issues include varying lighting and camera configurations. The difficulties are compounded if there are additional performance requirements. These images lack the three-dimensional information of an actual body pose, leading to further ambiguities. There is recent work in this area wherein images from RGBD cameras provide information about color and depth.\n\nThere is a need to develop accurate, tether-less, vision-based articulated body pose estimation systems to recover the pose of bodies, such as the human body, a hand, or non-human creatures. Such a system has several foreseeable applications, including the following: \n\nThe typical articulated body pose estimation system involves a model-based approach, in which the pose estimation is achieved by maximizing/minimizing a similarity/dissimilarity between an observation (input) and a template model. Different kinds of sensors have been explored for use in making the observation, including the following:\n\nThese sensors produce intermediate representations that are directly used by the model. The representations include the following:\n\nThe basic idea of part based model can be attributed to the human skeleton. Any object having the property of articulation can be broken down into smaller parts wherein each part can take different orientations, resulting in different articulations of the same object. Different scales and orientations of the main object can be articulated to scales and orientations of the corresponding parts. To formulate the model so that it can be represented in mathematical terms, the parts are connected to each other using springs. As such, the model is also known as a spring model. The degree of closeness between each part is accounted for by the compression and expansion of the springs. There is geometric constraint on the orientation of\nsprings. For example, limbs of legs cannot move 360 degrees. Hence parts cannot have that extreme orientation. This reduces the possible permutations.\n\nThe spring model forms a graph G(V,E) where V (nodes) corresponds to the parts and E (edges) represents springs connecting two neighboring parts. Each location in the image can be reached by the formula_1 and formula_2 coordinates of the pixel location. Let formula_3 be point at formula_4 location. Then the cost associated in joining the spring between formula_4 and the formula_6 point can be given by formula_7. Hence the\ntotal cost associated in placing formula_8 components at locations formula_9 is given by\n\nThe above equation simply represents the spring model used to describe body pose. To estimate pose from images, cost or energy function must be minimized. This energy function consists of two terms. The first is related to how each component matches the image data and the second deals with how much the\noriented (deformed) parts match, thus accounting for articulation along with object detection.\n\nThe part models, also known as pictorial structures, are of one the basic models on which other efficient models are built by slight modification. One such example is the flexible mixture model which reduces the database of hundreds or thousands of deformed parts by exploiting the notion of local rigidity.\n\nThe kinematic skeleton is constructed by a tree-structured chain, as illustrated in the Figure. Each rigid body segment has its local coordinate system that can be transformed to the world coordinate system via a 4×4 transformation matrix formula_11,\n\nwhere formula_13 denotes the local transformation from body segment formula_14 to its parent formula_15. Each joint in the body has 3 degrees of freedom (DoF) rotation. Given a transformation matrix formula_16 , the joint position at the T-pose can be transferred to its corresponding position in the world coordination. In many works, the 3D joint rotation is expressed as a normalized quaternion formula_17 due to its continuity that can facilitate gradient-based optimization in the parameter estimation.\n\nPersonal care robots may be deployed in future assisted living homes. For these robots, high-accuracy human detection and pose estimation is necessary to perform a variety of tasks, such as fall detection. Additionally, this application has a number of performance constraints. \n\nTraditionally, character animation has been a manual process. However, poses can be synced directly to a real-life actor through specialized pose estimation systems. Older systems relied on markers or specialized suits. Recent advances in pose estimation and motion capture have enabled markerless applications, sometimes in real time.\n\nCar accidents account for about two percent of deaths globally each year. As such, an intelligent system tracking driver pose may be useful for emergency alerts . Along the same lines, pedestrian detection algorithms have been used successfully in autonomous cars, enabling the car to make smarter decisions. \n\nCommercially, pose estimation has been used in the context of video games, popularized with the Microsoft Kinect sensor (a depth camera). These systems track the user to render their avatar in-game, in addition to performing tasks like gesture recognition to enable the user to interact with the game. As such, this application has a strict real-time requirement.\n\nOther applications include physical therapy, the study of the cognitive brain development of young children, video surveillance, animal tracking and behavior understanding, sign language detection, advanced human–computer interaction, and markerless motion capturing.\n\nA commercially successful but specialized computer vision-based articulated body pose estimation technique is optical motion capture. This approach involves placing markers on the individual at strategic locations to capture the 6 degrees-of-freedom of each body part.\n\nA number of groups and companies are actively researching pose estimation, including groups at Brown University, Carnegie Mellon University, MPI Saarbruecken, Stanford University, the University of California, San Diego, the University of Toronto, the École Centrale Paris, ETH Zurich, National University of Sciences and Technology (NUST), the University of California, Irvine, and wrnch.\n\nAt present, several companies are working on articulated body post estimation.\n\n", "id": "13274389", "title": "Articulated body pose estimation"}
{"url": "https://en.wikipedia.org/wiki?curid=15281107", "text": "Shape context\n\nShape context is a feature descriptor used in object recognition. Serge Belongie and Jitendra Malik proposed the term in their paper \"Matching with Shape Contexts\" in 2000.\n\nThe shape context is intended to be a way of describing shapes that allows for measuring shape similarity and the recovering of point correspondences. The basic idea is to pick \"n\" points on the contours of a shape. For each point \"p\" on the shape, consider the \"n\" − 1 vectors obtained by connecting \"p\" to all other points. The set of all these vectors is a rich description of the shape localized at that point but is far too detailed. The key idea is that the distribution over relative positions is a robust, compact, and highly discriminative descriptor. So, for the point \"p\", the coarse histogram of the relative coordinates of the remaining \"n\" − 1 points,\n\nis defined to be the shape context of formula_2. The bins are normally taken to be uniform in log-polar space. The fact that the shape context is a rich and discriminative descriptor can be seen in the figure below, in which the shape contexts of two different versions of the letter \"A\" are shown.\n\n(a) and (b) are the sampled edge points of the two shapes. (c) is the diagram of the log-polar bins used to compute the shape context. (d) is the shape context for the point marked with a circle in (a), (e) is that for the point marked as a diamond in (b), and (f) is that for the triangle. As can be seen, since (d) and (e) are the shape contexts for two closely related points, they are quite similar, while the shape context in (f) is very different.\n\nNow in order for a feature descriptor to be useful, it needs to have certain invariances. In particular it needs to be invariant to translation, scale, small perturbations, and depending on application rotation. Translational invariance come naturally to shape context. Scale invariance is obtained by normalizing all radial distances by the mean distance formula_3 between all the point pairs in the shape although the median distance can also be used. Shape contexts are empirically demonstrated to be robust to deformations, noise, and outliers using synthetic point set matching experiments.\n\nOne can provide complete rotation invariance in shape contexts. One way is to measure angles at each point relative to the direction of the tangent at that point (since the points are chosen on edges). This results in a completely rotationally invariant descriptor. But of course this is not always desired since some local features lose their discriminative power if not measured relative to the same frame. Many applications in fact forbid rotation invariance e.g. distinguishing a \"6\" from a \"9\".\n\nA complete system that uses shape contexts for shape matching consists of the following steps (which will be covered in more detail in the Details of Implementation section):\n\n\nThe approach assumes that the shape of an object is essentially captured by a finite subset of the points on the internal or external contours on the object. These can be simply obtained using the Canny edge detector and picking a random set of points from the edges. Note that these points need not and in general do not correspond to key-points such as maxima of curvature or inflection points. It is preferable to sample the shape with roughly uniform spacing, though it is not critical.\n\nThis step is described in detail in the Theory section.\n\nConsider two points \"p\" and \"q\" that have normalized \"K\"-bin histograms (i.e. shape contexts) \"g\"(\"k\") and \"h\"(\"k\"). As shape contexts are distributions represented as histograms, it is natural to use the \"χ\" test statistic as the \"shape context cost\" of matching the two points:\n\nThe values of this range from 0 to 1.\nIn addition to the shape context cost, an extra cost based on the appearance can be added. For instance, it could be a measure of tangent angle dissimilarity (particularly useful in digit recognition):\n\nThis is half the length of the chord in unit circle between the unit vectors with angles formula_6 and formula_7. Its values also range from 0 to 1. Now the total cost of matching the two points could be a weighted-sum of the two costs:\n\nNow for each point \"p\" on the first shape and a point \"q\" on the second shape, calculate the cost as described and call it \"C\". This is the cost matrix.\n\nNow, a one-to-one matching \"p\" that matches each point \"p\" on shape 1 and \"q\" on shape 2 that minimizes the total cost of matching,\n\nis needed. This can be done in formula_10 time using the Hungarian method, although there are more efficient algorithms.\nTo have robust handling of outliers, one can add \"dummy\" nodes that have a constant but reasonably large cost of matching to the cost matrix. This would cause the matching algorithm to match outliers to a \"dummy\" if there is no real match.\n\nGiven the set of correspondences between a finite set of points on the two shapes, a transformation formula_11 can be estimated to map any point from one shape to the other. There are several choices for this transformation, described below.\n\nThe affine model is a standard choice: formula_12. The least squares solution for the matrix formula_13 and the translational offset vector \"o\" is obtained by:\n\nWhere formula_15 with a similar expression for formula_16. formula_17 is the pseudoinverse of formula_16.\n\nThe thin plate spline (TPS) model is the most widely used model for transformations when working with shape contexts. A 2D transformation can be separated into two TPS function to model a coordinate transform:\n\nwhere each of the \"ƒ\" and \"ƒ\" have the form:\n\nand the kernel function formula_21 is defined by formula_22. The exact details of how to solve for the parameters can be found elsewhere but it essentially involves solving a linear system of equations. The bending energy (a measure of how much transformation is needed to align the points) will also be easily obtained.\n\nThe TPS formulation above has exact matching requirement for the pairs of points on the two shapes. For noisy data, it is best to relax this exact requirement. If we let formula_23 denote the target function values at corresponding locations formula_24 (Note that for formula_25, formula_23 would formula_27 the x-coordinate of the point corresponding to formula_2 and for formula_29 it would be the y-coordinate, formula_30), relaxing the requirement amounts to minimizing\n\nwhere formula_32 is the bending energy and formula_33 is called the regularization parameter. This \"ƒ\" that minimizes \"H\"[\"ƒ\"] can be found in a fairly straightforward way. If one uses normalize coordinates for formula_34, then scale invariance is kept. However, if one uses the original non-normalized coordinates, then the regularization parameter needs to be normalized.\n\nNote that in many cases, regardless of the transformation used, the initial estimate of the correspondences contains some errors which could reduce the quality of the transformation. If we iterate the steps of finding correspondences and estimating transformations (i.e. repeating steps 2–5 with the newly transformed shape) we can overcome this problem. Typically, three iterations are all that is needed to obtain reasonable results.\n\nNow, a shape distance between two shapes formula_35 and formula_16. This distance is going to be a weighted sum of three potential terms:\n\nShape context distance: this is the symmetric sum of shape context matching costs over best matching points:\n\nwhere \"T\"(·) is the estimated TPS transform that maps the points in \"Q\" to those in \"P\".\n\nAppearance cost: After establishing image correspondences and properly warping one image to match the other, one can define an appearance cost as the sum of squared brightness differences in Gaussian windows around corresponding image points:\n\nwhere formula_39 and formula_40 are the gray-level images (formula_40 is the image after warping) and formula_42 is a Gaussian windowing function.\n\nTransformation cost: The final cost formula_43 measures how much transformation is necessary to bring the two images into alignment. In the case of TPS, it is assigned to be the bending energy.\n\nNow that we have a way of calculating the distance between two shapes, we can use a nearest neighbor classifier (k-NN) with distance defined as the shape distance calculated here. The results of applying this to different situations is given in the following section.\n\nThe authors Serge Belongie and Jitendra Malik tested their approach on the MNIST database. Currently, more than 50 algorithms have been tested on the database. The database has a training set of 60,000 examples, and a test set of 10,000 examples. The error rate for this approach was 0.63% using 20,000 training examples and 3-NN. At the time of publication, this error rate was the lowest. Currently, the lowest error rate is 0.23%.\n\nThe authors experimented with the MPEG-7 shape silhouette database, performing Core Experiment CE-Shape-1 part B, which measures performance of similarity-based retrieval. The database has 70 shape categories and 20 images per shape category. Performance of a retrieval scheme is tested by using each image as a query and counting the number of correct images in the top 40 matches. For this experiment, the authors increased the amount of points sampled from each shape. Also, since the shapes in the database sometimes were rotated or flipped, the authors took defined the distance between a reference shape and query shape to be minimum shape distance between the query shape and either the unchanged reference, the vertically flipped, or the reference horizontally flipped. With these changes, they obtained a retrieval rate of 76.45%, which by 2002 was the best.\nThe next experiment performed on shape contexts involved the 20 common household objects in the Columbia Object Image Library (COIL-20). Each object has 72 views in the database. In the experiment, the method was trained on a number of equally spaced views for each object and the remaining views were used for testing. A 1-NN classifier was used. The authors also developed an \"editing\" algorithm based on shape context similarity and k-medoid clustering that improved on their performance.\n\nShape contexts were used to retrieve the closest matching trademarks from a database to a query trademark (useful in detecting trademark infringement). No visually similar trademark was missed by the algorithm (verified manually by the authors).\n\n", "id": "15281107", "title": "Shape context"}
{"url": "https://en.wikipedia.org/wiki?curid=15375961", "text": "Active vision\n\nAn area of computer vision is active vision, sometimes also called \"active computer vision\". An active vision system is one that can manipulate the viewpoint of the camera(s) in order to investigate the environment and get better information from it.\n\nThe interest in active camera system started as early as two decades ago. Beginning in the late 1980s, Aloimonos et al. introduced the first general framework for active vision in order to improve the perceptual quality of tracking results. Active vision is particularly important to cope with problems like occlusions, limited field of view and limited resolution of the camera. Other advantages can be reducing the motion blur of a moving object and enhancing depth perception of an object by focusing two cameras on the same object or moving the cameras. \nActive control of the camera view point also helps in focusing computational resources on the relevant element of the scene. In this selective aspect, active vision can be seen as strictly related to (overt & covert) visual attention in biological organisms, which has been shown to enhance the perception of selected part of the visual field. This selective aspect of human (active) vision can be easily related to the foveal structure of the human eye, where in about 5% of the retina more than the 50% of the colour receptors are located.\n\nIt has also been suggested that visual attention and the selective aspect of active camera control can help in other tasks like learning more robust models of objects and environments with less labeled samples or autonomously\n\nAutonomous cameras are cameras that can direct themselves in their environment. There has been some recent work using this approach. In work from Denzler et al., the motion of a tracked object is modeled using a Kalman filter while the focal length that minimizes the uncertainty in the state estimations is the one that is used. A stereo set-up with two zoom cameras was used. A handful of papers have been written for zoom control and do not deal with total object-camera position estimation. An attempt to join estimation and control in the same framework can be found in the work of Bagdanov et al., where a Pan-Tilt-Zoom camera is used to track faces. Both the estimation and control models used are ad hoc, and the estimation approach is based on image features rather than 3D properties of the target being tracked.\n\nIn a master/slave configuration, a supervising static camera is used to monitor a wide field of view and to track every moving target of interest. The position of each of these targets over time is then provided to a foveal camera, which tries to observe the targets at a higher resolution. Both the static and the active cameras are calibrated to a common reference, so that data coming from one of them can be easily projected onto the other, in order to coordinate the control of the active sensors. Another possible use of the master/slave approach consists of a static (master) camera extracting visual features of an object of interest, while the active (slave) sensor uses these features to detect the desired object without the need of any training data.\n\nIn recent years there has been growing interest in building networks of active cameras and optional static cameras so that you can cover a large area while maintaining high resolution of multiple targets. This is ultimately a scaled-up version of either the master/slave approach or the autonomous camera approach. This approach can be highly effective, but also incredibly costly. Not only are multiple cameras involved but you also must have them communicate with each other which can be computationally expensive.\n\nControlled active vision can be defined as a controlled motion of a vision sensor can maximize the performance of any robotic algorithm that involves a moving vision sensor. It is a hybrid of control theory and conventional vision. An application of this framework is real-time robotic servoing around static or moving arbitrary 3-D objects. See Visual Servoing. Algorithms that incorporate the use of multiple windows and numerically stable confidence measures are combined with stochastic controllers in order to provide a satisfactory solution to the tracking problem introduced by combining computer vision and control. In the case where there is an inaccurate model of the environment, adaptive control techniques may be introduced. The above information and further mathematical representations of controlled active vision can be seen in the thesis of Nikolaos Papanikolopoulos.\n\nExamples of active vision systems usually involve a robot mounted camera, but other systems have employed human operator-mounted cameras (a.k.a. \"wearables\"). Applications include automatic surveillance, human robot interaction (video), SLAM, route planning, etc. In the DARPA Grand Challenge most of the teams used LIDAR combined with active vision systems\nto guide driverless vehicles across an off-road course.\n\nA good example of active vision can be seen in this YouTube video. It shows face tracking using active vision with a pan-tilt camera system. https://www.youtube.com/watch?v=N0FjDOTnmm0\n\nActive Vision is also important to understand how humans.\nand organism endowed with visual sensors, actually see the world considering the limits of their sensors, the richness and continuous variability of the visual signal and the effects of their actions and goals on their perception.\n\nThe controllable active vision framework can be used in a number of different ways. Some examples might be vehicle tracking, robotics applications, and interactive MRI segmentation.\n\nInteractive MRI segmentation uses controllable active vision by using a Lyapanov control design to establish a balance between the influence of a data-driven gradient flow and the human’s input over time. This smoothly couples automatic segmentation with interactivity. More information on this method can be found in. Segmentation in MRIs is a difficult subject, and it takes an expert to trace out the desired segments due to the MRI picking up all fluid and tissue. This could prove impractical because it would be a very lengthy process. Controllable active vision methods described in the cited paper could help improve the process while relying on the human less.\n\nVarious downloads of different implementations of active vision can be found from this link to the active vision lab at Oxford University. http://www.robots.ox.ac.uk/ActiveVision/Downloads/index.html\n\n", "id": "15375961", "title": "Active vision"}
{"url": "https://en.wikipedia.org/wiki?curid=15966023", "text": "Pyramid (image processing)\n\nPyramid, or pyramid representation, is a type of multi-scale signal representation developed by the computer vision, image processing and signal processing communities, in which a signal or an image is subject to repeated smoothing and subsampling. Pyramid representation is a predecessor to scale-space representation and multiresolution analysis.\n\nThere are two main types of pyramids: lowpass and bandpass.\n\nA lowpass pyramid is made by smoothing the image with an appropriate smoothing filter and then subsampling the smoothed image, usually by a factor of 2 along each coordinate direction. The resulting image is then subjected to the same procedure, and the cycle is repeated multiple times. Each cycle of this process results in a smaller image with increased smoothing, but with decreased spatial sampling density (that is, decreased image resolution). If illustrated graphically, the entire multi-scale representation will look like a pyramid, with the original image on the bottom and each cycle's resulting smaller image stacked one atop the other.\n\nA bandpass pyramid is made by forming the difference between images at adjacent levels in the pyramid and performing some kind of image interpolation between adjacent levels of resolution, to enable computation of pixelwise differences.\n\nA variety of different smoothing kernels have been proposed for generating pyramids. Among the suggestions that have been given, the \"binomial kernels\" arising from the binomial coefficients stand out as a particularly useful and theoretically well-founded class. Thus, given a two-dimensional image, we may apply the (normalized) binomial filter (1/4, 1/2, 1/4) typically twice or more along each spatial dimension and then subsample the image by a factor of two. This operation may then proceed as many times as desired, leading to a compact and efficient multi-scale representation. If motivated by specific requirements, intermediate scale levels may also be generated where the subsampling stage is sometimes left out, leading to an \"oversampled\" or \"hybrid pyramid\". With the increasing computational efficiency of CPUs available today, it is in some situations also feasible to use wider support Gaussian filters as smoothing kernels in the pyramid generation steps.\n\nIn a Gaussian pyramid, subsequent images are weighted down using a Gaussian average (Gaussian blur) and scaled down. Each pixel containing a local average that corresponds to a pixel neighborhood on a lower level of the pyramid. This technique is used especially in texture synthesis.\n\nA Laplacian pyramid is very similar to a Gaussian pyramid but saves the difference image of the blurred versions between each levels. Only the smallest level is not a difference image to enable reconstruction of the high resolution image using the difference images on higher levels. This technique can be used in image compression.\n\nA steerable pyramid, developed by Simoncelli and others, is an implementation of a multi-scale, multi-orientation band-pass filter bank used for applications including image compression, texture synthesis, and object recognition. It can be thought of as an orientation selective version of a Laplacian pyramid, in which a bank of steerable filters are used at each level of the pyramid instead of a single Laplacian of Gaussian filter.\n\nIn the early days of computer vision, pyramids were used as the main type of multi-scale representation for computing multi-scale image features from real-world image data. More recent techniques include scale-space representation, which has been popular among some researchers due to its theoretical foundation, the ability to decouple the subsampling stage from the multi-scale representation, the more powerful tools for theoretical analysis as well as the ability to compute a representation at \"any\" desired scale, thus avoiding the algorithmic problems of relating image representations at different resolution. Nevertheless, pyramids are still frequently used for expressing computationally efficient approximations to scale-space representation.\n\nLaplacian image pyramids based on the bilateral filter provide a good framework for image detail enhancement and manipulation. The difference images between each layer are modified to exaggerate or reduce details at different scales in an image.\n\nSome image compression file formats use the Adam7 algorithm or some other interlacing technique.\nThese can be seen as a kind of image pyramid.\nBecause those file format store the \"large-scale\" features first, and fine-grain details later in the file,\na particular viewer displaying a small \"thumbnail\" or on a small screen can quickly download just enough of the image to display it in the available pixels—so one file can support many viewer resolutions, rather than having to store or generate a different file for each resolution.\n\n\n", "id": "15966023", "title": "Pyramid (image processing)"}
{"url": "https://en.wikipedia.org/wiki?curid=19132709", "text": "Photometric stereo\n\nPhotometric stereo is a technique in computer vision for estimating the surface normals of objects by observing that object under different lighting conditions. It is based on the fact that the amount of light reflected by a surface is dependent on the orientation of the surface in relation to the light source and the observer. By measuring the amount of light reflected into a camera, the space of possible surface orientations is limited. Given enough light sources from different angles, the surface orientation may be constrained to a single orientation or even overconstrained.\n\nThe technique was originally introduced by Woodham in 1980. The special case where the data is a single image is known as shape from shading, and was analyzed by B. K. P. Horn in 1989. Photometric stereo has since been generalized to many other situations, including extended light sources and non-Lambertian surface finishes. Current research aims to make the method work in the presence of projected shadows, highlights, and non-uniform lighting.\n\nUnder Woodham's original assumptions — Lambertian reflectance, known point-like distant light sources, and uniform albedo — the problem can be solved by inverting the linear equation formula_1, where formula_2 is a (known) vector of formula_3 observed intensities, formula_4 is the (unknown) surface normal, and formula_5 is a (known) formula_6 matrix of normalized light directions.\n\nThis model can easily be extended to surfaces with non-uniform albedo, while keeping the problem linear. Taking an albedo reflectivity of formula_7, the formula for the reflected light intensity becomes:\n\nIf formula_5 is square (there are exactly 3 lights) and non-singular, it can be inverted, giving:\n\nSince the normal vector is known to have length 1, formula_7 must be the length of the vector formula_12, and formula_4 is the normalised direction of that vector.\nIf formula_5 is not square (there are more than 3 lights), a generalisation of the inverse can be obtained using the Moore-Penrose pseudoinverse, by simply multiplying both sides with formula_15 giving:\n\nAfter which the normal vector and albedo can be solved as described above.\n\nThe classical photometric stereo problem concerns itself only with Lambertian surfaces, with perfectly diffuse reflection. This is unrealistic for many types of materials, especially metals, glass and smooth plastics, and will lead to aberrations in the resulting normal vectors.\n\nMany methods have been developed to lift this assumption. In this section, a few of these are listed.\n\nHistorically, in computer graphics, the commonly used model to render surfaces started with Lambertian surfaces and progressed first to include simple specular reflections. Computer vision followed a similar course with photometric stereo. Specular reflections were among the first deviations from the Lambertian model. These are a few adaptations that have been developed.\n\n\nAccording to the Bidirectional reflectance distribution function (BRDF) model, a surface may distribute the amount of light it receives in any outward direction. This is the most general known model for opaque surfaces. Some techniques have been developed to model (almost) general BRDFs. In practice, all of these require many light sources to obtain reliable data. These are methods in which surfaces with general BRDFs can be measured.\n\n\nSome progress has been made towards modelling an even more general surfaces, such as Spatially Varying Bidirectional Distribution Functions (SVBRDF), Bidirectional surface scattering reflectance distribution functions (BSSRDF), and accounting for interreflections. However, such methods are still fairly restrictive in photometric stereo. Better results have been achieved with structured light.\n\n", "id": "19132709", "title": "Photometric stereo"}
{"url": "https://en.wikipedia.org/wiki?curid=11591518", "text": "Visual descriptor\n\nIn computer vision, visual descriptors or image descriptors are descriptions of the visual features of the contents in images, videos, or algorithms or applications that produce such descriptions. They describe elementary characteristics such as the shape, the color, the texture or the motion, among others.\n\nAs a result of the new communication technologies and the massive use of Internet in our society, the amount of audio-visual information available in digital format is increasing considerably. Therefore, it has been necessary to design some systems that allow us to describe the content of several types of multimedia information in order to search and classify them.\n\nThe audio-visual descriptors are in charge of the contents description. These descriptors have a good knowledge of the objects and events found in a video, image or audio and they allow the quick and efficient searches of the audio-visual content.\n\nThis system can be compared to the search engines for textual contents. Although it is certain, that it is relatively easy to find text with a computer, is much more difficult to find concrete audio and video parts. For instance, imagine somebody searching a scene of a happy person. The happiness is a feeling and it is not evident its shape, color and texture description in images.\n\nThe description of the audio-visual content is not a superficial task and it is essential for the effective use of this type of archives. The standardization system that deals with audio-visual descriptors is the MPEG-7 (\"Motion Picture Expert Group - 7\").\n\nDescriptors are the first step to find out the connection between pixels contained in a digital image and what humans recall after having observed an image or a group of images after some minutes.\n\nVisual descriptors are divided in two main groups: \n\nGeneral information descriptors consist of a set of descriptors that covers different basic and elementary features like: color, texture, shape, motion, location and others. This description is automatically generated by means of signal processing.\n\n\nThese descriptors, which give information about objects and events in the scene, are not easily extractable, even more when the extraction is to be automatically done. Nevertheless, they can be manually processed.\n\nAs mentioned before, face recognition is a concrete example of an application that tries to automatically obtain this information.\n\nAmong all applications, the most important ones are:\n\n\nB.S. Manjunath (Editor), Philippe Salembier (Editor), and Thomas Sikora (Editor): \"Introduction to MPEG-7: Multimedia Content Description Interface\". Wiley & Sons, April 2002 - \n\n", "id": "11591518", "title": "Visual descriptor"}
{"url": "https://en.wikipedia.org/wiki?curid=21610688", "text": "Joint compatibility branch and bound\n\nJoint compatibility branch and bound (JCBB) is an algorithm in computer vision and robotics commonly used for data association in simultaneous localization and mapping. JCBB measures the joint compatibility of a set of pairings that successfully rejects spurious matchings and is hence known to robust in complex environments.\n", "id": "21610688", "title": "Joint compatibility branch and bound"}
{"url": "https://en.wikipedia.org/wiki?curid=22714727", "text": "Photo-consistency\n\nIn computer vision Photo-consistency determines whether a given voxel is occupied. A voxel is considered to be photo consistent when its color appears to be similar to all the cameras that can see it. Most voxel coloring or space carving techniques require using photo consistency as a check condition in Image-based modeling and rendering applications.\n\n", "id": "22714727", "title": "Photo-consistency"}
{"url": "https://en.wikipedia.org/wiki?curid=10416897", "text": "Image fusion\n\nIn computer vision, Multisensor Image fusion is the process of combining relevant information from two or more images into a single image. The resulting image will be more informative than any of the input images.\n\nIn remote sensing applications, the increasing availability of space borne sensors gives a motivation for different image fusion algorithms.\nSeveral situations in image processing require high spatial and high spectral resolution in a single image. Most of the available equipment is not capable of providing such data convincingly. Image fusion techniques allow the integration of different information sources. The fused image can have complementary spatial and spectral resolution characteristics. However, the standard image fusion techniques can distort the spectral information of the multispectral data while merging.\n\nIn satellite imaging, two types of images are available. The panchromatic image acquired by satellites is transmitted with the maximum resolution available and the multispectral data are transmitted with coarser resolution. This will usually be two or four times lower. At the receiver station, the panchromatic image is merged with the multispectral data to convey more information.\n\nMany methods exist to perform image fusion. The very basic one is the high pass filtering technique. Later techniques are based on Discrete Wavelet Transform, uniform rational filter bank, and Laplacian pyramid.\n\nMultisensor data fusion has become a discipline which demands more general formal solutions to a number of application cases. Several situations in image processing require both high spatial and high spectral information in a single image. This is important in remote sensing. However, the instruments are not capable of providing such information either by design or because of observational constraints. One possible solution for this is data fusion.\n\nImage fusion methods can be broadly classified into two groups - spatial domain fusion and transform domain fusion.\n\nThe fusion methods such as averaging, Brovey method, principal component analysis (PCA) and IHS based methods fall under spatial domain approaches. Another important spatial domain fusion method is the high pass filtering based technique. Here the high frequency details are injected into upsampled version of MS images. The disadvantage of spatial domain approaches is that they produce spatial distortion in the fused image. Spectral distortion becomes a negative factor while we go for further processing, such as classification problem. Spatial distortion can be very well handled by frequency domain approaches on image fusion. The multiresolution analysis has become a very useful tool for analysing remote sensing images. The discrete wavelet transform has become a very useful tool for fusion. Some other fusion methods are also there, such as Laplacian pyramid based, curvelet transform based etc. These methods show a better performance in spatial and spectral quality of the fused image compared to other spatial methods of fusion.\n\nThe images used in image fusion should already be registered. Misregistration is a major source of error in image fusion. Some well-known image fusion methods are:\n\nImage fusion in remote sensing has several application domains. An important domain is the multi-resolution image fusion (commonly referred to pan-sharpening). In satellite imagery we can have two types of images\n\nThe SPOT PAN satellite provides high resolution (10m pixel) panchromatic data. While the LANDSAT TM satellite provides low resolution (30m pixel) multispectral images. Image fusion attempts to merge these images and produce a single high resolution multispectral image.\n\nThe standard merging methods of image fusion are based on Red-Green-Blue (RGB) to Intensity-Hue-Saturation (IHS) transformation. The usual steps involved in satellite image fusion are as follows:\n\nAn explanation of how to do Pan-sharpening in Photoshop. \nFor other applications of image in remote sensing, interested readers can refer to: Beyond Pan-sharpening: Pixel-level Fusion in Remote Sensing Applications.\n\nImage fusion has become a common term used within medical diagnostics and treatment. The term is used when multiple images of a patient are registered and overlaid or merged to provide additional information. Fused images may be created from multiple images from the same imaging modality, or by combining information from multiple modalities, such as magnetic resonance image (MRI), computed tomography (CT), positron emission tomography (PET), and single photon emission computed tomography (SPECT). In radiology and radiation oncology, these images serve different purposes. For example, CT images are used more often to ascertain differences in tissue density while MRI images are typically used to diagnose brain tumors.\n\nFor accurate diagnoses, radiologists must integrate information from multiple image formats. Fused, anatomically consistent images are especially beneficial in diagnosing and treating cancer. With the advent of these new technologies, radiation oncologists can take full advantage of intensity modulated radiation therapy (IMRT). Being able to overlay diagnostic images into radiation planning images results in more accurate IMRT target tumor volumes.\n\nComparative analysis of image fusion methods demonstrates that different metrics support different user needs, sensitive to different image fusion methods, and need to be tailored to the application. Categories of image fusion metrics are based on information theory, features, structural similarity, or human perception.\n\n\n", "id": "10416897", "title": "Image fusion"}
{"url": "https://en.wikipedia.org/wiki?curid=16928506", "text": "Visual servoing\n\nVisual servoing, also known as vision-based robot control and abbreviated VS, is a technique which uses feedback information extracted from a vision sensor (visual feedback) to control the motion of a robot. One of the earliest papers that talks about visual servoing was from the SRI International Labs in 1979.\n\nThere are two fundamental configurations of the robot end-effector (hand) and the camera:\n\n\nVisual Servoing control techniques are broadly classified into the following types:\n\nIBVS was proposed by Weiss and Sanderson. The control law is based on the error between current and desired features on the image plane, and does not involve any estimate of the pose of the target. The features may be the coordinates of visual features, lines or moments of regions. IBVS has difficulties with motions very large rotations, which has come to be called camera retreat.\n\nPBVS is a model-based technique (with a single camera). This is because the pose of the object of interest is estimated with respect to the camera and then a command is issued to the robot controller, which in turn controls the robot. In this case the image features are extracted as well, but are additionally used to estimate 3D information (pose of the object in Cartesian space), hence it is servoing in 3D.\n\nHybrid approaches use some combination of the 2D and 3D servoing. There have been a few different approaches to hybrid servoing\n\nThe following description of the prior work is divided into 3 parts\n\n\nVisual servo systems, also called servoing, have been around since the early 1980s \n, although the term visual servo itself was only coined in 1987.\nVisual Servoing is, in essence, a method for robot control where the sensor used is a camera (visual sensor). \nServoing consists primarily of two techniques,\none involves using information from the image to directly control the degrees of freedom (DOF) of the robot, thus referred to as Image Based Visual Servoing (IBVS).\nWhile the other involves the geometric interpretation of the information extracted from the camera, such as estimating the pose of the target and parameters of the camera (assuming some basic model of the target is known). Other servoing classifications exist based on the variations in each component of a servoing system \ne.g. the location of the camera, the two kinds are eye-in-hand and hand–eye configurations. \nBased on the control loop, the two kinds are end-point-open-loop and end-point-closed-loop. Based on whether the control is applied to the joints (or DOF)\ndirectly or as a position command to a robot controller the two types are\ndirect servoing and dynamic look-and-move.\nBeing one of the earliest works \nthe authors proposed a hierarchical\nvisual servo scheme applied to image-based servoing. The technique relies on\nthe assumption that a good set of features can be extracted from the object\nof interest (e.g. edges, corners and centroids) and used as a partial model\nalong with global models of the scene and robot. The control strategy is\napplied to a simulation of a two and three DOF robot arm.\n\nFeddema et al.\nintroduced the idea of generating task trajectory\nwith respect to the feature velocity. This is to ensure that the sensors are\nnot rendered ineffective (stopping the feedback) for any the robot motions.\nThe authors assume that the objects are known a priori (e.g. CAD model)\nand all the features can be extracted from the object.\nThe work by Espiau et al.\ndiscusses some of the basic questions in\nvisual servoing. The discussions concentrate on modeling of the interaction\nmatrix, camera, visual features (points, lines, etc..).\nIn \nan adaptive servoing system was proposed with a look-and-move\nservoing architecture. The method used optical flow along with SSD to\nprovide a confidence metric and a stochastic controller with Kalman filtering\nfor the control scheme. The system assumes (in the examples) that the plane\nof the camera and the plane of the features are parallel., discusses an approach of velocity control using the Jacobian relationship s˙ = Jv˙ . In addition the author uses Kalman filtering, assuming that\nthe extracted position of the target have inherent errors (sensor errors). A\nmodel of the target velocity is developed and used as a feed-forward input\nin the control loop. Also, mentions the importance of looking into kinematic\ndiscrepancy, dynamic effects, repeatability, settling time oscillations and lag\nin response.\n\nCorke poses a set of very critical questions on visual servoing and tries\nto elaborate on their implications. The paper primarily focuses the dynamics\nof visual servoing. The author tries to address problems like lag and stability,\nwhile also talking about feed-forward paths in the control loop. The paper\nalso, tries to seek justification for trajectory generation, methodology of axis\ncontrol and development of performance metrics.\n\nChaumette in provides good insight into the two major problems with\nIBVS. One, servoing to a local minima and second, reaching a Jacobian singularity. The author show that image points alone do not make good features\ndue to the occurrence of singularities. The paper continues, by discussing the\npossible additional checks to prevent singularities namely, condition numbers\nof J_s and Jˆ+_s, to check the null space of ˆ J_s and J^T_s . One main point that\nthe author highlights is the relation between local minima and unrealizable\nimage feature motions.\n\nOver the years many hybrid techniques have been developed. These\ninvolve computing partial/complete pose from Epipolar Geometry using multiple views or multiple cameras. The values are obtained by direct estimation or through a learning or a statistical scheme. While others have used\na switching approach that changes between image-based and position-based\nbased on a Lyapnov function.\nThe early hybrid techniques that used a combination of image-based and\npose-based (2D and 3D information) approaches for servoing required either\na full or partial model of the object in order to extract the pose information\nand used a variety of techniques to extract the motion information from the\nimage. used an affine motion model from the image motion in addition\nto a rough polyhedral CAD model to extract the object pose with respect to\nthe camera to be able to servo onto the object (on the lines of PBVS).\n\n2-1/2-D visual servoing developed by Malis et al. is a well known technique that breaks down the information required for servoing into an organized fashion which decouples rotations and translations. The papers\nassume that the desired pose is known a priori. The rotational information is\nobtained from partial pose estimation, a homography, (essentially 3D information) giving an axis of rotation and the angle (by computing the eigenvalues and eigenvectors of the homography). The translational information is\nobtained from the image directly by tracking a set of feature points. The only\nconditions being that the feature points being tracked never leave the field of\nview and that a depth estimate be predetermined by some off-line technique.\n2-1/2-D servoing has been shown to be more stable than the techniques that\npreceded it. Another interesting observation with this formulation is that\nthe authors claim that the visual Jacobian will have no singularities during\nthe motions.\nThe hybrid technique developed by Corke and Hutchinson, popularly called portioned approach partitions the visual (or image) Jacobian into\nmotions (both rotations and translations) relating X and Y axes and motions related to the Z axis. outlines the technique, to break out columns\nof the visual Jacobian that correspond to the Z axis translation and rotation\n(namely, the third and sixth columns). The partitioned approach is shown to\nhandle the Chaumette Conundrum discussed in. This technique requires\na good depth estimate in order to function properly.\nnamely main and secondary. The main task is keep the features of inter-\nest within the field of view. While the secondary task is to mark a fixation\npoint and use it as a reference to bring the camera to the desired pose. The\ntechnique does need a depth estimate from an off-line procedure. The paper\ndiscusses two examples for which depth estimates are obtained from robot\nodometry and by assuming that all features are on a plane. The secondary\ntask is achieved by using the notion of parallax. The features that are tracked\nare chosen by an initialization performed on the first frame, which are typi-\ncally points.\nmodeling and model-based tracking. Primary assumption made is that the\n3D model of the object is available. The authors highlights the notion that\nideal features should be chosen such that the DOF of motion can be decoupled\nby linear relation. The authors also introduce an estimate of the target\nvelocity into the interaction matrix to improve tracking performance. The\nresults are compared to well known servoing techniques even when occlusions\noccur.\n\nThis section discusses the work done in the field of visual servoing. We try\nto track the various techniques in the use of features. Most of the work\nhas used image points as visual features. The formulation of the interaction\nmatrix in assumes points in the image are used to represent the target.\nThere has some body of work that deviates from the use of points and use\nfeature regions, lines, image moments and moment invariants.\nIn, the authors discuss an affine based tracking of image features.\nThe image features are chosen based on a discrepancy measure, which is\nbased on the deformation that the features undergo. The features used were\ntexture patches. One of key points of the paper was that it highlighted the\nneed to look at features for improving visual servoing.\nIn the authors look into choice of image features (the same question\nwas also discussed in in the context of tracking). The effect of the choice\nof image features on the control law is discussed with respect to just the\ndepth axis. Authors consider the distance between feature points and the\narea of an object as features. These features are used in the control law with\nslightly different forms to highlight the effects on performance. It was noted\nthat better performance was achieved when the servo error was proportional\nto the change in depth axis.\nauthors provide a new formulation of the interaction matrix using the velocity\nof the moments in the image, albeit complicated. Even though the moments\nare used, the moments are of the small change in the location of contour\npoints with the use of Green’s theorem. The paper also tries to determine\nthe set of features (on a plane) to for a 6 DOF robot.\nIn discusses the use of image moments to formulate the visual Jacobian.\nThis formulation allows for decoupling of the DOF based on type of moments\nchosen. The simple case of this formulation is notionally similar to the 2-1/2-\nD servoing. The time variation of the moments (m˙ij) are determined using\nthe motion between two images and Greens Theorem. The relation between\nm˙ij and the velocity screw (v) is given as m˙_ij = L_m_ij v. This technique\navoids camera calibration by assuming that the objects are planar and using\na depth estimate. The technique works well in the planar case but tends to\nbe complicated in the general case. The basic idea is based on the work in [4]\nMoment Invariants have been used in. The key idea being to find\nthe feature vector that decouples all the DOF of motion. Some observations\nmade were that centralized moments are invariant for 2D translations. A\ncomplicated polynomial form is developed for 2D rotations. The technique\nfollows teaching-by-showing, hence requiring the values of desired depth and\narea of object (assuming that the plane of camera and object are parallel,\nand the object is planar). Other parts of the feature vector are invariants\nR3,R4. The authors claim that occlusions can be handled.\nence being that the authors use a technique similar to, where the task is\nbroken into two (in the case where the features are not parallel to the cam-\nera plane). A virtual rotation is performed to bring the featured parallel to\nthe camera plane. consolidates the work done by the authors on image\nmoments.\n\nEspiau in showed from purely experimental work that image based visual servoing (IBVS)\nis robust to calibration errors. The author used a camera with no explicit\ncalibration along with point matching and without pose estimation. The\npaper looks at the effect of errors and uncertainty on the terms in the interaction matrix from an experimental approach. The targets used were points\nand were assumed to be planar.\n\nA similar study was done in where the\nauthors carry out experimental evaluation of a few uncalibrated visual servo\nsystems that were popular in the 90’s. The major outcome was the experimental evidence of the effectiveness of visual servo control over conventional\ncontrol methods.\nKyrki et al. analyze servoing errors for position based and 2-1/2-D\nvisual servoing. The technique involves determining the error in extracting\nimage position and propagating it to pose estimation and servoing control.\nPoints from the image are mapped to points in the world a priori to obtain a mapping (which is basically the homography, although not explicitly stated\nin the paper). This mapping is broken down to pure rotations and translations. Pose estimation is performed using standard technique from Computer\nVision. Pixel errors are transformed to the pose. These are propagating to\nthe controller. An observation from the analysis shows that errors in the\nimage plane are proportional to the depth and error in the depth-axis is\nproportional to square of depth.\nMeasurement errors in visual servoing have been looked into extensively.\nMost error functions relate to two aspects of visual servoing. One being\nsteady state error (once servoed) and two on the stability of the control\nloop. Other servoing errors that have been of interest are those that arise\nfrom pose estimation and camera calibration. In, the authors extend the\nwork done in by considering global stability in the presence of intrinsic\nand extrinsic calibration errors. provides an approach to bound the task\nfunction tracking error. In, the authors use teaching-by-showing visual\nservoing technique. Where the desired pose is known a priori and the robot\nis moved from a given pose. The main aim of the paper is to determine the\nupper bound on the positioning error due to image noise using a convex-\noptimization technique.\nin depth estimates. The authors conclude the paper with the observation that\nfor unknown target geometry a more accurate depth estimate is required in\norder to limit the error.\nMany of the visual servoing techniques implicitly assume that\nonly one object is present in the image and the relevant feature for tracking\nalong with the area of the object are available. Most techniques require either\na partial pose estimate or a precise depth estimate of the current and desired\npose.\n\n\n\n", "id": "16928506", "title": "Visual servoing"}
{"url": "https://en.wikipedia.org/wiki?curid=16741381", "text": "Boustrophedon cell decomposition\n\nThe boustrophedon cell decomposition (BCD) is a method used in artificial intelligence and robotics for configuration space solutions. Like other cellular decomposition methods, this method transforms the configuration space into cell regions that can be used for path planning.\n\nA strength of the boustrophedon cell decomposition is that it allows for more diverse, non-polygonal obstacles within a configuration space. The representation still depicts polygonal obstacles, but the representations are complex enough that they are very effective when describing things like rounded surfaces, jagged edges, etc.\n\nIt is a goal of the method to optimize a path that can be chosen by an intelligent system. While a BCD can represent the existence of objects in a physical space, it does very little to nothing in terms of recognizing the objects. This would be done using another method, one which most likely requires additional sensory data in order to be used.\n", "id": "16741381", "title": "Boustrophedon cell decomposition"}
{"url": "https://en.wikipedia.org/wiki?curid=1290557", "text": "Phase correlation\n\nPhase correlation is an approach to estimate the relative translative offset between two similar images (digital image correlation) or other data sets. It is commonly used in image registration and relies on a frequency-domain representation of the data, usually calculated by fast Fourier transforms. The term is applied particularly to a subset of cross-correlation techniques that isolate the phase information from the Fourier-space representation of the cross-correlogram.\n\nThe following image demonstrates the usage of phase correlation to determine relative translative movement between two images corrupted by independent Gaussian noise. The image was translated by (30,33) pixels. Accordingly, one can clearly see a peak in the phase-correlation representation at approximately (30,33).\n\nGiven two input images formula_1 and formula_2:\n\nApply a window function (e.g., a Hamming window) on both images to reduce edge effects (this may be optional depending on the image characteristics). Then, calculate the discrete 2D Fourier transform of both images.\n\nCalculate the cross-power spectrum by taking the complex conjugate of the second result, multiplying the Fourier transforms together elementwise, and normalizing this product elementwise.\n\nWhere formula_5 is the Hadamard product (entry-wise product) and the absolute values are taken entry-wise as well. Written out entry-wise for element index formula_6:\n\nObtain the normalized cross-correlation by applying the inverse Fourier transform.\n\nDetermine the location of the peak in formula_9. \n\nCommonly, interpolation methods are used to estimate the peak location in the cross-correlogram to non-integer values, despite the fact that the data are discrete, and this procedure is often termed 'subpixel registration'. A large variety of subpixel interpolation methods are given in the technical literature. Common peak interpolation methods such as parabolic interpolation have been used, and the OpenCV computer vision package uses a centroid-based method, though these generally have inferior accuracy compared to more sophisticated methods. \n\nBecause the Fourier representation of the data has already been computed, it is especially convenient to use the Fourier shift theorem with real-valued (sub-integer) shifts for this purpose, which essentially interpolates using the sinusoidal basis functions of the Fourier transform. An especially popular FT-based estimator is given by Foroosh \"et al.\" In this method, the subpixel peak location is approximated by a simple formula involving peak pixel value and the values of its nearest neighbors, where formula_11 is the peak value and formula_12 is the nearest neighbor in the x direction (assuming, as in most approaches, that the integer shift has already been found and the comparand images differ only by a subpixel shift). \n\nThe Foroosh \"et al.\" method is quite fast compared to most methods, though it is not always the most accurate. Some methods shift the peak in Fourier space and apply non-linear optimization to maximize the correlogram peak, but these tend to be very slow since they must apply an inverse Fourier transform or its equivalent in the objective function.\n\nIt is also possible to infer the peak location from phase characteristics in Fourier space without the inverse transformation, as noted by Stone. These methods usually use a linear least squares (LLS) fit of the phase angles to a planar model. The long latency of the phase angle computation in these methods is a disadvantage, but the speed can sometimes be comparable to the Foroosh \"et al.\" method depending on the image size. They often compare favorably in speed to the multiple iterations of extremely slow objective functions in iterative non-linear methods.\n\nSince all subpixel shift computation methods are fundamentally interpolative, the performance of a particular method depends on how well the underlying data conform to the assumptions in the interpolator. This fact also may limit the usefulness of high numerical accuracy in an algorithm, since the uncertainty due to interpolation method choice may be larger than any numerical or approximation error in the particular method.\n\nSubpixel methods are also particularly sensitive to noise in the images, and the utility of a particular algorithm is distinguished not only by its speed and accuracy but its resilience to the particular types of noise in the application.\n\nThe method is based on the Fourier shift theorem.\nLet the two images formula_1 and formula_2 be circularly-shifted versions of each other:\n\n(where the images are formula_17 in size).\n\nThen, the discrete Fourier transforms of the images will be shifted relatively in phase:\n\nOne can then calculate the normalized cross-power spectrum to factor out the phase difference:\n\nsince the magnitude of an imaginary exponential always is one, and the phase of formula_20 always is zero.\n\nThe inverse Fourier transform of a complex exponential is a Kronecker delta, i.e. a single peak:\n\nThis result could have been obtained by calculating the cross correlation directly. The advantage of this method is that the discrete Fourier transform and its inverse can be performed using the fast Fourier transform, which is much faster than correlation for large images.\n\nUnlike many spatial-domain algorithms, the phase correlation method is resilient to noise, occlusions, and other defects typical of medical or satellite images.\n\nThe method can be extended to determine rotation and scaling differences between two images by first converting the images to log-polar coordinates. Due to properties of the Fourier transform, the rotation and scaling parameters can be determined in a manner invariant to translation.\n\nIn practice, it is more likely that formula_2 will be a simple linear shift of formula_1, rather than a circular shift as required by the explanation above. In such cases, formula_9 will not be a simple delta function, which will reduce the performance of the method. In such cases, a window function (such as a Gaussian or Tukey window) should be employed during the Fourier transform to reduce edge effects, or the images should be zero padded so that the edge effects can be ignored. If the images consist of a flat background, with all detail situated away from the edges, then a linear shift will be equivalent to a circular shift, and the above derivation will hold exactly. The peak can be sharpened by using edge or vector correlation. \n\nFor periodic images (such as a chessboard), phase correlation may yield ambiguous results with several peaks in the resulting output.\n\nPhase correlation is the preferred method for television standards conversion, as it leaves the fewest artifacts.\n\nGeneral\n\nTelevision\n\n", "id": "1290557", "title": "Phase correlation"}
{"url": "https://en.wikipedia.org/wiki?curid=24286785", "text": "3D data acquisition and object reconstruction\n\n3D data acquisition and reconstruction is the generation of three-dimensional or spatiotemporal models from sensor data. The techniques and theories, generally speaking, work with most or all sensor types including optical, acoustic, laser scanning, radar, thermal, seismic.\n\nAcquisition can occur from a multitude of methods including 2D images, acquired sensor data and on site sensors.\n\n3D data acquisition and object reconstruction can be performed using stereo image pairs. Stereo photogrammetry or photogrammetry based on a block of overlapped images is the primary approach for 3D mapping and object reconstruction using 2D images. Close-range photogrammetry has also matured to the level where cameras or digital cameras can be used to capture the close-look images of objects, e.g., buildings, and reconstruct them using the very same theory as the aerial photogrammetry. An example of software which could do this is Vexcel FotoG 5. This software has now been replaced by Vexcel GeoSynth. Another similar software program is Microsoft Photosynth.\n\nA semi-automatic method for acquiring 3D topologically structured data from 2D aerial stereo images has been presented by Sisi Zlatanova. The process involves the manual digitizing of a number of points necessary for automatically reconstructing the 3D objects. Each reconstructed object is validated by superimposition of its wire frame graphics in the stereo model. The topologically structured 3D data is stored in a database and are also used for visualization of the objects. Software used for 3D data acquisition using 2D images include e.g. Agisoft Photoscan, RealityCapture, Autodesk 123D Catch, ENSAIS Engineering College TIPHON (Traitement d'Image et PHOtogrammétrie Numérique). CyberCity 3D Modeler, 3D UNDERWORLD SLS.\n\nA method for semi-automatic building extraction together with a concept for storing building models alongside terrain and other topographic data in a topographical information system has been developed by Franz Rottensteiner. His approach was based on the integration of building parameter estimations into the photogrammetry process applying a hybrid modeling scheme. Buildings are decomposed into a set of simple primitives that are reconstructed individually and are then combined by Boolean operators. The internal data structure of both the primitives and the compound building models are based on the boundary representation methods\n\nMultiple images are used in Zeng's approach to surface reconstruction from multiple images. A central idea is to explore the integration of both 3D stereo data and 2D calibrated images. This approach is motivated by the fact that only robust and accurate feature points that survived the geometry scrutiny of multiple images are reconstructed in space. The density insufficiency and the inevitable holes in the stereo data should then be filled in by using information from multiple images. The idea is thus to first construct small surface patches from stereo points, then to progressively propagate only reliable patches in their neighborhood from images into the whole surface using a best-first strategy. The problem thus reduces to searching for an optimal local surface patch going through a given set of stereo points from images.\n\nMulti-spectral images are also used for 3D building detection. The first and last pulse data and the normalized difference vegetation index are used in the process.\n\nNew measurement techniques are also employed to obtain measurements of and between objects from single images by using the projection, or the shadow as well as their combination. This technology is gaining attention given its fast processing time, and far lower cost than stereo measurements. GeoTango SilverEye technology is the first of this kind commercial product that can produce very realistic city models and buildings from single satellite and aerial images.\n\nSemi-Automatic building extraction from LIDAR Data and High-Resolution Images is also a possibility. Again, this approach allows modelling without physically moving towards the location or object. From airborne LIDAR data, digital surface model (DSM) can be generated and then the objects higher than the ground are automatically detected from the DSM. Based on general knowledge about buildings, geometric characteristics such as size, height and shape information are then used to separate the buildings from other objects. The extracted building outlines are then simplified using an orthogonal algorithm to obtain better cartographic quality. Watershed analysis can be conducted to extract the ridgelines of building roofs. The ridgelines as well as slope information are used to classify the buildings per type. The buildings are then reconstructed using three parametric building models (flat, gabled, hipped).\n\nLIDAR and other terrestrial laser scanning technology offers the fastest, automated way to collect height or distance information. LIDAR or laser for height measurement of buildings is becoming very promising. Commercial applications of both airborne LIDAR and ground laser scanning technology have proven to be fast and accurate methods for building height extraction. The building extraction task is needed to determine building locations, ground elevation, orientations, building size, rooftop heights, etc. Most buildings are described to sufficient details in terms of general polyhedra, i.e., their boundaries can be represented by a set of planar surfaces and straight lines. Further processing such as expressing building footprints as polygons is used for data storing in GIS databases.\n\nUsing laser scans and images taken from ground level and a bird's-eye perspective, Fruh and Zakhor present an approach to automatically create textured 3D city models. This approach involves registering and merging the detailed facade models with a complementary airborne model. The airborne modeling process generates a half-meter resolution model with a bird's-eye view of the entire area, containing terrain profile and building tops. Ground-based modeling process results in a detailed model of the building facades. Using the DSM obtained from airborne laser scans, they localize the acquisition vehicle and register the ground-based facades to the airborne model by means of Monte Carlo localization (MCL). Finally, the two models are merged with different resolutions to obtain a 3D model.\n\nUsing an airborne laser altimeter, Haala, Brenner and Anders combined height data with the existing ground plans of buildings. The ground plans of buildings had already been acquired either in analog form by maps and plans or digitally in a 2D GIS. The project was done in order to enable an automatic data capture by the integration of these different types of information. Afterwards virtual reality city models are generated in the project by texture processing, e.g. by mapping of terrestrial images. The project demonstrated the feasibility of rapid acquisition of 3D urban GIS. Ground plans proved are another very important source of information for 3D building reconstruction. Compared to results of automatic procedures, these ground plans proved more reliable since they contain aggregated information which has been made explicit by human interpretation. For this reason, ground plans, can considerably reduce costs in a reconstruction project. An example of existing ground plan data usable in building reconstruction is the Digital Cadastral map, which provides information on the distribution of property, including the borders of all agricultural areas and the ground plans of existing buildings. Additionally information as street names and the usage of buildings (e.g. garage, residential building, office block, industrial building, church) is provided in the form of text symbols. At the moment the Digital Cadastral map is built up as a data base covering an area, mainly composed by digitizing preexisting maps or plans.\n\nSoftware used for airborne laser scanning includes OPALS (Orientation and Processing of Airborne Laser Scanning data), ...\n\n\nAfter the data has been collected, the acquired (and sometimes already processed) data from images or sensors needs to be reconstructed. This may be done in the same program or in some cases, the 3D data needs to be exported and imported into another program for further refining, and/or to add additional data. Such additional data could be gps-location data, ... Also, after the reconstruction, the data might be directly implemented into a local (GIS) map or a worldwide map such as Google Earth.\n\nSeveral software packets are used in which the acquired (and sometimes already processed) data from images or sensors is imported. The software packets include (in alphabetical order):\n\n", "id": "24286785", "title": "3D data acquisition and object reconstruction"}
{"url": "https://en.wikipedia.org/wiki?curid=2997502", "text": "Orientation (computer vision)\n\nIn computer vision and image processing a common assumption is that sufficiently small image regions can be characterized as locally one-dimensional, e.g., in terms of lines or edges. For natural images this assumption is usually correct except at specific points, e.g., corners or line junctions or crossings, or in regions of high frequency textures. However, what size the regions have to be in order to appear as one-dimensional varies both between images and within an image. Also, in practice a local region is never exactly one-dimensional but can be so to a sufficient degree of approximation.\n\nImage regions which are one-dimensional are also referred to as simple or intrinsic one-dimensional (i1D).\n\nGiven an image of dimension d (d = 2 for ordinary images), a mathematical representation of a local i1D image region is\n\nformula_1\n\nwhere formula_2 is the image intensity function which varies over a local image coordinate formula_3 (a d-dimensional vector), formula_4 is a one-variable function, and formula_5 is a unit vector.\n\nThe intensity function formula_2 is constant in all directions which are perpendicular to formula_5. Intuitively, the orientation of an i1D-region is therefore represented by the vector formula_5. However, for a given formula_2, formula_5 is not uniquely determined. If\n\nformula_11\n\nformula_12\n\nthen formula_2 can be written as\n\nformula_14\n\nwhich implies that formula_11 also is a valid representation of the local orientation.\n\nIn order to avoid this ambiguity in the representation of local orientation two representations have been proposed\n\nThe double angle representation is only valid for 2D images (d=2), but the tensor representation can be defined for arbitrary dimensions d of the image data.\n\nA line between two points p1 and p2 has no given direction, but has a well-defined orientation. However, if one of the points p1 is used as a reference or origin, then the other point p2 can be described in terms of a vector which points in the direction to p2. Intuitively, orientation can be thought of as a direction without sign. Formally, this relates to projective spaces where the orientation of a vector corresponds to the equivalence class of vectors which are scaled versions of the vector.\n\nFor an image edge, we may talk of its direction which can be defined in terms of the gradient, pointing in the direction of maximum image intensity increase (from dark to bright). This implies that two edges can have the same orientation but the corresponding image gradients point in opposite directions if the edges go in different directions.\n\nIn image processing, the computation of the local image gradient is a common operation, e.g., for edge detection. If formula_2 above is an edge, then its gradient is parallel to formula_5. As is already discussed above the gradient is not a unique representation of orientation. Also, in the case of a local region which is centered on a line, the image gradient is approximately zero. However, in this case the vector formula_5 is still well-defined except for its sign. Therefore, formula_5 is a more appropriate starting point for defining local orientation than the image gradient.\n\nA number of methods have been proposed for computing or estimating an orientation representation from image data. These include\n\nThe first approach can be used both for the double angle representation (only 2D images) and the tensor representation, and the other methods compute a tensor representation of local orientation.\n\nGiven that a local image orientation representation has been computed for some image data, this formation can be used for solving the following tasks:\n", "id": "2997502", "title": "Orientation (computer vision)"}
{"url": "https://en.wikipedia.org/wiki?curid=25860534", "text": "3D pose estimation\n\n3D pose estimation is the problem of determining the transformation of an object in a 2D image which gives the 3D object. The need for 3D pose estimation arises from the limitations of feature based pose estimation. There exist environments where it is difficult to extract corners or edges from an image. To circumvent these issues, the object is dealt with as a whole through the use of free-form contours.\n\nIt is possible to estimate the 3D rotation and translation of a 3D object from a single 2D photo, if an approximate 3D model of the object is known and the corresponding points in the 2D image are known. A common technique for solving this has recently been \"POSIT\", where the 3D pose is estimated directly from the 3D model points and the 2D image points, and corrects the errors iteratively until a good estimate is found from a single image. Most implementations of POSIT only work on non-coplanar points (in other words, it won't work with flat objects or planes).\n\nAnother approach is to register a 3D CAD model over the photograph of a known object by optimizing a suitable distance measure with respect to the pose parameters.\n\nThe distance measure is computed between the object in the photograph and the 3D CAD model projection at a given pose.\nPerspective projection or orthogonal projection is possible depending on the pose representation used.\nThis approach is appropriate for applications where a 3D CAD model of a known object (or object category) is available.\n\nGiven a 2D image of an object, and the camera that is calibrated with respect to a world coordinate system, it is also possible to find the pose which gives the 3D object in its object coordinate system. This works as follows.\n\nStarting with a 2D image, image points are extracted which correspond to corners in an image. The projection rays from the image points are reconstructed from the 2D points so that the 3D points, which must be incident with the reconstructed rays, can be determined.\n\nThe algorithm for determining pose estimation is based on the Iterative Closest Point algorithm. The main idea is to determine the correspondences between 2D image features and points on the 3D model curve. \ncodice_1\n\nThe above algorithm does not account for images containing an object that is partially occluded. The following algorithm assumes that all contours are rigidly coupled, meaning the pose of one contour defines the pose of another contour.\n\ncodice_2\n\nIn practice, using a 2 GHz Intel Pentium processor, average speeds of 29fps have been reached using the above algorithm.\n\nSystems exist which use a database of an object at different rotations and translations to compare an input image against to estimate pose. These systems accuracy is limited to situations which are represented in their database of images, however the goal is to recognize a pose, rather than determine it.\n\n\n\n", "id": "25860534", "title": "3D pose estimation"}
{"url": "https://en.wikipedia.org/wiki?curid=26592482", "text": "ViBe\n\nViBe is a background subtraction algorithm which has been presented at the IEEE ICASSP 2009 conference and was refined in later publications. More precisely, it is a software module for extracting background information from moving images. It has been developed by Oliver Barnich and Marc Van Droogenbroeck of the Montefiore Institute, University of Liège, Belgium.\n\nViBe is patented: the patent covers various aspects such as stochastic replacement, spatial diffusion, and non-chronological handling.\n\nViBe is written in the programming language C, and has been implemented on CPU, GPU and FPGA.\n\nMany advanced techniques are used to provide an estimate of the temporal probability density function (pdf) of a pixel x. ViBe's approach is different, as it imposes the influence of a value in the polychromatic space to be limited to the local neighborhood. In practice, ViBe does not estimate the pdf, but uses a set of previously observed sample values as a pixel model. To classify a value pt(x), it is compared to its closest values among the set of samples.\n\nViBe ensures a smooth exponentially decaying lifespan for the sample values that constitute the pixel models. This makes ViBe able to successfully deal with concomitant events with a single model of a reasonable size for each pixel. This is achieved by choosing, randomly, which sample to replace when updating a pixel model. Once the sample to be discarded has been chosen, the new value replaces the discarded sample. It is interesting to note that the pixel model that would result from the update of a given pixel model with a given pixel sample cannot be predicted since the value to be discarded is chosen at random.\n\nTo ensure the spatial consistency of the whole image model and handle practical situations such as small camera movements or slowly evolving background objects, ViBe uses a technique similar to that developed for the updating process in which it chooses at random and update a pixel model in the neighborhood of the current pixel. By denoting NG(x) and p(x) respectively the spatial neighborhood of a pixel x and its value, and assuming that it was decided to update the set of samples of x by inserting p(x), then ViBe also use this value p(x) to update the set of samples of one of the pixels in the neighborhood NG(x), chosen at random. As a result, ViBe is able to produce spatially coherent results directly without the use of any post-processing method.\n\nAlthough the model could easily recover from any type of initialization, for example by choosing a set of random values, it is convenient to get an accurate background estimate as soon as possible. Ideally a segmentation algorithm would like to be able to segment the video sequences starting from the second frame, the first frame being used to initialize the model. Since no temporal information is available prior to the second frame, ViBe populates the pixel models with values found in the spatial neighborhood of each pixel; more precisely, it initializes the background model with values taken randomly in each pixel neighborhood of the first frame. The background\nestimate is therefore valid starting from the second frame of a video sequence.\n", "id": "26592482", "title": "ViBe"}
{"url": "https://en.wikipedia.org/wiki?curid=27395982", "text": "Automated Imaging Association\n\nAutomated Imaging Association (AIA) is the world's largest machine vision trade group. AIA has more than 330 members from 32 countries, including system integrators, camera, lighting and other vision components manufacturers, vision software providers, OEMs and distributors. The association's headquarters is located in Ann Arbor, Michigan.\n\nThe Camera Link and GigE vision communication protocols are maintained and administered by the Automated Imaging Association.\n\nThe AIA seeks to control dissemination of the standards under its care. Neither the Camera Link nor the GigE Vision standard is available for public download, and neither may be freely copied.\n\nBAE Systems, Texas Instruments, National Semiconductor and Sony are among the multi-billion dollar member companies in the AIA. Cognex Corporation and National Instruments are also two big names in the machine vision industry that are members of the AIA. In 2010, 51% of the members are from North America, 30% are from Europe, 15% are from Eastern Asia, less than 1% are from South America, 2% are from Western Asia, less than 1% are from Southern Asia, 1% are from Southeastern Asia and less than 1% of the members are from Australia. Surprisingly in this U.S. based association there is also some member company from Iran, named Kasra Hooshmand Engineering Co., P.J.S. (KDI) and Asian Bina Machine Co and novinilya company.\n", "id": "27395982", "title": "Automated Imaging Association"}
{"url": "https://en.wikipedia.org/wiki?curid=27169328", "text": "Randomized Hough transform\n\nHough transforms are techniques for object detection, a critical step in many implementations of computer vision, or data mining from images. Specifically, the Randomized Hough transform is a probabilistic variant to the classical Hough transform, and is commonly used to detect curves (straight line, circle, ellipse, etc.) The basic idea of Hough transform (HT) is to implement a voting procedure for all potential curves in the image, and at the termination of the algorithm, curves that do exist in the image will have relatively high voting scores. Randomized Hough transform (RHT) is different from HT in that it tries to avoid conducting the computationally expensive voting process for every nonzero pixel in the image by taking advantage of the geometric properties of analytical curves, and thus improve the time efficiency and reduce the storage requirement of the original algorithm. \n\nAlthough Hough transform (HT) has been widely used in curve detection, it has two major drawbacks: First, for each nonzero pixel in the image, the parameters for the existing curve and redundant ones are both accumulated during the voting procedure. Second, the accumulator array (or Hough space) is predefined in a heuristic way. The more accuracy needed, the higher parameter resolution should be defined. These two needs usually result in a large storage requirement and low speed for real applications. Therefore, RHT was brought up to tackle this problem.\n\nIn comparison with HT, RHT takes advantage of the fact that some analytical curves can be fully determined by a certain number of points on the curve. For example, a straight line can be determined by two points, and an ellipse (or a circle) can be determined by three points. The case of ellipse detection can be used to illustrate the basic idea of RHT. The whole process generally consists of three steps:\n\nOne general equation for defining ellipses is:\nformula_1\n\nwith restriction: formula_2\n\nHowever, an ellipse can be fully determined if one knows three points on it and the tangents in these points.\n\nRHT starts by randomly selecting three points on the ellipse. Let them be X, X and X. The first step is to find the tangents of these three points. They can be found by fitting a straight line using least squares technique for a small window of neighboring pixels. \n\nThe next step is to find the intersection points of the tangent lines. This can be easily done by solving the line equations found in the previous step. Then let the intersection points be T and T, the midpoints of line segments formula_3 and formula_4 be M and M. Then the center of the ellipse will lie in the intersection of formula_5 and formula_6. Again, the coordinates of the intersected point can be determined by solving line equations and the detailed process is skipped here for conciseness. \n\nLet the coordinates of ellipse center found in previous step be (x, y). Then the center can be translated to the origin with formula_7 and formula_8 so that the ellipse equation can be simplified to:\n\nformula_9\n\nNow we can solve for the rest of ellipse parameters: a, b and c by substituting the coordinates of X, X and X into the equation above.\n\nWith the ellipse parameters determined from previous stage, the accumulator array can be updated correspondingly. Different from classical Hough transform, RHT does not keep \"grid of buckets\" as the accumulator array. Rather, it first calculates the similarities between the newly detected ellipse and the ones already stored in accumulator array. Different metrics can be used to calculate the similarity. As long as the similarity exceeds some predefined threshold, replace the one in the accumulator with the average of both ellipses and add 1 to its score. Otherwise, initialize this ellipse to an empty position in the accumulator and assign a score of 1.\n\nOnce the score of one candidate ellipse exceeds the threshold, it is determined as existing in the image (in other words, this ellipse is detected), and should be removed from the image and accumulator array so that the algorithm can detect other potential ellipses faster. The algorithm terminates when the number of iterations reaches a maximum limit or all the ellipses have been detected.\n\nPseudo code for RHT:\n", "id": "27169328", "title": "Randomized Hough transform"}
{"url": "https://en.wikipedia.org/wiki?curid=4163064", "text": "Glossary of machine vision\n\nThe following are common definitions related to the machine vision field.\n\nGeneral related fields\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwhere formula_2 is the resonant frequency, formula_3 is the stored energy in the cavity, and formula_4 is the power dissipated. The optical \"Q\" is equal to the ratio of the resonant frequency to the bandwidth of the cavity resonance. The average lifetime of a resonant photon in the cavity is proportional to the cavity's \"Q\". If the \"Q\" factor of a laser's cavity is abruptly changed from a low value to a high one, the laser will emit a pulse of light that is much more intense than the laser's normal continuous output. This technique is known as Q-switching.\n\n\n\n\n\n\n\n\n\n\n", "id": "4163064", "title": "Glossary of machine vision"}
{"url": "https://en.wikipedia.org/wiki?curid=2288302", "text": "Active shape model\n\nActive shape models (ASMs) are statistical models of the shape of objects which iteratively deform to fit to an example of the object in a new image, developed by Tim Cootes and Chris Taylor in 1995. The shapes are constrained by the PDM (point distribution model) Statistical Shape Model to vary only in ways seen in a training set of labelled examples. \nThe shape of an object is represented by a set of points (controlled by the shape model). The ASM algorithm aims to match the model to a new image.\n\nThe ASM works by alternating the following steps:\n\nThe technique has been widely used to analyse images of faces, mechanical assemblies and medical images (in 2D and 3D).\n\nIt is closely related to the active appearance model. It is also known as a \"Smart Snakes\" method, since it is an analog to an active contour model which would respect explicit shape constraints.\n\n\n", "id": "2288302", "title": "Active shape model"}
{"url": "https://en.wikipedia.org/wiki?curid=474813", "text": "Color histogram\n\nIn image processing and photography, a color histogram is a representation of the distribution of colors in an image. For digital images, a color histogram represents the number of pixels that have colors in each of a fixed list of color ranges, that span the image's color space, the set of all possible colors.\n\nThe color histogram can be built for any kind of color space, although the term is more often used for three-dimensional spaces like RGB or HSV. For monochromatic images, the term intensity histogram may be used instead. For multi-spectral images, where each pixel is represented by an arbitrary number of measurements (for example, beyond the three measurements in RGB), the color histogram is \"N\"-dimensional, with N being the number of measurements taken. Each measurement has its own wavelength range of the light spectrum, some of which may be outside the visible spectrum.\n\nIf the set of possible color values is sufficiently small, each of those colors may be placed on a range by itself; then the histogram is merely the count of pixels that have each possible color. Most often, the space is divided into an appropriate number of ranges, often arranged as a regular grid, each containing many similar color values. The color histogram may also be represented and displayed as a smooth function defined over the color space that approximates the pixel counts.\n\nLike other kinds of histograms, the color histogram is a statistic that can be viewed as an approximation of an underlying continuous distribution of colors values.\n\nColor histograms are flexible constructs that can be built from images in various color spaces, whether RGB, rg chromaticity or any other color space of any dimension. A histogram of an image is produced first by discretization of the colors in the image into a number of bins, and counting the number of image pixels in each bin. For example, a Red–Blue chromaticity histogram can be formed by first normalizing color pixel values by dividing RGB values by R+G+B, then quantizing the normalized R and B coordinates into N bins each. A two-dimensional histogram of Red-Blue chromaticity divided into four bins (\"N\"=4) might yield a histogram that looks like this table:\n\nA histogram can be N-dimensional. Although harder to display, a three-dimensional color histogram for the above example could be thought of as four separate Red-Blue histograms, where each of the four histograms contains the Red-Blue values for a bin of green (0-63, 64-127, 128-191, and 192-255).\n\nThe histogram provides a compact summarization of the distribution of data in an image. The color histogram of an image is relatively invariant with translation and rotation about the viewing axis, and varies only slowly with the angle of view. By comparing histograms signatures of two images and matching the color content of one image with the other, the color histogram is particularly well suited for the problem of recognizing an object of unknown position and rotation within a scene. Importantly, translation of an RGB image into the illumination invariant rg-chromaticity space allows the histogram to operate well in varying light levels.\n\n1.What is a histogram?\n\nA histogram is a graphical representation of the number of pixels in an image. In a more simple way to explain, a histogram is a bar graph, whose X-axis represents the tonal scale(black at the left and white at the right), and Y-axis represents the number of pixels in an image in a certain area of the tonal scale. For example, the graph of a luminance histogram shows the number of pixels for each brightness level(from black to white), and when there are more pixels, the peak at the certain luminance level is higher.\n\n2.What is a color histogram?\n\nA color histogram of an image represents the distribution of the composition of colors in the image. It shows different types of colors appeared and the number of pixels in each type of the colors appeared. The relation between a color histogram and a luminance histogram is that a color histogram can be also expressed as “Three Luminance Histograms”, each of which shows the brightness distribution of each individual Red/Green/Blue color channel.\n\nA color histogram focuses only on the proportion of the number of different types of colors, regardless of the spatial location of the colors. The values of a color histogram are from statistics. They show the statistical distribution of colors and the essential tone of an image.\n\nIn general, as the color distributions of the foreground and background in an image are different, there might be a bimodal distribution in the histogram.\n\nFor the luminance histogram alone, there is no perfect histogram and in general, the histogram can tell whether it is over exposure or not, but there are times when you might think the image is over exposed by viewing the histogram; however, in reality it is not.\n\nThe formation of a color histogram is rather simple. From the definition above, we can simply count the number of pixels for each 256 scales in each of the 3 RGB channel, and plot them on 3 individual bar graphs.\n\nIn general, a color histogram is based on a certain color space, such as RGB or HSV. When we compute the pixels of different colors in an image, if the color space is large, then we can first divide the color space into certain numbers of small intervals. Each of the intervals is called a bin. This process is called color quantization. Then, by counting the number of pixels in each of the bins, we get the color histogram of the image.\n\nThe concrete steps of the principles can be viewed in Example 2.\n\nGiven the following image of a cat (an original version and a version that has been reduced to 256 colors for easy histogram purposes), the following data represents a color histogram in the RGB color space, using four bins. Bin 0 corresponds to intensities 0-63, bin 1 is 64-127, bin 2 is 128-191, and bin 3 is 192-255.\n\nApplication in camera:\n\nNowadays, some cameras have the ability of showing the 3 color histograms when we take photos. \n\nWe can examine clips(spikes on either the black or white side of the scale) in each of the 3 RGB color histograms. If we find one or more clipping on a channel of the 3 RGB channels, then this would result in a loss of detail for that color.\n\nTo illustrate this, consider this example:\n\n1. We know that each of the three R, G, B channels has a range of values from 0-255(8 bit). So consider a photo that has a luminance range of 0-255.\n\n2. Assume the photo we take is made of 4 blocks that are adjacent to each other and we set the luminance scale for each of the 4 blocks of original photo to be 10, 100, 205, 245. Thus, the image looks like the first figure on the right. \n\n3. Then, we over expose the photo a little, say, the luminance scale of each block is increased by 10. Thus, the luminance scale for each of the 4 blocks of new photo is 20, 110, 215, 255. Then, the image looks like the second figure on the right.\n\nThere is not much difference between figure 8 and figure 9, all we can see is that the whole image becomes brighter(the contrast for each of the blocks remain the same).\n\n4. Now, we over expose the original photo again, this time the luminance scale of each block is increased by 50. Thus, the luminance scale for each of the 4 blocks of new photo is 60, 150, 255, 255. The new image now looks like the third figure on the right.\n\nNote that the scale for last block is 255 instead of 295, for 255 is the top scale and thus the last block has clipped! When this happens, we lose the contrast of the last 2 blocks, and thus, we cannot recover the image no matter how we adjust it.\n\nTo conclude, when taking photos with a camera that displays histograms, always keep the brightest tone in the image below the largest scale 255 on the histogram in order to avoid losing details.\n\nThe main drawback of histograms for classification is that the representation is dependent of the color of the object being studied, ignoring its shape and texture. Color histograms can potentially be identical for two images with different object content which happens to share color information. Conversely, without spatial or shape information, similar objects of different color may be indistinguishable based solely on color histogram comparisons. There is no way to distinguish a red and white cup from a red and white plate. Put another way, histogram-based algorithms have no concept of a generic 'cup', and a model of a red and white cup is no use when given an otherwise identical blue and white cup. Another problem is that color histograms have high sensitivity to noisy interference such as lighting intensity changes and quantization errors. High dimensionality (bins) color histograms are also another issue. Some color histogram feature spaces often occupy more than one hundred dimensions.\n\nSome of the proposed solutions have been color histogram intersection, color constant indexing, cumulative color histogram, quadratic distance, and color correlograms. Although there are drawbacks of using histograms for indexing and classification, using color in a real-time system has several advantages. One is that color information is faster to compute compared to other invariants. It has been shown in some cases that color can be an efficient method for identifying objects of known location and appearance.\n\nFurther research into the relationship between color histogram data to the physical properties of the objects in an image has shown they can represent not only object color and illumination but relate to surface roughness and image geometry and provide an improved estimate of illumination and object color.\n\nUsually, Euclidean distance, histogram intersection, or cosine or quadratic distances are used for the calculation of image similarity ratings. Any of these values do not reflect the similarity rate of two images in itself; it is useful only when used in comparison to other similar values. This is the reason that all the practical implementations of content-based image retrieval must complete computation of all images from the database, and is the main disadvantage of these implementations.\n\nAnother approach to representative color image content is two-dimensional color histogram. A two-dimensional color histogram considers the relation between the pixel pair colors (not only the lighting component). A two-dimensional color histogram is a two-dimensional array. The size of each dimension is the number of colors that were used in the phase of color quantization. These arrays are treated as matrices, each element of which stores a normalized count of pixel pairs, with each color corresponding to the index of an element in each pixel neighborhood. For comparison of two-dimensional color histograms it is suggested calculating their correlation, because constructed as described above, is a random vector (in other words, a multi-dimensional random value). While creating a set of final images, the images should be arranged in decreasing order of the correlation coefficient.\n\nThe correlation coefficient may also be used for color histogram comparison. Retrieval results with correlation coefficient are better than with other metrics.\n\nThe idea of an intensity histogram can be generalized to continuous data,\nsay audio signals represented by real functions or images represented by functions with two-dimensional domain.\n\nLet formula_1 (see Lebesgue space), then the cumulative histogram operator formula_2 can be defined by:\nformula_4 is the Lebesgue measure of sets.\nformula_5 in turn is a real function.\nThe (non-cumulative) histogram is defined as its derivative.\n\n", "id": "474813", "title": "Color histogram"}
{"url": "https://en.wikipedia.org/wiki?curid=2132859", "text": "Image moment\n\nIn image processing, computer vision and related fields, an image moment is a certain particular weighted average (moment) of the image pixels' intensities, or a function of such moments, usually chosen to have some attractive property or interpretation.\n\nImage moments are useful to describe objects after segmentation. Simple properties of the image which are found \"via\" image moments include area (or total intensity), its centroid, and information about its orientation.\n\nFor a 2D continuous function \"f\"(\"x\",\"y\") the moment (sometimes called \"raw moment\") of order (\"p\" + \"q\") is defined as\n\nfor \"p\",\"q\" = 0,1,2...\nAdapting this to scalar (greyscale) image with pixel intensities \"I\"(\"x\",\"y\"), raw image moments \"M\" are calculated by\n\nIn some cases, this may be calculated by considering the image as a probability density function, \"i.e.\", by dividing the above by\n\nA uniqueness theorem (Hu [1962]) states that if \"f\"(\"x\",\"y\") \nis piecewise continuous and has nonzero values only in a finite part of the \"xy\" \nplane, moments of all orders exist, and the moment sequence (\"M\") is uniquely determined by \"f\"(\"x\",\"y\"). Conversely, (\"M\") uniquely determines \"f\"(\"x\",\"y\"). In practice, the image is summarized with functions of a few lower order moments.\n\nSimple image properties derived \"via\" raw moments include:\n\nCentral moments are defined as\n\nwhere formula_5 and formula_6 are the components of the centroid.\n\nIf \"ƒ\"(\"x\", \"y\") is a digital image, then the previous equation becomes\n\nThe central moments of order up to 3 are:\n\nIt can be shown that:\n\nCentral moments are translational invariant.\n\nInformation about image orientation can be derived by first using the second order central moments to construct a covariance matrix.\n\nThe covariance matrix of the image formula_22 is now\n\nThe eigenvectors of this matrix correspond to the major and minor axes of the image intensity, so the orientation can thus be extracted from the angle of the eigenvector associated with the largest eigenvalue towards the axis closest to this eigenvector. It can be shown that this angle Θ is given by the following formula:\n\nThe above formula holds as long as:\n\nThe eigenvalues of the covariance matrix can easily be shown to be\n\nand are proportional to the squared length of the eigenvector axes. The relative difference in magnitude of the eigenvalues are thus an indication of the eccentricity of the image, or how elongated it is. The eccentricity is\n\nMoments are well-known for their application in image analysis, since they can be used to derive invariants with respect to specific transformation classes.\n\nThe term \"invariant moments\" is often abused in this context. However, while \"moment invariants\" are invariants that are formed from moments, the only moments that are invariants themselves are the central moments.\n\nNote that the invariants detailed below are exactly invariant only in the continuous domain. In a discrete domain, neither scaling nor rotation are well defined: a discrete image transformed in such a way is generally an approximation, and the transformation is not reversible. These invariants therefore are only approximately invariant when describing a shape in a discrete image.\n\nThe central moments \"μ\" of any order are, by construction, invariant with respect to translations.\n\nInvariants \"η\" with respect to both translation and scale can be constructed from central moments by dividing through a properly scaled zero-th central moment:\n\nwhere \"i\" + \"j\" ≥ 2.\nNote that translational invariance directly follows by only using central moments.\n\nAs shown in the work of Hu et al.,\ninvariants with respect to translation, scale, and \"rotation\" can be constructed:\n\nformula_29\n\nformula_30\n\nformula_31\n\nformula_32\n\nformula_33\n\nformula_34\n\nformula_35\n\nThese are well-known as \"Hu moment invariants\".\n\nThe first one, \"I\", is analogous to the moment of inertia around the image's centroid, where the pixels' intensities are analogous to physical density. The last one, \"I\", is skew invariant, which enables it to distinguish mirror images of otherwise identical images.\n\nA general theory on deriving complete and independent sets of rotation moment invariants was proposed by J. Flusser. He showed that the traditional set of Hu moment invariants is not independent nor complete. \"I\" is not very useful as it is dependent on the others. In the original Hu's set there is a missing third order independent moment invariant:\n\nLater, J. Flusser and T. Suk specialized the theory for N-rotationally symmetric shapes case.\n\nZhang et al. applied Hu moment invariants to solve the Pathological Brain Detection (PBD) problem.\n\n", "id": "2132859", "title": "Image moment"}
{"url": "https://en.wikipedia.org/wiki?curid=172088", "text": "Machine vision\n\nMachine vision (MV) is the technology and methods used to provide imaging-based automatic inspection and analysis for such applications as automatic inspection, process control, and robot guidance, usually in industry. Machine vision is a term encompassing a large number of technologies, software and hardware products, integrated systems, actions, methods and expertise. Machine vision as a systems engineering discipline can be considered distinct from computer vision, a form of computer science. It attempts to integrate existing technologies in new ways and apply them to solve real world problems. The term is also used in a broader sense by trade shows and trade groups; this broader definition also encompasses products and applications most often associated with image processing. \n\nDefinitions of the term \"Machine vision\" vary, but all include the technology and methods used to extract information from an image, as opposed to image processing, where the output is another image. The information extracted can be a simple good-part/bad-part signal, or more a complex set of data such as the identity, position and orientation of each object in an image. The information can be used for such applications as automatic inspection and robot and process guidance in industry, or for security monitoring. This field encompasses a large number of technologies, software and hardware products, integrated systems, actions, methods and expertise. Machine vision as a systems engineering discipline can be considered distinct from computer vision, a form of basic computer science; machine vision attempts to integrate existing technologies in new ways and apply them to solve real world problems in a way that meets the requirements of industrial automation and similar application areas. The term is also used in a broader sense by trade shows and trade groups such as the Automated Imaging Association and the European Machine Vision Association. This broader definition also encompasses products and applications most often associated with image processing. The primary uses for machine vision are automatic inspection and industrial robot/process guidance. See glossary of machine vision.\nThe primary uses for machine vision are imaging-based automatic inspection and sorting and robot guidance.; in this section the former is abbreviated as \"automatic inspection\". The overall process includes planning the details of the requirements and project, and then creating a solution. This section describes the technical process that occurs during the operation of the solution.\n\nThe first step in the automatic inspection sequence of operation is acquisition of an image, typically using cameras, lenses, and lighting that has been designed to provide the differentiation required by subsequent processing. MV software packages and programs developed in them then employ various digital image processing techniques to extract the required information, and often make decisions (such as pass/fail) based on the extracted information.\n\nThe components of an automatic inspection system usually include lighting, a camera or other imager, a processor, software, and output devices.\n\nThe imaging device (e.g. camera) can either be separate from the main image processing unit or combined with it in which case the combination is generally called a smart camera or smart sensor. When separated, the connection may be made to specialized intermediate hardware, a custom processing appliance, or a frame grabber within a computer using either an analog or standardized digital interface (Camera Link, CoaXPress). MV implementations also use digital cameras capable of direct connections (without a framegrabber) to a computer via FireWire, USB or Gigabit Ethernet interfaces.\n\nWhile conventional (2D visible light) imaging is most commonly used in MV, alternatives include multispectral imaging, hyperspectral imaging, imaging various infrared bands, line scan imaging, 3D imaging of surfaces and X-ray imaging. Key divisions within MV 2D visible light imaging are monochromatic vs. color, resolution, and whether or not the imaging process is simultaneous over the entire image, making it suitable for moving processes.\n\nThough the vast majority of machine vision applications are solved using two-dimensional imaging, machine vision applications utilizing 3D imaging are a growing niche within the industry. The most commonly used method for 3D imaging is scanning based triangulation which utilizes motion of the product or image during the imaging process. A laser is projected onto the surfaces of an object and viewed from a different angle. In machine vision this is accomplished with a scanning motion, either by moving the workpiece, or by moving the camera & laser imaging system. The line is viewed by a camera from a different angle; the deviation of the line represents shape variations. Lines from multiple scans are assembled into a depth map or point cloud. Stereoscopic vision is used in special cases involving unique features present in both views of a pair of cameras. Other 3D methods used for machine vision are time of flight and grid based. One method is grid array based systems using pseudorandom structured light system as employed by the Microsoft Kinect system circa 2012.\n\nAfter an image is acquired, it is processed. Multiple stages of processing are generally used in a sequence that ends up as a desired result. A typical sequence might start with tools such as filters which modify the image, followed by extraction of objects, then extraction (e.g. measurements, reading of codes) of data from those objects, followed by communicating that data, or comparing it against target vales to create and communicate \"pass/fail\" results. Machine vision image processing methods include;\n\n\nA common output from automatic inspection systems is pass/fail decisions. These decisions may in turn trigger mechanisms that reject failed items or sound an alarm. Other common outputs include object position and orientation information for robot guidance systems. Additionally, output types include numerical measurement data, data read from codes and characters, counts and classification of objects, displays of the process or results, stored images, alarms from automated space monitoring MV systems, and process control signals. This also includes user interfaces, interfaces for the integration of multi-component systems and automated data interchange.\n\nMachine vision commonly provides location and orientation information to a robot to allow the robot to properly grasp the product. This capability is also used to guide motion that is simpler than robots, such as a 1 or 2 axis motion controller. The overall process includes planning the details of the requirements and project, and then creating a solution. This section describes the technical process that occurs during the operation of the solution. Many of the process steps are the same as with automatic inspection except with a focus on providing position and orientation information as the end result.\n\nThe global Machine Vision market is expected to reach USD 15.46 billion by the end of 2022 with 8.18% CAGR during forecast period 2017-2022. Machine Vision Market is growing with the positive growth in all the regions. Increasing application areas year on year and advancement in technology and integration is driving the market on global scale. Asia Pacific is dominating the global market with more than 30% of market share followed by Europe which stands as second biggest market due to the heavy demand from automotive and healthcare industry. North America stands as third biggest market.\n", "id": "172088", "title": "Machine vision"}
{"url": "https://en.wikipedia.org/wiki?curid=8210422", "text": "Stereo cameras\n\nThe stereo cameras approach is a method of distilling a noisy video signal into a coherent data set that a computer can begin to process into actionable symbolic objects, or abstractions. Stereo cameras is one of many approaches used in the broader fields of computer vision and machine vision.\n\nIn this approach, two cameras with a known physical relationship (i.e. a common field of view the cameras can see, and how far apart their focal points sit in physical space) are correlated via software. By finding mappings of common pixel values, and calculating how far apart these common areas reside in pixel space, a rough depth map can be created. This is very similar to how the human brain uses stereoscopic information from the eyes to gain depth cue information, i.e. how far apart any given object in the scene is from the viewer.\n\nThe camera attributes must be known, focal length and distance apart etc., and a calibration done. Once this is completed the systems can be used to sense the distances of objects by triangulation. Finding the same singular physical point in the two left and right images is known as the \"correspondence problem\". Correctly locating the point gives the computer the capability to calculate the distance that the robot or camera is from the object. On the BH2 Lunar Rover the cameras use five steps: a bayer array filter, photometric consistency dense matching algorithm, a Laplace of Gaussian (LoG) edge detection algorithm, a stereo matching algorithm and finally uniqueness constraint.\n\nThis type of stereoscopic image processing technique is used in applications such as robotic control and sensing, crowd dynamics monitoring and off-planet terrestrial rovers; for example, in mobile robot navigation, people tracking, gesture recognition, targeting, 3D surface visualization, immersive and interactive gaming. Although the Xbox Kinect sensor is also able to create a depth map of an image, it uses an infrared camera for this purpose, and does not use the dual-camera technique.\n\nOther approaches to stereoscopic sensing include time of flight sensors and ultrasound.\n\n", "id": "8210422", "title": "Stereo cameras"}
{"url": "https://en.wikipedia.org/wiki?curid=474939", "text": "Geometric hashing\n\nIn computer science, geometric hashing is originally a method for efficiently finding two-dimensional objects represented by discrete points that have undergone an affine transformation (example below is based on similarity transformation), though extensions exist to some other object representations and transformations. In an off-line step, the objects are encoded by treating each pair of points as a geometric basis. The remaining points can be represented in an invariant fashion with respect to this basis using two parameters. For each point, its quantized transformed coordinates are stored in the hash table as a key, and indices of the basis points as a value. Then a new pair of basis points is selected, and the process is repeated. In the on-line (recognition) step, randomly selected pairs of data points are considered as candidate bases. For each candidate basis, the remaining data points are encoded according to the basis and possible correspondences from the object are found in the previously constructed table. The candidate basis is accepted if a sufficiently large number of the data points index a consistent object basis.\n\nGeometric hashing was originally suggested in computer vision for object recognition in 2D and 3D, but later was applied to different problems such as structural alignment of proteins.\n\nGeometric Hashing is a method used for object recognition. Let’s say that we want to check if a model image can be seen in an input image. This can be accomplished with geometric hashing. The method could be used to recognize one of the multiple objects in a base, in this case the hash table should store not only the pose information but also the index of object model in the base.\n\nFor simplicity, this example will not use too many point features and assume that their descriptors are given by their coordinates only (in practice local descriptors such as SIFT could be used for indexing).\n\n\nHash Table:\n\nMost hash tables cannot have identical keys mapped to different values. So in real life one won’t encode basis keys (1.0, 0.0) and (-1.0, 0.0) in a hash table.\n\n\nIt seems that this method is only capable of handling scaling, translation, and rotation. However, the input Image may contain the object in mirror transform. Therefore, geometric hashing should be able to find the object, too. In fact, there are two ways to detect mirrored objects.\n\n\nSimilar to the example above, hashing applies to higher-dimensional data. For three-dimensional data points, three points are also needed for the basis. The first two points define the x-axis, and the third point defines the y-axis (with the first point). The z-axis is perpendicular to the created axis using the right-hand rule. Notice that the order of the points affects the resulting basis\n\n", "id": "474939", "title": "Geometric hashing"}
{"url": "https://en.wikipedia.org/wiki?curid=9170159", "text": "Binocular disparity\n\nBinocular disparity refers to the difference in image location of an object seen by the left and right eyes, resulting from the eyes’ horizontal separation (parallax). The brain uses binocular disparity to extract depth information from the two-dimensional retinal images in stereopsis. In computer vision, binocular disparity refers to the difference in coordinates of similar features within two stereo images.\n\nA similar disparity can be used in rangefinding by a coincidence rangefinder to determine distance and/or altitude to a target. In astronomy, the disparity between different locations on the Earth can be used to determine various celestial parallax, and Earth's orbit can be used for stellar parallax.\n\nHuman eyes are horizontally separated by about 50–75 mm (interpupillary distance) depending on each individual. Thus, each eye has a slightly different view of the world around. This can be easily seen when alternately closing one eye while looking at a vertical edge. The binocular disparity can be observed from apparent horizontal shift of the vertical edge between both views.\n\nAt any given moment, the line of sight of the two eyes meet at a point in space. This point in space projects to the same location (i.e. the center) on the retinae of the two eyes. Because of the different viewpoints observed by the left and right eye however, many other points in space do not fall on corresponding retinal locations. Visual binocular disparity is defined as the difference between the point of projection in the two eyes and is usually expressed in degrees as the visual angle.\n\nThe term \"binocular disparity\" refers to geometric measurements made external to the eye. The disparity of the images on the actual retina depends on factors internal to the eye, especially the location of the nodal points, even if the cross section of the retina is a perfect circle. Disparity on retina conforms to binocular disparity when measured as degrees, while much different if measured as distance due to the complicated structure inside eye.\n\nFigure 1: The full black circle is the point of fixation. The blue object lies nearer to the observer. Therefore, it has a \"near\" disparity d. Objects lying more far away (green) correspondingly have a \"far\" disparity d. Binocular disparity is the angle between two lines of projection in one eye (mathematically, d-d, with sign, measured counterclockwise). One of which is the real projection from the object to the actual point of projection. The other one is the imaginary projection running through the nodal point of the fixation point.\n\nIn computer vision, binocular disparity is calculated from stereo images taken from a set of stereo cameras. The variable distance between these cameras, called the baseline, can affect the disparity of a specific point on their respective image plane. As the baseline increases, the disparity increases due to the greater angle needed to align the sight on the point. However, in computer vision, binocular disparity is referenced as coordinate differences of the point between the right and left images instead of a visual angle. The units are usually measured in pixels.\n\nBrain cells (neurons) in a part of the brain responsible for processing visual information coming from the retinae (primary visual cortex) can detect the existence of disparity in their input from the eyes. Specifically, these neurons will be active, if an object with \"their\" special disparity lies within the part of the visual field to which they have access (receptive field).\n\nResearchers investigating precise properties of these neurons with respect to disparity present visual stimuli with different disparities to the cells and look whether they are active or not. One possibility to present stimuli with different disparities is to place objects in varying depth in front of the eyes. However, the drawback to this method may not be precise enough for objects placed further away as they possess smaller disparities while objects closer will have greater disparities. Instead, neuroscientists use an alternate method as schematised in Figure 2.\n\nFigure 2: The disparity of an object with different depth than the fixation point can alternatively be produced by presenting an image of the object to one eye and a laterally shifted version of the same image to the other eye. The full black circle is the point of fixation. Objects in varying depths are placed along the line of fixation of the left eye. The same disparity produced from a shift in depth of an object (filled coloured circles) can also be produced by laterally shifting the object in constant depth in the picture one eye sees (black circles with coloured margin). Note that for near disparities the lateral shift has to be larger to correspond to the same depth compared with far disparities. This is what neuroscientists usually do with random dot stimuli to study disparity selectivity of neurons since the lateral distance required to test disparities is less than the distances required using depth tests. This principle has also been applied in autostereogram illusions.\n\nThe disparity of features between two stereo images are usually computed as a shift to the left of an image feature when viewed in the right image. For example, a single point that appears at the x coordinate \"t\" (measured in pixels) in the left image may be present at the x coordinate \"t - 3\" in the right image. In this case, the disparity at that location in the right image would be 3 pixels.\n\nStereo images may not always be correctly aligned to allow for quick disparity calculation. For example, the set of cameras may be slightly rotated off level. Through a process known as image rectification, both images are rotated to allow for disparities in only the horizontal direction (i.e. there is no disparity in the y image coordinates). This is a property that can also be achieved by precise alignment of the stereo cameras before image capture.\n\nAfter rectification, the correspondence problem can be solved using an algorithm that scans both the left and right images for matching image features. A common approach to this problem is to form a smaller image patch around every pixel in the left image. These image patches are compared to all possible disparities in the right image by comparing their corresponding image patches. For example, for a disparity of 1, the patch in the left image would be compared to a similar-sized patch in the right, shifted to the left by one pixel. The comparison between these two patches can be made by attaining a computational measure from one of the following equations that compares each of the pixels in the patches. For all of the following equations, L and R refer to the left and right columns while r and c refer to the current row and column of either images being examined. \"d\" refers to the disparity of the right image.\n\n\nThe disparity with the lowest computed value using one of the above methods is considered the disparity for the image feature. This lowest score indicates that the algorithm has found the best match of corresponding features in both images.\n\nThe method described above is a brute-force search algorithm. With large patch and/or image sizes, this technique can be very time consuming as pixels are constantly being re-examined to find the lowest correlation score. However, this technique also involves unnecessary repetition as many pixels overlap. A more efficient algorithm involves remembering all values from the previous pixel. An even more efficient algorithm involves remembering column sums from the previous row (in addition to remembering all values from the previous pixel). Techniques that save previous information can greatly increase the algorithmic efficiency of this image analyzing process.\n\nKnowledge of disparity can be used in further extraction of information from stereo images. One case that disparity is most useful is for depth/distance calculation. Disparity and distance from the cameras are inversely related. As the distance from the cameras increases, the disparity decreases. This allows for depth perception in stereo images. Using geometry and algebra, the points that appear in the 2D stereo images can be mapped as coordinates in 3D space.\n\nThis concept is particularly useful for navigation. For example, the Mars Exploration Rover uses a similar method for scanning the terrain for obstacles. The rover captures a pair of images with its stereoscopic navigation cameras and disparity calculations are performed in order to detect elevated objects (such as boulders). Additionally, location and speed data can be extracted from subsequent stereo images by measuring the displacement of objects relative to the rover. In some cases, this is the best source of this type of information as the encoder sensors in the wheels may be inaccurate due to tire slippage.\n\nBinocular disparity forms the premise for a sketch from the film \"Wayne's World\" in which Wayne, who is lying in bed as Tia Carrere's character, Cassandra, perches above him, compares the respective images from his left and right eyes while noting which is which by saying \"Camera 1 ... Camera 2 ... Camera 1 ... Camera 2.\"\n\n", "id": "9170159", "title": "Binocular disparity"}
{"url": "https://en.wikipedia.org/wiki?curid=10999922", "text": "Mean shift\n\nMean shift is a non-parametric feature-space analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm. Application domains include cluster analysis in computer vision and image processing.\n\nThe mean shift procedure was originally presented in 1975 by Fukunaga and Hostetler.\n\nMean shift is a procedure for locating the maxima—the modes—of a density function given discrete data sampled from that function. This is an iterative method, and we start with an initial estimate formula_1. Let a kernel function formula_2 be given. This function determines the weight of nearby points for re-estimation of the mean. Typically a Gaussian kernel on the distance to the current estimate is used, formula_3. The weighted mean of the density in the window determined by formula_4 is\n\nwhere formula_6 is the neighborhood of formula_1, a set of points for which formula_8.\n\nThe difference formula_9 is called \"mean shift\" in Fukunaga and Hostetler. \nThe \"mean-shift algorithm\" now sets formula_10, and repeats the estimation until formula_11 converges.\n\nAlthough the mean shift algorithm has been widely used in many applications, a rigid proof for the convergence of the algorithm using a general kernel in a high dimensional space is still not known. Aliyari Ghassabeh showed the convergence of the mean shift algorithm in one-dimension with a differentiable, convex, and strictly decreasing profile function. However, the one-dimensional case has limited real world applications. Also, the convergence of the algorithm in higher dimensions with a finite number of the (or isolated) stationary points has been proved. However, sufficient conditions for a general kernel function to have finite (or isolated) stationary points have not been provided.\n\nLet data be a finite set formula_12 embedded in the n-dimensional Euclidean space, X. Let K be a flat kernel that is the characteristic function of the formula_13-ball in X,\nIn each iteration of the algorithm, formula_14 is performed for all formula_15 simultaneously. The first question, then, is how to estimate the density function given a sparse set of samples. One of the simplest approaches is to just smooth the data, e.g., by convolving it with a fixed kernel of width formula_16,\nwhere formula_17 are the input samples and formula_18 is the kernel function (or \"Parzen window\"). h is the only parameter in the algorithm and is called the bandwidth. This approach is known as \"kernel density estimation\" or the Parzen window technique. Once we have computed formula_19 from equation above, we can find its local maxima using gradient ascent or some other optimization technique. The problem with this \"brute force\" approach is that, for higher dimensions, it becomes computationally prohibitive to evaluate formula_19 over the complete search space. Instead, mean shift uses a variant of what is known in the optimization literature as \"multiple restart gradient descent\". Starting at some guess for a local maximum, formula_21, which can be a random input data point formula_22, mean shift computes the gradient of the density estimate formula_19 at formula_21 and takes an uphill step in that direction.\n\nKernel definition: Let X be the n-dimensional Euclidean space, formula_25. Denote the ith component of x by formula_26. The norm of x is a non-negative number.formula_27 A function K: formula_28 is said to be a kernel if there exists a \"profile\", formula_29 , such that\n\nformula_30\nand \n\nThe two frequently used kernel profiles for mean shift are:\n\n\nwhere the standard deviation parameter formula_34 works as the bandwidth parameter, formula_35.\n\nConsider a set of points in two-dimensional space. Assume a circular window centered at C and having radius r as the kernel. Mean shift is a hill climbing algorithm which involves shifting this kernel iteratively to a higher density region until convergence. Every shift is defined by a mean shift vector. The mean shift vector always points toward the direction of the maximum increase in the density. At every iteration the kernel is shifted to the centroid or the mean of the points within it. The method of calculating this mean depends on the choice of the kernel. In this case if a Gaussian kernel is chosen instead of a flat kernel, then every point will first be assigned a weight which will decay exponentially as the distance from the kernel's center increases. At convergence, there will be no direction at which a shift can accommodate more points inside the kernel.\n\nThe mean shift algorithm can be used for visual tracking. The simplest such algorithm would create a confidence map in the new image based on the color histogram of the object in the previous image, and use mean shift to find the peak of a confidence map near the object's old position. The confidence map is a probability density function on the new image, assigning each pixel of the new image a probability, which is the probability of the pixel color occurring in the object in the previous image. A few algorithms, such as kernel-based object tracking \nensemble tracking,\nCAMshift \nexpand on this idea.\n\nLet formula_17 and formula_37 be the d-dimensional input and filtered image pixels in the joint spatial-range domain. For each pixel,\n\n\n\n\n\n", "id": "10999922", "title": "Mean shift"}
{"url": "https://en.wikipedia.org/wiki?curid=33512861", "text": "Texton\n\nThe term texton was introduced by Bela Julesz in 1981 to describe \"the putative units of pre-attentive human texture perception.\"\nThe term reemerged in the late 1990s and early 2000s to describe vector quantized responses of a linear filter bank.\n", "id": "33512861", "title": "Texton"}
{"url": "https://en.wikipedia.org/wiki?curid=33512934", "text": "Cuboid (computer vision)\n\nIn computer vision, the term cuboid is used to describe a small spatiotemporal volume extracted for purposes of behavior recognition.\n", "id": "33512934", "title": "Cuboid (computer vision)"}
{"url": "https://en.wikipedia.org/wiki?curid=33870490", "text": "Image formation\n\nThe study of image formation encompasses the radiometric and geometric processes by which 2D images of 3D objects are formed. In the case of digital images, the image formation process also includes analog to digital conversion and sampling.\n\nImage Formation in Eye\n\nThe principal difference between the lens of the eye and an ordinary optical lens is that the former is flexible. The radius of the curvature of the anterior surface of the lens is greater than the radius of its posterior surface. The shape of the lens is controlled by tension in the fibers of the ciliary body. To focus on distant objects, the controlling muscles cause the lens to be relatively flattened. Similarly, these muscles allow the lens to become thicker in order to focus on objects near the eye.\n\nThe distance between the center of the lens and the retina (focal length) varies from approximately 17 mm to about 14 mm, as the refractive power of the lens increases from its minimum to its maximum. When the eye focuses on an object farther away than about 3 m, the lens exhibits its lowest refractivee power. When the eye focuses on a nearly object, the lens is most strongly refractive.\n", "id": "33870490", "title": "Image formation"}
{"url": "https://en.wikipedia.org/wiki?curid=33870526", "text": "Poisson image editing\n\nPoisson image editing refers to set of image processing routines that permit one to clone, adjust illumination, fill holes and tile texture patches without visible seams.\n", "id": "33870526", "title": "Poisson image editing"}
{"url": "https://en.wikipedia.org/wiki?curid=33270868", "text": "EigenMoments\n\nEigenMoments is a set of orthogonal, noise robust, invariant to rotation, scaling and translation and distribution sensitive moments. Their application can be found in signal processing and computer vision as descriptors of the signal or image. The descriptors can later be used for classification purposes.\n\nIt is obtained by performing orthogonalization, via eigen analysis on geometric moments.\n\nEigenMoments are computed by performing eigen analysis on the moment space of an image by maximizing Signal to Noise Ratio in the feature space in form of Rayleigh quotient.\n\nThis approach has several benefits in Image processing applications:\n\nAssume that a signal vector formula_1 is taken from a certain distribution having coorelation formula_2, i.e. formula_3 where E[.] denotes expected value.\n\nDimension of signal space, n, is often too large to be useful for practical application such as pattern classification, we need to transform the signal space into a space with lower dimensionality.\n\nThis is performed by a two-step linear transformation:\n\nformula_4\n\nwhere formula_5 is the transformed signal, formula_6 a fixed transformation matrix which transforms the signal into the moment space, and formula_7 the transformation matrix which we are going to determine by maximizing the SNR of the feature space resided by formula_8. For the case of Geometric Moments, X would be the monomials. If formula_9, a full rank transformation would result, however usually we have formula_10 and formula_11. This is specially the case when formula_12 is of high dimensions.\n\nFinding formula_13 that maximizes the SNR of the feature space:\n\nformula_14\n\nwhere N is the correlation matrix of the noise signal. The problem can thus be formulated as\n\nformula_15\n\nsubject to constraints:\n\nformula_16 where formula_17 is the Kronecker delta.\n\nIt can be observed that this maximization is Rayleigh quotient by letting formula_18 and formula_19 and therefore can be written as:\n\nformula_20, formula_21\n\nOptimization of Rayleigh quotient has the form:\n\nformula_22\n\nand formula_23 and formula_24, both are symmetric and formula_24 is positive definite and therefore invertible.\nScaling formula_26 does not change the value of the object function and hence and additional scalar constraint formula_27 can be imposed on formula_26 and no solution would be lost when the objective function is optimized.\n\nThis constraint optimization problem can be solved using Lagrangian multiplier:\n\nformula_29 subject to formula_30\n\nformula_31\n\nequating first derivative to zero and we will have:\n\nformula_32\n\nwhich is an instance of Generalized Eigenvalue Problem (GEP).\nThe GEP has the form:\n\nformula_33\n\nfor any pair formula_34 that is a solution to above equation, formula_26 is called a generalized eigenvector and formula_36 is called a generalized eigenvalue.\n\nFinding formula_26 and formula_38 that satisfies this equations would produce the result which optimizes Rayleigh quotient.\n\nOne way of maximizing Rayleigh quotient is through solving the Generalized Eigen Problem. Dimension reduction can be performed by simply choosing the first components formula_39, formula_40, with the highest values for formula_41 out of the formula_42 components, and discard the rest. Interpretation of this transformation is rotating and scaling the moment space, transforming it into a feature space with maximized SNR and therefore, the first formula_43 components are the components with highest formula_43 SNR values.\n\nThe other method to look at this solution is to use the concept of simultaneous diagonalization instead of Generalized Eigen Problem.\n\n\nformula_48\n\n\nformula_50.\n\nWhere formula_51 is a diagonal matrix sorted in increasing order. Since formula_24 is positive definite, thus formula_53. We can discard those eigenvalues that large and retain those close to 0, since this means the energy of the noise is close to 0 in this space, at this stage it is also possible to discard those eigenvectors that have large eigenvalues.\n\nLet formula_54 be the first formula_43 columns of formula_56, now formula_57 where formula_58 is the formula_59 principal submatrix of formula_51.\n\n\nformula_61\n\nand hence:\n\nformula_62.\n\nformula_49 whiten formula_24 and reduces the dimensionality from formula_42 to formula_43. The transformed space resided by formula_67 is called the noise space.\n\n\nformula_69,\n\nwhere formula_70. formula_71 is the matrix with eigenvalues of formula_68 on its diagonal. We may retain all the eigenvalues and their corresponding eigenvectors since the most of the noise are already discarded in previous step.\n\n\nformula_73\n\nwhere formula_13 diagonalizes both the numerator and denominator of the SNR,\n\nformula_75, formula_76 and the transformation of signal formula_77 is defined as formula_78.\n\nTo find the information loss when we discard some of the eigenvalues and eigenvectors we can perform following analysis:\n\nformula_79\n\nEigenmoments are derived by applying the above framework on Geometric Moments. They can be derived for both 1D and 2D signals.\n\nIf we let formula_80, i.e. the monomials, after the transformation formula_81 we obtain Geometric Moments, denoted by vector formula_82, of signal formula_83,i.e. formula_84.\n\nIn practice it is difficult to estimate the correlation signal due to insufficient number of samples, therefore parametric approaches are utilized.\n\nOne such model can be defined as:\n\nformula_85,\n\nwhere formula_86. This model of correlation can be replaced by other models however this model covers general natural images.\n\nSince formula_87 does not affect the maximization it can be dropped.\n\nformula_88\n\nThe correlation of noise can be modelled as formula_89, where formula_90 is the energy of noise.Again formula_90 can be dropped because the constant does not have any effect on the maximization problem.\n\nformula_92\nformula_93\n\nUsing the computed A and B and applying the algorithm discussed in previous section we find formula_13 and set of transformed monomials formula_95 which produces the moment kernels of EM. The moment kernels of EM decorrelate the correlation in the image.\n\nformula_96,\n\nand are orthogonal:\n\nformula_97\n\nTaking formula_98, the dimension of moment space as formula_99 and the dimension of feature space as formula_100, we will have:\n\nformula_101\n\nand\n\nformula_102\n\nThe derivation for 2D signal is the same as 1D signal except that conventional Geometric Moments are directly employed to obtain the set of 2D EigenMoments.\n\nThe definition of Geometric Moments of order formula_103 for 2D image signal is:\n\nformula_104.\n\nwhich can be denoted as formula_105. Then the set of 2D EigenMoments are:\n\nformula_106,\n\nwhere formula_107 is a matrix that contains the set of EigenMoments.\n\nformula_108.\n\nIn order to obtain a set of moment invariants we can use normalized Geometric Moments formula_109 instead of formula_82.\n\nNormalized Geometric Moments are invariant to Rotation, Scaling and Transformation and defined by:\n\nformula_111\n\nwhere:formula_112 is the centroid of the image formula_113 and\n\nformula_114.\n\nformula_115 in this equation is a scaling factor depending on the image. formula_115 is usually set to 1 for binary images.\n\n\n", "id": "33270868", "title": "EigenMoments"}
{"url": "https://en.wikipedia.org/wiki?curid=1703661", "text": "Scale space\n\nScale-space theory is a framework for multi-scale signal representation developed by the computer vision, image processing and signal processing communities with complementary motivations from physics and biological vision. It is a formal theory for handling image structures at different scales, by representing an image as a one-parameter family of smoothed images, the scale-space representation, parametrized by the size of the smoothing kernel used for suppressing fine-scale structures. The parameter formula_1 in this family is referred to as the \"scale parameter\", with the interpretation that image structures of spatial size smaller than about formula_2 have largely been smoothed away in the scale-space level at scale formula_1.\n\nThe main type of scale space is the \"linear (Gaussian) scale space\", which has wide applicability as well as the attractive property of being possible to derive from a small set of \"scale-space axioms\". The corresponding scale-space framework encompasses a theory for Gaussian derivative operators, which can be used as a basis for expressing a large class of visual operations for computerized systems that process visual information. This framework also allows visual operations to be made \"scale invariant\", which is necessary for dealing with the size variations that may occur in image data, because real-world objects may be of different sizes and in addition the distance between the object and the camera may be unknown and may vary depending on the circumstances.\n\nThe notion of scale space applies to signals of arbitrary numbers of variables. The most common case in the literature applies to two-dimensional images, which is what is presented here. For a given image formula_4, its linear (Gaussian) \"scale-space representation\" is a family of derived signals formula_5 defined by the convolution of formula_4 with the two-dimensional Gaussian kernel\n\nsuch that\n\nwhere the semicolon in the argument of formula_9 implies that the convolution is performed only over the variables formula_10, while the scale parameter formula_1 after the semicolon just indicates which scale level is being defined. This definition of formula_9 works for a continuum of scales formula_13, but typically only a finite discrete set of levels in the scale-space representation would be actually considered.\n\nThe scale parameter formula_14 is the variance of the Gaussian filter and as a limit for formula_15 the filter formula_16 becomes an impulse function such that formula_17 that is, the scale-space representation at scale level formula_18 is the image formula_19 itself. As formula_20 increases, formula_9 is the result of smoothing formula_19 with a larger and larger filter, thereby removing more and more of the details which the image contains. Since the standard deviation of the filter is formula_23, details which are significantly smaller than this value are to a large extent removed from the image at scale parameter formula_20, see the following figure and for graphical illustrations.\n\nWhen faced with the task of generating a multi-scale representation one may ask: could any filter \"g\" of low-pass type and with a parameter \"t\" which determines its width be used to generate a scale space? The answer is no, as it is of crucial importance that the smoothing filter does not introduce new spurious structures at coarse scales that do not correspond to simplifications of corresponding structures at finer scales. In the scale-space literature, a number of different ways have been expressed to formulate this criterion in precise mathematical terms.\n\nThe conclusion from several different axiomatic derivations that have been presented is that the Gaussian scale space constitutes the \"canonical\" way to generate a linear scale space, based on the essential requirement that new structures must not be created when going from a fine scale to any coarser scale.\nConditions, referred to as \"scale-space axioms\", that have been used for deriving the uniqueness of the Gaussian kernel include linearity, shift invariance, semi-group structure, non-enhancement of local extrema, scale invariance and rotational invariance.\nIn the works, the uniqueness claimed in the arguments based on scale invariance originally due to Iijima (1962) has been criticized, and alternative self-similar scale-space kernels have been proposed. The Gaussian kernel is, however, a unique choice according to the scale-space axiomatics based on causality or non-enhancement of local extrema.\n\n\"Equivalently\", the scale-space family can be defined as the solution of the diffusion equation (for example in terms of the heat equation),\n\nwith initial condition formula_26. This formulation of the scale-space representation \"L\" means that it is possible to interpret the intensity values of the image \"f\" as a \"temperature distribution\" in the image plane and that the process which generates the scale-space representation as a function of \"t\" corresponds to heat diffusion in the image plane over time \"t\" (assuming the thermal conductivity of the material equal to the arbitrarily chosen constant ½). Although this connection may appear superficial for a reader not familiar with differential equations, it is indeed the case that the main scale-space formulation in terms of non-enhancement of local extrema is expressed in terms of a sign condition on partial derivatives in the 2+1-D volume generated by the scale space, thus within the framework of partial differential equations. Furthermore, a detailed analysis of the discrete case shows that the diffusion equation provides a unifying link between continuous and discrete scale spaces, which also generalizes to nonlinear scale spaces, for example, using anisotropic diffusion. Hence, one may say that the primary way to generate a scale space is by the diffusion equation, and that the Gaussian kernel arises as the Green's function of this specific partial differential equation.\n\nThe motivation for generating a scale-space representation of a given data set originates from the basic observation that real-world objects are composed of different structures at different scales. This implies that real-world objects, in contrast to idealized mathematical entities such as points or lines, may appear in different ways depending on the scale of observation.\nFor example, the concept of a \"tree\" is appropriate at the scale of meters, while concepts such as leaves and molecules are more appropriate at finer scales.\nFor a computer vision system analysing an unknown scene, there is no way to know a priori what scales are appropriate for describing the interesting structures in the image data.\nHence, the only reasonable approach is to consider descriptions at multiple scales in order to be able to capture the unknown scale variations that may occur.\nTaken to the limit, a scale-space representation considers representations at all scales.\n\nAnother motivation to the scale-space concept originates from the process of performing a physical measurement on real-world data. In order to extract any information from a measurement process, one has to apply \"operators of non-infinitesimal size\" to the data. In many branches of computer science and applied mathematics, the size of the measurement operator is disregarded in the theoretical modelling of a problem. The scale-space theory on the other hand explicitly incorporates the need for a non-infinitesimal size of the image operators as an integral part of any measurement as well as any other operation that depends on a real-world measurement.\n\nThere is a close link between scale-space theory and biological vision. Many scale-space operations show a high degree of similarity with receptive field profiles recorded from the mammalian retina and the first stages in the visual cortex.\nIn these respects, the scale-space framework can be seen as a theoretically well-founded paradigm for early vision, which in addition has been thoroughly tested by algorithms and experiments.\n\nAt any scale in scale space, we can apply local derivative operators to the scale-space representation:\n\nDue to the commutative property between the derivative operator and the Gaussian smoothing operator, such \"scale-space derivatives\" can equivalently be computed by convolving the original image with Gaussian derivative operators. For this reason they are often also referred to as \"Gaussian derivatives\":\n\nInterestingly, the uniqueness of the Gaussian derivative operators as local operations derived from a scale-space representation can be obtained by similar axiomatic derivations as are used for deriving the uniqueness of the Gaussian kernel for scale-space smoothing.\n\nThese Gaussian derivative operators can in turn be combined by linear or non-linear operators into a larger variety of different types of feature detectors, which in many cases can be well modelled by differential geometry. Specifically, invariance (or more appropriately \"covariance\") to local geometric transformations, such as rotations or local affine transformations, can be obtained by considering differential invariants under the appropriate class of transformations or alternatively by normalizing the Gaussian derivative operators to a locally determined coordinate frame determined from e.g. a preferred orientation in the image domain or by applying a preferred local affine transformation to a local image patch (see the article on affine shape adaptation for further details).\n\nWhen Gaussian derivative operators and differential invariants are used in this way as basic feature detectors at multiple scales, the uncommitted first stages of visual processing are often referred to as a \"visual front-end\". This overall framework has been applied to a large variety of problems in computer vision, including feature detection, feature classification, image segmentation, image matching, motion estimation, computation of shape cues and object recognition. The set of Gaussian derivative operators up to a certain order is often referred to as the \"N-jet\" and constitutes a basic type of feature within the scale-space framework.\n\nFollowing the idea of expressing visual operation in terms of differential invariants computed at multiple scales using Gaussian derivative operators, we can express an edge detector from the set of points that satisfy the requirement that the gradient magnitude\nshould assume a local maximum in the gradient direction\nBy working out the differential geometry, it can be shown that this differential edge detector can equivalently be expressed from the zero-crossings of the second-order differential invariant\n\nthat satisfy the following sign condition on a third-order differential invariant:\n\nSimilarly, multi-scale blob detectors at any given fixed scale can be obtained from local maxima and local minima of either the Laplacian operator (also referred to as the Laplacian of Gaussian)\n\nor the determinant of the Hessian matrix\nIn an analogous fashion, corner detectors and ridge and valley detectors can be expressed as local maxima, minima or zero-crossings of multi-scale differential invariants defined from Gaussian derivatives. The algebraic expressions for the corner and ridge detection operators are, however, somewhat more complex and the reader is referred to the articles on corner detection and ridge detection for further details.\n\nScale-space operations have also been frequently used for expressing coarse-to-fine methods, in particular for tasks such as image matching and for multi-scale image segmentation.\n\nThe theory presented so far describes a well-founded framework for \"representing\" image structures at multiple scales. In many cases it is, however, also necessary to select locally appropriate scales for further analysis. This need for \"scale selection\" originates from two major reasons; (i) real-world objects may have different size, and this size may be unknown to the vision system, and (ii) the distance between the object and the camera can vary, and this distance information may also be unknown \"a priori\".\nA highly useful property of scale-space representation is that image representations can be made invariant to scales, by performing automatic local scale selection based on local maxima (or minima) over scales of normalized derivatives\nwhere formula_36 is a parameter that is related to the dimensionality of the image feature. This algebraic expression for \"scale normalized Gaussian derivative operators\" originates from the introduction of \"formula_37-normalized derivatives\" according to\nIt can be theoretically shown that a scale selection module working according to this principle will satisfy the following \"scale invariance property\": if for a certain type of image feature a local maximum is assumed in a certain image at a certain scale formula_40, then under a rescaling of the image by a scale factor formula_41 the local maximum over scales in the rescaled image will be transformed to the scale level formula_42.\n\nFollowing this approach of gamma-normalized derivatives, it can be shown that different types of \"scale adaptive and scale invariant feature detectors\" can be expressed for tasks such as blob detection, corner detection, ridge detection, edge detection and spatio-temporal interest points (see the specific articles on these topics for in-depth descriptions of how these scale-invariant feature detectors are formulated).\nFurthermore, the scale levels obtained from automatic scale selection can be used for determining regions of interest for subsequent affine shape adaptation to obtain affine invariant interest points or for determining scale levels for computing associated image descriptors, such as locally scale adapted N-jets.\n\nRecent work has shown that also more complex operations, such as scale-invariant object recognition can be performed in this way,\nby computing local image descriptors (N-jets or local histograms of gradient directions) at scale-adapted interest points obtained from scale-space extrema of the normalized Laplacian operator (see also scale-invariant feature transform) or the determinant of the Hessian (see also SURF); see also the Scholarpedia article on the scale-invariant feature transform for a more general outlook of object recognition approaches based on receptive field responses in terms Gaussian derivative operators or approximations thereof.\n\nAn image pyramid is a discrete representation in which a scale space is sampled in both space and scale. For scale invariance, the scale factors should be sampled exponentially, for example as integer powers of 2 or root 2. When properly constructed, the ratio of the sample rates in space and scale are held constant so that the impulse response is identical in all levels of the pyramid. \nFast, O(N), algorithms exist for computing a scale invariant image pyramid in which the image or signal is repeatedly smoothed then subsampled. \nValues for scale space between pyramid samples can easily be estimated using interpolation within and between scales and allowing for scale and position estimates with sub resolution accuracy.\n\nIn a scale-space representation, the existence of a continuous scale parameter makes it possible to track zero crossings over scales leading to so-called \"deep structure\".\nFor features defined as zero-crossings of differential invariants, the implicit function theorem directly defines trajectories across scales, and at those scales where bifurcations occur, the local behaviour can be modelled by singularity theory.\n\nExtensions of linear scale-space theory concern the formulation of non-linear scale-space concepts more committed to specific purposes. These \"non-linear scale-spaces\" often start from the equivalent diffusion formulation of the scale-space concept, which is subsequently extended in a non-linear fashion. A large number of evolution equations have been formulated in this way, motivated by different specific requirements (see the abovementioned book references for further information). It should be noted, however, that not all of these non-linear scale-spaces satisfy similar \"nice\" theoretical requirements as the linear Gaussian scale-space concept. Hence, unexpected artifacts may sometimes occur and one should be very careful of not using the term \"scale-space\" for just any type of one-parameter family of images.\n\nA first-order extension of the isotropic Gaussian scale space is provided by the \"affine (Gaussian) scale space\". One motivation for this extension originates from the common need for computing image descriptors subject for real-world objects that are viewed under a perspective camera model. To handle such non-linear deformations locally, partial invariance (or more correctly covariance) to local affine deformations can be achieved by considering affine Gaussian kernels with their shapes determined by the local image structure, see the article on affine shape adaptation for theory and algorithms. Indeed, this affine scale space can also be expressed from a non-isotropic extension of the linear (isotropic) diffusion equation, while still being within the class of linear partial differential equations.\n\nThere exists a more general extension of the Gaussian scale-space model to affine and spatio-temporal scale-spaces. In addition to variabilities over scale, which original scale-space theory was designed to handle, this \"generalized scale-space theory\" also comprises other types of variabilities caused by geometric transformations in the image formation process, including variations in viewing direction approximated by local affine transformations, and relative motions between objects in the world and the observer, approximated by local Galilean transformations. This generalized scale-space theory leads to predictions about receptive field profiles in good qualitative agreement with receptive field profiles measured by cell recordings in biological vision.\n\nThere are strong relations between scale-space theory and wavelet theory, although these two notions of multi-scale representation have been developed from somewhat different premises.\nThere has also been work on other multi-scale approaches, such as pyramids and a variety of other kernels, that do not exploit or require the same requirements as true scale-space descriptions do.\n\nThere are interesting relations between scale-space representation and biological vision and hearing.\nNeurophysiological studies of biological vision have shown that there are receptive field profiles in the mammalian retina and visual cortex,\nwhich can be well modelled by linear Gaussian derivative operators, in some cases also complemented by a non-isotropic affine scale-space model, a spatio-temporal scale-space model and/or non-linear combinations of such linear operators.\nRegarding biological hearing there are receptive field profiles in the inferior colliculus and the primary auditory cortex that can be well modelled by spectra-temporal receptive fields that can be well modelled by Gaussian derivates over logarithmic frequencies and windowed Fourier transforms over time with the window functions being temporal scale-space kernels.\n\nNormative theories for visual and auditory receptive fields founded on the scale-space framework are described in the article on axiomatic theory of receptive fields.\n\nWhen implementing scale-space smoothing in practice there are a number of different approaches that can be taken in terms of continuous or discrete Gaussian smoothing, implementation in the Fourier domain, in terms of pyramids based on binomial filters that approximate the Gaussian or using recursive filters. More details about this are given in a separate article on scale space implementation.\n\nComplementary articles on specific subtopics of scale space:\n\nMulti-scale feature detection within the scale-space framework:\n\nComputational modelling of biological receptive fields.\n\nThe Gaussian function and other smoothing or multi-scale approaches:\n\nMore general articles on feature detection, computer vision and image processing:\n\n\n", "id": "1703661", "title": "Scale space"}
{"url": "https://en.wikipedia.org/wiki?curid=37412518", "text": "Local ternary patterns\n\nLocal ternary patterns (LTP) are an extension of Local binary patterns (LBP). Unlike LBP, it does not threshold the pixels into 0 and 1, rather it uses a threshold constant to threshold pixels into three values. Considering k as the threshold constant, c as the value of the center pixel, a neighboring pixel p, the result of threshold is:\nformula_1\n\nIn this way, each thresholded pixel has one of the three values. Neighboring pixels are combined after thresholding into a ternary pattern. Computing a histogram of these ternary values will result in a large range, so the ternary pattern is split into two binary patterns. Histograms are concatenated to generate a descriptor double the size of LBP.\n\n", "id": "37412518", "title": "Local ternary patterns"}
{"url": "https://en.wikipedia.org/wiki?curid=16373249", "text": "Structured-light 3D scanner\n\nA structured-light 3D scanner is a 3D scanning device for measuring the three-dimensional shape of an object using projected light patterns and a camera system.\n\nProjecting a narrow band of light onto a three-dimensionally shaped surface produces a line of illumination that appears distorted from other perspectives than that of the projector, and can be used for an exact geometric reconstruction of the surface shape (light section).\n\nA faster and more versatile method is the projection of patterns consisting of many stripes at once, or of arbitrary fringes, as this allows for the acquisition of a multitude of samples simultaneously.\nSeen from different viewpoints, the pattern appears geometrically distorted due to the surface shape of the object.\n\nAlthough many other variants of structured light projection are possible, patterns of parallel stripes are widely used. The picture shows the geometrical deformation of a single stripe projected onto a simple 3D surface. The displacement of the stripes allows for an exact retrieval of the 3D coordinates of any details on the object's surface.\n\nTwo major methods of stripe pattern generation have been established: Laser interference and projection.\n\nThe laser \"interference method\" works with two wide planar laser beam fronts. Their interference results in regular, equidistant line patterns. Different pattern sizes can be obtained by changing the angle between these beams. The method allows for the exact and easy generation of very fine patterns with unlimited depth of field. Disadvantages are high cost of implementation, difficulties providing the ideal beam geometry, and laser typical effects like speckle noise and the possible self interference with beam parts reflected from objects. Typically, there is no means of modulating individual stripes, such as with Gray codes.\n\nThe \"projection method\" uses incoherent light and basically works like a video projector. Patterns are usually generated by passing light through a digital spatial light modulator, typically based on one of the three currently most widespread digital projection technologies, transmissive liquid crystal, reflective liquid crystal on silicon (LCOS) or digital light processing (DLP; moving micro mirror) modulators, which have various comparative advantages and disadvantages for this application. Other methods of projection could be and have been used, however.\n\nPatterns generated by digital display projectors have small discontinuities due to the pixel boundaries in the displays. Sufficiently small boundaries however can practically be neglected as they are evened out by the slightest defocus.\n\nA typical measuring assembly consists of one projector and at least one camera. For many applications, two cameras on opposite sides of the projector have been established as useful.\n\n\"Invisible\" (or \"imperceptible\") structured light uses structured light without interfering with other computer vision tasks for which the projected pattern will be confusing. Example methods include the use of infrared light or of extremely high framerates alternating between two exact opposite patterns.\n\nGeometric distortions by optics and perspective must be compensated by a calibration of the measuring equipment, using special calibration patterns and surfaces. A mathematical model is used for describing the imaging properties of projector and cameras. Essentially based on the simple geometric properties of a pinhole camera, the model also has to take into account the geometric distortions and optical aberration of projector and camera lenses. The parameters of the camera as well as its orientation in space can be determined by a series of calibration measurements, using photogrammetric bundle adjustment.\n\nThere are several depth cues contained in the observed stripe patterns. The displacement of any single stripe can directly be converted into 3D coordinates. For this purpose, the individual stripe has to be identified, which can for example be accomplished by tracing or counting stripes (pattern recognition method). Another common method projects alternating stripe patterns, resulting in binary Gray code sequences identifying the number of each individual stripe hitting the object.\nAn important depth cue also results from the varying stripe widths along the object surface. Stripe width is a function of the steepness of a surface part, i.e. the first derivative of the elevation. Stripe frequency and phase deliver similar cues and can be analyzed by a Fourier transform. Finally, the wavelet transform has recently been discussed for the same purpose.\n\nIn many practical implementations, series of measurements combining pattern recognition, Gray codes and Fourier transform are obtained for a complete and unambiguous reconstruction of shapes.\n\nAnother method also belonging to the area of fringe projection has been demonstrated, utilizing the depth of field of the camera.\n\nIt is also possible to use projected patterns primarily as a means of structure insertion into scenes, for an essentially photogrammetric acquisition.\n\nThe optical resolution of fringe projection methods depends on the width of the stripes used and their optical quality. It is also limited by the wavelength of light.\n\nAn extreme reduction of stripe width proves inefficient due to limitations in depth of field, camera resolution and display resolution. Therefore, the phase shift method has been widely established: A number of at least 3, typically about 10 exposures are taken with slightly shifted stripes. The first theoretical deductions of this method relied on stripes with a sine wave shaped intensity modulation, but the methods work with \"rectangular\" modulated stripes, as delivered from LCD or DLP displays as well. By phase shifting, surface detail of e.g. 1/10 the stripe pitch can be resolved.\n\nCurrent optical stripe pattern profilometry hence allows for detail resolutions down to the wavelength of light, below 1 micrometer in practice or, with larger stripe patterns, to approx. 1/10 of the stripe width. Concerning level accuracy, interpolating over several pixels of the acquired camera image can yield a reliable height resolution and also accuracy, down to 1/50 pixel.\n\nArbitrarily large objects can be measured with accordingly large stripe patterns and setups. Practical applications are documented involving objects several meters in size.\n\nTypical accuracy figures are:\n\n\nAs the method can measure shapes from only one perspective at a time, complete 3D shapes have to be combined from different measurements in different angles. This can be accomplished by attaching marker points to the object and combining perspectives afterwards by matching these markers. The process can be automated, by mounting the object on a motorized turntable or CNC positioning device. Markers can as well be applied on a positioning device instead of the object itself.\n\nThe 3D data gathered can be used to retrieve CAD (computer aided design) data and models from existing components (reverse engineering), hand formed samples or sculptures, natural objects or artifacts.\n\nAs with all optical methods, reflective or transparent surfaces raise difficulties. Reflections cause light to be reflected either away from the camera or right into its optics. In both cases, the dynamic range of the camera can be exceeded. Transparent or semi-transparent surfaces also cause major difficulties. In these cases, coating the surfaces with a thin opaque lacquer just for measuring purposes is a common practice. A recent method handles highly reflective and specular objects by inserting a 1-dimensional diffuser between the light source (e.g., projector) and the object to be scanned. Alternative optical techniques have been proposed for handling perfectly transparent and specular objects.\n\nDouble reflections and inter-reflections can cause the stripe pattern to be overlaid with unwanted light, entirely eliminating the chance for proper detection. Reflective cavities and concave objects are therefore difficult to handle. It is also hard to handle translucent materials, such as skin, marble, wax, plants and human tissue because of the phenomenon of sub-surface scattering. Recently, there has been an effort in the computer vision community to handle such optically complex scenes by re-designing the illumination patterns. These methods have shown promising 3D scanning results for traditionally difficult objects, such as highly specular metal concavities and translucent wax candles.\n\nAlthough several patterns have to be taken per picture in most structured light variants, high-speed implementations are available for a number of applications, for example:\nMotion picture applications have been proposed, for example the acquisition of spatial scene data for three-dimensional television.\n\n\n3DUNDERWORLD SLS - OPEN SOURCE \n\nDIY 3D scanner based on structured light and stereo vision in Python language \n\nSLStudio -- Open Source Real Time Structured Light \n\n\n", "id": "16373249", "title": "Structured-light 3D scanner"}
{"url": "https://en.wikipedia.org/wiki?curid=39799223", "text": "PatchMatch\n\nThe core PatchMatch algorithm quickly finds correspondences between small square regions (or patches) of an image. The algorithm can be used in various applications such as object removal from images, reshuffling or moving contents of images, or retargeting or changing aspect ratios of images, or optical flow estimation, or stereo correspondence.\n\nThe goal of the algorithm is to find the patch correspondence by defining a nearest-neighbor field (NNF) as a function formula_1 of offsets, which is over all possible matches of patch (location of patch centers) in image A, for some distance function of two patches formula_2. So, for a given patch coordinate formula_3 in image formula_4 and its corresponding nearest neighbor formula_5 in image formula_6, formula_7 is simply formula_8. However, if we search for every point in image formula_6, the work will be too hard to complete. So the following algorithm is done in a randomized approach in order to accelerate the calculation speed. \nThe algorithm has three main components. Initially, the nearest-neighbor field is filled with either random offsets or some prior information. Next, an iterative update process is applied to the NNF, in which good patch offsets are propagated to adjacent pixels, followed by random search in the neighborhood of the best offset found so far. Independent of these three components, the algorithm also use a coarse-to-fine approach by building an image pyramid to obtain the better result.\n\nWhen initializing with random offsets, we use independent uniform samples across the full range of image formula_6. This algorithm avoids using an initial guess from the previous level of the pyramid because in this way the algorithm can avoid being trapped in local minima.\n\nAfter initialization, the algorithm attempted to perform iterative process of improving the formula_11. The iterations examine the offsets in scan order (from left to right, top to bottom), and each undergoes propagation followed by random search.\n\nWe attempt to improve formula_12 using the known offsets of formula_13 and formula_14, assuming that the patch offsets\nare likely to be the same. That is, the algorithm will take new value for formula_12 to be formula_16. \nSo if formula_17 has a correct mapping and is in a coherent region formula_18, then all of formula_18 below and to the right of formula_17 will be filled with the correct mapping. Alternatively, on even iterations, the algorithm search for different direction, fill the new value to be formula_21.\n\nLet formula_22, we attempt to improveformula_17 by testing a sequence of candidate offsets at an exponentially decreasing distance from formula_24\n\nwhere formula_26 is a uniform random in formula_27, formula_28 is a large window search radius which will be set to maximum picture size, and formula_29 is a fixed ratio often assigned as 1/2. This part of the algorithm allows the formula_12 to jump out of local minimum through random process.\n\nThe often used halting criteria is set the iteration times to be about 4~5. Even with low iteration, the algorithm works well.\n\nThis is an efficient algorithm since it only takes a few second on a testing computer with Intel Core i5 CPU and Photoshop CS5.\n\n\n", "id": "39799223", "title": "PatchMatch"}
{"url": "https://en.wikipedia.org/wiki?curid=39922027", "text": "Cyclodisparity\n\nCyclodisparity refers to the difference in the rotation angle of an object or scene viewed by the left and right eyes. Cyclodisparity can result from the eyes' torsional rotation (\"cyclorotation\") or can be created artificially by presenting to the eyes two images that need to be rotated relative to each other for binocular fusion to take place.\n\nThe eyes and visual system can compensate for cyclodisparity up to a certain point; if the cyclodisparity is larger than a threshold, the images cannot be fused, resulting stereoblindness, and in double vision in subjects who otherwise have full stereo vision.\n\nWhen a human subject is presented with images that have artificial cyclodisparity, cyclovergence is evoked, that is, a motor response of the eye muscles that rotates the two eyes in opposite directions, thereby reducing cyclodisparity. Visually-induced cyclovergence of up to 8 degrees has been observed in normal subjects. Furthermore, up to about 8 degrees that can usually be compensated by purely sensory means, that is, without physical eye rotation. This means that the normal human observer can achieve binocular image fusion in presence of cyclodisparity of up to approximately 16 degrees.\n\nCyclodisparity due to images having been rotated inward can be compensated better when the gaze is directed downwards, and cyclodisparity due to an outward rotation can be compensated better when the gaze is directed upwards. A proposed explanation for this phenomenon is that the motor system is coordinated in such a way that the eyes perform a torsional movement to reduce the size of the search zones and thus the computational load required for solving the correspondence problem. The resulting cyclovergence at near gaze is smaller than the cyclovergence predicted by Listing's law.\n\nActive camera torsion can be used in machine and computer vision for several purposes. For instance, camera torsion can be used to make improved use of the search range over which matching detectors or stereo matching algorithms operate, or to make a 3D slanted surface appear frontoparallel for further stereo processing.\n\nFor image compression purposes, images with cyclodisparity are advantageously encoded using global motion compensation using a rotational motion model.\n", "id": "39922027", "title": "Cyclodisparity"}
{"url": "https://en.wikipedia.org/wiki?curid=5481404", "text": "Scale-space axioms\n\nIn image processing and computer vision, a scale space framework can be used to represent an image as a family of gradually smoothed images. This framework is very general and a variety of scale space representations exist. A typical approach for choosing a particular type of scale space representation is to establish a set of scale-space axioms, describing basic properties of the desired scale-space representation and often chosen so as to make the representation useful in practical applications. Once established, the axioms narrow the possible scale-space representations to a smaller class, typically with only a few free parameters.\n\nA set of standard scale space axioms, discussed below, leads to the linear Gaussian scale-space, which is the most common type of scale space used in image processing and computer vision.\n\nThe linear scale space representation formula_1 of signal formula_2 obtained by smoothing with the Gaussian kernel formula_3 satisfies a number of properties 'scale-space axioms' that make it a special form of multi-scale representation:\n\nwhere formula_5 and formula_6 are signals while formula_7 and formula_8 are constants,\nwhere formula_10 denotes the shift (translation) operator formula_11\nwith the associated \"cascade smoothing property\"\nfor some functions formula_21 and formula_22 where formula_23 denotes the Fourier transform of formula_24,\n\nIn fact, it can be shown that the Gaussian kernel is a \"unique choice\" given several different combinations of subsets of these scale-space axioms:\nmost of the axioms (linearity, shift-invariance, semigroup) correspond to scaling being a semigroup of shift-invariant linear operator, which is satisfied by a number of families integral transforms, while \"non-creation of local extrema\" for one-dimensional signals or \"non-enhancement of local extrema\" for higher-dimensional signals are the crucial axioms which relate scale-spaces to smoothing (formally, parabolic partial differential equations), and hence select for the Gaussian.\n\nThe Gaussian kernel is also separable in Cartesian coordinates, i.e. formula_27. Separability is, however, not counted as a scale-space axiom, since it is a coordinate dependent property related to issues of implementation. In addition, the requirement of separability in combination with rotational symmetry per se fixates the smoothing kernel to be a Gaussian.\n\nThere exists a generalization of the Gaussian scale-space theory to more general affine and spatio-temporal scale-spaces. In addition to variabilities over scale, which original scale-space theory was designed to handle, this \"generalized scale-space theory\" also comprises other types of variabilities, including image deformations caused by viewing variations, approximated by local affine transformations, and relative motions between objects in the world and the observer, approximated by local Galilean transformations. In this theory, rotational symmetry is not imposed as a necessary scale-space axiom and is instead replaced by requirements of affine and/or Galilean covariance. The generalized scale-space theory leads to predictions about receptive field profiles in good qualitative agreement with receptive field profiles measured by cell recordings in biological vision.\n\nIn the computer vision, image processing and signal processing literature there are many other multi-scale approaches, using wavelets and a variety of other kernels, that do not exploit or require the same requirements as scale space descriptions do; please see the article on related multi-scale approaches. There has also been work on discrete scale-space concepts that carry the scale-space properties over to the discrete domain; see the article on scale space implementation for examples and references.\n\n", "id": "5481404", "title": "Scale-space axioms"}
{"url": "https://en.wikipedia.org/wiki?curid=40374554", "text": "Point set registration\n\nIn computer vision and pattern recognition, point set registration, also known as point matching, is the process of finding a spatial transformation that aligns two point sets. The purpose of finding such a transformation includes merging multiple data sets into a globally consistent model, and mapping a new measurement to a known data set to identify features or to estimate its pose. A point set may be raw data from 3D scanning or an array of rangefinders. For use in image processing and feature-based image registration, a point set may be a set of features obtained by feature extraction from an image, for example corner detection. Point set registration is used in optical character recognition, augmented reality\nand aligning data from magnetic resonance imaging with computer aided tomography scans.\n\nThe problem may be summarized as follows:\nLet formula_1 be two finite size point sets in a finite-dimensional real vector space formula_2, which contain formula_3 and formula_4 points respectively. The problem is to find a transformation to be applied to the moving \"model\" point set formula_5 such that the difference between formula_5 and the static \"scene\" set formula_7 is minimized. In other words, a mapping from formula_2 to formula_2 is desired which yields the best alignment between the transformed \"model\" set and the \"scene\" set. The mapping may consist of a rigid or non-rigid transformation. The transformation model may be written as formula_10 where the transformed, registered model point set is:\n\nIt is useful to define an optimization parameter formula_11:\n\nsuch that it is clear that the optimizing algorithm adjusts formula_11. Depending on the problem and number of dimensions, there may be more such parameters. The output of a point set registration algorithm is therefore the transformation parameter formula_11 of model formula_10 so that formula_5 is optimally aligned to formula_7.\n\nIn convergence, it is desired for the distance between the two point sets to reach a global minimum. This is difficult without exhausting all possible transformations, so a local minimum suffices. The distance function between a transformed model point set formula_17 and the scene point set formula_7 is given by some function formula_19. A simple approach is to take the square of the Euclidean distance for every pair of points:\n\nMinimizing such a function in rigid registration is equivalent to solving a least squares problem. However, this function is sensitive to outlier data and consequently algorithms based on this function tend to be less robust against noisy data. A more robust formulation of the cost function uses some \"robust function\" formula_20:\n\nSuch a formulation is known as an \"M-estimator\". The robust function formula_20 is chosen such that the local configuration of the point set is insensitive to distant points, hence making it robust against outliers and noise.\n\nGiven two point sets, rigid registration yields a rigid transformation which maps one point set to the other. A rigid transformation is defined as a transformation that does not change the distance between any two points. Typically such a transformation consists of translation and rotation. In rare cases, the point set may also be mirrored.\n\nGiven two point sets, non-rigid registration yields a non-rigid transformation which maps one point set to the other. Non-rigid transformations include affine transformations such as scaling and shear mapping. However, in the context of point set registration, non-rigid registration typically involves nonlinear transformation. If the eigenmodes of variation of the point set are known, the nonlinear transformation may be parametrized by the eigenvalues. A nonlinear transformation may also be parametrized as a thin plate spline.\n\nSome approaches to point set registration use algorithms that solve the more general graph matching problem. However, the computational complexity of such methods tend to be high and they are limited to rigid registrations. Algorithms specific to the point set registration problem are described in the following sections.\nThe PCL (Point Cloud Library) is an open-source framework for n-dimensional point cloud and 3D geometry processing. It includes several point registration algorithms.\n\nThe iterative closest point (ICP) algorithm was introduced by Besl and McKay.\nThe algorithm performs rigid registration in an iterative fashion by assuming that every point in formula_5 corresponds with the closest point to it in formula_7, and then finding the least squares rigid transformation. As such, it works best if the initial pose of formula_5 is sufficiently close to formula_7. In pseudocode, the basic algorithm is implemented as follows:\n\nHere, the function codice_1 performs least squares regression to minimize the distance in each of the formula_26 pairs, i.e. minimizing the distance function in Equation ().\n\nBecause the cost function of registration depends on finding the closest point in formula_7 to every point in formula_5, it can change as the algorithm is running. As such, it is difficult to prove that ICP will in fact converge exactly to the local optimum. In fact, empirically, ICP and EM-ICP do not converge to the local minimum of the cost function. Nonetheless, because ICP is intuitive to understand and straightforward to implement, it remains the most commonly used point set registration algorithm. Many variants of ICP have been proposed, affecting all phases of the algorithm from the selection and matching of points to the minimization strategy.\nFor example, the expectation maximization algorithm is applied to the ICP algorithm to form the EM-ICP method, and the Levenberg-Marquardt algorithm is applied to the ICP algorithm to form the LM-ICP method.\n\nRobust point matching (RPM) was introduced by Gold et al. The method performs registration using deterministic annealing and soft assignment of correspondences between point sets. Whereas in ICP the correspondence generated by the nearest-neighbour heuristic is binary, RPM uses a \"soft\" correspondence where the correspondence between any two points can be anywhere from 0 to 1, although it ultimately converges to either 0 or 1. The correspondences found in RPM is always one-to-one, which is not always the case in ICP. Let formula_29 be the formula_30th point in formula_5 and formula_32 be the formula_33th point in formula_7. The \"match matrix\" formula_35 is defined as such:\n\nThe problem is then defined as: Given two point sets formula_5 and formula_7 find the Affine transformation formula_10 and the match matrix formula_35 that best relates them. Knowing the optimal transformation makes it easy to determine the match matrix, and vice versa. However, the RPM algorithm determines both simultaneously. The transformation may be decomposed into a translation vector and a transformation matrix:\n\nThe matrix formula_41 in 2D is composed of four separate parameters formula_42, which are scale, rotation, and the vertical and horizontal shear components respectively. The cost function is then:\n\nsubject to formula_43, formula_44, formula_45. The formula_46 term biases the objective towards stronger correlation by decreasing the cost if the match matrix has more ones in it. The function formula_47 serves to regularize the Affine transformation by penalizing large values of the scale and shear components:\n\nfor some regularization parameter formula_49.\n\nThe RPM method optimizes the cost function using the Softassign algorithm. The 1D case will be derived here. Given a set of variables formula_50 where formula_51. A variable formula_52 is associated with each formula_53 such that formula_54. The goal is to find formula_35 that maximizes formula_56. This can be formulated as a continuous problem by introducing a control parameter formula_57. In the deterministic annealing method, the control parameter formula_58 is slowly increased as the algorithm runs. Let formula_35 be:\n\nthis is known as the softmax function. As formula_58 increases, it approaches a binary value as desired in Equation (). The problem may now be generalized to the 2D case, where instead of maximizing formula_56, the following is maximized:\n\nwhere\n\nThis is straightforward, except that now the constraints on formula_63 are doubly stochastic matrix constraints: formula_64 and formula_65. As such the denominator from Equation () cannot be expressed for the 2D case simply. To satisfy the constraints, it is possible to use a result due to Sinkhorn, which states that a doubly stochastic matrix is obtained from any square matrix with all positive entries by the iterative process of alternating row and column normalizations. Thus the algorithm is written as such:\n\nwhere the deterministic annealing control parameter formula_58 is initially set to formula_67 and increases by factor formula_68 until it reaches the maximum value formula_69. The summations in the normalization steps sum to formula_70 and formula_71 instead of just formula_3 and formula_4 because the constraints on formula_63 are inequalities. As such the formula_70th and formula_71th elements are slack variables.\n\nThe algorithm can also be extended for point sets in 3D or higher dimensions. The constraints on the correspondence matrix formula_35 are the same in the 3D case as in the 2D case. Hence the structure of the algorithm remains unchanged, with the main difference being how the rotation and translation matrices are solved.\n\nThe thin plate spline robust point matching (TPS-RPM) algorithm by Chui and Rangarajan augments the RPM method to perform non-rigid registration by parametrizing the transformation as a thin plate spline.\nHowever, because the thin plate spline parametrization only exists in three dimensions, the method cannot be extended to problems involving four or more dimensions.\n\nThe kernel correlation (KC) approach of point set registration was introduced by Tsin and Kanade.\nCompared with ICP, the KC algorithm is more robust against noisy data. Unlike ICP, where, for every model point, only the closest scene point is considered, here every scene point affects every model point. As such this is a \"multiply-linked\" registration algorithm. For some kernel function formula_78, the kernel correlation formula_79 of two points formula_80 is defined thus:\n\nThe kernel function formula_78 chosen for point set registration is typically symmetric and non-negative kernel, similar to the ones used in the Parzen window density estimation. The Gaussian kernel typically used for its simplicity, although other ones like the Epanechnikov kernel and the tricube kernel may be substituted. The kernel correlation of an entire point set formula_82 is defined as the sum of the kernel correlations of every point in the set to every other point in the set:\n\nThe KC of a point set is proportional, within a constant factor, to the logarithm of the information entropy. Observe that the KC is a measure of a \"compactness\" of the point set—trivially, if all points in the point set were at the same location, the KC would evaluate to a large value. The cost function of the point set registration algorithm for some transformation parameter formula_11 is defined thus:\n\nSome algebraic manipulation yields:\n\nThe expression is simplified by observing that formula_84 is independent of formula_11. Furthermore, assuming rigid registration, formula_86 is invariant when formula_11 is changed because the Euclidean distance between every pair of points stays the same under rigid transformation. So the above equation may be rewritten as:\n\nThe kernel density estimates are defined as:\n\nThe cost function can then be shown to be the correlation of the two kernel density estimates:\n\nHaving established the cost function, the algorithm simply uses gradient descent to find the optimal transformation. It is computationally expensive to compute the cost function from scratch on every iteration, so a discrete version of the cost function Equation () is used. The kernel density estimates formula_90 can be evaluated at grid points and stored in a lookup table. Unlike the ICP and related methods, it is not necessary to find the nearest neighbour, which allows the KC algorithm to be comparatively simple in implementation.\n\nCompared to ICP and EM-ICP for noisy 2D and 3D point sets, the KC algorithm is less sensitive to noise and results in correct registration more often.\n\nThe kernel density estimates are sums of Gaussians and may therefore be represented as Gaussian mixture models (GMM). Jian and Vemuri use the GMM version of the KC registration algorithm to perform non-rigid registration parametrized by thin plate splines.\n\nCoherent point drift (CPD) was introduced by Myronenko and Song.\nThe algorithm takes a probabilistic approach to aligning point sets, similar to the GMM KC method. Unlike earlier approaches to non-rigid registration which assume a thin plate spline transformation model, CPD is agnostic with regard to the transformation model used. The point set formula_5 represents the Gaussian mixture model (GMM) centroids. When the two point sets are optimally aligned, the correspondence is the maximum of the GMM posterior probability for a given data point. To preserve the topological structure of the point sets, the GMM centroids are forced to move coherently as a group. The expectation maximization algorithm is used to optimize the cost function.\n\nLet there be points in formula_5 and points in formula_7. The GMM probability density function for a point is:\n\nwhere, in dimensions, formula_94 is the Gaussian distribution centered on point formula_95.\n\nThe membership probabilities formula_97 is equal for all GMM components. The weight of the uniform distribution is denoted as formula_98. The mixture model is then:\n\nThe GMM centroids are re-parametrized by a set of parameters formula_11 estimated by maximizing the likelihood. This is equivalent to minimizing the negative log-likelihood function:\n\nwhere it is assumed that the data is independent and identically distributed. The correspondence probability between two points formula_29 and formula_32 is defined as the posterior probability of the GMM centroid given the data point:\n\nThe expectation maximization (EM) algorithm is used to find formula_11 and formula_104. The EM algorithm consists of two steps. First, in the E-step or \"estimation\" step, it guesses the values of parameters (\"old\" parameter values) and then uses Bayes' theorem to compute the posterior probability distributions formula_105 of mixture components. Second, in the M-step or \"maximization\" step, the \"new\" parameter values are then found by minimizing the expectation of the complete negative log-likelihood function, i.e. the cost function:\n\nIgnoring constants independent of formula_11 and formula_107, Equation () can be expressed thus:\n\n(i|s_j) \\lVert s_j - T(m_i,\\theta) \\rVert^2 \n\nwhere\n\nwith formula_109 only if formula_110. The posterior probabilities of GMM components computed using previous parameter values formula_111 is:\n\n(i|s_j) = \n\nMinimizing the cost function in Equation () necessarily decreases the negative log-likelihood function in Equation () unless it is already at a local minimum. Thus, the algorithm can be expressed using the following pseudocode, where the point sets formula_5 and formula_7 are represented as formula_114 and formula_115 matrices formula_116 and formula_117 respectively:\n\nwhere the vector formula_118 is a column vector of ones. The codice_2 function differs by the type of registration performed. For example, in rigid registration, the output is a scale , a rotation matrix formula_119, and a translation vector formula_120. The parameter formula_11 can be written as a tuple of these:\n\nwhich is initialized to one, the identity matrix, and a column vector of zeroes:\n\nThe aligned point set is:\n\nThe codice_3 function for rigid registration can then be written as follows, with derivation of the algebra explained in Myronenko's 2010 paper.\nFor affine registration, where the goal is to find an affine transformation instead of a rigid one, the output is an affine transformation matrix formula_125 and a translation formula_120 such that the aligned point set is:\n\nThe codice_4 function for rigid registration can then be written as follows, with derivation of the algebra explained in Myronenko's 2010 paper.\n\nIt is also possible to use CPD with non-rigid registration using a parametrization derived using calculus of variation.\n\nSums of Gaussian distributions can be computed in linear time using the fast Gauss transform (FGT). Consequently, the time complexity of CPD is formula_128, which is asymptotically much faster than formula_129 methods.\n\nIntroduced in 2013 by H. Assalih in his PhD thesis to accommodate sonar image registration, this kind of images tend to have lots of noise so it is expected to have lots of outliers in the point sets to match, SCS delivers great robustness against outliers and can easily surpass ICP and CPD performance in the presence of outliers, SCS doesn’t use iterative optimization in high dimensional space, it is neither probabilistic nor spectral; the algorithm also uses a novel error formula to enhance its performance. SCS can match rigid and non-rigid transformations, however, it is too slow if the target transformation is a simple translation, yet, it is really robust if the target transformation is in 3 to 6 DoF.\n\n", "id": "40374554", "title": "Point set registration"}
{"url": "https://en.wikipedia.org/wiki?curid=289860", "text": "Alignments of random points\n\nAlignments of random points in the plane can be demonstrated by statistics to be remarkably and counter-intuitively easy to find when a large number of random points are marked on a bounded flat surface. This has been put forward as a demonstration that ley lines and other similar mysterious alignments believed by some to be phenomena of deep significance might exist solely due to chance alone, as opposed to the supernatural or anthropological explanations put forward by their proponents. The topic has also been studied in the fields of computer vision and astronomy.\n\nA number of studies have examined the mathematics of alignment of random points on the plane. In all of these, the width of the line - the allowed displacement of the positions of the points from a perfect straight line - is important. It allows the fact that real-world features are not mathematical points, and that their positions need not line up exactly for them to be considered in alignment. Alfred Watkins, in his classic work on ley lines \"The Old Straight Track\", used the width of a pencil line on a map as the threshold for the tolerance of what might be regarded as an alignment. For example, using a 1 mm pencil line to draw alignments on an 1:50,000 Ordnance Survey map, the corresponding width on the ground would be 50 m.\n\nContrary to intuition, finding alignments between randomly placed points on a landscape gets progressively easier as the geographic area to be considered increases. One way of understanding this phenomenon is to see that the increase in the number of possible combinations of sets of points in that area overwhelms the decrease in the probability that any given set of points in that area line up.\n\nOne definition which expresses the generally accepted meaning of \"alignment\" is:\n\nMore precisely, a path of width \"w\" may be defined as the set of all points within a distance of \"w/2\" of a straight line on a plane, or a great circle on a sphere, or in general any geodesic on any other kind of manifold. Note that, in general, any given set of points that are aligned in this way will contain a large number of infinitesimally different straight paths. Therefore, only the existence of at least one straight path is necessary to determine whether a set of points is an alignment. For this reason, it is easier to count the sets of points, rather than the paths themselves.\nThe number of alignments found is very sensitive to the allowed width \"w\", increasing approximately proportionately to \"w\", where \"k\" is the number of points in an alignment.\n\nThe following is a very approximate order-of-magnitude estimate of the likelihood of alignments, assuming a plane covered with uniformly distributed \"significant\" points.\n\nConsider a set of \"n\" points in a compact area with approximate diameter \"L\" and area approximately \"L\". Consider a valid line to be one where every point is within distance \"w\"/2 of the line (that is, lies on a track of width \"w\", where \"w\" ≪ \"L\").\n\nConsider all the unordered sets of \"k\" points from the \"n\" points, of which there are:\n\n(see factorial and binomial coefficient for notation).\n\nTo make a rough estimate of the probability that any given subset of \"k\" points is approximately collinear in the way defined above, let us consider the line between the \"leftmost\" and \"rightmost\" two points in that set (for some arbitrary left/right axis: we can choose top and bottom for the exceptional vertical case). These two points are by definition on this line. For each of the remaining \"k\"-2 points, the probability that the point is \"near enough\" to the line is roughly \"w\"/\"L\", which can be seen by considering the ratio of the area of the line tolerance zone (roughly \"wL\") and the overall area (roughly \"L\").\n\nSo, the expected number of k-point alignments, by this definition, is very roughly:\n\nAmong other things this can be used to show that, contrary to intuition, the number of k-point lines expected from random chance in a plane covered with points at a given density, for a given line width, increases much more than linearly with the size of the area considered, since the combinatorial explosion of growth in the number of possible combinations of points more than makes up for the increase in difficulty of any given combination lining up.\n\nA more precise expression for the number of 3-point alignments of maximum width \"w\" and maximum length \"d\" expected by chance among \"n\" points placed randomly on a square of side \"L\" is \n\nIf edge effects (alignments lost over the boundaries of the square) are included, then the expression becomes\n\nA generalisation to \"k\"-point alignments (ignoring edge effects) is\nwhich has roughly similar asymptotic scaling properties as the crude approximation in the previous section, with combinatorial explosion for large \"n\" overwhelming the effects of other variables.\n\nComputer simulations show that points on a plane tend to form alignments similar to those found by ley hunters in numbers consistent with the order-of-magnitude estimates above, suggesting that ley lines may also be generated by chance. This phenomenon occurs regardless of whether the points are generated pseudo-randomly by computer, or from data sets of mundane features such as pizza restaurants or telephone booths.\n\nIt is easy to find alignments of 4 to 8 points in reasonably small data sets with \"w\" = 50 m.\nChoosing large areas or larger values of \"w\" makes it easy to find alignments of 20 or more points.\n\n", "id": "289860", "title": "Alignments of random points"}
{"url": "https://en.wikipedia.org/wiki?curid=44315990", "text": "Ortho3D\n\nOrtho3D\n\nTrue 3D city reconstruction created automatically from multi-angle stereoscopic vertical and oblique aerial and/or street photographs, thanks to very robust photogrammetric dense matching software based on strong GPU computing.\n\nWorkflow :\n\n\n\n\n\n", "id": "44315990", "title": "Ortho3D"}
{"url": "https://en.wikipedia.org/wiki?curid=44534652", "text": "Spatial verification\n\nThe spatial verification consists in verify a spatial correlation between certain points of a pair of images.\n\nThe main problem is that outliers (that does not fit or does not match the selected model) affect adjustment called least squares (numerical analysis technique framed in mathematical optimization, which, given an set of ordered pairs: independent variable, dependent variable, and a family of functions, try to find the continuous function).\n\n\n\nThe most widely used for spatial verification and avoid errors caused by these outliers methods are:\nSeeks to avoid the impact of outliers, that not fit with the model, so only considers inline which match the model in question. If an outlier is chosen to calculate the current setting, then the resulting line will have little support from the rest of the points.\nThe algorithm that is performed is a loop that performs the following steps:\nThe goal is to keep the model with the highest number of matches and the main problem is the number of times you have to repeat the process to obtain the best estimate of the model.\nRANSAC set in advance the number of iterations of the algorithm.\n\nTo specify scenes or objects, is commonly used affine transformations to perform the spatial verification.\n\nThis is a technique for detecting shapes in digital images that solves the veracity of space by clusters of points belonging to the model through a voting procedure on a set of parametric figures.\n\nNot all possible combinations comprovar characteristics by adjusting a model for every possible subset, so that the voting technique, in which a vote is stored for each possible line in which each point is used. Then observe what were the lines with the most votes and those are selected.\n\nIf we use the local characteristics of scale, rotation and translation invariant, each feature coincidence gives a hypothesis alignment for scaling, translation and orientation of the model in the picture.\n\nOne hypothesis generated by a single match can be unreliable, so for each match (match), a vote is done to get a stronger hypothesis in the Hough space.\nSo we have two major phases:\n\nThe main disadvantages are:\n\n\n", "id": "44534652", "title": "Spatial verification"}
{"url": "https://en.wikipedia.org/wiki?curid=38561540", "text": "Background subtraction\n\nBackground subtraction, also known as foreground detection, is a technique in the fields of image processing and computer vision wherein an image's foreground is extracted for further processing (object recognition etc.). Generally an image's regions of interest are objects (humans, cars, text etc.) in its foreground. After the stage of image preprocessing (which may include image denoising, post processing like morphology etc.) object localisation is required which may make use of this technique.\n\nBackground subtraction is a widely used approach for detecting moving objects in videos from static cameras. The rationale in the approach is that of detecting the moving objects from the difference between the current frame and a reference frame, often called \"background image\", or \"background model\". Background subtraction is mostly done if the image in question is a part of a video stream. Background subtraction provides important cues for numerous applications in computer vision, for example surveillance tracking or human poses estimation.\n\nBackground subtraction is generally based on a static background hypothesis which is often not applicable in real environments. With indoor scenes, reflections or animated images on screens lead to background changes. Similarly, due to wind, rain or illumination changes brought by weather, static backgrounds methods have difficulties with outdoor scenes.\nA robust background subtraction algorithm should be able to handle lighting changes, repetitive motions from clutter and long-term scene changes. The following analyses make use of the function of \"V\"(\"x\",\"y\",\"t\") as a video sequence where \"t\" is the time dimension, \"x\" and \"y\" are the pixel location variables. e.g. \"V\"(1,2,3) is the pixel intensity at (1,2) pixel location of the image at \"t\" = 3 in the video sequence.\n\nA motion detection algorithm begins with the segmentation part where foreground or moving objects are segmented from\nthe background. The simplest way to implement this is to take an image as background and take the frames obtained at the time\nt, denoted by I(t) to compare with the background image denoted by B. Here using simple arithmetic calculations, we can\nsegment out the objects simply by using image subtraction technique of computer vision meaning for each pixels in I(t), take the\npixel value denoted by P[I(t)] and subtract it with the corresponding pixels at the same position on the background image\ndenoted as P[B].\n\nIn mathematical equation, it is written as:\n\nThe background is assumed to be the frame at time \"t\". This difference image would only show some intensity for the pixel locations which have changed in the two frames. Though we have seemingly removed the background, this approach will only work for cases where all foreground pixels are moving and all background pixels are static.\n\nA threshold \"Threshold\" is put on this difference image to improve the subtraction (see Image thresholding).\n\nThis means that the difference image's pixels' intensities are 'thresholded' or filtered on the basis of value of Threshold.\n\nThe accuracy of this approach is dependent on speed of movement in the scene. Faster movements may require higher thresholds.\n\nFor calculating the image containing only the background, a series of preceding images are averaged. For calculating the background image at the instant \"t\",\n\nwhere \"N\" is the number of preceding images taken for averaging. This averaging refers to averaging corresponding pixels in the given images. \"N\" would depend on the video speed (number of images per second in the video) and the amount of movement in the video. After calculating the background \"B\"(\"x\",\"y\",\"t\") we can then subtract it from the image \"V\"(\"x\",\"y\",\"t\") at time \"t\" = t and threshold it. Thus the foreground is\n\nwhere Th is threshold. Similarly we can also use median instead of mean in the above calculation of \"B\"(\"x\",\"y\",\"t\").\n\nUsage of global and time-independent thresholds (same Th value for all pixels in the image) may limit the accuracy of the above two approaches.\n\nFor this method, Wren et al. propose fitting a Gaussian probabilistic density function (pdf) on the most recent formula_5 frames. In order to avoid fitting the pdf from scratch at each new frame time formula_6, a running (or on-line cumulative) average is computed.\n\nThe pdf of every pixel is characterized by mean formula_7 and variance formula_8 . The following is a possible initial condition (assuming that initially every pixel is background):\n\nwhere formula_11 is the value of the pixel's intensity at time formula_6. In order to initialize variance, we can, for example, use the variance in x and y from a small window around each pixel.\n\nNote that background may change over time (e.g. due to illumination changes or non-static background objects). To accommodate for that change, at every frame formula_6, every pixel's mean and variance must be updated, as follows:\n\nWhere formula_17 determines the size of the temporal window that is used to fit the pdf (usually formula_18 ) and formula_19 is the Euclidean distance between the mean and the value of the pixel.\n\nWe can now classify a pixel as background if its current intensity lies within some confidence interval of its distribution's mean:\n\nwhere the parameter formula_22 is a free threshold (usually formula_23 ). A larger value for formula_24 allows for more dynamic background, while a smaller formula_24 increases the probability of a transition from background to foreground due to more subtle changes.\n\nIn a variant of the method, a pixel's distribution is only updated if it is classified as background. This is to prevent newly introduced foreground objects from fading into the background. The update formula for the mean is changed accordingly:\n\nwhere formula_27 when formula_11 is considered foreground and formula_29 otherwise. So when formula_27 , that is, when the pixel is detected as foreground, the mean will stay the same. As a result, a pixel, once it has become foreground, can only become background again when the intensity value gets close to what it was before turning foreground. This method, however, has several issues: It only works if all pixels are initially background pixels (or foreground pixels are annotated as such). Also, it cannot cope with gradual background changes: If a pixel is categorized as foreground for a too long period of time, the background intensity in that location might have changed (because illumination has changed etc.). As a result, once the foreground object is gone, the new background intensity might not be recognized as such anymore.\n\nMixture of Gaussians method approaches by modelling each pixel as a mixture of Gaussians and uses an on-line approximation to update the model. In this technique, it is assumed that every pixel's intensity values in the video can be modeled using a Gaussian mixture model. A simple heuristic determines which intensities are most probably of the background. Then the pixels which do not match to these are called the foreground pixels.\nForeground pixels are grouped using 2D connected component analysis.\n\nAt any time t, a particular pixel (formula_31)'s history is\n\nThis history is modeled by a mixture of \"K\" Gaussian distributions:\n\nwhere\n\nFirst, each pixel is characterized by its intensity in RGB color space. Then probability of observing the current pixel is given by the following formula in the multidimensional case\n\nWhere the parameters are K is the number of distributions, ω is a weight associated to the ith Gaussian at time t with mean µ and standard deviation Σ .\n\nOnce the parameters initialization is made, a first foreground detection can be made then the parameters are updated. The first B Gaussian distribution which exceeds the threshold \"T\" is retained for a background distribution\n\nThe other distributions are considered to represent a foreground distribution. Then, when the new frame incomes at times formula_38, a match test is made of each pixel. A pixel matches a Gaussian distribution if the Mahalanobis distance\n\nwhere \"k\" is a constant threshold equal to formula_40.Then, two cases can occur:\n\nCase 1: A match is found with one of the \"k\" Gaussians. For the matched component, the update is done as follows\n\nPower and Schoonees [3] used the same algorithm to segment the foreground of the image\n\nThe essential approximation to formula_43 is given by formula_44\n\nCase 2: No match is found with any of the formula_46 Gaussians. In this case, the least probable distribution formula_46 is replaced with a new one with parameters\n\nOnce the parameter maintenance is made, foreground detection can be made and so on. An on-line K-means approximation is used to update the Gaussians. Numerous improvements of this original method developed by Stauffer and Grimson have been proposed and a complete survey can be found in Bouwmans et al. A standard method of adaptive backgrounding is averaging the images over time, creating a background approximation which is similar to the current static scene except where motion occur.\n\nSeveral surveys which concern categories or sub-categories of models can be found as follows:\n\n\nSeveral comparison/evaluation papers can be found in the literature:\n\n\n\n\n\n\n\n\nThe Background Subtraction Website (T. Bouwmans, Univ. La Rochelle, France) contains a comprehensive list of the references in the field, and links to available datasets and software.\n\n\nThe BackgroundSubtractorCNT library implements a very fast and high quality algorithm written in C++ based on OpenCV. It is targeted at low spec hardware but works just as fast on modern Linux and Windows. (For more information: https://github.com/sagi-z/BackgroundSubtractorCNT).\n\nThe BGS Library (A. Sobral, Univ. La Rochelle, France) provides a C++ framework to perform background subtraction algorithms. The code works either on Windows or on Linux. Currently the library offers more than 30 BGS algorithms. (For more information: https://github.com/andrewssobral/bgslibrary)\n\n\n\n\n", "id": "38561540", "title": "Background subtraction"}
{"url": "https://en.wikipedia.org/wiki?curid=12555662", "text": "Active contour model\n\nActive contour model, also called snakes, is a framework in computer vision for delineating an object outline from a possibly noisy 2D image. The snakes model is popular in computer vision, and snakes are greatly used in applications like object tracking, shape recognition, segmentation, edge detection and stereo matching.\n\nA snake is an energy minimizing, deformable spline influenced by constraint and image forces that pull it towards object contours and internal forces that resist deformation. Snakes may be understood as a special case of the general technique of matching a deformable model to an image by means of energy minimization. In two dimensions, the active shape model represents a discrete version of this approach, taking advantage of the point distribution model to restrict the shape range to an explicit domain learned from a training set.\nSnakes do not solve the entire problem of finding contours in images, since the method requires knowledge of the desired contour shape beforehand. Rather, they depend on other mechanisms such as interaction with a user, interaction with some higher level image understanding process, or information from image data adjacent in time or space.\n\nIn computer vision, contour models describe the boundaries of shapes in an image. Snakes in particular are designed to solve problems where the approximate shape of the boundary is known. By being a deformable model, snakes can adapt to differences and noise in stereo matching and motion tracking. Additionally, the method can find Illusory contours in the image by ignoring missing boundary information.\n\nCompared to classical feature extraction techniques, snakes have multiple advantages:\n\nThe key drawbacks of the traditional snakes are\n\n\nA simple elastic snake is defined by a set of \"n\" points formula_1 where formula_2, the internal elastic energy term formula_3, and the external edge-based energy term formula_4. The purpose of the internal energy term is to control the deformations made to the snake, and the purpose of the external energy term is to control the fitting of the contour onto the image. The external energy is usually a combination of the forces due to the image itself formula_5 and the constraint forces introduced by the user formula_6\n\nThe energy function of the snake is the sum of its external energy and internal energy, or\n\nThe internal energy of the snake is composed of the continuity of the contour formula_8 and the smoothness of the contour formula_9.\n\nThis can be expanded as\n\nWhere formula_12 and formula_13 are user-defined weights; these control the internal energy function's sensitivity to the amount of stretch in the snake and the amount of curvature in the snake, respectively, and thereby control the number of constraints on the shape of the snake.\n\nIn practice, a large weight formula_12 for the continuity term penalizes changes in distances between points in the contour. A large weight formula_13 for the smoothness term penalizes oscillations in the contour and will cause the contour to act as a thin plate.\n\nEnergy in the image is some function of the features of the image. This is one of the most common points of modification in derivative methods. Features in images and images themselves can be processed in many and various ways.\n\nFor an image formula_16, lines, edges, and terminations present in the image, the general formulation of energy due to the image is\n\nwhere formula_18, formula_19, formula_20 are weights of these salient features. Higher weights indicate that the salient feature will have a larger contribution to the image force.\n\nThe line functional is the intensity of the image, which can be represented as\n\nThe sign of formula_18 will determine whether the line will be attracted to either dark lines or light lines.\n\nSome smoothing or noise reduction may be used on the image, which then the line functional appears as\n\nThe edge functional is based on the image gradient. One implementation of this is\n\nA snake originating far from the desired object contour may erroneously converge to some local minimum. Scale space continuation can be used in order to avoid these local minima. This is achieved by using a blur filter on the image and reducing the amount of blur as the calculation progresses to refine the fit of the snake. The energy functional using scale space continuation is\n\nwhere formula_26 is a Gaussian with standard deviation formula_27. Minima of this function fall on the zero-crossings of\nformula_28 which define edges as per Marr–Hildreth theory.\n\nCurvature of level lines in a slightly smoothed image can be used to detect corners and terminations in an image. Using this method, let formula_29 be the image smoothed by\n\nwith gradient angle\n\nunit vectors along the gradient direction\n\nand unit vectors perpendicular to the gradient direction\n\nThe termination functional of energy can be represented as\n\nSome systems, including the original snakes implementation, allowed for user interaction to guide the snakes, not only in initial placement but also in their energy terms. Such constraint energy formula_35 can be used to interactively guide the snakes towards or away from particular features.\n\nGiven an initial guess for a snake, the energy function of the snake is iteratively minimized. Gradient descent minimization is one of the simplest optimizations which can be used to minimize snake energy. Each iteration takes one step in the negative gradient of the point with controlled step size formula_36 to find local minima. This gradient-descent minimization can be implemented as\n\nWhere formula_38 is the force on the snake, which is defined by the negative of the gradient of the energy field.\n\nAssuming the weights formula_40 and formula_41 are constant with respect to formula_42, this iterative method can be simplified to\n\nIn practice, images have finite resolution and can only be integrated over finite time steps formula_44. As such, discrete approximations must be made for practical implementation of snakes.\n\nThe energy function of the snake can be approximated by using the discrete points on the snake.\n\nConsequentially, the forces of the snake can be approximated as\n\nGradient approximation can be done through any finite approximation method with respect to \"s\", such as Finite difference.\n\nThe introduction of discrete time into the algorithm can introduce updates which the snake is moved past the minima it is attracted to; this further can cause oscillations around the minima or lead to a different minima being found.\n\nThis can be avoided through tuning the time step such that the step size is never greater than a pixel due to the image forces. However, in regions of low energy, the internal energies will dominate the update.\n\nAlternatively, the image forces can be normalized for each step such that the image forces only update the snake by one pixel. This can be formulated as\n\nwhere formula_48 is near the value of the pixel size. This avoids the problem of dominating internal energies that arise from tuning the time step.\n\nThe energies in a continuous image may have zero-crossing that do not exist as a pixel in the image. In this case, a point in the snake would oscillate between the two pixels that neighbor this zero-crossing. This oscillation can be avoided by using interpolation between pixels instead of nearest neighbor.\n\nThe following pseudocode implements the snakes method in a general form\n\nfunction v = snakes (I, v)\n\nend\n\nWhere \"generateImageEnergy (I)\" can be written as\nfunction E_image = generateImageEnergy (I)\n\nend\n\nThe default method of snakes has various limitation and corner cases where the convergence performs poorly. Several alternatives exist which addresses issues of the default method, though with their own trade-offs. A few are listed here.\n\nThe gradient vector flow (GVF) snake model addresses two issues with snakes:\n\n\nIn 2D, the GVF vector field formula_49 minimizes the energy functional\n\nwhere formula_51 is a controllable smoothing term. This can be solved by solving the Euler equations\n\nThis can be solved through iteration towards a steady-state value.\n\nThis result replaces the default external force.\n\nThe primary issue with using GVF is the smoothing term formula_51 causes rounding of the edges of the contour. Reducing the value of formula_51 reduces the rounding but weakens the amount of smoothing.\n\nThe balloon model addresses these problems with the default active contour model:\n\n\nThe balloon model introduces an inflation term into the forces acting on the snake\n\nwhere formula_60 is the normal unitary vector of the curve at formula_61 and formula_62 is the magnitude of the force. formula_62 should have the same magnitude as the image normalization factor formula_64 and be smaller in value than formula_64 to allow forces at image edges to overcome the inflation force.\n\nThree issues arise from using the balloon model:\n\nThe diffusion snake model addresses the sensitivity of snakes to noise, clutter, and occlusion. It implements a modification of the Mumford–Shah functional and its cartoon limit and incorporates statistical shape knowledge. The default image energy functional formula_5 is replaced with\n\nwhere formula_68 is based on a modified Mumford–Shah functional\n\nwhere formula_70 is the piecewise smooth model of the image formula_71 of domain formula_72. Boundaries formula_73 are defined as\n\nwhere formula_75 are quadratic B-spline basis functions and formula_76 are the control points of the splines. The modified cartoon limit is obtained as formula_77 and is a valid configuration of formula_68.\n\nThe functional formula_79 is based on training from binary images of various contours and is controlled in strength by the parameter formula_80. For a Gaussian distribution of control point vectors formula_81 with mean control point vector formula_82 and covariance matrix formula_83 , the quadratic energy that corresponds to the Gaussian probability is\n\nThe strength of this method relies on the strength of the training data as well as the tuning of the modified Mumford–Shah functional. Different snakes will require different training data sets and tunings.\n\nGeometric active contour, or geodesic active contour (GAC) or conformal active contours employs ideas from Euclidean curve shortening evolution. Contours split and merge depending on the detection of objects in the image. These models are largely inspired by level sets, and have been extensively employed in medical image computing.\n\nFor example, the gradient descent curve evolution equation of GAC is \nwhere formula_86 is a halting function, \"c\" is a Lagrange multiplier, formula_87 is the curvature, and formula_88 is the unit inward normal. This particular form of curve evolution equation is only dependent on the velocity in the normal direction. It therefore can be rewritten equivalently in an Eulerian form by inserting the level set function formula_89 into it as follows\n\nThis simple yet powerful level-set reformation enables active contours to handle topology changes during the gradient descent curve evolution. It has inspired tremendous progress in the related fields, and using numerical methods to solve the level-set reformulation is now commonly known as the level set method. Although the level set method has become quite a popular tool for implementing active contours, Wang and Chan argued that not all curve evolution equations should be \"directly\" solved by it.\n\nMore recent developments in active contours address modeling of regional properties, incorporation of flexible shape priors and fully automatic segmentation, etc.\n\nStatistical models combining local and global features have been formulated by Lankton and Allen Tannenbaum.\n\nGraph cuts, or max-flow/min-cut, is a generic method for minimizing a particular form of energy called Markov random field (MRF) energy. The Graph cuts method has been applied to image segmentation as well, and it sometimes outperforms the level set method when the model is MRF or can be approximated by MRF.\n\n\n", "id": "12555662", "title": "Active contour model"}
{"url": "https://en.wikipedia.org/wiki?curid=97922", "text": "Digital image processing\n\nDigital image processing is the use of computer algorithms to perform image processing on digital images. As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and signal distortion during processing. Since images are defined over two dimensions (perhaps more) digital image processing may be modeled in the form of multidimensional systems.\n\nMany of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s at the Jet Propulsion Laboratory, Massachusetts Institute of Technology, Bell Laboratories, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement. The cost of processing was fairly high, however, with the computing equipment of that era. That changed in the 1970s, when digital image processing proliferated as cheaper computers and dedicated hardware became available. Images then could be processed in real time, for some dedicated problems such as television standards conversion. As general-purpose computers became faster, they started to take over the role of dedicated hardware for all but the most specialized and computer-intensive operations.\nWith the fast computers and signal processors available in the 2000s, digital image processing has become the most common form of image processing and generally, is used because it is not only the most versatile method, but also the cheapest.\n\nDigital image processing technology for medical applications was inducted into the Space Foundation Space Technology Hall of Fame in 1994.\n\nDigital image processing allows the use of much more complex algorithms, and hence, can offer both more sophisticated performance at simple tasks, and the implementation of methods which would be impossible by analog means.\n\nIn particular, digital image processing is the only practical technology for:\n\nSome techniques which are used in digital image processing include:\n\nDigital filters are used to blur and sharpen digital images. Filtering can be performed in the spatial domain by convolution with specifically designed kernels (filter array), or in the frequency (Fourier) domain by masking specific frequency regions. The following examples show both methods:\n\nImages are typically padded before being transformed to the Fourier space, the highpass filtered images below illustrate the consequences of different padding techniques:\n\nNotice that the highpass filter shows extra edges when zero padded compared to the repeated edge padding.\n\nMATLAB example for spatial domain highpass filtering.\n\nMATLAB example for Fourier domain highpass filtering.\n\nAffine transformations enable basic image transformations including scale, rotate, translate, mirror and sheer as is shown in the following examples show:\n\nDigital cameras generally include specialized digital image processing hardware – either dedicated chips or added circuitry on other chips – to convert the raw data from their image sensor into a color-corrected image in a standard image file format\n\n\"Westworld\" (1973) was the first feature film to use the digital image processing to pixellate photography to simulate an android's point of view.\n\n\n\n\n", "id": "97922", "title": "Digital image processing"}
{"url": "https://en.wikipedia.org/wiki?curid=44390264", "text": "OrCam device\n\nThe OrCam MyEye is a portable, artificial vision device that allows vision impaired people to understand text and identify objects. The device was developed by Israeli-based company OrCam Technologies Limited, and was released as a prototype in September 2013.\n\nOrCam was founded in 2010 in Israel, with the mission to \"use advanced computer vision to help the visually impaired.\" The company was started by Amnon Shashua and Ziv Aviram who also founded the accident avoidance systems development company Mobileye in 1999. OrCam debuted the device in September 2013 after years of development and testing. Because of a $15 million investment in OrCam by Intel Capital in March 2014, the company has been evaluated at approximately $100 million. Today, OrCam has approximately 80 employees, is headquartered in Jerusalem, Israel and has offices in New York and London.\n\nThe OrCam MyEye consists of two main components: the head unit and the base unit. The head unit consists of a camera and a microphone, and is mounted on the frames of a pair of eyeglasses. The box-like base unit contains the algorithms and processing components that give the device its functionality, and can be clipped to a belt or left to rest in a pocket. The head unit and base unit are adjoined by a connecting cable.\n\nThe OrCam MyEye recognizes text and products, and speaks to the person wearing the device via a bone-conduction earpiece. With the point of the person's finger, the device instantly responds and will infer whether it needs to read, find an item, or recognize a product depending on the environment. It may do so without searching for audio books, learning new software, or using other tools.\n\n\n\n\nThe OrCam MyEye's processing power comes from an i.MX 6Quad processor, paired with the added power management provided by an PF0100 power management integrated circuit (PMIC). The i.MX processor, which computes algorithms, gives the device its high performance and energy efficiency. This allows the device to interpret visual inputs and communicate their meaning in real time to the person wearing the device.\n\nIn 2011, Hebrew University researchers documented OrCam's processing technique, revealing some of the algorithms. The technique, known as ShareBoost, allows the device to balance recognition accuracy with speed.\n\nOn a full charge, the battery provides four hours of constant use or 24 hours on standby. It takes four hours to fully charge and is designed to be charged each night.\n\nThe device is priced at $3,500 and is currently available for purchase in the United States, Canada, UK(£2,400 ) and Australia with American English as the primary source of communication. The device is also available in Hebrew in Israel. OrCam does intend to make the device available in other regions and languages in the future.\n\nThe OrCam Reader, an OrCam MyEye that only focuses on reading texts, is available in the United States and Canada for $3,500.\n\nUpon its release, news of the OrCam MyEye was covered by several major international media outlets and technology companies.\n\nOrCam said that there were over 342 million adults worldwide with significant visual impairment, and that about 52 million of them had middle-class incomes. The 2011 National Health Survey by the National Center for Health Statistics estimates that there are 21.2 million people in the United States over the age of 18 that have some kind of visual impairment.\n\n", "id": "44390264", "title": "OrCam device"}
{"url": "https://en.wikipedia.org/wiki?curid=35026656", "text": "Color normalization\n\nColor normalization is a topic in computer vision concerned with artificial color vision and object recognition. In general, the distribution of color values in an image depends on the illumination, which may vary depending on lighting conditions, cameras, and other factors. Colour normalisation allows for object recognition techniques based on colour to compensate for these variations.\n\nColor constancy is a feature of the human internal model of perception, which provides humans with the ability to assign a relatively constant color to objects even under different illumination conditions. This is helpful for object recognition as well as identification of light sources in an environment. For example, humans see an object approximately as the same color when the sun is bright or when the sun is dim.\n\nColor normalization has been used for object recognition on color images in the field of robotics, bioinformatics and general artificial intelligence, when it is important to remove all intensity values from the image while preserving color values. One example is in case of a scene shot by a surveillance camera over the day, where it is important to remove shadows or lighting changes on same color pixels and recognize the people that passed. Another example is automated screening tools used for the detection of diabetic retinopathy as well as molecular diagnosis of cancer states, where it is important to include color information during classification.\n\nThe main issue about certain applications of color normalization is that the end result looks unnatural or too distant from the original colors. In cases where there is a subtle variation between important aspects, this can be problematic. More specifically, the side effect can be that pixels become divergent and not reflect the actual color value of the image.\nA way of combating this issue is to use color normalization in combination with thresholding to correctly and consistently segment a colored image.\n\nThere is a vast array of different transformations and algorithms for achieving color normalization and a limited list is presented here. The performance of an algorithm is dependent on the task and one algorithm which performs better than another in one task might perform worse in another (no free lunch theorem). Additionally, the choice of the algorithm depends on the preferences of the user for the end-result, e.g. they may want a more natural-looking color image.\n\nThe grey world normalization makes the assumption that changes in the lighting spectrum can be modelled by three constant factors applied to the red, green and blue channels of color. More specifically, a change in illuminated color can be modelled as a scaling α, β and γ in the R, G and B color channels and as such the grey world algorithm is invariant to illumination color variations. Therefore a constancy solution can be achieved by dividing each color channel by its average value as shown in the following formula:\n\nformula_1\n\nAs mentioned above, grey world color normalization is invariant to illuminated color variations α, β and γ, however it has one important problem: it does not account for all variations of illumination intensity and it is not dynamic; when new objects appear in the scene it fails. To solve this problem there are several variants of the grey world algorithm.\nAdditionally there is an iterative variation of the grey world normalization, however it was not found to perform significantly better.\n\nHistogram equalization is a non-linear transform which maintains pixel rank and is capable of normalizing for any monotonically increasing color transform function. It is considered to be a more powerful normalization transformation than the grey world method. The results of histogram equalization tend to have an exaggerated blue channel and look unnatural, due to the fact that in most images the distribution of the pixel values is usually more similar to a Gaussian distribution, rather than uniform.\n\nHistogram specification transforms the red, green and blue histograms to match the shapes of three specific histograms, rather than simply equalizing them. It refers to a class of image transforms which aims to obtain images of which the histograms have a desired shape.\nAs specified, firstly it is necessary to convert the image so that it has a particular histogram.\nAssume an image x. The following formula is the equalization transform of this image:\n\nformula_2\n\nThen assume wanted image z. The equalization transform of this image is:\n\nformula_3\n\nOf course formula_4 is the histogram of the output image.\nThe formula to find the inverse of the above transform is:\n\nformula_5\n\nTherefore, since images y and y' have the same equalized histogram they are actually the same image, meaning y = y' and the transform from the given image x to the wanted image z is:\n\nformula_6\n\nHistogram specification has the advantage of producing more realistic looking images, as it does not exaggerate the blue channel like histogram equalization.\n\nThe comprehensive color normalization is shown to increase localization and object classification results in combination with color indexing. It is an iterative algorithm which works in two stages. The first stage is to use the red, green and blue color space with the intensity normalized, to normalize each pixel. The second stage is to normalize each color channel separately, so that the sum of the color components is equal to one third of the number of pixels. The iterations continue until convergence, meaning no additional changes. Formally:\n\nNormalize the color image\n\nformula_7\n\nwhich consists of color vectors\n\nformula_8\n\nFor the first step explained above, compute:\n\nformula_9\n\nwhich leads to\n\nformula_10\n\nand\n\nformula_11\n\nFor the second step explained above, compute:\n\nformula_12\n\nand normalize\n\nformula_13\n\nOf course the same process is done for b' and g'. Then these two steps are repeated until the changes between iteration t and t+2 are less than some set threshold.\n\nComprehensive color normalization, just like the histogram equalization method previously mentioned, produces results that may look less natural due to the reduction in the number of color values.\n\n\n", "id": "35026656", "title": "Color normalization"}
{"url": "https://en.wikipedia.org/wiki?curid=48661480", "text": "Extensible Device Metadata\n\nThe Extensible Device Metadata (XDM) specification is an open file format for embedding device-related metadata in JPEG and other common image files without breaking compatibility with ordinary image viewers. The metadata types include: depth map, camera pose, point cloud, lens model, image reliability data, and identifying info about the hardware components. This metadata can be used, for instance, to create depth effects such as a bokeh filter, recreate the exact location and position of the camera when the picture was taken, or create 3D data models of environments or objects.\n\nThe format uses XML and is based on the XMP standard. It can support multiple \"cameras\" (image sources and types) in a single image file, and each can include data about its position and orientation relative to the primary camera. A camera data structure may include an image, depth map, etc. The XDM 1.0 documentation uses JPEG as the basic model, but states that the concepts generally apply to other image-file types supported by XMP, including PNG, TIFF, and GIF.\n\nThe XDM specification is developed and maintained by a working group that includes engineers from Intel and Google. The version 1.01 specification is posted at the website xdm.org; an earlier 1.0 version was posted at the Intel website in late 2015.\n\nXDM builds upon the Depthmap Metadata specification, introduced in 2014 and used in commercial applications including Google Lens Blur and Intel RealSense Depth Enabled Photography (DEP). That original specification was designed only for depth-photography use cases. Due to changes and expansions of the data structure, and the use of different namespaces, the two standards are not compatible. Existing applications that used that older standard will not work with XDM without modifications.\n\n\n", "id": "48661480", "title": "Extensible Device Metadata"}
{"url": "https://en.wikipedia.org/wiki?curid=42003835", "text": "Tango (platform)\n\nTango (formerly named Project Tango, while in-testing) is an augmented reality computing platform, developed and authored by the Advanced Technology and Projects (ATAP), a skunkworks division of Google. It uses computer vision to enable mobile devices, such as smartphones and tablets, to detect their position relative to the world around them without using GPS or other external signals. This allows application developers to create user experiences that include indoor navigation, 3D mapping, physical space measurement, environmental recognition, augmented reality, and windows into a virtual world.\n\nThe first product to emerge from ATAP, Tango was developed by a team led by computer scientist Johnny Lee, a core contributor to Microsoft's Kinect. In an interview in June 2015, Lee said, \"We're developing the hardware and software technologies to help everything and everyone understand precisely where they are, anywhere.\"\n\nGoogle has produced two devices to demonstrate the Tango technology: the discontinued Peanut phone and the Yellowstone 7-inch tablet. More than 3,000 of these devices had been sold as of June 2015, chiefly to researchers and software developers interested in building applications for the platform. In the summer of 2015, Qualcomm and Intel both announced that they are developing Tango reference devices as models for device manufacturers who use their mobile chipsets.\n\nAt CES, in January 2016, Google announced a partnership with Lenovo to release a consumer smartphone during the summer of 2016 to feature Tango technology marketed at consumers, noting a less than $500 price-point and a small form factor below 6.5 inches. At the same time, both companies also announced an application incubator to get applications developed to be on the device on launch.\n\nAt Lenovo Tech World 2016, Lenovo launched the world's first consumer phone based on Tango, as well as releasing it as \"Tango\".\n\nOn 15 December 2017, Google announced that they will be turning down support for Tango on March 1, 2018, in favor of ARCore.\n\nTango is different from other emerging 3D-sensing computer vision products, in that it's designed to run on a standalone mobile phone or tablet and is chiefly concerned with determining the device's position and orientation within the environment.\n\nThe software works by integrating three types of functionality:\n\nTogether, these generate data about the device in \"six degrees of freedom\" (3 axes of orientation plus 3 axes of motion) and detailed three-dimensional information about the environment.\n\nProject tango was also the first project to graduate from Google X in 2012 \n\nApplications on mobile devices use Tango's C and Java APIs to access this data in real time. In addition, an API is also provided for integrating Tango with the Unity game engine; this enables the rapid conversion or creation of games that allow the user to interact and navigate in the game space by moving and rotating a Tango device in real space. These APIs are documented on the Google developer website.\n\nTango enables apps to track a device's position and orientation within a detailed 3D environment, and to recognize known environments. This makes possible applications such as in-store navigation, visual measurement and mapping utilities, presentation and design tools, and a variety of immersive games. At Augmented World Expo 2015, Johnny Lee demonstrated a construction game that builds a virtual structure in real space, an AR showroom app that allows users to view a full-size virtual automobile and customize its features, a hybrid Nerf gun with mounted Tango screen for dodging and shooting AR monsters superimposed on reality, and a multiplayer VR app that lets multiple players converse in a virtual space where their avatar movements match their real-life movements.\n\nTango apps are distributed through Play. Google has encouraged the development of more apps with hackathons, an app contest, and promotional discounts on the development tablet.\n\nSome of the Tango apps are MeasureIt (for measuring objects), Solar Simulator, Wayfair (for testing furniture arrangements in rooms), and Cydalion (navigation app for people with visual impairments), among many others.\n\nAs a platform for software developers and a model for device manufacturers, Google has created two Tango devices to date.\n\n\"Peanut\" was the first production Tango device, released in the first quarter of 2014. It was a small Android phone with a Qualcomm MSM8974 quad-core processor and additional special hardware including a fisheye motion camera, \"RGB-IR\" camera for color image and infrared depth detection, and Movidius Vision processing units. A high-performance accelerometer and gyroscope were added after testing several competing models in the MARS lab at the University of Minnesota.\n\nSeveral hundred Peanut devices were distributed to early-access partners including university researchers in computer vision and robotics, as well as application developers and technology startups. Google stopped supporting the Peanut device in September 2015, as by then the Tango software stack had evolved beyond the versions of Android that run on the device.\n\n\"Yellowstone\" is a 7-inch tablet with full Tango functionality, released in June 2014, and sold as the Project Tango Tablet Development Kit. It features a 2.3 GHz quad-core Nvidia Tegra K1 processor, 128GB flash memory, 1920x1200-pixel touchscreen, 4MP color camera, fisheye-lens (motion-tracking) camera, an IR projector with RGB-IR camera for integrated depth sensing, and 4G LTE connectivity. The device is sold through the official Project Tango website and the Google Play Store. As of May 27, 2017, the Tango tablet is considered officially unsupported by Google.\n\nIn May 2014, two Peanut phones were delivered to the International Space Station to be part of a NASA project to develop autonomous robots that navigate in a variety of environments, including outer space. The soccer-ball-sized, 18-sided polyhedral SPHERES robots were developed at the NASA Ames Research Center, adjacent to the Google campus in Mountain View, California. Andres Martinez, SPHERES manager at NASA, said \"We are researching how effective [Tango's] vision-based navigation abilities are for performing localization and navigation of a mobile free flyer on ISS.\n\nLenovo Phab 2 Pro is the first commercial smartphone with the Tango Technology, the device was announced at the beginning of 2016, launched in August, and available for purchase in the US in November. The Phab 2 Pro has a 6.4 inch screen, contains a Snapdragon 652 processor, and 64 GB of internal storage, with a rear facing 16 Megapixels camera and 8 MP front camera.\n\nAsus Zenfone AR, announced at CES 2017, will be the second commercial smartphone with the Tango Technology. It runs Tango AR & Daydream VR on Snapdragon 821, with 6GB or 8GB of RAM and 128 or 256GB of internal memory depending on the configuration. The exclusive launch carrier in the US was announced to be Verizon in May, with a targeted release of summer 2017.\n\nIn creating Project Tango, Google has collaborated with developers in nine countries and several organizations including Bosch, Bsquare, CompalComm, ETH Zurich, Flyby Media, George Washington University, MMSolutions, Movidius, University of Minnesota MARS Lab, JPL Computer Vision Group, OLogic, OmniVision, Open Source Robotics Foundation, Paracosm, Sunny Optical Technology, PMD Technologies, Mantis Vision, Prizmiq, Intermodalics and SagivTech.\n\nPartnerships have been announced with companies that include apelab, Autodesk, Aisle411, Bosch, Defective Studios, Durovis (Dive), Infineon, JPL, Left Field Labs, Legacy Games, Limbic, moBack, NVYVE, OmniVision, Otherworld Interactive, PMD Technologies, Sagivtech, SideKick, Speck Design, Stinkdigital, and Inuitive.\n\n\n", "id": "42003835", "title": "Tango (platform)"}
{"url": "https://en.wikipedia.org/wiki?curid=48715673", "text": "Saliency map\n\nIn computer vision, a saliency map is an image that shows each pixel's unique quality. The goal of a saliency map is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. For example, if a pixel has a high grey level or other unique color quality in a color image, that pixel's quality will show in the saliency map and in an obvious way. Saliency is a kind of Image segmentation.\n\nThe result of saliency map is set of contours extracted from the image (see edge detection). Each of the pixels in a region are similar with respect to some characteristic or computed property, such as color, intensity, or texture.\n\nSaliency is a part of image segmentation. In computer vision, image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as superpixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.\n\nThe result of image segmentation is a set of segments that collectively cover the entire image or a set of contours extracted from the image (see edge detection). Each of the pixels in a region are similar with respect to some characteristic or computed property, such as color, intensity, or texture. Adjacent regions are significantly different with respect to the same characteristic(s). When applied to a stack of images, typical in medical imaging, the resulting contours after image segmentation can be used to create 3D reconstructions with the help of interpolation algorithms like Marching cubes.\n\nFirst, we should calculate the distance of each pixel to the rest of pixels in the same frame. And then add all of this minus value together. We use as the representative of the pixel value. Then we get the following equation:\nWhere N is the total number of pixels in the current frame. Then we can further restructure our formula. We put the value that has same I together.\nWhich is the frequency of . And the value of n belongs to [0,255]. The frequencies is expressed in the form of histogram, and the computational time of histogram is time complexity.\n\nThis saliency map algorithm has time complexity. Since the computational time of histogram is time complexity which N is the number of pixel's number of a frame. Besides, the minus part and multiply part of this equation need 256 times operation. Consequently, the time complexity of this algorithm is which equals to .\n\nAll of the following code is pseudo matlab code. First, read Data from video sequences.\n\nAfter we read data, we do superpixel process to each frame.\nSpnum1 and Spnum2 represent the pixel number of current frame and previous pixel.\n\nThen we calculate the color distance of each pixel, this process we call it contract function.\n\nAfter this two process, we will get a saliency map, and then store all of these maps into a new FileFolder.\n\nThe major difference between function one and two is the difference of contract function. If spnum1 and spnum2 both represent the current frame’s pixel number, then this contract function is for the first saliency function. If spnum1 is the current frame’s pixel number and spnum2 represent the previous frame’s pixel number, then this contract function is for second saliency function. If we use the second contract function which using the pixel of the same frame to get center distance to get a saliency map, then we apply this saliency function to each frame and use current frame’s saliency map minus previous frame’s saliency map to get a new image which is the new saliency result of the third saliency function.\n\n", "id": "48715673", "title": "Saliency map"}
{"url": "https://en.wikipedia.org/wiki?curid=44722153", "text": "Underwater computer vision\n\nUnderwater computer vision is a subfield of computer vision. In recent years, with the development of underwater vehicles ( ROV, AUV, gliders), the need to be able to record and process huge amount of information has become increasingly important. Applications range from inspection of underwater structures for the offshore industry to the identification and counting of fishes for biological research. However, no matter how big the impact of this technology can be to industry and research, it still is in a very early stage of development compared to traditional computer vision. One reason for this is that, the moment the camera goes into the water, a whole new set of challenges appear. On one hand, cameras have to be made waterproof, marine corrosion deteriorates materials quickly and access and modifications to experimental setups are costly, both in time and resources. On the other hand, the physical properties of the water make light behave differently, changing the appearance of a same object with variations of depth, organic material, currents, temperature etc.\n\n\nIn air, light comes from the whole hemisphere on cloudy days, and is dominated by the sun. In water lightning comes from a finite cone above the scene. This phenomenon is called Snell's window.\n\nAs opposite to air, water attenuates light exponentially. This results in hazy images with very low contrast. The main reasons for light attenuation are light absorption (where energy is removed from the light) and light scattering, by which the direction of light is changed. Light scattering can further be divided into forward scattering, which results in an increased blurriness and backward scattering that limits the contrast and is responsible for the characteristic veil of underwater images. Both scattering and attenuation are heavily influenced by the amount of organic matter dissolved in the water.\n\nAnother problem with water is that light attenuation is a function of the wavelength. This means that different color get attenuated faster than others, leading to color degradation. Red and orange light is the first to get attenuated, followed by yellows and greens. Blue is the less attenuated wavelength.\n\nIn high level computer vision, human structures are frequently used as image features for image matching in different applications. However, the sea bottom lacks such features, making it hard to find correspondences in two images.\n\nIn order to be able to use a camera in the water, a watertight housing is required. However, refraction will happen at the water-glass and glass-air interface due to differences in density of the materials. This has the effect of introducing a non-linear image deformation.\n\nThe motion of the vehicle presents another special challenge. Underwater vehicles are constantly moving due to currents and other phenomena. This introduces another uncertainty to algorithms, where small motions may appear in all directions. This can be specially important for video tracking. In order to reduce this problem image stabilization algorithms may be applied.\n\nImage restoration aims to model the degradation process and then invert it, obtaining the new image after solving. It is generally a complex approach that requires plenty of parameters that vary a lot between different water conditions.\n\nImage enhancement on the opposite only tries to provide a visually more appealing image without taking the physical image formation process into account. These methods are usually simpler and less computational intensive.\n\nDifferent algorithms exist that perform automatic color correction. The UCM (Unsupervised Color Correction Method), for example, does this in the following steps: It firstly reduces the color cast by equalizing the color values. Then it enhances contrast by stretching the red histogram towards the maximum and finally Saturation and Intensity components are optimized.\n\nIt is usually assumed that stereo cameras have been calibrated previously, geometrically and radiometrically. This leads to the assumption that corresponding pixels should have the same color. However this can not be guaranteed in an underwater scene, because of dispersion and backscatter as mentioned earlier. However, it is possible to model this phenomenon and create a virtual image with those effects removed\n\nIn recent years imaging sonars have become more and more accessible and gained resolution, delivering better images. Sidescan sonars are used to produce complete maps of regions of the sea floor stitching together sequences of sonar images. However, imaging sonar images often lack proper contrast and are degraded by artefacts and distortions due to noise, attitude changes of the AUV/ROV carrying the sonar or non uniform beam patterns. Another common problem with sonar computer vision is the comparatively low frame rate of sonar images.\n", "id": "44722153", "title": "Underwater computer vision"}
{"url": "https://en.wikipedia.org/wiki?curid=10531718", "text": "Graph cuts in computer vision\n\nAs applied in the field of computer vision, graph cuts can be employed to efficiently solve a wide variety of low-level computer vision problems (\"early vision\"), such as image smoothing, the stereo correspondence problem, image segmentation, and many other computer vision problems that can be formulated in terms of energy minimization. Many of these energy minimization problems can be approximated by solving a maximum flow problem in a graph (and thus, by the max-flow min-cut theorem, define a minimal cut of the graph). Under most formulations of such problems in computer vision, the minimum energy solution corresponds to the maximum a posteriori estimate of a solution. Although many computer vision algorithms involve cutting a graph (e.g., normalized cuts), the term \"graph cuts\" is applied specifically to those models which employ a max-flow/min-cut optimization (other graph cutting algorithms may be considered as graph partitioning algorithms).\n\n\"Binary\" problems (such as denoising a binary image) can be solved exactly using this approach; problems where pixels can be labeled with more than two different labels (such as stereo correspondence, or denoising of a grayscale image) cannot be solved exactly, but solutions produced are usually near the global optimum.\n\nThe theory of graph cuts was first applied in computer vision in the seminal paper by Greig, Porteous and Seheult of Durham University. In the Bayesian statistical context of smoothing noisy (or corrupted) images, they showed how the maximum a posteriori estimate of a binary image can be obtained \"exactly\" by maximizing the flow through an associated image network, involving the introduction of a \"source\" and \"sink\". The problem was therefore shown to be efficiently solvable. Prior to this result, \"approximate\" techniques such as simulated annealing (as proposed by the Geman brothers), or iterated conditional modes (a type of greedy algorithm as suggested by Julian Besag) were used to solve such image smoothing problems.\n\nAlthough the general formula_1-colour problem remains unsolved for formula_2 the approach of Greig, Porteous and Seheult has turned out to have wide applicability in general computer vision problems. Greig, Porteous and Seheult approaches are often applied iteratively to a sequence of binary problems, usually yielding near optimal solutions.\n\n\n\n\nformula_9\nwhere the energy formula_10 is composed of 2 different models (formula_11 and formula_12):\n\nformula_11 — unary term describing the likelihood of each color.\n\n\n\n\n\nformula_12 — binary term describing the coherence between neighborhood pixels.\n\nGraph cuts methods have become popular alternatives to the level set-based approaches for optimizing the location of a contour (see for an extensive comparison). However, graph cut approaches have been criticized in the literature for several issues:\n\n\nThe Boykov-Kolmogorov algorithm is an efficient way to compute the max-flow for computer vision related graph.\n\nThe Sim Cut algorithm approximates the graph cut by the solution of the set of simultaneous non-linear equations based on the analog electrical model of the flow network. Acceleration of the algorithm is possible through parallel computing.\n\n", "id": "10531718", "title": "Graph cuts in computer vision"}
{"url": "https://en.wikipedia.org/wiki?curid=155555", "text": "Image registration\n\nImage registration is the process of transforming different sets of data into one coordinate system. Data may be multiple photographs, data from different sensors, times, depths, or viewpoints. It is used in computer vision, medical imaging, military automatic target recognition, and compiling and analyzing images and data from satellites. Registration is necessary in order to be able to compare or integrate the data obtained from these different measurements.\n\nImage registration or image alignment algorithms can be classified into intensity-based and feature-based. One of the images is referred to as the \"reference\" or \"source\" and the others are referred to as the \"target\", \"sensed\" or \"subject\" images. Image registration involves spatially transforming the source/reference image(s) to align with the target image. The reference frame in the target image is stationary, while the other datasets are transformed to match to the target. Intensity-based methods compare intensity patterns in images via correlation metrics, while feature-based methods find correspondence between image features such as points, lines, and contours. Intensity-based methods register entire images or sub-images. If sub-images are registered, centers of corresponding sub images are treated as corresponding feature points. Feature-based methods establish a correspondence between a number of especially distinct points in images. Knowing the correspondence between a number of points in images, a geometrical transformation is then determined to map the target image to the reference images, thereby establishing point-by-point correspondence between the reference and target images.\n\nImage registration algorithms can also be classified according to the transformation models they use to relate the target image space to the reference image space. The first broad category of transformation models includes linear transformations, which include rotation, scaling, translation, and other affine transforms. Linear transformations are global in nature, thus, they cannot model local geometric differences between images.\n\nThe second category of transformations allow 'elastic' or 'nonrigid' transformations. These transformations are capable of locally warping the target image to align with the reference image. Nonrigid transformations include radial basis functions (thin-plate or surface splines, multiquadrics, and compactly-supported transformations), physical continuum models (viscous fluids), and large deformation models (diffeomorphisms).\n\nThere are a number of programs that implement both estimation and application of a warp-field. It is a part of the SPM and AIR programs.\n\nAlternatively, many advanced methods for spatial normalization are building on structure preserving transformations homeomorphisms and diffeomorphisms since they carry smooth submanifolds smoothly during transformation. Diffeomorphisms are generated in the modern field of Computational Anatomy based on flows since diffeomorphisms are not additive although they form a group, but a group under the law of function composition. For this reason, flows which generalize the ideas of additive groups allow for generating large deformations that preserve topology, providing 1-1 and onto transformations. Computational methods for generating such transformation are often called LDDMM which provide flows of diffeomorphisms as the main computational tool for connecting coordinate systems corresponding to the geodesic flows of Computational Anatomy.\n\nThere are a number of programs which generate diffeomorphic transformations of coordinates via diffeomorphic mapping including MRI Studio and MRI Cloud.org\n\nSpatial methods operate in the image domain, matching intensity patterns or features in images. Some of the feature matching algorithms are outgrowths of traditional techniques for performing manual image registration, in which an operator chooses corresponding control points (CP) in images. When the number of control points exceeds the minimum required to define the appropriate transformation model, iterative algorithms like RANSAC can be used to robustly estimate the parameters of a particular transformation type (e.g. affine) for registration of the images.\n\nFrequency-domain methods find the transformation parameters for registration of the images while working in the transform domain. Such methods work for simple transformations, such as translation, rotation, and scaling. Applying the phase correlation method to a pair of images produces a third image which contains a single peak. The location of this peak corresponds to the relative translation between the images. Unlike many spatial-domain algorithms, the phase correlation method is resilient to noise, occlusions, and other defects typical of medical or satellite images. Additionally, the phase correlation uses the fast Fourier transform to compute the cross-correlation between the two images, generally resulting in large performance gains. The method can be extended to determine rotation and scaling differences between two images by first converting the images to log-polar coordinates. Due to properties of the Fourier transform, the rotation and scaling parameters can be determined in a manner invariant to translation.\n\nAnother classification can be made between single-modality and multi-modality methods. Single-modality methods tend to register images in the same modality acquired by the same scanner/sensor type, while multi-modality registration methods tended to register images acquired by different scanner/sensor types.\n\nMulti-modality registration methods are often used in medical imaging as images of a subject are frequently obtained from different scanners. Examples include registration of brain CT/MRI images or whole body PET/CT images for tumor localization, registration of contrast-enhanced CT images against non-contrast-enhanced CT images for segmentation of specific parts of the anatomy, and registration of ultrasound and CT images for prostate localization in radiotherapy.\n\nRegistration methods may be classified based on the level of automation they provide. Manual, interactive, semi-automatic, and automatic methods have been developed. Manual methods provide tools to align the images manually. Interactive methods reduce user bias by performing certain key operations automatically while still relying on the user to guide the registration. Semi-automatic methods perform more of the registration steps automatically but depend on the user to verify the correctness of a registration. Automatic methods do not allow any user interaction and perform all registration steps automatically.\n\nImage similarities are broadly used in medical imaging. An image similarity measure quantifies the degree of similarity between intensity patterns in two images. The choice of an image similarity measure depends on the modality of the images to be registered. Common examples of image similarity measures include cross-correlation, mutual information, sum of squared intensity differences, and ratio image uniformity. Mutual information and normalized mutual information are the most popular image similarity measures for registration of multimodality images. Cross-correlation, sum of squared intensity differences and ratio image uniformity are commonly used for registration of images in the same modality.\n\nMany new features have been derived for cost functions based on matching methods via large deformations have emerged in the field Computational Anatomy including \n\nThere is a level of uncertainty associated with registering images that have any spatio-temporal differences. A confident registration with a measure of uncertainty is critical for many change detection applications such as medical diagnostics.\n\nIn remote sensing applications where a digital image pixel may represent several kilometers of spatial distance (such as NASA's LANDSAT imagery), an uncertain image registration can mean that a solution could be several kilometers from ground truth. Several notable papers have attempted to quantify uncertainty in image registration in order to compare results. However, many approaches to quantifying uncertainty or estimating deformations are computationally intensive or are only applicable to limited sets of spatial transformations.\n\nImage registration has applications in remote sensing (cartography updating), and computer vision. Due to the vast range of applications to which image registration can be applied, it is impossible to develop a general method that is optimized for all uses.\n\nMedical image registration (for data of the same patient taken at different points in time such as change detection or tumor monitoring) often additionally involves \"elastic\" (also known as \"nonrigid\") registration to cope with deformation of the subject (due to breathing, anatomical changes, and so forth). Nonrigid registration of medical images can also be used to register a patient's data to an anatomical atlas, such as the Talairach atlas for neuroimaging.\n\nIn astrophotography image alignment and stacking are often used to increase the signal to noise ratio for faint objects. Without stacking it may be used to produce a timelapse of events such as a planets rotation of a transit across the Sun. Using control points (automatically or manually entered), the computer performs transformations on one image to make major features align with a second or multiple images. This technique may also be used for images of different sizes, to allow images taken through different telescopes or lenses to be combined.\n\nImage registration is an essential part of panoramic image creation. There are many different techniques that can be implemented in real time and run on embedded devices like cameras and camera-phones.\n\n\n", "id": "155555", "title": "Image registration"}
{"url": "https://en.wikipedia.org/wiki?curid=40409788", "text": "Convolutional neural network\n\nIn machine learning, a convolutional neural network (CNN, or ConvNet) is a class of deep, feed-forward artificial neural networks that has successfully been applied to analyzing visual imagery.\n\nCNNs use a variation of multilayer perceptrons designed to require minimal preprocessing. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics.\n\nConvolutional networks were inspired by biological processes in which the connectivity pattern between neurons is inspired by the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\n\nCNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns the filters that in traditional algorithms were hand-engineered. This independence from prior knowledge and human effort in feature design is a major advantage.\n\nThey have applications in image and video recognition, recommender systems and natural language processing.\n\nA CNN consists of an input and an output layer, as well as multiple hidden layers. The hidden layers of a CNN typically consist of convolutional layers, pooling layers, fully connected layers and normalization layers.\n\nDescription of the process as a convolution in neural networks is by convention. Mathematically it is a cross-correlation rather than a convolution. This only has significance for the indices in the matrix, and thus which weights are placed at which index.\n\nConvolutional layers apply a convolution operation to the input, passing the result to the next layer. The convolution emulates the response of an individual neuron to visual stimuli.\n\nEach convolutional neuron processes data only for its receptive field.\n\nAlthough fully connected feedforward neural networks can be used to learn features as well as classify data, it is not practical to apply this architecture to images. A very high number of neurons would be necessary, even in a shallow (opposite of deep) architecture, due to the very large input sizes associated with images, where each pixel is a relevant variable. For instance, a fully connected layer for a (small) image of size 100 x 100 has 10000 weights. The convolution operation brings a solution to this problem as it reduces the number of free parameters, allowing the network to be deeper with fewer parameters. For instance, regardless of image size, tiling regions of size 5 x 5, each with the same shared weights, requires only 25 learnable parameters. In this way, it resolves the vanishing or exploding gradients problem in training traditional multi-layer neural networks with many layers by using backpropagation.\n\nConvolutional networks may include local or global pooling layers, which combine the outputs of neuron clusters at one layer into a single neuron in the next layer. For example, \"max pooling\" uses the maximum value from each of a cluster of neurons at the prior layer. Another example is \"average pooling\", which uses the average value from each of a cluster of neurons at the prior layer.\n\nFully connected layers connect every neuron in one layer to every neuron in another layer. It is in principle the same as the traditional multi-layer perceptron neural network (MLP).\n\nCNNs share weights in convolutional layers, which means that the same filter (weights bank) is used for each receptive field in the layer; this reduces memory footprint and improves performance..\n\nCNN design follows vision processing in living organisms.\n\nWork by Hubel and Wiesel in the 1950s and 1960s showed that cat and monkey visual cortexes contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field. Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space. The cortex in each hemisphere represents the contralateral visual field.\n\nTheir 1968 paper identified two basic visual cell types in the brain:\n\n\nThe neocognitron was introduced in 1980. The neocognitron does not require units located at multiple network positions to have the same trainable weights. This idea appears in 1986 in the book version of the original backpropagation paper. Neocognitrons were developed in 1988 for temporal signals. Their design was improved in 1998, generalized in 2003 and simplified in the same year.\n\nLeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1998, that classifies digits, was applied by several banks to recognise hand-written numbers on checks (cheques) digitized in 32x32 pixel images. The ability to process higher resolution images requires larger and more convolutional layers, so this technique is constrained by the availability of computing resources.\n\nSimilarly, a shift invariant neural network was proposed for image character recognition in 1988. The architecture and training algorithm were modified in 1991 and applied for medical image processing and automatic detection of breast cancer in mammograms.\n\nA different convolution-based design was proposed in 1988 for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.\n\nThe feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated.\n\nFollowing the 2005 paper that established the value of GPGPU for machine learning, several publications described more efficient ways to train convolutional neural networks using GPUs. In 2011, they were refined and implemented on a GPU, with impressive results. In 2012, Ciresan et al. significantly improved on the best performance in the literature for multiple image databases, including the MNIST database, the NORB database, the HWDB1.0 dataset (Chinese characters), the CIFAR10 dataset (dataset of 60000 32x32 labeled RGB images), and the ImageNet dataset.\n\nWhile traditional multilayer perceptron (MLP) models were successfully used for image recognition, due to the full connectivity between nodes they suffer from the curse of dimensionality, and thus do not scale well to higher resolution images.\n\nFor example, in CIFAR-10, images are only of size 32x32x3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in a first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200x200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.\n\nAlso, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. Thus, full connectivity of neurons is wasteful for the purposes such as image recognition that are dominated by spatially local input patterns.\n\nConvolutional neural networks are biologically inspired variants of multilayer perceptrons, designed to emulate the behaviour of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:\nTogether, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.\n\nA CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. We discuss them further below:\n\nThe convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the entries of the filter and the input and producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.\n\nStacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input and shares parameters with neurons in the same activation map.\n\nWhen dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume. The extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.\n\nThree hyperparameters control the size of the output volume of the convolutional layer: the depth, stride and zero-padding.\nThe spatial size of the output volume can be computed as a function of the input volume size formula_1, the kernel field size of the Conv Layer neurons formula_2, the stride with which they are applied formula_3, and the amount of zero padding formula_4 used on the border. The formula for calculating how many neurons \"fit\" in a given volume is given by formula_5. If this number is not an integer, then the strides are set incorrectly and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be formula_6 when the stride is formula_7 ensures that the input volume and output volume will have the same size spatially. Though it's generally not completely necessary to use up all of the neurons of the previous layer, for example, you may decide to use just a portion of padding.\n\nA parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on one reasonable assumption: That if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. In other words, denoting a single 2-dimensional slice of depth as a depth slice, we constrain the neurons in each depth slice to use the same weights and bias.\n\nSince all neurons in a single depth slice share the same parameters, then the forward pass in each depth slice of the CONV layer can be computed as a convolution of the neuron's weights with the input volume (hence the name: convolutional layer). Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.\n\nSometimes the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure, in which we expect completely different features to be learned on different spatial locations. One practical example is when the input are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a locally connected layer.\n\nAnother important concept of CNNs is pooling, which is a form of non-linear down-sampling. There are several non-linear functions to implement pooling among which \"max pooling\" is the most common. It partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum. The intuition is that the exact location of a feature is less important than its rough location relative to other features. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters and amount of computation in the network, and hence to also control overfitting. It is common to periodically insert a pooling layer between successive convolutional layers in a CNN architecture. The pooling operation provides another form of translation invariance.\n\nThe pooling layer operates independently on every depth slice of the input and resizes it spatially. The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples at every depth slice in the input by 2 along both width and height, discarding 75% of the activations. In this case, every max operation is over 4 numbers. The depth dimension remains unchanged.\n\nIn addition to max pooling, the pooling units can use other functions, such as average pooling or L2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which works better in practice.\n\nDue to the aggressive reduction in the size of the representation, the trend is towards using smaller filters or discarding the pooling layer altogether.\nRegion of Interest pooling (also known as RoI pooling) is a variant of max pooling, in which output size is fixed and input rectangle is a parameter.\n\nPooling is an important component of convolutional neural networks for object detection based on Fast R-CNN architecture.\n\nReLU is the abbreviation of Rectified Linear Units. This layer applies the non-saturating activation function formula_8. It increases the nonlinear properties of the decision function and of the overall network without affecting the receptive fields of the convolution layer.\n\nOther functions are also used to increase nonlinearity, for example the saturating hyperbolic tangent formula_9, formula_10, and the sigmoid function formula_11. ReLU is often preferred to other functions, because it trains the neural network several times faster without a significant penalty to generalisation accuracy.\n\nFinally, after several convolutional and max pooling layers, the high-level reasoning in the neural network is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular neural networks. Their activations can hence be computed with a matrix multiplication followed by a bias offset.\n\nThe loss layer specifies how training penalizes the deviation between the predicted and true labels and is normally the final layer. Various loss functions appropriate for different tasks may be used there. Softmax loss is used for predicting a single class of K mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting K independent probability values in formula_12. Euclidean loss is used for regressing to real-valued labels formula_13.\n\nCNNs use more hyperparameters than a standard MLP. While the usual rules for learning rates and regularization constants still apply, the following should be kept in mind when optimising.\n\nSince feature map size decreases with depth, layers near the input layer will tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the feature x pixel position product is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.\n\nThe number of feature maps directly controls capacity and depends on the number of available examples and task complexity.\n\nCommon field shapes found in the literature vary greatly, and are usually chosen based on the dataset.\n\nThe challenge is thus to find the right level of granularity so as to create abstractions at the proper scale, given a particular dataset.\n\nTypical values are 2x2. Very large input volumes may warrant 4x4 pooling in the lower-layers. However, choosing larger shapes will dramatically reduce the dimension of the signal, and may result in excess information loss. Often, non-overlapping pooling windows perform best.\n\nRegularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. ANNs use various types of regularization.\n\nBecause a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting is dropout. At each training stage, individual nodes are either \"dropped out\" of the net with probability formula_14 or kept with probability formula_15, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.\n\nIn the training stages, the probability that a hidden node will be dropped is usually 0.5; for input nodes, this should be much lower, intuitively because information is directly lost when input nodes are ignored.\n\nAt testing time after training has finished, we would ideally like to find a sample average of all possible formula_16 dropped-out networks; unfortunately this is unfeasible for large values of formula_17. However, we can find an approximation by using the full network with each node's output weighted by a factor of formula_15, so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates formula_16 neural nets, and as such allows for model combination, at test time only a single network needs to be tested.\n\nBy avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes model combination practical, even for deep neural nets. The technique seems to reduce node interactions, leading them to learn more robust features that better generalize to new data.\n\nDropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability formula_14. Each unit thus receives input from a random subset of units in the previous layer.\n\nDropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.\n\nA major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.\n\nIn stochastic pooling, the conventional deterministic pooling operations are replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. The approach is hyperparameter free and can be combined with other regularization approaches, such as dropout and data augmentation.\n\nAn alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images, which delivers excellent MNIST performance. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.\n\nSince the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Since these networks are usually trained with all available data, one approach is to either generate new data from scratch (if possible) or perturb existing data to create new ones. For example, input images could be asymmetrically cropped by a few percent to create new examples with the same label as the original.\n\nOne of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.\n\nAnother simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a \"zero norm\".\n\nA simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant, thus increasing the penalty for large weight vectors.\n\nL2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.\n\nL1 regularization is another common form. It is possible to combine L1 with L2 regularization (this is called Elastic net regularization). The L1 regularization leads the weight vectors to become sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs.\n\nAnother form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector formula_21 of every neuron to satisfy formula_22. Typical values of formula_23 are order of 3–4. Some papers report improvements when using this form of regularization.\n\nPooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.\n\nCurrently, the common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and to use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.\n\nThus, one way of representing something is to embed the coordinate frame within it. Once this is done, large features can be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). Using this approach ensures that the higher level entity (e.g. face) is present when the lower level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (\"pose vectors\") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.\n\nCNNs are often used in image recognition systems. In 2012 an error rate of 0.23 percent on the MNIST database was reported. Another paper on using CNN for image classification reported that the learning process was \"surprisingly fast\"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database.\n\nWhen applied to facial recognition, CNNs achieved a large decrease in error rate. Another paper reported a 97.6 percent recognition rate on \"5,600 still images of more than 10 subjects\". CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.\n\nThe ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014, a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.\n\nIn 2015 a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded with competitive performance. The network trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.\n\nCompared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space. Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream. Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines and Independent Subspace Analysis.\n\nCNNs have also explored natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing, search query retrieval, sentence modeling, classification, prediction and other traditional NLP tasks.\n\nCNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based rational drug design. The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures, AtomNet discovers chemical features, such as aromaticity, sp3 carbons and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus and multiple sclerosis.\n\nCNNs have been used in the game of checkers. From 1999–2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checkers using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and . Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%. It also earned a win against the program Chinook at its \"expert\" level of play.\n\nCNNs have been used in computer Go. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play. Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.\n\nA couple of CNNs for choosing moves to try (\"policy network\") and evaluating positions (\"value network\") driving MCTS were used by AlphaGo, the first to beat the best human player at the time.\n\nFor many applications, little training data is available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights. This allows convolutional networks to be successfully applied to problems with small training sets.\n\nA deep Q-network (DQN) is a type of deep learning model that combines a deep CNN with Q-learning, a form of reinforcement learning. Unlike earlier reinforcement learning agents, DQNs can learn directly from high-dimensional sensory inputs.\n\nPreliminary results were presented in 2014, with an accompanying paper in February 2015. The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it.\n\nConvolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR have been obtained using CDBNs.\n\nA time delay neural network allows speech signals to be processed time-invariantly, analogous to the translation invariance offered by CNNs. They were introduced in the early 1980s. The tiling of neuron outputs can cover timed stages.\n\n\n\nConvolutional neural networks are mentioned in the 2017 novel \"Infinity Born.\"\n\n\n", "id": "40409788", "title": "Convolutional neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=5104401", "text": "Outline of computer vision\n\nThe following outline is provided as an overview of and topical guide to computer vision:\n\nComputer vision – interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. Computer vision tasks include methods for acquiring digital images (through image sensors), image processing, and image analysis, to reach an understanding of digital images. In general, it deals with the extraction of high-dimensional data from the real world in order to produce numerical or symbolic information that the computer can interpret. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images.\n\n\nHistory of computer vision\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "5104401", "title": "Outline of computer vision"}
{"url": "https://en.wikipedia.org/wiki?curid=51396023", "text": "Dynamic Graphics Project\n\nThe Dynamic Graphics Project (commonly referred to as dgp) is an interdisciplinary research laboratory at the University of Toronto devoted to projects involving Computer Graphics, Computer Vision, and Human Computer Interaction. The lab began as the computer graphics research group of Computer Science Professor Leslie Mezei in 1967. Mezei invited Bill Buxton, a pioneer of human–computer interaction to join. In 1972, Ronald Baecker, another HCI pioneer joined dgp, establishing dgp as the first Canadian university group focused on computer graphics and human-computer interaction.\n\nSince then, dgp has hosted many well known faculty and students in computer graphics, computer vision and HCI (e.g., Alain Fournier, Bill Reeves, Jos Stam, Demetri Terzopoulos, Marilyn Tremaine). dgp also occasionally hosts artists in residence (e.g., Oscar-winner Chris Landreth). Many past and current researchers at Autodesk (and before that Alias Wavefront) graduated after working at dgp. dgp is located in the St. George Campus of University of Toronto in the Bahen Centre for Information Technology. dgp researchers regularly publish at ACM SIGGRAPH, ACM SIGCHI and ICCV.\n\ndgp hosts the Toronto User Experience (TUX) Speaker Series and the Sanders Series Lectures \n\n", "id": "51396023", "title": "Dynamic Graphics Project"}
{"url": "https://en.wikipedia.org/wiki?curid=40914533", "text": "Vicarious (company)\n\nVicarious is an artificial intelligence company based in the San Francisco Bay Area, California. They are using the theorized computational principles of the brain to build software that can think and learn like a human.\n\nThe company was founded in 2010 by D. Scott Phoenix and Dr. Dileep George. Before co-founding Vicarious, Phoenix was Entrepreneur in Residence at Founders Fund and CEO of Frogmetrics, a touchscreen analytics company he co-founded through the Y Combinator incubator program. Previously, Dr. George was Chief Technology Officer at Numenta, a company he co-founded with Jeff Hawkins and Donna Dubinsky (PALM, Handspring) while completing his PhD at Stanford University.\n\nThe company launched in February 2011 with funding from Founders Fund, Dustin Moskovitz, Adam D’Angelo (former Facebook CTO and co-founder of Quora), Felicis Ventures, and Palantir co-founder Joe Lonsdale. In August 2012, in its Series A round of funding, it raised an additional $15 million. The round was led by Good Ventures; Founders Fund, Open Field Capital and Zarco Investment Group also participated.\n\nThe company received $40 million in its Series B round of funding. The round was led by such notables as Mark Zuckerberg, Elon Musk, Peter Thiel, Vinod Khosla, and Ashton Kutcher. An additional undisclosed amount was later contributed by Amazon.com CEO Jeff Bezos, Yahoo! co-founder Jerry Yang, Skype co-founder Janus Friis and Salesforce.com CEO Marc Benioff.\n\nVicarious is developing machine learning software based on the computational principles of the human brain. Known as the Recursive Cortical Network (RCN), it is a visual perception system that interprets the contents of photographs and videos in a manner similar to humans. The system is powered by a balanced approach that takes sensory data, mathematics, and biological plausibility into consideration.\nOn October 22, 2013, beating CAPTCHA, Vicarious announced its AI was reliably able to solve modern CAPTCHAs, with character recognition rates of 90% or better. However, Luis von Ahn, a pioneer of early CAPTCHA and founder of reCAPTCHA, expressed skepticism, stating: \"It's hard for me to be impressed since I see these every few months.\" He pointed out that 50 similar claims to that of Vicarious had been made since 2003. Vicarious later published their findings in peer-reviewed journal Science showing they had definitively cracked the popular Turing test.\n\nVicarious has indicated that its AI was not specifically designed to complete CAPTCHAs and its success at the task is a product of its advanced vision system. Because Vicarious's algorithms are based on insights from the human brain, it is also able to recognize photographs, videos, and other visual data. In October 2017, According to the reports, Vicarious modified machine learning technique capable of solving CAPTCHA challenges using a novel network layout called Recursive Cortical Network. \n\n", "id": "40914533", "title": "Vicarious (company)"}
{"url": "https://en.wikipedia.org/wiki?curid=40671192", "text": "Umoove\n\nUmoove is a high tech startup company that has developed and patented a software-only face and eye tracking technology. The idea was first conceived as an attempt to aid disabled people but has since evolved. The only compatibility qualification for tablet computers and smartphones to run Umoove software is a front-facing camera. \nUmoove headquarters are in Israel on Jerusalem’s Har Hotzvim.\n\nUmoove has 15 employees and received two million dollars in financing in 2012. The company’s original founders invested around $800,000 to start the business in 2010.\nIn 2013 Umoove was named one of the top three most promising Israeli start ups by Newsgeeks magazine.\nThe company also participated in the 2013 LeWeb conference in Paris, France, where innovative technology startups are showcased.\n\nThe technology uses information extracted from previous frames, such as the angle of the user’s head to predict where to look for facial targets in the next frame. This anticipation minimizes the amount of computation needed to scan each image.\nUmoove accounts for variances in environment, lighting conditions and user hand shake/movement. The technology is designed to provide a consistent experience, whether you’re in a brightly lit area or a darkened basement, and to work fluidly between them by adapting its processing when it detects color and brightness shifts. It uses an active stabilization technique to filter out natural body movements from an unstable camera in order to minimize false-positive motion detection.\n\nRunning the Umoove software on a Samsung Galaxy S3 is said to take up only 2% CPU.\nUmoove works exclusively with software and there is no hardware add-on necessary. It can be run on any smartphone or tablet computer that has a front-facing camera. Umoove claims that even a low-quality camera on an old device will run their software flawlessly.\n\nIn January 2014 Umoove released its first game onto the app store. The Umoove Experience game lets users control where they are 'flying' in the game through simple gestures and motions with their head. The avatar will basically go toward wherever the user looks. The game was created to showcase the technology for game developers but that did not stop some from criticizing its simplicity. Umoove also announced that they raised another one million dollars and that they are opening offices in Silicon Valley, California.\n\nIn February 2014, Umoove announced that their face-tracking software development kit is available for Android developers as well as iOS.\n\nThe Umoove Experience garnered mostly positive reviews from bloggers and mainstream media with some predicting that it could be the future of mobile gaming. Mashable wrote that Umoove’s technology could be the emergence of gesture recognition technology in the mobile space, similar to Kinect with console gaming and what Leap Motion has done with desktop computers.\n\nSome, however, remain skeptical. CNET, for example, did not give the game a positive review and called the eye tracking technology ‘freaky but cool’. They also noted that pioneering technologies have been known to fall short of expectations, citing Apple Inc’s Siri as an example. The technology blog GigaOM said that the Umoove Experience is ’awesome’ and technology evangelist Robert Scoble has called Umoove \"brilliant\".\n\nIn January 2015, Umoove released uHealth, an mobile application that uses eye tracking game-like exercise to challenge the user’s ability to be attentive, continuously focus, follow commands and avoid distractions. The app is designed in the form of two games, one to improve attention and another that hones focus.\nuHealth is a training tool, not a diagnostic. Umoove has stated that they want to use their technology for diagnosing neurological disorders but this will depend on clinical tests and FDA approval. The company cites the direct relationship between eye movements and brain activity as well as various vision based therapies have been backed by many scientific studies conducted over the past decades. uHealth is the first time this type of therapy is delivered right to the end user through a simple download.\n\nIn March 2013 there were rumors on the internet that Umoove would be the functioning software embedded into the Samsung Galaxy S4, which was due to launch that month. This rumor was perpetrated by, among others, New York Times, Techcrunch and Yahoo. \nOnce Samsung launched without the Umoove technology rumors about a potential collaboration with Apple Inc hit the web. It has been said that due to the fact that Apple Inc is losing market share and stock value to Samsung they will be more aggressive and eye tracking is a logical place to make that move.\n", "id": "40671192", "title": "Umoove"}
{"url": "https://en.wikipedia.org/wiki?curid=10019306", "text": "Digital graffiti\n\nDigital graffiti is the act of creating graffiti art using a computer vision system. Various groups and companies have pioneered digital graffiti since technology advances made it possible. Most notably is the Graffiti Research Lab based in the US with their L.A.S.E.R. Tag system.\n\nInspired by the New York laser graffiti movement, in 2008 the first commercially available digital graffiti wall was produced by Luma, named the YrWall. A specially adapted spray can emits IR light instead of paint, which is then tracked by a computer vision system to recreate the \"sprayed\" image onto the wall using a projector.\n\nAny system that allows art to be created on a large scale in a similar manner to more traditional graffiti falls under the heading digital graffiti.\n\nCisco Systems has released a mobile application called [Digital Graffiti] patented by Cisco Systems, Inc. to allow people to place messages of varying size, color, length of time visible, and viewing distance (say visible from 20 feet away) on a physical location, say a building, an office, a cubicle, or a specific location using their augmented reality mobile application. This message alerts other visitors approaching the message coordinates by playing the Cisco chime and the mobile user's country origin filter when the app was installed. It is like a virtual yellow stickie note, that can be delivered to an individual when they arrive at a message location. Digital Graffiti leverages the Cisco MSE location server (which tracks users mobile devices and provides x, y coordinates of the mobile devices over WiFi).\n\n\n", "id": "10019306", "title": "Digital graffiti"}
{"url": "https://en.wikipedia.org/wiki?curid=53065847", "text": "3D body scanning\n\n3D body scanning is a reliable application of various technologies such as Structured-light 3D scanner, 3D depth sensing, stereoscopic vision and others for ergonomic and anthropometric investigation of the human form as a point-cloud. The technology and practice within research has found 3D body scanning measurement extraction methodologies to be comparable to traditional anthropometric measurement techniques.\n\nWhile the technology is still developing in its application, the technology has successfully been applied in the areas of:\n\nHowever, despite the potential for the technology to have an impact in made-to-measure and mass customisation of items with ergonomic properties, 3D body scanning has yet to reach an early adopter or early majority stage of innovation diffusion. This in part due to the lack of ergonomic theory relating to how to identify key landmarks on the body morphology.\n\nA key limitation of 3D body scanning has been the upfront cost of the equipment, and skills by which to collect data and apply it to scientific and technical fields.\n\nAlthough the process has been established for a considerable amount of time with international conferences held annually for industry and academics (e.g. the International Conference and Exhibition on 3D Body Scanning Technologies), the protocol and process of how to scan individuals is yet to be universally formalised. However, earlier research \nhas proposed a standardised protocol of body scanning based on research and practice that demonstrates how non-standardised protocol and posture significantly influences body measurements; including the hip. \n", "id": "53065847", "title": "3D body scanning"}
{"url": "https://en.wikipedia.org/wiki?curid=230834", "text": "CAPTCHA\n\nA CAPTCHA (, an acronym for \"Completely Automated Public Turing test to tell Computers and Humans Apart\") is a type of challenge-response test used in computing to determine whether or not the user is human.\n\nThe term was coined in 2003 by Luis von Ahn, Manuel Blum, Nicholas J. Hopper, and John Langford. The most common type of CAPTCHA was first invented in 1997 by two groups working in parallel: (1) Mark D. Lillibridge, Martin Abadi, Krishna Bharat, and Andrei Z. Broder; and (2) Eran Reshef, Gili Raanan and Eilon Solan. This form of CAPTCHA requires that the user type the letters of a distorted image, sometimes with the addition of an obscured sequence of letters or digits that appears on the screen. Because the test is administered by a computer, in contrast to the standard Turing test that is administered by a human, a CAPTCHA is sometimes described as a reverse Turing test.\n\nThis user identification procedure has received many criticisms, especially from disabled people, but also from other people who feel that their everyday work is slowed down by distorted words that are difficult to read. It takes the average person approximately 10 seconds to solve a typical CAPTCHA.\n\nSince the early days of the Internet, users have wanted to make text illegible to computers. The first such people could be hackers, posting about sensitive topics to Internet forums they thought were being automatically monitored for keywords. To circumvent such filters, they would replace a word with look-alike characters. \"HELLO\" could become or , as well as numerous other variants, such that a filter could not possibly detect \"all\" of them. This later became known as leetspeak.\n\nOne of the earliest commercial uses of CAPTCHAs was in the Gausebeck-Levchin test. In 2000, idrive.com began to protect its signup page with a CAPTCHA and prepared to file a patent on this seemingly novel technique. In 2001, PayPal used such tests as part of a fraud prevention strategy in which they asked humans to \"retype distorted text that programs have difficulty recognizing.\"\nPayPal cofounder and CTO Max Levchin helped commercialize this early use.\n\nTwo teams of people have claimed to be the first to invent the CAPTCHAs used widely on the Web today. The first team consists of Mark D. Lillibridge, Martín Abadi, Krishna Bharat, and Andrei Broder, who used CAPTCHAs in 1997 at AltaVista to prevent bots from adding URLs to their web search engine. Looking for a way to make their images resistant to OCR attack, the team looked at the manual of their Brother scanner, which had recommendations for improving OCR's results (similar typefaces, plain backgrounds, etc.). The team created puzzles by attempting to simulate what the manual claimed would cause bad OCR.\n\nThe second team to claim inventorship of CAPTCHAs consists of Luis von Ahn, Manuel Blum, Nicholas J. Hopper, and John Langford, who first described CAPTCHAs in a 2003 publication and subsequently received much coverage in the popular press. Their notion of CAPTCHA covers any program that can distinguish humans from computers, including many different examples of CAPTCHAs.\n\nThe controversy of inventorship has been resolved by the existence of a 1997 priority date patent application by Eran Reshef, Gili Raanan and Eilon Solan who worked at Sanctum on Application Security Firewall. Their patent application details that \"The invention is based on applying human advantage in applying sensory and cognitive skills to solving simple problems that prove to be extremely hard for computer software. Such skills include, but are not limited to processing of sensory information such as identification of objects and letters within a noisy graphical environment\"; and a 1998 patent by Lillibridge, Abadi, Bharat, and Broder. Both patents predate other publications by several years, though they do not use the term CAPTCHA, they describe the ideas in detail and precisely depict the graphical CAPTCHAs used in the Web today.\n\nCAPTCHAs are, by definition, fully automated, requiring little human maintenance or intervention to administer, producing benefits in cost and reliability.\n\nBy definition, the algorithm used to create the CAPTCHA must be made public, though it may be covered by a patent. This is done to demonstrate that breaking it requires the solution to a difficult problem in the field of artificial intelligence (AI) rather than just the discovery of the (secret) algorithm, which could be obtained through reverse engineering or other means.\n\nModern text-based CAPTCHAs are designed such that they require the simultaneous use of three separate abilities—invariant recognition, segmentation, and parsing—to correctly complete the task with any consistency.\n\n\nEach of these problems poses a significant challenge for a computer, even in isolation. The presence of all three at the same time is what makes CAPTCHAs difficult to solve.\n\nUnlike computers, humans excel at this type of task. While segmentation and recognition are two separate processes necessary for understanding an image for a computer, they are part of the same process for a person. For example, when an individual understands that the first letter of a CAPTCHA is an “a”, that individual also understands where the contours of that “a” are, and also where it melds with the contours of the next letter. Additionally, the human brain is capable of dynamic thinking based upon context. It is able to keep multiple explanations alive and then pick the one that is the best explanation for the whole input based upon contextual clues. This also means it will not be fooled by variations in letters.\n\nWhile used mostly for security reasons, CAPTCHAs also serve as a benchmark task for artificial intelligence technologies. According to an article by Ahn, Blum and Langford, “Any program that passes the tests generated by a CAPTCHA can be used to solve a hard unsolved AI problem.”\n\nThey argue that the advantages of using hard AI problems as a means for security are twofold. Either the problem goes unsolved and there remains a reliable method for distinguishing humans from computers, or the problem is solved and a difficult AI problem is resolved along with it. In the case of image and text based CAPTCHAs, if an AI were capable of accurately completing the task without exploiting flaws in a particular CAPTCHA design, then it would have solved the problem of developing an AI that is capable of complex object recognition in scenes.\n\nCAPTCHAs based on reading text — or other visual-perception tasks — prevent blind or visually impaired users from accessing the protected resource. However, CAPTCHAs do not have to be visual. Any hard artificial intelligence problem, such as speech recognition, can be used as the basis of a CAPTCHA. Some implementations of CAPTCHAs permit users to opt for an audio CAPTCHA, though a 2011 paper demonstrated a technique for defeating the popular schemes at the time.\n\nFor non-sighted users (for example blind users, or color blind people on a color-using test), visual CAPTCHAs present serious problems. Because CAPTCHAs are designed to be unreadable by machines, common assistive technology tools such as screen readers cannot interpret them. Since sites may use CAPTCHAs as part of the initial registration process, or even every login, this challenge can completely block access. In certain jurisdictions, site owners could become targets of litigation if they are using CAPTCHAs that discriminate against certain people with disabilities. For example, a CAPTCHA may make a site incompatible with Section 508 in the United States. In other cases, those with sight difficulties can choose to identify a word being read to them.\n\nWhile providing an audio CAPTCHA allows blind users to read the text, it still hinders those who are both blind and deaf. According to sense.org.uk, about 4% of people over 60 in the UK have both vision and hearing impairments. There are about 23,000 people in the UK who have serious vision and hearing impairments. According to The National Technical Assistance Consortium for Children and Young Adults Who Are Deaf-Blind (NTAC), the number of deafblind children in the USA increased from 9,516 to 10,471 during the period 2004 to 2012. Gallaudet University quotes 1980 to 2007 estimates which suggest upwards of 35,000 fully deafblind adults in the USA. Deafblind population estimates depend heavily on the degree of impairment used in the definition.\n\nThe use of CAPTCHA thus excludes a small number of individuals from using significant subsets of such common Web-based services as PayPal, Gmail, Orkut, Yahoo!, many forum and weblog systems, etc.\n\nEven for perfectly sighted individuals, new generations of graphical CAPTCHAs, designed to overcome sophisticated recognition software, can be very hard or impossible to read.\n\nA method of improving the CAPTCHA to ease the work with it was proposed by ProtectWebForm and was called \"Smart CAPTCHA\". Developers advise to combine the CAPTCHA with JavaScript support. Since it is too hard for most of spam robots to parse and execute JavaScript, using a simple script which fills the CAPTCHA fields and hides the image and the field from human eyes was proposed.\n\nOne alternative method involves displaying to the user a simple mathematical equation and requiring the user to enter the solution as verification. Although these are much easier to defeat using software, they are suitable for scenarios where graphical imagery is not appropriate, and they provide a much higher level of accessibility for blind users than the image-based CAPTCHAs. These are sometimes referred to as MAPTCHAs (M = 'mathematical'). However, these may be difficult for users with a cognitive disorder.\n\nOther kinds of challenges, such as those that require understanding the meaning of some text (e.g., a logic puzzle, trivia question, or instructions on how to create a password) can also be used as a CAPTCHA. Again, there is little research into their resistance against countermeasures.\n\nThere are a few approaches to defeating CAPTCHAs: using cheap human labor to recognize them, exploiting bugs in the implementation that allow the attacker to completely bypass the CAPTCHA, and finally using machine learning to build an automated solver. According to former Google \"click fraud czar\" Shuman Ghosemajumder, there are numerous services which solve CAPTCHAs automatically.\n\nIn its earliest iterations there was not a systematic methodology for designing or evaluating CAPTCHAs. As a result, there were many instances in which CAPTCHAs were of a fixed length and therefore automated tasks could be constructed to successfully make educated guesses about where segmentation should take place. Other early CAPTCHAs contained limited sets of words, which made the test much easier to game. Still others made the mistake of relying too heavily on background confusion in the image. In each case, algorithms were created that were successfully able to complete the task by exploiting these design flaws. These methods proved brittle however, and slight changes to the CAPTCHA were easily able to thwart them. Modern CAPTCHAs like reCAPTCHA no longer rely just on fixed patterns but instead present variations of characters that are often collapsed together, making segmentation almost impossible. These newest iterations have been much more successful at warding off automated tasks.\n\nIn October 2013, artificial intelligence company Vicarious claimed that it had developed a generic CAPTCHA-solving algorithm that was able to solve modern CAPTCHAs with character recognition rates of up to 90%. However, Luis von Ahn, a pioneer of early CAPTCHA and founder of reCAPTCHA, expressed skepticism, stating: \"It's hard for me to be impressed since I see these every few months.\" He pointed out that 50 similar claims to that of Vicarious had been made since 2003.\n\nIn August 2014 at Usenix WoOT conference, Bursztein et al. presented the first generic CAPTCHA-solving algorithm based on reinforcement learning and demonstrated its efficiency against many popular CAPTCHA schemas. They concluded that text distortion based CAPTCHAs schemes should be considered insecure moving forward.\n\nIt may be possible to subvert CAPTCHAs by relaying them to a sweatshop of human operators who are employed to decode CAPTCHAs. A 2005 paper from a W3C working group stated that such an operator \"could easily verify hundreds of them each hour\". In 2010 the University of California at San Diego conducted a large scale study of those CAPTCHA's farms and found out that the retail price for solving one million CAPTCHAs is as low as $1,000.\n\nAnother technique used consists of using a script to re-post the target site's CAPTCHA as a CAPTCHA to a site owned by the attacker, which unsuspecting humans visit and correctly solve within a short while for the script to use.\nHowever, there is controversy around the economic viability of such an attack.\n\nHoward Yeend has identified two implementation issues with poorly designed CAPTCHA systems:\n\nSometimes, if part of the software generating the CAPTCHA is client-side (the validation is done on a server but the text that the user is required to identify is rendered on the client side), then users can modify the client to display the un-rendered text. Some CAPTCHA systems use MD5 hashes stored client-side, which may leave the CAPTCHA vulnerable to a brute-force attack.\n\nSome notable attacks against various CAPTCHAs schemas include:\n\nWith the demonstration that text distortion based CAPTCHAs are vulnerable to machine learning based attacks, some researchers have proposed alternatives including image recognition CAPTCHAs which require users to identify simple objects in the images presented. The argument in favor of these schemes is that tasks like object recognition are typically more complex to perform than text recognition and therefore should be more resilient to machine learning based attacks. Here are some of notable alternative CAPTCHA schemas:\n\n\n", "id": "230834", "title": "CAPTCHA"}
{"url": "https://en.wikipedia.org/wiki?curid=54397496", "text": "Eyes of Things\n\nEyes of Things (EoT) is the name of a Project funded by the European Union’s Horizon 2020 Research and Innovation Programme under grant agreement number 643924. The purpose of the project, which is funded under the Smart Cyber-physical systems topic, is to develop a generic hardware/software platform for embedded, wearable, mobile, computer vision. \n\nEoT is based on the following tenets:\n\n\n", "id": "54397496", "title": "Eyes of Things"}
{"url": "https://en.wikipedia.org/wiki?curid=54637700", "text": "Teknomo–Fernandez algorithm\n\nThe Teknomo–Fernandez algorithm (TF algorithm), is an efficient algorithm for generating the background image of a given video sequence.\n\nBy assuming that the background image is shown in the majority of the video, the algorithm is able to generate a good background image of a video in formula_1-time using only a small number of binary operations and Boolean Bit operations, which require a small amount of memory and has built-in operators found in many programming languages such as C, C++, and Java.\n\nPeople tracking from videos usually involves some form of background subtraction to segment foreground from background. Once foreground images are extracted, then desired algorithms (such as those for motion tracking, object tracking, and facial recognition) may be executed using these images.\n\nHowever, background subtraction requires that the background image is already available and unfortunately, this is not always the case. Traditionally, the background image is searched for manually or automatically from the video images when there are no objects. More recently, automatic background generation through object detection, medial filtering, medoid filtering, approximated median filtering, linear predictive filter, non-parametric model, Kalman filter, and adaptive smoothening have been suggested; however, most of these methods have high computational complexity and are resource-intensive.\n\nThe Teknomo–Fernandez algorithm is also an automatic background generation algorithm. Its advantage, however, is its computational speed of only formula_1-time, depending on the resolution formula_3 of an image and its accuracy gained within a manageable number of frames. Only at least three frames from a video is needed to produce the background image assuming that for every pixel position, the background occurs in the majority of the videos. Furthermore, it can be performed for both grayscale and colored videos.\n\n\nGenerally, however, the algorithm will certainly work whenever the following single important assumption holds: For each pixel position, the majority of the pixel values in the entire video contain the pixel value of the actual background image (at that position).As long as each part of the background is shown in the majority of the video, the entire background image needs not to appear in any of its frames. The algorithm is expected to work accurately.\n\n\nAt the first level, three frames are selected at random from the image sequence to produce a background image by combining them using the first equation. This yields a better background image at the second level. The procedure is repeated until desired level formula_13.\n\nAt level formula_14, the probability formula_15 that the modal bit predicted is the actual modal bit is represented by the equation formula_16.\nThe table below gives the computed probability values across several levels using some specific initial probabilities. It can be observed that even if the modal bit at the considered position is at a low 60% of the frames, the probability of accurate modal bit determination is already more than 99% at 6 levels.\nThe space requirement of the Teknomo–Fernandez algorithm is given by the function formula_17, depending on the resolution formula_3 of the image, the number formula_19 of frames in the video, and the desired number formula_13 of levels. However, the fact that formula_13 will probably not exceed 6 reduces the space complexity to formula_22.\n\nThe entire algorithm runs in formula_1-time, only depending on the resolution of the image. Computing the modal bit for each bit can be done in formula_24-time while the computation of the resulting image from the three given images can be done in formula_1-time. The number of the images to be processed in formula_13 levels is formula_27. However, since formula_28, then this is actually formula_24, thus the algorithm runs in formula_1.\n\nA variant of the Teknomo–Fernandez algorithm that incorporates the Monte-Carlo method named CRF has been developed. Two different configurations of CRF were implemented: CRF9,2 and CRF81,1. Experiments on some colored video sequences showed that the CRF configurations outperform the TF algorithm in terms of accuracy. However, the TF algorithm remains more efficient in terms of processing time.\n\n\n\n", "id": "54637700", "title": "Teknomo–Fernandez algorithm"}
{"url": "https://en.wikipedia.org/wiki?curid=48411649", "text": "Harris Corner Detector\n\nHarris Corner Detector is a corner detection operator that is commonly used in computer vision algorithms to extract corners and infer features of an image. It was first introduced by Chris Harris and Mike Stephens in 1988 upon the improvement of Moravec's corner detector. Compared to the previous one, Harris' corner detector takes the differential of the corner score into account with reference to direction directly, instead of using shifting patches for every 45 degree angles, and has been proved to be more accurate in distinguishing between edges and corners. Since then, it has been improved and adopted in many algorithms to preprocess images for subsequent applications.\n\nA corner is a point whose local neighborhood stands in two dominant and different edge directions. In other words, a corner can be interpreted as the junction of two edges, where an edge is a sudden change in image brightness. Corners are the important features in the image, and they are generally termed as interest points which are invariant to translation, rotation and illumination. Although corners are only a small percentage of the image, they contain the most important features in restoring image information, and they can be used to minimize the amount of processed data for motion tracking, image stitching, building 2D mosaics, stereo vision, image representation and other related computer vision areas.\n\nIn order to capture the corners from the image, researchers have proposed many different corner detectors including the Kanade-Lucas-Tomasi (KLT) operator and the Harris operator which are most simple, efficient and reliable for use in corner detection. These two popular methodologies are both closely associated with and based on the local structure matrix. Compared to the Kanade-Lucas-Tomasi corner detector, the Harris corner detector provides good repeatability under changing illumination and rotation, and therefore, it is more often used in stereo matching and image database retrieval. Although there still exists drawbacks and limitations, the Harris corner detector is still an important and fundamental technique for many computer vision applications.\n\nWithout loss of generality, we will assume a grayscale 2-dimensional image is used. Let this image be given by formula_1. Consider taking an image patch over the area formula_2 and shifting it by formula_3. The \"sum of squared differences\" (SSD) between these two patches, denoted formula_4, is given by:\n\nformula_6 can be approximated by a Taylor expansion. Let formula_7 and formula_8 be the partial derivatives of formula_9, such that\n\nThis produces the approximation\n\nwhich can be written in matrix form:\n\nwhere \"M\" is the structure tensor,\n\nCommonly, Harris corner detector algorithm can be divided into five steps.\n\n1. Color to grayscale\n\n2. Spatial derivative calculation\n\n3. Structure tensor setup\n\n4. Harris response calculation\n\n5. Non-maximum suppression\n\nIf we use Harris corner detector in a color image, the first step is to convert it into a grayscale image, which will enhance the processing speed.\n\nNext, we are going to compute formula_14 .\n\nWith formula_15 , formula_16, we can construct the structure tensor formula_17.\n\nIn this step, we will compute the smallest eigenvalue of the structure tensor with following approximation equation.\n\nformula_18\n\nwhere formula_19.\n\nAnother common used Harris response calculation is shown as below,\n\nformula_20\n\nwhere formula_21 is an empirically determined constant; formula_22 .\n\nIn order to pick up the optimal values to indicate corners, we find the local maxima as corners within the window which is a 3 by 3 filter.\n\n1. Harris-Laplace Corner Detector\n\n2. Differential Morphological Decomposition Based Corner Detector\n\n3. Multi-scale Bilatera Structure Tensor Based Corner Detector\n\n1. Image Alignment, Stitching and Registration\n\n2. 2D Mosaics Creation\n\n3. 3D Scene Modeling and Reconstruction\n\n4. Motion Detection\n\n5. Object Recognition\n\n6. Image Indexing and Content-based Retrieval\n\n7. Video Tracking\n\n\n", "id": "48411649", "title": "Harris Corner Detector"}
{"url": "https://en.wikipedia.org/wiki?curid=55517935", "text": "Wrnch\n\nwrnch is an AI and computer vision software engineering company based in Montreal, Quebec, Canada. The company was founded in 2014 by Paul Kruszewski, an AI technologist and serial entrepreneur. Wrnch's BodySLAM software is a deep learning AI engine for body tracking and human pose estimation.\n\nwrnch is Kruszewski's third AI startup. He founded BGT BioGraphic Technologies in 2000, GripHeavyIndustries in 2007, and wrnch in 2014. With each new business, Kruszewski developed more and more refined software specializing in crowd simulation, NPC behaviours, and human pose estimation.\n\nKruszewski founded wrnch in Montreal's TandemLaunch incubator. The company was initially focused on a video denoiser that it licensed from a local university. Denoisers remove unwanted video noise and grain from footage. In 2015, the company moved out of the incubator. In 2016, it licensed its denoising software to Red Giant.\n\nAfterwards, wrnch developed BodySLAM, a deep learning AI body-tracking software, with investment from Mark Cuban. As a human-machine interface, BodySLAM has multiple possible applications in various industries, such as health care, security, retail, and entertainment.\n\nBodySLAM runs under NVIDIA’s TensorRT and is thereby compatible with all CUDA-based GPUs, analyzing and returning motion data in real time.\n", "id": "55517935", "title": "Wrnch"}
{"url": "https://en.wikipedia.org/wiki?curid=56084743", "text": "3D selfie\n\nA 3D selfie is a 3D-printed scale replica of a person. These three-dimensional selfies are also known as 3D portraits, 3D figurines, 3D-printed figurines, mini-me figurines and miniature statues. The 3D figurine is usually printed in full color using gypsum-based binder jetting techniques, giving the figurine a sandstone-like texture and look. While the printing process is fairly standardized, the capture of a subject as a 3D model can be accomplished in many ways. Most systems use digital camera or cameras to take 2D pictures of the subject, under normal lighting, under projected light patterns, or a combination of these. A piece of software then reconstructs a 3D model of the subject from these pictures. \n\nInexpensive systems use a single camera which is moved around the subject in 360° at various heights, over minutes, while the subject stays immobile. More elaborate systems have a vertical bar of cameras rotate around the subject, usually achieving a full scan in 10 seconds. Most expensive systems have an enclosed 3D photo booth with 50 to 100 cameras statically embedded in walls and the ceiling, firing all at once, eliminating differences in image capture caused by movements of the subject. \n", "id": "56084743", "title": "3D selfie"}
{"url": "https://en.wikipedia.org/wiki?curid=56249073", "text": "Egocentric vision\n\nEgocentric vision or first-person vision is a sub-field of computer vision that entails analyzing images and videos captured by a wearable camera, which is typically worn on the head or on the chest and naturally approximates the visual field of the camera wearer. Consequently, visual data capture the part of the scene on which the user focuses to carry out the task at hand and offer a valuable perspective to understand the user's activities and their context in a naturalistic setting.\n\nThe wearable camera looking forwards is often supplemented with a camera looking inward at the user’s eye and able to measure a user’s eye gaze, which is useful to reveal attention and to better understand the\nuser’s activity and intentions.\n\nThe idea of using a wearable camera to gather visual data from a first-person perspective dates back to the 70s, when Steve Mann invented \"Eye Glass\", a device that, when worn, causes the human eye itself to effectively become both an electronic camera and a television display. But it was only after the introduction to the market of the Microsoft SenseCam in 2006 that wearable cameras were used for the first time in large scale experimental health research works. The interest of the computer vision community into the egocentric paradigm has been arising slowly entering the 2010s and it is rapidly growing in recent years, boosted by both the impressive advanced in the field of wearable technology and by the increasingly number of potential applications.\n\nThe prototypical first-person vision system described by Kanade and Hebert, in 2012 is composed by three basic components: a localization component able to estimate the surrounding, a recognition component able to identify object and people, and an activity recognition component, able to provide information about the current activity of the user. Together, these three components provide a complete situational awareness of the user, which in turn can be used to provide assistance to the itself or to the caregiver. Following this idea, the first computational techniques for egocentric analysis focused on hand-related activity recognition and social interaction analysis. Also, given the unconstrained nature of the video and the huge amount of data generated, temporal segmentation and summarization where among the first problem addressed. After almost ten years of egocentric vision (2007 - 2017), the field is still undergoing diversification. Emerging research topics include:\n\n\nToday's wearable cameras are small and lightweight digital recording devices that can acquire images and videos automatically, without the user intervention, with different resolutions and frame rates, and from a first-person point of view. Therefore, wearable cameras are naturally primed to gather visual information from our everyday interactions since they offer an intimate perspective of the visual field of the camera wearer.\n\nDepending on the frame rate, it is common to distinguish between photo-cameras (also called lifelogging cameras) and video-cameras. \n\nIn both cases, since the camera is worn in a naturalistic setting, visual data present a huge variability in terms of illumination conditions and object appearance.\nMoreover, the camera wearer is not visible in the image and what he/she is doing has to be inferred from the information in the visual field of the camera, implying that important information about the wearer, such for instance as pose or facial expression estimation, is not available.\n\nA collection of studies published in a special theme issue of the American Journal of Preventive Medicine has demonstrated the potential of lifelogs captured through wearable cameras from a number of viewpoints. In particular, it has been shown that used as a tool for understanding and tracking lifestyle behaviour, lifelogs would enable the prevention of noncommunicable diseases associated to unhealthy trends and risky profiles (such as obesity, depression, etc.). In addition, used as a tool of re-memory cognitive training, lifelogs would enable the prevention of cognitive and functional decline in elderly people.\n\nMore recently, egocentric cameras have been used to study human and animal cognition, human-human social interaction, human-robot interaction, human expertise in complex tasks.\nOther applications include navigation/assistive technologies for the blind, monitoring and assistance of industrial workflows.\n", "id": "56249073", "title": "Egocentric vision"}
{"url": "https://en.wikipedia.org/wiki?curid=47319644", "text": "Error level analysis\n\nError Level Analysis is the analysis of compression artifacts in digital data with lossy compression such as JPEG.\n\nWhen used, lossy compression is normally applied uniformly to a set of data, such as an image, resulting in a uniform level of compression artifacts.\n\nAlternatively, the data may consist of parts with different levels of compression artifacts. This difference may arise from the different parts having been repeatedly subjected to the same lossy compression a different number of times, or the different parts having been subjected to different kinds of lossy compression. A difference in the level of compression artifacts in different parts of the data may therefore indicate that the data have been edited.\n\nIn the case of JPEG, even a composite with parts subjected to matching compressions will have a difference in the compression artifacts.\n\nIn order to make the typically faint compression artifacts more readily visible, the data to be analyzed is subjected to an additional round of lossy compression, this time at a known, uniform level, and the result is subtracted from the original data under investigation. The resulting difference image is then inspected manually for any variation in the level of compression artifacts. In 2007, N. Krawetz denoted this method \"error level analysis\".\n\nAdditionally, digital data formats such as JPEG sometimes include metadata describing the specific lossy compression used. If in such data the observed compression artifacts differ from those expected from the given metadata description, then the metadata may not describe the actual compressed data, and thus indicate that the data have been edited.\n\nBy its nature, data without lossy compression, such as a PNG image, cannot be subjected to error level analysis. Consequently, since editing could have been performed on data without lossy compression with lossy compression applied uniformly to the edited, composite data, the presence of a uniform level of compression artifacts does not rule out editing of the data.\n\nAdditionally, any non-uniform compression artifacts in a composite may be removed by subjecting the composite to repeated, uniform lossy compression. Also, if the image color space is reduced to 256 colors or less, for example, by conversion to GIF, then error level analysis will generate useless results.\n\nMore significant, the actual interpretation of the level of compression artifacts in a given segment of the data is subjective, and the determination of whether editing has occurred is therefore not robust.\n\nIn May 2013, N. Krawetz used error level analysis on the 2012 World Press Photo of the Year and concluded on the Hacker Factor blog that it was \"a composite\" with modifications that \"fail to adhere to the acceptable journalism standards used by Reuters, Associated Press, Getty Images, National Press Photographer's Association, and other media outlets\". The World Press Photo organizers responded by letting two independent experts analyze the image files of the winning photographer and subsequently confirmed the integrity of the files. One of the experts, \nHany Farid, said about error level analysis that \"It incorrectly labels altered images as original and incorrectly labels original images as altered with the same likelihood\". Krawetz responded by clarifying that \"It is up to the user to interpret the results. Any errors in identification rest solely on the viewer\".\n\nIn May 2015, the citizen journalism team Bellingcat wrote that error level analysis revealed that the Russian Ministry of Defense had edited satellite images related to the Malaysia Airlines Flight 17 disaster. In a reaction to this, image forensics expert J. Kriese said about error level analysis: \"The method is subjective and not based entirely on science\", and that it is \"a method used by hobbyists\". On his Hacker Factor Blog the inventor of error level analysis N. Krawetz criticized both Bellingcat's use of error level analysis as \"misinterpreting the results\" but also on several points J. Kriese's \"ignorance\" regarding error level analysis.\n\n\n", "id": "47319644", "title": "Error level analysis"}
{"url": "https://en.wikipedia.org/wiki?curid=1247612", "text": "Intelligent Machines Research Corporation\n\nIntelligent Machines Research Corporation (IMR) was founded by David H. Shepard and William Lawless, Jr. in 1952 to commercialize the work Shepard had done with the help of Harvey Cook in building \"Gismo\", a machine later called the \"Analyzing Reader\".\n\nIBM obtained a license on IMR's patents in 1953 and in 1955 contracted with IMR to build a developmental system which was able to read constrained hand printed numeric characters if reasonably well formed. However, IBM did not market this system. In 1959 IBM did market a system of its own, classifying it as an Optical Character Recognition (OCR) system, and the term OCR from then on has been standard in the industry for this technology.\n\nIMR went on to deliver the world's first several commercially used systems, including one used by Readers Digest in its book subscription department. Readers Digest donated this system many years later to the Smithsonian, where it was once put on display. The second system was sold to the Standard Oil Company of California, as arranged by the Farrington Manufacturing Company, a leading company in the credit card business at that time, with many systems to read oil company credit cards to follow, one of which was also on display at the Smithsonian later on.\n\nIn 1959 Farrington acquired IMR, and the numeric font designed by Shepard, called Farrington 7B, has been standard for most of the well known credit cards since that time. Shepard later left Farrington and founded Cognitronics Corporation in 1962.\n\nBoth Shepard and Lawless had been NSA employees at one time. Lawless later held key positions in IBM.\n", "id": "1247612", "title": "Intelligent Machines Research Corporation"}
{"url": "https://en.wikipedia.org/wiki?curid=4103607", "text": "Dalle Molle Institute for Artificial Intelligence Research\n\nThe Dalle Molle Institute for Artificial Intelligence Research (, IDSIA) is a research institution in Manno, in the district of Lugano, in Ticino in southern Switzerland. It was founded in 1988 by Angelo Dalle Molle through the private Fondation Dalle Molle. In 2000 it became a public research institute, affiliated with the University of Lugano and SUPSI in Ticino, Switzerland. In 1997 it was listed among the top ten artificial intelligence laboratories, and among the top four in the field of biologically-inspired AI.\nIn 2007 a robotics lab with focus on intelligent and learning robots, especially in the fields of swarm and humanoid robotics, was established. \n\nBetween 2009 and 2012, artificial neural networks developed at the institute won eight international competitions in pattern recognition and machine learning.\n\nIDSIA is one of four Swiss research organisations founded by the Dalle Molle foundation, of which three are in the field of artificial intelligence..\n", "id": "4103607", "title": "Dalle Molle Institute for Artificial Intelligence Research"}
{"url": "https://en.wikipedia.org/wiki?curid=4225178", "text": "TASSL\n\nThe Application Software Systems Laboratory (TASSL) is a research lab, as a part of Center for Advanced Information Processing (CAIP), and Department of Electrical and Computing Engineering at Rutgers University . It is under the direction of Dr. Manish Parashar and the current research fields include Autonomic Computing, Parallel Computing and Distributed Computing, Grid Computing, Peer-to-peer Computing, Adaptive Computing Systems, and Scientific Computation..\n\nIt is one of the leading research groups in the field of Autonomic Computing and adaptive computation systems.\n\n", "id": "4225178", "title": "TASSL"}
{"url": "https://en.wikipedia.org/wiki?curid=7753733", "text": "Carnegie Building (Troy, New York)\n\nThe Carnegie Building is the current home of the Cognitive Science Department at Rensselaer Polytechnic Institute in Troy, New York. It is one of the western-most buildings on the campus and as such provides scenic views overlooking the city of Troy and the Hudson River. The four-story building is named for Andrew Carnegie who donated $125,000 for its construction, which was completed in 1906.\n\nFires in 1862 and 1904 damaged buildings of downtown Troy and the RPI campus. The destruction in 1862 prompted the construction of the Main Building (literally the main building of Rensselaer at the time). The destruction of the Main Building in 1904 prompted the Institute to consider expanding the campus. Being built around a hill, the Institute considered extending the campus downhill within the downtown Troy area and even considered leaving Troy altogether by accepting Columbia University's proposed merger with its New York City campus. Ultimately, the decision was made to remain in Troy but move uphill, away from the downtown area.\n<br>In 1905 Andrew Carnegie donated $125,000 to replace the Main Building. As part of the Institute's plan to move uphill, the Walter Phelps Warren estate was acquired. There the Carnegie Building was erected, made of Harvard brick and Indiana limestone. It was completed in September 1906 at a cost of $133,000. The Carnegie Building was dedicated along with the Walker Laboratory on June 12, 1907.\n\nThe building was designed by architects Whitfield & King. Henry D. Whitfield was Andrew Carnegie's brother-in-law.\n\nIn addition to being built with Harvard brick and Indiana limestone - concrete was used, steel floors, and partitions were used to split rooms. This fireproof building had terrazzo floors and its walls lined with white tiling.\n\nOn the first floor, the four large recitation rooms were utilized by the departments of Geodesy and Drawings while the second floor was occupied by the department of Mechanics and Mathematics.\n\nThe third floor has a small drawing room and several classrooms. The topmost fourth floor consisted of the largest drawing room of its time (sixty by one hundred feet,) with a small side room for blue-printing.\n\nThe walls of corridors and the underclass lobby contain drawings and photographs of structures designed by RPI male students, while the walls of the upperclass lobby contain etchings of familiar classic figures.\n\n\"...Located at the head of the entrance of the Institute grounds; the interior is admirably arranged for the purposes intended, well lighted, heated and ventilated; which all go toward making the work of the students easy.\" \n\nThe building is home to the Cogworks laboratory, which directs research in cognitive science.\n\nThe research and doctoral program taking place in the Cognitive Science Department is currently having its current goals set on integrating an interdisciplinary department where the research and teaching is aimed at three powerful, driving ideas:\n\n\nOne small program that is held in this building is the Experimetrix where students could sign up to participate in experiments that deal with how the brain works or cognitive science. Graduate students and professors work together to collect data and in turn analyze them for further development in cognitive research. Students being participants for this research allows them to receive extra credit for their course if allowed by their professor. This area of experiments is currently being held on the second floor of the building.\n\nThere is also the closely related Rensselaer Artificial Intelligence and Reasoning (RAIR) Laboratory, which is located in the Russell Sage Laboratory.\n\n", "id": "7753733", "title": "Carnegie Building (Troy, New York)"}
{"url": "https://en.wikipedia.org/wiki?curid=9168065", "text": "Kiev Laboratory for Artificial Intelligence\n\nThe Kiev Laboratory for Artificial Intelligence (NeuroTechnica) is a research institute in Kiev, the capital of Ukraine.\n\n\n(\"PICAP\"):\n\n\n", "id": "9168065", "title": "Kiev Laboratory for Artificial Intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=12685352", "text": "Research Institute for Advanced Computer Science\n\nThe Research Institute for Advanced Computer Science (RIACS) was founded June 1, 1983 as a joint collaboration between the Universities Space Research Association (USRA) and the NASA Ames Research Center. The Institute was created to conduct basic and applied research in computer science, covering a broad range of research topics of interest to the aerospace community including supercomputing, computational fluid dynamics, computational chemistry, high performance networking, and artificial intelligence.\nSince its inception, a goal of the Institute’s research has been to support scientific research and engineering from problem formulation to results dissemination, combining concurrent processing systems with intelligent systems to allow users to interact in the language of their discipline. This goal has since expanded to support a broad range of activities associated with space exploration and science, including mission operations and innovative information systems for technology research and development.\n\nAn underlying philosophy and approach of the Institute is that successful research is interdisciplinary, and that challenging applications associated with NASA’s mission provide a driving force for developing innovative information systems and advancing computer science. To implement this approach, research staff undertakes collaborative projects with research groups at NASA, integrating computer science with other disciplines to support NASA’s mission.\n\nOver its nearly twenty five-year history, RIACS has acted as a bridge between academia and government research, engaging talented researchers from around the world to collaborate with NASA on challenging research topics. RIACS has also acted as a bridge between industry and the government to mature information technologies for infusion into NASA operations, enabling broader public benefit from research results.\n\nFor NASA, RIACS has collaborated most closely with the Intelligent Systems Division and the NASA Advanced Supercomputing Division (NAS) at the NASA Ames Research Center – NASA’s Center for Excellence in Information Technology. RIACS, which was formed the same year as the NAS, worked closely with the division in its early years to develop a strong competency in supercomputing and computational fluid dynamics at NASA. RIACS helped establish the Intelligent Systems Division, and has since collaborated closely with the division to develop and infuse a number of software innovations in the areas of autonomous systems; intelligent information management and data understanding; and human-centered computing.\n\nExamples of RIACS contributions to NASA Ames as part of the \"Intelligent Systems Divisions\" include leadership roles in developing and infusing: \n\nThe stated purpose of RIACS since its formation is to:\n\n", "id": "12685352", "title": "Research Institute for Advanced Computer Science"}
{"url": "https://en.wikipedia.org/wiki?curid=14027739", "text": "Centre for Artificial Intelligence and Robotics\n\nThe Centre for Artificial Intelligence and Robotics (CAIR) is a laboratory of the Defence Research & Development Organization (DRDO). Located in Bangalore, Karnataka, involved in the Research & Development of high quality Secure Communication, Command and Control, and Intelligent Systems. CAIR is the primary laboratory for R&D in different areas of Defence Information and Communication Technology (ICT).\n\nCAIR was established in October 1986. Its research focus was initially in the areas of Artificial Intelligence (AI), Robotics, and Control systems. In November 2000, R&D groups working in the areas of Command, Control, Communications & Intelligence (C3I) systems, Communication and Networking, and communication secrecy in Electronics and Radar Development Establishment (LRDE) were merged with CAIR. \n\nCAIR, which was operating from different campuses across Bangalore has now moved to a new unified campus. \n\n\n", "id": "14027739", "title": "Centre for Artificial Intelligence and Robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=434274", "text": "MIT Computer Science and Artificial Intelligence Laboratory\n\nMIT Computer Science and Artificial Intelligence Laboratory (CSAIL) is a research institute at the Massachusetts Institute of Technology formed by the 2003 merger of the Laboratory for Computer Science and the Artificial Intelligence Laboratory. Housed within the Stata Center, CSAIL is the largest on-campus laboratory as measured by research scope and membership.\n\nCSAIL's research activities are organized around a number of semi-autonomous research groups, each of which is headed by one or more professors or research scientists. These groups are divided up into seven general areas of research:\n\n\nIn addition, CSAIL hosts the World Wide Web Consortium (W3C).\n\nComputing research at MIT began with Vannevar Bush's research into a differential analyzer and Claude Shannon's electronic Boolean algebra in the 1930s, the wartime Radiation Laboratory, the post-war Project Whirlwind and Research Laboratory of Electronics (RLE), and Lincoln Laboratory's SAGE in the early 1950s.\n\nResearch at MIT in the field of artificial intelligence began in 1959.\n\nOn July 1, 1963, Project MAC (the Project on Mathematics and Computation, later backronymed to Multiple Access Computer, Machine Aided Cognitions, or Man and Computer) was launched with a $2 million grant from the Defense Advanced Research Projects Agency (DARPA). Project MAC's original director was Robert Fano of MIT's Research Laboratory of Electronics (RLE). Fano decided to call MAC a \"project\" rather than a \"laboratory\" for reasons of internal MIT politics — if MAC had been called a laboratory, then it would have been more difficult to raid other MIT departments for research staff. The program manager responsible for the DARPA grant was J. C. R. Licklider, who had previously been at MIT conducting research in RLE, and would later succeed Fano as director of Project MAC.\n\nProject MAC would become famous for groundbreaking research in operating systems, artificial intelligence, and the theory of computation. Its contemporaries included Project Genie at Berkeley, the Stanford Artificial Intelligence Laboratory, and (somewhat later) University of Southern California's (USC's) Information Sciences Institute.\n\nAn \"AI Group\" including Marvin Minsky (the director), John McCarthy (inventor of Lisp) and a talented community of computer programmers was incorporated into the newly formed Project MAC. It was interested principally in the problems of vision, mechanical motion and manipulation, and language, which they view as the keys to more intelligent machines. In the 1960s - 1970s the AI Group shared a computer room with a computer (initially a PDP-6, and later a PDP-10) for which they built a time-sharing operating system called Incompatible Timesharing System (ITS).\n\nThe early Project MAC community included Fano, Minsky, Licklider, Fernando J. Corbató, and a community of computer programmers and enthusiasts among others who drew their inspiration from former colleague John McCarthy. These founders envisioned the creation of a computer utility whose computational power would be as reliable as an electric utility. To this end, Corbató brought the first computer time-sharing system, Compatible Time-Sharing System (CTSS), with him from the MIT Computation Center, using the DARPA funding to purchase an IBM 7094 for research use. One of the early focuses of Project MAC would be the development of a successor to CTSS, Multics, which was to be the first high availability computer system, developed as a part of an industry consortium including General Electric and Bell Laboratories.\n\nIn 1966, \"Scientific American\" featured Project MAC in the September thematic issue devoted to computer science, that was later published in book form. At the time, the system was described as having approximately 100 TTY terminals, mostly on campus but with a few in private homes. Only 30 users could be logged in at the same time. The project enlisted students in various classes to use the terminals simultaneously in problem solving, simulations, and multi-terminal communications as tests for the multi-access computing software being developed.\n\nIn the late 1960s, Minsky's artificial intelligence group was seeking more space, and was unable to get satisfaction from project director Licklider. University space-allocation politics being what it is, Minsky found that although Project MAC as a single entity could not get the additional space he wanted, he could split off to form his own laboratory and then be entitled to more office space. As a result, the MIT AI Lab was formed in 1970, and many of Minsky's AI colleagues left Project MAC to join him in the new laboratory, while most of the remaining members went on to form the Laboratory for Computer Science (LCS). Talented programmers such as Richard Stallman, who used TECO to write EMACS, flourished in the AI Lab during this time.\n\nThose researchers who did not join the smaller AI Lab formed the Laboratory for Computer Science and continued their research into operating systems, programming languages, distributed systems, and the theory of computation. Two professors, Hal Abelson and Gerald Jay Sussman, chose to remain neutral – their group was referred to variously as Switzerland and Project MAC for the next 30 years.\n\nThe AI Lab led to the invention of Lisp machines and their attempted commercialization by two companies in the 1980s: Symbolics and Lisp Machines Inc. This divided the AI lab into \"camps\" and resulted in a hiring away of many employees. The experience was influential on Stallman's later work on the GNU project. \"Nobody had envisioned that the AI lab's hacker group would be wiped out, but it was.\" ... \"That is the basis for the free software movement — the experience I had, the life that I've lived at the MIT AI lab — to be working on human knowledge, and not be standing in the way of anybody's further using and further disseminating human knowledge\".\n\nOn the fortieth anniversary of Project MAC's establishment, July 1, 2003, LCS was merged with the AI Lab to form the MIT Computer Science and Artificial Intelligence Laboratory, or CSAIL. This merger created the largest laboratory (over 600 personnel) on the MIT campus and was regarded as a reuniting of the diversified elements of Project MAC.\n\nThe IMARA (from Swahili word for \"power\") group sponsors a variety of outreach programs which bridge the Global Digital Divide. Its aim is to find and implement long-term, sustainable solutions which will increase the availability of educational technology and resources to domestic and international communities. These projects are run under the aegis of CSAIL and staffed by MIT volunteers who give training, install and donate computer setups in greater Boston, Massachusetts, Kenya, Native American Indian tribal reservations in the American Southwest such as the Navajo Nation, the Middle East, and Fiji Islands. The CommuniTech project strives to empower under-served communities through sustainable technology and education and does this through the MIT Used Computer Factory (UCF), providing refurbished computers to under-served families, and through the Families Accessing Computer Technology (FACT) classes, it trains those families to become familiar and comfortable with computer technology.\n\n\nSeveral Project MAC alumni went on to further revolutionize the computer industry.\n\n\n\n\n\n\n", "id": "434274", "title": "MIT Computer Science and Artificial Intelligence Laboratory"}
{"url": "https://en.wikipedia.org/wiki?curid=2828572", "text": "Institute for Creative Technologies\n\nThe Institute for Creative Technologies (ICT) is a research institute of the University of Southern California located in Playa Vista, California. ICT was established in 1999 with funding from the US Army. ICT was created to combine the assets of a major research university with the creative resources of Hollywood and the game industry to advance the state-of-the-art in training and simulation. The institute’s research has also led to applications for education, entertainment and rehabilitation, including virtual patients, virtual museum guides and Academy Award-winning visual effects technologies. Core areas include virtual humans, graphics, mixed-reality, learning sciences, games, storytelling and medical virtual reality.\n\nRandall Hill, Jr., \nExecutive Director\n\nCheryl Birch, \nDirector of Finance, Administration, and Human Resources\n\nClarke Lethin, \nManaging Director\n\nWilliam Swartout, \nDirector of Technology\n\nAndrew Gordon, Director of Interactive Narrative Research \n\nJonathan Gratch, Director of Virtual Humans Research\n\nHao Li, Director of Vision & Graphics Lab\n\nRyan McAlinden, Director of Modeling, Simulation and Training \n\nBenjamin Nye, Director of Learning Science Research \n\nDavid Pynadath, Director of Social Simulation Research \n\nTodd Richmond, Director of Mixed Reality Lab/Studio + Advanced Prototypes\n\nAlbert \"Skip\" Rizzo, Director of Medical Virtual Reality\n\nPaul Rosenbloom, Director of Cognitive Architecture Research \n\nDavid Traum, Director of Natural Language Research\n\nArno Hartholt, Director of Research and Development Integration\n\nRob Groome, IT Director\n\nRichard DiNinni, Project Director\n\nMatthew Trimmer, Project Director\n\nJulia Campbell, Project Director\n\nStefan Scherer, Associate Director for Neural Information Processing\n\nAbout a dozen ICT researchers hold academic appointments at various schools and departments across USC. These include research professors, research associate professors and research assistant professors in Department of Computer Science at the USC Viterbi School of Engineering, the Interactive Media Division at the USC School of Cinematic Arts, the Department of Psychiatry & Behavioral Sciences at the Keck School of Medicine and the USC Davis School of Gerontology.\n\nICT also has several post-docs and hosts university-level interns each summer.\n\nNotable researchers include:\nNotable engineers include:\n\n\nEstablished in 1999, ICT is a DoD-sponsored University Affiliated Research Center (UARC) working in collaboration with the U.S. Army Research Laboratory to further advance our research and development efforts for societal benefit in areas including veterans issues, mental health support, rehabilitation and job training. UARCs are aligned with prestigious institutions conducting research at the forefront of science and innovation.\n\n\n", "id": "2828572", "title": "Institute for Creative Technologies"}
{"url": "https://en.wikipedia.org/wiki?curid=2227943", "text": "Krasnow Institute for Advanced Study\n\nThe Krasnow Institute for Advanced Study brings together researchers from many disciplines to study the phenomenon known as the mind. A unit of George Mason University, the Institute for Advanced Study also serves as a center for doctoral education in neuroscience. Research at the Institute is funded by agencies such as the National Institutes of Health, the National Science Foundation and the Department of Defense.\n\nThe Krasnow Institute for Advanced Study of George Mason University was chartered in 1990 as a result of a bequest from Shelley Krasnow, a long-time resident of the National Capital Area. The work of the Institute began in 1993 with a scientific conference, co-sponsored with The Santa Fe Institute (SFI) and hosted at George Mason University. This conference on \"The Mind, the Brain, and Complex Adaptive Systems\" brought together an unusual group of scientists including two Nobel laureates (Murray Gell-Mann and Herbert A. Simon) and produced new approaches to this frontier in addition to a book published by SFI. \nThese efforts set the Institute on the path of human cognition within the context of the intersection of neuroscience, cognitive psychology and computer sciences.\n\nThe Krasnow Institute for Advanced Study is home to a scientific community of 100 (many of them Ph.D.'s) most of whom are also either faculty or trainees at George Mason University. The range of their research on cognition spans from molecules to mind. The Institute is housed in a dedicated facility on the Fairfax Campus of George Mason which includes extensive wet-laboratories, computer labs, a 3T MRI human brain imaging center, a cellular imaging facility, as well as faculty offices and breakout space. Additional research space on the Fairfax campus houses the Institute's Center for Social Complexity.\n\nThe Krasnow Institute for Advanced Study is also an academic unit of George Mason University. It houses the department of Molecular Neuroscience, which plays a key role in the interdisciplinary neuroscience doctoral program.\n\nThe Institute hosted several events in the \"Decade of the Mind\" initiative, which urged the U.S. Congress to invest in understanding how mind emerges from brain.\n\nThe Institute has collaborated closely with the Janelia Farm Campus of Howard Hughes Medical Institute, the Allen Institute for Brain Science and the National Institutes of Health to develop better methods of reconstructing neuronal architectures.\n\nResearchers at the Institute include neuroscientists, bio-engineers, computer scientists, and computational social scientists. The Institute's various research programs are conducted by three centers (described below) and multiple lab groups. Many of the programs focus on research that may lead to cures to some of the most devastating brain diseases.\n\nThe Center for Neural Informatics, Neural Structures, and Neural Plasticity (CN3) pursues fundamental breakthroughs in neuroscience by fostering neuroinformatic and computational approaches to neuroplasticity and neuroanatomy. By bringing together faculty expertise in these multiple disciplines, the Center provides opportunities for cross-training in neuroscience, psychology, and engineering, both at the graduate and postdoctoral levels. CN3 researchers investigate the relationship between brain structure, activity, and function from the subcellular to the network level, with a specific focus on the biophysical and biochemical mechanisms of learning and memory. In the long term, we seek to create large-scale, biologically plausible network models of entire portions of the mammalian brain, such as the hippocampus, to understand the neural circuits and cellular events underlying the expression, storage, and retrieval of associative memory.\n\nKrasnow scientists are using functional magnetic resonance imaging of the human brain to study human interactions within the context of free-markets. These studies allow, for the first time, the elucidation of a biological basis for economic decisions.\n\nScientific projects at the Center for Social Complexity focus on investigating social systems and processes on multiple scales: groups, organizations, economies, societies, regions, international systems. Researchers use a variety of interdisciplinary tools, including multi-agent systems and agent-based models (including the MASON toolkit in Java), cellular automata and other social simulation methods, network and graph-theoretic models, GIS (geographic information systems), events data analysis, complexity-theoretic models and other advanced computational methods. The Center houses a specialized simulation environment (the Simulatorium), where faculty, postdoctoral researchers, and graduate research assistants collaborate in a variety of projects. Conflict and cooperation, emergent economic systems, network dynamics, and long-term societal adaptation to environmental change are among the current lines of investigation. Funding for the Center is provided by grants from the US National Science Foundation, the Department of Defense, and other agencies. (*see also: Department of Computational Social Science). The center does work in Computational sociology.\n", "id": "2227943", "title": "Krasnow Institute for Advanced Study"}
{"url": "https://en.wikipedia.org/wiki?curid=23981191", "text": "VisLab\n\nVisLab is an Italian company working on computer vision and environmental perception for vehicular applications. It was founded in the early 90s as a research laboratory of University of Parma, Dipartimento di Ingegneria dell'Informazione. It started its activities in 1990, with its involvement within the Eureka PROMETHEUS Project. Since then the research group focused on vehicular applications.\n\nVisLab, directed by Alberto Broggi, undertakes basic and applied research, including the perception of the surrounding environment in vehicular applications using cameras and fusion with other sensors. Its researchers contribute to fields such as artificial vision, image processing, machine learning, neural networks, robotics, and sensor fusion.\n\nIn 2009, eleven VisLab researchers started a spinoff company, named VisLab srl, to commercialize the results of their main researches. The University of Parma owned a share of 5%.In 2015, VisLab was acquired by Silicon Valley company Ambarella Inc., and about 30 researchers were hired by VisLab to staff their Parma location.\n\nIn the early years, the research group formed by Broggi, Massimo Bertozzi and Alessandra Fascioli designed, realized, and tested ARGO. ARGO was a passenger car able to perceive the environment through the use of microcameras, analyze the surroundings, plan a trajectory, and drive itself on normal roads. It was tested in 1998 with a 2000+ km tour in Italy, dubbed MilleMiglia in Automatico. In this test the vehicle drove for more than 94% in automatic mode. It was the first test in the world to use off-the-shelf and low cost technology (a Pentium 200 MHz PC and two low-cost video-phone cameras) in normal conditions of traffic, environment, and weather.\n\nIn 2005 a vehicle called TerraMax was able to successfully conclude the DARPA Grand Challenge; VisLab's vision system was its primary means of perception.\n\nIn 2007 a new version of TerraMax qualified for the DARPA Urban Challenge, which was not completed due to a fault.\n\nIn 2010 VisLab launched VIAC, the VisLab Intercontinental Autonomous Challenge, a 13,000 km test run for autonomous vehicles, from Italy to China. This was the first autonomous driving test on an intercontinental route; it lasted three months.\n\nOn 12 July 2013 VisLab tested the BRAiVE vehicle in downtown Parma. BRAiVE successfully negotiating two-way narrow rural roads, traffic lights, pedestrian crossings, speed bumps, pedestrian areas, and tight roundabouts. VisLab engineers activated the vehicle in Parma University Campus and stopped it in Piazza della Pilotta (downtown Parma): a 20 minutes run in a real environment, together with real traffic at 11am on a working day, that required absolutely no human intervention.\n\nOn 31 March 2014 VisLab unveiled the new autonomous car, whose name is DEEVA, and which features more than 20 cameras, 4 lasers, GPS and IMU, and all sensors are hidden.\n\nVisLab is located in the University of Parma main campus, south of Parma, Italy.\n\n", "id": "23981191", "title": "VisLab"}
{"url": "https://en.wikipedia.org/wiki?curid=29152426", "text": "Smart Agent Technologies\n\nThe Studio Smart Agent Technologies (SAT) is part of the\nResearch Studios Austria ForschungsgesmbH, a non-profit research organization. It aims to facilitate the transfer of academic research into commercial applications thus implementing an innovation pipeline from universities into markets. To this end, SAT cooperates with both, universities and other academic institutions, as well as innovative companies.\n\nThe Studio SAT competes for national and European research grants and funding in research excellence. It does contract research for clients in the private and public sectors and it receives the funding for its independent research from the Austrian Federal Ministry for Science and Research.\n\nThe Studio Smart Agent Technologies has been founded in 2003 as one of the first research units of the Research Studios Austria, a division of the Austrian Institute of Technology (AIT) formerly known as Austrian Research Centers (ARC). In April 2008 the Research Studios Austria were spun out into a new company and the Research Studios Austria Forschungsgesellschaft mbH was founded.\n\nDuring the years 2004 and 2005 SAT accompanied the development of a mobile content download platform by 3united and Ericsson as a research partner concerning personalization.\n\nFrom 2006 to 2008 SAT guided research activities concerning semantic systems and recommender systems for Verisign Communications GmbH, the Viennese branch of VeriSign.\n\nWith its flagship project easyrec, an open source recommender engine, SAT launched an incubator for spreading the ideas and usage of personalization in early 2010.\n\nThe main research areas of SAT include:\n\n\n", "id": "29152426", "title": "Smart Agent Technologies"}
{"url": "https://en.wikipedia.org/wiki?curid=29152434", "text": "Research Studios Austria\n\nThe Research Studios Austria Forschungsgesellschaft mbH (RSA FG) is a non-profit research institution and manages innovations from universities to market in the area of eTechnologies and Smart Media. It operates a network of currently five research units called Studios cooperating with and creating research synergies among universities in Vienna, Linz, Salzburg and Innsbruck.\n\nThe RSA FG undertakes applied research projects. The research and development conducted at the Research Studios Austria is based on a process of rapid prototyping and a special research approach MIR (Modular Iterative Re-framing).\n\nThe Research Studios Austria FG competes for national and European research grants and funding in research excellence. It does contract research for clients in the private and public sectors and it receives the funding for its independent research from the Austrian Federal Ministry for Science and Research.\n\nThe current director is Prof. Dr. Peter A. Bruck, PhD., MA (CEO).\n\nThe Research Studios Austria have been established in 2003, and were part of the Austrian Institute of Technology (AIT) formerly known as Austrian Research Centers (ARC) until Spring of 2008. In April 2008 the Research Studios Austria were spun out into a new company and the Research Studios Austria Forschungsgesellschaft mbH was founded.\n\nCurrently five different Studios are working in applied ICT research:\n\n\nThe main research areas of the Research Studios Austria include:\n\n\n", "id": "29152434", "title": "Research Studios Austria"}
{"url": "https://en.wikipedia.org/wiki?curid=6610123", "text": "German Research Centre for Artificial Intelligence\n\nThe German Research Center for Artificial Intelligence (German: Deutsches Forschungszentrum für Künstliche Intelligenz, DFKI) is one of the world's largest nonprofit contract research institutes for software technology based on artificial intelligence (AI) methods. DFKI was founded in 1988, and has facilities in the German cities of Kaiserslautern, Saarbrücken, Bremen and Berlin.\n\nDFKI shareholders include Google, Microsoft, SAP, BMW and Daimler. The directors are Wolfgang Wahlster (CEO) and Walter G. Olthoff (CFO).\n\nDFKI conducts contract research in virtually all fields of modern AI, including image and pattern recognition, knowledge management, intelligent visualization and simulation, deduction and multi-agent systems, speech- and language technology, intelligent user interfaces, business informatics and robotics. DFKI led the national project Verbmobil, a project with the aim to translate spontaneous speech robustly and bidirectionally for German/English and German/Japanese.\n\nThere are different research departments.\n\n\n\n\n\n\n", "id": "6610123", "title": "German Research Centre for Artificial Intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=33524353", "text": "List of ubiquitous computing research centers\n\nThis is a list of notable institutions who claim to have a focus on Ubiquitous computing sorted by country:\n\n\n\n\n\n\n\n", "id": "33524353", "title": "List of ubiquitous computing research centers"}
{"url": "https://en.wikipedia.org/wiki?curid=33900354", "text": "Artificial Intelligence Center\n\nThe Artificial Intelligence Center is a laboratory in the Information and Computing Sciences Division of SRI International. It was founded in 1966 by Charles Rosen and studies artificial intelligence. One of their early projects was Shakey the Robot, the first general-purpose mobile robot. More recently, the center funded early development of CALO and Siri. The center has also provided the military with various technology.\n\n\n", "id": "33900354", "title": "Artificial Intelligence Center"}
{"url": "https://en.wikipedia.org/wiki?curid=41268341", "text": "Turing Institute\n\nThe Turing Institute was an Artificial Intelligence laboratory based in Glasgow, Scotland between 1983 and 1994. The company undertook basic and applied research, working directly with large companies across Europe, the United States, and Japan developing software as well as providing training, consultancy and information services.\n\nThe Institute was formed in June 1983 by Donald Michie, Peter Mowforth and Tim Niblett. The Institute was named after Alan Turing who Donald Michie had worked with at Bletchley Park during the Second World War.\n\nThe organisation grew out of the Machine Intelligence Research Unit at Edinburgh University with a plan to combine research in Artificial Intelligence with technology transfer to industry. In 1983, Sir Graham Hills was instrumental in the Institute moving to Glasgow where, with support from the Scottish Development Agency it formed a close working relationship with Strathclyde University.\nLord Balfour (Chairman) and Shirley Williams joined the board along with a growing team of researchers and AI specialists. Notable amongst these was Stephen Muggleton who was responsible for work developing Inductive Logic Programming.\n\nProfessor Jim Alty moved his Man Machine Interaction (HCI) group (later the Scottish HCI Centre) to the Turing Institute in 1984. The move included a significant expansion of the Postgraduate school at the Institute. Jim Alty joined the Turing Institute Board and became Chief Executive. The HCI Centre and the Institute collaborated on a wide range of projects.\n\nIn 1984, following the UK Government Alvey Report on AI, the Institute became an Alvey Journeyman centre for the UK. Under the guidance of Judith Richards, companies such as IBM (see:John Roycroft), Burroughs, British Airways, Shell, and Unilever seconded researchers to develop new industrial AI applications.\nThe Turing Institute Library was formed in 1983 and grew by selling access by subscription to its Information Services. The library developed a large searchable electronic database of content from most of the main AI research and Development centres around the world. Library affiliates logged into the system by dial-up and received weekly summaries of newly added items that could be ordered or downloaded as abstracts.\nThe publisher Addison-Wesley developed a close working relationship and published the Turing Institute Press series of books.\n\nIn 1984, Jim Alty wrote a text book which was adopted by many Universities and a much-cited paper on Expert Systems (with Mike Coombs).\n\nThroughout its existence, the Institute organised ran a wide range of workshops and International conferences. Notable amongst these were the Turing Memorial Lecture Series whose speakers included Tony Hoare, Herbert Simon, and John McCarthy. Major conferences included The British Association (147th conference in 1985), BMVC'91, IEEE International Symposium on Intelligent Control (1992) and the Machine Intelligence Series.\n\nThe Institute won research funding from the Westinghouse Corporation after it developed a machine learned rule-based system to improve the efficiency of a nuclear power plant. The research funding was used to launch the Freddy 3 advanced robotics project aimed at studying robot learning and robot social interaction. Barry Shepherd developed much of the Freddy 3 software infrastructure. Tatjana Zrimec used the system to investigate how playing robots could develop structured knowledge about their world while Claude Sammut used the system to investigate machine learning and control and helped develop Reinforcement Learning. Ivan Bratko made several visits to the Turing Institute undertaking research in machine learning and Advanced Robotics.\n\nThe Institute undertook several projects for the US military (e.g. Personnel Allocation for the US Office of Naval Research), credit card scoring for a South African Bank and seed sorting for Scottish Agricultural Sciences Agency. Other large projects included the ESPRIT Machine Learning Toolbox developing CN2 and Electrophoretic Gel analysis with Unilever.\n\nIn 1984 the Institute worked under contract from Radian Corp to develop code for the Space Shuttle auto-lander.\nThe code was developed with an inductive rule generator, Rulemaster, using training examples from a NASA simulator. A similar approach was later used by Danny Pearce to develop qualitative models to control and diagnose satellites for ESA as well as optimising gas flow in the North Sea for Enterprise Oil. Similar approaches based on pole-balancing automata were used to control submersible vehicles and develop a control system for helicopters carrying sling loads.\nStephen Muggleton and his group developed Inductive Logic Programming and was involved in the practical use of machine learning for the generation of expert knowledge. Applications included the discovery of rules for protein folding (with Ross King) and drug design as well as systems such as CIGOL that were capable of discovering new concepts and hypotheses.\n\nIn 1986, Jim Alty's HCI group won a major ESPRIT 1 contract to investigate the use of Knowledge Based systems in Process Control Interfaces called GRADIENT (Graphical Intelligent Dialogues, P600). (with Gunnar Johannsen of Kassel University), Peter Elzer (Clausthal University) and Asea Brown Boveri) to create intelligent interfaces for process control operators. This work had a major impact on process control interface design. The initial pilot phase report (Alty, Elzer et al., 1985) was widely used and cited. Many research papers were produced A follow-on large ESPRIT research project was PROMISE (Process Operators Multimedia Intelligent Support Environment) working with DOW Benelux (Netherlands), Tecsiel (Italy) and Scottish Power (Scotland).\n\nIn 1987, the Turing Institute won a project to build a large, scalable, network-available user-manual for \nS.W.I.F.T. The worldwide-web-like system was launched in 1988. Its success as a global hypertext resource for its users led to SWIFT sponsoring the Turing Memorial series of Lectures. The close working relationship came to an end, in part, when a key member of the SWIFT team, Arnaud Rubin, was killed by a terrorist bomb on Pan Am flight 103 over Lockerbie.\n\nOne of the strongest business relationships the Institute had was with Sun Microsystems. Sun funded a series of projects where the key Institute Personnel were Tim Niblett and Arthur van Hoff. Several projects concerned the development of new user-interface tools and environments (e.g. GoodNews, HyperNews and HyperLook).\n\nHyperLook was written in PostScript and PDB, an ANSI C to PostScript compiler developed at the Institute, and it ran on Sun's NeWS Windowing System. Don Hopkins, while studying at the Turing Institute, ported SimCity to Unix with HyperLook as its front-end.\n\nArthur van Hoff left the Institute in 1992 and joined Sun Microsystems where he authored the Java 1.0 compiler, the beta version of the HotJava browser and helped with the design of the Java language.\n\nThroughout the 1980s, the Turing Institute Vision Group developed multi-scale tools and applications. A series of 3D industrial applications were developed and deployed using the Multi-Scale Signal Matching (MSSM) technology, specifically:\n\n\nVarious other robot projects were undertaken at the Turing Institute where key researchers included Paul Siebert, Eddie Grant, Paul Grant, David Wilson, Bing Zhang and Colin Urquhart (e.g. ).\n\nIn 1990 the Turing Institute organised and ran the First Robot Olympics with the venue at the University of Strathclyde.\n\nFrom 1989 onwards, the company faced financial difficulties that caused it to close in 1994.\n", "id": "41268341", "title": "Turing Institute"}
{"url": "https://en.wikipedia.org/wiki?curid=35782731", "text": "Allen Institute for Artificial Intelligence\n\nThe Allen Institute for Artificial Intelligence (abbreviated AI2) is a research institute funded by Microsoft co-founder Paul Allen to achieve scientific breakthroughs by constructing AI systems with reasoning, learning, and reading capabilities. Oren Etzioni was appointed by Paul Allen in September 2013 to direct the research at the institute.\n\n\nLaunched in 2015, the institute's startup incubator will try to develop technologies in the artificial intelligence field.\n\nAI2 was the subject of an in-depth article in \"The Verge\". Its launch was covered in Xconomy, and GeekWire. Allen and Etzioni co-authored an article for CNN about artificial intelligence and AI2 in December 2013. AI2 has also been mentioned in other articles discussing the current state of and trends in artificial intelligence research.\n\n", "id": "35782731", "title": "Allen Institute for Artificial Intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=41755648", "text": "DeepMind\n\nDeepMind Technologies Limited is a British artificial intelligence company founded in September 2010.\n\nAcquired by Google in 2014, the company has created a neural network that learns how to play video games in a fashion similar to that of humans, as well as a Neural Turing machine, or a neural network that may be able to access an external memory like a conventional Turing machine, resulting in a computer that mimics the short-term memory of the human brain.\n\nThe company made headlines in 2016 after its AlphaGo program beat a human professional Go player for the first time in October 2015 and again when AlphaGo beat Lee Sedol the world champion in a five-game match, which was the subject of a documentary film.\n\nA more generic program, AlphaZero, beat the most powerful programs playing go, chess and shogi (Japanese chess) after a few hours of play against itself using reinforcement learning.\nThe start-up was founded by Demis Hassabis, Shane Legg and Mustafa Suleyman in 2010. Hassabis and Legg first met at University College London's Gatsby Computational Neuroscience Unit. On 26 January 2014, Google announced the company had acquired DeepMind for $500 million, and that it had agreed to take over DeepMind Technologies.\n\nSince then major venture capital firms Horizons Ventures and Founders Fund have invested in the company, as well as entrepreneurs Scott Banister and Elon Musk. Jaan Tallinn was an early investor and an adviser to the company. The sale to Google took place after Facebook reportedly ended negotiations with DeepMind Technologies in 2013. The company was afterwards renamed Google DeepMind and kept that name for about two years.\n\nIn 2014, DeepMind received the \"Company of the Year\" award by Cambridge Computer Laboratory.\n\nIn September 2015, DeepMind and the Royal Free NHS Trust signed their initial Information Sharing Agreement (ISA) to co-develop a clinical task management app, Streams.\n\nAfter Google's acquisition the company established an artificial intelligence ethics board. The ethics board for AI research remains a mystery, with both Google and DeepMind declining to reveal who sits on the board. DeepMind, together with Amazon, Google, Facebook, IBM, and Microsoft, is a founding member of \"Partnership on AI\", an organization devoted to the society-AI interface. DeepMind has opened a new unit called DeepMind Ethics and Society and focused on the ethical and societal questions raised by artificial intelligence featuring prominent transhumanist Nick Bostrom as advisor. In October 2017, Deepmind launched new 'ethics and society' research team to investigate AI ethics.\n\nDeepMind Technologies' goal is to \"solve intelligence\", which they are trying to achieve by combining \"the best techniques from machine learning and systems neuroscience to build powerful general-purpose learning algorithms\".\nThey are trying to formalize intelligence in order to not only implement it into machines, but also understand the human brain, as Demis Hassabis explains:\n\nGoogle Research has released a paper in 2016 regarding AI Safety and avoiding undesirable behaviour during the AI learning process. Deepmind has also released several publications via their website. In 2017 DeepMind released GridWorld, an open-source testbed for evaluating whether an algorithm learns to disable its kill switch or otherwise exhibits certain undesirable behaviors.\n\nTo date, the company has published research on computer systems that are able to play games, and developing these systems, ranging from strategy games such as Go to arcade games. According to Shane Legg human-level machine intelligence can be achieved \"when a machine can learn to play a really wide range of games from perceptual stream input and output, and transfer understanding across games[...].\"\nResearch describing an AI playing seven different Atari 2600 video games (the \"Pong\" game in \"Video Olympics\", \"Breakout\", \"Space Invaders\", \"Seaquest\", \"Beamrider\", \"Enduro\", and \"Q*bert\") reportedly led to their acquisition by Google. Hassabis has mentioned the popular e-sport game \"StarCraft\" as a possible future challenge, since it requires a high level of strategic thinking and handling imperfect information.\n\nAs opposed to other AIs, such as IBM's Deep Blue or Watson, which were developed for a pre-defined purpose and only function within its scope, DeepMind claims that their system is not pre-programmed: it learns from experience, using only raw pixels as data input. Technically it uses deep learning on a convolutional neural network, with a novel form of Q-learning, a form of model-free reinforcement learning. They test the system on video games, notably early arcade games, such as \"Space Invaders\" or \"Breakout\". Without altering the code, the AI begins to understand how to play the game, and after some time plays, for a few games (most notably \"Breakout\"), a more efficient game than any human ever could.\n\n, DeepMind played below the current World Record for most games, for example \"Space Invaders\", \"Ms Pac-Man\" and \"Q*Bert\". DeepMind's AI had been applied to video games made in the 1970s and 1980s; work was ongoing for more complex 3D games such as \"Doom\", which first appeared in the early 1990s.\n\nIn October 2015, a computer Go program called AlphaGo, developed by DeepMind, beat the European Go champion Fan Hui, a 2 dan (out of 9 dan possible) professional, five to zero. This is the first time an artificial intelligence (AI) defeated a professional Go player. Previously, computers were only known to have played Go at \"amateur\" level. Go is considered much more difficult for computers to win compared to other games like chess, due to the much larger number of possibilities, making it prohibitively difficult for traditional AI methods such as brute-force. In March 2016 it beat Lee Sedol—a 9th dan Go player and one of the highest ranked players in the world—with 4-1 in a five-game match. In the 2017 Future of Go Summit, AlphaGo won a three-game match with Ke Jie, who at the time continuously held the world No. 1 ranking for two years. It used a supervised learning protocol, studying large numbers of games played by humans against each other.\n\nIn 2017, an improved version, AlphaGo Zero, defeated AlphaGo 100 games to 0. AlphaGo Zero's strategies were self-taught. AlphaGo Zero was able to beat its predecessor after just three days with less processing power than AlphaZero; in comparison, the original AlphaGo needed months to learn how to play.\n\nLater that year, AlphaZero, a modified version of AlphaGo Zero, gained superhuman abilities at chess and shogi solely. Like AlphaGo Zero, AlphaZero learned through self-play.\n\nAlphaGo used two deep neural networks: a policy network to evaluate move probabilities and a value network to assess positions. The policy network trained via supervised learning, and was subsequently refined by policy-gradient reinforcement learning. The value network learned to predict winners of games played by the policy network against itself. After training these networks employed a lookahead Monte Carlo tree search (MCTS), using the policy network to identify candidate high-probability moves, while the value network (in conjunction with Monte Carlo rollouts using a fast rollout policy) evaluated tree positions.\n\nZero trained using reinforcement learning in which the system played millions of games against itself. Its only guide was to increase its win rate. It did so without learning from games played by humans. Its only input features are the black and white stones from the board. It uses a single neural network, rather than separate policy and value networks. Its simplified tree search relies upon this neural network to evaluate positions and sample moves, without Monte Carlo rollouts. A new reinforcement learning algorithm incorporates lookahead search inside the training loop. AlphaGo Zero employed around 15 people and millions in computing resources. Ultimately, it needed much less computing power than AlphaGo, running on four specialized AI processors (Google TPUs), instead of AlphaGo's 48.\n\nWaveNet is DeepMind's deep generative model of raw audio waveforms. WaveNet was originally too computationally intensive for use in consumer products when it debuted in 2016; however, in late 2017, it became ready for use in consumer applications such as Google Assistant.\n\nIn July 2016, a collaboration between DeepMind and Moorfields Eye Hospital was announced. DeepMind would be applied to the analysis of anonymised eye scans, searching for early signs of diseases leading to blindness.\n\nIn August 2016, a research programme with University College London Hospital was announced with the aim of developing an algorithm that can automatically differentiate between healthy and cancerous tissues in head and neck areas.\n\nThere are also projects with the Royal Free London NHS Foundation Trust and Imperial College Healthcare NHS Trust to develop new clinical mobile apps linked to electronic patient records. Staff at the Royal Free Hospital were reported as saying in December 2017 that access to patient data through the app had saved a ‘huge amount of time’ and made a ‘phenomenal’ difference to the management of patients with acute kidney injury. Test result data is sent to staff’s mobile phones and alerts them to change in the patient's condition. It also enables staff to see if someone else has responded, and to show patients their results in visual form.\n\nIn April 2016, \"New Scientist\" obtained a copy of a data-sharing agreement between DeepMind and the Royal Free London NHS Foundation Trust. The latter operates the three London hospitals where an estimated 1.6 million patients are treated annually. The revelation has exposed the ease with which private companies can obtain highly sensitive medical information without patient consent. The agreement shows DeepMind Health had access to admissions, discharge and transfer data, accident and emergency, pathology and radiology, and critical care at these hospitals. This included personal details such as whether patients had been diagnosed with HIV, suffered from depression or had ever undergone an abortion in order to conduct research to seek better outcomes in various health conditions. The agreement is seen as controversial and its legality has been questioned.\n\nThe concerns were widely reported and have led to a complaint to the Information Commissioner's Office (ICO), arguing that the data should be pseudonymised and encrypted.\n\nIn May 2016, \"New Scientist\" published a further article claiming that the project had failed to secure approval from the Confidentiality Advisory Group of the Medicines and Healthcare Products Regulatory Agency.\n\nIn May 2017, \"Sky News\" published a leaked letter from the National Data Guardian, Dame Fiona Caldicott, revealing that in her \"considered opinion\" the data sharing agreement between DeepMind and the Royal Free took place on an \"inappropriate legal basis\".\n\nThe Information Commissioner’s Office ruled in July 2017 that London’s Royal Free hospital failed to comply with the Data Protection Act when it handed over personal data of 1.6 million patients to DeepMind.\nAs of October 2017, the DeepMind team has expanded their focus to also include AI ethics. With the former Google UK and EU policy manager Sean Legassick leading this new team, their goal is to fund external research of the following themes: privacy transparency and fairness; economic impacts; governance and accountability; managing AI risk; AI morality and values; and how AI can address the world’s challenges. As a result, the team hopes to further understand the ethical implications of AI and aid society to seeing AI can be beneficial.\n\nThis new subdivision of DeepMind is a completely separate unit from the large partnership of major tech companies of the name Partnership on Artificial Intelligence to Benefit People and Society to which DeepMind is also a part of.\n\n", "id": "41755648", "title": "DeepMind"}
{"url": "https://en.wikipedia.org/wiki?curid=15034", "text": "Information Sciences Institute\n\nThe USC Information Sciences Institute (ISI) is a component of the University of Southern California (USC) Viterbi School of Engineering, and specializes in research and development in information processing, computing, and communications technologies. It is located in Marina del Rey, California.\n\nISI actively participated in the information revolution, and it played a leading role in developing and managing the early Internet and its predecessor ARPAnet. The Institute conducts basic and applied research supported by more than 20 U.S. government agencies involved in defense, science, health, homeland security, energy and other areas. Annual funding is about $100 million.\n\nISI employs about 350 research scientists, research programmers, graduate students and administrative staff at its Marina del Rey, California headquarters and in Arlington, Virginia. About half of the research staff hold PhD degrees, and about 40 are research faculty who teach at USC and advise graduate students. Several senior researchers are tenured USC faculty in the Viterbi School.\n\nISI research spans artificial intelligence (AI), cybersecurity, grid computing, quantum computing, microelectronics, supercomputing, nano-satellites and many other areas. AI expertise includes natural language processing, in which ISI has an international reputation, reconfigurable robotics, information integration, motion analysis and social media analysis. Hardware/software expertise includes cyber-physical system security, data mining, reconfigurable computing and cloud computing. In networking, ISI explores Internet resilience, Internet traffic analysis and photonics, among other areas. Researchers also work in scientific data management, wireless technologies, biomimetics and electrical smart grid, in which ISI is advising the Los Angeles Department of Water and Power on a major demonstration project. Another current initiative involves big data brain imaging jointly with the Keck School of Medicine of USC.\n\nFederal agency sponsors include the Air Force Office of Scientific Research, Department of Defense Advanced Research Projects Agency, Department of Education, Department of Energy, Department of Homeland Security, National Institutes of Health, National Science Foundation, and other scientific, technical and defense-related agencies.\n\nCorporate partners include Chevron Corp. in the Center for Interactive Smart Oilfield Technologies (CiSoft), Lockheed Martin Company in the USC-Lockheed Martin Quantum Computation Center, and Parsons Corp. subsidiary Sparta Inc. in the DETER Project, a cybersecurity research initiative and international testbed. ISI also has partnered with businesses including IBM Corporation, Samsung Electronics Company, the Raytheon Company, GlobalFoundries Inc., Northrop Grumman Corporation and Carl Zeiss AG, and currently is working with Micron Technology, Inc., Altera Corporation and Fujitsu Ltd.\n\nISI also operates the Metal Oxide Semiconductor Implementation Service (MOSIS), a multi-project electronic circuit wafer service that has prototyped more than 60,000 chips since 1981. MOSIS provides design tools and pools circuit designs to produce specialty and low-volume chips for corporations, universities and other research entities worldwide. The Institute also has given rise to several startup and spinoff companies in grid software, geospatial information fusion, machine translation, data integration and other technologies.\n\nISI was founded by Keith Uncapher, who headed the computer research group at RAND Corporation in the 1960s and early 1970s. Uncapher decided to leave RAND after his group’s funding was cut in 1971. He approached the University of California at Los Angeles about creating an off-campus technology institute, but was told that a decision would take 15 months. He then presented the concept to USC, which approved the proposal in five days. ISI was launched with three employees in 1972. Its first proposal was funded by the Defense Advanced Research Projects Agency (DARPA) in 30 days for $6 million.\n\nISI became one of the earliest nodes on ARPANET, the predecessor to the Internet, and in 1977 figured prominently in a demonstration of its international viability. ISI also helped refine the TCP/IP communications protocols fundamental to Net operations, and researcher Paul Mockapetris developed the now-familiar Domain Name System characterized by .com, .org, .net, .gov, and .edu on which the Net still operates. (The names .com, .org et al. were invented at SRI International, an ongoing collaborator.) Steve Crocker originated the Request for Comments (RFC) series, the written record of the network's technical structure and operation that both documented and shaped the emerging Internet. Another ISI researcher, Danny Cohen, became first to implement packet voice and packet video over ARPANET, demonstrating the viability of packet switching for real-time applications.\n\nJonathan Postel collaborated in development of TCP/IP, DNS and the SMTP protocol that supports email. He also edited the RFC for nearly three decades until his sudden death in 1998, when ISI colleagues assumed responsibility. The Institute retained that role until 2009. Postel simultaneously directed the Internet Assigned Number Authority (IANA) and its predecessor, which assign Internet addresses. IANA was administered from ISI until a nonprofit organization, ICANN, was created for that purpose in 1998.\n\nCohen was the first entity to implement, Voice Over Internet Protocol (Voice over IP or VOIP). Some of the first Net security applications, and one of the world's first portable computers, also originated at ISI.\n\nISI researchers also created or co-created the:\n\nIn 2011, several ISI natural language experts advised the IBM team that created Watson, the computer that became the first machine to win against human competitors on the \"Jeopardy!\" TV show. In 2012, ISI’s Kevin Knight spearheaded a successful drive to crack the Copiale cipher, a lengthy encrypted manuscript that had remained unreadable for 250 years. Also in 2012, the USC-Lockheed Martin Quantum Computation Center (QCC) became the first organization to operate a quantum annealing system outside of its manufacturer, D-Wave Systems, Inc. USC, ISI and Lockheed Martin now are performing basic and applied research into quantum computing. A second quantum annealing system is located at NASA Ames Research Center, and is operated jointly by NASA and Google.\n\nThe USC Andrew and Erna Viterbi School of Engineering was ranked among the nation’s top 10 engineering graduate schools by \"US News & World Report\" in 2015. Including ISI, USC is ranked first nationally in federal computer science research and development expenditures.\n\nISI is organized into six divisions focused on differing areas of research expertise:\n\nSmaller, specialized research groups operate within almost all divisions.\n\nISI is led by Executive Director Prem Natarajan, previously an executive vice president and principal scientist at Raytheon BBN Technologies. He is a natural language specialist with research interests that focus on optical character recognition, speech processing, and multimedia analysis. Natarajan joined ISI in 2013, succeeding USC Viterbi School vice dean John O'Brien, who served as interim executive director in 2012 and 2013. From 1988 to 2012, ISI was led by former IBM executive Herbert Schorr.\n\n", "id": "15034", "title": "Information Sciences Institute"}
{"url": "https://en.wikipedia.org/wiki?curid=48795986", "text": "OpenAI\n\nOpenAI is a non-profit artificial intelligence (AI) research company that aims to promote and develop friendly AI in such a way as to benefit humanity as a whole. The organization aims to \"freely collaborate\" with other institutions and researchers by making its patents and research open to the public. The founders (notably Elon Musk and Sam Altman) are motivated in part by concerns about existential risk from artificial general intelligence.\n\nIn October 2015, Musk, Altman and other investors announced the formation of the organization, pledging over US$1 billion to the venture.\n\nOn April 27, 2016, OpenAI released a public beta of \"OpenAI Gym\", its platform for reinforcement learning research. \n\nOn December 5, 2016, OpenAI released Universe, a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites and other applications.\n\nDuring The International 2017 \"Dota 2\" video game tournament in August 2017, OpenAI let a machine-learned bot play 1v1 demonstration game against professional \"Dota 2\" player, Dendi, who played against it live and lost. After the demonstration, CTO Greg Brockman explained that the bot had learned by playing against itself for two weeks of real time, and that the learning software was a step in the direction of creating software that can handle complex tasks \"like being a surgeon\".\n\nSome scientists, such as Stephen Hawking and Stuart Russell, believe that if advanced AI someday gains the ability to re-design itself at an ever-increasing rate, an unstoppable \"intelligence explosion\" could lead to human extinction. Musk characterizes AI as humanity's biggest existential threat. OpenAI's founders structured it as a non-profit so that they could focus its research on creating a positive long-term human impact.\n\nOpenAI states that \"it's hard to fathom how much human-level AI could benefit society,\" and that it's equally difficult to comprehend \"how much it could damage society if built or used incorrectly\". Research on safety cannot safely be postponed: \"because of AI's surprising history, it's hard to predict when human-level AI might come within reach.\" OpenAI states that AI \"should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible...\" Co-chair Sam Altman expects the decades-long project to surpass human intelligence.\n\nVishal Sikka, former CEO of Infosys, stated that an \"openness\" where the endeavor would \"produce results generally in the greater interest of humanity\" was a fundamental requirement for his support, and that OpenAI \"aligns very nicely with our long-held values\" and their \"endeavor to do purposeful work\". Cade Metz of \"Wired\" suggests that corporations such as Amazon may be motivated by a desire to use open-source software and data to level the playing field against corporations such as Google and Facebook that own enormous supplies of proprietary data. Altman states that Y Combinator companies will share their data with OpenAI.\n\nMusk poses the question: \"what is the best thing we can do to ensure the future is good? We could sit on the sidelines or we can encourage regulatory oversight, or we could participate with the right structure with people who care deeply about developing AI in a way that is safe and is beneficial to humanity.\" Musk acknowledges that \"there is always some risk that in actually trying to advance (friendly) AI we may create the thing we are concerned about\"; nonetheless, the best defense is \"to empower as many people as possible to have AI. If everyone has AI powers, then there's not any one person or a small set of individuals who can have AI superpower.\"\n\nMusk and Altman's counter-intuitive strategy of trying to reduce the risk that AI will cause overall harm, by giving AI to everyone, is controversial among those who are concerned with existential risk from artificial intelligence. Philosopher Nick Bostrom is skeptical of Musk's approach: \"If you have a button that could do bad things to the world, you don't want to give it to everyone.\" During a 2016 conversation about the technological singularity, Altman said that \"we don’t plan to release all of our source code\" and mentioned a plan to \"allow wide swaths of the world to elect representatives to a new governance board\". Greg Brockman stated that \"Our goal right now... is to do the best thing there is to do. It’s a little vague.\"\n\nGym aims to provide an easy-to-setup general-intelligence benchmark with a wide variety of different environments (somewhat akin to, but broader than, the ImageNet Large Scale Visual Recognition Challenge used in supervised learning research), and that hopes to standardize the way in which environments are defined in AI research publications, so that published research becomes more easily reproducible. The project claims to provide the user with a simple interface. As of June2017, the gym can only be used with Python. As of September 2017, the gym documentation site is not maintained, and active work is focused instead on its GitHub page.\n\nOpenAI's Igor Mordatch argues for that competition between agents can create an intelligence \"arms race\" that can increase an agent's ability to function, even outside the context of the competition. In OpenAI's \"RoboSumo\", virtual humanoid \"metalearning\" robots initially lack knowledge of how to even walk, and given the goals of learning to move around, and pushing the opposing agent out of the ring. Through this adversial learning process, the agents learn how to adapt to changing conditions; when an agent is then removed from this virtual environment and placed in a new virtual environment with high winds, the agent braces to remain upright, suggesting it had learned how to balance in a generalized way.\n\nThe two co-chairs of the project are:\n\n\nOther backers of the project include:\n\n\nHigh-profile staff include:\n\nThe group started in early January 2016 with nine researchers. According to \"Wired\", Brockman met with Yoshua Bengio, one of the \"founding fathers\" of the deep learning movement, and drew up a list of the \"best researchers in the field\". Microsoft's Peter Lee stated that the cost of a top AI researcher exceeds the cost of a top NFL quarterback prospect. While OpenAI pays corporate-level (rather than nonprofit-level) salaries, it doesn't currently pay AI researchers salaries comparable to those of Facebook or Google. Nevertheless, Sutskever stated that he was willing to leave Google for OpenAI \"partly of because of the very strong group of people and, to a very large extent, because of its mission.\" Brockman stated that \"the best thing that I could imagine doing was moving humanity closer to building real AI in a safe way.\" OpenAI researcher Wojciech Zaremba stated that he turned down \"borderline crazy\" offers of two to three times his market value to join OpenAI instead.\n\n\n", "id": "48795986", "title": "OpenAI"}
{"url": "https://en.wikipedia.org/wiki?curid=23087437", "text": "Stanford University centers and institutes\n\nStanford University has many centers and institutes dedicated to the study of various specific topics. These centers and institutes may be within a department, within a school but across departments, an independent laboratory, institute or center reporting directly to the Dean of Research and outside any school, or semi-independent of the University itself.\n\nThese report directly to the Vice-Provost and Dean of Research and are outside any school though any faculty involved in them must belong to a department in one of the schools. These include Bio-X and Spectrum in the area of Biological and Life Sciences; Precourt Institute for Energy and Woods Institute for the Environment in the Environmental Sciences area; the Center for Advanced Study in the Behavioral Sciences (CASBS), the Center for the Study of Language and Information (CSLI) (see below), Freeman Spogli Institute for International Studies (FSI) (see below), Human-Sciences and Technologies Advance Research Institute (H-STAR), Stanford Center on Longevity (SCL), Stanford Humanities Center (see below), and the Stanford Institute for Economic Policy Research (SIEPR) in the area of Humanities and Social Sciences; and, for Physical Sciences, the Edward L. Ginzton Laboratory, the Geballe Laboratory for Advanced Materials, the Kavli Institute for Particle Astrophysics and Cosmology, Photon Ultrafast Laser Science and Engineering (PULSE), Stanford Institute for Materials and Energy Sciences (SIMES), and W. W. Hansen Experimental Physics Laboratory (HEPL).\n\nThe Center for the Study of Language and Information (CSLI) is an independent research center at Stanford University. Founded in 1983 by philosophers, computer scientists, linguists, and psychologists from Stanford, SRI International, and Xerox PARC, it strives to study all forms of information and improve how humans and computers acquire and process it.\n\nCSLI was initially funded by a US$15 million grant from the System Development Foundation (SDF) for the Situated Language Project, the name of which reflects the strong influence of the work on situation semantics by philosophers John Perry and Jon Barwise, two of the initial leaders of CSLI. This funding supported operations for the first few years as well as the construction of Cordura Hall. Subsequent funding has come from research grants and from an industrial affiliates program.\n\nCSLI's publications branch, founded and still headed by Dikran Karagueuzian, has grown into an important publisher of work in linguistics and related fields. Researchers associated with CSLI include Ronald Kaplan, Patrick Suppes, the mathematicians Keith Devlin, and Solomon Feferman, the linguists Ivan Sag and Joan Bresnan, Annie Zaenen, Lauri Karttunen, and psychologists Herb Clark, B. J. Fogg and Clifford Nass.\n\nCSLI houses the Stanford Encyclopedia of Philosophy. It also housed the Reuters Digital Vision Program.\n\n\nThe Freeman Spogli Institute for International Studies is a university-wide research and teaching organization at Stanford devoted to understanding international and cross-border policies, problems and institutions.\n\nFSI's core and affiliated faculty represent a range of academic backgrounds and perspectives, including medicine, law, engineering, history, political science, economics, and sociology. The faculty's research and teaching focus on a variety of issues, including governance, domestic and international health policy, migration, development, and security. Their work often examines regional dynamics in areas such as Asia, Europe, Africa and Latin America. FSI faculty conduct research, lead interdisciplinary research programs, educate graduate and undergraduate students, and organize policy outreach that engages Stanford in addressing some of the world's most pressing problems.\n\nThe institute is composed of 12 centers and programs, including six major research centers:\n\nThe institute was founded in 1987 following a faculty committee review that concluded Stanford “should be leading the way in International Studies as we do in science and technology,” encompassing interdisciplinary teaching, research, public service and administrative functions. It was first called the institute for International Studies, and was created under the direction of former Stanford President Richard Wall Lyman.\n\nThe institute was renamed the Freeman Spogli Institute for International Studies in 2005 following a $50 million gift made by Stanford alumni Bradford M. Freeman and Ronald P. Spogli.\n\nThe immediate past director of FSI was Mariano-Florentino Cuéllar, the former Stanley Morrison Professor of Law at Stanford Law School, a former official in the Obama and Clinton presidential administrations, and current Justice of the California Supreme Court. Previous Directors include Stanford President Emeritus Gerhard Casper; Coit D. Blacker, who served as Special Assistant to the President for National Security Affairs and Senior Director for Russian, Ukrainian and Eurasian Affairs at the National Security Council under National Security Advisor Anthony Lake during the Clinton administration; David Holloway; Walter Falcon; and Stanford President Emeritus Richard Lyman.\n\nFSI appoints faculty and research staff, funds research and scholarly initiatives, directs research projects, and sponsors lectures, policy seminars and conferences. By tradition, FSI undertakes joint faculty appointments with Stanford's seven schools and draws faculty together from the University's academic departments and schools to conduct interdisciplinary research on international issues that transcend academic boundaries.\n\nThe institute is home to 40 billeted faculty members – most with joint appointments – and 115 affiliated faculty members with a wide range of academic perspectives.\n\nIn addition to its six centers, the institute sponsors the Ford Dorsey Program in International Policy Studies, the Inter-University Center for Japanese Language Studies, the Program on Energy & Sustainable Development, the Rural Education Action Program, the Stanford Center at Peking University, and the Stanford Program on International and Cross-Cultural Education.\n\n\nFounded in 1980, the Stanford Humanities Center is a multidisciplinary research institute dedicated to advancing knowledge about culture, philosophy, history, and the arts.\n\nSince its founding in 1980, the Stanford Humanities Center has been sponsoring advanced research into the historical, philosophical, literary, artistic, and cultural dimensions of the human experience. The Humanities Center's annual fellows, international visitors, research workshops, digital humanities laboratory, and roughly fifty annual public events strengthen the intellectual and creative life of the university, foster innovative and interdisciplinary scholarship and teaching, and enrich our understanding of our common humanity. The humanities support democratic culture by nurturing an informed citizenry and seeking solutions to society’s most formidable challenges.\n\nThe Center offers approximately twenty-five year-long residential fellowships to Stanford and non-Stanford scholars at different career stages, giving them the opportunity to pursue their research in a supportive intellectual community.\n\nEach year, Stanford faculty and graduate students create fifteen diverse research workshops to ask new intellectual questions that often challenge disciplinary boundaries. In addition to providing a space for incubating new ideas in a collegial setting, the workshops professionalize graduate students by introducing them to the conventions of academic life.\n\nAssembling a team of faculty experts from Stanford and other universities, the Manuscript Review workshops provide critical feedback to junior faculty preparing monographs or other academic manuscripts of similar scope for submission for publication.\n\nThe Center brings eminent scholars, public intellectuals, and renowned critics to the Stanford campus for lectures and interdisciplinary conferences that enrich the Stanford community with a lively exchange of ideas. Speakers have included Isabel Allende, Roger Chartier, Stephen Jay Gould, Douglas Hofstadter, Gayatri Spivak, Marilynne Robinson, David Adjaye, David Eggers, and other well-known scholars.\n\nThe Humanities Center, with the Center for Spatial and Textual Analysis (CESTA), is expanding the possibilities of humanities research and teaching at Stanford by creating opportunities for the discovery and dissemination of new knowledge. Humanities Center scholars are on the forefront of innovation with access to new digital tools to interpret the human experience.\n\nThe Center's short-term visitorships draw distinguished international scholars to Stanford to share their research in lectures and seminars with Stanford faculty and students.\n\nThe Humanities Center awards Hume Humanities Honors Fellowships to Stanford seniors writing an honors thesis in a humanities department. In residence for an academic year, Hume fellows contribute to the collegial life of the Center and receive intellectual guidance and mentoring from staff and fellows.\n\n\nThe Distinguished Careers Institute (DCI), established in 2014, is a year-long residential fellowship for approximately 20 individuals who have already established leadership careers. Fellows are selected based on \"how their participation in the program will shape their future life journeys\" as well as \"what future Fellows will contribute to the program and the broader global community.\"\n\nThe Stanford Artificial Intelligence Laboratory (also known as Stanford AI Lab, SU-AI or SAIL) is the artificial intelligence (AI) research laboratory of Stanford University. The current director is Associate Professor Fei-Fei Li.\n\nSAIL was started in 1963 by John McCarthy, after he moved from Massachusetts Institute of Technology to Stanford. Lester D. \"Les\" Earnest, also previously of MIT, served as executive officer (self-deprecatingly, \"Chief Bureaucrat\") at SAIL from 1965 to 1980. During the same years, SAIL was housed in the D.C. Power building, named not for \"Direct Current\" but rather for Donald Clinton Power, who held the positions of president, C.E.O. and chairman of General Telephone & Electronics Corporation (later GTE Corporation) between 1951 and 1971. GT&E donated the unfinished building to Stanford University after abandoning plans to establish a research center there. During this period SAIL was one of the leading centers for AI research and an early ARPANET site.\n\nD.C. Power was on a hill overlooking Felt Lake in the foothills of the Santa Cruz Mountains overlooking Stanford.\nIt was about 5 miles (8 km) from the main campus, at 1600 Arastradero Road, midway between Page Mill Road and Alpine Road. \nThis area was, and remains, quite rural in nature. Combined with the rather extreme 1960s architecture of the place, this remote setting led to a certain isolation. Some people who worked there reported feeling as if they were already in the future. The building was demolished in 1986; as of 2003, the site is home to Portola Pastures (an equestrian center adjacent to the Arastradero Open Space Preserve).\n\nSAIL created the WAITS operating system on a computer called SAIL. WAITS ran on various models of Digital Equipment Corporation PDP-10 computers, starting with the PDP-6, then the KA10 and KL10. WAITS also ran on Foonly systems at CCRMA and LLL. The SAIL system was shut down in 1991.\n\nSAIL, the Stanford Artificial Intelligence Language, was developed by Dan Swinehart and Bob Sproull of the Stanford AI Lab in 1970.\n\nIn 1980, its activities were merged into the university's Computer Science Department and it moved into Margaret Jacks Hall in the main Stanford campus.\n\nSAIL was reopened in 2004, with Sebastian Thrun becoming its new director. SAIL's 21st century mission is to \"change the way we understand the world\"; its researchers contribute to fields such as bioinformatics, cognition, computational geometry, computer vision, decision theory, distributed systems, game theory, general game playing, image processing, information retrieval, knowledge systems, logic, machine learning, multi-agent systems, natural language, neural networks, planning, probabilistic inference, sensor networks, and robotics.\n\nSAIL alumni played a major role in many Silicon Valley firms, becoming founders of now-large firms such as Cisco Systems and Sun Microsystems as well as smaller companies such as Vicarm Inc. (acquired by Unimation), Foonly, Elxsi, Imagen, Xidex, Valid Logic Systems, and D.E. Shaw & Co. Research accomplishments at SAIL were many, including in the fields of speech recognition and robotics.\n\nThe Center for Entrepreneurial Studies (CES) at Stanford University is a multidisciplinary business oriented program targeted to both undergraduate and graduate students. It incorporates courses from Stanford University School of Engineering and Stanford Graduate School of Business. It also incorporates Stanford Mayfield Scholars Program that seeks to give select undergraduate students an opportunity to take business related coursework and to intern in high tech startups. CES was founded by Tom Byers and Charles A. Holloway.\n\nThe Stanford University Center for Computer Research in Music and Acoustics (CCRMA), founded by John Chowning, is a multi-discipline facility where composers and researchers work together using computer-based technology both as an artistic medium and as a research tool. CCRMA's director is Chris Chafe. CCRMA's current faculty includes a mix of musicians and engineers including Julius Smith, Jonathan Berger, Max Mathews (emeritus), Ge Wang, Takako Fujioka, Tom Rossing, Jonathan Abel, Marina Bosi, David Berners, Jay Kadis, and Fernando Lopez-Lezcano. Emeritus professor Max Mathews died in 2011 \n\nWidely used digital sound synthesis techniques like FM synthesis and digital waveguide synthesis were developed CCRMA and licensed to industry partners. The FM synthesis patent brought Stanford $20 million before it expired, making it (in 1994) \"the second most lucrative licensing agreement in Stanford's history\".\n\nAlmost 100 years ago, this Spanish Gothic residence, known as the Knoll, was originally designed by Louis Christian Mullgardt, and built as a residence for the University's President. In 1946, the building became home to the Music Department, and then in 1986, CCRMA took over residency.\n\nDamaged in 1989 during the Loma Prieta earthquake, the Knoll nonetheless housed CCRMA in its damaged condition until a complete internal reconstruction between 2004–2005. The reopening of the facility was celebrated in the Spring of 2005 with the CCRMA: newStage Festival. This unique building now comprises several state-of-the-art music studios and top-notch research facilities, hosting a variety of students, artists and scientists.\n\nCCRMA is affiliated with the Center for Computer Assisted Research in the Humanities (CCARH), also located at Stanford. CCARH conducts research on constructing computer databases for music and on creating programs that allow researchers to access, analyze, print, and electronically perform the music.\n\nThe Stanford Institute for Creativity and the Arts (SiCa), established in 2006, serves as the core programmatic hub for the Stanford Arts Initiative, leading the development of new undergraduate arts programs, hosting artists in residence, awarding grants for multidisciplinary arts research and teaching, incubating collaborative performances and exhibitions with campus partners and other institutions, and providing centralized communication for arts events and programs at Stanford University.\n\nThe Department of Civil and Environmental Engineering maintains the National Performance of Dams Program, a national database of structural and operational data related to dam systems in the U.S. Begun in 1994, this program provides data to the dam engineering and safety community about the in-service performance of dam systems. The analysis of this data covering both successful operations and incidents, including failures, is intended to lead to improvements in design and requirements, engineering processes and standards, operational procedures and guidelines, and public policy development.\nFounded in 1974, and named after economist Michelle R. Clayman, the Michelle R. Clayman Institute for Gender Research at Stanford University is one of the nation’s oldest research organizations focused on the study of gender. The Clayman Institute designs basic interdisciplinary research, creates knowledge] networks people and ideas at Stanford, nationally, and internationally to effect change and promote gender equality. The Clayman Institute plays an integral role in the Stanford community by bringing together local, national and international scholars and thought leaders from across disciplines to create knowledge and effect change. The Clayman Institute is located at Serra House at Stanford.\n\nIn 1972 faculty and graduate students in the feminist movement were the impetus behind the formation of the Institute. In 1974, the Center for Research on Women (CROW) was the first interdisciplinary center or institute of its kind and quickly built a strong reputation under the direction of Myra Strober, the founding Director. The reputation of CIGR grew outside Stanford, and the University of Chicago Press chose Stanford as the base of the second five-year rotation of its new interdisciplinary journal, \"Signs\". \nIn 1983 the Institute was renamed the Institute for Research on Women and Gender (IRWG) and continued to expand the gender conversation with the “Difficult Dialogues” program, which ran in the 1990s through 2004. In 2004, the new Director, Professor Londa Schiebinger, a historian of science, formed a plan to create a series of research initiatives on gender issues, backed by a research fellowship program, that would attract scholars from Stanford and abroad. With the help of matching funds from the William and Flora Hewlett Foundation and strong support from the Institute’s Advisory Council, Schiebinger spearheaded a fundraising drive to create an endowment for the Institute. IRWG was renamed in honor of Michelle R. Clayman, the major donor in the campaign, who serves as the Chair of the Institute’s Advisory Council.\n\nThe Clayman Institute designs basic research and supports the creation of knowledge through its Fellowships and interdisciplinary programs. Recent reports/publications include:\n\nThe Clayman Institute runs two fellowship programs. The Faculty Research Fellowships seek to drive intellectual and social innovation through interdisciplinary gender studies. They include residential fellowships for tenured, tenure-track, and postdoctoral scholars from Stanford University, and U.S. and foreign universities. The Clayman Institute also offers Graduate Dissertation Fellowships for Stanford University doctoral students. Fellowships are awarded to students who are in the writing stages of their dissertations, and whose research focuses on women and/or gender.\n\n\nStanford's Center for Computer Research and Acoustics is part of a consortium with CNMAT and the Institut de Recherche et Coordination Acoustique/Musique (IRCAM) in Paris.\n\n\n", "id": "23087437", "title": "Stanford University centers and institutes"}
{"url": "https://en.wikipedia.org/wiki?curid=54407907", "text": "Oxbotica\n\nOxbotica is a British autonomous vehicle software company, headquartered in Summertown, Oxford, England and with a test facility at Culham, Oxford.\n\nOxbotica produces full stack autonomy software for land based vehicles. Its autonomy system is called Selenium. Selenium’s application domains and clients are varied and include Mobility as a Service providers (MaaS), Automotive OEMs, Defence, Port and Airport Logistics and Mining. \n\nOxbotica was founded as a spin-out from Oxford University's Department of Engineering Science Mobile Robotics Group, by two Oxford professors, Paul Newman and Ingmar Posner in 2014.\n\nIn 2017, Oxbotica ran an autonomous grocery delivery service with Ocado in London.\n\nIn 2017, Oxbotica provided the autonomy software for an autonomous shuttle in Greenwich.\nAs part of the UK CCAV Funded DRIVEN project Oxbotica is developing and deploying a fleet of autonomous vehicles (Ford Fusion) running between London and Oxford, and in conjunction with its consortium partners, running real-time insurance.\n\nXL Catlin has announced a partnership with Oxbotica on the development of smart insurance products using Oxbotica Autonomy Technology.\n\n2017<br>\n<br>\n\n2016\n", "id": "54407907", "title": "Oxbotica"}
{"url": "https://en.wikipedia.org/wiki?curid=1757239", "text": "Copycat (software)\n\nCopycat is a model of analogy making and human cognition based on the concept of the parallel terraced scan, developed in 1988 by Douglas Hofstadter, Melanie Mitchell, and others at the Center for Research on Concepts and Cognition, Indiana University Bloomington. The original Copycat was written in Common Lisp and is bitrotten (as it relies on now-outdated graphics libraries); however, Java and Python ports exist.\n\nCopycat produces answers to such problems as \"abc is to abd as ijk is to what?\" (abc:abd :: ijk:?). Hofstadter and Mitchell consider analogy making as the core of high-level cognition, or \"high-level perception\", as Hofstadter calls it, basic to recognition and categorization.\nHigh-level perception emerges from the spreading activity of many independent processes, called \"codelets\", running in parallel, competing or cooperating. They create and destroy temporary perceptual constructs, probabilistically trying out variations to eventually produce an answer. The codelets rely on an associative network, \"slipnet\", built on pre-programmed concepts and their associations (a long-term memory). The changing activation levels of the concepts make a conceptual overlap with neighboring concepts.\n\nCopycat's architecture is tripartite, consisting of a \"slipnet\", a \"working area\" (also called workspace, similar to blackboard systems), and the \"coderack\" (with the codelets). The slipnet is a network composed of nodes, which represent permanent concepts, and weighted links, which are relations, between them. It differs from traditional semantic networks as the effective weight associated with a particular link may vary through time according to the activation level of specific concepts (nodes). The codelets build structures in the working area and modify activations in the slipnet accordingly (bottom-up processes), and the current state of slipnet determines probabilistically which codelets must be run (top-down influences).\n\nCopycat differs considerably in many respects from other cognitive architectures such as ACT-R, Soar, DUAL, Psi, or subsumption architectures.\n\nCopycat is Hofstadter's most popular model. Other models presented by Hofstadter et al. are similar in architecture, but different in the so-called microdomain, their application, e.g. Letter Spirit, etc.\n\nSince the 1995 book \"Fluid Concepts and Creative Analogies\" describing the work of the Fluid Analogies Research Group (FARG) book, work on Copycat-like models has continued: as of 2008 the latest models are Phaeaco (a Bongard problem solver), SeqSee (number sequence extrapolation), George (geometric exploration), and Musicat (a melodic expectation model). The architecture is known as the \"FARGitecture\" and current implementations use a variety of modern languages including C# and Java. A future FARG goal is to build a single generic FARGitecture software framework to facilitate experimentation.\n\n\n\n", "id": "1757239", "title": "Copycat (software)"}
{"url": "https://en.wikipedia.org/wiki?curid=2311118", "text": "Parallel terraced scan\n\nThe parallel terraced scan is a multi-agent based search technique that is basic to cognitive architectures, such as Copycat, Letter-string, the Examiner, Tabletop, and others. It was developed by John Rehling and Douglas Hofstadter at the Center for Research on Concepts and Cognition at Indiana University, Bloomington. \n\nThe parallel terraced scan builds on the concepts of the workspace, coderack, conceptual memory, and temperature. According to Hofstadter the parallel and random nature of the processing captures aspects of human cognition.\n\n\n", "id": "2311118", "title": "Parallel terraced scan"}
{"url": "https://en.wikipedia.org/wiki?curid=8233911", "text": "Procedural reasoning system\n\nIn artificial intelligence, a procedural reasoning system (PRS) is a framework for constructing real-time reasoning systems that can perform complex tasks in dynamic environments. It is based on the notion of a rational agent or intelligent agent using the belief–desire–intention software model.\n\nA user application is predominately defined, and provided to a PRS system is a set of \"knowledge areas\". Each knowledge area is a piece of procedural knowledge that specifies how to do something, e.g., how to navigate down a corridor, or how to plan a path (in contrast with robotic architectures where the programmer just provides a model of what the states of the world are and how the agent's primitive actions affect them). Such a program, together with a PRS interpreter, is used to control the agent.\n\nThe interpreter is responsible for maintaining beliefs about the world state, choosing which goals to attempt to achieve next, and choosing which knowledge area to apply in the current situation. How exactly these operations are performed might depend on domain-specific meta-level knowledge areas. Unlike traditional AI planning systems that generate a complete plan at the beginning, and replan if unexpected things happen, PRS interleaves planning and doing actions in the world. At any point, the system might only have a partially specified plan for the future.\n\nPRS is based on the BDI or belief–desire–intention framework for intelligent agents. Beliefs consist of what the agent believes to be true about the current state of the world, desires consist of the agent's goals, and intentions consist of the agent's current plans for achieving those goals. Furthermore, each of these three components is typically \"explicitly\" represented somewhere within the memory of the PRS agent at runtime, which is in contrast to purely reactive systems, such as the subsumption architecture.\n\nThe PRS concept was developed by the Artificial Intelligence Center at SRI International during the 1980s, by many workers including Michael Georgeff, Amy L. Lansky, and François Félix Ingrand. Their framework was responsible for exploiting and popularizing the BDI model in software for control of an intelligent agent. The seminal application of the framework was a fault detection system for the reaction control system of the NASA Space Shuttle Discovery. Development on this PRS continued at the Australian Artificial Intelligence Institute through to the late 1990s, which lead to the development of a C++ implementation and extension called dMARS.\n\nThe system architecture of SRI's PRS includes the following components:\n\nSRI's PRS was developed for embedded application in dynamic and real-time environments. As such it specifically addressed the limitations of other contemporary control and reasoning architectures like expert systems and the blackboard system. The following define the general requirements for the development of their PRS:\n\n\nThe seminal application of SRI's PRS was a monitoring and fault detection system for the reaction control system (RCS) on the NASA space shuttle. The RCS provides propulsive forces from a collection of jet thrusters and controls altitude of the space shuttle. A PRS-based fault diagnostic system was developed and tested using a simulator. It included over 100 KAs and over 25 meta level KAs. RCS specific KAs were written by space shuttle mission controllers. It was implemented on the Symbolics 3600 Series LISP machine and used multiple communicating instances of PRS. The system maintained over 1000 facts about the RCS, over 650 facts for the forward RCS alone and half of which are updated continuously during the mission. A version of the PRS was used to monitor the reaction control system on the NASA Space Shuttle Discovery.\n\nPRS was tested on Shakey the robot including navigational and simulated jet malfunction scenarios based on the space shuttle. Later applications included a network management monitor called the Interactive Real-time Telecommunications Network Management System (IRTNMS) for Telecom Australia.\n\nThe following list the major implementations and extensions of the PRS architecture.\n\n\n\n", "id": "8233911", "title": "Procedural reasoning system"}
{"url": "https://en.wikipedia.org/wiki?curid=3559731", "text": "DUAL (cognitive architecture)\n\nDUAL is a general cognitive architecture integrating the connectionist and symbolic approaches at the micro level. DUAL is based on decentralized representation and emergent computation. It was inspired by the Society of Mind idea proposed by Marvin Minsky but departs from the initial proposal in many ways. Computations in DUAL emerge from the interaction of many micro-agents each of which is hybrid symbolic/connectionist device. The agents exchange messages and activation via links that can be learnt and modified, they form coalitions which collectively represent concepts, episodes, and facts. \n\nSeveral models have been developed on the basis of DUAL. These include: AMBR (a model of analogy-making and memory), JUDGEMAP (a model of judgment), PEAN (a model of perception), etc.\n\nDUAL is developed by a team at the New Bulgarian University led by Boicho Kokinov. The second version was co-authored by Alexander Petrov. The third version is co-authored by Georgi Petkov and Ivan Vankov.\n\n", "id": "3559731", "title": "DUAL (cognitive architecture)"}
{"url": "https://en.wikipedia.org/wiki?curid=7827235", "text": "R-CAST\n\nR-CAST is a group decision support system based on research on naturalistic decision making. Its architecture, based on multiple software agents, supports decision-making teams by anticipating information relevant to their decisions based on a shared mental model about the context of decision making.\n\nIn this digital information age, decision-making teams are often flooded with an overwhelming amount of information. This leads to two challenges:\n\nThe R-CAST technology aims to address both of these challenges.\n\nThe R-CAST approach is based on four major concepts:\n\nIn addition to anticipating information needed for decision makings, R-CAST agents also collaborate to seek and fuse information in a distributed environment such as Service-oriented architecture. R-CAST is developed at the Intelligent Agents Laboratory in the College of Information Sciences and Technology at Pennsylvania State University, led by Dr. John Yen. \n\nThe R-CAST architecture is component-based and reconfigurable. By selecting components suitable for an application, R-CAST can be configured into a wide range of agents: from simple reflex agents to RPD-enabled agents. Key components of R-CAST include the RPD model interpreter, the knowledge base, the information manager, the process manager, the communication manager, and adapters for various domains. The RPD model interpreter matches the current situation with known experiences, which are organized into a hierarchy. Missing cues relevant to the current decision are identified. The information manager uses the information dependency in the knowledge base to infer missing information that is relevant to the higher-level cues, option evaluations, and anomalies. The communication manager then contact agents that provide the missing information. To build a model, one has to (a) determine what components are involved to compose the model, (b) analyze tasks and elicit relevant knowledge for the components, and (c) develop domain adapter to integrate agents to the external environment.\n\nR-CAST agents have been used to develop decision-making aids for human teams. They have also be used to study team cognition and issues related to human-agent collaboration in time-stressed application domains.\n\n\n\n", "id": "7827235", "title": "R-CAST"}
{"url": "https://en.wikipedia.org/wiki?curid=729751", "text": "Soar (cognitive architecture)\n\nSoar is a cognitive architecture, originally created by John Laird, Allen Newell, and Paul Rosenbloom at Carnegie Mellon University. (Rosenbloom continued to serve as co-principal investigator after moving to Stanford University, then to the University of Southern California's Information Sciences Institute.) It is now maintained and developed by John Laird's research group at the University of Michigan.\n\nThe goal of the Soar project is to develop the fixed computational building blocks necessary for general intelligent agents – agents that can perform a wide range of tasks and encode, use, and learn all types of knowledge to realize the full range of cognitive capabilities found in humans, such as decision making, problem solving, planning, and natural language understanding. It is both a theory of what cognition is and a computational implementation of that theory. Since its beginnings in 1983 as John Laird’s thesis, it has been widely used by AI researchers to create intelligent agents and cognitive models of different aspects of human behavior. The most current and comprehensive description of Soar is the 2012 book, \"The Soar Cognitive Architecture.\"\n\nSoar embodies multiple hypotheses about the computational structures underlying general intelligence, many of which are shared with other cognitive architectures, including ACT-R, which was created by John R. Anderson, and LIDA, which was created by Stan Franklin. Recently, the emphasis on Soar has been on general AI (functionality and efficiency), whereas the emphasis on ACT-R has always been on cognitive modeling (detailed modeling of human cognition).\n\nThe original theory of cognition underlying Soar is the Problem Space Hypothesis, which is described in Allen Newell's book, \"Unified Theories of Cognition\". and dates back to one of the first AI systems created, Newell, Simon, and Shaw's Logic Theorist, first presented in 1955. The Problem Space Hypothesis contends that all goal-oriented behavior can be cast as search through a space of possible states (a \"problem space\") while attempting to achieve a goal. At each step, a single operator is selected, and then applied to the agent’s current state, which can lead to internal changes, such as retrieval of knowledge from long-term memory or modifications or external actions in the world. (Soar’s name is derived from this basic cycle of State, Operator, And Result; however, it is no longer regarded as an acronym.) Inherent to the Problem Space Hypothesis is that all behavior, even a complex activity such as planning, is decomposable into a sequence of selection and application of primitive operators, which when mapped onto human behavior take ~50ms.\n\nA second hypothesis of Soar’s theory is that although only a single operator can be selected at each step, forcing a serial bottleneck, the processes of selection and application are implemented through parallel rule firings, which provide context-dependent retrieval of procedural knowledge.\n\nA third hypothesis is that if the knowledge to select or apply an operator is incomplete or uncertain, an impasse arises and the architecture automatically creates a substate. In the substate, the same process of problem solving is recursively used, but with the goal to retrieve or discover knowledge so that decision making can continue. This can lead to a stack of substates, where traditional problem methods, such as planning or hierarchical task decomposition, naturally arise. When results created in the substate resolve the impasse, the substate and its associated structures are removed. The overall approach is called Universal Subgoaling.\n\nThese assumptions lead to an architecture that supports three levels of processing. At the lowest level, is bottom-up, parallel, and automatic processing. The next level is the deliberative level, where knowledge from the first level is used to propose, select, and apply a single action. These two levels implement fast, skilled behavior, and roughly correspond to Kahneman’s System 1 processing level. More complex behavior arises automatically when knowledge is incomplete or uncertain, through a third level of processing using substates, roughly corresponding to System 2.\n\nA fourth hypothesis in Soar is that the underlying structure is modular, but not in terms of task or capability based modules, such as planning or language, but instead as task independent modules including: a decision making module; memory modules (short-term spatial/visual and working memories; long-term procedural, declarative, and episodic memories), learning mechanisms associated with all long-term memories; and perceptual and motor modules. There are further assumptions about the specific properties of these memories described below, including that all learning is online and incremental.\n\nA fifth hypothesis is that memory elements (except those in the spatial/visual memory) are represented as symbolic, relational structures. The hypothesis that a symbolic system is necessary for general intelligence is known as the \"physical symbol system hypothesis\". An important evolution in Soar is that all symbolic structures have associated statistical metadata (such as information on recency and frequency of use, or expected future reward) that influences retrieval, maintenance, and learning of the symbolic structures.\n\nSoar’s main processing cycle arises from the interaction between procedural memory (its knowledge about how to do things) and working memory (its representation of the current situation) to support the selection and application of operators. Information in working memory is represented as a symbolic graph structure, rooted in a \"state.\" The knowledge in procedural memory is represented as if-then rules (sets of conditions and actions), that are continually matched against the contents of working memory. When the conditions of a rule matches structures in working memory, it \"fires\" and performs its actions. This combination of rules and working memory is also called a production system. In contrast to most production systems, in Soar, all rules that match, fire in parallel.\n\nInstead of having the selection of a single rule being the crux of decision making, Soar’s decision making occurs through the selection and applications of \"operators\", that are proposed, evaluated, and applied by rules. An operator is proposed by rules that test the current state and create a representation of the operator in working memory as well as an \"acceptable preference\", which indicates that the operator should be considered for selection and application. Additional rules match with the proposed operator and create additional preferences that compare and evaluate it against other proposed operators. The preferences are analyzed by a decision procedure, which selects the preferred operator and installs it as the current operator in working memory. Rules that match the current operator then fire to apply it and make changes to working memory. The changes to working memory can be simple inferences, queries for retrieval from Soar’s long-term semantic or episodic memories, commands to the motor system to perform actions in an environment, or interactions with the Spatial Visual System (SVS), which is working memory’s interface to perception. These changes to working memory lead to new operators being proposed and evaluated, followed by the selection of one and its application.\n\nSoar supports reinforcement learning, which tunes the values of rules that create numeric preferences for evaluating operators, based on reward. To provide maximal flexibility, there is a structure in working memory where reward is created.\n\nIf the preferences for the operators are insufficient to specify the selection of a single operator, or there are insufficient rules to apply an operator, an impasse arises. In response to an impasse, a substate is created in working memory, with the goal being to resolve the impasse. Additional procedural knowledge can then propose and select operators in the substate to gain more knowledge, and either create preferences in the original state or modify that state so the impasse is resolved. Substates provide a means for on-demand complex reasoning, including hierarchical task decomposition, planning, and access to the declarative long-term memories. Once the impasse is resolved, all of the structures in the substate are removed except for any results. Soar’s chunking mechanism compiles the processing in the substate which led to results into rules. In the future, the learned rules automatically fire in similar situations so that no impasse arises, incrementally converting complex reasoning into automatic/reactive processing.\n\nSymbolic input and output occurs through working memory structures attached to the top state called the input-link and the output-link. If structures are created on the output-link in working memory, these are translated into commands for external actions (e.g., motor control).\n\nTo support interaction with vision systems and non-symbolic reasoning, Soar has its Spatial Visual System (SVS). SVS internally represents the world as a \"scene graph\", a collection of objects and component subobjects each with spatial properties such as shape, location, pose, relative position, and scale. A Soar agent using SVS can create filters to automatically extract features and relations from its scene graph, which are then added to working memory. In addition, a Soar agent can add structures to SVS and use it for mental imagery. For example, an agent can create a hypothetical object in SVS at a given location and query to see if it collides with any perceived objects.\n\nSemantic Memory (SMEM) in Soar is designed to be a very large long-term memory of fact-like structures. Data in SMEM is represented as directed cyclic graphs. Structures can be stored or retrieved by rules that create commands in a reserved area of working memory. Retrieved structures are added to working memory.\n\nSMEM structures have activation values that represent the frequency or recency of usage of each memory, implementing the \"base-level activation\" scheme originally developed for ACT-R. During retrieval, the structure in SMEM that matches the query and has the highest activation is retrieved. Soar also supports \"spreading activation\", where activation spreads from SMEM structures that have been retrieved into working memory to other long-term memories that they are linked to. These memories in turn spread activation to their neighbor memories, with some decay. Spreading activation is a mechanism for allowing the current context to influence retrievals from semantic memory.\n\nEpisodic Memory (EPMEM) automatically records snapshots of working memory in a temporal stream. Prior episodes can be retrieved into working memory through query. Once an episode has been retrieved, the next (or previous) episode can then be retrieved. An agent may employ EPMEM to sequentially play through episodes from its past (allowing it to predict the effects of actions), retrieve specific memories, or query for episodes possessing certain memory structures.\n\nEach of Soar’s long-term memories have associated online learning mechanisms that create new structures or modify metadata based on an agent’s experience. For example, Soar learns new rules for procedural memory through a process called \"chunking\" and uses reinforcement learning to tune rules involved in the selection of operators.\n\nThe standard approach to developing an agent in Soar starts with writing rules that are loaded into procedural memory, and initializing semantic memory with appropriate declarative knowledge. \nThe process of agent development is explained in detail in the official Soar manual as well as in several tutorials which are provided at the research group's website.\n\nThe Soar architecture is maintained and extended by John Laird's research group at the University of Michigan. The current architecture is written in a combination of C and C++, and is freely available (BSD license) at the research group's website.\n\nSoar can interface with external language environments including C++, Java, Tcl, and Python through the Soar Markup Language (SML). SML is a primary mechanism for creating instances of Soar agents and interacting with their I/O links.\n\nJSoar is an implementation of Soar written in Java. It is maintained by SoarTech, an AI research and development company. JSoar closely follows the University of Michigan architecture implementation, although it generally does not reflect the latest developments and changes of that C/C++ version.\n\nBelow is a historical list of different areas of applications that have been implemented in Soar. There have been over a hundred systems implemented in Soar, although the vast majority of them are toy tasks or puzzles.\n\nThroughout its history, Soar has been used to implement a wide variety of classic AI puzzles and games, such as Tower of Hanoi, Water Jug, Tic Tac Toe, Eight Puzzle, Missionaries and Cannibals, and variations of the Blocks World. One of the initial achievements of Soar was showing that many different weak methods would naturally arise from the task knowledge that was encoded in it, a property called, the \"Universal Weak Method.\" \n\nThe first large-scale application of Soar was R1-Soar, a partial reimplementation by Paul Rosenbloom of the R1 (XCON) expert system John McDermott developed for configuring DEC computers. R1-Soar demonstrated the ability of Soar to scale to moderate-size problems, use hierarchical task decomposition and planning, and convert deliberate planning and problem solving to reactive execution through chunking.\n\nNL-Soar was a natural language understanding system developed in Soar by Jill Fain Lehman, Rick Lewis, Nancy Green, Deryle Lonsdale and Greg Nelson. It included capabilities for natural language comprehension, generation, and dialogue, emphasizing real-time incremental parsing and generation. NL-Soar was used in an experimental version of TacAir-Soar and in NTD-Soar.\n\nThe second large-scale application of Soar involved developing agents for use in training in large-scale distributed simulation. Two major systems for flying U.S. tactical air missions were co-developed at the University of Michigan and Information Sciences Institute (ISI) of University of Southern California. The Michigan system was called TacAir-Soar and flew (in simulation) fixed-wing U. S. military tactical missions (such as close-air support, strikes, CAPs, refueling, and SEAD missions). The ISI system was called RWA-Soar and flew rotary-wing (helicopter) missions. Some of the capabilities incorporated in TacAir-Soar and RWA-Soar were attention, situational awareness and adaptation, real-time planning and dynamic replanning, and complex communication, coordination, and cooperation among combinations of Soar agents and humans. These systems participated in DARPA’s Synthetic Theater of War (STOW-97) Advanced Concept Technology Demonstration (ACTD), which at the time was the largest fielding of synthetic agents in a joint battlespace over a 48-hour period, and involved training of active duty personnel. These systems demonstrated the viability of using AI agents for large-scale training.\n\nOne of the important outgrowths of the RWA-Soar project was the development of STEAM by Milind Tambe, a framework for flexible teamwork in which agents maintained models of their teammates using the joint intentions framework by Cohen & Levesque.\n\nNTD-Soar was a simulation of the NASA Test Director (NTD), the person responsible for coordinating the preparation of the NASA Space Shuttle before launch. It was an integrated cognitive model that incorporated many different complex cognitive capabilities including natural language processing, attention and visual search, and problem solving in a broad agent model.\n\nSoar has been used to simulate virtual humans supporting face-to-face dialogues and collaboration within a virtual world developed at the Institute of Creative Technology at USC. Virtual humans have integrated capabilities of perception, natural language understanding, emotions, body control, and action, among others.\n\nGame AI agents have been built using Soar for games such as , Quake II, Descent 3, Unreal Tournament, and Minecraft, supporting capabilities such as spatial reasoning, real-time strategy, and opponent anticipation. AI agents have also been created for video games including Infinite Mario which used reinforcement learning, and Frogger II, Space Invaders, and Fast Eddie, which used both reinforcement learning and mental imagery.\n\nSoar can run natively on mobile devices. A mobile application for the game Liar’s Dice has been developed for iOS which runs the Soar architecture directly from the phone as the engine for opponent AIs.\n\nMany different robotic applications have been built using Soar since the original Robo-Soar was implemented in 1991 for controlling a Puma robot arm. These have ranged from mobile robot control to humanoid service REEM robots, taskable robotic mules and unmanned underwater vehicles.\n\nA current focus of research and development in the Soar community is Interactive Task Learning (ITL), the automatic learning of new tasks, environment features, behavioral constraints, and other specifications through natural instructor interaction. Research in ITL has been applied to tabletop game playing and multi-room navigation.\n\nEarly on, Merle-Soar demonstrated how Soar could learn a complex scheduling task modeled after the lead human scheduler in a windshield production plant located near Pittsburgh. \n\n\n\n", "id": "729751", "title": "Soar (cognitive architecture)"}
{"url": "https://en.wikipedia.org/wiki?curid=12838512", "text": "EPAM\n\nEPAM (Elementary Perceiver and Memorizer) is a psychological theory of learning and memory implemented as a computer program. Originally designed by Herbert A. Simon and Edward Feigenbaum to simulate phenomena in verbal learning, it has been later adapted to account for data on the psychology of expertise and concept formation. It was influential in formalizing the concept of a chunk. In EPAM, learning consists in the growth of a discrimination net.\nEPAM was written in IPL/V.\n\n\n", "id": "12838512", "title": "EPAM"}
{"url": "https://en.wikipedia.org/wiki?curid=13550938", "text": "CLARION (cognitive architecture)\n\nConnectionist Learning with Adaptive Rule Induction On-line (CLARION) is a computational cognitive architecture that has been used to simulate many domains and tasks in cognitive psychology and social psychology, as well as implementing intelligent systems in artificial intelligence applications. An important feature of CLARION is the distinction between implicit and explicit processes and focusing on capturing the interaction between these two types of processes. The system was created by the research group led by Ron Sun.\nCLARION is an integrative architecture, consisting of a number of distinct subsystems, with a dual representational structure in each subsystem (implicit versus explicit representations; Sun et al., 2005). Its subsystems include the action-centered subsystem, the non-action-centered subsystem, the motivational subsystem, and the meta-cognitive subsystem.\n\nThe role of the action-centered subsystem is to control both external and internal actions. The implicit layer is made of neural networks called Action Neural Networks, while the explicit layer has is made up of action rules. There can be synergy between the two layers, for example learning a skill can be expedited when the agent has to make explicit rules for the procedure at hand. It has been argued that implicit knowledge alone cannot optimize as well as the combination of both explicit and implicit.\n\nThe role of the non-action-centered subsystem is to maintain general knowledge. The implicit layer is made of Associative Neural Networks, while the bottom layer is associative rules. Knowledge is further divided into semantic and episodic, where semantic is generalized knowledge, and episodic is knowledge applicable to more specific situations. It is also important to note since there is an implicit layer, that not all declarative knowledge has to be explicit.\n\nThe role of the motivational subsystem is to provide underlying motivations for perception, action, and cognition.The motivational system in CLARION is made up of drives on the bottom level, and each drive can have varying strengths. There are low level drives, and also high level drives aimed at keeping an agent sustained, purposeful, focused, and adaptive. The explicit layer of the motivational system is composed of goals. explicit goals are used because they are more stable than implicit motivational states. the CLARION framework views that human motivational processes are highly complex and can't be represented through just explicit representation.\n\nExamples of some low level drives include:\n\nExamples of some high level drives include:\n\nThere is also a possibility for derived drives (usually from trying to satisfy primary drives) that can be created by either conditioning, or through external instructions. each drive needed will have a proportional strength, opportunity will also be taken into account\n\nThe role of the meta-cognitive subsystem is to monitor, direct, and modify the operations of all the other subsystems. Actions in the meta-cognitive subsystem include: setting goals for the action-centred subsystem, setting parameters for the action and non-action subsystems, and changing an ongoing process in both the action and non-action subsystems.\n\nLearning can be represented with both explicit and implicit knowledge individually while also representing bottom-up and top-down learning. Learning with implicit knowledge is represented through Q-learning, while learning with just explicit knowledge is represented with one-shot learning such as hypothesis testing. Bottom-up learning (Sun et al., 2001) is represented through a neural network propagating up to the explicit layer through the Rule-Extraction-Refinement algorithm (RER), while top-down learning can be represented through a variety of ways.\n\nTo compare with a few other cognitive architectures (Sun, 2016):\n\n\nCLARION has been used to account for a variety of psychological data (Sun, 2002, 2016), such as the serial reaction time task, the artificial grammar learning task, the process control task, a categorical inference task, an alphabetical arithmetic task, and the Tower of Hanoi task. The serial reaction time and process control tasks are typical implicit learning tasks (mainly involving implicit reactive routines), while the Tower of Hanoi and alphabetic arithmetic are high-level cognitive skill acquisition tasks (with a significant presence of explicit processes). In addition, extensive work has been done on a complex minefield navigation task, which involves complex sequential decision-making. Work on organizational decision tasks and other social simulation tasks (e.g., Naveh and Sun, 2006), as well as meta-cognitive tasks, has also been initiated.\n\nOther applications of the cognitive architecture include simulation of creativity (Helie and Sun, 2010) and addressing the computational basis of consciousness (or artificial consciousness) (Coward and Sun, 2004).\n\nCoward, L.A. & Sun, R. (2004). Criteria for an effective theory of consciousness and some preliminary attempts. \"Consciousness and Cognition\", \"13\", 268-301.\n\nHelie, H. and Sun, R. (2010). Incubation, insight, and creative problem solving: A unified theory and a connectionist model. \"Psychological Review\", \"117\", 994-1024.\n\nNaveh, I. & Sun, R. (2006). A cognitively based simulation of academic science. \"Computational and Mathematical Organization Theory\", \"12\", 313-337.\n\nSun, R. (2002). Duality of the Mind: A Bottom-up Approach Toward Cognition. Mahwah, NJ: Lawrence Erlbaum Associates.\n\nSun, R. (2016). Anatomy of the Mind: Exploring Psychological Mechanisms and Processes with the Clarion Cognitive Architecture. Oxford University Press, New York. \n\nSun, R. (2003). A Tutorial on CLARION 5.0. Technical Report, Cognitive Science Department, Rensselaer Polytechnic Institute.\n\nSun, R., Merrill, E., & Peterson, T. (2001). From implicit skills to explicit knowledge: A bottom-up model of skill learning. \"Cognitive Science\", \"25\", 203-244. http://www.cogsci.rpi.edu/~rsun/\n\nSun, R., Slusarz, P., & Terry, C. (2005). The interaction of the explicit and the implicit in skill learning: A dual-process approach. \"Psychological Review\", \"112\", 159-192. http://www.cogsci.rpi.edu/~rsun/\n\nSun, R. & Zhang, X. (2006). Accounting for a variety of reasoning data within a cognitive architecture. \"Journal of Experimental and Theoretical Artificial Intelligence\", \"18\", 169-191.\n\n", "id": "13550938", "title": "CLARION (cognitive architecture)"}
{"url": "https://en.wikipedia.org/wiki?curid=12632281", "text": "CHREST\n\nCHREST (Chunk Hierarchy and REtrieval STructures) is a symbolic cognitive architecture based on the concepts of limited attention, limited short-term memories, and chunking. Learning, which is essential in the architecture, is modelled as the development of a network of nodes (chunks) which are connected in various ways. This can be contrasted with Soar and ACT-R, two other cognitive architectures, which use productions for representing knowledge. CHREST has often been used to model learning using large corpora of stimuli representative of the domain, such as chess games for the simulation of chess expertise or child-directed speech for the simulation of children’s development of language. In this respect, the simulations carried out with CHREST have a flavor closer to those carried out with connectionist models than with traditional symbolic models.\n\nThe architecture contains a number of capacity parameters (e.g., capacity of visual short-term memory, set at three chunks) and time parameters (e.g., time to learn a chunk or time to put information into short-term memory). This makes it possible to derive precise and quantitative predictions about human behaviour.\n\nModels based on CHREST have been used, among other things, to simulate data on the acquisition of chess expertise from novice to grandmaster, children’s acquisition of vocabulary, children’s acquisition of syntactic structures, and concept formation.\n\nCHREST is developed by Fernand Gobet at Brunel University and Peter C. Lane at the University of Hertfordshire. It is the successor of EPAM, a cognitive model originally developed by Herbert A. Simon and Edward Feigenbaum.\n\n\n", "id": "12632281", "title": "CHREST"}
{"url": "https://en.wikipedia.org/wiki?curid=22912887", "text": "MIBE architecture\n\nMIBE architecture (\"Motivated Independent BEhavior\") is a behavior-based robot architecture developed at Artificial Intelligence and Robotics Lab of Politecnico di Milano by Fabio La Daga and Andrea Bonarini in 1998. MIBE architecture is based on the idea of animat and derived from subsumption architecture, formerly developed by Rodney Brooks and colleagues at MIT in 1986.\n\nMIBE architecture is based on the belief that autonomy is grounded on motivation and autonomy should arise from superimposition of synergetic activities in response to multiple drives. An autonomous agent is developed to achieve several goals (primary goals), but secondary goals also originate as a consequence of environmental or functional constraints. In MIBE architecture both primary and secondary goals are handled in the same way and defined as \"needs\". A specific drive originates from each need.\nMIBE architecture generates and weights all these drives in an explicit \"motivational state\". The higher the urgency to satisfy a specific need, the higher its weight in the motivational state and the higher the drive to perform a behavior that satisfies the given need.\n\nMIBE architecture mainly departs from subsumption architecture due to the introduction of a top level \"motivational structure\" which determines behavior priorities at run time. That is, there are not layers and static hierarchical dependencies between behavioral modules, but each behavior constantly competes with others for taking control of the agent through the top level motivational state from which specific drives originate (via predetermined or reinforcement-learned functions).\n\nWhile subsumption architecture is built on a predetermined hierarchy of behavioral modules, MIBE architecture consists of a more complex structure, where several behaviors (that always compete for taking control of the robot via the motivational state) can activate and control dynamically an adaptive set of underlying modules, called \"abilities\". Each behavior performs its task by activating and tuning the abilities it needs.\nAbilities supply the functional modules for performing specific activities and may activate each other in a hierarchical structure in the same way behaviors use abilities.\nBoth behaviors and abilities are implemented by the same kind of functional modules, but a fundamental difference exists: behaviors are \"self-activating\" modules in response to a robot+environment state, whilst abilities are just \"functional blocks\" activated and controlled by behaviors for accomplishing their tasks (or by higher-level abilities that have been already activated by a behavior).\nBehaviors exist for satisfying specific needs, whilst abilities are not related to any need, because they are used by behaviors for accomplishing their tasks, but have no meaning alone.\n\nThe list of abilities needed by each module (behavior or ability) is represented by its \"activation tree\"; the complete set of activation trees can be represented by a system-wide acyclic \"activation graph\".\n\nBehaviors are activated on the basis of their specific drive pressure in the motivational state at run time: the most motivated behavior (i.e., the most urgent or convenient) is always activated. Nevertheless, less motivated behaviors could even be activated at the same time, with the constraint they cannot use any of the abilities already collected by a more motivated behavior which is already acting. The BCR subsystem (\"Behavior Conflict Resolver\") ensures no conflicting behaviors (i.e., sharing one or more abilities in their activation trees) can be active at the same time.\n\nThe main advantage of MIBE architecture consists in its high \"molecularity\": new abilities and behaviors can be added easily without changing the existing modular structure. Similarly, a behavior can be modified or removed with no functional effects on other modules.\nEven \"drives\" can be added to/removed from the motivational structure or modified with no need to change the previous system structure, except rebalancing the drive-generation functions.\n\nThe main issue of MIBE architecture consists in defining the boundaries of the \"state-space\" by shaping the motivational structure (i.e.: tuning the drive-generation functions and/or their learning algorithms) so that the autonomous agent performs the right behavior for each robot+environment state.\n\n\n", "id": "22912887", "title": "MIBE architecture"}
{"url": "https://en.wikipedia.org/wiki?curid=23572558", "text": "Source of activation confusion model\n\nSAC (source of activation confusion) is a computational model of memory encoding and retrieval. It has been developed by Lynne M. Reder at Carnegie Mellon University. It shares many commonalities with ACT-R.\n\nSAC specifies a memory representation consisting of a network of both semantic (concept) and perceptual nodes (such as font) and associated episodic (context) nodes. Similar to her husband's (John Anderson) model, ACT-R, the node activations are governed by a set of common computational principles such as spreading activation and the strengthening and decay of activation. However, a unique feature of the SAC model are episode nodes, which are newly formed memory traces that binds the concepts involved with the current experiential context. A recent addition to SAC are assumptions governing the probability of forming an association during encoding. These bindings are affected by working memory resources available.\n\nSAC is considered among a class of dual-process models of memory, since recognition involves two processes: a general \"familiarity\" process based on the activation of semantic (concept) nodes and a more specific \"recollection\" process based on the activation of episodic (context) nodes. This feature has allowed SAC to model a variety of memory phenomena, such as meta-cognitive (rapid) feeling of knowing judgments, remember-know judgments, the word frequency mirror effect, age-related memory loss perceptual fluency, paired associate recognition and cued recall, as well as account for implicit and explicit memory tasks without positing an unconscious memory system for priming.\n", "id": "23572558", "title": "Source of activation confusion model"}
{"url": "https://en.wikipedia.org/wiki?curid=25256781", "text": "Comparison of cognitive architectures\n\nThe following table compares cognitive architectures.\n\n\n", "id": "25256781", "title": "Comparison of cognitive architectures"}
{"url": "https://en.wikipedia.org/wiki?curid=1700176", "text": "Cognitive architecture\n\nA cognitive architecture can refer to a theory about the structure of the human mind. One of the main goals of a cognitive architecture is to summarize the various results of cognitive psychology in a comprehensive computer model. However, the results need to be formalized so far as they can be the basis of a computer program. The formalized models can be used to further refine a comprehensive theory of cognition, and more immediately, as a commercially usable model. Successful cognitive architectures include ACT-R (Adaptive Control of Thought, ACT) and SOAR.\n\nThe Institute of Creative Technologies defines cognitive architecture as: \"\"hypothesis about the fixed structures that provide a mind, whether in natural or artificial systems, and how they work together – in conjunction with knowledge and skills embodied within the architecture – to yield intelligent behavior in a diversity of complex environments.\"\n\nHerbert A. Simon, one of the founders of the field of artificial intelligence, stated that the 1960 thesis by his student Ed Feigenbaum, EPAM provided a possible \"architecture for cognition\" because it included some commitments for how more than one fundamental aspect of the human mind worked (in EPAM's case, human memory and human learning).\n\nJohn R. Anderson started research on human memory in the early 1970s and his 1973 thesis with Gordon H. Bower provided a theory of human associative memory. He included more aspects of his research on long-term memory and thinking processes into this research and eventually designed a cognitive architecture he eventually called ACT. He and his student used the term \"cognitive architecture\" in his lab to refer to the ACT theory as embodied in the collection of papers and designs since they didn't yet have any sort of complete implementation at the time.\n\nIn 1983 John R. Anderson published the seminal work in this area, entitled \"The Architecture of Cognition.\" One can distinguish between the theory of cognition and the implementation of the theory. The theory of cognition outlined the structure of the various parts of the mind and made commitments to the use of rules, associative networks, and other aspects. The cognitive architecture implements the theory on computers. The software used to implement the cognitive architectures were also \"cognitive architectures\". Thus, a cognitive architecture can also refer to a blueprint for intelligent agents. It proposes (artificial) computational processes that act like certain cognitive systems, most often, like a person, or acts intelligent under some definition. Cognitive architectures form a subset of general agent architectures. The term 'architecture' implies an approach that attempts to model not only behavior, but also structural properties of the modelled system.\n\nCognitive architectures can be symbolic, connectionist, or hybrid. Some cognitive architectures or models are based on a set of generic rules, as, e.g., the Information Processing Language (e.g., Soar based on the unified theory of cognition, or similarly ACT-R). Many of these architectures are based on the-mind-is-like-a-computer analogy. In contrast subsymbolic processing specifies no such rules a priori and relies on emergent properties of processing units (e.g. nodes). Hybrid architectures combine both types of processing (such as CLARION). A further distinction is whether the architecture is centralized with a neural correlate of a processor at its core, or decentralized (distributed). The decentralized flavor, has become popular under the name of parallel distributed processing in mid-1980s and connectionism, a prime example being neural networks. A further design issue is additionally a decision between holistic and atomistic, or (more concrete) modular structure. By analogy, this extends to issues of knowledge representation.\n\nIn traditional AI, intelligence is often programmed from above: the programmer is the creator, and makes something and imbues it with its intelligence, though many traditional AI systems were also designed to learn (e.g. improving their game-playing or problem-solving competence). Biologically inspired computing, on the other hand, takes sometimes a more bottom-up, decentralised approach; bio-inspired techniques often involve the method of specifying a set of simple generic rules or a set of simple nodes, from the interaction of which emerges the overall behavior. It is hoped to build up complexity until the end result is something markedly complex (see complex systems). However, it is also arguable that systems designed top-down on the basis of observations of what humans and other animals can do rather than on observations of brain mechanisms, are also biologically inspired, though in a different way.\n\nA comprehensive review of implemented cognitive architectures has been undertaken in 2010 by Samsonovich et al. and is available as an online repository. Some well-known cognitive architectures, in alphabetical order:\n\n\n", "id": "1700176", "title": "Cognitive architecture"}
{"url": "https://en.wikipedia.org/wiki?curid=23882349", "text": "4CAPS\n\n4CAPS (Cortical Capacity-Constrained Concurrent Activation-based Production System) is a cognitive architecture developed by Marcel A. Just and Sashank Varma at Carnegie Mellon University. It is the successor of the CAPS and 3CAPS cognitive architectures.\n\nIn 4CAPS computations are distributed and dynamically balanced among independent processing centers. Like in other cognitive architectures (e.g., ACT-R), these processing centers have been identified with corresponding cortical regions in the human brain. Performing specific task, such as reading or driving, requires the simultaneous contribution of many of such regions.\n\nNotably, 4CAPS differs from other architectures for its stress on the capacity constraints (that is, limited computational power), and the dynamic collaboration between different centers. In particular, according to Just and Varma, 4CAPS is based on four characteristic assumptions:\n\n\nLike other cognitive architectures (such as ACT-R, EPIC, and Soar), 4CAPS is implemented as a production system. It is written in the Common Lisp programming language. This system has been used to create computational models for a variety of phenomena, especially in the field of cognitive neuroscience. In particular, 4CAPS models have been created and used to fit behavioral and imaging data for tasks such as the Tower of London, mental rotation, and dual-tasking.\n\n\n", "id": "23882349", "title": "4CAPS"}
{"url": "https://en.wikipedia.org/wiki?curid=821071", "text": "ACT-R\n\nACT-R (pronounced /ˌækt ˈɑr/; short for \"Adaptive Control of Thought—Rational\") is a cognitive architecture mainly developed by John Robert Anderson at Carnegie Mellon University. Like any cognitive architecture, ACT-R aims to define the basic and irreducible cognitive and perceptual operations that enable the human mind. \nIn theory, each task that humans can perform should consist of a series of these discrete operations.\n\nMost of the ACT-R's basic assumptions are also inspired by the progress of cognitive neuroscience, and ACT-R can be seen and described as a way of specifying how the brain itself is organized in a way that enables individual processing modules to produce cognition.\n\nACT-R has been inspired by the work of Allen Newell, and especially by his lifelong championing the idea of unified theories as the only way to truly uncover the underpinnings of cognition.\nIn fact, John Anderson usually credits Allen Newell as the major source of influence over his own theory.\n\nLike other influential cognitive architectures (including Soar, CLARION, and EPIC), the ACT-R theory has a computational implementation as an interpreter of a special coding language. The interpreter itself is written in Common Lisp, and might be loaded into any of the Common Lisp language distributions.\n\nThis means that any researcher may download the ACT-R code from the ACT-R website, load it into a Common Lisp distribution, and gain full access to the theory in the form of the ACT-R interpreter.\n\nAlso, this enables researchers to specify models of human cognition in the form of a script in the ACT-R language. The language primitives and data-types are designed to reflect the theoretical assumptions about human cognition. \nThese assumptions are based on numerous facts derived from experiments in cognitive psychology and brain imaging. \n\nLike a programming language, ACT-R is a framework: for different tasks (e.g., Tower of Hanoi, memory for text or for list of words, language comprehension, communication, aircraft controlling), researchers create \"models\" (i.e., programs) in ACT-R.\nThese models reflect the modelers' assumptions about the task within the ACT-R view of cognition. \nThe model might then be run. \n\nRunning a model automatically produces a step-by-step simulation of human behavior which specifies each individual cognitive operation (i.e., memory encoding and retrieval, visual and auditory encoding, motor programming and execution, mental imagery manipulation). \nEach step is associated with quantitative predictions of latencies and accuracies. \nThe model can be tested by comparing its results with the data collected in behavioral experiments.\n\nIn recent years, ACT-R has also been extended to make quantitative predictions of patterns of activation in the brain, as detected in experiments with fMRI.\nIn particular, ACT-R has been augmented to predict the shape and time-course of the BOLD response of several brain areas, including the hand and mouth areas in the motor cortex, the left prefrontal cortex, the anterior cingulate cortex, and the basal ganglia.\n\nACT-R's most important assumption is that human knowledge can be divided into two irreducible kinds of representations: \"declarative\" and \"procedural\". \nWithin the ACT-R code, declarative knowledge is represented in the form of \"chunks\", i.e. vector representations of individual properties, each of them accessible from a labelled slot.\n\nChunks are held and made accessible through \"buffers\", which are the front-end of what are \"modules\", i.e. specialized and largely independent brain structures.\n\nThere are two types of modules:\n\n\nAll the modules can only be accessed through their buffers. The contents of the buffers at a given moment in time represents the state of ACT-R at that moment. The only exception to this rule is the procedural module, which stores and applies procedural knowledge. It does not have an accessible buffer and is actually used to access other module's contents. \n\nProcedural knowledge is represented in form of \"productions\". The term \"production\" reflects the actual implementation of ACT-R as a production system, but, in fact, a production is mainly a formal notation to specify the information flow from cortical areas (i.e. the buffers) to the basal ganglia, and back to the cortex.\n\nAt each moment, an internal pattern matcher searches for a production that matches the current state of the buffers. Only one such production can be executed at a given moment. That production, when executed, can modify the buffers and thus change the state of the system. Thus, in ACT-R, cognition \nunfolds as a succession of production firings.\n\nIn the cognitive sciences, different theories are usually ascribed to either the \"symbolic\" or the \"connectionist\" approach to cognition. ACT-R clearly belongs to the \"symbolic\" field and is classified as such in standard textbooks and collections. Its entities (chunks and productions) are discrete and its operations are syntactical, that is, not referring to the semantic content of the representations but only to their properties that deem them appropriate to participate in the computation(s). This is seen clearly in the chunk slots and in the properties of buffer matching in productions, both of which function as standard symbolic variables. \n\nMembers of the ACT-R community, including its developers, prefer to think of ACT-R as a general framework that specifies how the brain is organized, and how its organization gives birth to what is perceived (and, in cognitive psychology, investigated) as mind, going beyond the traditional symbolic/connectionist debate. None of this, naturally, argues against the classification of ACT-R as symbolic system, because all symbolic approaches to cognition aim to describe the mind, as a product of brain function, using a certain class of entities and systems to achieve that goal.\n\nA common misunderstanding suggests that ACT-R may not be a symbolic system because it attempts to characterize brain function. This is incorrect on two counts: First, all approaches to computational modeling of cognition, symbolic or otherwise, must in some respect characterize brain function, because the mind is brain function. And second, all such approaches, including connectionist approaches, attempt to characterize the mind at a cognitive level of description and not at the neural level, because it is only at the cognitive level at which important generalizations can be retained. \n\nFurther misunderstandings arise because of the associative character of certain ACT-R properties, such as chunks spreading activation to each other, or chunks and productions carrying quantitative properties relevant to their selection. None of these properties counter the fundamental nature of these entities as symbolic, regardless of their role in unit selection and, ultimately, in computation.\n\nThe importance of distinguishing between the theory itself and its implementation is usually highlighted by ACT-R developers. \n\nIn fact, much of the implementation does not reflect the theory. \nFor instance, the actual implementation makes use of additional 'modules' that exist only for purely computational reasons, and are not supposed to reflect anything in the brain (e.g., one computational module contains the pseudo-random number generator used to produce noisy parameters, while another holds naming routines for generating data structures accessible through variable names). \n\nAlso, the actual implementation is designed to enable researchers to modify the theory, e.g. by altering the standard parameters, or creating new modules, or partially modifying the behavior of the existing ones. \n\nFinally, while Anderson's laboratory at CMU maintains and releases the official ACT-R code, other alternative implementations of the theory have been made available. These alternative implementations include \"jACT-R\" (written in Java by Anthony M. Harrison at the Naval Research Laboratory) and \"Python ACT-R\" (written in Python by Terrence C. Stewart and Robert L. West at Carleton University, Canada).\n\nSimilarly, ACT-RN (now discontinued) was a full-fledged neural implementation of the 1993 version of the theory. \nAll of these versions were fully functional, and models have been written and run with all of them. \n\nBecause of these implementational degrees of freedom, the ACT-R community usually refers to the \"official\", Lisp-based, version of the theory, when adopted in its original form and left unmodified, as \"Vanilla ACT-R\".\n\nOver the years, ACT-R models have been used in more than 700 different scientific publications, and have been cited in many more.\n\nThe ACT-R declarative memory system has been used to model human memory since its inception. In the course of years, it has been adopted to successfully model a large number of known effects. They include the fan effect of interference for associated information, primacy and recency effects for list memory, and serial recall. \n\nACT-R has been used to model attentive and control processes in a number of cognitive paradigms. These include the Stroop task, task switching, the psychological refractory period, and multi-tasking.\n\nA number of researchers have been using ACT-R to model several aspects of natural language understanding and production. They include models of syntactic parsing, language understanding, language acquisition and metaphor comprehension.\n\nACT-R has been used to capture how humans solve complex problems like the Tower of Hanoi, or how people solve algebraic equations. It has also been used to model human behavior in driving and flying.\n\nWith the integration of perceptual-motor capabilities, ACT-R has become increasingly popular as a modeling tool in human factors and human-computer interaction. In this domain, it has been adopted to model driving behavior under different conditions, menu selection and visual search on computer application, and web navigation.\n\nMore recently, ACT-R has been used to predict patterns of brain activation during imaging experiments. In this field, ACT-R models have been successfully used to predict prefrontal and parietal activity in memory retrieval, anterior cingulate activity for control operations, and practice-related changes in brain activity.\n\nACT-R has been often adopted as the foundation for cognitive tutors. These systems use an internal ACT-R model to mimic the behavior of a student and personalize his/her instructions and curriculum, trying to \"guess\" the difficulties that students may have and provide focused help.\n\nSuch \"Cognitive Tutors\" are being used as a platform for research on learning and cognitive modeling as part of the Pittsburgh Science of Learning Center. Some of the most successful applications, like the Cognitive Tutor for Mathematics, are used in thousands of schools across the United States.\n\nACT-R is the ultimate successor of a series of increasingly precise models of human cognition developed by John R. Anderson. \n\nIts roots can be backtraced to the original HAM (Human Associative Memory) model of memory, described by John R. Anderson and Gordon Bower in 1973. The HAM model was later expanded into the first version of the ACT theory. This was the first time the procedural memory was added to the original declarative memory system, introducing a computational dichotomy that was later proved to hold in human brain. The theory was then further extended into the ACT* model of human cognition.\n\nIn the late eighties, Anderson devoted himself to exploring and outlining a mathematical approach to cognition that he named Rational analysis. The basic assumption of Rational Analysis is that cognition is optimally adaptive, and precise estimates of cognitive functions mirror statistical properties of the environment. Later on, he came back to the development of the ACT theory, using the Rational Analysis as a unifying framework for the underlying calculations. To highlight the importance of the new approach in the shaping of the architecture, its name was modified to ACT-R, with the \"R\" standing for \"Rational\" \n\nIn 1993, Anderson met with Christian Lebiere, a researcher in connectionist models mostly famous for developing with Scott Fahlman the Cascade Correlation learning algorithm. Their joint work culminated in the release of ACT-R 4.0. Thanks to Mike Byrne (now at Rice University), version 4.0 also included optional perceptual and motor capabilities, mostly inspired from the EPIC architecture, which greatly expanded the possible applications of the theory.\n\nAfter the release of ACT-R 4.0, John Anderson became more and more interested in the underlying neural plausibility of his life-time theory, and began to use brain imaging techniques pursuing his own goal of understanding the computational underpinnings of human mind.\n\nThe necessity of accounting for brain localization pushed for a major revision of the theory. ACT-R 5.0 introduced the concept of modules, specialized sets of procedural and declarative representations that could be mapped to known brain systems. In addition, the interaction between procedural and declarative knowledge was mediated by newly introduced buffers, specialized structures for holding temporarily active information (see the section above). Buffers were thought to reflect cortical activity, and a subsequent series of studies later confirmed that activations in cortical regions could be successfully related to computational operations over buffers. \n\nA new version of the code, completely rewritten, was presented in 2005 as ACT-R 6.0. It also included significant improvements in the ACT-R coding language.\n\nThe long development of the ACT-R theory gave birth to a certain number of parallel and related projects. \n\nThe most important ones are the PUPS production system, an initial implementation of Anderson's theory, later abandoned; and ACT-RN, a neural network implementation of the theory developed by Christian Lebiere.\n\nLynne M. Reder, also at Carnegie Mellon University, developed in the early nineties SAC, a model of conceptual and perceptual aspects of memory that shares many features with the ACT-R core declarative system, although differing in some assumptions.\n\n\n", "id": "821071", "title": "ACT-R"}
{"url": "https://en.wikipedia.org/wiki?curid=83552", "text": "Subsumption architecture\n\nSubsumption architecture is a reactive robotic architecture heavily associated with behavior-based robotics which was very popular in the 1980s and 90s. The term was introduced by Rodney Brooks and colleagues in 1986. Subsumption has been widely influential in autonomous robotics and elsewhere in real-time AI.\n\nSubsumption architecture is a control architecture that was proposed in opposition to traditional AI, or GOFAI. Instead of guiding behavior by symbolic mental representations of the world, subsumption architecture couples sensory information to action selection in an intimate and bottom-up fashion.\n\nIt does this by decomposing the complete behavior into sub-behaviors. These sub-behaviors are organized into a hierarchy of layers. Each layer implements a particular level of behavioral competence, and higher levels are able to subsume lower levels in order to create viable behavior. For example, a robot's lowest layer could be \"avoid an object\". The second layer would be \"wander around\", which runs beneath the third layer \"explore the world\". Because a robot must have the ability to \"avoid objects\" in order to \"wander around\" effectively, the subsumption architecture creates a system in which the higher layers utilize the lower-level competencies. The layers, which all receive sensor-information, work in parallel and generate outputs. These outputs can be commands to actuators, or signals that suppress or inhibit other layers.\n\nSubsumption architecture attacks the problem of intelligence from a significantly different perspective than traditional AI. Disappointed with the performance of Shakey the robot and similar representation-inspired projects, Rodney Brooks started creating robots based on a different notion of intelligence. Instead of modelling aspects of human intelligence via symbol manipulation, this approach is aimed at real-time interaction and viable responses to a dynamic lab or office environment.\n\nThe goal was informed by four key ideas:\n\nThe ideas outlined above are still a part of an ongoing debate regarding the nature of intelligence and how the progress of robotics and AI should be fostered.\n\nEach layer is made up by a set of processors that are augmented finite-state machines (AFSM), the augmentation being added instance variables to hold programmable data-structures. A layer is a module and is responsible for a single behavioral goal, such as \"wander around.\" There is no central control within or between these behavioral modules. All AFSMs continuously and asynchronously receive input from the relevant sensors and send output to actuators (or other AFSMs). Input signals that are not read by the time a new one is delivered end up getting discarded. These discarded signals are common, and is useful for performance because it allows the system to work in real time by dealing with the most immediate information.\n\nBecause there is no central control, AFSMs communicate with each other via inhibition and suppression signals. Inhibition signals block signals from reaching actuators or AFSMs, and suppression signals blocks or replaces the inputs to layers or their AFSMs. This system of AFSM communication is how higher layers subsume lower ones (see figure 1), as well as how the architecture deals with priority and action selection arbitration in general.\n\nThe development of layers follows an intuitive progression. First the lowest layer is created, tested, and debugged. Once that lowest level is running, one creates and attaches the second layer with the proper suppression and inhibition connections to the first layer. After testing and debugging the combined behavior, this process can be repeated for (theoretically) any number of behavioral modules.\n\nThe following is a small list of robots that utilize the subsumption architecture.\n\n\nThe above are described in detail along with other robots in \"Elephants Don't Play Chess\".\n\nThe main advantages of the architecture are:\n\nThe main disadvantages of the architecture are:\n\nWhen subsumption architecture was developed, the novel setup and approach of subsumption architecture allowed it to be successful in many important domains where traditional AI had failed, namely real-time interaction with a dynamic environment. The lack of large memory storage, symbolic representations, and central control, however, places it at a disadvantage at learning complex actions, in-depth mapping, and understanding language.\n\n\nKey papers include:\n\n", "id": "83552", "title": "Subsumption architecture"}
{"url": "https://en.wikipedia.org/wiki?curid=15311631", "text": "Psi-theory\n\nPsi-theory, developed by Dietrich Dörner at the University of Bamberg, is a systemic psychological theory covering human action regulation, intention selection and emotion. It models the human mind as an information processing agent, controlled by a set of basic physiological, social and cognitive drives. Perceptual and cognitive processing are directed and modulated by these drives, which allow the autonomous establishment and pursuit of goals in an open environment.\n\nNext to the motivational and emotional system, Psi-theory suggests a neuro-symbolic model of representation, which encodes semantic relationships in a hierarchical spreading activation network. The representations are grounded in sensors and actuators, and are acquired by autonomous exploration.\n\nThe concepts of Psi-theory may be reduced to a set of basic assumptions. Psi-theory describes a cognitive system as a structure consisting of relationships and dependencies that is designed to maintain a homeostatic balance in the face of a dynamic environment.\n\nPsi-theory suggests hierarchical networks of nodes as a universal mode of representation for declarative, procedural and tacit knowledge. These nodes may encode localist and distributed representations. The activity of the system is modeled using modulated and directional spreading of activation within these networks.\n\nPlans, episodes, situations and objects are described with a semantic network formalism that relies on a fixed number of pre-defined link types, which especially encode causal/sequential ordering, and partonomic hierarchies (the theory specifies four basic link-types). Special nodes (representing neural circuits) control the spread of activation and the forming of temporary or permanent associations and their dissociations.\n\nAt any time, the Psi agent possesses a world model (\"situation image\"). This is extrapolated into a branching \"expectation horizon\" (consisting of anticipated developments and active plans). In addition, the working memory also contains a \"hypothetical world model\" that is used for comparisons during recognition, and for planning.\n\nThe situation image is gradually transferred into an episodic memory (protocol). By selective decay and reinforcement, portions of this long-term memory provide automated behavioral routines, and elements for plans (procedural memory).\n\nThe atoms of plans and behavior sequences are \"triplets\" of a (partial, hierarchical) situation description, forming a condition, an operator (a hierarchical action description) and an expected outcome of the operation as another (partial, hierarchical) situation description. Object descriptions (mainly declarative) are also part of long-term memory and the product of perceptual processes and affordances. Situations and operators in long-term memory may be associated with motivational relevance, which is instrumental in retrieval and reinforcement. Operations on memory content are subject to emotional modulation.\n\nPerception is based on \"conceptual hypotheses\", which guide the recognition of objects, situations and episodes. Hypothesis based perception (\"HyPercept\") is understood as a bottom-up (data-driven and context-dependent) cueing of hypotheses that is interleaved with a top-down verification. The acquisition of schematic hierarchical descriptions and their gradual adaptation and revision can be described as assimilation and accommodation.\n\nHypothesis based perception is a universal principle that applies to visual perception, auditory perception, discourse interpretation and even memory interpretation. Perception is subject to emotional modulation.\n\nThe activity of the system is directed towards the satisfaction of a finite set of primary, pre-defined drives (or urges). All goals are situations that are associated (by learning) with the satisfaction of an urge, or situations that are instrumental in achieving such a situation (this also includes abstract problem solving, aesthetics, the maintenance of social relationships and altruistic behavior). These urges reflect demands of the system: a mismatch between a target value of a demand and the current value results in an urge signal, which is proportional to the deviation, and which might give rise to a motive.\n\nThere are three categories of drives:\n\n\nChanges in systemic demands are reflected in a \"pleasure\" or \"distress signal\", which is used as for reinforcement learning of associations between demands and goals, as well as episodic sequences and behavior scripts leading up to these goals.\n\nCognitive processing is subject to global modulatory parameters, which adjust the cognitive resources of the system to the environmental and internal situation. These modulators control behavioral tendencies (action readiness via general activation or arousal), stability of active behaviors/chosen goals (selection threshold), the rate of orientation behavior (sampling rate or securing threshold) and the width and depth of activation spreading in perceptual processing, memory retrieval and planning (activation and resolution level). The effect and the range of modulator values are subject to individual variance.\n\nEmotion is not understood as an independent sub-system, a module or a parameter set, but an intrinsic aspect of cognition. Emotion is an emergent property of the modulation of perception, behavior and cognitive processing, and it can therefore not be understood outside the context of cognition. To model emotion, we need a cognitive system that can be modulated to adapt its use of processing resources and behavior tendencies.\n\nIn the Psi-theory, emotions are interpreted as a configurational setting of the cognitive modulators along with the pleasure/distress dimension and the assessment of the cognitive urges. The phenomenological qualities of emotion are due to the effect of modulatory settings on perception and cognitive functioning (i.e. the perception yields different representations of memory, self and environment depending on the modulation), and to the experience of accompanying physical sensations that result from the effects of the particular modulator settings on the physiology of the system (for instance, by changing the muscular tension, the digestive functions, blood pressure and so on). The experience of emotion as such (i.e. as having an emotion) requires reflective capabilities. Undergoing a modulation is a necessary, but not a sufficient condition of experiencing it as an emotion.\n\nMotives are combinations of drives and a goal. Goals are represented by a situation that affords the satisfaction of the corresponding urge. Several motives may be active at a time, but only one is chosen to determine the choice of behaviors of the agent. The choice of the dominant motive depends on the anticipated probability of satisfying the associated urge and the strength of the urge signal. (This means also that the agent may opportunistically satisfy another urge if presented with that option.)\n\nThe stability of the dominant motive against other active motivations is regulated using the \"selection threshold\" parameter, which depends on the urgency of the demand and individual variance.\n\n\"Perceptual learning\" comprises the assimilation/accommodation of new/existing schemas by hypothesis based perception. \"Procedural learning\" depends on reinforcing the associations of actions and preconditions (situations that afford these actions) with appetitive or aversive goals, which is triggered by pleasure and distress signals. \"Abstractions\" may be learned by evaluating and reorganizing episodic and declarative descriptions to generalize and fill in missing interpretations (this facilitates the organization of knowledge according to conceptual frames and scripts).\n\nBehavior sequences and object/situation representations are strengthened by use. \"Tacit knowledge\" (especially sensory-motor capabilities) may be acquired by neural learning.\n\nUnused associations decay, if their strength is below a certain threshold: highly relevant knowledge may not be forgotten, while spurious associations tend to disappear.\n\nProblem solving is directed towards finding a path between a given situation and a goal situation, on completing or reorganizing mental representations (for instance, the identification of relationships between situations or of missing features in a situational frame) or serves an exploratory goal.\n\nProblem solving is organized in stages: If no immediate response to a problem is found, the system first attempts to resort to a behavioral routine (automatism), and if this is not successful, it attempts to construct a plan. If planning fails, the system resorts to exploration (or switches to another motive). Problem solving is context dependent (contextual priming is served by associative pre-activation of mental content) and subject to modulation.\n\nThe strategies that encompass problem solving are parsimonious. They can be reflected upon and reorganized according to learning and experience. Many advanced problem solving strategies can not be adequately modeled without assuming linguistic capabilities.\n\nLanguage has to be explained as syntactically organized symbols that designate conceptual representations, and a model of language thus starts with a model of mental representation. Language extends cognition by affording the categorical organization of concepts and by aiding in meta-cognition. (Cognition is not interpreted an extension of language by the Psi-theory.)\n\nThe understanding of discourse may be modeled along the principles of hypothesis based perception and assimilation/accommodation of schematic representations. Consciousness is related to the abstraction of a concept of self over experiences and protocols of the system and the integration of that concept with sensory experience; there is no explanatory gap between conscious experience and a computational model of cognition.\n\nEvaluating the Psi-theory in an experimental paradigm is difficult, not least because of the many free variables it posits. The predictions and propositions of the Psi-theory are mostly qualitative. Where quantitative statements are made, for instance about the rate of decay of the associations in episodic memory, the width and depth of activation spreading during memory retrieval, these statements are rarely supported by experimental evidence; they represent ad hoc solutions to engineering requirements posed by the design of a problem solving and learning agent.\n\nA partial exception to this rule is the emotional model, which has been tested as a set of computational simulation experiments. While it contains many free variables that determine the settings of modulator parameters and the response to motive pressures, it can be fitted to human subjects in behavioral experiments and thereby demonstrate similar performance in an experimental setting as different personality types. The parameter set can also be fitted to an environment by an evolutionary simulation; the free parameters of the emotional and motivational model allow a reproduction of personal variances.\n\nThe Psi-theory can also be interpreted as a specification for a cognitive architecture.\n\n\"MicroPsi\" is a cognitive architecture built by Joscha Bach at the Humboldt University of Berlin and the Institute of Cognitive Science of the University of Osnabrück. MicroPsi extends the representations of the Psi-theory with taxonomies, inheritance and linguistic labeling; MicroPsi's spreading activation networks allow for neural learning, planning and associative retrieval.\n\nMicroPsi's first generation (2003–2009) is implemented in Java, and includes a framework for editing and simulating software agents using spreading activation networks, and a graphics engine for visualization. MicroPsi has also been used as a robot control architecture.\n\n\"MicroPsi 2\" is a new implementation of MicroPsi, written in Python, and currently used as a tool for knowledge management.\n\nThe OpenCog cognitive architecture includes a simple implementation of Psi-theory, dubbed OpenPsi. It includes interfaces to Hanson Robotics robots for emotion modelling.\n\n\n", "id": "15311631", "title": "Psi-theory"}
{"url": "https://en.wikipedia.org/wiki?curid=31462536", "text": "FORR\n\nFORR (FOr the Right Reasons) is a cognitive architecture for learning and problem solving inspired by Herbert A. Simon's ideas of bounded rationality and satisficing. It was first developed in the early 1990s at the City University of New York. It has been used in game playing, robot pathfinding, recreational park design, spoken dialog systems, and solving NP-hard constraint satisfaction problems, and is general enough for many problem solving applications.\n\nFORR does not have perfect knowledge of how to solve a problem, but instead learns from experience. Intelligent agents are not optimal, but make decisions based on only a subset of all possible good reasons and informative data. These agents can still be considered rational. This idea of bounded rationality was introduced by Herbert A. Simon, who along with Allen Newell developed the early foundations of the study of cognitive architectures and also inspired early architectures such as Soar and ACT-R.\n\nFORR depends upon the idea that there are multiple reasons or rationales for performing actions while solving a problem. These reasons can be always right (it's always right to make a move in chess that will put the opponent in checkmate) or just sometimes right. The always-right reasons are the minority. The sometimes-right reasons can complete with each other: for example, in game playing, one good reason might be to capture pieces, while another might be to control some area of the board. In FORR, these competing reasons are called Advisors.\n\nThe tiered Advisor system is general enough that any potential good reason, such as probabilistic, deductive, or perceptual can be implemented, so long as it gives advice on its preference of one action over another.\n\nBecause of its reliance on a set of independent agents (the Advisors), FORR can be considered a connectionist architecture.\n\nA FORR architecture has three components: a set of \"descriptives\" that describe the state of the problem, a tiered set of \"Advisors\" that are consulted in order to decide what action to perform, and a \"behavioral script\" that queries the Advisors and performs the action that they suggest.\n\nThe Advisors are the set of rationales or heuristics for making a decision. They can be considered the procedural memory component of the architecture. Upon each new decision, Advisors are queried in order to decide which action to perform. Advisors never communicate with each other or learn on their own: they simply ask for information about the state of the problem stored in the form of descriptives, and make a suggestion based on that information. The Advisors are divided into three tiers, which are queried in the following order:\n\n\nThe declarative memory component of the architecture, the descriptives represent the state of the problem and are available to any Advisor.\n\nThe behavioral script queries each tier of Advisors sequentially. If a tier 1 Advisor suggests an action, the script performs the action. Otherwise, if a tier 2 Advisor is triggered, it means that a sub-problem has been encountered. A tier 1 Advisor guarantees that only one tier 2 Advisor is active at any time. If no tier 1 Advisor comments and no tier 2 Advisor is triggered, the behavioral script asks for suggestions or comments from all tier 3 Advisors and lets them vote. The script performs the action with the highest vote among all tier 3 advisors.\n\nA problem domain is a set of similar problems, called the problem classes. If the problem domain is playing simple board games, then tic-tac-toe is a problem class, and one particular game of tic-tac-toe is a problem instance. If navigating a maze is the problem domain, then a particular maze is the class and one attempt at its navigation is an instance. Once the problem domain is identified, the implementation of a FORR architecture for that domain has two basic stages: finding possible right reasons (the Advisors) and learning their weights for a particular class.\n\n\nThe Advisors are the same for all problem classes in a domain, but the weights can be different for each class within the domain. Important heuristics for tic-tac-toe might not be important for a different board game. FORR learns the weights for its tier 3 Advisors by experience. Advisors that suggest an action resulting in failure have their weights penalized, and Advisors whose suggestions result in success have their weights increased. Learning algorithms vary between implementations.\n\nFORR has been used for game playing, robot pathfinding, constraint satisfaction problems, park design, and spoken dialog systems.\n\n\n\n", "id": "31462536", "title": "FORR"}
{"url": "https://en.wikipedia.org/wiki?curid=6456832", "text": "Biologically inspired cognitive architectures\n\nBiologically Inspired Cognitive Architectures (BICA) was a DARPA project administered by the Information Processing Technology Office (IPTO) which began in 2005 and is designed to create the next generation of cognitive architecture models of human artificial intelligence. Its first phase (Design) ran from September 2005 to around October 2006, and was intended to generate new ideas for biological architectures that could be used to create embodied computational architectures of human intelligence. \nThe second phase (Implementation) of BICA was set to begin in the spring of 2007, and would have involved the actual construction of new intelligent agents that live and behave in a virtual environment. However, this phase was canceled by DARPA, reportedly because it was seen as being too ambitious.\n\nNow BICA is a transdisciplinary study that aims to design, characterise and implement human-level cognitive architectures. There is also BICA Society, a nonprofit organization formed to promote and facilitate this study. On their website, they have an extensive comparison table of various cognitive architectures.\n\n", "id": "6456832", "title": "Biologically inspired cognitive architectures"}
{"url": "https://en.wikipedia.org/wiki?curid=446606", "text": "Modularity of mind\n\nModularity of mind is the notion that a mind may, at least in part, be composed of innate neural structures or modules which have distinct established evolutionarily developed functions. Somewhat different definitions of \"module\" have been proposed by different authors.\n\nHistorically, questions regarding the \"functional architecture\" of the mind have been divided into two different theories of the nature of the faculties. The first can be characterized as a horizontal view because it refers to mental processes as if they are interactions between faculties such as memory, imagination, judgement, and perception, which are not domain specific (e. g., a judgement remains a judgement whether it refers to a perceptual experience or to the conceptualization/comprehension process). The second can be characterized as a vertical view because it claims that the mental faculties are differentiated on the basis of domain specificity, are genetically determined, are associated with distinct neurological structures, and are computationally autonomous.\n\nThe vertical vision goes back to the 19th century movement called phrenology and its founder Franz Joseph Gall, who claimed that the individual mental faculties could be associated precisely, in a sort of one-to-one correspondence, with specific physical areas of the brain. Hence, someone's level of intelligence, for example, could be literally \"read off\" from the size of a particular bump on his posterior parietal lobe. This simplistic view of modularity has been disproven over the course of the last century.\n\nIn the 1980s, however, Jerry Fodor revived the idea of the modularity of mind, although without the notion of precise physical localizability. Drawing from Noam Chomsky's idea of the language acquisition device and other work in linguistics as well as from the philosophy of mind and the implications of optical illusions, he became a major proponent of the idea with the 1983 publication of \"Modularity of Mind\".\n\nAccording to Fodor, a module falls somewhere between the behaviorist and cognitivist views of lower-level processes.\n\nBehaviorists tried to replace the mind with reflexes which Fodor describes as encapsulated (cognitively impenetrable or unaffected by other cognitive domains) and non-inferential (straight pathways with no information added). Low level processes are unlike reflexes in that they are inferential. This can be demonstrated by poverty of the stimulus arguments in which the proximate stimulus, that which is initially received by the brain (such as the 2D image received by the retina), cannot account for the resulting output (for example, our 3D perception of the world), thus necessitating some form of computation.\n\nIn contrast, cognitivists saw lower level processes as continuous with higher level processes, being inferential and cognitively penetrable (influenced by other cognitive domains, such as beliefs). The latter has been shown to be untrue in some cases, such as with many visual illusions (ex. Müller-Lyer illusion), which can persist despite a person's awareness of their existence. This is taken to indicate that other domains, including one's beliefs, cannot influence such processes.\n\nFodor arrives at the conclusion that such processes are inferential like higher order processes and encapsulated in the same sense as reflexes.\n\nAlthough he argued for the modularity of \"lower level\" cognitive processes in \"Modularity of Mind\" he also argued that higher level cognitive processes are not modular since they have dissimilar properties. \"The Mind Doesn't Work That Way\", a reaction to Steven Pinker's \"How the Mind Works\", is devoted to this subject.\n\nFodor (1983) states that modular systems must—at least to \"some interesting extent\"—fulfill certain properties:\n\nPylyshyn (1999) has argued that while these properties tend to occur with modules, one—information encapsulation—stands out as being the real signature of a module; that is the encapsulation of the processes inside the module from both cognitive influence and from cognitive access. One example is that conscious awareness of the Müller-Lyer illusion being an illusion does not correct visual processing.\n\nOther perspectives on modularity come from evolutionary psychology, particularly from the work of Leda Cosmides and John Tooby. This perspective suggests that modules are units of mental processing that evolved in response to selection pressures. On this view, much modern human psychological activity is rooted in adaptations that occurred earlier in human evolution, when natural selection was forming the modern human species.\n\nEvolutionary psychologists propose that the mind is made up of genetically influenced and domain-specific mental algorithms or computational modules, designed to solve specific evolutionary problems of the past. Cosmides and Tooby also state in a brief \"primer\" on their website, that \"…the brain is a physical system. It functions like a computer,\" \"…the brain’s function is to process information,\" \"different neural circuits are specialized for solving different adaptive problems,\" and \"our modern skulls house a stone age mind.\"\n\nThe definition of \"module\" has caused confusion and dispute. J. A. Fodor initially defined module as \"functionally specialized cognitive systems\" that have nine features but not necessarily all at the same time. In his views modules can be found in peripheral processing such as low-level visual processing but not in central processing. Later he narrowed the two essential features to \"domain-specificity\" and \"information encapsulation\". Frankenhuis and Ploeger write that domain-specificity means that \"a given cognitive mechanism accepts, or is specialized to operate on, only a specific class of information\". Information encapsulation means that information processing in the module cannot be affected by information in the rest of the brain. One example is that being aware that a certain optical illusion, caused by low level processing, is false does not prevent the illusion from persisting.\n\nEvolutionary psychologists instead usually define modules as functionally specialized cognitive systems that are domain-specific and may also contain innate knowledge about the class of information processed. Modules can be found also for central processing. This theory is sometimes referred to as \"massive modularity.\"\n\nA 2010 review by evolutionary psychologists Confer et al. suggested that domain general theories, such as for \"rationality,\" has several problems: 1. Evolutionary theories using the idea of numerous domain-specific adaptions have produced testable predictions that have been empirically confirmed; the theory of domain-general rational thought has produced no such predictions or confirmations. 2. The rapidity of responses such as jealousy due to infidelity indicates a domain-specific dedicated module rather than a general, deliberate, rational calculation of consequences. 3. Reactions may occur instinctively (consistent with innate knowledge) even if a person have not learned such knowledge. One example being that in the ancestral environment it is unlikely that males during development learn that infidelity (usually secret) may cause paternal uncertainty (from observing the phenotypes of children born many months later and making a statistical conclusion from the phenotype dissimilarity to the cuckolded fathers). With respect to general purpose problem solvers, Barkow, Cosmides, and Tooby (1992) have suggested in \"The Adapted Mind: Evolutionary Psychology and The Generation of Culture\" that a purely general problem solving mechanism is impossible to build due to the frame problem. Clune et al. (2013) have argued that computer simulations of the evolution of neural nets suggest that modularity evolves because, compared to non-modular networks, connection costs are lower.\n\nSeveral groups of critics, including psychologists working within evolutionary frameworks, argue that the massively modular theory of mind does little to explain adaptive psychological traits. Proponents of other models of the mind argue that the computational theory of mind is no better at explaining human behavior than a theory with mind entirely a product of the environment. Even within evolutionary psychology there is discussion about the degree of modularity, either as a few generalist modules or as many highly specific modules. Other critics suggest that there is little empirical support in favor of the domain-specific theory beyond performance on the Wason selection task, a task critics state is too limited in scope to test all relevant aspects of reasoning. Moreover, critics argue that Cosmides and Tooby's conclusions contain several inferential errors and that the authors use untested evolutionary assumptions to eliminate rival reasoning theories.\n\nWallace (2010) observes that the evolutionary psychologists' definition of \"mind\" have been heavily influenced by cognitivism and/or information processing definitions of the mind. Critics point out that these assumptions underlying evolutionary psychologists' hypotheses are controversial and have been contested by some psychologists, philosophers, and neuroscientists. For example, Jaak Panksepp, an affective neuroscientist, point to the \"remarkable degree of neocortical plasticity within the human brain, especially during development\" and states that \"the developmental interactions among ancient special-purpose circuits and more recent general-purpose brain mechanisms can generate many of the \"modularized\" human abilities that evolutionary psychology has entertained.\"\n\nPhilosopher David Buller agrees with the general argument that the human mind has evolved over time but disagrees with the specific claims evolutionary psychologists make. He has argued that the contention that the mind consists of thousands of modules, including sexually dimorphic jealousy and parental investment modules, are unsupported by the available empirical evidence. He has suggested that the \"modules\" result from the brain's developmental plasticity and that they are adaptive responses to local conditions, not past evolutionary environments. However, Buller has also stated that even if massive modularity is false this does not necessarily have broad implications for evolutionary psychology. Evolution may create innate motives even without innate knowledge.\n\nIn contrast to modular mental structure, some theories posit domain-general processing, in which mental activity is distributed across the brain and cannot be decomposed, even abstractly, into independent units. A staunch defender of this view is William Uttal, who argues in \"The New Phrenology\" (2003) that there are serious philosophical, theoretical, and methodological problems with the entire enterprise of trying to localise cognitive processes in the brain. Part of this argument is that a successful taxonomy of mental processes has yet to be developed.\n\nMerlin Donald argues that over evolutionary time the mind has gained adaptive advantage from being a general problem solver. The mind, as described by Donald, includes module-like \"central\" mechanisms, in addition to more recently evolved \"domain-general\" mechanisms.\n\n\n\n", "id": "446606", "title": "Modularity of mind"}
{"url": "https://en.wikipedia.org/wiki?curid=1847118", "text": "Language module\n\nThe language module is a hypothesized structure in the human brain or cognitive system that some psycholinguists such as Steven Pinker claim contains innate capacities for language. There is ongoing debate about this in the fields of cognitive science and neuroscience.\n\nThe debate on the issue of modularity in language is underpinned, in part, by different understandings of this concept. There is, however, some consensus in the literature that a module is considered committed to processing specialized representations (domain-specificity) (Bryson and Stein, 2001) in an informationally encapsulated way. A distinction should be drawn between anatomical modularity, which proposes there is one 'area' in the brain that deals with this processing, and functional modularity that obviates anatomical modularity whilst maintaining information encapsulation in distributed parts of the brain.\n\nThe available evidence points towards no one anatomical area solely devoted to processing language. The Wada test, where sodium amobarbital is used to anaesthetise one hemisphere, shows that the left-hemisphere appears to be crucial in language processing. Yet, neuroimaging does not implicate any single area but rather identifies many different areas as being involved in different aspects of language processing. and not just in the left hemisphere. Further, individual areas appear to subserve a number of different functions. Thus, the extent to which language processing occurs within an anatomical module is considered to be minimal. Nevertheless, as many have suggested, modular processing can still exist even when implemented across the brain; that is, language processing could occur within a functional module.\n\nA common way to demonstrate modularity is to find a double dissociation. That is two groups: First, people for whom language is severely damaged and yet have normal cognitive abilities and, second, persons for whom normal cognitive abilities are grossly impaired and yet language remains intact. Whilst extensive lesions in the left hemisphere perisylvian area can render persons unable to produce or perceive language (global aphasia), there is no known acquired case where language is completely intact in the face of severe non-linguistic deterioration. Thus, functional module status cannot be granted to language processing based on this evidence.\n\nHowever, other evidence from developmental studies has been presented (most famously by Pinker) as supporting a language module, namely the purported dissociation between Specific Language Impairment (SLI), where language is disrupted whilst other mental abilities are not, and Williams Syndrome (WS) where language is said to be spared despite severe mental deficits. More recent and empirically robust work has shown that these claims may be inaccurate, thus, considerably weakening support for dissociation. For example, work reviewed by Brock and Mervis and Beccera demonstrated that language abilities in WS are no more than would be predicted by non-linguistic abilities. Further, there is considerable debate concerning whether SLI is actually a language disorder or whether its aetiology is due to a more general cognitive (e.g. phonological) problem. Thus, the evidence needed to complete the picture for modularity intact language coupled with gross intellectual deterioration is not forthcoming. Consequently, developmental data offers little support for the notion that language processing occurs within a module.\n\nThus, the evidence from double dissociations does not support modularity, although it should be noted that lack of dissociation is not evidence against a module; this inference cannot be logically made.\n\nIndeed, if language were a module it would be informationally encapsulated. Yet, there is evidence to suggest that this is not the case. For instance, in the McGurk effect, watching lips say one phoneme whilst another is played creates the percept of a blended phoneme. Further, Tanenhaus, Spivey-Knowlton, Eberhard and Sedivy (1995) demonstrated visual information mediating syntactic processing. In addition, the putative language module should process only that information relevant to language (i.e., be domain-specific). Yet evidence suggests that areas purported to subserve language also mediate motor control and non-linguistic sound comprehension. Although it is possible that separate processes could be occurring but below the resolution of current imaging techniques, when all this evidence is taken together the case for information encapsulation is weakened.\n\nThe alternative, as it is framed, is that language occurs within a more general cognitive system. The counterargument is that there appears to be something ‘special’ about human language. This is usually supported by evidence such as all attempts to teach animals human languages to any great success have failed (Hauser et al. 2003) and that language can be selectively damaged (a single dissociation) suggesting proprietary computation may be required. Instead of postulating 'pure' modularity, theorists have opted for a weaker version, domain-specificity implemented in functionally specialised neural circuits and computation (e.g. Jackendoff and Pinker’s words, we must investigate language “not as a monolith but as a combination of components, some special to language, others rooted in more general capacities”).\n\nThe debate is ongoing.\n\n\n", "id": "1847118", "title": "Language module"}
{"url": "https://en.wikipedia.org/wiki?curid=13030638", "text": "Cognitive module\n\nA cognitive module is, in theories of the modularity of mind and the closely related society of mind theory, a specialised tool or sub-unit that can be used by other parts to resolve cognitive tasks. The question of their existence and nature is a major topic in cognitive science and evolutionary psychology. Some see cognitive modules as an independent part of the mind. Others also see new thought patterns achieved by experience as cognitive modules.\n\nOther theories similar to the \"cognitive module\" are \"cognitive description\", \"cognitive pattern\" and \"psychological mechanism\". Such a mechanism, if created by evolution, is known as \"evolved psychological mechanism\".\n\nSome examples of cognitive modules:\n\nMany common psychological and personality disorders are caused by cognitive modules running amok.\n\nJealousy: A common cause of unnecessary conflict in relations is that a man is jealous of a woman's previous sexual partners before she met him. All people are born with a basic jealousy cognitive module, developed through as evolutionary strategy in order to safeguard a mate and trigger aggression towards competitors to ensure paternity and prevent bastards. If this module is activated to too strong a degree, it becomes a personality disorder.\n\nStalking: An extreme psychological disorder related to jealousy is stalking. A stalker is a person (usually a man) who behaves as if he had a relation to another person (usually a woman) who is not interested in him. There are also women who stalk men, men who stalk men and women who stalk women, but most common is a man stalking a woman. In modern western culture this behaviour is strongly frowned upon.\n\nParanoia: Being suspicious of fellow human beings is a trait to safeguard against perceived, secret plots against us, a basic human cognitive module useful for survival. But in some people, this turns into unreasonable suspiciousness where there is in reality no plotting against one. Such behaviour is by psychiatrists labeled as \"paranoid schizophrenia\" or in milder forms as \"paranoid personality disorder\". These disorders thus occur when the suspiciousness cognitive module is triggered too often and too strongly for triggers that would not trigger this module in normal people.\n\nObsessive-compulsive disorder: In this quite common disorder, a person will repeatedly check, for example, that a door is locked. One may repeatedly wash hands or other body parts, sometimes for hours, to ensure cleanliness. Again, this disorder is a malfunction of a normal adaptation in all humans to check that a door is locked, to wash to keep us clean, etc.\n\nTransference: A cognitive module developed to solve a particular problem can sometimes crop up in other situations where it is not appropriate. One may be angry at one's boss, but take the anger out on one's fellow man. Often, the transference is unconscious (see also Subconscious mind and Unconscious mind). In psychotherapy, the patient is made aware of this, which makes it easier to modify the unsuitable behaviour.\n\nSigmund Freud's theory of sublimation: said that cognitive modules for some activities, such as sex, may incorrectly show up in disguise in cases where they are not suitable. Freud also introduced the idea of the unconscious, which interpreted as cognitive modules where a person is not aware of the initial cause of these modules and may use them inappropriately.\n\nSchizophrenia: is a psychotic disorder where cognitive modules are triggered too often, overwhelming the brain with information. The inability to repress overwhelming information is a cause of schizophrenia.\n\nCognitive therapy is a psychotherapeutic method that helps people better understand the cognitive modules that cause them to do certain things, and to teach them alternative, more appropriate cognitive modules to use instead in the future.\n\nAccording to psychoanalytic theory, many cognitive modules are unconscious and repressed, to avoid mental conflicts. \"Defenses\" are meant to be cognitive modules used to suppress the awareness of other cognitive modules. Unconscious cognitive modules may influence our behaviour without our being aware of it.\n\nIn the research field of evolutionary psychology it is believed that some cognitive modules are inherited and some are created by learning, but the creation of new modules by learning is often guided by inherited modules.\n\nFor example, the ability to drive a car or throw a basketball are certainly learned and not inherited modules, but they may make use of inherited modules to rapidly compute trajectories.\n\nThere is some disagreement between different social scientists on the importance to the capabilities of the human mind of inherited modules. Evolutionary psychologists claim that other social scientists do not accept that some modules are partially inherited, other social scientists claim that evolutionary psychologists are exaggerating the importance of inherited cognitive modules.\n\nA very important aspect of how humans think is the ability, when encountering a situation or problem, to find more or less similar, but not identical, experiences or cognitive modules. This can be compared to what happens if you sound a tone near a piano. The piano string corresponding to this particular tone will then vibrate. But also other strings, from nearby strings, will vibrate to a lesser extent.\n\nExactly how the human mind does this is not known, but it is believed that when you encounter a situation or problem, many different cognitive modules are activated at the same time, and the mind selects those most useful for understanding a new situation or solving a new problem.\n\nMost law-abiding people have cognitive modules that stop them from committing crimes. Criminals have different modules, causing criminal behaviour. Thus, cognitive modules can be a cause of both ethical and unethical behaviour.\n\n\nThis article is based on an article in Web4Health.\n", "id": "13030638", "title": "Cognitive module"}
{"url": "https://en.wikipedia.org/wiki?curid=14241792", "text": "TRACE (psycholinguistics)\n\nTRACE is a connectionist model of speech perception, proposed by James McClelland and Jeffrey Elman in 1986.. It is based on a structure called \"the Trace,\" a dynamic processing structure made up of a network of units, which performs as the system's working memory as well as the perceptual processing mechanism.. TRACE was made into a working computer program for running perceptual simulations. These simulations are predictions about how a human mind/brain processes speech sounds and words as they are heard in real time.\n\nTRACE was created during the formative period of connectionism, and was included as a chapter in \"Parallel Distributed Processing: Explorations in the Microstructures of Cognition\". The researchers found that certain problems regarding speech perception could be conceptualized in terms of a connectionist interactive activation model. The problems were that (1) speech is extended in time, (2) the sounds of speech (phonemes) overlap with each other, (3) the articulation of a speech sound is affected by the sounds that come before and after it, and (4) there is natural variability in speech (e.g. foreign accent) as well as noise in the environment (e.g. busy restaurant). Each of these causes the speech signal to be complex and often ambiguous, making it difficult for the human mind/brain to decide what words it is really hearing. In very simple terms, an interactive activation model solves this problem by placing different kinds of processing units (phonemes, words) in isolated layers, allowing activated units to pass information between layers, and having units within layers compete with one another, until the “winner” is considered “recognized” by the model.\n\n\"TRACE was the first model that instantiated the activation of multiple word candidates that match any part of the speech input.\"A simulation of speech perception involves presenting the TRACE computer program with mock speech input, running the program, and generating a result. A successful simulation indicates that the result is found to be meaningfully similar to how people process speech.\n\nIt is generally accepted in psycholinguistics that (1) when the beginning of a word is heard, a set of words that share the same initial sound become activated in memory, (2) the words that are activated compete with each other while more and more of the word is heard, (3) at some point, due to both the auditory input and the lexical competition, one word is recognized.\n\nFor example, a listener hears the beginning of \"bald\", and the words bald, ball, bad, bill become active in memory. Then, soon after, only bald and ball remain in competition (bad, bill have been eliminated because the vowel sound doesn't match the input). Soon after, bald is recognized. TRACE simulates this process by representing the temporal dimension of speech, allowing words in the lexicon to vary in activation strength, and by having words compete during processing. Figure 1 shows a line graph of word activation in a simple TRACE simulation.\nIf an ambiguous speech sound is spoken that is exactly in between and , the hearer may have difficulty deciding what it is. But, if that same ambiguous sound is heard at the end of a word like woo/?/ (where ? is the ambiguous sound), then the hearer will more likely perceive the sound as a . This probably occurs because wood is a word but woot is not. An ambiguous phoneme presented in a lexical context will be perceived as consistent with the surrounding lexical context. This perceptual effect is known as the Ganong effect. TRACE reliably simulates this, and can explain it in relatively simple terms. Essentially, the lexical unit which has become activated by the input (i.e. wood) feeds back activation to the phoneme layer, boosting the activation of its constituent phonemes (i.e. ), thus resolving the ambiguity.\n\nSpeakers usually don't leave pauses in between words when speaking, yet listeners seem to have no difficulty hearing speech as a sequence of words. This is known as the segmentation problem, and is one of the oldest problems in the psychology of language. TRACE proposed the following solution, backed up by simulations. When words become activated and recognized, this reveals the location of word boundaries. Stronger word activation leads to greater confidence about word boundaries, which informs the hearer of where to expect the next word to begin.\n\nThe TRACE model is a connectionist network with an input layer and three processing layers: pseudo-spectra (feature), phoneme and word. Figure 2 shows a schematic diagram of TRACE. There are three types of connectivity: (1) feedforward excitatory connections from input to features, features to phonemes, and phonemes to words; (2) lateral (i.e., within layer) inhibitory connections at the feature, phoneme and word layers; and (3) top-down feedback excitatory connections from words to phonemes. \nThe input to TRACE works as follows. The user provides a phoneme sequence that is converted into a multi-dimensional feature vector. This is an approximation of acoustic spectra extended in time. \nThe input vector is revealed a little at a time to simulate the temporal nature of speech. As each new chunk of input is presented, this sends activity along the network connections, changing the activation values in the processing layers. Features activate phoneme units, and phonemes activate word units. Parameters govern the strength of the excitatory and inhibitory connections, as well as many other processing details. \nThere is no specific mechanism that determines when a word or a phoneme has been recognized. If simulations are being compared to reaction time data from a perceptual experiment (e.g. lexical decision), then typically an activation threshold is used. This allows for the model behavior to be interpreted as recognition, and a recognition time to be recorded as the number of processing cycles that have elapsed. For deeper understanding of TRACE processing dynamics, readers are referred to the original publication and to a TRACE software tool that runs simulations with a graphical user interface.\n\nTRACE’s relevance to the modularity debate has recently been brought to the fore by Norris, Cutler and McQueen’s (2001) report on the Merge (?) model of speech perception. While it shares a number of features with TRACE, a key difference is the following. While TRACE permits word units to feed back activation to the phoneme level, Merge restricts its processing to feed-forward connections. In the terms of this debate, TRACE is considered to violate the principle of information encapsulation, central to modularity, when it permits a later stage of processing (words) to send information to an earlier stage (phonemes). Merge advocates for modularity by arguing that the same class of perceptual phenomena that is accounted for in TRACE can be explained in a connectionist architecture that \"does not\" include feedback connections. Norris et al. point out that when two theories can explain the same phenomenon, parsimony dictates that the simpler theory is preferable.\n\nModels of language processing can be used to conceptualize the nature of impairment in persons with speech and language disorder. For example, it has been suggested that language deficits in expressive aphasia may be caused by excessive competition between lexical units, thus preventing any word from becoming sufficiently activated. Arguments for this hypothesis consider that mental dysfunction can be explained by slight perturbation of the network model's processing. This emerging line of research incorporates a wide range of theories and models, and TRACE represents just one piece of a growing puzzle.\n\nPsycholinguistic models of speech perception, e.g. TRACE, must be distinguished from computer speech recognition tools. The former are psychological theories about how the human mind/brain processes information. The latter are engineered solutions for converting an acoustic signal into text. Historically, the two fields have had little contact, but this is beginning to change.\n\nTRACE’s influence in the psychology literature can be assessed by the number of articles that cite it. There are 345 citations of McClelland and Elman (1986) in the PsycINFO database. Figure 3 shows the distribution of those citations over the years since publication. The figure suggests that interest in TRACE grew significantly in 2001, and has remained strong, with about 30 citations per year.\n\n", "id": "14241792", "title": "TRACE (psycholinguistics)"}
{"url": "https://en.wikipedia.org/wiki?curid=32043571", "text": "SCOP formalism\n\nThe SCOP formalism or State Context Property formalism is an abstract mathematical formalism for describing states of a system that generalizes both quantum and classical descriptions. The formalism describes entities, which may exist in different states, which in turn have various properties. In addition there is a set of \"contexts\" (corresponding to measurements) by which an entity may be observed. The formalism has primarily found use outside of physics as a theory of concepts, in particular in the field of quantum cognition, which develops quantum-like models of cognitive phenomena (such as the conjunction fallacy) that may seem paradoxical or irrational when viewed from a perspective of classical states and logic.\n\nOur minds are able to construct a multitude of imaginary, hypothetical, or counterfactual deviations from the more prototypical states of particular concept, and the State COntext Property (SCOP) can model this. The SCOP formalism was inspired by the need to incorporate the effect of context into the formal description of a concept. It builds on an operational approach in the foundations of quantum mechanics in which a physical system is determined by the mathematical structure of its set of states, set of properties, the possible (measurement) contexts which can be applied to this entity, and the relations between these sets. The SCOP formalism is part of a longstanding effort to develop an operational approach to quantum mechanics known as the Geneva-Brussels approach. With SCOP it is possible to describe situations with any degree of contextuality. In fact, classical and quantum come out as special cases: quantum at the one end of extreme contextuality and classical at the other end of extreme lack of contextuality. The SCOP formalism permits one to describe not only physical or conceptual entities, but also potential entities of a more abstract nature, which means that SCOP aims at a very general description of how the interaction between context and the state of an entity plays a fundamental role in its evolution.\n\nThe description of SCOP entities seeks for a general description of an \"observable\" entity that evolves with time. Thus, the description of the entity needs to consider the different \"states\" that the entity can assume. In order to establish the differences among the states, we need to consider the \"properties\" that the states can hold. Note that a complete description of the states in terms of their properties requires that each state must hold a different properties, but in principle this is not the case. In order to \"observe\" the entity, we need a mechanism that permits us to measure properties on the states, i.e. there must exist a set of useful measurements or \"contexts\" that permits us to \"observe\" what properties are held by the current state the entity. However, the context can affect the state of the entity, and change its state (this is well known as the observer effect).\n\nFormally, a SCOP entity consists of a 5-tuple formula_1, where formula_2 represents the set of \"states\" that the entity can assume, formula_3 represents a set of \"contexts\" (measurements), formula_4 represents a set of properties that the entity can hold, formula_5 is a \"state-transition probability function\" that represents the likelihood to transition from the state formula_6 to the state formula_7 under the influence of the context formula_8, and formula_9 is an \"property-applicability function\" that estimates how applicable is the property formula_10 to the state formula_6 of the entity.\n\nIt is possible to identify relations among the states and contexts, that recall the basic elements of the quantum formalism:\n\nIt is possible that the entity is in a situation of \"no contextual influence\". We identify such situation by the \"unitary context\", denoted by formula_12. Moreover, the state of the entity in this situation is identified by the \"ground state\" formula_13. We have that formula_14, and thus formula_15 for all formula_16.\n\nThus, the interaction of the entity with any context different from the unitary context will lead to an evolutionary process of the state's entity.\n\nIf for some context formula_8 there is a state formula_6 such that formula_19, we say formula_6 is an \"eigenstate\" for the context formula_8. Any state formula_7 that is not an eigenstate, it is referred to as \"potential state\".\n\nIt is possible to describe the elements of SCOP using order-theoretic structures. In it has been shown how to obtain a pre-ordered set of states and properties. In it is shown that the set of contexts and properties can be equipped with an orthocomplemented lattice structure. By imposing axioms on the order-theoretical structures of the former elements of a SCOP entity it is possible, via representation theoretical techniques, to obtain the Hilbert space description of the entity.\n\nThe SCOP approach to concepts belongs to the emergent field of quantum cognition. In a SCOP model of a concept we are able to incorporate all of the possible contexts that could influence the state of a concept. The more states and contexts included, the richer the model becomes. The level of refinement is determined by the role the model is expected to play.\nIt is outstanding that in SCOP, unlike other mathematical models of concepts, the potential to include this richness is present in the formalism, i.e., it can incorporate even improbable states, and largely but not completely irrelevant contexts.\nThe SCOP formalism has been successfully applied to model conceptual entities. It has been used shown how to solve the inconsistencies of other mathematical model of concepts, and at the same time it permits to join different perspectives coming from psychology and phylosophy.\n\nEarly concept theorists such Eleanor Rosch have noted that the context in which a concept is elicited plays a fundamental role in the meaning that it takes in natural reasoning tasks such as learning or planning. While in some theories, such as Gärdenfors or Nosofsky theories of concepts, the context is modeled as a weighting function across attributes or properties, in SCOP any effect of context occurs by way of its effect on the state.\n\nMany researchers in concepts such as Hampton, Kamp and Partee, Osherson and Smith, among others, have noticed that measures of typicality and membership in concepts are not equivalent. Typicality refers to how common or representative is an instance of a concept. Membership, rather than measuring representativeness, only measures the allegiance or inclusiveness of a conceptual instances in the category determined by the concept. It is well known that both measures are context-dependent, and they have been related to the notion of similarity of concepts, in that \"concept-similarity\" would be a more fundamental notion and will imply determine their values. But no satisfactory mathematical theory of concept-similarity has been developed yet. In SCOP, similarity, membership, typicality, and any other measure, is modeled as a \"measurement-operator\" that acts on the state of the concept, in the same way as context do. This general manner of approach the measurement of quantities that permits to differentiate states are called \"experiment-contexts\".\n\nThe emergence of meaning when concepts are combined is at the core of the drawbacks in concept theories. For example, it has been shown that \"Guppy\" is neither a typical instance of concept PET, nor of concept FISH, but it is a highly typical instance of the combined concept PET-FISH. It has been proven that no logical-based approach can explain the ways these effects in concept combination. The SCOP-based approach to concepts has been shown to model the concept combination in a satisfactory manner, by embedding the states of the combined concept in tensor space formed by the Hilbert spaces representing each concept.\n", "id": "32043571", "title": "SCOP formalism"}
{"url": "https://en.wikipedia.org/wiki?curid=33548913", "text": "Dehaene–Changeux model\n\nThe Dehaene–Changeux model (DCM), also known as the global neuronal workspace or the global cognitive workspace model is a part of Bernard Baars's \"global workspace model\" for consciousness.\n\nIt is a computer model of the neural correlates of consciousness programmed as a neural network. It attempts to reproduce the swarm behaviour of the brain's \"higher cognitive functions\" such as consciousness, decision-making and the central executive functions. It was developed by cognitive neuroscientists Stanislas Dehaene and Jean-Pierre Changeux beginning in 1986. It has been used to provide a predictive framework to the study of inattentional blindness and the solving of the Tower of London test.\n\nThe Dehaene–Changeux model was initially established as a spin glass neural network attempting to represent learning and to then provide a stepping stone towards artificial learning among other objectives. It would later be used to predict observable reaction times within the priming paradigm and in inattentional blindness.\n\nThe Dehaene–Changeux model is a meta neural network (i.e. a network of neural networks) composed of a very large number of integrate-and-fire neurons programmed in either a stochastic or deterministic way. The neurons are organised in complex thalamo-cortical columns with long-range connexions and a critical role played by the interaction between von Economo's areas. Each thalamo-cortical column is composed of pyramidal cells and inhibitory interneurons receiving a long-distance excitatory neuromodulation which could represent noradrenergic input.\n\nAmong others Cohen & Hudson (2002) had already used \"\"Meta neural networks as intelligent agents for diagnosis \"\" Similarly to Cohen & Hudson, Dehaene & Changeux have established their model as an interaction of meta-neural networks (thalamocortical columns) themselves programmed in the manner of a \"\"hierarchy of neural networks that together act as an intelligent agent\"\", in order to use them as a system composed of a large scale of inter-connected intelligent agents for predicting the self-organized behaviour of the neural correlates of consciousness. It may also be noted that Jain et al. (2002) had already clearly identified spiking neurons as intelligent agents since the lower bound for computational power of networks of spiking neurons is the capacity to simulate in real-time for boolean-valued inputs any Turing machine. The DCM being composed of a very large number of interacting sub-networks which are themselves intelligent agents, it is formally a Multi-agent system programmed as a Swarm or neural networks and \"a fortiori\" of spiking neurons.\n\nThe DCM exhibits several surcritical emergent behaviors such as multistability and a Hopf bifurcation between two very different regimes which may represent either sleep or arousal with a various all-or-none behaviors which Dehaene et al. use to determine a testable taxonomy between different states of consciousness. \n\nThe Dehaene-Changeux Model contributed to the study of nonlinearity and self-organized criticality in particular as an explanatory model of the brain's emergent behaviors, including consciousness. Studying the brain's phase-locking and large-scale synchronization, Kitzbichler et al. (2011a) confirmed that criticality is a property of human brain functional network organization at all frequency intervals in the brain's physiological bandwidth.\n\nFurthermore, exploring the neural dynamics of cognitive efforts after, \"inter alia\", the Dehaene-Changeux Model, Kitzbichler et al. (2011b) demonstrated how cognitive effort breaks the modularity of mind to make human brain functional networks transiently adopt a more efficient but less economical configuration. Werner (2007a) used the Dehaene-Changeux Global Neuronal Workspace to defend the use of statistical physics approaches for exploring phase transitions, scaling and universality properties of the so-called \"Dynamic Core\" of the brain, with relevance to the macroscopic electrical activity in EEG and EMG. Furthermore, building from the Dehaene-Changeux Model, Werner (2007b) proposed that the application of the twin concepts of scaling and universality of the theory of non-equilibrium phase transitions can serve as an informative approach for elucidating the nature of underlying neural-mechanisms, with emphasis on the dynamics of recursively reentrant activity flow in intracortical and cortico-subcortical neuronal loops. Friston (2000) also claimed that \"\"the nonlinear nature of asynchronous coupling enables the rich, context-sensitive interactions that characterize real brain dynamics, suggesting that it plays a role in functional integration that may be as important as synchronous interactions\"\".\n\nIt contributed to the study of phase transition in the brain under sedation, and notably GABA-ergic sedation such as that induced by propofol (Murphy et al. 2011, Stamatakis et al. 2010). The Dehaene-Changeux Model was contrasted and cited in the study of collective consciousness and its pathologies (Wallace et al. 2007). Boly et al. (2007) used the model for a reverse somatotopic study, demonstrating a correlation between baseline brain activity and somatosensory perception in humans. Boly et al. (2008) also used the DCM in a study of the baseline state of consciousness of the human brain's default network.\n\n\n", "id": "33548913", "title": "Dehaene–Changeux model"}
{"url": "https://en.wikipedia.org/wiki?curid=6874", "text": "Cyc\n\nCyc () is the world's longest-lived artificial intelligence project, attempting to assemble a comprehensive ontology and knowledge base that spanns the basic concepts and \"rules of thumb\" about how the world works (think common sense knowledge but focusing more on things that rarely get written down or said, in contrast with facts one might find somewhere on the internet or retrieve via Google or Wikipedia), with the goal of enabling AI applications to perform human-like reasoning and be less \"brittle\" when confronted with novel situations that were not preconceived.\n\nDouglas Lenat began the project in July, 1984 at MCC, where he was Principal Scientist 1984-1994, and then, since January 1995, has been under active development by the Cycorp company, where he is the CEO.\nuntil early 2017. Parts of the project were released as OpenCyc, which provided an API, RDF endpoint, and data dump under an open source license.\n\nThe need for a massive symbolic artificial intelligence project of this ilk was born in the early 1980s out of a large number of experiences early AI researchers had, in the previous 25 years, wherein their AI programs would generate encouraging early results but then fail to \"scale up\"—fail to cope with novel situations and problems outside the narrow area they were conceived and engineered to cope with. Douglas Lenat and Alan Kay publicized this need, and organized a meeting at Stanford in 1983 to consider the problem; the back-of-the-envelope calculations by them and colleagues including Marvin Minsky, Allen Newell, Edward Feigenbaum, and John McCarthy indicated that that effort would require between 1000 and 3000 person-years of effort, hence not fit into the standard academic project model. Fortuitously, events within a year of that meeting enabled that Manhattan-Project-sized effort to get underway.\n\nThe project was started in July,1984 as the flagship project of the 400-person Microelectronics and Computer Technology Corporation, a research consortium started by two dozen large United States based corporations \"to counter a then ominous Japanese effort in AI, the so-called \"fifth-generation\" project.\" The US Government reacted to the Fifth Generation threat by passing the National Cooperative Research Act of 1984, which for the first time allowed US companies to \"collude\" on long-term high-risk high-payoff research, and MCC and Sematech sprang up to take advantage of that ten-year opportunity. MCC's first President and CEO was Bobby Ray Inman, former NSA Director and Central Intelligence Agency deputy director.\n\nThe objective of the Cyc project was to codify, in machine-usable form, the millions of pieces of knowledge that compose human common sense. This entailed, along the way, (1) developing an adequately expressive representation language, CycL, (2) developing an ontology spanning all human concepts down to some appropriate level of detail, (3) developing a knowledge base on that ontological framework, comprising all human knowledge about those concepts down to some appropriate level of detail, and (4) developing an inference engine exponentially faster than those used in then-conventional expert systems, to be able to infer the same types and depth of conclusions that humans are capable of, given their knowledge of the world.\n\nIn slightly more detail:\n\nCycL has a publicly released specification and dozens of HL modules were described in, but the actual Cyc inference engine code, and the full list of 1000+ HL modules, is Cycorp-proprietary.\n\nThe name \"Cyc\" (from \"encyclopedia\", pronounced like \"syke\") is a registered trademark owned by Cycorp. Access to Cyc is through paid licenses, but \"bona fide\" AI research groups are given research-only no-cost licenses (cf. ResearchCyc); there are currently over 600 such groups worldwide with such licenses.\n\nTypical pieces of knowledge represented in the Cyc knowledge base are \"Every tree is a plant\" and \"Plants die eventually\". When asked whether trees die, the inference engine can draw the obvious conclusion and answer the question correctly.\n\nMost of Cyc's knowledge, outside math and games, is only true by default. For example, Cyc knows that \"as a default\" parents love their children, when you're made happy you smile, taking your first step is a big accomplishment, when someone you love has a big accomplishment that makes you happy, and only adults have children. When asked whether a picture captioned \"Someone watching his daughter take her first step\" contains a smiling adult person, Cyc can logically infer that the answer is \"Yes\", and \"show its work\" by presenting the step by step logical argument using those five pieces of knowledge from its KB. These are formulated in the language CycL, which is based on predicate calculus and has a syntax similar to that of the Lisp programming language.\n\nIn 2008, Cyc resources were mapped to many Wikipedia articles, potentially easing connecting with other open datasets like DBpedia and Freebase.\n\nMuch of the current work Cyc continues to be knowledge engineering, representing facts about the world by hand, and implementing efficient inference mechanisms on that knowledge. Increasingly, however, work at Cycorp involves giving the Cyc system the ability to communicate with end users in natural language, and to assist with the ongoing knowledge formation process via machine learning and natural language understanding. Another large effort at Cycorp is building a suite of Cyc-powered ontological engineering tools to lower the bar to entry for individuals to contribute to, edit, browse, and query Cyc.\n\nLike many companies, Cycorp has ambitions to use Cyc's natural language processing to parse the entire internet to extract structured data; unlike all others, it is able to call on the Cyc system itself to act as an inductive bias and as an adjudicator of ambiguity, metaphor, and ellipsis.\n\nThe concept names in Cyc are CycL \"constants\". Constants start with an optional \"#$\" and are case-sensitive. There are constants for:\n\nTwo important binary predicates are #$isa and #$genls. The first one describes that one item is an instance of some collection, the second one that one collection is a subcollection of another one. Facts about concepts are asserted using certain CycL \"sentences\". Predicates are written before their arguments, in parentheses:\n\"Bill Clinton belongs to the collection of U.S. presidents\" and\n\"All trees are plants\".\n\"Paris is the capital of France.\"\n\nSentences can also contain variables, strings starting with \"?\". These sentences are called \"rules\". One important rule asserted about the #$isa predicate reads\nwith the interpretation \"if OBJ is an instance of the collection SUBSET and SUBSET is a subcollection of SUPERSET, then OBJ is an instance of the collection SUPERSET\". Another typical example is\nwhich means that for every instance of the collection #$ChordataPhylum (i.e. for every chordate), there exists a female animal (instance of #$FemaleAnimal), which is its mother (described by the predicate #$biologicalMother).\n\nThe knowledge base is divided into \"microtheories\" (Mt), collections of concepts and facts typically pertaining to one particular realm of knowledge. Unlike the knowledge base as a whole, each microtheory must be free from \"monotonic\" contradictions. Each microtheory is a first-class object in the Cyc ontology; it has a name that is a regular constant; microtheory constants contain the string \"Mt\" by convention. An example is #$MathMt, the microtheory containing mathematical knowledge. The microtheories can inherit from each other and are organized in a hierarchy:\none specialization of #$MathMt is #$GeometryGMt, the microtheory about geometry.\n\nAn inference engine is a computer program that tries to derive answers from a knowledge base.\nThe Cyc inference engine performs general logical deduction (including modus ponens, modus tollens, universal quantification and existential quantification). It also performs inductive reasoning, statistical machine learning and symbolic machine learning, and abductive reasoning (but of course sparingly and using the existing KB as a filter and guide).\n\nThe first version of OpenCyc was released in spring 2002 and contained only 6,000 concepts and 60,000 facts. The knowledge base was released under the Apache License. Cycorp stated its intention to release OpenCyc under parallel, unrestricted licences to meet the needs of its users. The CycL and SubL interpreter (the program that allows users to browse and edit the database as well as to draw inferences) was released free of charge, but only as a binary, without source code. It was made available for Linux and Microsoft Windows. The open source Texai project released the RDF-compatible content extracted from OpenCyc. A version of OpenCyc, 4.0, was released in June 2012. OpenCyc 4.0 included much of the Cyc ontology at that time, containing hundreds of thousands of terms, along with millions of assertions relating the terms to each other; however, these are mainly taxonomic assertions, not the complex rules available in Cyc. The OpenCyc 4.0 knowledge base contained 239,000 concepts and 2,093,000 facts.\n\nThe main point of releasing OpenCyc was to help AI researchers understand what was \"missing\" from what they now call ontologies and knowledge graphs. It's useful and important to have properly taxonomized concepts like person, night, sleep, lying down, waking, happy, etc., but what's \"missing\" from the OpenCyc content about those terms, but present in the Cyc KB content, are the various rules of thumb that most of us share about those terms: that (as a default, in the ModernWesternHumanCultureMt) each person sleeps a night, sleeps lying down, can be woken up, is not happy about being woken up, \"and so on.\" That point does not require continually-updated releases of OpenCyc, so, as of 2017, OpenCyc is no longer available.\n\nIn July 2006, Cycorp released the executable of ResearchCyc 1.0, a version of Cyc aimed at the research community, at no charge. (ResearchCyc was in beta stage of development during all of 2004; a beta version was released in February 2005.) In addition to the taxonomic information contained in OpenCyc, ResearchCyc includes significantly more semantic knowledge (i.e., additional facts and rules of thumb) involving the concepts in its knowledge base; it also includes a large lexicon, English parsing and generation tools, and Java based interfaces for knowledge editing and querying. In addition it contains a system for Ontology-based data integration. As of 2017, regular releases of ResearchCyc continue to appear, and are licensed to over 600 research groups around the world at no cost for noncommercial research purposes.\n\nThere have been over 100 successful applications of Cyc; listed here are a few mutually dissimilar instances:\n\nFor over a decade, Glaxo has used Cyc to semi-automatically integrate all the large (hundreds of thousands of terms) thesauri of pharmaceutical-industry terms that reflect differing usage across companies, countries, years, and sub-industries. This ontology integration task requires domain knowledge, shallow semantic knowledge, but also arbitrarily deep common sense knowledge and reasoning. Pharma vocabulary varies across countries, (sub-) industries, companies, departments, and decades of time.  E.g., what’s a\" gel pak\"?  What’s the “street name” for \"ranitidine hydrochloride\"?  Each of these \"n \"controlled vocabularies is an ontology with approximately 300k terms.  Glaxo researchers need to issue a query \"in their current vocabulary\", have it translated into a neutral “true meaning”, and then have that transformed in the opposite direction to find potential matches against documents each of which was written to comply with a particular known vocabulary.  They had been using a large staff to do that manually.  Cyc is used as the universal interlingua capable of representing the union of all the terms’ “true meanings”, and capable of representing the 300k transformations between each of those controlled vocabularies and Cyc, thereby converting an \"n2\" problem into a linear one without introducing the usual sort of “telephone game” attenuation of meaning.  Furthermore, creating each of those 300k mappings for each thesaurus is done in a largely automated fashion, by Cyc.\n\nThe comprehensive Terrorism Knowledge Base is an application of Cyc in development that will try to ultimately contain all relevant knowledge about \"terrorist\" groups, their members, leaders, ideology, founders, sponsors, affiliations, facilities, locations, finances, capabilities, intentions, behaviors, tactics, and full descriptions of specific terrorist events. The knowledge is stored as statements in mathematical logic, suitable for computer understanding and reasoning.\n\nCyclopedia is being developed; it superimposes Cyc keywords on pages taken from Wikipedia pages.\n\nThe Cleveland Clinic has used Cyc to develop a natural language query interface of biomedical information, spanning decades of information on cardiothoracic surgeries. A query is parsed into a set of CycL (higher-order logic) fragments with open variables (e.g., \"this question is talking about a person who developed an endocarditis infection\", \"this question is talking about a subset of Cleveland Clinic patients who underwent surgery there in 2009\", etc.); then various constraints are applied (medical domain knowledge, common sense, discourse pragmatics, syntax) to see how those fragments could possibly fit together into one semantically meaningful formal query; significantly, in most cases, there is exactly \"one and only one\" such way of incorporating and integrating those fragments Integrating the fragments involves (i) deciding which open variables in which fragments actually represent the same variable, and (ii) for all the final variables, decide what order and scope of quantification that variable should have, and what type (universal or existential). That logical (CycL) query is then converted into a SPARQL query that is passed to the CCF SemanticDB that is its data lake.\n\nOne Cyc application aims to help students doing math at a 6th grade level, helping them much more deeply understand that subject matter. It is based on the experience that we often have \"thought\" we understood something, but only \"really\" understood it after we had to explain or teach it to someone else. Unlike almost all other educational software, where the computer plays the role of the teacher, this application of Cyc, called MathCraft, has Cyc play the role of a fellow student who is always slightly more confused than you, the user, are, about the subject. The user's role is to observe the Cyc avatar and give it advice, correct its errors, mentor it, get it to see what it's doing wrong, etc. As the user gives good advice, Cyc allows the avatar to make fewer mistakes of that type, hence, from the user's point of view, it seems as though the user has just successfully taught it something. This is a variation of Learning by Teaching.\n\nThe Cyc project has been described as \"one of the most controversial endeavors of the artificial intelligence history\". Catherine Havasi, CEO of Luminoso, says that Cyc is the predecessor project to IBM's Watson. Machine-learning scientist Pedro Domingos refers to the project as a \"catastrophic failure\" for several reasons, including the unending amount of data required to produce any viable results and the inability for Cyc to evolve on its own.\n\nRobin Hanson, a professor of economics at George Mason University, gives a more fair and balanced analysis: \"\"Of course the CYC project is open to criticism on its many particular choices.  People have complained about its logic-like and language-like representations, about its selection of prototypical cases to build from (e.g., encyclopedia articles), about its focus on answering over acting,  about how often it rebuilds vs. maintaining legacy systems, and about being private vs. publishing everything. But any large project like this would produce such disputes, and it is not obvious any of its choices have been seriously wrong.  They had to start somewhere, and in my opinion they have now collected a knowledge base with a truly spectacular size, scope, and integration. Other architectures may well work better, but if knowing lots is anywhere near as important as Lenat thinks, I’d expect serious AI attempts to import CYC’s knowledge, translating it into a new representation.  No other source has anywhere near CYC’s size, scope, and integration\".\"\n\nA similar sentiment was expressed by Marvin Minsky: \"Unfortunately, the strategies most popular among AI researchers in the 1980s have come to a dead end,\" said Minsky. So-called “expert systems,” which emulated human expertise within tightly defined subject areas like law and medicine, could match users’ queries to relevant diagnoses, papers and abstracts, yet they could not learn concepts that most children know by the time they are 3 years old. “For each different kind of problem,” said Minsky, “the construction of expert systems had to start all over again, because they didn’t accumulate common-sense knowledge.” Only one researcher has committed himself to the colossal task of building a comprehensive common-sense reasoning system, according to Minsky. Douglas Lenat, through his Cyc project, has directed the line-by-line entry of more than 1 million rules into a commonsense knowledge base.\"\n\nGary Marcus, a professor of psychology and neural science at New York University and the cofounder of an AI company called Geometric Intelligence, says \"it represents an approach that is very different from all the deep-learning stuff that has been in the news.” This is consistent with Doug Lenat's position that \"Sometimes the \"veneer\" of intelligence is not enough\".\n\nStephen Wolfram writes \"\"In the early days of the field of artificial intelligence, there were plenty of discussions of “knowledge representation”, with approaches based variously on the grammar of natural language, the structure of predicate logic or the formalism of databases. Very few large-scale projects were attempted (Doug Lenat’s Cyc being a notable counterexample).\"\n\nEvery few years since it began publishing, there is a new Wired Magazine article about Cyc, some positive and some negative (including one issue which contained one of each).\n\nThis is a list of some of the notable people who work or have worked on Cyc either while it was a project at MCC (where Cyc was first started) or Cycorp.\n\n\n", "id": "6874", "title": "Cyc"}
{"url": "https://en.wikipedia.org/wiki?curid=50568976", "text": "Spaun (Semantic Pointer Architecture Unified Network)\n\nSpaun (Semantic Pointer Architecture Unified Network) is a cognitive architecture pioneered by Chris Eliasmith of the University of Waterloo Centre for Theoretical Neuroscience. It consists of 2.5 million simulated neurons organized into subsystems that resemble specific brain regions, such as the prefrontal cortex, basal ganglia, and thalamus. It can recognize numbers, remember them, figure out numeric sequences, and even write them down with a robotic arm. \n", "id": "50568976", "title": "Spaun (Semantic Pointer Architecture Unified Network)"}
{"url": "https://en.wikipedia.org/wiki?curid=3682165", "text": "Subject matter expert Turing test\n\nA subject matter expert Turing test is a variation of the Turing test where a computer system attempts to replicate an expert in a given field such as chemistry or marketing. It is also known as a Feigenbaum test and was proposed by Edward Feigenbaum in a 2003 paper.\n\nThe concept is also described by Ray Kurzweil in his 2005 book \"The Singularity is Near\". Kurzweil argues that machines who pass this test are an inevitable consequence of Moore's Law.\n\n\n", "id": "3682165", "title": "Subject matter expert Turing test"}
{"url": "https://en.wikipedia.org/wiki?curid=3897595", "text": "Minimum intelligent signal test\n\nThe minimum intelligent signal test, or MIST, is a variation of the Turing test proposed by Chris McKinstry in which only (yes/no or true/false) answers may be given to questions. The purpose of such a test is to provide a quantitative statistical measure of \"humanness\", which may subsequently be used to optimize the performance of artificial intelligence systems intended to imitate human responses.\n\nMcKinstry gathered approximately 80,000 propositions that could be answered yes or no, e.g.:\n\n\nHe called these propositions Mindpixels.\n\nThese questions test both specific knowledge of aspects of culture, and basic facts about the meaning of various words and concepts. It could therefore be compared with the SAT, intelligence testing and other controversial measures of mental ability. McKinstry's aim was not to distinguish between shades of intelligence but to identify whether a computer program could be considered intelligent at all.\n\nAccording to McKinstry, a program able to do much better than chance on a large number of MIST questions would be judged to have some level of intelligence and understanding. For example, on a 20-question test, if a program were guessing the answers at random, it could be expected to score 10 correct on average. But the probability of a program scoring 20 out of 20 correct by guesswork is only one in 2, i.e. one in 1,048,576; so if a program were able to sustain this level of performance over several independent trials, with no prior access to the propositions, it should be considered intelligent.\n\nMcKinstry criticized existing approaches to artificial intelligence such as chatterbots, saying that his questions could \"kill\" AI programs by quickly exposing their weaknesses. He contrasted his approach, a series of direct questions assessing an AI's capabilities, to the Turing test and Loebner Prize method of engaging an AI in undirected typed conversation.\n\nCritics of the MIST have noted that it would be easy to \"kill\" a McKinstry-style AI too, due to the impossibility of supplying it with correct answers to all possible yes/no questions by ways of a finite set of human-generated Mindpixels: the fact that an AI can answer the question \"Is the sun bigger than my foot?\" correctly does not mean that it can answer variations like \"Is the sun bigger than (my hand | my liver | an egg yolk | Alpha Centauri A | ...)\" correctly, too.\n\nHowever, the late McKinstry might have replied that a truly intelligent, knowledgeable entity (on a par with humans) would be able to work out answers such as (yes | yes | yes | don't know | ...) by applying its knowledge of the relative sizes of the objects named. In other words, the MIST was intended as a test of AI, not as a suggestion for implementing AI.\n\nIt can also be argued that the MIST is a more objective test of intelligence than the Turing test, a subjective assessment that some might consider to be more a measure of the interrogator's gullibility than of the machine's intelligence. According to this argument, a human's judgment of a Turing test is vulnerable to the ELIZA effect, a tendency to mistake superficial signs of intelligence for the real thing, anthropomorphizing the program. The response, suggested by Alan Turing's essay \"Can Machines Think?\", is that if a program is a convincing imitation of an intelligent being, it is in fact intelligent. The dispute is thus over what it means for a program to have \"real\" intelligence, and by what signs it can be detected.\n\nA similar debate exists in the controversy over Great Ape language, in which nonhuman primates are said to have learned some aspects of sign languages but the significance of this learning is disputed.\n\n", "id": "3897595", "title": "Minimum intelligent signal test"}
{"url": "https://en.wikipedia.org/wiki?curid=19451459", "text": "Computer game bot Turing Test\n\nThe Computer Game Bot Turing Test is a variant of the Turing Test, where a human judge viewing and interacting with a virtual world must distinguish between other humans interacting with the world and game bots that interact with the world. This variant was first proposed in 2008 by Associate Professor Philip Hingston of Edith Cowan University, and implemented through a tournament called the 2K BotPrize.\nThe Computer Game Bot Turing Test was proposed to advance the fields of Artificial Intelligence and Computational Intelligence with respect to video games. It was considered that a poorly implemented bot implied a subpar game, so a bot that would be capable of passing this test, and therefore might be indistinguishable from a human player, would directly improve the quality of a game. It also served to debunk a flawed notion that \"game AI is a solved problem.\"\n\nEmphasis is placed on a game bot that interacts with other players in a multiplayer environment. Unlike a bot that simply needs to make optimal human-like decisions to play or beat a game, this bot must make the same decisions while also convincing another in-game player of its human-likeness.\n\nThe Computer Game Bot Turing Test was designed to test a bot's ability to interact with a game environment in comparison with a human player, simply 'winning' was insufficient. This evolved into a contest with a few important goals in mind:\n\n\nIn 2008, the first 2K BotPrize tournament took place. The contest was held with the game Unreal Tournament 2004 as the platform. Contestants created their bots in advance using the GameBots interface. GameBots had some modifications made so as to adhere to the above conditions, such as removing data about vantage points or weapon damage that unfairly informed the bots of relevant strengths/weakness that a human would otherwise need to learn.\n\nThe first BotPrize Tournament was held in Perth, Australia, on 17 December 2008, as part of the 2008 IEEE Symposium on Computational Intelligence and Games. Each competing team was given time to set up and adjust their bots to the modified game client, although no coding changes were allowed at that point. The tournament was run in rounds, each a 10-minute death match. Judges were the last to join the server and every judge observed every player and every bot exactly once, although the pairing of players and bots did change. When the tournament ended, no bot was rated as more human than any player.\n\nIn subsequent tournaments, run during 2009–2011, bots achieved scores that were increasingly human-like, but no contestant had won the BotPrize in any of these contests.\n\nIn 2012, the annual 2K BotPrize was held once again, and two teams programmed bots that achieved scores greater than those of human players.\n\nTo date, there have been two successfully programmed bots that passed the Computer Game Bot Turing Test:\n\nComments from the winners can be found in detail at the BotPrize website. Interestingly, these victors succeeded in the year 2012, Alan Turing's centenary year.\n\nThe outcome of a bot that appears more human-like than a human player is possibly overstated, since in the tournament in which the bots succeeded, the average 'humanness' rating of the human players was only 41.4%. This showcases some limits of this Turing Test, since the results demonstrate that human behaviour is more complicated and quantitative than was accounted for. In light of this, the BotPrize competition organizers will increase the difficulty in upcoming years with new challenges, forcing competitors to improve their bots.\n\nIt is also believed that methods and techniques developed for the Computer Game Bot Turing Test will be useful in fields other than video games, such as virtual training environments and in improving robot-human interaction.\n\nThe Computer Game Bot Turing test differs from the traditional or generic Turing test in a number of ways:\n\n\n", "id": "19451459", "title": "Computer game bot Turing Test"}
{"url": "https://en.wikipedia.org/wiki?curid=723435", "text": "Reverse Turing test\n\nA reverse Turing test is a Turing test in which the objective or roles between computers and humans have been reversed.\n\nConventionally, the Turing test is conceived as having a human judge and a computer subject which attempts to appear human. Critical to the concept is the parallel situation of a human judge and a human subject, who also attempts to appear human. The intent of the test is\nfor the judge to attempt to distinguish which of these two situations is actually occurring. It is presumed that a human subject will always be judged human, and a computer is then said to \"pass the Turing test\" if it is also judged human. Any of these roles may be changed to form a \"reverse Turing test\".\n\nArguably the standard form of the reverse Turing test is one in which the subjects attempt to appear to be a computer rather than a human.\n\nA formal reverse Turing test follows the same format as a Turing test. Human subjects attempt to imitate the conversational style of a conversation program such as ELIZA. Doing this well involves deliberately ignoring, to some degree, the meaning of the conversation\nthat is immediately apparent to a human, and the simulation of the kinds of errors that conversational programs typically make. Arguably unlike the conventional Turing test, this is most interesting when the judges are very familiar with the art of conversation programs, meaning that in the regular Turing test they can very rapidly tell the difference\nbetween a computer program and a human acting normally.\n\nThe humans that perform best (some would say worst) in the reverse Turing test are those that know computers best, and so know the types of errors that computers can be expected to make in conversation. There is much shared ground between the skill of the reverse Turing\ntest and the skill of mentally simulating a program's operation in the course of computer programming and especially debugging. As a result, programmers (especially hackers) will sometimes indulge in an informal reverse Turing test for recreation.\n\nAn informal reverse Turing test involves an attempt to simulate a computer without the formal structure of the Turing test. The judges of the test are typically not aware in advance that a reverse Turing test is occurring, and the test subject attempts to elicit from the 'judges' (who, correctly, think they are speaking to a human) a response along the lines of \"is this really a human?\". Describing such a situation as a \"reverse Turing test\" typically occurs retroactively.\n\nThere are also cases of accidental reverse Turing tests, occurring when a programmer is in a sufficiently non-human mood that his conversation unintentionally resembles that of a computer. In these cases the description is invariably retroactive and humorously intended.\nThe subject may be described as having passed or failed a reverse Turing test or as having failed a Turing test. The latter description is arguably more accurate in these cases; see also the next section.\n\nSince Turing test judges are sometimes presented with genuinely human subjects, as a control, it inevitably occurs that a small proportion of such control subjects are judged to be computers. This is considered humorous and often embarrassing for the subject.\n\nThis situation may be described literally as the human \"failing the Turing test\", for a computer (the intended subject of the test) achieving the same result would be described in the same terms as having failed. The same situation may also be described as the human \"failing the reverse Turing test\" because to consider the human to be a subject of the test involves reversing the roles of the real and control subjects.\n\nWhen a Turing test is applied to a mixed group of human and computer subjects, the computers are the nominal subjects. However, the humans are judged in exactly the same way, and so their Turing test scores can also be calculated.\n\nThis is another way of viewing the situation described in the previous section.\n\nThe term \"reverse Turing test\" has also been applied to a Turing test (test of humanity) that is administered by a computer. In other words, a computer administers a test to determine if the subject is or is not human. As of 2004, such procedures, called CAPTCHAs, are used in some anti-spam systems to prevent automated bulk use of communications systems.\n\nThe use of captchas is controversial. Circumvention methods exist that reduce their effectiveness. Also, many implementations of captchas (particularly ones desired to counter circumvention) are inaccessible to humans with disabilities, and/or are difficult for humans to pass.\n\nNote that \"CAPTCHA\" is an acronym for \"Completely Automated Public Turing test to tell Computers and Humans Apart\" so that the original designers of the test regard the test as a Turing test to some degree.\n\nAn alternative conception of a Reverse Turing Test is to use the test to determine whether sufficient information is being transmitted between the tester and the subject. For example, if the information sent by the tester is insufficient for the human doctor to perform diagnosis accurately, then a medical diagnostic program could not be blamed for also failing to diagnose accurately. \n\nThis formulation is of particular use in developing Artificial Intelligence programs, because it gives an indication of the input needed for a system that attempts to emulate human activities.\n\n\n", "id": "723435", "title": "Reverse Turing test"}
{"url": "https://en.wikipedia.org/wiki?curid=28037152", "text": "NuCaptcha\n\nNuCaptcha is an early fraud detection service which utilises behavior analytics to provision threat appropriate, animated video CAPTCHA's. NuCaptcha is developed and operated by Canadian-based firm, NuData Security.\n\nStatic image-based CAPTCHAs are routinely used to prevent automated sign-ups to websites by using text or images of words disguised so that optical character recognition (OCR) software has trouble reading them. However, in common CAPTCHA systems, users often fail to correctly solve the CAPTCHA 7% - 25% of the time. NuCaptcha uses animated video technology that it claims make puzzles easier for humans to solve, but harder for bots and hackers to decipher.\nNuCaptcha attempts to solve usability of static image-based CAPTCHAS using two main technologies: 1) video animation to display CAPTCHA puzzles, and 2) a behaviour analysis system to monitor interactions with the platform.\n\nSecurity researcher Elie Bursztein demonstrated a practical attack against NuCaptcha's video CAPTCHA scheme by employing optical flow techniques to isolate individual CAPTCHA characters. The proposed attack is able to break the video CAPTCHAs in more than 90% of cases.\n\nIn response, NuCaptcha noted that Bursztein’s findings underscore the need for CAPTCHA puzzles to be part of a larger security construct, such as behavior monitoring to assess the risk of individual users. NuCaptcha also pointed out that the CAPTCHAS analyzed in Bursztein's blog post were middle-security puzzles focused on usability, and not the stronger puzzles presented to high-risk users. In addition to this, NuCaptcha noted that the optical flow technique relies on static non-animated features of the puzzle. Changes were made to NuCaptcha puzzles to remove the static non-animated features.\n\nNuCaptcha APIs are currently available in PHP, .NET, and Java. Plugins are available for WordPress, Drupal, Codelgniter, vBulletin, and phpBB. In October 2011, NuCaptcha announced its CAPTCHA solutions for mobile devices across all platforms, including Android and iOS.\n\n", "id": "28037152", "title": "NuCaptcha"}
{"url": "https://en.wikipedia.org/wiki?curid=11451897", "text": "ReCAPTCHA\n\nreCAPTCHA is a CAPTCHA-like system designed to establish that a computer user is human (normally in order to protect websites from bots) and, at the same time, assist in the digitization of books. reCAPTCHA was originally developed by Luis von Ahn, Ben Maurer, Colin McMillen, David Abraham and Manuel Blum at Carnegie Mellon University's main Pittsburgh campus. It was acquired by Google in September 2009.\n\nreCAPTCHA has completed digitizing the archives of \"The New York Times\" and books from Google Books, as of 2011. The archive can be searched from the \"New York Times\" Article Archive, where more than 13 million articles in total have been archived, dating from 1851 to the present day. Through mass collaboration, reCAPTCHA was helping to digitize books that are too illegible to be scanned by computers, as well as translate books to different languages, as of 2015.\n\nThe system has been reported as displaying over 100 million CAPTCHAs every day, on sites such as Facebook, TicketMaster, Twitter, 4chan, CNN.com, StumbleUpon, Craigslist (since June 2008), and the U.S. National Telecommunications and Information Administration's digital TV converter box coupon program website (as part of the US DTV transition).\n\nreCAPTCHA's slogan was \"Stop Spam, Read Books.\", until the introduction of a new version of the reCAPTCHA plugin in 2014; the slogan has now disappeared from the website and from the classic version of the reCAPTCHA plugin. A new system featuring image verification was also introduced. In this system, users are asked to just click on a checkbox (the system will verify whether the user is a human or not, for example, with some clues such as already-known cookies or mouse movements within the ReCAPTCHA frame) or, if it fails, select one or more images from a selection of nine images.\n\nDistributed Proofreaders was the first project to volunteer its time to decipher scanned text that could not be read by OCR. It works with Project Gutenberg to digitize public domain material and uses methods quite different from reCAPTCHA.\n\nThe reCAPTCHA program originated with Guatemalan computer scientist Luis von Ahn, and was aided by a MacArthur Fellowship. An early CAPTCHA developer, he realized \"he had unwittingly created a system that was frittering away, in ten-second increments, millions of hours of a most precious resource: human brain cycles\".\n\nScanned text is subjected to analysis by two different optical character recognition programs – one of them, as mentioned the project developer Ben Maurer, is ABBYY FineReader. Their respective outputs are then aligned with each other by standard string-matching algorithms and compared both to each other and to an English dictionary. Any word that is deciphered differently by both OCR programs or that is not in the English dictionary is marked as \"suspicious\" and converted into a CAPTCHA. The suspicious word is displayed, out of context, sometimes along with a control word already known. If the human types the control word correctly, then the response to the questionable word is accepted as probably valid. If enough users were to correctly type the control word, but incorrectly type the second word which OCR had failed to recognize, then the digital version of documents could end up containing the incorrect word. The identification performed by each OCR program is given a value of 0.5 points, and each interpretation by a human is given a full point. Once a given identification hits 2.5 points, the word is considered valid. Those words that are consistently given a single identity by human judges are later recycled as control words. If the first three guesses match each other but do not match either of the OCRs, they are considered a correct answer, and the word becomes a control word. When six users reject a word before any correct spelling is chosen, the word is discarded as unreadable.\n\nThe original reCAPTCHA method was designed to show the questionable words separately, as out-of-context correction, rather than in use, such as within a phrase of five words from the original document. Also, the control word might mislead context for the second word, such as a request of \"/metal/ /fife/\" being entered as \"metal file\" due to the logical connection of filing with a metal tool being considered more common than the musical instrument \"fife\".\n\nIn 2012, reCAPTCHA began using photographs of house numbers taken from Google's Street View project, in addition to scanned words.\nIn 2014, reCAPTCHA implemented another system in which users are asked to select one or more images from a selection of nine images.\n\nIn 2017, reCAPTCHA was improved to require no interaction for most users.\n\nIn 2013, reCAPTCHA began implementing behavioral analysis of the browser's interactions with the CAPTCHA to predict whether the user was a human or a bot before displaying the captcha, and presenting a \"considerably more difficult\" captcha in cases where it had reason to think the user might be a bot. By end of 2014 this mechanism started to be rolled out to most of the public Google services. Because NoCAPTCHA relies on the use of Google cookies that are at least a few weeks old, ReCAPTCHA has become nearly impossible to complete for people who frequently clear their cookies.\n\nIn 2017, Google improved this mechanism, calling it an \"invisible reCAPTCHA\". According to former Google \"click fraud czar\" Shuman Ghosemajumder, this capability \"creates a new sort of challenge that very advanced bots can still get around, but introduces a lot less friction to the legitimate human.\"\n\nThe reCAPTCHA tests are displayed from the central site of the reCAPTCHA project, which supplies the words to be deciphered. This is done through a JavaScript API with the server making a callback to reCAPTCHA after the request has been submitted. The reCAPTCHA project provides libraries for various programming languages and applications to make this process easier. reCAPTCHA is a gratis service (that is, the CAPTCHA images are provided to websites free of charge, in return for assistance with the decipherment), but the reCAPTCHA software itself is not open source.\n\nAlso, reCAPTCHA offers plugins for several web-application platforms, like ASP.NET, Ruby, or PHP, to ease the implementation of the service.\n\nSome have criticized Google for using reCAPTCHA as a source of unpaid labor. They say Google is unfairly using people around the world to help it transcribe books, addresses, and newspapers without any compensation.\n\nThe use of reCAPTCHA has been labelled \"a serious barrier to internet use\" for people with sight problems or disabilities such as dyslexia by a BBC journalist.\n\nSoftware engineer Andrew Munsell, in his article \"Captchas Are Becoming Ridiculous\" states \"A couple of years ago, I don’t remember being truly baffled by a captcha. In fact, reCAPTCHA was one of the better systems I’d seen. It wasn’t difficult to solve, and it seemed to work when I used it on my own websites.\" Munsell goes on to state, after encountering a series of unintelligible images that despite refreshing \"Again, and again, and again. The captchas were not only difficult for a computer to read, but impossible for a human.\" Munsell then provided numerous examples.\n\nThe main purpose of a CAPTCHA system is to prevent automated access to a system by computer programs or \"bots\". On 14 December 2009, Jonathan Wilkins released a paper describing weaknesses in reCAPTCHA that allowed a solve rate of 18%.\n\nOn 1 August 2010, Chad Houck gave a presentation to the DEF CON 18 Hacking Conference detailing a method to reverse the distortion added to images which allowed a computer program to determine a valid response 10% of the time. The reCAPTCHA system was modified on 21 July 2010, before Houck was to speak on his method. Houck modified his method to what he described as an \"easier\" CAPTCHA to determine a valid response 31.8% of the time. Houck also mentioned security defenses in the system, including a high security lock out if an invalid response is given 32 times in a row.\n\nOn 26 May 2012, Adam, C-P and Jeffball of DC949 gave a presentation at the LayerOne hacker conference detailing how they were able to achieve an automated solution with an accuracy rate of 99.1%. Their tactic was to use techniques from machine learning, a subfield of artificial intelligence, to analyse the audio version of reCAPTCHA which is available for the visually impaired. Google released a new version of reCAPTCHA just hours before their talk, making major changes to both the audio and visual versions of their service. In this release, the audio version was increased in length from 8 seconds to 30 seconds, and is much more difficult to understand, both for humans as well as bots. In response to this update and the following one, the members of DC949 released two more versions of Stiltwalker which beat reCAPTCHA with an accuracy of 60.95% and 59.4% respectively. After each successive break, Google updated reCAPTCHA within a few days. According to DC949, they often reverted to features that had been previously hacked.\n\nOn 27 June 2012, Claudia Cruz, Fernando Uceda, and Leobardo Reyes (a group of students from Mexico) published a paper showing a system running on reCAPTCHA images with an accuracy of 82%. The authors have not said if their system can solve recent reCAPTCHA images, although they claim their work to be intelligent OCR and robust to some, if not all changes in the image database.\n\nIn an August 2012 presentation given at BsidesLV 2012, DC949 called the latest version \"unfathomably impossible for humans\" – they were not able to solve them manually either. The web accessibility organization WebAIM reported in May 2012, \"Over 90% of respondents [screen reader users] find CAPTCHA to be very or somewhat difficult.\"\n\nreCAPTCHA frequently modifies its system, requiring spammers to frequently update their methods of decoding, which may frustrate potential abusers.\n\nOnly words that both OCR programs failed to recognize are used as control words. Thus, any program that can recognize these words with nonnegligible probability would represent an improvement over state of the art OCR programs.\n\nreCAPTCHA had also created project Mailhide, which protects email addresses on web pages from being harvested by spammers. By default, the email address is converted into a format that does not allow a crawler to see the full email address; for example, \"mailme@example.com\" would be converted to \"mai...@example.com\". The visitor would then click on the \"...\" and solve the CAPTCHA in order to obtain the full email address. One can also edit the pop-up code so that none of the address is visible.\n\n", "id": "11451897", "title": "ReCAPTCHA"}
{"url": "https://en.wikipedia.org/wiki?curid=48589354", "text": "Visual Turing Test\n\nComputer Vision research is driven by standard evaluation practices. The current systems are tested by their accuracy for tasks like object detection, segmentation and localization. Methods like the convolutional neural networks seem to be doing pretty well in these tasks, but the current systems are still not close to solving the ultimate problem of understanding images the way humans do. So motivated by the ability of humans to understand an image and even tell a story about it, Geman \"et al.\" have introduced the Visual Turing Test for computer vision systems.\n\nAs described in, it is “an operator-assisted device that produces a stochastic sequence of binary questions from a given test image”. The query engine produces a sequence of questions that have unpredictable answers given the history of questions. The test is only about vision and does not require any natural language processing. The job of the human operator is to provide the correct answer to the question or reject it as ambiguous. The query generator produces questions such that they follow a “natural story line”, similar to what humans do when they look at a picture.\n\nResearch in Computer Vision dates back to the 1960s when Seymour Papert first attempted to solve the problem. This unsuccessful attempt was referred to as the Summer Vision Project. The reason why it was not successful was because computer vision is more complicated than what people think. The complexity is in alignment with the human visual system. Roughly 50% of the human brain is devoted in processing vision, which clearly indicates that it is a difficult problem.\n\nLater there were attempts to solve the problems with models inspired by the human brain. Perceptrons by Frank Rosenblatt, which is a form of the neural networks, was one of the first such approaches. These simple neural networks could not live up to their expectations and had certain limitations due to which they were not considered in future research.\n\nLater with the availability of the hardware and some processing power the research shifted to Image processing which involves pixel-level operations, like finding edges, de-noising images or applying filters to name a few. There was some great progress in this field but the problem of vision which was to make the machines understand the images was still not being addressed. During this time the neural networks also resurfaced as it was shown that the limitations of the perceptrons can be overcome by Multi-layer perceptrons. Also in the early 90’s convolutional neural networks were born which showed great results on digit recognition but did not scale up well on harder problems.\n\nLate 1990’s and early 2000’s saw the birth of modern computer vision. One of the reasons this happened was due to the availability of key, feature extraction and representation algorithms. Features along with the already present machine learning algorithms were used to detect, localise and segment objects in Images.\n\nWhile all these advancements were being made, the community felt the need to have standardised datasets and evaluation metrics so the performances can be compared. This led to the emergence of challenges like the Pascal VOC challenge and the ImageNet challenge. The availability of standard evaluation metrics and the open challenges gave directions to the research. Better algorithms were introduced for specific tasks like object detection and classification.\n\nVisual Turing Test aims to give a new direction to the computer vision research which would lead to the introduction of systems that will be one step closer to understanding images the way humans do.\n\nA large number of datasets have been annotated and generalised to benchmark performances of difference classes of algorithms to assess different vision tasks (e.g., object detection/recognition) on some image domain (e.g., scene images).\n\nOne of the most famous datasets in computer vision is ImageNet which is used to assess the problem of object level Image classification. ImageNet is one of the largest annotated datasets available and has over one million images. The other important vision task is object detection and localisation which refers to detecting the object instance in the image and providing the bounding box coordinates around the object instance or segmenting the object. The most popular dataset for this task is the Pascal dataset. Similarly there are other datasets for specific tasks like the H3D dataset for human pose detection, Core dataset to evaluate the quality of detected object attributes such as colour, orientation, and activity.\n\nHaving these standard datasets has helped the vision community to come up with extremely well performing algorithms for all these tasks. The next logical step is to create a larger task encompassing of these smaller subtasks. Having such a task would lead to building systems that would understand images, as understanding images would inherently involve detecting objects, localising them and segmenting them.\n\nThe Visual Turing Test (VTT) unlike the Turing Test has a query engine system which interrogates a computer vision system in the presence of a human co-ordinator.\n\nIt is a system that generates a random sequence of binary questions specific to the test image, such that the answer to any question \"k\" is unpredictable given the true answers to the previous \"k-1\" questions (also known as history of questions).\n\nThe test happens in the presence of a human operator who serves two main purposes: removing the ambiguous questions and providing the correct answers to the unambiguous questions. Given an Image infinite possible binary questions can be asked and a lot of them are bound to be ambiguous. These questions if generated by the query engine are removed by the human moderator and instead the query engine generates another question such that the answer to it is unpredictable given the history of the questions.\n\nThe aim of the Visual Turing Test is to evaluate the Image understanding of a computer system, and an important part of image understanding is the story line of the image. When humans look at an image, they do not think that there is a car at ‘\"x\"’ pixels from the left and ‘\"y\"’ pixels from the top, but instead they look at it as a story, for e.g. they might think that there is a car parked on the road, a person is exiting the car and heading towards a building. The most important elements of the story line are the objects and so to extract any story line from an image the first and the most important task is to instantiate the objects in it, and that is what the query engine does.\n\nThe query engine is the core of the Visual Turing Test and it comprises two main parts : Vocabulary and Questions\n\nVocabulary is a set of words that represent the elements of the images. This vocabulary when used with appropriate grammar leads to a set of questions. The grammar is defined in the next section in a way that it leads to a space of binary questions.\n\nThe vocabulary formula_1 consist of three components:\nFor Images of urban street scenes the types of objects include \"people\", \"vehicle\" and \"buildings\". Attributes refer to the properties of these objects, for e.g. \"female, child, wearing a hat or carrying something\", for people and \"moving, parked, stopped, one tire visible or two tires visible\" for vehicles. Relationships between each pair of object classes can be either “ordered” or “unordered”. The unordered relationships may include \"talking\", \"walking\" \"together\" and the ordered relationships include \"taller\", \"closer to the camera, occluding, being occluded\" etc.\n\nAdditionally all of this vocabulary is used in context of rectangular image regions w \\in W which allow for the localisation of objects in the image. An extremely large number of such regions are possible and this complicates the problem, so for this test, regions at specific scales are only used which include 1/16 the size of image, 1/4 the size of image, 1/2 the size of image or larger.\n\nThe question space is composed of 4 types of questions:\n \"Q = 'Is there an instance of an object of type t with attributes A partially visible in region w that was not previously instantiated?\"'\n \"Q = 'Is there a unique instance of an object of type t with attributes A partially visible in region w that was not previously instantiated?\"'\nThe uniqueness questions along with the existence questions form the instantiation questions. As mentioned earlier instantiating objects leads to other interesting questions and eventually a story line. Uniqueness questions follow the existence questions and a positive answer to it leads to instantiation of an object.\n \"Q(o) = {'Does object o have attribute a?'\" , '\"Does object o have attribute a or attribute a?'\" , \"'Does object o have attribute a and attribute a?'}\"\n \"Q(o,o) = 'Does object o have relationship r with object o?\"'\n\nAs mentioned before the core of the Visual Turing Test is the query generator which generates a sequence of binary questions such that the answer to any question \"k\" is unpredictable given the correct answers to the previous \"k-1\" questions. This is a recursive process, given a history of questions and their correct answers, the query generator either stops because there are no more unpredictable questions, or randomly selects an unpredictable question and adds it to the history.\n\nThe question space defined earlier implicitly imposes a constraint on the flow of the questions. To make it more clear this means that the attribute and relationship questions can not precede the instantiation questions. Only when the objects have been instantiated, can they be queried about their attributes and relations to other previously instantiated objects. Thus given a history we can restrict the possible questions that can follow it, and this set of questions are referred to as the candidate questions formula_5.\n\nThe task is to choose an unpredictable question from these candidate questions such that it conforms with the question flow that we will describe in the next section. For this, find the unpredictability of every question among the candidate questions.\n\nLet formula_6 be a binary random variable, where formula_7, if the history formula_6 is valid for the Image formula_9 and formula_10 otherwise. Let formula_11 can be the proposed question, and formula_12 be the answer to the question formula_13.\n\nThen, find the conditional probability of getting the answer X to the question \"q\" given the history H.\n\nformula_14\n\nGiven this probability the measure of the unpredictability is given by:\n\nformula_15\n\nThe closer formula_16 is to 0, the more unpredictable the question is. formula_16 for every question is calculated. The questions for which formula_18, are the set of almost unpredictable questions and the next question is randomly picked from these.\n\nAs discussed in the previous section there is an implicit ordering in the question space, according to which the attribute questions come after the instantiation questions and the relationship questions come after the attribute questions, once multiple objects have been instantiated.\n\nTherefore, the query engine follows a loop structure where it first instantiates an object with the existence and uniqueness questions, then queries about its attributes, and then the relationship questions are asked for that object with all the previously instantiated objects.\n\nIt is clear that the interesting questions about the attributes and the relations come after the instantiation questions, and so the query generator aims at instantiating as many objects as possible.\n\nInstantiation questions are composed of both the existence and the uniqueness questions, but it is the uniqueness questions that actually instantiate an object if they get a positive response. So if the query generator has to randomly pick an instantiation question, it prefers to pick an unpredictable uniqueness question if present. If such a question is not present, the query generator picks an existence question such that it will lead to a uniqueness question with a high probability in the future. Thus the query generator performs a look-ahead search in this case.\n\nAn integral part of the ultimate aim of building systems that can understand images the way humans do, is the story line. Humans try to figure out a story line in the Image they see. The query generator achieves this by a continuity in the question sequences.\n\nThis means that once the object has been instantiated it tries to explore it in more details. Apart from finding its attributes and relation to the other objects, localisation is also an important step. Thus, as a next step the query generator tries to localise the object in the region it was first identified, so it restricts the set of instantiation questions to the regions within the original region.\n\nSimplicity preference states that the query generator should pick simpler questions over the more complicated ones. Simpler questions are the ones that have less number of attributes in them. So this gives an ordering to the questions based on the number of attributes, and the query generator prefers the simpler ones.\n\nTo select the next question in the sequence, VTT has to estimate the predictability of every proposed question. This is done using the annotated training set of Images. Each Image is annotated with bounding box around the objects and labelled with the attributes, and pairs of objects are labelled with the relations.<br>Let us consider each question type separately: \n\nDetailed example sequences can be found here.\n\nThe Images considered for the Geman \"et al.\" work are that of ‘Urban street scenes’ dataset, which has scenes of streets from different cities across the world. This why the types of objects are constrained to people and vehicles for this experiment.\nAnother dataset introduced by the Max Planck Institute for Informatics is known as DAQUAR dataset which has real world images of indoor scenes. But they propose a different version of the visual Turing test which takes on a holistic approach and expects the participating system to exhibit human like common sense.\nThis is a very recent work published on March 9, 2015, in the journal \"Proceedings\" of the National Academy of Sciences, by researchers from Brown University and Johns Hopkins University. It evaluates how the computer vision systems understand the Images as compared to humans. Currently the test is written and the interrogator is a machine because having an oral evaluation by a human interrogator gives the humans an undue advantage of being subjective, and also expects real time answers.\n\nThe Visual Turing Test is expected to give a new direction to the computer vision research. Companies like Google and Facebook are investing millions of dollars into computer vision research, and are trying to build systems that closely resemble the human visual system. Recently Facebook announced its new platform M, which looks at an image and provides a description of it to help the visually impaired. Such systems might be able to perform well on the VTT.\n", "id": "48589354", "title": "Visual Turing Test"}
{"url": "https://en.wikipedia.org/wiki?curid=16795043", "text": "Graphics Turing Test\n\nIn computer graphics the Graphics Turing Test is a variant of the Turing Test, the twist being that a human judge viewing and interacting with an artificially generated world should be unable to reliably distinguish it from reality.\n\nThe original formulation of the test is:\n\nThe \"Graphics Turing Scale\" of computer power is then defined as the computing power necessary to achieve success in the test. It was estimated in as 1036.8 TFlops peak and 518.4 TFlops sustained. Actual rendering tests with a Blue Gene supercomputer showed that current supercomputers are not up to the task scale yet.\n\nA restricted form of the graphic Turing test has been investigated, where test subjects look into a box and try to tell whether the contents are real or virtual objects. For the very simple case of scenes with a cardboard pyramid or a styrofoam sphere, subjects were not able to reliably tell reality and graphics apart. \n\n\n", "id": "16795043", "title": "Graphics Turing Test"}
{"url": "https://en.wikipedia.org/wiki?curid=21391751", "text": "Turing test\n\nThe Turing test, developed by Alan Turing in 1950, is a test of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. Turing proposed that a human evaluator would judge natural language conversations between a human and a machine designed to generate human-like responses. The evaluator would be aware that one of the two partners in conversation is a machine, and all participants would be separated from one another. The conversation would be limited to a text-only channel such as a computer keyboard and screen so the result would not depend on the machine's ability to render words as speech. If the evaluator cannot reliably tell the machine from the human, the machine is said to have passed the test. The test does not check the ability to give correct answers to questions, only how closely answers resemble those a human would give.\n\nThe test was introduced by Turing in his paper, \"Computing Machinery and Intelligence\", while working at the University of Manchester (Turing, 1950; p. 460). It opens with the words: \"I propose to consider the question, 'Can machines think? Because \"thinking\" is difficult to define, Turing chooses to \"replace the question by another, which is closely related to it and is expressed in relatively unambiguous words.\" Turing's new question is: \"Are there imaginable digital computers which would do well in the \"imitation game\"?\" This question, Turing believed, is one that can actually be answered. In the remainder of the paper, he argued against all the major objections to the proposition that \"machines can think\".\n\nSince Turing first introduced his test, it has proven to be both highly influential and widely criticised, and it has become an important concept in the philosophy of artificial intelligence.\n\nThe question of whether it is possible for machines to think has a long history, which is firmly entrenched in the distinction between dualist and materialist views of the mind. René Descartes prefigures aspects of the Turing Test in his 1637 Discourse on the Method when he writes: \nHere Descartes notes that automata are capable of responding to human interactions but argues that such automata cannot respond appropriately to things said in their presence in the way that any human can. Descartes therefore prefigures the Turing Test by defining the insufficiency of appropriate linguistic response as that which separates the human from the automaton. Descartes fails to consider the possibility that future automata might be able to overcome such insufficiency, and so does not propose the Turing Test as such, even if he prefigures its conceptual framework and criterion.\n\nDenis Diderot formulates in his \"Pensées philosophiques\" a Turing-test criterion:\n\n\"If they find a parrot who could answer to everything, I would claim it to be an intelligent being without hesitation.\"\n\nThis does not mean he agrees with this, but that it was already a common argument of materialists at that time.\n\nAccording to dualism, the mind is non-physical (or, at the very least, has non-physical properties) and, therefore, cannot be explained in purely physical terms. According to materialism, the mind can be explained physically, which leaves open the possibility of minds that are produced artificially.\n\nIn 1936, philosopher Alfred Ayer considered the standard philosophical question of other minds: how do we know that other people have the same conscious experiences that we do? In his book, \"Language, Truth and Logic\", Ayer suggested a protocol to distinguish between a conscious man and an unconscious machine: \"The only ground I can have for asserting that an object which appears to be conscious is not really a conscious being, but only a dummy or a machine, is that it fails to satisfy one of the empirical tests by which the presence or absence of consciousness is determined.\" (This suggestion is very similar to the Turing test, but is concerned with consciousness rather than intelligence. Moreover, it is not certain that Ayer's popular philosophical classic was familiar to Turing.) In other words, a thing is not conscious if it fails the consciousness test.\n\nResearchers in the United Kingdom had been exploring \"machine intelligence\" for up to ten years prior to the founding of the field of artificial intelligence (AI) research in 1956. It was a common topic among the members of the Ratio Club, who were an informal group of British cybernetics and electronics researchers that included Alan Turing, after whom the test is named.\n\nTuring, in particular, had been tackling the notion of machine intelligence since at least 1941 and one of the earliest-known mentions of \"computer intelligence\" was made by him in 1947. In Turing's report, \"Intelligent Machinery\", he investigated \"the question of whether or not it is possible for machinery to show intelligent behaviour\" and, as part of that investigation, proposed what may be considered the forerunner to his later tests:\nIt is not difficult to devise a paper machine which will play a not very bad game of chess. Now get three men as subjects for the experiment. A, B and C. A and C are to be rather poor chess players, B is the operator who works the paper machine. ... Two rooms are used with some arrangement for communicating moves, and a game is played between C and either A or the paper machine. C may find it quite difficult to tell which he is playing.\n\n\"Computing Machinery and Intelligence\" (1950) was the first published paper by Turing to focus exclusively on machine intelligence. Turing begins the 1950 paper with the claim, \"I propose to consider the question 'Can machines think? As he highlights, the traditional approach to such a question is to start with definitions, defining both the terms \"machine\" and \"intelligence\". Turing chooses not to do so; instead he replaces the question with a new one, \"which is closely related to it and is expressed in relatively unambiguous words.\" In essence he proposes to change the question from \"Can machines think?\" to \"Can machines do what we (as thinking entities) can do?\" The advantage of the new question, Turing argues, is that it draws \"a fairly sharp line between the physical and intellectual capacities of a man.\"\n\nTo demonstrate this approach Turing proposes a test inspired by a party game, known as the \"Imitation Game,\" in which a man and a woman go into separate rooms and guests try to tell them apart by writing a series of questions and reading the typewritten answers sent back. In this game both the man and the woman aim to convince the guests that they are the other. (Huma Shah argues that this two-human version of the game was presented by Turing only to introduce the reader to the machine-human question-answer test.) Turing described his new version of the game as follows:\n\nWe now ask the question, \"What will happen when a machine takes the part of A in this game?\" Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, \"Can machines think?\"\n\nLater in the paper Turing suggests an \"equivalent\" alternative formulation involving a judge conversing only with a computer and a man. While neither of these formulations precisely matches the version of the Turing Test that is more generally known today, he proposed a third in 1952. In this version, which Turing discussed in a BBC radio broadcast, a jury asks questions of a computer and the role of the computer is to make a significant proportion of the jury believe that it is really a man.\n\nTuring's paper considered nine putative objections, which include all the major arguments against artificial intelligence that have been raised in the years since the paper was published (see \"Computing Machinery and Intelligence\").\n\nIn 1966, Joseph Weizenbaum created a program which appeared to pass the Turing test. The program, known as ELIZA, worked by examining a user's typed comments for keywords. If a keyword is found, a rule that transforms the user's comments is applied, and the resulting sentence is returned. If a keyword is not found, ELIZA responds either with a generic riposte or by repeating one of the earlier comments. In addition, Weizenbaum developed ELIZA to replicate the behaviour of a Rogerian psychotherapist, allowing ELIZA to be \"free to assume the pose of knowing almost nothing of the real world.\" With these techniques, Weizenbaum's program was able to fool some people into believing that they were talking to a real person, with some subjects being \"very hard to convince that ELIZA [...] is \"not\" human.\" Thus, ELIZA is claimed by some to be one of the programs (perhaps the first) able to pass the Turing Test, even though this view is highly contentious (see below).\n\nKenneth Colby created PARRY in 1972, a program described as \"ELIZA with attitude\". It attempted to model the behaviour of a paranoid schizophrenic, using a similar (if more advanced) approach to that employed by Weizenbaum. To validate the work, PARRY was tested in the early 1970s using a variation of the Turing Test. A group of experienced psychiatrists analysed a combination of real patients and computers running PARRY through teleprinters. Another group of 33 psychiatrists were shown transcripts of the conversations. The two groups were then asked to identify which of the \"patients\" were human and which were computer programs. The psychiatrists were able to make the correct identification only 48 percent of the time – a figure consistent with random guessing.\n\nIn the 21st century, versions of these programs (now known as \"chatterbots\") continue to fool people. \"CyberLover\", a malware program, preys on Internet users by convincing them to \"reveal information about their identities or to lead them to visit a web site that will deliver malicious content to their computers\". The program has emerged as a \"Valentine-risk\" flirting with people \"seeking relationships online in order to collect their personal data\".\n\nJohn Searle's 1980 paper \"Minds, Brains, and Programs\" proposed the \"Chinese room\" thought experiment and argued that the Turing test could not be used to determine if a machine can think. Searle noted that software (such as ELIZA) could pass the Turing Test simply by manipulating symbols of which they had no understanding. Without understanding, they could not be described as \"thinking\" in the same sense people do. Therefore, Searle concludes, the Turing Test cannot prove that a machine can think. Much like the Turing test itself, Searle's argument has been both widely criticised and highly endorsed.\n\nArguments such as Searle's and others working on the philosophy of mind sparked off a more intense debate about the nature of intelligence, the possibility of intelligent machines and the value of the Turing test that continued through the 1980s and 1990s.\n\nThe Loebner Prize provides an annual platform for practical Turing Tests with the first competition held in November 1991. It is underwritten by Hugh Loebner. The Cambridge Center for Behavioral Studies in Massachusetts, United States, organised the prizes up to and including the 2003 contest. As Loebner described it, one reason the competition was created is to advance the state of AI research, at least in part, because no one had taken steps to implement the Turing Test despite 40 years of discussing it.\n\nThe first Loebner Prize competition in 1991 led to a renewed discussion of the viability of the Turing Test and the value of pursuing it, in both the popular press and academia. The first contest was won by a mindless program with no identifiable intelligence that managed to fool naïve interrogators into making the wrong identification. This highlighted several of the shortcomings of the Turing Test (discussed below): The winner won, at least in part, because it was able to \"imitate human typing errors\"; the unsophisticated interrogators were easily fooled; and some researchers in AI have been led to feel that the test is merely a distraction from more fruitful research.\n\nThe silver (text only) and gold (audio and visual) prizes have never been won. However, the competition has awarded the bronze medal every year for the computer system that, in the judges' opinions, demonstrates the \"most human\" conversational behaviour among that year's entries. Artificial Linguistic Internet Computer Entity (A.L.I.C.E.) has won the bronze award on three occasions in recent times (2000, 2001, 2004). Learning AI Jabberwacky won in 2005 and 2006.\n\nThe Loebner Prize tests conversational intelligence; winners are typically chatterbot programs, or Artificial Conversational Entities (ACE)s. Early Loebner Prize rules restricted conversations: Each entry and hidden-human conversed on a single topic, thus the interrogators were restricted to one line of questioning per entity interaction. The restricted conversation rule was lifted for the 1995 Loebner Prize. Interaction duration between judge and entity has varied in Loebner Prizes. In Loebner 2003, at the University of Surrey, each interrogator was allowed five minutes to interact with an entity, machine or hidden-human. Between 2004 and 2007, the interaction time allowed in Loebner Prizes was more than twenty minutes. In 2008, the interrogation duration allowed was five minutes per pair, because the organiser, Kevin Warwick, and coordinator, Huma Shah, consider this to be the duration for any test, as Turing stated in his 1950 paper: \" ... making the right identification after five minutes of questioning\". They felt Loebner's longer test, implemented in Loebner Prizes 2006 and 2007, was inappropriate for the state of artificial conversation technology. It is ironic that the 2008 winning entry, Elbot from Artificial Solutions, does not mimic a human; its personality is that of a robot, yet Elbot deceived three human judges that it was the human during human-parallel comparisons.\n\nDuring the 2009 competition, held in Brighton, UK, the communication program restricted judges to 10 minutes for each round, 5 minutes to converse with the human, 5 minutes to converse with the program. This was to test the alternative reading of Turing's prediction that the 5-minute interaction was to be with the computer. For the 2010 competition, the Sponsor again increased the interaction time, between interrogator and system, to 25 minutes, well above the figure given by Turing.\n\nOn 7 June 2014 a Turing test competition, organised by Huma Shah and Kevin Warwick to mark the 60th anniversary of Turing's death, was held at the Royal Society London and was won by the Russian chatter bot Eugene Goostman. The bot, during a series of five-minute-long text conversations, convinced 33% of the contest's judges that it was human. Judges included John Sharkey, a sponsor of the bill granting a government pardon to Turing, AI Professor Aaron Sloman and \"Red Dwarf\" actor Robert Llewellyn.\n\nThe competition's organisers believed that the Turing test had been \"passed for the first time\" at the event, saying that \"The event involved more simultaneous comparison tests than ever before, was independently verified and, crucially, the conversations were unrestricted. A true Turing Test does not set the questions or topics prior to the conversations.\"\n\nSaul Traiger argues that there are at least three primary versions of the Turing test, two of which are offered in \"Computing Machinery and Intelligence\" and one that he describes as the \"Standard Interpretation.\" While there is some debate regarding whether the \"Standard Interpretation\" is that described by Turing or, instead, based on a misreading of his paper, these three versions are not regarded as equivalent, and their strengths and weaknesses are distinct.\n\nHuma Shah points out that Turing himself was concerned with whether a machine could think and was providing a simple method to examine this: through human-machine question-answer sessions. Shah argues there is one imitation game which Turing described could be practicalised in two different ways: a) one-to-one interrogator-machine test, and b) simultaneous comparison of a machine with a human, both questioned in parallel by an interrogator. Since the Turing test is a test of indistinguishability in performance capacity, the verbal version generalises naturally to all of human performance capacity, verbal as well as nonverbal (robotic).\n\nTuring's original article describes a simple party game involving three players. Player A is a man, player B is a woman and player C (who plays the role of the interrogator) is of either sex. In the Imitation Game, player C is unable to see either player A or player B, and can communicate with them only through written notes. By asking questions of player A and player B, player C tries to determine which of the two is the man and which is the woman. Player A's role is to trick the interrogator into making the wrong decision, while player B attempts to assist the interrogator in making the right one.\n\nTuring then asks:\n\nWhat will happen when a machine takes the part of A in this game? Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, \"Can machines think?\"\n\nThe second version appeared later in Turing's 1950 paper. Similar to the Original Imitation Game Test, the role of player A is performed by a computer. However, the role of player B is performed by a man rather than a woman.\n\n\"Let us fix our attention on one particular digital computer \"C.\" Is it true that by modifying this computer to have an adequate storage, suitably increasing its speed of action, and providing it with an appropriate programme, \"C\" can be made to play satisfactorily the part of A in the imitation game, the part of B being taken by a man?\"\n\nIn this version, both player A (the computer) and player B are trying to trick the interrogator into making an incorrect decision.\n\nCommon understanding has it that the purpose of the Turing Test is not specifically to determine whether a computer is able to fool an interrogator into believing that it is a human, but rather whether a computer could \"imitate\" a human. While there is some dispute whether this interpretation was intended by Turing, Sterrett believes that it was and thus conflates the second version with this one, while others, such as Traiger, do not – this has nevertheless led to what can be viewed as the \"standard interpretation.\" In this version, player A is a computer and player B a person of either sex. The role of the interrogator is not to determine which is male and which is female, but which is a computer and which is a human. The fundamental issue with the standard interpretation is that the interrogator cannot differentiate which responder is human, and which is machine. There are issues about duration, but the standard interpretation generally considers this limitation as something that should be reasonable.\n\nControversy has arisen over which of the alternative formulations of the test Turing intended. Sterrett argues that two distinct tests can be extracted from his 1950 paper and that, \"pace\" Turing's remark, they are not equivalent. The test that employs the party game and compares frequencies of success is referred to as the \"Original Imitation Game Test,\" whereas the test consisting of a human judge conversing with a human and a machine is referred to as the \"Standard Turing Test,\" noting that Sterrett equates this with the \"standard interpretation\" rather than the second version of the imitation game. Sterrett agrees that the Standard Turing Test (STT) has the problems that its critics cite but feels that, in contrast, the Original Imitation Game Test (OIG Test) so defined is immune to many of them, due to a crucial difference: Unlike the STT, it does not make similarity to human performance the criterion, even though it employs human performance in setting a criterion for machine intelligence. A man can fail the OIG Test, but it is argued that it is a virtue of a test of intelligence that failure indicates a lack of resourcefulness: The OIG Test requires the resourcefulness associated with intelligence and not merely \"simulation of human conversational behaviour.\" The general structure of the OIG Test could even be used with non-verbal versions of imitation games.\n\nStill other writers have interpreted Turing as proposing that the imitation game itself is the test, without specifying how to take into account Turing's statement that the test that he proposed using the party version of the imitation game is based upon a criterion of comparative frequency of success in that imitation game, rather than a capacity to succeed at one round of the game.\n\nSaygin has suggested that maybe the original game is a way of proposing a less biased experimental design as it hides the participation of the computer. The imitation game also includes a \"social hack\" not found in the standard interpretation, as in the game both computer and male human are required to play as pretending to be someone they are not.\n\nA crucial piece of any laboratory test is that there should be a control. Turing never makes clear whether the interrogator in his tests is aware that one of the participants is a computer. However, if there were a machine that did have the potential to pass a Turing test, it would be safe to assume a double blind control would be necessary.\n\nTo return to the Original Imitation Game, he states only that player A is to be replaced with a machine, not that player C is to be made aware of this replacement. When Colby, FD Hilf, S Weber and AD Kramer tested PARRY, they did so by assuming that the interrogators did not need to know that one or more of those being interviewed was a computer during the interrogation. As Ayse Saygin, Peter Swirski, and others have highlighted, this makes a big difference to the implementation and outcome of the test. An experimental study looking at Gricean maxim violations using transcripts of Loebner's one-to-one (interrogator-hidden interlocutor) Prize for AI contests between 1994–1999, Ayse Saygin found significant differences between the responses of participants who knew and did not know about computers being involved.\n\nHuma Shah and Kevin Warwick, who organised the 2008 Loebner Prize at Reading University which staged simultaneous comparison tests (one judge-two hidden interlocutors), showed that knowing/not knowing did not make a significant difference in some judges' determination. Judges were not explicitly told about the nature of the pairs of hidden interlocutors they would interrogate. Judges were able to distinguish human from machine, including when they were faced with control pairs of two humans and two machines embedded among the machine-human set-ups. Spelling errors gave away the hidden humans; machines were identified by 'speed of response' and lengthier utterances.\n\nThe power and appeal of the Turing test derives from its simplicity. The philosophy of mind, psychology, and modern neuroscience have been unable to provide definitions of \"intelligence\" and \"thinking\" that are sufficiently precise and general to be applied to machines. Without such definitions, the central questions of the philosophy of artificial intelligence cannot be answered. The Turing test, even if imperfect, at least provides something that can actually be measured. As such, it is a pragmatic attempt to answer a difficult philosophical question.\n\nThe format of the test allows the interrogator to give the machine a wide variety of intellectual tasks. Turing wrote that \"the question and answer method seems to be suitable for introducing almost any one of the fields of human endeavour that we wish to include.\" John Haugeland adds that \"understanding the words is not enough; you have to understand the \"topic\" as well.\"\n\nTo pass a well-designed Turing test, the machine must use natural language, reason, have knowledge and learn. The test can be extended to include video input, as well as a \"hatch\" through which objects can be passed: this would force the machine to demonstrate the skill of vision and robotics as well. Together, these represent almost all of the major problems that artificial intelligence research would like to solve.\n\nThe Feigenbaum test is designed to take advantage of the broad range of topics available to a Turing test. It is a limited form of Turing's question-answer game which compares the machine against the abilities of experts in specific fields such as literature or chemistry. IBM's Watson machine achieved success in a man versus machine television quiz show of human knowledge, Jeopardy!\n\nAs a Cambridge honours graduate in mathematics, Turing might have been expected to propose a test of computer intelligence requiring expert knowledge in some highly technical field, and thus anticipating a more recent approach to the subject. Instead, as already noted, the test which he described in his seminal 1950 paper requires the computer to be able to compete successfully in a common party game, and this by performing as well as the typical man in answering a series of questions so as to pretend convincingly to be the woman contestant.\n\nGiven the status of human sexual dimorphism as one of the most ancient of subjects, it is thus implicit in the above scenario that the questions to be answered will involve neither specialised factual knowledge nor information processing technique. The challenge for the computer, rather, will be to demonstrate empathy for the role of the female, and to demonstrate as well a characteristic aesthetic sensibility—both of which qualities are on display in this snippet of dialogue which Turing has imagined:\n\nWhen Turing does introduce some specialised knowledge into one of his imagined dialogues, the subject is not maths or electronics, but poetry:\n\nTuring thus once again demonstrates his interest in empathy and aesthetic sensitivity as components of an artificial intelligence; and in light of an increasing awareness of the threat from an AI run amuck, it has been suggested that this focus perhaps represents a critical intuition on Turing's part, i.e., that emotional and aesthetic intelligence will play a key role in the creation of a \"friendly AI\". It is further noted, however, that whatever inspiration Turing might be able to lend in this direction depends upon the preservation of his original vision, which is to say, further, that the promulgation of a \"standard interpretation\" of the Turing test—i.e., one which focuses on a discursive intelligence only—must be regarded with some caution.\n\nTuring did not explicitly state that the Turing test could be used as a measure of intelligence, or any other human quality. He wanted to provide a clear and understandable alternative to the word \"think\", which he could then use to reply to criticisms of the possibility of \"thinking machines\" and to suggest ways that research might move forward.\n\nNevertheless, the Turing test has been proposed as a measure of a machine's \"ability to think\" or its \"intelligence\". This proposal has received criticism from both philosophers and computer scientists. It assumes that an interrogator can determine if a machine is \"thinking\" by comparing its behaviour with human behaviour. Every element of this assumption has been questioned: the reliability of the interrogator's judgement, the value of comparing only behaviour and the value of comparing the machine with a human. Because of these and other considerations, some AI researchers have questioned the relevance of the test to their field.\n\nThe Turing test does not directly test whether the computer behaves intelligently. It tests only whether the computer behaves like a human being. Since human behaviour and intelligent behaviour are not exactly the same thing, the test can fail to accurately measure intelligence in two ways:\n\n\n\nThe Turing test is concerned strictly with how the subject \"acts\" – the external behaviour of the machine. In this regard, it takes a behaviourist or functionalist approach to the study of the mind. The example of ELIZA suggests that a machine passing the test may be able to simulate human conversational behaviour by following a simple (but large) list of mechanical rules, without thinking or having a mind at all.\n\nJohn Searle has argued that external behaviour cannot be used to determine if a machine is \"actually\" thinking or merely \"simulating thinking.\" His Chinese room argument is intended to show that, even if the Turing test is a good operational definition of intelligence, it may not indicate that the machine has a mind, consciousness, or intentionality. (Intentionality is a philosophical term for the power of thoughts to be \"about\" something.)\n\nTuring anticipated this line of criticism in his original paper, writing: \n\nIn practice, the test's results can easily be dominated not by the computer's intelligence, but by the attitudes, skill, or naïveté of the questioner.\n\nTuring does not specify the precise skills and knowledge required by the interrogator in his description of the test, but he did use the term \"average interrogator\": \"[the] average interrogator would not have more than 70 per cent chance of making the right identification after five minutes of questioning\".\n\nChatterbot programs such as ELIZA have repeatedly fooled unsuspecting people into believing that they are communicating with human beings. In these cases, the \"interrogators\" are not even aware of the possibility that they are interacting with computers. To successfully appear human, there is no need for the machine to have any intelligence whatsoever and only a superficial resemblance to human behaviour is required.\n\nEarly Loebner Prize competitions used \"unsophisticated\" interrogators who were easily fooled by the machines. Since 2004, the Loebner Prize organisers have deployed philosophers, computer scientists, and journalists among the interrogators. Nonetheless, some of these experts have been deceived by the machines.\n\nMichael Shermer points out that human beings consistently choose to consider non-human objects as human whenever they are allowed the chance, a mistake called the anthropomorphic fallacy: They talk to their cars, ascribe desire and intentions to natural forces (e.g., \"nature abhors a vacuum\"), and worship the sun as a human-like being with intelligence. If the Turing test is applied to religious objects, Shermer argues, then, that inanimate statues, rocks, and places have consistently passed the test throughout history. This human tendency towards anthropomorphism effectively lowers the bar for the Turing test, unless interrogators are specifically trained to avoid it.\n\nOne interesting feature of the Turing Test is the frequency of the confederate effect, when the confederate (tested) humans are misidentified by the interrogators as machines. It has been suggested that what interrogators expect as human responses is not necessarily typical of humans. As a result, some individuals can be categorised as machines. This can therefore work in favour of a competing machine. The humans are instructed to \"act themselves\", but sometimes their answers are more like what the interrogator expects a machine to say. This raises the question of how to ensure that the humans are motivated to \"act human\".\n\nA critical aspect of the Turing test is that a machine must give itself away as being a machine by its utterances. An interrogator must then make the \"right identification\" by correctly identifying the machine as being just that. If however a machine remains silent during a conversation, i.e. takes the fifth, then it is not possible for an interrogator to accurately identify the machine other than by means of a calculated guess.\nEven taking into account a parallel/hidden human as part of the test may not help the situation as humans can often be misidentified as being a machine.\n\nMainstream AI researchers argue that trying to pass the Turing Test is merely a distraction from more fruitful research. Indeed, the Turing test is not an active focus of much academic or commercial effort—as Stuart Russell and Peter Norvig write: \"AI researchers have devoted little attention to passing the Turing test.\" There are several reasons.\n\nFirst, there are easier ways to test their programs. Most current research in AI-related fields is aimed at modest and specific goals, such as automated scheduling, object recognition, or logistics. To test the intelligence of the programs that solve these problems, AI researchers simply give them the task directly. Russell and Norvig suggest an analogy with the history of flight: Planes are tested by how well they fly, not by comparing them to birds. \"Aeronautical engineering texts,\" they write, \"do not define the goal of their field as 'making machines that fly so exactly like pigeons that they can fool other pigeons.\n\nSecond, creating lifelike simulations of human beings is a difficult problem on its own that does not need to be solved to achieve the basic goals of AI research. Believable human characters may be interesting in a work of art, a game, or a sophisticated user interface, but they are not part of the science of creating intelligent machines, that is, machines that solve problems using intelligence.\n\nTuring wanted to provide a clear and understandable example to aid in the discussion of the philosophy of artificial intelligence. John McCarthy observes that the philosophy of AI is \"unlikely to have any more effect on the practice of AI research than philosophy of science generally has on the practice of science.\"\n\nRobert French (1990) makes the case that an interrogator can distinguish human and non-human interlocutors by posing questions that reveal the low-level (i.e., unconscious) processes of human cognition, as studied by cognitive science. Such questions reveal the precise details of the human embodiment of thought and can unmask a computer unless it experiences the world as humans do.\n\nNumerous other versions of the Turing test, including those expounded above, have been mooted through the years.\n\nA modification of the Turing test wherein the objective of one or more of the roles have been reversed between machines and humans is termed a reverse Turing test. An example is implied in the work of psychoanalyst Wilfred Bion, who was particularly fascinated by the \"storm\" that resulted from the encounter of one mind by another. In his 2000 book, among several other original points with regard to the Turing test, literary scholar Peter Swirski discussed in detail the idea of what he termed the Swirski test—essentially the reverse Turing test. He pointed out that it overcomes most if not all standard objections levelled at the standard version.\n\nCarrying this idea forward, R. D. Hinshelwood described the mind as a \"mind recognizing apparatus.\" The challenge would be for the computer to be able to determine if it were interacting with a human or another computer. This is an extension of the original question that Turing attempted to answer but would, perhaps, offer a high enough standard to define a machine that could \"think\" in a way that we typically define as characteristically human.\n\nCAPTCHA is a form of reverse Turing test. Before being allowed to perform some action on a website, the user is presented with alphanumerical characters in a distorted graphic image and asked to type them out. This is intended to prevent automated systems from being used to abuse the site. The rationale is that software sufficiently sophisticated to read and reproduce the distorted image accurately does not exist (or is not available to the average user), so any system able to do so is likely to be a human.\n\nSoftware that could reverse CAPTCHA with some accuracy by analysing patterns in the generating engine started being developed soon after the creation of CAPTCHA.\nIn 2013, researchers at Vicarious announced that they had developed a system to solve CAPTCHA challenges from Google, Yahoo!, and PayPal up to 90% of the time.\nIn 2014, Google engineers demonstrated a system that could defeat CAPTCHA challenges with 99.8% accuracy.\nIn 2015, Shuman Ghosemajumder, former click fraud czar of Google, stated that there were cybercriminal sites that would defeat CAPTCHA challenges for a fee, to enable various forms of fraud.\n\nAnother variation is described as the subject matter expert Turing test, where a machine's response cannot be distinguished from an expert in a given field. This is also known as a \"Feigenbaum test\" and was proposed by Edward Feigenbaum in a 2003 paper.\n\nThe \"Total Turing test\" variation of the Turing test, proposed by cognitive scientist Stevan Harnad, adds two further requirements to the traditional Turing test. The interrogator can also test the perceptual abilities of the subject (requiring computer vision) and the subject's ability to manipulate objects (requiring robotics).\n\nThe Minimum Intelligent Signal Test was proposed by Chris McKinstry as \"the maximum abstraction of the Turing test\", in which only binary responses (true/false or yes/no) are permitted, to focus only on the capacity for thought. It eliminates text chat problems like anthropomorphism bias, and does not require emulation of unintelligent human behaviour, allowing for systems that exceed human intelligence. The questions must each stand on their own, however, making it more like an IQ test than an interrogation. It is typically used to gather statistical data against which the performance of artificial intelligence programs may be measured.\n\nThe organisers of the Hutter Prize believe that compressing natural language text is a hard AI problem, equivalent to passing the Turing test.\n\nThe data compression test has some advantages over most versions and variations of a Turing test, including:\n\n\nThe main disadvantages of using data compression as a test are:\n\nA related approach to Hutter's prize which appeared much earlier in the late 1990s is the inclusion of compression problems in an extended Turing Test. or by tests which are completely derived from Kolmogorov complexity.\nOther related tests in this line are presented by Hernandez-Orallo and Dowe.\n\nAlgorithmic IQ, or AIQ for short, is an attempt to convert the theoretical Universal Intelligence Measure from Legg and Hutter (based on Solomonoff's inductive inference) into a working practical test of machine intelligence.\n\nTwo major advantages of some of these tests are their applicability to nonhuman intelligences and their absence of a requirement for human testers.\n\nThe Turing test inspired the Ebert test proposed in 2011 by film critic Roger Ebert which is a test whether a computer-based synthesised voice has sufficient skill in terms of intonations, inflections, timing and so forth, to make people laugh.\n\nTuring predicted that machines would eventually be able to pass the test; in fact, he estimated that by the year 2000, machines with around 100 MB of storage would be able to fool 30% of human judges in a five-minute test, and that people would no longer consider the phrase \"thinking machine\" contradictory. (In practice, from 2009–2012, the Loebner Prize chatterbot contestants only managed to fool a judge once, and that was only due to the human contestant pretending to be a chatbot.) He further predicted that machine learning would be an important part of building powerful machines, a claim considered plausible by contemporary researchers in artificial intelligence.\n\nIn a 2008 paper submitted to 19th Midwest Artificial Intelligence and Cognitive Science Conference, Dr. Shane T. Mueller predicted a modified Turing Test called a \"Cognitive Decathlon\" could be accomplished within 5 years.\n\nBy extrapolating an exponential growth of technology over several decades, futurist Ray Kurzweil predicted that Turing test-capable computers would be manufactured in the near future. In 1990, he set the year around 2020. By 2005, he had revised his estimate to 2029.\n\nThe Long Bet Project Bet Nr. 1 is a wager of $20,000 between Mitch Kapor (pessimist) and Ray Kurzweil (optimist) about whether a computer will pass a lengthy Turing Test by the year 2029. During the Long Now Turing Test, each of three Turing Test Judges will conduct online interviews of each of the four Turing Test Candidates (i.e., the Computer and the three Turing Test Human Foils) for two hours each for a total of eight hours of interviews. The bet specifies the conditions in some detail.\n\n1990 marked the fortieth anniversary of the first publication of Turing's \"Computing Machinery and Intelligence\" paper, and, thus, saw renewed interest in the test. Two significant events occurred in that year: The first was the Turing Colloquium, which was held at the University of Sussex in April, and brought together academics and researchers from a wide variety of disciplines to discuss the Turing Test in terms of its past, present, and future; the second was the formation of the annual Loebner Prize competition.\n\nBlay Whitby lists four major turning points in the history of the Turing Test – the publication of \"Computing Machinery and Intelligence\" in 1950, the announcement of Joseph Weizenbaum's ELIZA in 1966, Kenneth Colby's creation of PARRY, which was first described in 1972, and the Turing Colloquium in 1990.\n\nIn November 2005, the University of Surrey hosted an inaugural one-day meeting of artificial conversational entity developers,\nattended by winners of practical Turing Tests in the Loebner Prize: Robby Garner, Richard Wallace and Rollo Carpenter. Invited speakers included David Hamill, Hugh Loebner (sponsor of the Loebner Prize) and Huma Shah.\n\nIn parallel to the 2008 Loebner Prize held at the University of Reading,\nthe Society for the Study of Artificial Intelligence and the Simulation of Behaviour (AISB), hosted a one-day symposium to discuss the Turing Test, organised by John Barnden, Mark Bishop, Huma Shah and Kevin Warwick.\nThe Speakers included Royal Institution's Director Baroness Susan Greenfield, Selmer Bringsjord, Turing's biographer Andrew Hodges, and consciousness scientist Owen Holland. No agreement emerged for a canonical Turing Test, though Bringsjord expressed that a sizeable prize would result in the Turing Test being passed sooner.\n\nThroughout 2012, a number of major events took place to celebrate Turing's life and scientific impact. The Turing100 group supported these events and also, organised a special Turing test event in Bletchley Park on 23 June 2012 to celebrate the 100th anniversary of Turing's birth.\n\nThe Turing test has been used in a number of instances in popular media where machines are discussed. The 13th May 2017 comic strip of the popular comic Dilbert featured the Turing test.\n\n", "id": "21391751", "title": "Turing test"}
{"url": "https://en.wikipedia.org/wiki?curid=1732703", "text": "Negamax\n\nNegamax search is a variant form of minimax search that relies on the zero-sum property of a two-player game.\n\nThis algorithm relies on the fact that to simplify the implementation of the minimax algorithm. More precisely, the value of a position to player A in such a game is the negation of the value to player B. Thus, the player on move looks for a move that maximizes the negation of the value resulting from the move: this successor position must by definition have been valued by the opponent. The reasoning of the previous sentence works regardless of whether A or B is on move. This means that a single procedure can be used to value both positions. This is a coding simplification over minimax, which requires that A selects the move with the maximum-valued successor while B selects the move with the minimum-valued successor.\n\nIt should not be confused with negascout, an algorithm to compute the minimax or negamax value quickly by clever use of alpha-beta pruning discovered in the 1980s. Note that alpha-beta pruning is itself a way to compute the minimax or negamax value of a position quickly by avoiding the search of certain uninteresting positions.\n\nMost adversarial search engines are coded using some form of negamax search.\n\nNegaMax operates on the same game trees as those used with the minimax search algorithm. Each node and root node in the tree are game states (such as game board configuration) of a two player game. Transitions to child nodes represent moves available to a player who's about to play from a given node.\n\nThe negamax search objective is to find the node score value for the player who is playing at the root node. The pseudocode below shows the negamax base algorithm, with a configurable limit for the maximum search depth:\n\nThe root node inherits its score from one of its immediate child nodes.\nThe child node that ultimately sets the root node's best score also represents\nthe best move to play.\nAlthough the negamax function shown only returns the node's best score as \"bestValue\",\npractical negamax implementations may also retain and return both best move and best score for the root node.\nAssuming basic negamax, only the node's best score is essential with non-root nodes. And the node's best move isn't necessary to retain nor return for those nodes.\n\nWhat can be confusing is how the heuristic value of the current node is calculated. In this implementation, this value is always calculated from the point of view of player A, whose color value is one. In other words, higher heuristic values always represent situations more favorable for player A. This is the same behavior as the normal minimax algorithm. The heuristic value is not necessarily the same as a node's return value, \"bestValue\", due to value negation by negamax and the color parameter. The negamax node's return value is a heuristic score from the point of view of the node's current player.\n\nNegamax scores match minimax scores for nodes where player A is about to play, and\nwhere player A is the maximizing player in the minimax equivalent.\nNegamax always searches for the maximum value for all its nodes.\nHence for player B nodes, the minimax score is a negation of its negamax score.\nPlayer B is the minimizing player in the minimax equivalent.\n\nVariations in negamax implementations may omit the color parameter.\nIn this case, the heuristic evaluation function must return values from the point of view of the node's current player.\n\nAlgorithm optimizations for minimax are also equally applicable for Negamax. Alpha-beta pruning can decrease the number of nodes the negamax algorithm evaluates in a search tree in a manner similar with its use with the minimax algorithm.\n\nThe pseudocode for depth-limited negamax search with alpha-beta pruning follows:\n\nAlpha (α) and beta (β) represent lower and upper bounds for child node values at a given tree depth.\nNegamax sets the arguments α and β for the root node to the lowest and highest values possible.\nOther search algorithms, such as negascout and MTD-f,\nmay initialize α and β with alternate values to further improve tree search performance.\n\nWhen negamax encounters a child node value outside an alpha/beta range, the negamax search cuts off (pseudocode line 12 \"break\" statement) thereby pruning portions of the game tree from exploration.\nCut offs are implicit based on the node return value, \"bestValue\".\nA node value found within the range of its initial α and β is the node's exact (or true) value.\nThis value is identical to the result the negamax base algorithm would return,\nwithout cut offs and without any α and β bounds.\nIf a node return value is out of range, then the value represents an upper (if value ≤ α)\nor lower (if value ≥ β) bound for the node's exact value.\nAlpha-beta pruning eventually discards any value bound results. Such values do not contribute nor affect the negamax value at its root node.\n\nThis pseudocode shows the fail-soft variation of alpha-beta pruning.\nFail-soft never returns α or β directly as a node value.\nThus, a node value may be outside the initial α and β range bounds set with a negamax function call.\nIn contrast, fail-hard alpha-beta pruning always limits a node value in the range of α and β.\n\nThis implementation also shows optional move ordering prior to the foreach loop that evaluates child nodes. Move ordering is an optimization for alpha beta pruning that attempts to guess the most probable child nodes that yield the node's score. The algorithm searches those child nodes first. The result of good guesses is earlier and more frequent alpha/beta cut offs occur, thereby pruning additional game tree branches and remaining child nodes from the search tree.\n\nTransposition tables selectively memoize the values of nodes in the game tree. \"Transposition\" is a term reference that a given game board position can be reached in more than one way with differing game move sequences.\n\nWhen negamax searches the game tree, and encounters the same node multiple times, a transposition table can return a previously computed value of the node, skipping potentially lengthy and duplicate re-computation of the node's value. Negamax performance improves particularly for game trees with many paths that lead to a given node in common.\n\nThe pseudo code that adds transposition table functions to negamax with alpha/beta pruning is given as follows:\n\nAlpha/beta pruning and maximum search depth constraints in negamax can result in partial, inexact, and entirely skipped evaluation of nodes in a game tree. This complicates adding transposition table optimizations for negamax. It is insufficient to track only the node's \"bestValue\" in the table, because \"bestValue\" may not be the node's true value. The code therefore must preserve and restore the relationship of \"bestValue\" with alpha/beta parameters and the search depth for each transposition table entry.\n\nTransposition tables are typically lossy and will omit or overwrite previous values of certain game tree nodes in its tables. This is necessary since the number of nodes negamax visits often far exceeds the transposition table size. Lost or omitted table entries are non-critical and will not affect the negamax result. However, lost entries may require negamax to re-compute certain game tree node values more frequently, thus affecting performance.\n\n\n", "id": "1732703", "title": "Negamax"}
{"url": "https://en.wikipedia.org/wiki?curid=242748", "text": "Transposition table\n\nIn computer chess and other computer games, transposition tables are used to speed up the search of the game tree. Transposition tables are primarily useful in perfect-information games (where the entire state of the game is known to all players at all times). The usage of transposition tables is essentially memoization applied to the tree search and is a form of dynamic programming.\n\nGame-playing programs work by analyzing millions of positions that could arise in the next few moves of the game. Typically, these programs employ strategies resembling depth-first search, which means that they do not keep track of all the positions analyzed so far. In many games, it is possible to reach a given position in more than one way. These are called transpositions. In chess, for example, the sequence of moves \"1. d4 Nf6 2. c4 g6\" (see algebraic chess notation) has 4 possible transpositions, since either player may swap their move order. In general, after \"n\" moves, an upper limit on the possible transpositions is (\"n\"!). Although many of these are illegal move sequences, it is still likely that the program will end up analyzing the same position several times.\n\nTo avoid this problem, transposition tables are used. Such a table is a hash table of each of the positions analyzed so far up to a certain depth. On encountering a new position, the program checks the table to see whether the position has already been analyzed; this can be done quickly, in expected constant time. If so, the table contains the value that was previously assigned to this position; this value is used directly. If not, the value is computed, and the new position is entered into the hash table.\n\nThe number of positions searched by a computer often greatly exceeds the memory constraints of the system it runs on; thus not all positions can be stored. When the table fills up, less-used positions are removed to make room for new ones; this makes the transposition table a kind of cache.\n\nThe computation saved by a transposition table lookup is not just the evaluation of a single position. Instead, the evaluation of an entire subtree is avoided. Thus, transposition table entries for nodes at a shallower depth in the game tree are more valuable (since the size of the subtree rooted at such a node is larger) and are therefore given more importance when the table fills up and some entries must be discarded.\n\nThe hash table implementing the transposition table can have other uses than finding transpositions. In alpha–beta pruning, the search is fastest (in fact, optimal) when the child of a node corresponding to the best move is always considered first. Of course, there is no way of knowing the best move beforehand, but when iterative deepening is used, the move that was found to be the best in a shallower search is a good approximation. Therefore this move is tried first. For storing the best child of a node, the entry corresponding to that node in the transposition table is used.\n\nUse of a transposition table can lead to incorrect results if the graph-history interaction problem is not studiously avoided. This problem arises in certain games because the history of a position may be important. For example, in chess a player may not castle if the king or the rook to be castled with has moved during the course of the game. A common solution to this problem is to add the castling rights as part of the Zobrist hashing key. Another example is draw by repetition: given a position, it may not be possible to determine whether it has already occurred. A solution to the general problem is to store history information in each node of the transposition table, but this is inefficient and rarely done in practice.\n\n\n\n", "id": "242748", "title": "Transposition table"}
{"url": "https://en.wikipedia.org/wiki?curid=19589", "text": "Minimax\n\nMinimax (sometimes MinMax or MM) is a decision rule used in decision theory, game theory, statistics and philosophy for \"mini\"mizing the possible loss for a worst case (\"max\"imum loss) scenario. When dealing with gains, it is referred to as \"maximin\"—to maximize the minimum gain. Originally formulated for two-player zero-sum game theory, covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision-making in the presence of uncertainty.\n\nThe maximin value of a player is the largest value that the player can be sure to get without knowing the actions of the other players; equivalently, it is the smallest value the other players can force the player to receive when they know the player's action. Its formal definition is:\n\nWhere:\n\nCalculating the maximin value of a player is done in a worst-case approach: for each possible action of the player, we check all possible actions of the other players and determine the worst possible combination of actions—the one that gives player the smallest value. Then, we determine which action player can take in order to make sure that this smallest value is the largest possible.\n\nFor example, consider the following game for two players, where the first player (\"row player\") may choose any of three moves, labelled , , or , and the second player (\"column\" player) may choose either of two moves, or . The result of the combination of both moves is expressed in a payoff table:\n(where the first number in each cell is the pay-out of the row player and the second number is the pay-out of the column player).\n\nFor the sake of example, we consider only pure strategies. Check each player in turn:\n\nIf both players play their respective maximin strategies formula_9, the payoff vector is formula_10.\n\nThe minimax value of a player is the smallest value that the other players can force the player to receive, without knowing the player's actions; equivalently, it is the largest value the player can be sure to get when they \"know\" the actions of the other players. Its formal definition is:\n\nThe definition is very similar to that of the maximin value—only the order of the maximum and minimum operators is inverse. In the above example:\n\nFor every player , the maximin is at most the minimax:\nIntuitively, in maximin the maximization comes before the minimization, so player tries to maximize their value before knowing what the others will do; in minimax the maximization comes after the minimization, so player is in a much better position—they maximize their value knowing what the others did.\n\nAnother way to understand the \"notation\" is by reading from right to left: when we write\nthe initial set of outcomes formula_16 depends on both formula_17 and formula_18. We first \"marginalize away\" formula_17 from formula_16, by maximizing over formula_17 (for every possible value of formula_18) to yield a set of marginal outcomes formula_23, which depends only on formula_18. We then minimize over formula_18 over these outcomes. (Conversely for maximin.)\n\nAlthough it is always the case that formula_26 and formula_27, the payoff vector resulting from both players playing their minimax strategies, formula_28 in the case of formula_29 or formula_30 in the case of formula_31, cannot similarly be ranked against the payoff vector formula_10 resulting from both players playing their maximin strategy.\n\nIn zero-sum games, the minimax solution is the same as the Nash equilibrium.\n\nIn the context of zero-sum games, the minimax theorem is equivalent to:\n\nFor every two-person, zero-sum game with finitely many strategies, there exists a value V and a mixed strategy for each player, such that\n\nEquivalently, Player 1's strategy guarantees him a payoff of V regardless of Player 2's strategy, and similarly Player 2 can guarantee himself a payoff of −V. The name minimax arises because each player minimizes the maximum payoff possible for the other—since the game is zero-sum, he/she also minimizes his/their own maximum loss (i.e. maximize his/her minimum payoff).\nSee also example of a game without a value.\n\nThe following example of a zero-sum game, where A and B make simultaneous moves, illustrates \"minimax\" solutions. Suppose each player has three choices and consider the payoff matrix for A displayed on the right. Assume the payoff matrix for B is the same matrix with the signs reversed (i.e. if the choices are A1 and B1 then B pays 3 to A). Then, the minimax choice for A is A2 since the worst possible result is then having to pay 1, while the simple minimax choice for B is B2 since the worst possible result is then no payment. However, this solution is not stable, since if B believes A will choose A2 then B will choose B1 to gain 1; then if A believes B will choose B1 then A will choose A1 to gain 3; and then B will choose B2; and eventually both players will realize the difficulty of making a choice. So a more stable strategy is needed.\n\nSome choices are \"dominated\" by others and can be eliminated: A will not choose A3 since either A1 or A2 will produce a better result, no matter what B chooses; B will not choose B3 since some mixtures of B1 and B2 will produce a better result, no matter what A chooses.\n\nA can avoid having to make an expected payment of more than 1∕3 by choosing A1 with probability 1∕6 and A2 with probability 5∕6: The expected payoff for A would be 3 × (1∕6) − 1 × (5∕6) = −1∕3 in case B chose B1 and −2 × (1∕6) + 0 × (5∕6) = −1/3 in case B chose B2. Similarly, B can ensure an expected gain of at least 1/3, no matter what A chooses, by using a randomized strategy of choosing B1 with probability 1∕3 and B2 with probability 2∕3. These mixed minimax strategies are now stable and cannot be improved.\n\nFrequently, in game theory, maximin is distinct from minimax. Minimax is used in zero-sum games to denote minimizing the opponent's maximum payoff. In a zero-sum game, this is identical to minimizing one's own maximum loss, and to maximizing one's own minimum gain.\n\n\"Maximin\" is a term commonly used for non-zero-sum games to describe the strategy which maximizes one's own minimum payoff. In non-zero-sum games, this is not generally the same as minimizing the opponent's maximum gain, nor the same as the Nash equilibrium strategy.\n\nThe minimax values are very important in the theory of repeated games. One of the central theorems in this theory, the folk theorem, relies on the minimax values.\n\nIn combinatorial game theory, there is a minimax algorithm for game solutions.\n\nA simple version of the minimax \"algorithm\", stated below, deals with games such as tic-tac-toe, where each player can win, lose, or draw.\nIf player A \"can\" win in one move, his best move is that winning move.\nIf player B knows that one move will lead to the situation where player A \"can\" win in one move, while another move will lead to the situation where player A can, at best, draw, then player B's best move is the one leading to a draw.\nLate in the game, it's easy to see what the \"best\" move is.\nThe Minimax algorithm helps find the best move, by working backwards from the end of the game. At each step it assumes that player A is trying to maximize the chances of A winning, while on the next turn player B is trying to minimize the chances of A winning (i.e., to maximize B's own chances of winning).\n\nA minimax algorithm is a recursive algorithm for choosing the next move in an n-player game, usually a two-player game. A value is associated with each position or state of the game. This value is computed by means of a position evaluation function and it indicates how good it would be for a player to reach that position. The player then makes the move that maximizes the minimum value of the position resulting from the opponent's possible following moves. If it is A<nowiki>'s</nowiki> turn to move, A gives a value to each of his legal moves.\n\nA possible allocation method consists in assigning a certain win for A as +1 and for B as −1. This leads to combinatorial game theory as developed by John Horton Conway. An alternative is using a rule that if the result of a move is an immediate win for A it is assigned positive infinity and, if it is an immediate win for B, negative infinity. The value to A of any other move is the minimum of the values resulting from each of B<nowiki>'s</nowiki> possible replies. For this reason, A is called the \"maximizing player\" and B is called the \"minimizing player\", hence the name \"minimax algorithm\". The above algorithm will assign a value of positive or negative infinity to any position since the value of every position will be the value of some final winning or losing position. Often this is generally only possible at the very end of complicated games such as chess or go, since it is not computationally feasible to look ahead as far as the completion of the game, except towards the end, and instead positions are given finite values as estimates of the degree of belief that they will lead to a win for one player or another.\n\nThis can be extended if we can supply a heuristic evaluation function which gives values to non-final game states without considering all possible following complete sequences. We can then limit the minimax algorithm to look only at a certain number of moves ahead. This number is called the \"look-ahead\", measured in \"plies\". For example, the chess computer Deep Blue (the first one to beat a reigning world champion, Garry Kasparov at that time) looked ahead at least 12 plies, then applied a heuristic evaluation function.\n\nThe algorithm can be thought of as exploring the nodes of a \"game tree\". The \"effective branching factor\" of the tree is the average number of children of each node (i.e., the average number of legal moves in a position). The number of nodes to be explored usually increases exponentially with the number of plies (it is less than exponential if evaluating forced moves or repeated positions). The number of nodes to be explored for the analysis of a game is therefore approximately the branching factor raised to the power of the number of plies. It is therefore impractical to completely analyze games such as chess using the minimax algorithm.\n\nThe performance of the naïve minimax algorithm may be improved dramatically, without affecting the result, by the use of alpha-beta pruning.\nOther heuristic pruning methods can also be used, but not all of them are guaranteed to give the same result as the un-pruned search.\n\nA naive minimax algorithm may be trivially modified to additionally return an entire Principal Variation along with a minimax score.\n\nThe pseudocode for the depth limited minimax algorithm is given below.\n\nThe minimax function returns a heuristic value for leaf nodes (terminal nodes and nodes at the maximum search depth).\nNon leaf nodes inherit their value, \"bestValue\", from a descendant leaf node.\nThe heuristic value is a score measuring the favorability of the node for the maximizing player.\nHence nodes resulting in a favorable outcome, such as a win, for the maximizing player have higher scores than nodes more favorable for the minimizing player.\nThe heuristic value for terminal (game ending) leaf nodes are scores corresponding to win, loss, or draw, for the maximizing player.\nFor non terminal leaf nodes at the maximum search depth, an evaluation function estimates a heuristic value for the node.\nThe quality of this estimate and the search depth determine the quality and accuracy of the final minimax result.\n\nMinimax treats the two players (the maximizing player and the minimizing player) separately in its code. Based on the observation that formula_33, minimax may often be simplified into the negamax algorithm.\n\nSuppose the game being played only has a maximum of two possible moves per player each turn. The algorithm generates the tree on the right, where the circles represent the moves of the player running the algorithm (\"maximizing player\"), and squares represent the moves of the opponent (\"minimizing player\"). Because of the limitation of computation resources, as explained above, the tree is limited to a \"look-ahead\" of 4 moves.\n\nThe algorithm evaluates each \"leaf node\" using a heuristic evaluation function, obtaining the values shown. The moves where the \"maximizing player\" wins are assigned with positive infinity, while the moves that lead to a win of the \"minimizing player\" are assigned with negative infinity. At level 3, the algorithm will choose, for each node, the smallest of the \"child node\" values, and assign it to that same node (e.g. the node on the left will choose the minimum between \"10\" and \"+∞\", therefore assigning the value \"10\" to itself). The next step, in level 2, consists of choosing for each node the largest of the \"child node\" values. Once again, the values are assigned to each \"parent node\". The algorithm continues evaluating the maximum and minimum values of the child nodes alternately until it reaches the \"root node\", where it chooses the move with the largest value (represented in the figure with a blue arrow). This is the move that the player should make in order to \"minimize\" the \"maximum\" possible loss.\n\nMinimax theory has been extended to decisions where there is no other player, but where the consequences of decisions depend on unknown facts. For example, deciding to prospect for minerals entails a cost which will be wasted if the minerals are not present, but will bring major rewards if they are. One approach is to treat this as a game against \"nature\" (see move by nature), and using a similar mindset as Murphy's law or resistentialism, take an approach which minimizes the maximum expected loss, using the same techniques as in the two-person zero-sum games.\n\nIn addition, expectiminimax trees have been developed, for two-player games in which chance (for example, dice) is a factor.\n\nIn classical statistical decision theory, we have an estimator formula_34 that is used to estimate a parameter formula_35. We also assume a risk function formula_36, usually specified as the integral of a loss function. In this framework, formula_37 is called minimax if it satisfies\n\nAn alternative criterion in the decision theoretic framework is the Bayes estimator in the presence of a prior distribution formula_39. An estimator is Bayes if it minimizes the \"average\" risk\n\nA key feature of minimax decision making is being non-probabilistic: in contrast to decisions using expected value or expected utility, it makes no assumptions about the probabilities of various outcomes, just scenario analysis of what the possible outcomes are. It is thus robust to changes in the assumptions, as these other decision techniques are not. Various extensions of this non-probabilistic approach exist, notably minimax regret and Info-gap decision theory.\n\nFurther, minimax only requires ordinal measurement (that outcomes be compared and ranked), not \"interval\" measurements (that outcomes include \"how much better or worse\"), and returns ordinal data, using only the modeled outcomes: the conclusion of a minimax analysis is: \"this strategy is minimax, as the worst case is (outcome), which is less bad than any other strategy\". Compare to expected value analysis, whose conclusion is of the form: \"this strategy yields E(\"X\")=\"n.\"\" Minimax thus can be used on ordinal data, and can be more transparent.\n\nIn philosophy, the term \"maximin\" is often used in the context of John Rawls's \"A Theory of Justice,\" where he refers to it (Rawls (1971, p. 152)) in the context of The Difference Principle.\nRawls defined this principle as the rule which states that social and economic inequalities should be arranged so that \"they are to be of the greatest benefit to the least-advantaged members of society\".\n\n", "id": "19589", "title": "Minimax"}
{"url": "https://en.wikipedia.org/wiki?curid=2292305", "text": "Principal variation search\n\nPrincipal variation search (sometimes equated with the practically identical NegaScout) is a negamax algorithm that can be faster than alpha-beta pruning. Like alpha-beta pruning, NegaScout is a directional search algorithm for computing the minimax value of a node in a tree. It dominates alpha-beta pruning in the sense that it will never examine a node that can be pruned by alpha-beta; however, it relies on accurate node ordering to capitalize on this advantage.\n\nNegaScout works best when there is a good move ordering. In practice, the move ordering is often determined by previous shallower searches. It produces more cutoffs than alpha-beta by assuming that the first explored node is the best. In other words, it supposes the first node is in the principal variation. Then, it can check whether that is true by searching the remaining nodes with a null window (also known as a scout window; when alpha and beta are equal), which is faster than searching with the regular alpha-beta window. If the proof fails, then the first node was not in the principal variation, and the search continues as normal alpha-beta. Hence, NegaScout works best when the move ordering is good. With a random move ordering, NegaScout will take more time than regular alpha-beta; although it will not explore any nodes alpha-beta did not, it will have to re-search many nodes.\n\nIn chess engines, NegaScout has typically given a 10 percent performance increase.\n\nAlexander Reinefeld invented NegaScout several decades after the invention of alpha-beta pruning. He gives a proof of correctness of NegaScout in his book.\n\nAnother search algorithm called SSS* can theoretically result in fewer nodes searched. However, its original formulation has practical issues (in particular, it relies heavily on an OPEN list for storage) and nowadays most chess engines still use a form of NegaScout in their search. Most chess engines use a transposition table in which the relevant part of the search tree is stored. This part of the tree has the same size as SSS*'s OPEN list would have. A reformulation called MT-SSS* allowed it to be implemented as a series of null window calls to Alpha-Beta (or NegaScout) that use a transposition table, and direct comparisons using game playing programs could be made. It did not outperform NegaScout in practice. Yet another search algorithm, which does tend to do better than NegaScout in practice, is the best-first algorithm called MTD-f, although neither algorithm dominates the other. There are trees in which NegaScout searches fewer nodes than SSS* or MTD-f and vice versa.\n\nNegaScout takes after SCOUT, invented by Judea Pearl in 1980, which was the first algorithm to outperform alpha-beta and to be proven asymptotically optimal. Null windows, with β=α+1 in a negamax setting, were invented independently by J.P. Fishburn and used in an algorithm similar to SCOUT in an appendix to his Ph.D. thesis, in a parallel alpha-beta algorithm, and on the last subtree of a search tree root node.\n\n \"(* Negascout is also termed Principal Variation Search - hence - pvs *)\"\n\n", "id": "2292305", "title": "Principal variation search"}
{"url": "https://en.wikipedia.org/wiki?curid=159513", "text": "Evaluation function\n\nAn evaluation function, also known as a heuristic evaluation function or static evaluation function, is a function used by game-playing programs to estimate the value or goodness of a position in the minimax and related algorithms. The evaluation function is typically designed to prioritize speed over accuracy; the function looks only at the current position and does not explore possible moves (therefore static).\n\nOne popular strategy for constructing evaluation functions is as a weighted sum of various factors that are thought to influence the value of a position. For instance, an evaluation function for chess might take the form \nSuch as\n\nin which: \n\n\nEvaluation functions in Go take into account both territory controlled, influence of stones, number of prisoners and life and death of groups on the board.\n\n\n", "id": "159513", "title": "Evaluation function"}
{"url": "https://en.wikipedia.org/wiki?curid=1963880", "text": "Zobrist hashing\n\nZobrist hashing (also referred to as Zobrist keys or Zobrist signatures ) is a hash function construction used in computer programs that play abstract board games, such as chess and Go, to implement transposition tables, a special kind of hash table that is indexed by a board position and used to avoid analyzing the same position more than once. Zobrist hashing is named for its inventor, Albert Lindsey Zobrist. It has also been applied as a method for recognizing substitutional alloy configurations in simulations of crystalline materials.\n\nZobrist hashing starts by randomly generating bitstrings for each possible element of a board game, i.e. for each combination of a piece and a position (in the game of chess, that's 12 pieces × 64 board positions, or 14 x 64 if a king that may still castle and a pawn that may capture \"en passant\" are treated separately). Now any board configuration can be broken up into independent piece/position components, which are mapped to the random bitstrings generated earlier. The final Zobrist hash is computed by combining those bitstrings using bitwise XOR. Example pseudocode for the game of chess:\n\nIf the bitstrings are long enough, different board positions will almost certainly hash to different values; however longer bitstrings require proportionally more computer resources to manipulate. The most commonly used bitstring (key) length is 64 bits. Many game engines store only the hash values in the transposition table, omitting the position information itself entirely to reduce memory usage, and assuming that hash collisions will not occur, or will not greatly influence the results of the table if they do.\n\nZobrist hashing is the first known instance of tabulation hashing. The result is a 3-wise independent hash family. In particular, it is strongly universal.\n\nAs an example, in chess, each of the 64 squares can at any time be empty, or contain one of the 6 game pieces, which are either black or white. That is, each square can be in one of 1 + 6 x 2 = 13 possible states at any time. Thus one needs to generate at most 13 x 64 = 832 random bitstrings. Given a position, one obtains its Zobrist hash by finding out which pieces are on which squares, and combining the relevant bitstrings together.\n\nRather than computing the hash for the entire board every time, as the pseudocode above does, the hash value of a board can be updated simply by XORing out the bitstring(s) for positions that have changed, and XORing in the bitstrings for the new positions. For instance, if a pawn on a chessboard square is replaced by a rook from another square, the resulting position would be produced by XORing the existing hash with the bitstrings for:\nThis makes Zobrist hashing very efficient for traversing a game tree.\n\nIn computer go, this technique is also used for superko detection.\n\nThe same method has been used to recognize substitutional alloy configurations during Monte Carlo simulations in order to prevent wasting computational effort on states that have already been calculated.\n\n", "id": "1963880", "title": "Zobrist hashing"}
{"url": "https://en.wikipedia.org/wiki?curid=904345", "text": "International Computer Games Association\n\nThe International Computer Games Association (ICGA) was founded as the International Computer Chess Association (ICCA) in 1977 by computer chess programmers to organise championship events for computer programs and to facilitate the sharing of technical knowledge via the ICCA Journal.\n\nRenamed the 'ICGA' in 2002, the association now more broadly fosters the computer games and game artificial intelligence community by organizing the Computer Olympiad, the World Computer Chess Championship, and the International Conference on Computers and Games. The ICGA also publishes a quarterly journal, the ICGA Journal, and maintains relationships with Computer Science, Commercial, and Game organisations throughout the world. The ICGA is led by David Levy.\n\n", "id": "904345", "title": "International Computer Games Association"}
{"url": "https://en.wikipedia.org/wiki?curid=253556", "text": "Horizon effect\n\nThe horizon effect, also known as the horizon problem, is a problem in artificial intelligence where, in many games, the number of possible states or positions is immense and computers can only feasibly search a small portion of it, typically a few plies down the game tree. Thus, for a computer searching only five plies, there is a possibility that it will make a detrimental move, but the effect is not visible because the computer does not search to the depth of the error (\"i.e.\" beyond its \"horizon\").\n\nWhen evaluating a large game tree using techniques such as minimax with alpha-beta pruning, search depth is limited for feasibility reasons. However, evaluating a partial tree may give a misleading result. When a significant change exists just over the horizon of the search depth, the computational device falls victim to the horizon effect.\n\nHans Berliner named this phenomenon in 1973, \"The Horizon Effect\", which he and other researchers had observed. He split the effect into two, the Negative Horizon Effect is when it \"results in creating diversions which ineffectively delay an unavoidable consequence or make an unachievable one appear achievable.\" For the \"largely overlooked\" Positive Horizon Effect, \"the program grabs much too soon at a consequence that can be imposed on an opponent at leisure, frequently in a more effective form.\"\n\nThe horizon effect can be mitigated by extending the search algorithm with a quiescence search. This gives the search algorithm ability to look beyond its horizon for a certain class of moves of major importance to the game state, such as captures in chess.\n\nRewriting the evaluation function for leaf nodes and/or analyzing more nodes will solve many horizon effect problems.\n\nFor example, in chess, assume a situation where the computer only searches the game tree to six plies, and from the current position, it determines that the queen is lost in the sixth ply, and suppose there is a move in the search depth where it may sacrifice a rook, and the loss of the queen is pushed to the eighth ply. This is, of course, a worse move than sacrificing the queen, because it leads to losing both a queen and a rook. However, because the loss of the queen was pushed over the horizon of search, it is not discovered and evaluated by the search. Losing the rook seems to be better than losing the queen, so the sacrifice is returned as the best option, while delaying the sacrifice of the queen has in fact additionally weakened the computer's position.\n\n\n", "id": "253556", "title": "Horizon effect"}
{"url": "https://en.wikipedia.org/wiki?curid=173889", "text": "Killer heuristic\n\nIn competitive two-player games, the killer heuristic is a technique for improving the efficiency of alpha-beta pruning, which in turn improves the efficiency of the minimax algorithm. This algorithm has an exponential search time to find the optimal next move, so general methods for speeding it up are very useful.\n\nAlpha-beta pruning works best when the best moves are considered first. This is because the best moves are the ones most likely to produce a \"cutoff\", a condition where the game playing program knows that the position it is considering could not possibly have resulted from best play by both sides and so need not be considered further. I.e. the game playing program will always make its best available move for each position. It only needs to consider the other player's possible responses to that best move, and can skip evaluation of responses to (worse) moves it will not make.\n\nThe killer heuristic attempts to produce a cutoff by assuming that a move that produced a cutoff in another branch of the game tree at the same depth is likely to produce a cutoff in the present position, that is to say that a move that was a very good move from a different (but possibly similar) position might also be a good move in the present position. By trying the \"killer move\" before other moves, a game playing program can often produce an early cutoff, saving itself the effort of considering or even generating all legal moves from a position.\n\nIn practical implementation, game playing programs frequently keep track of two killer moves for each depth of the game tree (greater than depth of 1) and see if either of these moves, if legal, produces a cutoff before the program generates and considers the rest of the possible moves. If a non-killer move produces a cutoff, it replaces one of the two killer moves at its depth. This idea can be generalized into a set of refutation tables.\n\nA generalization of the killer heuristic is the \"history heuristic\". The history heuristic can be implemented as a table that is indexed by some characteristic of the move, for example \"from\" and \"to\" squares or piece moving and the \"to\" square. When there is a cutoff, the appropriate entry in the table is incremented, such as by adding \"d²\" or \"2\" where \"d\" is the current search depth. This information is used when ordering moves.\n\n", "id": "173889", "title": "Killer heuristic"}
{"url": "https://en.wikipedia.org/wiki?curid=2426057", "text": "Pathfinding\n\nPathfinding or pathing is the plotting, by a computer application, of the shortest route between two points. It is a more practical variant on solving mazes. This field of research is based heavily on Dijkstra's algorithm for finding a shortest path on a weighted graph.\n\nPathfinding is closely related to the shortest path problem, within graph theory, which examines how to identify the path that best meets some criteria (shortest, cheapest, fastest, etc) between two points in a large network.\n\nAt its core, a pathfinding method searches a graph by starting at one vertex and exploring adjacent nodes until the destination node is reached, generally with the intent of finding the cheapest route. Although graph searching methods such as a breadth-first search would find a route if given enough time, other methods, which \"explore\" the graph, would tend to reach the destination sooner. An analogy would be a person walking across a room; rather than examining every possible route in advance, the person would generally walk in the direction of the destination and only deviate from the path to avoid an obstruction, and make deviations as minor as possible.\n\nTwo primary problems of pathfinding are (1) to find a path between two nodes in a graph; and (2) the shortest path problem—to find the optimal shortest path. Basic algorithms such as breadth-first and depth-first search address the first problem by exhausting all possibilities; starting from the given node, they iterate over all potential paths until they reach the destination node. These algorithms run in formula_1, or linear time, where V is the number of vertices, and E is the number of edges between vertices.\n\nThe more complicated problem is finding the optimal path. The exhaustive approach in this case is known as the Bellman–Ford algorithm, which yields a time complexity of formula_2, or quadratic time. However, it is not necessary to examine all possible paths to find the optimal one. Algorithms such as A* and Dijkstra's algorithm strategically eliminate paths, either through heuristics or through dynamic programming. By eliminating impossible paths, these algorithms can achieve time complexities as low as formula_3.\n\nThe above algorithms are among the best general algorithms which operate on a graph without preprocessing. However, in practical travel-routing systems, even better time complexities can be attained by algorithms which can pre-process the graph to attain better performance. One such algorithm is contraction hierarchies.\n\nA common example of a graph-based pathfinding algorithm is Dijkstra's algorithm. This algorithm begins with a start node and an \"open set\" of candidate nodes. At each step, the node in the open set with the lowest distance from the start is examined. The node is marked \"closed\", and all nodes adjacent to it are added to the open set if they have not already been examined. This process repeats until a path to the destination has been found. Since the lowest distance nodes are examined first, the first time the destination is found, the path to it will be the shortest path.\n\nDijkstra's algorithm fails if there is a negative edge weight. In the hypothetical situation where Nodes A, B, and C form a connected undirected graph with edges AB = 3, AC = 4, and BC = −2, the optimal path from A to C costs 1, and the optimal path from A to B costs 2. Dijkstra's Algorithm starting from A will first examine B, as that is the closest. It will assign a cost of 3 to it, and mark it closed, meaning that its cost will never be reevaluated. Therefore, Dijkstra's cannot evaluate negative edge weights. However, since for many practical purposes there will never be a negative edgeweight, Dijkstra's algorithm is largely suitable for the purpose of pathfinding.\n\nA* is a variant of Dijkstra's algorithm commonly used in games. A* assigns a weight to each open node equal to the weight of the edge to that node plus the approximate distance between that node and the finish. This approximate distance is found by the heuristic, and represents a minimum possible distance between that node and the end. This allows it to eliminate longer paths once an initial path is found. If there is a path of length x between the start and finish, and the minimum distance between a node and the finish is greater than x, that node need not be examined.\n\nA* uses this heuristic to improve on the behavior relative to Dijkstra's algorithm. When the heuristic evaluates to zero, A* is equivalent to Dijkstra's algorithm. As the heuristic estimate increases and gets closer to the true distance, A* continues to find optimal paths, but runs faster (by virtue of examining fewer nodes). When the value of the heuristic is exactly the true distance, A* examines the fewest nodes. (However, it is generally impractical to write a heuristic function that always computes the true distance.) As the value of the heuristic increases, A* examines fewer nodes but no longer guarantees an optimal path. In many applications (such as video games) this is acceptable and even desirable, in order to keep the algorithm running quickly.\n\nThis is a fairly simple and easy-to-understand pathfinding algorithm for tile-based maps. To start off, you have a map, a start coordinate and a destination coordinate. The map will look like this, X being walls, S being the start, O being the finish and _ being open spaces, the numbers along the top and right edges are the column and row numbers:\n\nFirst, create a list of coordinates, which we will use as a queue. The queue will be initialized with one coordinate, the end coordinate. Each coordinate will also have a counter variable attached (the purpose of this will soon become evident). Thus, the queue starts off as ((3,8,0)).\n\nThen, go through every element in the queue, including elements added to the end over the course of the algorithm, and to each element, do the following:\n\n\nThus, after turn 1, the list of elements is this: ((3,8,0),(2,8,1),(4,8,1))\n\nNow, map the counters onto the map, getting this:\n\nNow, start at S (7) and go to the nearby cell with the lowest number (unchecked cells cannot be moved to). The path traced is (1,3,7) -> (1,4,6) -> (1,5,5) -> (1,6,4) -> (1,7,3) -> (1,8,2) -> (2,8,1) -> (3,8,0). In the event that two numbers are equally low (for example, if S was at (2,5)), pick a random direction – the lengths are the same. The algorithm is now complete.\n\nChris Crawford in 1982 described how he \"expended a great deal of time\" trying to solve a problem with pathfinding in \"Tanktics\", in which computer tanks became trapped on land within U-shaped lakes. \"After much wasted effort I discovered a better solution: delete U-shaped lakes from the map\", he said.\n\n\nMulti-agent pathfinding is to find the paths for multiple agents from their current locations to their target locations without colliding with each other, while at the same time optimizing a cost function, such as the sum of the path lengths of all agents. It is a generalization of pathfinding. Many multi-agent pathfinding algorithms are generalized from A*, or based on reduction to other well studied problems such as integer linear programming. However, such algorithms are typically incomplete; in other words, not proven to produce a solution within polynomial time. A different category of algorithms sacrifice optimality for performance by either making use of known navigation patterns (such as traffic flow) or the topology of the problem space.\n\n\n", "id": "2426057", "title": "Pathfinding"}
{"url": "https://en.wikipedia.org/wiki?curid=3509077", "text": "Ply (game theory)\n\nIn two-player sequential games, a ply refers to one turn taken by one of the players. The word is used to clarify what is meant when one might otherwise say \"turn\". \n\nThe word \"turn\" can be a problem since it means different things in different traditions. For example, in standard chess terminology, one \"move\" consists of a turn by each player; therefore a ply in chess is a \"half-move\". Thus, after 20 moves in a chess game, 40 plies have been completed—20 by white and 20 by black. In the game of Go, by contrast, a ply is the normal unit of counting moves; so for example to say that a game is \"250 moves long\" is to imply 250 plies.\n\nThe word \"ply\" used as a synonym for \"layer\" goes back to the 15th century. Arthur Samuel used the term in its game-theoretic sense in his seminal paper on machine learning in checkers in 1959. \n\nIn computing, the concept of ply is important because one ply corresponds to one level of the game tree. The Deep Blue chess computer which defeated Kasparov in 1997 would typically search to a depth of between six and sixteen plies to a maximum of forty plies in some situations.\n\n", "id": "3509077", "title": "Ply (game theory)"}
{"url": "https://en.wikipedia.org/wiki?curid=8539064", "text": "Alexander Brudno\n\nAlexander L'vovich Brudno () (January 10, 1918 – December 1, 2009) was a Russian computer scientist, best known for fully describing the alpha-beta pruning algorithm. From 1991 until his death he lived in Israel.\n\nBrudno developed the \"mathematics/machine interface\" for the M-2 computer constructed in 1952 at the Krzhizhanovskii laboratory of the Institute of Energy of the Russian Academy of Sciences in the Soviet Union. He was a great friend of Alexander Kronrod.\n\nBrudno's work on alpha-beta pruning was published in 1963 in Russian and English.\n\nThe algorithm was used in computer chess program written by Vladimir Arlazarov and others at the Institute for Theoretical and Experimental Physics (ITEF or ITEP). According to Monty Newborn and the Computer History Museum, the algorithm was used later in Kaissa the world computer chess champion in 1974.\n\nIn 1980, Brudno became a founder and scientific director of the first Russian school for young programmers . He was the scientific director of the first Russian programming Olympiads for the students, and published a book of problems from these competitions.\n\nIn 1959 Brudno and Alexander Kronrod organized seminar devoted to the presentation of different works in areas of system programming, programming of games (including chess), and artificial intelligence. Many well known results were presented and discussed at this seminar, including: Gauss-Kronrod quadrature formula, AVL trees, computer chess, Pattern recognition (M. Bongard , P. Kunin and others), Method of Four Russians and others.\n\nIn 1963 Brudno published his work on alpha-beta pruning. The key intuition was that a player could avoid evaluating certain moves that were clearly inferior to one already considered.\n\nIn the following game tree vertices represent positions and edges represent moves. The position’s valuations are in the brackets\n\nAssume that “whites” should make a move in position A and then “blacks” could make their own move. ‘Whites” should find better strategy to maximize their win (Minimax strategy).\n\nAfter evaluating AB and CD, it is easy to see that the best move for “whites’ is AB and it is not necessary to check move CE as the overall value of vertex C will be no better than 1. This is unchanged if B, D, E are trees and not leaves. Such considerations, taken on all levels of the game tree, are known as alpha-better pruning. It has been used in different game programming applications even before Brudno’s work; Brudno’s contribution was the formalization of the algorithm and analysis of its speedup.\n\nIn 1959 Brudno's work on alpha-beta pruning was motivated by an analysis of the card game where two players are dealt n cards each, with values 1…2n, and one player is chosen to go first. Each player puts down one card, with the larger card taking the trick, and the taker going first in the next move. The goal is to determine an optimal strategy given the players initial hand and move order. The analysis of this card game was used in the seminar to refine the understanding of recursion and structured programming, and development of updatable dictionaries.\n\nAllen Newell and Herbert A. Simon who used what John McCarthy calls an \"approximation\" in 1958 wrote that alpha-beta \"appears to have been reinvented a number of times\". Arthur Samuel had an early version and Richards, Hart, Levine and/or Edwards found alpha-beta independently in the United States. McCarthy proposed similar ideas during the Dartmouth Conference in 1956 and suggested it to a group of his students including Alan Kotok at MIT in 1961. Donald Knuth and Ronald W. Moore refined the algorithm in 1975 and it continued to be advanced.\n\n\n", "id": "8539064", "title": "Alexander Brudno"}
{"url": "https://en.wikipedia.org/wiki?curid=1074656", "text": "Logistello\n\nLogistello is a computer program that plays the game Othello, also known as Reversi. Logistello was written by Michael Buro and is regarded as a strong player, having beaten the human world champion Takeshi Murakami six games to none in 1997 — the best Othello programs are now much stronger than any human player. Logistello's evaluation function is based on disc patterns and features over a million numerical parameters which were tuned using linear regression.\n", "id": "1074656", "title": "Logistello"}
{"url": "https://en.wikipedia.org/wiki?curid=6476399", "text": "Game Description Language\n\nGame Description Language, or GDL, is a language designed by Michael Genesereth as part of the General Game Playing Project at Stanford University, California. GDL describes the state of a game as a series of facts, and the game mechanics as logical rules.\n\nQuoted in an article in New Scientist, Genesereth pointed out that although Deep Blue is able to play chess at a grandmaster level, it is incapable of playing checkers at all because it is a specialized game player. Both chess and checkers can be described in GDL. This enables general game players to be built that can play both of these games and any other game that can be described using GDL.\n\nGDL is a variant of Datalog, and the syntax is largely the same. It is usually given in prefix notation. Variables begin with \"codice_1\".\n\nThe following is the list of keywords in GDL, along with brief descriptions of their functions:\n\n\n\n\n\n\n\n\n\n\nA game description in GDL provides complete rules for each of the following elements of a game.\n\nFacts that define the roles in a game. The following example is from a GDL description of the two-player game Tic-tac-toe:\nRules that entail all facts about the initial game state. An example is:\nRules that describe each move by the conditions on the current position under which it can be taken by a player. An example is:\nRules that describe all facts about the next state relative to the current state and the moves taken by the players. An example is:\nRules that describe the conditions under which the current state is a terminal one. An example is:\nThe goal values for each player in a terminal state. An example is:\nWith GDL one can describe finite games with an arbitrary numbers of players. However, GDL can not describe games which contain an element of chance (for example, rolling of dice) or games where players have incomplete information about the current state of the game (for example, in many card games the opponents' cards are not visible). GDL-II, the Game Description Language for Incomplete Information games, extends GDL by two keywords that allow for the description of elements of chance and incomplete information:\n\n\n\nThe following is an example from a GDL-II description of the card game Texas hold 'em:\n\n", "id": "6476399", "title": "Game Description Language"}
{"url": "https://en.wikipedia.org/wiki?curid=7408685", "text": "General game playing\n\nGeneral game playing (GGP) is the design of artificial intelligence programs to be able to play more than one game successfully. For many games like chess, computers are programmed to play these games using a specially designed algorithm, which cannot be transferred to another context. For example, a chess-playing computer program cannot play checkers. A general game playing system, if well designed, would be able to help in other areas, such as in providing intelligence for search and rescue missions.\n\n\"General Game Playing\" is a project of the Stanford Logic Group of Stanford University, California, which aims to create a platform for general game playing. It is the most well-known effort at standardizing GGP AI, and generally seen as the standard for GGP systems. The games are defined by sets of rules represented in the Game Description Language. In order to play the games, players interact with a game hosting server that monitors moves for legality and keeps players informed of state changes.\n\nSince 2005, there have been annual General Game Playing competitions at the AAAI Conference. The competition judges competitor AI's abilities to play a variety of different games, by recording their performance on each individual game. In the first stage of the competition, entrants are judged on their ability to perform legal moves, gain the upper hand, and complete games faster. In the following runoff round, the AIs face off against each other in increasingly complex games. The AI that wins the most games at this stage wins the competition, and until 2013 its creator used to win a $10,000 prize. So far, the following programs were victorious:\n\n\nThere are other general game playing systems, which use their own languages for defining the game rules. In 1992, Barney Pell defined the concept of Meta-Game Playing, and developed the \"MetaGame\" system. This was the first program to automatically generate game rules of chess-like games, and one of the earliest programs to use automated game generation. Pell then developed the system \"Metagamer\". This system was able to play a number of chess-like games, given game rules definition in a special language called Game Description Language, without any human interaction once the games were generated.\n\nIn 1998, the commercial system Zillions of Games was developed by Jeff Mallett and Mark Lefler. The system used a LISP-like language to define the game rules. Zillions of Games derived the evaluation function automatically from the game rules based on piece mobility, board structure and game goals. It also employed usual algorithms as found in computer chess systems: alpha-beta pruning with move ordering, transposition tables, etc. The package was extended in 2007 by the addition of the Axiom plug-in, an alternate metagame engine that incorporates a complete Forth-based programming language.\n\nOther general game playing software include:\n\n\n\nThe General Video Game AI Competition (GVGAI) has been running since 2014. In this competition, two-dimensional video games similar to (and sometimes based on) 80s-era arcade and console games are used instead of the board games used in the GGP competition.\n\nSince GGP AI must be designed to play multiple games, its design cannot rely on algorithms created specifically for certain games. Instead, the AI must be designed using algorithms whose methods can be applied to a wide range of games. The AI must also be an ongoing process, that can adapt to its current state rather than the output of previous states. For this reason, open loop techniques are often most effective.\n\nA popular method for developing GGP AI is the Monte Carlo tree search (MCTS) algorithm. Often used together with the UCT method (\"Upper Confidence Bound applied to Trees\"), variations of MCTS have been proposed to better play certain games, as well as to make it compatible with video game playing. Another variation of tree search algorithms used is the Directed Breadth First Search (DBS), in which a child node to the current state is created for each available action, and visits each child ordered by highest average reward, until either the game ends or runs out of time. In each tree search method, the AI simulates potential actions and ranks each based on the average highest reward of each path, in terms of points earned.\n\nIn order to interact with games, algorithms must operate under the assumption that games all share common characteristics. In the book \"Half-Real: Video Games Between Real Worlds and Fictional Worlds\", Jesper Juul gives the following definition of games: Games are based on rules, they have variable outcomes, different outcomes give different values, player effort influences outcomes, the player is attached to the outcomes, and the game has negotiable consequences. Using these assumptions, game playing AI can be created by quantifying the player input, the game outcomes, and how the various rules apply, and using algorithms to compute the most favorable path.\n\n\n\n", "id": "7408685", "title": "General game playing"}
{"url": "https://en.wikipedia.org/wiki?curid=7249174", "text": "Blondie24\n\nBlondie24 is an artificial intelligence checkers-playing computer program named after the screen name used by a team led by David B. Fogel. The purpose was to determine the effectiveness of an artificial intelligence checkers-playing computer program.\n\nThe screen name was used on The Zone, an internet boardgaming site in 1999. During this time, Blondie24 played against some 165 human opponents and was shown to achieve a rating of 2048, or better than 99.61% of the playing population of that web site.\n\nThe design of Blondie24 is based on a minimax algorithm of the checkers game tree in which the evaluation function is a deep learning convolutional artificial neural network. The neural net receives as input a vector representation of the checkerboard positions and returns a single value which is passed on to the minimax algorithm.\n\nThe weights of the neural network were obtained by an evolutionary algorithm (an approach now called neuroevolution). In this case, a population of Blondie24-like programs played each other in checkers, and those were eliminated that performed relatively poorly. Performance was measured by a points system: Each program earned one point for a win, none for a draw, and two points were subtracted for a loss. Points were earned for each neural network after a multiple of games; the neural networks did not know which individual games were won, lost, or drawn. After the poor programs were eliminated, the process was repeated with a new population derived from the winners. In this way, the result was an evolutionary process that selected programs that played better checkers games.\n\nThe significance of the Blondie24 program is that its ability to play checkers did not rely on any human expertise of the game. Rather, it came solely from the total points earned by each player and the evolutionary process itself. \n\nDavid Fogel, along with his colleague Kumar Chellapilla, documented their experiment in several publications. Fogel also authored a book on the development of Blondie24, and the experiences he and his team had while running Blondie24 in on-line checkers games, and eventually in obtaining a victory against a dumbed-down version of Chinook.\n\n\n", "id": "7249174", "title": "Blondie24"}
{"url": "https://en.wikipedia.org/wiki?curid=287939", "text": "Rog-O-Matic\n\nRog-O-Matic is a bot developed in 1981 to play and win the computer game \"Rogue\", by four graduate students in the Computer Science Department at Carnegie-Mellon University in Pittsburgh: Andrew Appel, Leonard Hamey, Guy Jacobson and Michael Loren Mauldin.\n\nDescribed as a \"belligerent expert system\", Rog-O-Matic performs well when tested against expert \"Rogue\" players, even winning the game.\n\nBecause all information in \"Rogue\" is communicated to the player via ASCII text, Rog-O-Matic has automatic access to the same information a human player has. The program is still the subject of some scholarly interest; a 2005 paper said:\n\nOne of Rog-O-Matic's authors, Michael Loren Mauldin, would go on to write the Lycos search engine.\n\n", "id": "287939", "title": "Rog-O-Matic"}
{"url": "https://en.wikipedia.org/wiki?curid=14819817", "text": "Variation (game tree)\n\nA Variation can refer to a specific sequence of successive moves in a turn-based game, often used to specify a hypothetical future state of a game that is being played. Although the term is most commonly used in the context of Chess analysis, it has been applied to other games. It also is a useful term used when describing computer tree-search algorithms (for example minimax) for playing games such as Go or Chess.\n\nA variation can be any number of steps as long as each step would be legal if it were to be played. It is often as far ahead as a human or computer can calculate; or however long is necessary to reach a particular position of interest. It may also lead to a terminal state in the game, in which case the term \"Winning Variation\" or \"Losing Variation\" is sometimes used.\n\nThe principal variation refers to the particular variation that is the most advantageous to the current player, assuming each other player will respond with the move that best improves their own position. In other words, it is the \"best\" or \"correct\" line of play. In the context of tree-searching game Artificial Intelligence – in which this term is most common – it may also refer to the sequence of moves which is currently \"believed\" to be the most advantageous, but is not guaranteed due to the technical limitations of the algorithm.\n\n\n", "id": "14819817", "title": "Variation (game tree)"}
{"url": "https://en.wikipedia.org/wiki?curid=284528", "text": "Bitboard\n\nA bitboard is a data structure commonly used in computer systems that play board games.\n\nA bitboard, often used for boardgames such as chess, checkers, othello and word games, is a specialization of the bit array data structure, where each bit represents a game position or state, designed for optimization of speed and/or memory or disk use in mass calculations. Bits in the same bitboard relate to each other in the rules of the game, often forming a game position when taken together. Other bitboards are commonly used as masks to transform or answer queries about positions. The \"game\" may be any game-like system where information is tightly packed in a structured form with \"rules\" affecting how the individual units or pieces relate.\n\nBitboards are used in many of the world's highest-rated chess playing programs such as Houdini, Stockfish, and Critter. They help the programs analyze chess positions with few CPU instructions and hold a massive number of positions in memory efficiently.\n\nBitboards allow the computer to answer some questions about game state with one logical operation. For example, if a chess program wants to know if the white player has any pawns in the center of the board (center four squares) it can just compare a bitboard for the player's pawns with one for the center of the board using a logical AND operation. If there are no center pawns then the result will be zero.\n\nQuery results can also be represented using bitboards. For example, the query \"What are the squares between X and Y?\" can be represented as a bitboard. These query results are generally pre-calculated, so that a program can simply retrieve a query result with one memory load.\n\nHowever, as a result of the massive compression and encoding, bitboard programs are not easy for software developers to either write or debug.\n\nThe bitboard method for holding a board game appears to have been invented in the mid-1950s, by Arthur Samuel and was used in his checkers program. The method was published in 1959 as \"Some Studies in Machine Learning Using the Game of Checkers\" in the IBM Journal of Research and Development.\n\nFor the more complicated game of chess, it appears the method was independently rediscovered later by the Kaissa team in the Soviet Union in the late 1960s, published in 1970 \"Programming a computer to play chess\" in Russ. Math. Surv.\n, and again by the authors of the U.S. Northwestern University program \"Chess\" in the early 1970s, and documented in 1977 in \"Chess Skill in Man and Machine\".\n\nA bitboard or bit field is a format that stuffs a whole group of related boolean variables into the same machine word, typically representing positions on a board game. Each bit is a position, and when the bit is positive, a property of that position is true. In chess, for example, there would be a bitboard for black knights. There would be 64-bits where each bit represents a chess square. Another bitboard might be a constant representing the center four squares of the board. By comparing the two numbers with a bitwise logical AND instruction, we get a third bitboard which represents the black knights on the center four squares, if any. This format is generally more CPU and memory friendly than others.\n\nThe advantage of the bitboard representation is that it takes advantage of the essential logical bitwise operations available on nearly all CPUs that complete in one cycle and are fully pipelined and cached etc. Nearly all CPUs have AND, OR, NOR, and XOR. Many CPUs have additional bit instructions, such as finding the \"first\" bit, that make bitboard operations even more efficient. If they do not have instructions well known algorithms can perform some \"magic\" transformations that do these quickly.\n\nFurthermore, modern CPUs have instruction pipelines that queue instructions for execution. A processor with multiple execution units can perform more than one instruction per cycle if more than one instruction is available in the pipeline. Branching (the use of conditionals like if) makes it harder for the processor to fill its pipeline(s) because the CPU cannot tell what it needs to do in advance. Too much branching makes the pipeline less effective and potentially reduces the number of instructions the processor can execute per cycle. Many bitboard operations require fewer conditionals and therefore increase pipelining and make effective use of multiple execution units on many CPUs.\n\nCPUs have a bit width which they are designed toward and can carry out bitwise operations in one cycle in this width. So, on a 64-bit or more CPU, 64-bit operations can occur in one instruction. There may be support for higher or lower width instructions. Many 32-bit CPUs may have some 64-bit instructions and those may take more than one cycle or otherwise be handicapped compared to their 32-bit instructions.\n\nIf the bitboard is larger than the width of the instruction set, then a performance hit will be the result. So a program using 64-bit bitboards would run faster on a real 64-bit processor than on a 32-bit processor.\n\nSome queries are going to take longer than they would with perhaps arrays, so bitboards are generally used in conjunction with array boards in chess programs.\n\nBitboards are extremely compact. Since only a very small amount of memory is required to represent a position or a mask, more positions can find their way into registers, full speed cache, Level 2 cache, etc. In this way, compactness translates into better performance (on most machines). Also on some machines this might mean that more positions can be stored in main memory before going to disk.\n\nFor some games, writing a suitable bitboard engine requires a fair amount of source code that will be longer than the straightforward implementation. For limited devices (such as cell phones) with a limited number of registers or processor instruction cache, this can cause a problem. For full-sized computers, it may cause cache misses between level-one and level-two cache. This is only a potential problem, not a major drawback, as most machines will have enough instruction cache for this not to be an issue.\n\nThe first bit usually represents the square a1 (the lower left square), and the 64th bit represents the square h8 (the diagonally opposite square).\n\nThere are twelve types of pieces, and each type gets its own bitboard. Black pawns get a board, white pawns, etc. Together these twelve boards can represent a position. Some trivial information also needs to be tracked elsewhere; the programmer may use boolean variables for whether each side is in check, can castle, etc.\n\nConstants are likely available, such as WHITE_SQUARES, BLACK_SQUARES, FILE_A, RANK_4 etc. More interesting ones might include CENTER, CORNERS, CASTLE_SQUARES, etc.\n\nExamples of variables would be WHITE_ATTACKING, ATTACKED_BY_PAWN, WHITE_PASSED_PAWN, etc.\n\n\"Rotated\" bitboards are usually used in programs that use bitboards. Rotated bitboards make certain operations more efficient. While engines are simply referred to as \"rotated bitboard engines,\" this is a misnomer as rotated boards are used in \"addition\" to normal boards making these hybrid standard/rotated bitboard engines.\n\nThese bitboards rotate the bitboard positions by 90 degrees, 45 degrees, and/or 315 degrees. A typical bitboard will have one byte per rank of the chess board. With this bitboard it's easy to determine rook attacks across a rank, using a table indexed by the occupied square and the occupied positions in the rank (because rook attacks stop at the first occupied square). By rotating the bitboard 90 degrees, rook attacks across a file can be examined the same way. Adding bitboards rotated 45 degrees and 315 degrees produces bitboards in which the diagonals are easy to examine. The queen can be examined by combining rook and bishop attacks. Rotated bitboards appear to have been developed separately and (essentially) simultaneously by the developers of the DarkThought and Crafty programs.\n\nMagic move bitboard generation is a new and fast alternative to rotated move bitboard generators. These are also more versatile than rotated move bitboard generators because the generator can be used independently from any position. The basic idea is that you can use a multiply, right-shift hashing function to index a move database, which can be as small as 1.5K. A speedup is gained because no rotated bitboards need to be updated, and because the lookups are more cache-friendly.\n\nMany other games besides chess benefit from bitboards.\n\n\n\n\n\n\n\n\n", "id": "284528", "title": "Bitboard"}
{"url": "https://en.wikipedia.org/wiki?curid=23073520", "text": "Proof-number search\n\nProof-number search (short: PN search) is a game tree search algorithm invented by Victor Allis, with applications mostly in endgame solvers, but also for sub-goals during games.\n\nUsing a binary goal (e.g. first player wins the game), game trees of two-person perfect-information games can be mapped to an and–or tree. Maximizing nodes become OR-nodes, minimizing nodes are mapped to AND-nodes. For all nodes proof and disproof numbers are stored, and updated during the search.\n\nTo each node of the partially expanded game tree the proof number and\ndisproof number are associated. A proof number represents the minimum number of leaf\nnodes which have to be proved in order to prove the node. Analogously, a disproof\nnumber represents the minimum number of leaves which have to be disproved\nin order to disprove the node. Because the goal of the tree is to prove a forced\nwin, winning nodes are regarded as proved. Therefore, they have proof number\n0 and disproof number ∞. Lost or drawn nodes are regarded as\ndisproved. They have proof number ∞ and disproof number\n0. Unknown leaf nodes have a proof and disproof number of unity. \nThe proof number of an internal AND node is equal to the sum of\nits children's proof numbers, since to prove an AND node all the children have\nto be proved. The disproof number of an AND node is equal to the minimum of\nits children's disproof numbers. The disproof number of an internal OR node is\nequal to the sum of its children's disproof numbers, since to disprove an OR node\nall the children have to be disproved. Its proof number is equal to the minimum\nof its children's proof numbers. \n\nThe procedure of selecting the most-proving node\nto expand is the following. We start at the root. Then, at each OR node the child\nwith the lowest proof number is selected as successor, and at each AND node the\nchild with the lowest disproof number is selected as successor. Finally, when a\nleaf node is reached, it is expanded and its children are evaluated. \n\nThe proof and disproof numbers represent lower bounds on the number of nodes to be evaluated to prove (or disprove) certain nodes. By always selecting the most proving (disproving) node to expand, an efficient search is generated.\n\nSome variants of proof number search like dfPN, PN, PDS-PN have been developed to address the quite big memory\nrequirements of the algorithm.\n\nA. Kishimoto, M.H.M. Winands, M. Müller, and J-T. Saito (2012) \"Game-tree search using proof numbers: The first twenty years\", ICGA, 35(3):131–156, pdf\n", "id": "23073520", "title": "Proof-number search"}
{"url": "https://en.wikipedia.org/wiki?curid=6172005", "text": "Computer bridge\n\nComputer bridge is the playing of the game contract bridge using computer software. After years of limited progress, since around the end of the 20th century the field of computer bridge has made major advances. In 1996 the American Contract Bridge League (ACBL) established an official World Computer-Bridge Championship, to be held annually along with a major bridge event. The first championship took place in 1997 at the North American Bridge Championships in Albuquerque. Since 1999 the event has been conducted as a joint activity of the American Contract Bridge League and the World Bridge Federation. Alvin Levy, ACBL Board member, initiated this championship and has coordinated the event annually since its inception. The event history, articles and publications, analysis, and playing records can be found at the official website.\n\nThe World Computer-Bridge Championship is typically played as a round robin followed by a knock-out between the top four contestants. Winners of the annual event are:\n\n\nIn Zia Mahmood's book, \"Bridge, My Way\" (1992), Zia offered a £1 million bet that no four-person team of his choosing would be beaten by a computer. A few years later the bridge program \"GIB\", brainchild of American computer scientist Matthew Ginsberg, proved capable of expert declarer plays like winkle squeezes in play tests. In 1996, Zia withdrew his bet. Two years later, \"GIB\" became the world champion in computer bridge, and also had a 12th place score (11210) in declarer play compared to 34 of the top humans in the 1998 Par Contest (including Zia Mahmood). However, such a par contest measures technical bridge analysis skills only, and in 1999 Zia beat various computer programs, including \"GIB\", in an individual round robin match.\n\nFurther progress in the field of computer bridge has resulted in stronger bridge playing programs, including \"Jack\" and \"Wbridge5\". These programs have been ranked highly in national bridge rankings. A series of articles published in 2005 and 2006 in the Dutch bridge magazine \"IMP\" describes matches between five-time computer bridge world champion \"Jack\" and seven top Dutch pairs including a Bermuda Bowl winner and two reigning European champions. A total of 196 boards were played. \"Jack\" defeated three out of the seven pairs (including the European champions). Overall, the program lost by a small margin (359 versus 385 IMPs).\n\nBridge poses challenges to its players that are different from board games such as chess and go. Most notably, bridge is a stochastic game of incomplete information. At the start of a deal, the information available to each player is limited to just his/her own cards. During the bidding and the subsequent play, more information becomes available via the bidding of the other three players at the table, the cards of the partner of the declarer (the dummy) being put open on the table, and the cards played at each trick. However, it is only at the end of the play that full information is obtained.\n\nToday's top-level bridge programs deal with this probabilistic nature by generating many samples representing the unknown hands. Each sample is generated at random, but constrained to be compatible with all information available so far from the bidding and the play. Next, the result of different lines of play are tested against optimal defense for each sample. This testing is done using a so-called \"double-dummy solver\" that uses extensive search algorithms to determine the optimum line of play for both parties. The line of play that generates the best score averaged over all samples is selected as the optimal play.\n\nEfficient double-dummy solvers are key to successful bridge-playing programs. Also, as the amount of computation increases with sample size, techniques such as importance sampling are used to generate sets of samples that are of minimum size but still representative.\n\nWhile bridge is a game of incomplete information, a double-dummy solver analyses a simplified version of the game where there is perfect information; the bidding is ignored, the contract (trump suit and declarer) is given, and all players are assumed to know all cards from the very start. The solver can therefore use many of the game tree search techniques typically used in solving two-player perfect-information win/lose/draw games such as chess, go and reversi. However, there are some significant differences.\n\n\n\nIn comparison to computer chess, computer bridge has not reached world-class level, but the top robots have demonstrated a consistent high level of play. To demonstrate this, see analysis of the last few years play at www.computerbridge.com. Yet, whereas computer chess has taught programmers little about building machines that offer human-like intelligence, more intuitive and probabilistic games such as bridge might provide a better testing ground.\n\nThe question whether bridge-playing programs will reach world-class level in the foreseeable future is not easy to answer. Computer bridge has not attracted an amount of interest anywhere near to that of computer chess. On the other hand, researchers working in the field have accomplished most of the current progress in the last decade.\n\nRegardless of bridge robots' level of play, computer bridge already has changed the analysis of the game. Commercially available double-dummy programs can solve bridge problems in which all four hands are known, typically within a fraction of a second. These days, few editors of books and magazines will solely rely on humans to analyse bridge problems before publications. Also, more and more bridge players and coaches utilize computer analysis in the \"post-mortem\" of a match.\n\n\n", "id": "6172005", "title": "Computer bridge"}
{"url": "https://en.wikipedia.org/wiki?curid=501462", "text": "Computer poker player\n\nA computer poker player is a computer program designed to play the game of poker against human opponents or other computer opponents. It is commonly referred to as pokerbot or just simply bot.\n\nThese bots or computer programs are used often in online poker situations as either legitimate opponents for humans players or a form of cheating. Whether or not the use of bot constitutes cheating is typically defined by the poker room that hosts the actual poker games. Most (if not all) cardrooms forbid the use of bots although the level of enforcement from site operators varies considerably.\n\nThe subject of player bots and computer assistance, while playing online poker, is very controversial. Player opinion is quite varied when it comes to deciding which types of computer software fall into the category unfair advantage. One of the primary factors in defining a bot is whether or not the computer program can interface with the poker client (in other words, play by itself) without the help of its human operator. Computer programs with this ability are said to have or be an autoplayer and are universally defined to be in the category of bots regardless of how well they play poker.\n\nThe issue of unfair advantage has much to do with what types of information and artificial intelligence are available to the computer program. In addition, bots can play for many hours at a time without human weaknesses such as fatigue and can endure the natural variances of the game without being influenced by human emotion (or \"tilt\"). On the other hand, bots have some significant disadvantages - for example, it is very difficult for a bot to accurately read a bluff or adjust to the strategy of opponents the way humans can.\n\nWhile the terms and conditions of poker sites generally forbid the use of bots, the level of enforcement depends on the site operator. Some will seek out and ban bot users through the utilization of a variety of software tools. The poker client can be programmed to detect bots although this is controversial in its own right as it might be seen as tantamount to embedding spyware in the client software. Another method is to use CAPTCHAs at random intervals during play.\n\nThe subject of house bots is even more controversial due to the conflict of interest it potentially poses. By the strictest definition, a house bot is an automated player operated by the online poker room itself, although some would define more indirect examples (for example, a player operating bots with the knowledge and consent of the operator) as \"house bots\" as well. These type of bots would be the equivalent of brick and mortar shills.\n\nIn a brick and mortar casino, a house player does not subvert the fairness of the game being offered as long as the house is dealing honestly. In an online setting the same is also true. By definition, an honest online poker room that chooses to operate house bots would guarantee that the house bots did not have access to any information not also available to any other player in the hand (the same would apply to any human shill as well). The problem is that in an online setting the house has no way to prove their bots are not receiving sensitive information from the card server. This is further exacerbated by the ease with which clandestine information sharing can be accomplished in a digital environment. It is essentially impossible even for the house to prove that they do not control some players - probably the only real way that could be done would be to disclose the confidential personal information of every player and that obviously cannot be done due to privacy considerations.\n\nPoker is a game of imperfect information (because some cards in play are concealed) thus making it difficult for anyone (including a computer) to deduce the final outcome of the hand. Because of this lack of information, the computer's programmers have to implement systems based on the Bayes theorem, Nash equilibrium, Monte Carlo simulation or neural networks, all of which are imperfect techniques.\n\nAIs like PokerSnowie and Claudico have been created by allowing the computer to determine the best possible strategy by letting it play itself an enormous number of times. This seems to be the current approach to poker AI, as opposed to attempting to make a computer that plays like a human. This results in odd bet sizing and a much different strategy than humans are used to seeing.\n\nMethods are being developed to at least approximate perfect poker strategy from the game theory perspective in the heads-up (two player) game, and increasingly good systems are being created for the multi-player game. Perfect strategy has multiple meanings in this context. From a game-theoretic optimal point of view, a perfect strategy is one that cannot expect to lose to any other player's strategy; however, optimal strategy can vary in the presence of sub-optimal players who have weaknesses that can be exploited. In this case, a perfect strategy would be one that correctly or closely models those weaknesses and takes advantage of them to make a profit, such as those explained above.\n\nA large amount of the research into computer poker players is being performed at the University of Alberta by the Computer Poker Research Group, led by Dr. Michael Bowling. The group developed the agents \"Poki\", \"PsOpti\", \"Hyperborean\" and Polaris. \"Poki\" has been licensed for the entertainment game \"STACKED\" featuring Canadian poker player Daniel Negreanu. \"PsOpti\" was available under the name \"SparBot\" in the poker training program \"Poker Academy\". The series of \"Hyperborean\" programs have competed in the Annual Computer Poker Competition, most recently taking three gold medals out of six events in the 2012 competition. The same line of research also produced Polaris, which played against human professionals in 2007 and 2008, and became the first computer poker program to win a meaningful poker competition.\n\nIn January 2015, an article in \"Science\" by Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin claimed that their poker bot Cepheus had \"essentially weakly solved\" the game of heads-up limit Texas hold 'em.\n\nT. Sandholm and A. Gilpin from Carnegie Mellon University have started poker AI research in 2004 beginning with unbeatable agent for 3-card game called Rhode-Island Hold 'em. Next step was GS1 which outperformed the best commercially available poker bots. Since 2006 poker agents from this group have participated in annual computer competitions. \"At some point we will have a program better than the best human players\" – claims Sandholm. His bot, Claudico, faced off against four human opponents in 2015. In 2017 the program's latest software, Libratus, faced off against four professional poker players. By the end of the experiment the four human players had lost a combined $1.8 million.\n\nA team from the University of Auckland consists of a small number of scientists who employ case-based reasoning to create and enhance Texas Hold’em poker agents. The group applies different AI techniques to a number of games including participation in the commercial projects Small Worlds and Civilization (video game).\n\nNeo Poker Lab is an established science team focused on the research of poker artificial intelligence. For several years it has developed and applied state-of-the-art algorithms and procedures like regret minimization and gradient search equilibrium approximation, decision trees, recursive search methods as well as expert algorithms to solve a variety of problems related to the game of poker.\n\nOne of the earliest no-limit poker bot competitions was organized in 2004 by International Conference on Cognitive Modelling. The tournament hosted five bots from various universities from around the world. The winner was Ace Gruber, from University of Toronto.\n\nThe ACM has hosted competitions where the competitors submit an actual piece of software able to play poker on their specific platform. The event hosts operate everything and conduct the contest and report the results. (citations and references and links needed).\n\nIn the summer 2005, the online poker room Golden Palace hosted a promotional tournament in Las Vegas, at the old Binions, with a $100k giveaway prize. It was billed as the 2005 World Series of Poker Robots. The tournament was bots only with no entry fee. The bot developers were computer scientists from six nationalities who traveled at their own expense. The host platform was Poker Academy. The event also featured a demonstration headsup event with Phil Laak.\n\nIn the summer 2007, the University of Alberta hosted a highly specialized headsup tournament between humans and their Polaris bot, at the AAAI conference in Vancouver, BC, Canada. The host platform was written by the University of Alberta. There was a $50k maximum giveaway purse with special rules to motivate the humans to play well. The humans paid no entry fee. The unique tournament featured four duplicate style sessions of 500 hands each. The humans won by a narrow margin.\n\nIn the summer of 2008, the University of Alberta and the poker coaching website Stoxpoker ran a second tournament during the World Series of Poker in Las Vegas. The tournament had six duplicate sessions of 500 hands each, and the human players were Heads-Up Limit specialists. Polaris won the tournament with 3 wins, 2 losses and a draw. The results of the tournament, including the hand histories from the matches, are available on the competition website.\n\nFrom April–May 2015, Carnegie Mellon University Sandholm's latest bot, Claudico, faced off against four human opponents, in a series of no-limit Texas Hold'em matches. Finally, after playing 80,000 hands, humans were up by a combined total of $732,713. But even though humans technically won, scientists considered the win as statistically insignificant (rather, a statistical tie) when that $732,713 is compared to the total betting amount of $170,000,000 ($170 million). However, some have determined this claim to be disingenuous. Statistically insignificant here means that the programmers of Claudico can not say with 95% confidence (a 95% confidence interval) that humans are better than the computer program. However, it is a statistically significant win on a 90% confidence interval. This means that the human players are somewhere between a 10 to 1 and 20 to 1 favorite.\n\nThe way the tournament was structured was in two sets of two players each. In each of the two sets, the players got the opposite cards. Meaning if the computer has As9c (Ace of Spades & Nine of Clubs) and the human has Jh8d on one computer, the other of the two players in the set will have As9c up against the computer's Jh8d. However, even with the human players winning more than the computer—not all of the players were positive in their head to head match ups.\n\nThe totals for each of the players winnings were as follows:\n\nSince 2006, the Annual Computer Poker Competition has run a series of competitions for poker programs. In 2011, three types of poker were played: Heads-Up Limit Texas Hold'em, Heads-Up No-Limit Texas Hold'em, and 3-player Limit Texas Hold'em. Within each event, two winners are named: the agent that wins the most matches, and the agent that wins the most money. These winners are often not the same agent, as one evaluation rewards robust players, and the other rewards players that are good at exploiting the other agents' mistakes. The competition is motivated by scientific research, and there is an emphasis on ensuring that all of the results are statistically significant by running millions of hands of poker. The 2012 competition had the same formats with more than 70 million hands played to eliminate luck factor.\n\nSome researchers developed web application where people could play and assess quality of the AI. So as of December 2012 the following top groups and individual researchers’ agents could be found:\n\n\n\n", "id": "501462", "title": "Computer poker player"}
{"url": "https://en.wikipedia.org/wiki?curid=19496019", "text": "Computer shogi\n\nComputer shogi is a field of artificial intelligence concerned with the creation of computer programs which can play shogi. The research and development of shogi software has been carried out mainly by freelance programmers, university research groups and private companies. By 2017, the strongest programs were outperforming the strongest human players.\n\nShogi has the distinctive feature of reusing captured pieces. Therefore, shogi has a higher branching factor than other chess variants. The computer has more positions to examine because each piece in hand can be dropped on many squares. This gives shogi the highest number of legal positions and the highest number of possible games of all the popular chess variants. The higher numbers for shogi mean it is harder to reach the highest levels of play. The number of legal positions and the number of possible games are two measures of shogis game complexity.\n\nThe complexity of Go can be found at Go and mathematics.\nMore information on the complexity of Chess can be found at Shannon number.\n\nThe primary components of a computer shogi program are the opening book, the search algorithm and the endgame. The \"opening book\" helps put the program in a good position and saves time. Shogi professionals, however, do not always follow an opening sequence as in chess, but make different moves to create good formation of pieces. The \"search algorithm\" looks ahead more deeply in a sequence of moves and allows the program to better evaluate a move. The search is harder in shogi than in chess because of the larger number of possible moves. A program will stop searching when it reaches a stable position. The problem is many positions are unstable because of the drop move. Finally, the \"endgame\" starts when the king is attacked and ends when the game is won. In chess, there are fewer pieces which leads to perfect play by endgame databases; However, pieces can be dropped in shogi so there are no endgame databases. A tsumeshogi solver is used to quickly find mating moves.\n\nIn the 1980s, due to the immaturity of the technology in such fields as programming, CPUs and memory, computer shogi programs took a long time to think, and often made moves for which there was no apparent justification. These programs had the level of an amateur of kyu rank.\n\nIn the first decade of the 21st century, computer shogi has taken large steps forward in software and hardware technology. In 2007, top shogi player Yoshiharu Habu estimated the strength of the 2006 world computer shogi champion Bonanza. He contributed to the newspaper Nihon Keizai Shimbun evening edition on 26 March 2007 about the match between Bonanza and then Ryūō Champion Akira Watanabe. Habu rated Bonanzas game at the level of 2 dan shogi apprentice (\"shōreikai\").\n\nIn particular, computers are most suited to brute-force calculation, and far outperform humans at the task of finding ways of checkmating from a given position, which involves many fewer possibilities. In games with time limits of 10 seconds from the first move, computers are becoming a tough challenge for even professional shogi players. The past steady progress of computer shogi is a guide for the future. In 1996 Habu predicted a computer would beat him in 2015. Akira Watanabe gave an interview to the newspaper Asahi Shimbun in 2012. He estimated the computer played at the 4 dan professional level. Watanabe also said the computer sometimes found moves for him.\n\nOn 23 October 2005, at the 3rd International Shogi Forum, the Japan Shogi Association permitted Toshiyuki Moriuchi, 2005 Meijin, to play computer shogi program YSS. Toshiyuki Moriuchi won the game playing 30 seconds per move with a Bishop handicap. In 2012, a retired professional lost a match with computer publicly first, and in 2013, active shogi professionals too.\n\nThe Japan Shogi Association (JSA) gave reigning Ryuo Champion Watanabe permission to compete against the reigning World Computer Shogi Champion Bonanza on 21 March 2007. Daiwa Securities sponsored the match. Hoki Kunihito wrote Bonanza. The computer was an Intel Xeon 2.66 GHz 8 core with 8 gigabytes of memory and 160-gigabyte hard drive. The game was played with 2 hours each and 1 minute byo-yomi per move after that. Those conditions favor Watanabe because longer time limits mean there are fewer mistakes from time pressure. Longer playing time also means human players can make long-term plans beyond the computers calculating horizon. The 2 players were not at the same playing level. Watanabe was 2006 Ryuo Champion and Bonanza was at the level of 2 dan shoreikai. Bonanza was a little stronger than before due to program improvements and a faster computer. Watanabe prepared for a weaker Bonanza as Watanabe studied old Bonanza game records.\n\nBonanza moved first and played Fourth File Rook Anaguma as Watanabe expected. Watanabe thought some of Bonanza's moves were inferior. However, Watanabe deeply analyzed these moves thinking that maybe the computer saw something that Watanabe did not \nsee. Watanabe commented after the game that he could have lost if Bonanza had played defensive moves before entering the endgame. But the computer choose to attack immediately instead of taking its time (and using its impressive endgame strategies) which cost it the match. Bonanza resigned after move 112.\n\nAfter Bonanza's loss Watanabe commented on computers in his blog, “I thought they still had quite a way to go, but now we have to recognize that they've reached the point where they are getting to be a match for professionals.” Watanabe further clarified his position on computers playing shogi in the Yomiuri Shimbun on 27 June 2008 when he said \"I think I'll be able to defeat shogi software for the next 10 years\". Another indication Bonanza was far below the level of professional Watanabe came 2 months after the match at the May 2007 World Computer Shogi Championship. Bonanza lost to the 2007 World Computer Shogi Champion YSS. Then YSS lost to amateur Yukio Kato in a 15-minute game.\n\nThe winners of CSA tournaments played exhibition games with strong players. These exhibition games started in 2003.\n\nIn each succeeding year, the human competition was stronger to match the stronger programs. Yukio Kato was the Asahi Amateur Meijin champion. Toru Shimizugami was the Amateur Meijin champion. Eiki Ito, the creator of Bonkras, said in 2011, at present, top Shogi programs like Bonkras are currently at a level of lower- to middle-class professional players.\n\nThe computer program Akara defeated the womens Osho champion Ichiyo Shimizu. Akara contained 4 computer engines, Gekisashi, GPS Shogi, Bonanza and YSS. Akara ran on a network of 169 computers. The 4 engines voted on the best moves. Akara selects the move with the most votes. If there is a tie vote then Akara selects Gekisashis move. Researchers at the University of Tokyo and the University of Electro-Communications developed Akara.\n\nShimizu moved first and resigned in 86 moves after 6 hours and 3 minutes. Shimizu said she was trying to play her best as if she was facing a human player. She played at the University of Tokyo on 11 October 2010. The allotted thinking time per player is 3 hours and 60 seconds byoyomi. 750 fans attended the event. This is the third time since 2005 that the Japan Shogi Association granted permission to a professional to play a computer, and the first victory against a female professional.\n\nAkara aggressively pursued Shimizu from the start of the game. Akara played with a ranging rook strategy and offered an exchange of bishops. Shimizu made a questionable move partway though the game, and Akara went on to win. Ryuo champion, Akira Watanabe, criticized Shimizus game. On 19 November 2010, the Daily Yomiuri quoted Watanabe. Watanabe said, \"Ms. Shimizu had plenty of chances to win\".\n\nOn 24 July 2011, there was a two-game amateur versus computer match. Two computer shogi programs beat a team of two amateurs. One amateur, Mr. Kosaku, was a Shoreikai three Dan player. The other amateur, Mr. Shinoda, was the 1999 Amateur Ryuo. The allotted time for the amateurs was main time 1 hour and then 3 minutes per move. The allotted time for the computer was main time 25 minutes and then 10 seconds per move.\n\nOn 21 December 2011, computer program Bonkras crushed retired 68-year-old Kunio Yonenaga, the 1993 Meijin. They played 85 moves in 1 hour, 3 minutes 39 seconds on Shogi Club 24. Main time was 15 minutes then additional 60 seconds per move. Yonenaga was gote (white) and played 2. K-62. This move was to confuse the computer by playing a move not in Bonkrass joseki (opening book). On 14 January 2012, Bonkras again defeated Yonenaga. This match is the first Denou-sen match. The game had 113 moves. Time allowed was 3 hours and then 1 minute per move. Bonkras moved first and used a ranging rook opening. Yonenaga made the same second move, K-6b, as in the previous game he lost. Bonkras ran on a Fujitsu Primergy BX400 with 6 blade servers to search 18,000,000 moves per second. Yonenaga used 2 hours 33 minutes. Bonkras used 1 hour 55 minutes. Bonkras evaluated its game with Yonenaga in January 2012.\n\n \nDenou-sen is a human versus machine battle. This match is the second Denou-sen match. Niconico is sponsoring 5 games. 5 professional shogi players play 5 computers. The winners of the previous World Computer Shogi Championship play the professional shogi players. Each player starts with 4 hours. After the player finishes 4 hours, the player must complete each move in 60 seconds. Niconico is broadcasting the games live with commentary.\n\nHiroyuki Miura said before his game he would play with \"all his heart and soul\". Miura decided to use trusted tactics instead of an anti-computer strategy. The computer played book moves and they castled symmetrically to defend their kings. The computer attacked quickly and Miura counterattacked with a drop move. More than 8 hours later Miura resigned. After the game, Miura said that \"he should not have prepared for the game the way he did. He should have prepared for the game with a genuine sense of urgency, if only he knew, the computer was so strong.\" Miura expressed disappointment and said he has yet to figure out where he went wrong. The evaluation of the game by GPS is on the GPS Shogi web site.\n\nOn 31 December 2013, Funae and Tsutsukana played a second game. Tsutsukana was the same version that beat Funae on 6 April 2013. The computer was one Intel processor with 6 cores. Funae won.\n\nOn 21 August 2013, the Japan Shogi Association announced, five professional shogi players will play five computers from 15 March to 12 April 2014. On 7 October 2013, the Japan Shogi Association picked the five players. \nThe professional shogi players will play the winners of a preliminary computer tournament. The preliminary computer tournament was held 2–4 November 2013. \n\nEach player starts with 5 hours at 10 am. After the player finishes 5 hours, the player must complete each move in 1 minute. There is 1 hour lunch break at 12:00 and half hour dinner break at 5 pm. Niconico is broadcasting the games live with commentary. Japanese auto parts maker Denso developed a robotic arm to move the pieces for the computer.\n\nŌshō and Kiō champion Akira Watanabe wrote in his blog that \"a human cannot think of some of Ponanza's moves such as 60.L*16 and 88.S*79. I am not sure they were the best moves or not right now, but I feel like I'm watching something incredible.\" Kisei, Ōi and Ōza champion Yoshiharu Habu told the Asahi Shimbum Newspaper, \"I felt the machines were extraordinarily strong when I saw their games this time.\"\n\nOn Saturday 19 July 2014, Tatsuya Sugai once again got the chance to play against Shueso in what was billed as the \"Shogi Denou-sen Revenge Match\". Sugai had already been beaten by Shueso four months earlier in game one of Denou-sen 3, so this was seen as his chance to gain revenge for that loss. The game was sponsored by both the Japan Shogi Association and the telecommunications and media company Dwango and was held at the Tokyo Shogi Kaikan (the Japan Shogi Association's head office). Although the playing site was closed to the public, the game was streamed live via \"Niconico Live\" with commentary being provided by various shogi professionals and women's professionals. Shuesho's moves were made by Denso's robotic arm. The initial time control for each player was eight hours which was then followed by a 1-minute byoyomi. In addition, four 1-hour breaks were scheduled throughout the playing session to allow both sides time to eat and rest. The game lasted through the night and into the next day and finally finished almost 20 hours after it started when Sugai resigned after Shueso's 144 move.\n\nShogidokoro (将棋所) is a Windows graphical user interface (GUI) that calls a program to play shogi and displays the moves on a board. Shogidokoro was created in 2007. Shogidokoro uses the Universal Shogi Interface (USI). The USI is an open communication protocol that shogi programs use to communicate with a user interface. USI was designed by Norwegian computer chess programmer Tord Romstad in 2007. Tord Romstad based USI on Universal Chess Interface (UCI). UCI was designed by computer chess programmer Stefan Meyer-Kahlen in 2000. Shogidokoro can automatically run a tournament between two programs. This helps programmers to write shogi programs faster because they can skip writing the user interface part. It is also useful for testing changes to a program. Shogidokoro can be used to play shogi by adding a shogi engine to Shogidokoro. Some engines that will run under Shogidokoro are the following:\n\n\nThe interface can also use tsumeshogi solver-only engines like SeoTsume (脊尾詰). \nThe software's menus have both Japanese and English language options available.\n\nXBoard/WinBoard is another GUI that supports shogi and other chess variants including western chess and xiangqi. Shogi support was added to WinBoard in 2007 by H.G. Muller. WinBoard uses its own protocol (Chess Engine Communication Protocol) to communicate with engines, but can connect to USI engines through the UCI2WB adapter. Engines that can natively support WinBoard protocol are Shokidoki, TJshogi, GNU Shogi and Bonanza. Unlike Shogidokoro, WinBoard is free/libre and open source, and also available for the X Window System as XBoard (for Linux and Mac systems).\n\nA number of Shogi variants, such as Chu Shogi and Dai Shogi, are playable against AI using a forked version of Winboard. Included engines are: Shokidoki, which can play the smaller variants with drops (i.e. Minishogi); and HaChu, a large Shogi variant engine designed for playing Chu Shogi and has improved in strength over time.\n\n将棋ぶらうざＱ (Shogi Browser Q) is a free GUI written for Mac, Linux, and Windows operating systems. It can run USI engines and compete on Floodgate. Only a Japanese language version is available.\n\nBCMShogi is an English language graphical user interface for the USI protocol and the WinBoard shogi protocol. It is no longer developed and currently is unavailable from the author's website.\n\nFloodgate is a computer shogi server for computers to compete and receive ratings. Programs running under Shogidokoro can connect to Floodgate. The GPS team created Floodgate. Floodgate started operating continuously in 2008. The most active players have played 4,000 games. From 2008 to 2010, 167 players played 28,000 games on Floodgate. Humans are welcome to play on Floodgate. The time limit is 15 minutes per player, sudden death. From 2011 to 2017, the Floodgates number one program increased by 944 points, an average of 157 points per year.\nThe annual computer vs computer world shogi championship is organized by the Computer Shogi Association (CSA) of Japan. The computers play automated games through a server. Each program has 25 minutes to complete a game. The first championship was in 1990 with six programs. In 2001, it grew to 55 programs. The championship is broadcast on the Internet. At the 19th annual CSA tournament, four programs (GPS Shogi, Otsuki Shogi, Monju and KCC Shogi) that had never won a CSA tournament defeated three of the previous years strongest programs (Bonanza, Gekisashi and YSS). The top three winners of the 2010 CSA tournament are Gekisashi, Shueso and GPS Shogi.\n\nIn 2011, Bonkras won the CSA tournament with five wins out of seven games. Bonkras ran on a computer with three processors containing 16 cores and six gigabytes of memory. Bonanza won second place on a computer with 17 processors containing 132 cores and 300 gigabytes of memory. Shueso won third place. The 2010 CSA winner, Gekisashi, won fourth place. Ponanza won fifth place. GPS Shogi won sixth place on a computer with 263 processors containing 832 cores and 1486 gigabytes of memory. In 2012, GPS Shogi searched 280,000,000 moves per second and the average search depth was 22.2 moves ahead. Hiroshi Yamashita, the author of YSS, maintains a list of all shogi programs that played in World Computer Shogi Championship by year and winning rank. \nSome commercial game software which play shogi are \"Habu Meijin no Omoshiro Shōgi\" for Super Famicom, \"Clubhouse Games\" for Nintendo DS and \"Shotest Shogi\" for Xbox.\n\nOn 18 September 2005 a Japan Shogi Association professional 5 dan played shogi against a computer. The game was played at the 29th Hokkoku Osho-Cup Shogi Tournament in Komatsu, Japan. The Matsue National College of Technology developed the computer program Tacos. Tacos played first and chose the static rook line in the opening. Professional Hashimoto followed the opening line while changing his bishop with the bishop of Tacos. Tacos had a good development with some advantages in the opening and middle game even until move 80. Many amateur players expected Tacos to win. However, professional Hashimoto defended and Tacos played strange moves. Tacos lost.\n\nOn 14 October 2005, the Japan Shogi Association banned professional shogi players from competing against a computer. The Japan Shogi Association said the rule is to preserve the dignity of its professionals, and to make the most of computer shogi as a potential business opportunity. The ban prevents the rating of computers relative to professional players.\n\nFrom 2008 to 2012, the Japan Shogi Association did not permit any games between a professional and a computer.\n\n\n\n", "id": "19496019", "title": "Computer shogi"}
{"url": "https://en.wikipedia.org/wiki?curid=28134188", "text": "GADDAG\n\nA GADDAG is a data structure presented by Steven Gordon in 1994, for use in generating moves for Scrabble and other word-generation games where such moves require words that \"hook into\" existing words. It is often in contrast to move-generation algorithms using a directed acyclic word graph (DAWG) such as the one used by Maven. It is generally twice as fast as the traditional DAWG algorithms, but take about 5 times as much space for regulation Scrabble dictionaries.\n\nQuackle uses a GADDAG to generate moves.\n\nA GADDAG is a specialization of a Trie, containing states and branches to other GADDAGs. It is distinct for its storage of every reversed prefix of every word in a dictionary. This means every word has as many representations as it does letters; since the average word in most Scrabble regulation dictionaries is 5 letters long, this makes the GADDAG about 5 times as big as a simple DAWG.\n\nFor any word in a dictionary that is formed by a non-empty prefix \"x\" and a suffix \"y,\" a GADDAG contains a direct, deterministic path for any string REV(\"x\")+\"y\", where + is a concatenation operator.\n\nFor example, for the word \"\"explain\",\" a GADDAG will contain direct paths to the strings\n\nThis setup enables searching for a word given any letter that occurs in it.\n\nAny move-generation algorithm must adhere to three types of constraints:\n\n\nDAWG-based algorithms take advantage of the second and third constraint: the DAWG is built around the dictionary, and is traverse using tiles in the rack. It fails, however, to address the first constraint: supposing one want to 'hook into' the letter \"P\" in \"HARPY\", and the board has 2 spaces before the P, one must search the dictionary for all words containing letters from the rack where the third letter is \"P\". This is non-deterministic when searching through the DAWG, as many searches through the trie will be fruitless.\n\nThis is addressed by the GADDAG's storage of prefixes: by traversing the \"P\" branch of a GADDAG, one sees all words that have a \"P\" somewhere in their composition, and can \"travel up\" the prefix to form the word with tiles in the rack. To use the example from the section, searching for \"P\" turns up \"\"pxe+lain\"\". The letters between \"P\" and the + can be placed above the \"P\" on the board, and the rest below it (if space on the board permits).\n\n\n", "id": "28134188", "title": "GADDAG"}
