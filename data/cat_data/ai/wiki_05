{"url": "https://en.wikipedia.org/wiki?curid=602401", "text": "Facial recognition system\n\nA facial recognition system is a computer application capable of identifying or verifying a person from a digital image or a video frame from a video source. One of the ways to do this is by comparing selected facial features from the image and a face database.\n\nIt is typically used in security systems and can be compared to other biometrics such as fingerprint or eye iris recognition systems. Recently, it has also become popular as a commercial identification and marketing tool.\n\nSome face recognition algorithms identify facial features by extracting landmarks, or features, from an image of the subject's face. For example, an algorithm may analyze the relative position, size, and/or shape of the eyes, nose, cheekbones, and jaw. These features are then used to search for other images with matching features.\nOther algorithms normalize a gallery of face images and then compress the face data, only saving the data in the image that is useful for face recognition. A probe image is then compared with the face data. One of the earliest successful systems is based on template matching techniques applied to a set of salient facial features, providing a sort of compressed face representation.\n\nRecognition algorithms can be divided into two main approaches, geometric, which looks at distinguishing features, or photometric, which is a statistical approach that distills an image into values and compares the values with templates to eliminate variances.\n\nPopular recognition algorithms include principal component analysis using eigenfaces, linear discriminant analysis, elastic bunch graph matching using the Fisherface algorithm, the hidden Markov model, the multilinear subspace learning using tensor representation, and the neuronal motivated dynamic link matching.\n\nThree-dimensional face recognition technique uses 3D sensors to capture information about the shape of a face. This information is then used to identify distinctive features on the surface of a face, such as the contour of the eye sockets, nose, and chin.\n\nOne advantage of 3D face recognition is that it is not affected by changes in lighting like other techniques. It can also identify a face from a range of viewing angles, including a profile view. Three-dimensional data points from a face vastly improve the precision of face recognition. 3D research is enhanced by the development of sophisticated sensors that do a better job of capturing 3D face imagery. The sensors work by projecting structured light onto the face. Up to a dozen or more of these image sensors can be placed on the same CMOS chip—each sensor captures a different part of the spectrum...\n\nEven a perfect 3D matching technique could be sensitive to expressions.\nFor that goal a group at the Technion applied tools from metric geometry\nto treat expressions as isometries\n\nA new method is to introduce a way to capture a 3D picture by using three tracking cameras that point at different angles; one camera will be pointing at the front of the subject, second one to the side, and third one at an angle. All these cameras will work together so it can track a subject’s face in real time and be able to face detect and recognize.\n\nAnother emerging trend uses the visual details of the skin, as captured in standard digital or scanned images. This technique, called skin texture analysis, turns the unique lines, patterns, and spots apparent in a person’s skin into a mathematical space.\n\nTests have shown that with the addition of skin texture analysis, performance in recognizing faces can increase 20 to 25 percent.\n\nA different form of taking input data for face recognition is by using thermal cameras, by this procedure the cameras will only detect the shape of the head and it will ignore the subject accessories such as glasses, hats, or make up. A problem with using thermal pictures for face recognition is that the databases for face recognition is limited. Diego Socolinsky, and Andrea Selinger (2004) research the use of thermal face recognition in real life, and operation sceneries, and at the same time build a new database of thermal face images. The research uses low-sensitive, low-resolution ferro-electric electrics sensors that are capable of acquire long wave thermal infrared (LWIR). The results show that a fusion of LWIR and regular visual cameras has the greater results in outdoor probes. Indoor results show that visual has a 97.05% accuracy, while LWIR has 93.93%, and the Fusion has 98.40%, however on the outdoor proves visual has 67.06%, LWIR 83.03%, and fusion has 89.02%. The study used 240 subjects over the period of 10 weeks to create the new database. The data was collected on sunny, rainy, and cloudy days.\n\nThe Australian Border Force and New Zealand Customs Services have set up an automated border processing system called SmartGate that uses face recognition, which compares the face of the traveller with the data in the e-passport microchip. Major Canadian airports will be using a new facial recognition program as part of the Primary Inspection Kiosk program that will compare people's faces to their passports. This program will first come to Ottawa International Airport in early 2017 and to other airports in 2018. The Tocumen International Airport in Panama operates an airport-wide surveillance system using hundreds of live face recognition cameras to identify wanted individuals passing through the airport.\n\nThe U.S. Department of State operates one of the largest face recognition systems in the world with a database of 117 million American adults, with photos typically drawn from driver's license photos. Although it is still far from completion, it is being put to use in certain cities to give clues as to who was in the photo. The FBI uses the photos as an investigative tool not for positive identification.\n\nIn recent years Maryland has used face recognition by comparing people's faces to their driver's license photos. The system drew controversy when it was used in Baltimore to arrest unruly protesters after the death of Freddie Gray in police custody. Many other states are using or developing a similar system however some states have laws prohibiting its use.\n\nThe FBI has also instituted its Next Generation Identification program to include face recognition, as well as more traditional biometrics like fingerprints and iris scans, which can pull from both criminal and civil databases.\n\nIn 2017, Time & Attendance company ClockedIn released facial recognition as a form of attendance tracking for businesses and organisations looking to have a more automated system of keeping track of hours worked as well as for security and health and safety control.\n\nIn May 2017, a man was arrested using an automatic facial recognition (AFR) system mounted on a van operated by the South Wales Police. Ars Technica reported that \"this appears to be the first time [AFR] has led to an arrest\". \n\nIn addition to being used for security systems, authorities have found a number of other applications for face recognition systems. While earlier post-9/11 deployments were well publicized trials, more recent deployments are rarely written about due to their covert nature.\n\nAt Super Bowl XXXV in January 2001, police in Tampa Bay, Florida used Viisage face recognition software to search for potential criminals and terrorists in attendance at the event. 19 people with minor criminal records were potentially identified.\n\nIn the 2000 Mexican presidential election, the Mexican government employed face recognition software to prevent voter fraud. Some individuals had been registering to vote under several different names, in an attempt to place multiple votes. By comparing new face images to those already in the voter database, authorities were able to reduce duplicate registrations. Similar technologies are being used in the United States to prevent people from obtaining fake identification cards and driver’s licenses.\n\nFace recognition has been leveraged as a form of biometric authentication for various computing platforms and devices; Android 4.0 \"Ice Cream Sandwich\" added facial recognition using a smartphone's front camera as a means of unlocking devices, while Microsoft introduced face recognition login to its Xbox 360 video game console through its Kinect accessory, as well as Windows 10 via its \"Windows Hello\" platform (which requires an infrared-illuminated camera). Apple's iPhone X smartphone introduced facial recognition to the product line with its \"Face ID\" platform, which uses an infrared illumination system.\n\nFace recognition systems have also been used by photo management software to identify the subjects of photographs, enabling features such as searching images by person, as well as suggesting photos to be shared with a specific contact if their presence were detected in a photo.\n\nAmong the different biometric techniques, face recognition may not be most reliable and efficient. However, one key advantage is that it does not require the cooperation of the test subject to work. Properly designed systems installed in airports, multiplexes, and other public places can identify individuals among the crowd, without passers-by even being aware of the system. Other biometrics like fingerprints, iris scans, and speech recognition cannot perform this kind of mass identification. However, questions have been raised on the effectiveness of face recognition software in cases of railway and airport security.\n\nRalph Gross, a researcher at the Carnegie Mellon Robotics Institute in 2008, describes one obstacle related to the viewing angle of the face: \"Face recognition has been getting pretty good at full frontal faces and 20 degrees off, but as soon as you go towards profile, there've been problems.\" Besides the pose variations, low-resolution face images are also very hard to recognize. This is one of the main obstacles of face recognition in surveillance systems. \n\nFace recognition software generally doesn't do as well in identifying minorities when most of the subjects used in training the technology were from the majority group. Other conditions where face recognition does not work well include poor lighting, sunglasses, hats, scarves, beards, long hair, makeup or other objects partially covering the subject’s face.\n\nFace recognition is less effective if facial expressions vary. A big smile can render the system less effective. For instance: Canada, in 2009, allowed only neutral facial expressions in passport photos.\n\nThere is also inconstancy in the datasets used by researchers. Researchers may use anywhere from several subjects to scores of subjects, and a few hundred images to thousands of images. It is important for researchers to make available the datasets they used to each other, or have at least a standard dataset.\n\nCritics of the technology complain that the London Borough of Newham scheme has, , never recognized a single criminal, despite several criminals in the system's database living in the Borough and the system having been running for several years. \"Not once, as far as the police know, has Newham's automatic face recognition system spotted a live target.\" This information seems to conflict with claims that the system was credited with a 34% reduction in crime (hence why it was rolled out to Birmingham also). However it can be explained by the notion that when the public is regularly told that they are under constant video surveillance with advanced face recognition technology, this fear alone can reduce the crime rate, whether the face recognition system technically works or does not. This has been the basis for several other face recognition based security systems, where the technology itself does not work particularly well but the user's perception of the technology does.\n\nAn experiment in 2002 by the local police department in Tampa, Florida, had similarly disappointing results.\n\nA system at Boston's Logan Airport was shut down in 2003 after failing to make any matches during a two-year test period.\n\nSystems are often advertised as having accuracy near 100%, this is misleading as the studies often uses much smaller sample sizes than would be necessary for large scale applications. Because facial recognition is not completely accurate, it creates a list of potential matches. A human operator must then look through these potential matches and studies show the operators pick the correct match out of the list only about half the time. This causes the issue of targeting the wrong suspect.\n\nCivil rights right organizations and privacy campaigners such as the Electronic Frontier Foundation and the ACLU express concern that privacy is being compromised by the use of surveillance technologies. Some fear that it could lead to a “total surveillance society,” with the government and other authorities having the ability to know the whereabouts and activities of all citizens around the clock. This knowledge has been, is being, and could continue to be deployed to prevent the lawful exercise of rights of citizens to criticize those in office, specific government policies or corporate practices. Many centralized power structures with such surveillance capabilities have abused their privileged access to maintain control of the political and economic apparatus, and to curtail populist reforms.\n\nFace recognition can be used not just to identify an individual, but also to unearth other personal data associated with an individual – such as other photos featuring the individual, blog posts, social networking profiles, Internet behavior, travel patterns, etc. – all through facial features alone. Concerns have been raised over who would have access to the knowledge of one's whereabouts and people with them at any given time. Moreover, individuals have limited ability to avoid or thwart face recognition tracking unless they hide their faces. This fundamentally changes the dynamic of day-to-day privacy by enabling any marketer, government agency, or random stranger to secretly collect the identities and associated personal information of any individual captured by the face recognition system. Consumers may not understand or be aware of what their data is being used for, which denies them the ability to consent to how their personal information gets shared. \n\nSocial media web sites such as Facebook have very large numbers of photographs of people, annotated with names. This represents a database which may be abused by governments for face recognition purposes. Face recognition was used in Russia to harass women allegedly involved in online pornography. In Russia there is an app 'FindFace' which can identify faces with about 70% accuracy using the social media app called VK. This app would not be possible in other countries which do not use VK as their social media platform photos are not stored the same way as with VK.\n\nIn July 2012, a hearing was held before the Subcommittee on Privacy, Technology and the Law of the Committee on the Judiciary, United States Senate, to address issues surrounding what face recognition technology means for privacy and civil liberties.\n\nIn 2014, the National Telecommunications and Information Association (NTIA) began a multi-stakeholder process to engage privacy advocates and industry representatives to establish guidelines regarding the use of face recognition technology by private companies. In June 2015, privacy advocates left the bargaining table over what they felt was an impasse based on the industry representatives being unwilling to agree to consent requirements for the collection of face recognition data. The NTIA and industry representatives continued without the privacy representatives, and draft rules are expected to be presented in the spring of 2016.\n\nStates have begun enacted legislation to protect citizen's biometric data privacy. Illinois enacted the Biometric Information Privacy Act in 2008. Facebook's DeepFace has become the subject of several class action lawsuits under the Biometric Information Privacy Act, with claims alleging that Facebook is collecting and storing face recognition data of its users without obtaining informed consent, in direct violation of the Biometric Information Privacy Act. The most recent case was dismissed in January 2016 because the court lacked jurisdiction. Therefore, it is still unclear if the Biometric Information Privacy Act will be effective in protecting biometric data privacy rights.\n\nIn July 2015, the United States Government Accountability Office conducted a Report to the Ranking Member, Subcommittee on Privacy, Technology and the Law, Committee on the Judiciary, U.S. Senate. The report discussed facial recognition technology's commercial uses, privacy issues, and the applicable federal law. It states that previously, issues concerning facial recognition technology were discussed and represent the need for updated federal privacy laws that continually match the degree and impact of advanced technologies. Also, that some industry, government, and privacy organizations are in the process of developing, or have developed, \"voluntary privacy guidelines\". These guidelines vary between the groups, but overall aim to gain consent and inform citizens of the intended use of facial recognition technology. This helps counteract the privacy issues that arise when citizens are unaware where their personal, privacy data gets put to use as the report indicates as a prevalent issue. \n\nPioneers of automated face recognition include Woody Bledsoe, Helen Chan Wolf, and Charles Bisson.\n\nDuring 1964 and 1965, Bledsoe, along with Helen Chan and Charles Bisson, worked on using the computer to recognize human faces (Bledsoe 1966a, 1966b; Bledsoe and Chan 1965). He was proud of this work, but because the funding was provided by an unnamed intelligence agency that did not allow much publicity, little of the work was published. Given a large database of images (in effect, a book of mug shots) and a photograph, the problem was to select from the database a small set of records such that one of the image records matched the photograph. The success of the method could be measured in terms of the ratio of the answer list to the number of records in the database. Bledsoe (1966a) described the following difficulties:\n\nThis project was labeled man-machine because the human extracted the coordinates of a set of features from the photographs, which were then used by the computer for recognition. Using a graphics tablet (GRAFACON or RAND TABLET), the operator would extract the coordinates of features such as the center of pupils, the inside corner of eyes, the outside corner of eyes, point of widows peak, and so on. From these coordinates, a list of 20 distances, such as width of mouth and width of eyes, pupil to pupil, were computed. These operators could process about 40 pictures an hour. When building the database, the name of the person in the photograph was associated with the list of computed distances and stored in the computer. In the recognition phase, the set of distances was compared with the corresponding distance for each photograph, yielding a distance between the photograph and the database record. The closest records are returned.\n\nBecause it is unlikely that any two pictures would match in head rotation, lean, tilt, and scale (distance from the camera), each set of distances is normalized to represent the face in a frontal orientation. To accomplish this normalization, the program first tries to determine the tilt, the lean, and the rotation. Then, using these angles, the computer undoes the effect of these transformations on the computed distances. To compute these angles, the computer must know the three-dimensional geometry of the head. Because the actual heads were unavailable, Bledsoe (1964) used a standard head derived from measurements on seven heads.\n\nAfter Bledsoe left PRI in 1966, this work was continued at the Stanford Research Institute, primarily by Peter Hart. In experiments performed on a database of over 2000 photographs, the computer consistently outperformed humans when presented with the same recognition tasks (Bledsoe 1968). Peter Hart (1996) enthusiastically recalled the project with the exclamation, \"It really worked!\"\n\nBy about 1997, the system developed by Christoph von der Malsburg and graduate students of the University of Bochum in Germany and the University of Southern California in the United States outperformed most systems with those of Massachusetts Institute of Technology and the University of Maryland rated next. The Bochum system was developed through funding by the United States Army Research Laboratory. The software was sold as ZN-Face and used by customers such as Deutsche Bank and operators of airports and other busy locations. The software was \"robust enough to make identifications from less-than-perfect face views. It can also often see through such impediments to identification as mustaches, beards, changed hair styles and glasses—even sunglasses\".\n\nIdentix, a company out of Minnesota, has developed the software, FaceIt. FaceIt can pick out someone's face in a crowd and compare it to databases worldwide to recognize and put a name to a face. The software is written to detect multiple features on the human face. It can detect the distance between the eyes, width of the nose, shape of cheekbones, length of jawlines and many more facial features. The software does this by putting the image of the face on a faceprint, a numerical code that represents the human face. Face recognition software used to have to rely on a 2D image with the person almost directly facing the camera. Now, with FaceIt, a 3D image can be compared to a 2D image by choosing 3 specific points off of the 3D image and converting it into a 2D image using a special algorithm that can be scanned through almost all databases.\n\nIn 2006, the performance of the latest face recognition algorithms were evaluated in the Face Recognition Grand Challenge (FRGC). High-resolution face images, 3-D face scans, and iris images were used in the tests. The results indicated that the new algorithms are 10 times more accurate than the face recognition algorithms of 2002 and 100 times more accurate than those of 1995. Some of the algorithms were able to outperform human participants in recognizing faces and could uniquely identify identical twins.\n\nU.S. Government-sponsored evaluations and challenge problems have helped spur over two orders-of-magnitude in face-recognition system performance. Since 1993, the error rate of automatic face-recognition systems has decreased by a factor of 272. The reduction applies to systems that match people with face images captured in studio or mugshot environments. In Moore's law terms, the error rate decreased by one-half every two years.\n\nLow-resolution images of faces can be enhanced using face hallucination.\n\nFacial recognition systems have been used for emotion recognition In 2016 Facebook acquired emotion detection startup FacioMetrics.\n\nIn January 2013 Japanese researchers from the National Institute of Informatics created 'privacy visor' glasses that uses nearly infrared light to make the face underneath it unrecognizable to face recognition software. The latest version uses a titanium frame, light-reflective material and a mask which uses angles and patterns to disrupt facial recognition technology through both absorbing and bouncing back light sources. In December 2016 a form of anti-CCTV and facial recognition sunglasses called 'reflectacles' were invented by a custom-spectacle-craftsmen based in Chicago named Scott Urban. They reflect infrared and, optionally, visible light which makes the users face a white blur to cameras.\n\nAnother method to protect from facial recognition systems are specific haircuts and make-up patterns that prevent the used algorithms to detect a face.\n\n\n\n", "id": "602401", "title": "Facial recognition system"}
{"url": "https://en.wikipedia.org/wiki?curid=265421", "text": "Inverted pendulum\n\nAn inverted pendulum is a pendulum that has its center of mass above its pivot point. It is unstable and without additional help will fall over. It can be suspended stably in this inverted position by using a control system to monitor the angle of the pole and move the pivot point horizontally back under the center of mass when it starts to fall over, keeping it balanced. The inverted pendulum is a classic problem in dynamics and control theory and is used as a benchmark for testing control strategies. It is often implemented with the pivot point mounted on a cart that can move horizontally under control of an electronic servo system as shown in the photo; this is called a cart and pole apparatus. Most applications limit the pendulum to 1 degree of freedom by affixing the pole to an axis of rotation. Whereas a normal pendulum is stable when hanging downwards, an inverted pendulum is inherently unstable, and must be actively balanced in order to remain upright; this can be done either by applying a torque at the pivot point, by moving the pivot point horizontally as part of a feedback system, changing the rate of rotation of a mass mounted on the pendulum on an axis parallel to the pivot axis and thereby generating a net torque on the pendulum, or by oscillating the pivot point vertically. A simple demonstration of moving the pivot point in a feedback system is achieved by balancing an upturned broomstick on the end of one's finger. \n\nA second type of inverted pendulum is a tiltmeter for tall structures, which consists of a wire anchored to the bottom of the foundation and attached to a float in a pool of oil at the top of the structure that has devices for measuring movement of the neutral position of the float away from its original position.\n\nA pendulum with its bob hanging directly below the support pivot is at a stable equilibrium point; there is no torque on the pendulum so it will remain motionless, and if displaced from this position will experience a restoring torque which returns it toward the equilibrium position. A pendulum with its bob in an inverted position, supported on a rigid rod directly above the pivot, 180° from its stable equilibrium position, is at an unstable equilibrium point. At this point again there is no torque on the pendulum, but the slightest displacement away from this position will cause a gravitation torque on the pendulum which will accelerate it away from equilibrium, and it will fall over. \nIn order to stabilize a pendulum in this inverted position, a feedback control system can be used, which monitors the pendulum's angle and moves the position of the pivot point sideways when the pendulum starts to fall over, to keep it balanced. The inverted pendulum is a classic problem in dynamics and control theory and is widely used as a benchmark for testing control algorithms (PID controllers, state space representation, neural networks, fuzzy control, genetic algorithms, etc.). Variations on this problem include multiple links, allowing the motion of the cart to be commanded while maintaining the pendulum, and balancing the cart-pendulum system on a see-saw. The inverted pendulum is related to rocket or missile guidance, where the center of gravity is located behind the center of drag causing aerodynamic instability. The understanding of a similar problem can be shown by simple robotics in the form of a balancing cart. Balancing an upturned broomstick on the end of one's finger is a simple demonstration, and the problem is solved by self-balancing personal transporters such as the Segway PT, the self-balancing hoverboard and the self-balancing unicycle.\n\nAnother way that an inverted pendulum may be stabilized, without any feedback or control mechanism, is by oscillating the pivot rapidly up and down. This is called Kapitza's pendulum. If the oscillation is sufficiently strong (in terms of its acceleration and amplitude) then the inverted pendulum can recover from perturbations in a strikingly counterintuitive manner. If the driving point moves in simple harmonic motion, the pendulum's motion is described by the Mathieu equation.\n\nThe equations of motion of inverted pendulums are dependent on what constraints are placed on the motion of the pendulum. Inverted pendulums can be created in various configurations resulting in a number of Equations of Motion describing the behavior of the pendulum.\n\nIn a configuration where the pivot point of the pendulum is fixed in space, the equation of motion is similar to that for an uninverted pendulum. The equation of motion below assumes no friction or any other resistance to movement, a rigid massless rod, and the restriction to 2-dimensional movement.\n\nWhere formula_2 is the angular acceleration of the pendulum, formula_3 is the standard gravity on the surface of the Earth, formula_4 is the length of the pendulum, and formula_5 is the angular displacement measured from the equilibrium position.\n\nWhen added to both sides, it will have the same sign as the angular acceleration term:\n\nThus, the inverted pendulum will accelerate away from the vertical unstable equilibrium in the direction initially displaced, and the acceleration is inversely proportional to the length. Tall pendulums fall more slowly than short ones.\n\nDerivation using torque and moment of inertia:\nThe pendulum is assumed to consist of a point mass, of mass formula_7, affixed to the end of a massless rigid rod, of length formula_4, attached to a pivot point at the end opposite the point mass.\n\nThe net torque of the system must equal the moment of inertia times the angular acceleration:\nThe torque due to gravity providing the net torque:\nWhere formula_11 is the angle measured from the inverted equilibrium position.\n\nThe resulting equation:\n\nThe moment of inertia for a point mass:\nIn the case of the inverted pendulum the radius is the length of the rod, formula_14.\n\nSubstituting in formula_15\n\nMass and formula_17 is divided from each side resulting in:\n\nAn inverted pendulum on a cart consists of a mass formula_19 at the top of a pole of length formula_14 pivoted on a horizontally moving base as shown in the adjacent image. The cart is restricted to linear motion and is subject to forces resulting in or hindering motion.\n\nThe essentials of stabilizing the inverted pendulum can be summarized qualitatively in three steps.\n1. If the tilt angle formula_21 is to the right, the cart must accelerate to the right and vice versa.\n\n2. The position of the cart formula_22 relative to track center is stabilized by slightly modulating the null angle (the angle error that the control system tries to null) by the position of the cart, that is, null angle formula_23 where formula_24 is small. This makes the pole want to lean slightly toward track center and stabilize at track center where the tilt angle is exactly vertical. Any offset in the tilt sensor or track slope that would otherwise cause instability translates into a stable position offset. A further added offset gives position control.\n\n3. A normal pendulum subject to a moving pivot point such as a load lifted by a crane, has a peaked response at the pendulum radian frequency of formula_25. To prevent uncontrolled swinging, the frequency spectrum of the pivot motion should be suppressed near formula_26. The inverted pendulum requires the same suppression filter to achieve stability.\n\nNote that, as a consequence of the null angle modulation strategy, the position feedback is positive, that is, a sudden command to move right will produce an initial cart motion to the left followed by a move right to rebalance the pendulum. The interaction of the pendulum instability and the positive position feedback instability to produce a stable system is a feature that makes the mathematical analysis an interesting and challenging problem.\n\nThe equations of motion can be derived using Lagrange's equations. We refer to the drawing to the right where formula_27 is the angle of the pendulum of length formula_28 with respect to the vertical direction and the acting forces are gravity and an external force \"F\" in the x-direction. Define formula_29 to be the position of the cart. The Lagrangian formula_30 of the system is:\n\nwhere formula_32 is the velocity of the cart and formula_33 is the velocity of the point mass formula_34.\nformula_32 and formula_33 can be expressed in terms of x and formula_5 by writing the velocity as the first derivative of the position;\nSimplifying the expression for formula_33 leads to:\n\nThe Lagrangian is now given by:\nand the equations of motion are:\n\nsubstituting formula_45 in these equations and simplifying leads to the equations that describe the motion of the inverted pendulum:\nThese equations are nonlinear, but since the goal of a control system would be to keep the pendulum upright the equations can be linearized around formula_48.\n\nOftentimes it is beneficial to use Newton's Second Law instead of Lagrange's equations because Newton's equations give the reaction forces at the joint between the pendulum and the cart. These equations give rise to two equations for each body one in the x-direction and the other in the y-direction. The equations of motion of the cart are shown below where the LHS is the sum of the forces on the body and the RHS is the acceleration.\n\nIn the equations above formula_51 and formula_52 are reaction forces at the joint. formula_53 is the normal force applied to the cart. This second equation only depends on the vertical reaction force thus the equation can be used to solve for the normal force. The first equation can be used to solve for the horizontal reaction force. In order to complete the equations of motion, the acceleration of the point mass attached to the pendulum must be computed. The position of the point mass can be given in inertial coordinates as\n\nTaking two derivatives yields the acceleration vector in the inertial reference frame.\n\nThen, using Newton's second law, two equations can be written in the x-direction and the y-direction. Note that the reaction forces are positive as applied to the pendulum and negative when applied to the cart. This is due to Newton's Third Law.\n\nThe first equation allows yet another way to compute the horizontal reaction force in the event the applied force formula_58 is not known. The second equation can be used to solve for the vertical reaction force. The first equation of motion is derived by substituting formula_59 into formula_60 which yields\n\nBy inspection this equation is identical to the result from Lagrange's Method. In order to obtain the second equation the pendulum equation of motion must be dotted with a unit vector which runs perpendicular to the pendulum at all times and is typically noted as the x-coordinate of the body frame. In inertial coordinates this vector can be written using a simple 2-D coordinate transformation\n\nThe pendulum equation of motion written in vector form is formula_63. Dotting formula_64 with both sides yields the following on the LHS (note that a transpose is the same as a dot product)\n\nIn the above equation the relationship between body frame components of the reaction forces and inertial frame components of reaction forces is used. The assumption that the bar connecting the point mass to the cart is massless implies that this bar cannot transfer any load perpendicular to the bar. Thus, the inertial frame components of the reaction forces can be written simply as formula_66 which signifies that the bar can only transfer loads along the axis of the bar itself. This gives rise to another equation which can be used to solve for the tension in the rod itself\n\nThe RHS of the equation is computed similarly by dotting formula_64 with the acceleration of the pendulum. The result (after some simplification) is shown below.\n\nCombining the LHS with the RHS and dividing through by m yields\n\nwhich again is identical to the result of Lagrange's method. The benefit of using Newton's method is that all reaction forces are revealed to ensure that nothing will be damaged.\nAn inverted pendulum in which the pivot is oscillated rapidly up and down can be stable in the inverted position. This is called Kapitza's pendulum, after Russian physicist Pyotr Kapitza who first analysed it. The equation of motion for a pendulum connected to a massless, oscillating base is derived the same way as with the pendulum on the cart. The position of the point mass is now given by:\nand the velocity is found by taking the first derivative of the position:\n\nThe Lagrangian for this system can be written as:\nand the equation of motion follows from:\nresulting in:\nIf \"y\" represents a simple harmonic motion, formula_76, the following differential equation is:\n\nThis equation does not have elementary closed-form solutions, but can be explored in a variety of ways. It is closely approximated by the Mathieu equation, for instance, when the amplitude of oscillations are small. Analyses show that the pendulum stays upright for fast oscillations. The first plot shows that when formula_78 is a slow oscillation, the pendulum quickly falls over when disturbed from the upright position. The angle formula_5 exceeds 90° after a short time, which means the pendulum has fallen on the ground. If formula_78 is a fast oscillation the pendulum can be kept stable around the vertical position. The second plot shows that when disturbed from the vertical position, the pendulum now starts an oscillation around the vertical position (formula_81). The deviation from the vertical position stays small, and the pendulum doesn't fall over.\nAchieving stability of an inverted pendulum has become a common engineering challenge for researchers. There are different variations of the inverted pendulum on a cart ranging from a rod on a cart to a multiple segmented inverted pendulum on a cart. Another variation places the inverted pendulum's rod or segmented rod on the end of a rotating assembly. In both, (the cart and rotating system) the inverted pendulum can only fall in a plane. The inverted pendulums in these projects can either be required to only maintain balance after an equilibrium position is achieved or be able to achieve equilibrium by itself. Another platform is a two-wheeled balancing inverted pendulum. The two wheeled platform has the ability to spin on the spot offering a great deal of maneuverability. Yet another variation balances on a single point. A spinning top, a unicycle, or an inverted pendulum atop a spherical ball all balance on a single point. As derived above the inverted pendulum can also be achieved by having a vertically oscillating base.\n\nArguably the most prevalent example of a stabilized inverted pendulum is a human being. A person standing upright acts as an inverted pendulum with his feet as the pivot, and without constant small muscular adjustments would fall over. The human nervous system contains an unconscious feedback control system, the sense of balance or righting reflex, that uses proprioceptive input from the eyes, muscles and joints, and orientation input from the vestibular system consisting of the three semicircular canals in the inner ear, and two otolith organs, to make continual small adjustments to the skeletal muscles to keep us standing upright. Walking, running, or balancing on one leg puts additional demands on this system. Certain diseases and alcohol or drug intoxication can interfere with this reflex, causing dizziness and disequilibration, an inability to stand upright. A field sobriety test used by police to test drivers for the influence of alcohol or drugs, tests this reflex for impairment. \n\nSome simple examples include balancing brooms or meter sticks by hand. \nThe inverted pendulum has been employed in various devices and trying to balance an inverted pendulum presents a unique engineering problem for researchers. The inverted pendulum was a central component in the design of several early seismometers due to its inherent instability resulting in a measurable response to any disturbance.\n\nThe inverted pendulum model has been used in some recent personal transportation vehicles, the two-wheeled self-balancing scooters such as the Segway PT and self-balancing hoverboard. These devices are kinematically unstable and use an electronic feedback servo system to keep them upright.\n\n\n\n\n", "id": "265421", "title": "Inverted pendulum"}
{"url": "https://en.wikipedia.org/wiki?curid=29468", "text": "Speech recognition\n\nSpeech recognition is the inter-disciplinary sub-field of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language into text by computers. It is also known as \"automatic speech recognition\" (ASR), \"computer speech recognition\", or just \"speech to text\" (STT). It incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields.\n\nSome speech recognition systems require \"training\" (also called \"enrollment\") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called \"speaker independent\" systems. Systems that use training are called \"speaker dependent\".\n\nSpeech recognition applications include voice user interfaces such as voice dialing (e.g. \"Call home\"), call routing (e.g. \"I would like to make a collect call\"), domotic appliance control, search (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input).\n\nThe term \"voice recognition\" or \"speaker identification\" refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.\n\nFrom the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems. These speech industry players include Google, Microsoft, IBM, Baidu, Apple, Amazon, Nuance, SoundHound, IflyTek, CDAC, Speechmatics many of which have publicized the core technology in their speech recognition systems as being based on deep learning.\n\nIn 1952 three Bell Labs researchers built a system for single-speaker digit recognition. Their system worked by locating the formants in the power spectrum of each utterance. The 1950s era technology was limited to single-speaker systems with vocabularies of around ten words.\n\nGunnar Fant developed the source-filter model of speech production and published it in 1960, which proved to be a useful model of speech production.\n\nUnfortunately, funding at Bell Labs dried up for several years when, in 1969, the influential John Pierce wrote an open letter that was critical of speech recognition research. Pierce defunded speech recognition research at Bell Labs where no research on speech recognition was done until Pierce retired and James L. Flanagan took over.\n\nRaj Reddy was the first person to take on continuous speech recognition as a graduate student at Stanford University in the late 1960s. Previous systems required the users to make a pause after each word. Reddy's system was designed to issue spoken commands for the game of chess.\n\nAlso around this time Soviet researchers invented the dynamic time warping (DTW) algorithm and used it to create a recognizer capable of operating on a 200-word vocabulary. The DTW algorithm processed the speech signal by dividing it into short frames, e.g. 10ms segments, and processing each frame as a single unit. Although DTW would be superseded by later algorithms, the technique of dividing the signal into frames would carry on. Achieving speaker independence was a major unsolved goal of researchers during this time period.\n\nIn 1971, DARPA funded five years of speech recognition research through its Speech Understanding Research program with ambitious end goals including a minimum vocabulary size of 1,000 words. BBN, IBM, Carnegie Mellon and Stanford Research Institute all participated in the program. The government funding revived speech recognition research that had been largely abandoned in the United States after John Pierce's letter.\n\nDespite the fact that CMU's Harpy system met the original goals of the program, many predictions turned out to be nothing more than hype, disappointing DARPA administrators. This disappointment led to DARPA not continuing the funding. Several innovations happened during this time, such as the invention of beam search for use in CMU's Harpy system. The field also benefited from the discovery of several algorithms in other fields such as linear predictive coding and cepstral analysis.\n\nDuring the late 1960s Leonard Baum developed the mathematics of Markov chains at the Institute for Defense Analysis. At CMU, Raj Reddy's students James Baker and Janet M. Baker began using the Hidden Markov Model (HMM) for speech recognition. James Baker had learned about HMMs from a summer job at the Institute of Defense Analysis during his undergraduate education. The use of HMMs allowed researchers to combine different sources of knowledge, such as acoustics, language, and syntax, in a unified probabilistic model.\n\nUnder Fred Jelinek's lead, IBM created a voice activated typewriter called Tangora, which could handle a 20,000 word vocabulary by the mid 1980s. Jelinek's statistical approach put less emphasis on emulating the way the human brain processes and understands speech in favor of using statistical modeling techniques like HMMs. (Jelinek's group independently discovered the application of HMMs to speech.) This was controversial with linguists since HMMs are too simplistic to account for many common features of human languages. However, the HMM proved to be a highly useful way for modeling speech and replaced dynamic time warping to become the dominant speech recognition algorithm in the 1980s. IBM had a few competitors including Dragon Systems founded by James and Janet M. Baker in 1982. The 1980s also saw the introduction of the n-gram language model. Katz introduced the back-off model in 1987, which allowed language models to use multiple length n-grams. During the same time, also CSELT was using HMM (the diphonies were studied since 1980) to recognize language like Italian. At the same time, CSELT led a series of European projects (Esprit I, II), and summarized the state-of-the-art in a book, later (2013) reprinted.\n\nMuch of the progress in the field is owed to the rapidly increasing capabilities of computers. At the end of the DARPA program in 1976, the best computer available to researchers was the PDP-10 with 4 MB ram. Using these computers it could take up to 100 minutes to decode just 30 seconds of speech. A few decades later, researchers had access to tens of thousands of times as much computing power. As the technology advanced and computers got faster, researchers began tackling harder problems such as larger vocabularies, speaker independence, noisy environments and conversational speech. In particular, this shifting to more difficult tasks has characterized DARPA funding of speech recognition since the 1980s. For example, progress was made on speaker independence first by training on a larger variety of speakers and then later by doing explicit speaker adaptation during decoding. Further reductions in word error rate came as researchers shifted acoustic models to be discriminative instead of using maximum likelihood models.\n\nIn the mid-Eighties new speech recognition microprocessors were released: for example RIPAC, an independent-speaker recognition (for continuous speech) chip tailored for telephone services, was presented in the Netherlands in 1986. It was designed by CSELT/Elsag and manufactured by SGS.\n\nThe 1990s saw the first introduction of commercially successful speech recognition technologies. Two of the earliest products were Dragon Dictate, a consumer product released in 1990 and originally priced at $9,000, and a recognizer from Kurzweil Applied Intelligence released in 1987. AT&T deployed the Voice Recognition Call Processing service in 1992 to route telephone calls without the use of a human operator. The technology was developed by Lawrence Rabiner and others at Bell Labs.\nBy this point, the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary. Raj Reddy's former student, Xuedong Huang, developed the Sphinx-II system at CMU. The Sphinx-II system was the first to do speaker-independent, large vocabulary, continuous speech recognition and it had the best performance in DARPA's 1992 evaluation. Handling continuous speech with a large vocabulary was a major milestone in the history of speech recognition. Huang went on to found the speech recognition group at Microsoft in 1993. Raj Reddy's student Kai-Fu Lee joined Apple where, in 1992, he helped develop a speech interface prototype for the Apple computer known as Casper.\n\nLernout & Hauspie, a Belgium-based speech recognition company, acquired several other companies, including Kurzweil Applied Intelligence in 1997 and Dragon Systems in 2000. The L&H speech technology was used in the Windows XP operating system. L&H was an industry leader until an accounting scandal brought an end to the company in 2001. The speech technology from L&H was bought by ScanSoft which became Nuance in 2005. Apple originally licensed software from Nuance to provide speech recognition capability to its digital assistant Siri.\n\nIn the 2000s DARPA sponsored two speech recognition programs: Effective Affordable Reusable Speech-to-Text (EARS) in 2002 and Global Autonomous Language Exploitation (GALE). Four teams participated in the EARS program: IBM, a team led by BBN with LIMSI and Univ. of Pittsburgh, \nCambridge University, and a team composed of ISCI, SRI and University of Washington. EARS funded the collection of the Switchboard telephone speech corpus containing 260 hours of recorded conversations from over 500 speakers. The GALE program focused on Arabic and Mandarin broadcast news speech. Google's first effort at speech recognition came in 2007 after hiring some researchers from Nuance. The first product was GOOG-411, a telephone based directory service. The recordings from GOOG-411 produced valuable data that helped Google improve their recognition systems. Google voice search is now supported in over 30 languages.\n\nIn the United States, the National Security Agency has made use of a type of speech recognition for keyword spotting since at least 2006. This technology allows analysts to search through large volumes of recorded conversations and isolate mentions of keywords. Recordings can be indexed and analysts can run queries over the database to find conversations of interest. Some government research programs focused on intelligence applications of speech recognition, e.g. DARPA's EARS's program and IARPA's Babel program.\n\nIn the early 2000s, speech recognition was still dominated by traditional approaches such as Hidden Markov Models combined with feedforward artificial neural networks.\nToday, however, many aspects of speech recognition have been taken over by a deep learning method called Long short-term memory (LSTM), \na recurrent neural network published by Sepp Hochreiter & Jürgen Schmidhuber in 1997. LSTM RNNs avoid the vanishing gradient problem and can learn \"Very Deep Learning\" tasks that require memories of events that happened thousands of discrete time steps ago, which is important for speech.\nAround 2007, LSTM trained by Connectionist Temporal Classification (CTC) started to outperform traditional speech recognition in certain applications. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to all smartphone users.\n\nThe use of deep feedforward (non-recurrent) networks for acoustic modeling was introduced during later part of 2009 by Geoffrey Hinton and his students at University of Toronto and by Li Deng and colleagues at Microsoft Research, initially in the collaborative work between Microsoft and University of Toronto which was subsequently expanded to include IBM and Google (hence \"The shared views of four research groups\" subtitle in their 2012 review paper). A Microsoft research executive called this innovation \"the most dramatic change in accuracy since 1979.\" In contrast to the steady incremental improvements of the past few decades, the application of deep learning decreased word error rate by 30%. This innovation was quickly adopted across the field. Researchers have begun to use deep learning techniques for language modeling as well.\n\nIn the long history of speech recognition, both shallow form and deep form (e.g. recurrent nets) of artificial neural networks had been explored for many years during 1980s, 1990s and a few years into the 2000s.\nBut these methods never won over the non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.\nA number of key difficulties had been methodologically analyzed in the 1990s, including gradient diminishing and weak temporal correlation structure in the neural predictive models.\nAll these difficulties were in addition to the lack of big training data and big computing power in these early days. Most speech recognition researchers who understood such barriers hence subsequently moved away from neural nets to pursue generative modeling approaches until the recent resurgence of deep learning starting around 2009–2010 that had overcome all these difficulties. Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups (University of Toronto, Microsoft, Google, and IBM) ignited a renaissance of applications of deep feedforward neural networks to speech recognition.\n\nBoth acoustic modeling and language modeling are important parts of modern statistically-based speech recognition algorithms. Hidden Markov models (HMMs) are widely used in many systems. Language modeling is also used in many other natural language processing applications such as document classification or statistical machine translation.\n\nModern general-purpose speech recognition systems are based on Hidden Markov Models. These are statistical models that output a sequence of symbols or quantities. HMMs are used in speech recognition because a speech signal can be viewed as a piecewise stationary signal or a short-time stationary signal. In a short time-scale (e.g., 10 milliseconds), speech can be approximated as a stationary process. Speech can be thought of as a Markov model for many stochastic purposes.\n\nAnother reason why HMMs are popular is because they can be trained automatically and are simple and computationally feasible to use. In speech recognition, the hidden Markov model would output a sequence of \"n\"-dimensional real-valued vectors (with \"n\" being a small integer, such as 10), outputting one of these every 10 milliseconds. The vectors would consist of cepstral coefficients, which are obtained by taking a Fourier transform of a short time window of speech and decorrelating the spectrum using a cosine transform, then taking the first (most significant) coefficients. The hidden Markov model will tend to have in each state a statistical distribution that is a mixture of diagonal covariance Gaussians, which will give a likelihood for each observed vector. Each word, or (for more general speech recognition systems), each phoneme, will have a different output distribution; a hidden Markov model for a sequence of words or phonemes is made by concatenating the individual trained hidden Markov models for the separate words and phonemes.\n\nDescribed above are the core elements of the most common, HMM-based approach to speech recognition. Modern speech recognition systems use various combinations of a number of standard techniques in order to improve results over the basic approach described above. A typical large-vocabulary system would need context dependency for the phonemes (so phonemes with different left and right context have different realizations as HMM states); it would use cepstral normalization to normalize for different speaker and recording conditions; for further speaker normalization it might use vocal tract length normalization (VTLN) for male-female normalization and maximum likelihood linear regression (MLLR) for more general speaker adaptation. The features would have so-called delta and delta-delta coefficients to capture speech dynamics and in addition might use heteroscedastic linear discriminant analysis (HLDA); or might skip the delta and delta-delta coefficients and use splicing and an LDA-based projection followed perhaps by heteroscedastic linear discriminant analysis or a global semi-tied co variance transform (also known as maximum likelihood linear transform, or MLLT). Many systems use so-called discriminative training techniques that dispense with a purely statistical approach to HMM parameter estimation and instead optimize some classification-related measure of the training data. Examples are maximum mutual information (MMI), minimum classification error (MCE) and minimum phone error (MPE).\n\nDecoding of the speech (the term for what happens when the system is presented with a new utterance and must compute the most likely source sentence) would probably use the Viterbi algorithm to find the best path, and here there is a choice between dynamically creating a combination hidden Markov model, which includes both the acoustic and language model information, and combining it statically beforehand (the finite state transducer, or FST, approach).\n\nA possible improvement to decoding is to keep a set of good candidates instead of just keeping the best candidate, and to use a better scoring function (re scoring) to rate these good candidates so that we may pick the best one according to this refined score. The set of candidates can be kept either as a list (the N-best list approach) or as a subset of the models (a lattice). Re scoring is usually done by trying to minimize the Bayes risk (or an approximation thereof): Instead of taking the source sentence with maximal probability, we try to take the sentence that minimizes the expectancy of a given loss function with regards to all possible transcriptions (i.e., we take the sentence that minimizes the average distance to other possible sentences weighted by their estimated probability). The loss function is usually the Levenshtein distance, though it can be different distances for specific tasks; the set of possible transcriptions is, of course, pruned to maintain tractability. Efficient algorithms have been devised to re score lattices represented as weighted finite state transducers with edit distances represented themselves as a finite state transducer verifying certain assumptions.\n\nDynamic time warping is an approach that was historically used for speech recognition but has now largely been displaced by the more successful HMM-based approach.\n\nDynamic time warping is an algorithm for measuring similarity between two sequences that may vary in time or speed. For instance, similarities in walking patterns would be detected, even if in one video the person was walking slowly and if in another he or she were walking more quickly, or even if there were accelerations and deceleration during the course of one observation. DTW has been applied to video, audio, and graphics – indeed, any data that can be turned into a linear representation can be analyzed with DTW.\n\nA well-known application has been automatic speech recognition, to cope with different speaking speeds. In general, it is a method that allows a computer to find an optimal match between two given sequences (e.g., time series) with certain restrictions. That is, the sequences are \"warped\" non-linearly to match each other. This sequence alignment method is often used in the context of hidden Markov models.\n\nNeural networks emerged as an attractive acoustic modeling approach in ASR in the late 1980s. Since then, neural networks have been used in many aspects of speech recognition such as phoneme classification, isolated word recognition, audiovisual speech recognition, audiovisual speaker recognition and speaker adaptation.\n\nIn contrast to HMMs, neural networks make no assumptions about feature statistical properties and have several qualities making them attractive recognition models for speech recognition. When used to estimate the probabilities of a speech feature segment, neural networks allow discriminative training in a natural and efficient manner. Few assumptions on the statistics of input features are made with neural networks. However, in spite of their effectiveness in classifying short-time units such as individual phonemes and isolated words, neural networks are rarely successful for continuous recognition tasks, largely because of their lack of ability to model temporal dependencies.\n\nHowever, recently LSTM Recurrent Neural Networks (RNNs) and Time Delay Neural Networks(TDNN's) have been used which have been shown to be able to identify latent temporal dependencies and use this information to perform the task of speech recognition.\n\nDeep Neural Networks and Denoising Autoencoders were also being experimented with to tackle this problem in an effective manner.\n\nDue to the inability of feedforward Neural Networks to model temporal dependencies, an alternative approach is to use neural networks as a pre-processing e.g. feature transformation, dimensionality reduction, for the HMM based recognition.\n\nA deep feedforward neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers. Similar to shallow neural networks, DNNs can model complex non-linear relationships. DNN architectures generate compositional models, where extra layers enable composition of features from lower layers, giving a huge learning capacity and thus the potential of modeling complex patterns of speech data.\n\nA success of DNNs in large vocabulary speech recognition occurred in 2010 by industrial researchers, in collaboration with academic researchers, where large output layers of the DNN based on context dependent HMM states constructed by decision trees were adopted.\nrecent overview articles.\n\nOne fundamental principle of deep learning is to do away with hand-crafted feature engineering and to use raw features. This principle was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features, showing its superiority over the Mel-Cepstral features which contain a few stages of fixed transformation from spectrograms.\nThe true \"raw\" features of speech, waveforms, have more recently been shown to produce excellent larger-scale speech recognition results.\n\nSince 2014, there has been much research interest in \"end-to-end\" ASR. Traditional phonetic-based (i.e., all HMM-based model) approaches required separate components and training for the pronunciation, acoustic and language model. End-to-end models jointly learn all the components of the speech recognizer. This is valuable since it simplifies the training process and deployment process. For example, a n-gram language model is required for all HMM-based systems, and a typical n-gram language model often takes several gigabytes in memory making them impractical to deploy on mobile devices. Consequently, modern commercial ASR systems from Google and Apple (as of 2017) are deployed on the cloud and require a network connection as opposed to the device locally.\n\nThe first attempt of end-to-end ASR was with Connectionist Temporal Classification (CTC) based systems introduced by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014. The model consisted of recurrent neural networks and a CTC layer. Jointly, the RNN-CTC model learns the pronunciation and acoustic model together, however it is incapable of learning the language due to conditional independence assumptions similar to a HMM. Consequently, CTC models can directly learn to map speech acoustics to English characters, but the models make many common spelling mistakes and must rely on a separate language model to clean up the transcripts. Later, Baidu expanded on the work with extremely large datasets and demonstrated some commercial success in Chinese Mandarin and English. In 2016, University of Oxford presented LipNet, the first end-to-end sentence-level lip reading model, using spatiotemporal convolutions coupled with an RNN-CTC architecture, surpassing human-level performance in a restricted grammar dataset.\n\nAn alternative approach to CTC-based models are attention-based models. Attention-based ASR models were introduced simultaneously by Chan et al. of Carnegie Mellon University and Google Brain and Bahdanaua et al. of the University of Montreal in 2016. The model named \"Listen, Attend and Spell\" (LAS), literally \"listens\" to the acoustic signal, pays \"attention\" to different parts of the signal and \"spells\" out the transcript one character at a time. Unlike CTC-based models, attention-based models do not have conditional-independence assumptions and can learn all the components of a speech recognizer including the pronunciation, acoustic and language model directly. This means, during deployment, there is no need to carry around a language model making it very practical for deployment onto applications with limited memory. By the end of 2016, the attention-based models have seen considerable success including outperforming the CTC models (with or without an external language model). Various extensions have been proposed since the original LAS model. Latent Sequence Decompositions (LSD) was proposed by Carnegie Mellon University, MIT and Google Brain to directly emit sub-word units which are more natural than English characters; University of Oxford and Google DeepMind extended LAS to \"Watch, Listen, Attend and Spell\" (WLAS) to handle lip reading surpassing human-level performance.\n\nTypically a manual control input, for example by means of a finger control on the steering-wheel, enables the speech recognition system and this is signalled to the driver by an audio prompt. Following the audio prompt, the system has a \"listening window\" during which it may accept a speech input for recognition.\nSimple voice commands may be used to initiate phone calls, select radio stations or play music from a compatible smartphone, MP3 player or music-loaded flash drive. Voice recognition capabilities vary between car make and model. Some of the most recent car models offer natural-language speech recognition in place of a fixed set of commands, allowing the driver to use full sentences and common phrases. With such systems there is, therefore, no need for the user to memorize a set of fixed command words.\n\nIn the health care sector, speech recognition can be implemented in front-end or back-end of the medical documentation process. Front-end speech recognition is where the provider dictates into a speech-recognition engine, the recognized words are displayed as they are spoken, and the dictator is responsible for editing and signing off on the document. Back-end or deferred speech recognition is where the provider dictates into a digital dictation system, the voice is routed through a speech-recognition machine and the recognized draft document is routed along with the original voice file to the editor, where the draft is edited and report finalized. Deferred speech recognition is widely used in the industry currently.\n\nOne of the major issues relating to the use of speech recognition in healthcare is that the American Recovery and Reinvestment Act of 2009 (ARRA) provides for substantial financial benefits to physicians who utilize an EMR according to \"Meaningful Use\" standards. These standards require that a substantial amount of data be maintained by the EMR (now more commonly referred to as an Electronic Health Record or EHR). The use of speech recognition is more naturally suited to the generation of narrative text, as part of a radiology/pathology interpretation, progress note or discharge summary: the ergonomic gains of using speech recognition to enter structured discrete data (e.g., numeric values or codes from a list or a controlled vocabulary) are relatively minimal for people who are sighted and who can operate a keyboard and mouse.\n\nA more significant issue is that most EHRs have not been expressly tailored to take advantage of voice-recognition capabilities. A large part of the clinician's interaction with the EHR involves navigation through the user interface using menus, and tab/button clicks, and is heavily dependent on keyboard and mouse: voice-based navigation provides only modest ergonomic benefits. By contrast, many highly customized systems for radiology or pathology dictation implement voice \"macros\", where the use of certain phrases – e.g., \"normal report\", will automatically fill in a large number of default values and/or generate boilerplate, which will vary with the type of the exam – e.g., a chest X-ray vs. a gastrointestinal contrast series for a radiology system.\n\nAs an alternative to this navigation by hand, cascaded use of speech recognition and information extraction has been studied as a way to fill out a handover form for clinical proofing and sign-off. The results are encouraging, and the paper also opens data, together with the related performance benchmarks and some processing software, to the research and development community for studying clinical documentation and language-processing.\n\nProlonged use of speech recognition software in conjunction with word processors has shown benefits to short-term-memory restrengthening in brain AVM patients who have been treated with resection. Further research needs to be conducted to determine cognitive benefits for individuals whose AVMs have been treated using radiologic techniques.\n\nSubstantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in fighter aircraft. Of particular note have been the US program in speech recognition for the Advanced Fighter Technology Integration (AFTI)/F-16 aircraft (F-16 VISTA), the program in France for Mirage aircraft, and other programs in the UK dealing with a variety of aircraft platforms. In these programs, speech recognizers have been operated successfully in fighter aircraft, with applications including: setting radio frequencies, commanding an autopilot system, setting steer-point coordinates and weapons release parameters, and controlling flight display.\n\nWorking with Swedish pilots flying in the JAS-39 Gripen cockpit, Englund (2004) found recognition deteriorated with increasing g-loads. The report also concluded that adaptation greatly improved the results in all cases and that the introduction of models for breathing was shown to improve recognition scores significantly. Contrary to what might have been expected, no effects of the broken English of the speakers were found. It was evident that spontaneous speech caused problems for the recognizer, as might have been expected. A restricted vocabulary, and above all, a proper syntax, could thus be expected to improve recognition accuracy substantially.\n\nThe Eurofighter Typhoon, currently in service with the UK RAF, employs a speaker-dependent system, requiring each pilot to create a template. The system is not used for any safety-critical or weapon-critical tasks, such as weapon release or lowering of the undercarriage, but is used for a wide range of other cockpit functions. Voice commands are confirmed by visual and/or aural feedback. The system is seen as a major design feature in the reduction of pilot workload, and even allows the pilot to assign targets to his aircraft with two simple voice commands or to any of his wingmen with only five commands.\n\nSpeaker-independent systems are also being developed and are under test for the F35 Lightning II (JSF) and the Alenia Aermacchi M-346 Master lead-in fighter trainer. These systems have produced word accuracy scores in excess of 98%.\n\nThe problems of achieving high recognition accuracy under stress and noise pertain strongly to the helicopter environment as well as to the jet fighter environment. The acoustic noise problem is actually more severe in the helicopter environment, not only because of the high noise levels but also because the helicopter pilot, in general, does not wear a facemask, which would reduce acoustic noise in the microphone. Substantial test and evaluation programs have been carried out in the past decade in speech recognition systems applications in helicopters, notably by the U.S. Army Avionics Research and Development Activity (AVRADA) and by the Royal Aerospace Establishment (RAE) in the UK. Work in France has included speech recognition in the Puma helicopter. There has also been much useful work in Canada. Results have been encouraging, and voice applications have included: control of communication radios, setting of navigation systems, and control of an automated target handover system.\n\nAs in fighter applications, the overriding issue for voice in helicopters is the impact on pilot effectiveness. Encouraging results are reported for the AVRADA tests, although these represent only a feasibility demonstration in a test environment. Much remains to be done both in speech recognition and in overall speech technology in order to consistently achieve performance improvements in operational settings.\n\nTraining for air traffic controllers (ATC) represents an excellent application for speech recognition systems. Many ATC training systems currently require a person to act as a \"pseudo-pilot\", engaging in a voice dialog with the trainee controller, which simulates the dialog that the controller would have to conduct with pilots in a real ATC situation.\nSpeech recognition and synthesis techniques offer the potential to eliminate the need for a person to act as pseudo-pilot, thus reducing training and support personnel. In theory, Air controller tasks are also characterized by highly structured speech as the primary output of the controller, hence reducing the difficulty of the speech recognition task should be possible. In practice, this is rarely the case. The FAA document 7110.65 details the phrases that should be used by air traffic controllers. While this document gives less than 150 examples of such phrases, the number of phrases supported by one of the simulation vendors speech recognition systems is in excess of 500,000.\n\nThe USAF, USMC, US Army, US Navy, and FAA as well as a number of international ATC training organizations such as the Royal Australian Air Force and Civil Aviation Authorities in Italy, Brazil, and Canada are currently using ATC simulators with speech recognition from a number of different vendors.\n\nASR is now commonplace In the field of telephony, and is becoming more widespread in the field of computer gaming and simulation. Despite the high level of integration with word processing in general personal computing. However, ASR in the field of document production has not seen the expected increases in use.\n\nThe improvement of mobile processor speeds has made speech recognition practical in smartphones. Speech is used mostly as a part of a user interface, for creating predefined or custom speech commands. Leading software vendors in this field are: Google, Microsoft Corporation (Microsoft Voice Command), Digital Syphon (Sonic Extractor), LumenVox, Nuance Communications (Nuance Voice Control), Voci Technologies, VoiceBox Technology, Speech Technology Center, Vito Technologies (VITO Voice2Go), Speereo Software (Speereo Voice Translator), Verbyx VRX and SVOX.\n\nFor language learning, speech recognition can be useful for learning a second language. It can teach proper pronunciation, in addition to helping a person develop fluency with their speaking skills.\n\nStudents who are blind (see Blindness and education) or have very low vision can benefit from using the technology to convey words and then hear the computer recite them, as well as use a computer by commanding with their voice, instead of having to look at the screen and keyboard.\n\nStudents who are physically disabled or suffer from Repetitive strain injury/other injuries to the upper extremities can be relieved from having to worry about handwriting, typing, or working with scribe on school assignments by using speech-to-text programs. They can also utilize speech recognition technology to freely enjoy searching the Internet or using a computer at home without having to physically operate a mouse and keyboard.\n\nSpeech recognition can allow students with learning disabilities to become better writers. By saying the words aloud, they can increase the fluidity of their writing, and be alleviated of concerns regarding spelling, punctuation, and other mechanics of writing. Also, see Learning disability.\n\nUse of voice recognition software, in conjunction with a digital audio recorder and a personal computer running word-processing software has proven to be positive for restoring damaged short-term-memory capacity, in stroke and craniotomy individuals.\n\nPeople with disabilities can benefit from speech recognition programs. For individuals that are Deaf or Hard of Hearing, speech recognition software is used to automatically generate a closed-captioning of conversations such as discussions in conference rooms, classroom lectures, and/or religious services.\n\nSpeech recognition is also very useful for people who have difficulty using their hands, ranging from mild repetitive stress injuries to involve disabilities that preclude using conventional computer input devices. In fact, people who used the keyboard a lot and developed RSI became an urgent early market for speech recognition. Speech recognition is used in deaf telephony, such as voicemail to text, relay services, and captioned telephone. Individuals with learning disabilities who have problems with thought-to-paper communication (essentially they think of an idea but it is processed incorrectly causing it to end up differently on paper) can possibly benefit from the software but the technology is not bug proof. Also the whole idea of speak to text can be hard for intellectually disabled person's due to the fact that it is rare that anyone tries to learn the technology to teach the person with the disability.\n\nThis type of technology can help those with dyslexia but other disabilities are still in question. The effectiveness of the product is the problem that is hindering it being effective. Although a kid may be able to say a word depending on how clear they say it the technology may think they are saying another word and input the wrong one. Giving them more work to fix, causing them to have to take more time with fixing the wrong word.\n\n\nThe performance of speech recognition systems is usually evaluated in terms of accuracy and speed. Accuracy is usually rated with word error rate (WER), whereas speed is measured with the real time factor. Other measures of accuracy include Single Word Error Rate (SWER) and Command Success Rate (CSR).\n\nSpeech recognition by machine is a very complex problem, however. Vocalizations vary in terms of accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed. Speech is distorted by a background noise and echoes, electrical characteristics. Accuracy of speech recognition may vary with the following:\n\nAs mentioned earlier in this article, accuracy of speech recognition may vary depending on the following factors:\ne.g. the 10 digits \"zero\" to \"nine\" can be recognized essentially perfectly, but vocabulary sizes of 200, 5000 or 100000 may have error rates of 3%, 7% or 45% respectively.\ne.g. the 26 letters of the English alphabet are difficult to discriminate because they are confusable words (most notoriously, the E-set: \"B, C, D, E, G, P, T, V, Z\");\nan 8% error rate is considered good for this vocabulary.\nA speaker-dependent system is intended for use by a single speaker.\nA speaker-independent system is intended for use by any speaker (more difficult).\nWith isolated speech, single words are used, therefore it becomes easier to recognize the speech.\nWith discontinuous speech full sentences separated by silence are used, therefore it becomes easier to recognize the speech as well as with isolated speech. \nWith continuous speech naturally spoken sentences are used, therefore it becomes harder to recognize the speech, different from both isolated and discontinuous speech.\ne.g. Querying application may dismiss the hypothesis \"The apple is red.\" \ne.g. Constraints may be semantic; rejecting \"The apple is angry.\" \ne.g. Syntactic; rejecting \"Red is apple the.\" \nConstraints are often represented by a grammar. \nWhen a person reads it's usually in a context that has been previously prepared, but when a person uses spontaneous speech, it is difficult to recognize the speech because of the disfluencies (like \"uh\" and \"um\", false starts, incomplete sentences, stuttering, coughing, and laughter) and limited vocabulary. \nEnvironmental noise (e.g. Noise in a car or a factory) \nAcoustical distortions (e.g. echoes, room acoustics)\nSpeech recognition is a multi-levelled pattern recognition task.\ne.g. Phonemes, Words, Phrases, and Sentences;\ne.g. Known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at lower level;\nBy combining decisions probabilistically at all lower levels, and making more deterministic decisions only at the highest level, speech recognition by a machine is a process broken into several phases. Computationally, it is a problem in which a sound pattern has to be recognized or classified into a category that represents a meaning to a human. Every acoustic signal can be broken in smaller more basic sub-signals. As the more complex sound signal is broken into the smaller sub-sounds, different levels are created, where at the top level we have complex sounds, which are made of simpler sounds on lower level, and going to lower levels even more, we create more basic and shorter and simpler sounds. The lowest level, where the sounds are the most fundamental, a machine would check for simple and more probabilistic rules of what sound should represent. Once these sounds are put together into more complex sound on upper level, a new set of more deterministic rules should predict what new complex sound should represent. The most upper level of a deterministic rule should figure out the meaning of complex expressions. In order to expand our knowledge about speech recognition we need to take into a consideration neural networks. There are four steps of neural network approaches: \nFor telephone speech the sampling rate is 8000 samples per second; \ncomputed every 10 ms, with one 10 ms section called a frame;\n\nAnalysis of four-step neural network approaches can be explained by further information. Sound is produced by air (or some other medium) vibration, which we register by ears, but machines by receivers. Basic sound creates a wave which has 2 descriptions; Amplitude (how strong is it), and frequency (how often it vibrates per second).\n\nThe sound waves can be digitized: Sample a strength at short intervals like in picture above to get bunch of numbers that approximate at each time step the strength of a wave. Collection of these numbers represent analog wave. This new wave is digital. Sound waves are complicated because they superimpose one on top of each other. Like the waves would. This way they create odd-looking waves. For example, if there are two waves that interact with each other we can add them which creates new odd-looking wave.\n\n\nGiven basic sound blocks that a machine digitized, one has a bunch of numbers which describe a wave and waves describe words. Each frame has a unit block of sound, which are broken into basic sound waves and represented by numbers which, after Fourier Transform, can be statistically evaluated to set to which class of sounds it belongs. The nodes in the figure on a slide represent a feature of a sound in which a feature of a wave from the first layer of nodes to the second layer of nodes based on statistical analysis. This analysis depends on programmer's instructions. At this point, a second layer of nodes represents higher level features of a sound input which is again statistically evaluated to see what class they belong to. Last level of nodes should be output nodes that tell us with high probability what original sound really was.\n\nSpeech recognition can become a means of attack, theft, or accidental operation. For example, activation words like \"Alexa\" spoken in an audio or video broadcast or by non-owners in the same room can cause devices in audience homes and offices to start listening for input inappropriately, or possibly take an unwanted action. Another demonstrated approach is to transmit ultrasound and attempt to send commands without nearby people noticing.\n\nPopular speech recognition conferences held each year or two include SpeechTEK and SpeechTEK Europe, ICASSP, Interspeech/Eurospeech, and the IEEE ASRU. Conferences in the field of natural language processing, such as ACL, NAACL, EMNLP, and HLT, are beginning to include papers on speech processing. Important journals include the IEEE Transactions on Speech and Audio Processing (later renamed IEEE Transactions on Audio, Speech and Language Processing and since Sept 2014 renamed IEEE/ACM Transactions on Audio, Speech and Language Processing—after merging with an ACM publication), Computer Speech and Language, and Speech Communication.\n\nBooks like \"Fundamentals of Speech Recognition\" by Lawrence Rabiner can be useful to acquire basic knowledge but may not be fully up to date (1993). Another good source can be \"Statistical Methods for Speech Recognition\" by Frederick Jelinek and \"Spoken Language Processing (2001)\" by Xuedong Huang etc. More up to date are \"Computer Speech\", by Manfred R. Schroeder, second edition published in 2004, and \"Speech Processing: A Dynamic and Optimization-Oriented Approach\" published in 2003 by Li Deng and Doug O'Shaughnessey. The recently updated textbook of \"Speech and Language Processing (2008)\" by Jurafsky and Martin presents the basics and the state of the art for ASR. Speaker recognition also uses the same features, most of the same front-end processing, and classification techniques as is done in speech recognition. A most recent comprehensive textbook, \"Fundamentals of Speaker Recognition\" is an in depth source for up to date details on the theory and practice. A good insight into the techniques used in the best modern systems can be gained by paying attention to government sponsored evaluations such as those organised by DARPA (the largest speech recognition-related project ongoing as of 2007 is the GALE project, which involves both speech recognition and translation components).\n\nA good and accessible introduction to speech recognition technology and its history is provided by the general audience book \"The Voice in the Machine. Building Computers That Understand Speech\" by Roberto Pieraccini (2012).\n\nThe most recent book on speech recognition is \"Automatic Speech Recognition: A Deep Learning Approach\" (Publisher: Springer) written by D. Yu and L. Deng published near the end of 2014, with highly mathematically-oriented technical detail on how deep learning methods are derived and implemented in modern speech recognition systems based on DNNs and related deep learning methods. A related book, published earlier in 2014, \"Deep Learning: Methods and Applications\" by L. Deng and D. Yu provides a less technical but more methodology-focused overview of DNN-based speech recognition during 2009–2014, placed within the more general context of deep learning applications including not only speech recognition but also image recognition, natural language processing, information retrieval, multimodal processing, and multitask learning.\n\nIn terms of freely available resources, Carnegie Mellon University's Sphinx toolkit is one place to start to both learn about speech recognition and to start experimenting. Another resource (free but copyrighted) is the HTK book (and the accompanying HTK toolkit). For more recent and state-of-the-art techniques, Kaldi toolkit can be used.\n\nA Demo of an on-line speech recognizer is available on Cobalt's webpage.\n\nFor more software resources, see List of speech recognition software.\n\n\n", "id": "29468", "title": "Speech recognition"}
{"url": "https://en.wikipedia.org/wiki?curid=21523", "text": "Artificial neural network\n\nArtificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve performance on) tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the results to identify cats in other images. They do this without any a priori knowledge about cats, e.g., that they have fur, tails, whiskers and cat-like faces. Instead, they evolve their own set of relevant characteristics from the learning material that they process.\n\nAn ANN is based on a collection of connected units or nodes called artificial neurons (analogous to biological neurons in an animal brain). Each connection (analogous to a synapse) between artificial neurons can transmit a signal from one to another. The artificial neuron that receives the signal can process it and then signal artificial neurons connected to it.\n\nIn common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is calculated by a non-linear function of the sum of its inputs. Artificial neurons and connections typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that only if the aggregate signal crosses that threshold is the signal sent. Typically, artificial neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.\n\nThe original goal of the ANN approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology. ANNs have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\n\nWarren McCulloch and Walter Pitts (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two approaches. One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.\n\nIn the late 1940s, D.O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines.\n\nFarley and Clark (1954) first used computational machines, then called \"calculators\", to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956).\n\nRosenblatt (1958) created the perceptron, an algorithm for pattern recognition. With mathematical notation, Rosenblatt described circuitry not in the basic perceptron, such as the exclusive-or circuit that could not be processed by neural networks at the time.\n\nIn 1959, a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells.\n\nThe first functional networks with many layers were published by Ivakhnenko and Lapa in 1965, becoming the Group Method of Data Handling.\n\nNeural network research stagnated after machine learning research by Minsky and Papert (1969), who discovered two key issues with the computational machines that processed neural networks. The first was that basic perceptrons were incapable of processing the exclusive-or circuit. The second was that computers didn't have enough processing power to effectively handle the work required by large neural networks. Neural network research slowed until computers achieved far greater processing power.\n\nMuch of artificial intelligence had focused on high-level (symbolic) models that are processed by using algorithms, characterized for example by expert systems with knowledge embodied in \"if-then\" rules, until in the late 1980s research expanded to low-level (sub-symbolic) machine learning, characterized by knowledge embodied in the parameters of a cognitive model.\n\nA key trigger for renewed interest in neural networks and learning was Werbos's (1975) backpropagation algorithm that effectively solved the exclusive-or problem and more generally accelerated the training of multi-layer networks. Backpropagation distributed the error term back up through the layers, by modifying the weights at each node.\n\nIn the mid-1980s, parallel distributed processing became popular under the name connectionism. Rumelhart and McClelland (1986) described the use of connectionism to simulate neural processes.\n\nSupport vector machines and other, much simpler methods such as linear classifiers gradually overtook neural networks in machine learning popularity.\n\nEarlier challenges in training deep neural networks were successfully addressed with methods such as unsupervised pre-training, while available computing power increased through the use of GPUs and distributed computing. Neural networks were deployed on a large scale, particularly in image and visual recognition problems. This became known as \"deep learning\", although deep learning is not strictly synonymous with deep neural networks.\n\nIn 1992, max-pooling was introduced to help with least shift invariance and tolerance to deformation to aid in 3D object recognition.\nIn 2010, Backpropagation training though max-pooling was accelerated by GPUs and shown to perform better than other pooling variants.\n\nThe vanishing gradient problem affects many-layered feedforward networks that used backpropagation and also recurrent neural networks (RNNs). As errors propagate from layer to layer, they shrink exponentially with the number of layers, impeding the tuning of neuron weights that is based on those errors, particularly affecting deep networks.\n\nTo overcome this problem, Schmidhuber adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning and fine-tuned by backpropagation. Behnke (2003) relied only on the sign of the gradient (Rprop) on problems such as image reconstruction and face localization.\n\nHinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. Once sufficiently many layers have been learned, the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an \"ancestral pass\") from the top level feature activations. In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.\n\nComputational devices were created in CMOS, for both biophysical simulation and neuromorphic computing. Nanodevices for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally analog rather than digital (even though the first implementations may use digital devices). Ciresan and colleagues (2010) in Schmidhuber's group showed that despite the vanishing gradient problem, GPUs makes back-propagation feasible for many-layered feedforward neural networks.\n\nBetween 2009 and 2012, recurrent neural networks and deep feedforward neural networks developed in Schmidhuber's research group, won eight international competitions in pattern recognition and machine learning. For example, the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three languages to be learned.\n\nCiresan and colleagues won pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge and others. Their neural networks were the first pattern recognizers to achieve human-competitive or even superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem.\n\nResearchers demonstrated (2010) that deep neural networks interfaced to a hidden Markov model with context-dependent states that define the neural network output layer can drastically reduce errors in large-vocabulary speech recognition tasks such as voice search.\n\nGPU-based implementations of this approach won many pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, the ISBI 2012 Segmentation of neuronal structures in EM stacks challenge, the ImageNet Competition and others.\n\nDeep, highly nonlinear neural architectures similar to the neocognitron and the \"standard architecture of vision\", inspired by simple and complex cells were pre-trained by unsupervised methods by Hinton. A team from his lab won a 2012 contest sponsored by Merck to design software to help find molecules that might identify new drugs.\n\nAs of 2011, the state of the art in deep learning feedforward networks alternated convolutional layers and max-pooling layers, topped by several fully or sparsely connected layers followed by a final classification layer. Learning is usually done without unsupervised pre-training.\n\nSuch supervised deep learning methods were the first to achieve human-competitive performance on certain tasks.\n\nANNs were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes, only when invariance extended beyond shift, to all ANN-learned concepts, such as location, type (object class label), scale, lighting and others. This was realized in Developmental Networks (DNs) whose embodiments are Where-What Networks, WWN-1 (2008) through WWN-7 (2013).\n\nAn \"(artificial) neural network\" is a network of simple elements called \"neurons\", which receive input, change their internal state (\"activation\") according to that input, and produce output depending on the input and activation. The \"network\" forms by connecting the output of certain neurons to the input of other neurons forming a directed, weighted graph. The weights as well as the functions that compute the activation can be modified by a process called \"learning\" which is governed by a \"learning rule\".\n\nA neuron with label formula_1 receiving an input formula_2 from predecessor neurons consists of the following components:\n\nOften the output function is simply the Identity function.\n\nAn \"input neuron\" has no predecessor but serves as input interface for the whole network. Similarly an \"output neuron\" has no successor and thus serves as output interface of the whole network.\n\nThe \"network\" consists of connections, each connection transferring the output of a neuron formula_13 to the input of a neuron formula_1. In this sense formula_13 is the predecessor of formula_1 and formula_1 is the successor of formula_13. Each connection is assigned a weight formula_19.\n\nThe \"propagation function\" computes the \"input\" formula_2 to the neuron formula_1 from the outputs formula_22 of predecessor neurons and typically has the form\n\nThe \"learning rule\" is a rule or an algorithm which modifies the parameters of the neural network, in order for a given input to the network to produce a favored output. This \"learning\" process typically amounts to modifying the weights and thresholds of the variables within the network.\n\nNeural network models can be viewed as simple mathematical models defining a function formula_24 or a distribution over formula_25 or both formula_25 and formula_27. Sometimes models are intimately associated with a particular learning rule. A common use of the phrase \"ANN model\" is really the definition of a \"class\" of such functions (where members of the class are obtained by varying parameters, connection weights, or specifics of the architecture such as the number of neurons or their connectivity).\n\nMathematically, a neuron's network function formula_28 is defined as a composition of other functions formula_29, that can further be decomposed into other functions. This can be conveniently represented as a network structure, with arrows depicting the dependencies between functions. A widely used type of composition is the \"nonlinear weighted sum\", where formula_30, where formula_31 (commonly referred to as the activation function) is some predefined function, such as the hyperbolic tangent or sigmoid function or softmax function or rectifier function. The important characteristic of the activation function is that it provides a smooth transition as input values change, i.e. a small change in input produces a small change in output. The following refers to a collection of functions formula_32 as a vector formula_33.\n\nThis figure depicts such a decomposition of formula_34, with dependencies between variables indicated by arrows. These can be interpreted in two ways.\n\nThe first view is the functional view: the input formula_35 is transformed into a 3-dimensional vector formula_36, which is then transformed into a 2-dimensional vector formula_37, which is finally transformed into formula_34. This view is most commonly encountered in the context of optimization.\n\nThe second view is the probabilistic view: the random variable formula_39 depends upon the random variable formula_40, which depends upon formula_41, which depends upon the random variable formula_25. This view is most commonly encountered in the context of graphical models.\n\nThe two views are largely equivalent. In either case, for this particular architecture, the components of individual layers are independent of each other (e.g., the components of formula_37 are independent of each other given their input formula_36). This naturally enables a degree of parallelism in the implementation.\n\nNetworks such as the previous one are commonly called feedforward, because their graph is a directed acyclic graph. Networks with cycles are commonly called recurrent. Such networks are commonly depicted in the manner shown at the top of the figure, where formula_34 is shown as being dependent upon itself. However, an implied temporal dependence is not shown.\n\nThe possibility of learning has attracted the most interest in neural networks. Given a specific \"task\" to solve, and a class of functions formula_46, learning means using a set of observations to find formula_47 which solves the task in some optimal sense.\n\nThis entails defining a cost function formula_48 such that, for the optimal solution formula_49, formula_50 formula_51 i.e., no solution has a cost less than the cost of the optimal solution (see mathematical optimization).\n\nThe cost function formula_52 is an important concept in learning, as it is a measure of how far away a particular solution is from an optimal solution to the problem to be solved. Learning algorithms search through the solution space to find a function that has the smallest possible cost.\n\nFor applications where the solution is data dependent, the cost must necessarily be a function of the observations, otherwise the model would not relate to the data. It is frequently defined as a statistic to which only approximations can be made. As a simple example, consider the problem of finding the model formula_34, which minimizes formula_54, for data pairs formula_55 drawn from some distribution formula_56. In practical situations we would only have formula_57 samples from formula_56 and thus, for the above example, we would only minimize formula_59. Thus, the cost is minimized over a sample of the data rather than the entire distribution.\n\nWhen formula_60 some form of online machine learning must be used, where the cost is reduced as each new example is seen. While online machine learning is often used when formula_56 is fixed, it is most useful in the case where the distribution changes slowly over time. In neural network methods, some form of online machine learning is frequently used for finite datasets.\n\nWhile it is possible to define an ad hoc cost function, frequently a particular cost (function) is used, either because it has desirable properties (such as convexity) or because it arises naturally from a particular formulation of the problem (e.g., in a probabilistic formulation the posterior probability of the model can be used as an inverse cost). Ultimately, the cost function depends on the task.\n\nA DNN can be discriminatively trained with the standard backpropagation algorithm. Backpropagation is a method to calculate the gradient of the loss function (produces the cost associated with a given state) with respect to the weights in an ANN.\n\nThe basics of continuous backpropagation were derived in the context of control theory by Kelley in 1960 and by Bryson in 1961, using principles of dynamic programming. In 1962, Dreyfus published a simpler derivation based only on the chain rule. Bryson and Ho described it as a multi-stage dynamic system optimization method in 1969. In 1970, Linnainmaa finally published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions. This corresponds to the modern version of backpropagation which is efficient even when the networks are sparse. In 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. In 1974, Werbos mentioned the possibility of applying this principle to ANNs, and in 1982, he applied Linnainmaa's AD method to neural networks in the way that is widely used today. In 1986, Rumelhart, Hinton and Williams noted that this method can generate useful internal representations of incoming data in hidden layers of neural networks. In 1993, Wan was the first to win an international pattern recognition contest through backpropagation.\n\nThe weight updates of backpropagation can be done via stochastic gradient descent using the following equation:\nwhere, formula_63 is the learning rate, formula_64 is the cost (loss) function and formula_65 a stochastic term. The choice of the cost function depends on factors such as the learning type (supervised, unsupervised, reinforcement, etc.) and the activation function. For example, when performing supervised learning on a multiclass classification problem, common choices for the activation function and cost function are the softmax function and cross entropy function, respectively. The softmax function is defined as formula_66 where formula_67 represents the class probability (output of the unit formula_68) and formula_69 and formula_70 represent the total input to units formula_68 and formula_72 of the same level respectively. Cross entropy is defined as formula_73 where formula_74 represents the target probability for output unit formula_68 and formula_67 is the probability output for formula_68 after applying the activation function.\n\nThese can be used to output object bounding boxes in the form of a binary mask. They are also used for multi-scale regression to increase localization precision. DNN-based regression can learn features that capture geometric information in addition to serving as a good classifier. They remove the requirement to explicitly model parts and their relations. This helps to broaden the variety of objects that can be learned. The model consists of multiple layers, each of which has a rectified linear unit as its activation function for non-linear transformation. Some layers are convolutional, while others are fully connected. Every convolutional layer has an additional max pooling. The network is trained to minimize \"L\" error for predicting the mask ranging over the entire training set containing bounding boxes represented as masks.\n\nAlternatives to backpropagation include Extreme Learning Machines, \"No-prop\" networks, training without backtracking, \"weightless\" networks, and non-connectionist neural networks.\n\nThe three major learning paradigms each correspond to a particular learning task. These are supervised learning, unsupervised learning and reinforcement learning.\n\nSupervised learning uses a set of example pairs formula_78 and the aim is to find a function formula_79 in the allowed class of functions that matches the examples. In other words, we wish to infer the mapping implied by the data; the cost function is related to the mismatch between our mapping and the data and it implicitly contains prior knowledge about the problem domain.\n\nA commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output, formula_80, and the target value formula_81 over all the example pairs. Minimizing this cost using gradient descent for the class of neural networks called multilayer perceptrons (MLP), produces the backpropagation algorithm for training neural networks.\n\nTasks that fall within the paradigm of supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). The supervised learning paradigm is also applicable to sequential data (e.g., for hand writing, speech and gesture recognition). This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.\n\nIn unsupervised learning, some data formula_35 is given and the cost function to be minimized, that can be any function of the data formula_35 and the network's output, formula_34.\n\nThe cost function is dependent on the task (the model domain) and any \"a priori\" assumptions (the implicit properties of the model, its parameters and the observed variables).\n\nAs a trivial example, consider the model formula_85 where formula_86 is a constant and the cost formula_87. Minimizing this cost produces a value of formula_86 that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between formula_35 and formula_28, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples those quantities would be maximized rather than minimized).\n\nTasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.\n\nIn reinforcement learning, data formula_35 are usually not given, but generated by an agent's interactions with the environment. At each point in time formula_92, the agent performs an action formula_93 and the environment generates an observation formula_94 and an instantaneous cost formula_95, according to some (usually unknown) dynamics. The aim is to discover a policy for selecting actions that minimizes some measure of a long-term cost, e.g., the expected cumulative cost. The environment's dynamics and the long-term cost for each policy are usually unknown, but can be estimated.\n\nMore formally the environment is modeled as a Markov decision process (MDP) with states formula_96 and actions formula_97 with the following probability distributions: the instantaneous cost distribution formula_98, the observation distribution formula_99 and the transition formula_100, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two then define a Markov chain (MC). The aim is to discover the policy (i.e., the MC) that minimizes the cost.\n\nANNs are frequently used in reinforcement learning as part of the overall algorithm. Dynamic programming was coupled with ANNs (giving neurodynamic programming) by Bertsekas and Tsitsiklis and applied to multi-dimensional nonlinear problems such as those involved in vehicle routing, natural resources management or medicine because of the ability of ANNs to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of the original control problems.\n\nTasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.\n\nThis is a learning method specially designed for cerebellar model articulation controller (CMAC) neural networks. In 2004 a recursive least squares algorithm was introduced to train CMAC neural network online. This algorithm can converge in one step and update all weights in one step with any new input data. Initially, this algorithm had computational complexity of \"O\"(\"N\"). Based on QR decomposition, this recursive learning algorithm was simplified to be \"O\"(\"N\").\n\nTraining a neural network model essentially means selecting one model from the set of allowed models (or, in a Bayesian framework, determining a distribution over the set of allowed models) that minimizes the cost. Numerous algorithms are available for training neural network models; most of them can be viewed as a straightforward application of optimization theory and statistical estimation.\n\nMost employ some form of gradient descent, using backpropagation to compute the actual gradients. This is done by simply taking the derivative of the cost function with respect to the network parameters and then changing those parameters in a gradient-related direction. Backpropagation training algorithms fall into three categories: \n\nEvolutionary methods, gene expression programming, simulated annealing, expectation-maximization, non-parametric methods and particle swarm optimization are other methods for training neural networks.\n\nThe Group Method of Data Handling (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov-Gabor polynomials that permit additions and multiplications. It used a deep feedforward multilayer perceptron with eight layers. It is a supervised learning network that grows layer by layer, where each layer is trained by regression analysis. Useless items are detected using a validation set, and pruned through regularization. The size and depth of the resulting network depends on the task.\n\nA convolutional neural network (CNN) is a class of deep, feed-forward networks, composed of one or more convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular, max-pooling is often structured via Fukushima's convolutional architecture. This architecture allows CNNs to take advantage of the 2D structure of input data.\n\nCNNs are suitable for processing visual and other two-dimensional data. They have shown superior results in both image and speech applications. They can be trained with standard backpropagation. CNNs are easier to train than other regular, deep, feed-forward neural networks and have many fewer parameters to estimate. Examples of applications in computer vision include DeepDream.\n\nLong short-term memory (LSTM) networks are RNNs that avoid the vanishing gradient problem. LSTM is normally augmented by recurrent gates called forget gates. LSTM networks prevent backpropagated errors from vanishing or exploding. Instead errors can flow backwards through unlimited numbers of virtual layers in space-unfolded LSTM. That is, LSTM can learn \"very deep learning\" tasks that require memories of events that happened thousands or even millions of discrete time steps ago. Problem-specific LSTM-like topologies can be evolved. LSTM can handle long delays and signals that have a mix of low and high frequency components.\n\nStacks of LSTM RNNs trained by Connectionist Temporal Classification (CTC) can find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.\n\nIn 2003, LSTM started to become competitive with traditional speech recognizers. In 2007, the combination with CTC achieved first good results on speech data. In 2009, a CTC-trained LSTM was the first RNN to win pattern recognition contests, when it won several competitions in connected handwriting recognition. In 2014, Baidu used CTC-trained RNNs to break the Switchboard Hub5'00 speech recognition benchmark, without traditional speech processing methods. LSTM also improved large-vocabulary speech recognition, text-to-speech synthesis, for Google Android, and photo-real talking heads. In 2015, Google's speech recognition experienced a 49% improvement through CTC-trained LSTM.\n\nLSTM became popular in Natural Language Processing. Unlike previous models based on HMMs and similar concepts, LSTM can learn to recognise context-sensitive languages. LSTM improved machine translation, language modeling and multilingual language processing. LSTM combined with CNNs improved automatic image captioning.\n\nDeep Reservoir Computing and Deep Echo State Networks (deepESNs) provide a framework for efficiently trained models for hierarchical processing of temporal data, while enabling the investigation of the inherent role of RNN layered composition.\n\nA deep belief network (DBN) is a probabilistic, generative model made up of multiple layers of hidden units. It can be considered a composition of simple learning modules that make up each layer.\n\nA DBN can be used to generatively pre-train a DNN by using the learned DBN weights as the initial DNN weights. Backpropagation or other discriminative algorithms can then tune these weights. This is particularly helpful when training data are limited, because poorly initialized weights can significantly hinder model performance. These pre-trained weights are in a region of the weight space that is closer to the optimal weights than were they randomly chosen. This allows for both improved modeling and faster convergence of the fine-tuning phase.\n\nLarge memory storage and retrieval neural networks (LAMSTAR) are fast deep learning neural networks of many layers that can use many filters simultaneously. These filters may be nonlinear, stochastic, logic, non-stationary, or even non-analytical. They are biologically motivated and learn continuously.\n\nA LAMSTAR neural network may serve as a dynamic neural network in spatial or time domains or both. Its speed is provided by Hebbian link-weights that integrate the various and usually different filters (preprocessing functions) into its many layers and to dynamically rank the significance of the various layers and functions relative to a given learning task. This grossly imitates biological learning which integrates various preprocessors (cochlea, retina, \"etc.\") and cortexes (auditory, visual, \"etc.\") and their various regions. Its deep learning capability is further enhanced by using inhibition, correlation and its ability to cope with incomplete data, or \"lost\" neurons or layers even amidst a task. It is fully transparent due to its link weights. The link-weights allow dynamic determination of innovation and redundancy, and facilitate the ranking of layers, of filters or of individual neurons relative to a task.\n\nLAMSTAR has been applied to many domains, including medical and financial predictions, adaptive filtering of noisy speech in unknown noise, still-image recognition, video image recognition, software security and adaptive control of non-linear systems. LAMSTAR had a much faster learning speed and somewhat lower error rate than a CNN based on ReLU-function filters and max pooling, in 20 comparative studies.\n\nThese applications demonstrate delving into aspects of the data that are hidden from shallow learning networks and the human senses, such as in the cases of predicting onset of sleep apnea events, of an electrocardiogram of a fetus as recorded from skin-surface electrodes placed on the mother's abdomen early in pregnancy, of financial prediction or in blind filtering of noisy speech.\n\nLAMSTAR was proposed in 1996 () and was further developed Graupe and Kordylewski from 1997-2002. A modified version, known as LAMSTAR 2, was developed by Schneider and Graupe in 2008.\n\nThe auto encoder idea is motivated by the concept of a \"good\" representation. For example, for a classifier, a good representation can be defined as one that yields a better-performing classifier.\n\nAn \"encoder\" is a deterministic mapping formula_101 that transforms an input vector x into hidden representation y, where formula_102, formula_103 is the weight matrix and b is an offset vector (bias). A \"decoder\" maps back the hidden representation y to the reconstructed input z via formula_104. The whole process of auto encoding is to compare this reconstructed input to the original and try to minimize the error to make the reconstructed value as close as possible to the original.\n\nIn \"stacked denoising auto encoders\", the partially corrupted output is cleaned (de-noised). This idea was introduced in 2010 by Vincent et al. with a specific approach to \"good\" representation, a \"good representation\" is one that can be obtained robustly from a corrupted input and that will be useful for recovering the corresponding clean input\".\" Implicit in this definition are the following ideas:\nThe algorithm starts by a stochastic mapping of formula_105 to formula_106 through formula_107, this is the corrupting step. Then the corrupted input formula_106 passes through a basic auto-encoder process and is mapped to a hidden representation formula_109. From this hidden representation, we can reconstruct formula_110. In the last stage, a minimization algorithm runs in order to have z as close as possible to uncorrupted input formula_105. The reconstruction error formula_112 might be either the cross-entropy loss with an affine-sigmoid decoder, or the squared error loss with an affine decoder.\n\nIn order to make a deep architecture, auto encoders stack. Once the encoding function formula_101 of the first denoising auto encoder is learned and used to uncorrupt the input (corrupted input), the second level can be trained.\n\nOnce the stacked auto encoder is trained, its output can be used as the input to a supervised learning algorithm such as support vector machine classifier or a multi-class logistic regression.\n\nA a deep stacking network (DSN) (deep convex network) is based on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Dong. It formulates the learning as a convex optimization problem with a closed-form solution, emphasizing the mechanism's similarity to stacked generalization. Each DSN block is a simple module that is easy to train by itself in a supervised fashion without backpropagation for the entire blocks.\n\nEach block consists of a simplified multi-layer perceptron (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units, and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T, and the input data vectors x form the columns of matrix X. The matrix of hidden units is formula_114. Modules are trained in order, so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class \"y\", and its estimate is concatenated with original input X to form the expanded input for the next block. Thus, the input to the first block contains the original data only, while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem:\nwhich has a closed-form solution.\n\nUnlike other deep architectures, such as DBNs, the goal is not to discover the transformed feature representation. The structure of the hierarchy of this kind of architecture makes parallel learning straightforward, as a batch-mode optimization problem. In purely discriminative tasks, DSNs perform better than conventional DBNs.\n\nThis architecture is a DSN extension. It offers two important improvements: it uses higher-order information from covariance statistics, and it transforms the non-convex problem of a lower-layer to a convex sub-problem of an upper-layer. TDSNs use covariance statistics in a bilinear mapping from each of two distinct sets of hidden units in the same layer to predictions, via a third-order tensor.\n\nWhile parallelization and scalability are not considered seriously in conventional , all learning for s and s is done in batch mode, to allow parallelization. Parallelization allows scaling the design to larger (deeper) architectures and data sets.\n\nThe basic architecture is suitable for diverse tasks such as classification and regression.\n\nThe need for deep learning with real-valued inputs, as in Gaussian restricted Boltzmann machines, led to the \"spike-and-slab\" RBM (\"ss\"RBM), which models continuous-valued inputs with strictly binary latent variables. Similar to basic RBMs and its variants, a spike-and-slab RBM is a bipartite graph, while like GRBMs, the visible units (input) are real-valued. The difference is in the hidden layer, where each hidden unit has a binary spike variable and a real-valued slab variable. A spike is a discrete probability mass at zero, while a slab is a density over continuous domain; their mixture forms a prior.\n\nAn extension of ssRBM called µ-ssRBM provides extra modeling capacity using additional terms in the energy function. One of these terms enables the model to form a conditional distribution of the spike variables by marginalizing out the slab variables given an observation.\n\nCompound hierarchical-deep models compose deep networks with non-parametric Bayesian models. Features can be learned using deep architectures such as DBNs, DBMs, deep auto encoders, convolutional variants, ssRBMs, deep coding networks, DBNs with sparse feature learning, RNNs, conditional DBNs, de-noising auto encoders. This provides a better representation, allowing faster learning and more accurate classification with high-dimensional data. However, these architectures are poor at learning novel classes with few examples, because all network units are involved in representing the input (a ) and must be adjusted together (high degree of freedom). Limiting the degree of freedom reduces the number of parameters to learn, facilitating learning of new classes from few examples. \"Hierarchical Bayesian (HB)\" models allow learning from few examples, for example for computer vision, statistics and cognitive science.\n\nCompound HD architectures aim to integrate characteristics of both HB and deep networks. The compound HDP-DBM architecture is a \"hierarchical Dirichlet process (HDP)\" as a hierarchical model, incorporated with DBM architecture. It is a full generative model, generalized from abstract concepts flowing through the layers of the model, which is able to synthesize new examples in novel classes that look \"reasonably\" natural. All the levels are learned jointly by maximizing a joint log-probability score.\n\nIn a DBM with three hidden layers, the probability of a visible input is:\nwhere formula_117 is the set of hidden units, and formula_118 are the model parameters, representing visible-hidden and hidden-hidden symmetric interaction terms.\n\nA learned DBM model is an undirected model that defines the joint distribution formula_119. One way to express what has been learned is the conditional model formula_120 and a prior term formula_121.\n\nHere formula_120 represents a conditional DBM model, which can be viewed as a two-layer DBM but with bias terms given by the states of formula_123:\n\nA deep predictive coding network (DPCN) is a predictive coding scheme that uses top-down information to empirically adjust the priors needed for a bottom-up inference procedure by means of a deep, locally connected, generative model. This works by extracting sparse features from time-varying observations using a linear dynamical model. Then, a pooling strategy is used to learn invariant feature representations. These units compose to form a deep architecture and are trained by greedy layer-wise unsupervised learning. The layers constitute a kind of Markov chain such that the states at any layer depend only on the preceding and succeeding layers.\n\nDPCNs predict the representation of the layer, by using a top-down approach using the information in upper layer and temporal dependencies from previous states.\n\nDPCNs can be extended to form a convolutional network.\n\nIntegrating external memory with ANNs dates to early research in distributed representations and Kohonen's self-organizing maps. For example, in sparse distributed memory or hierarchical temporal memory, the patterns encoded by neural networks are used as addresses for content-addressable memory, with \"neurons\" essentially serving as address encoders and decoders. However, the early controllers of such memories were not differentiable.\n\nApart from long short-term memory (LSTM), other approaches also added differentiable memory to recurrent functions. For example:\n\nNeural Turing machines couple LSTM networks to external memory resources, with which they can interact by attentional processes. The combined system is analogous to a Turing machine but is differentiable end-to-end, allowing it to be efficiently trained by gradient descent. Preliminary results demonstrate that neural Turing machines can infer simple algorithms such as copying, sorting and associative recall from input and output examples.\n\nDifferentiable neural computers (DNC) are an NTM extension. They out-performed Neural turing machines, long short-term memory systems and memory networks on sequence-processing tasks.\n\nApproaches that represent previous experiences directly and use a similar experience to form a local model are often called nearest neighbour or k-nearest neighbors methods. Deep learning is useful in semantic hashing where a deep graphical model the word-count vectors obtained from a large set of documents. Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by accessing all the addresses that differ by only a few bits from the address of the query document. Unlike sparse distributed memory that operates on 1000-bit addresses, semantic hashing works on 32 or 64-bit addresses found in a conventional computer architecture.\n\nMemory networks are another extension to neural networks incorporating long-term memory. The long-term memory can be read and written to, with the goal of using it for prediction. These models have been applied in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base and the output is a textual response.\n\nDeep neural networks can be potentially improved by deepening and parameter reduction, while maintaining trainability. While training extremely deep (e.g., 1 million layers) neural networks might not be practical, CPU-like architectures such as pointer networks and neural random-access machines overcome this limitation by using external random-access memory and other components that typically belong to a computer architecture such as registers, ALU and pointers. Such systems operate on probability distribution vectors stored in memory cells and registers. Thus, the model is fully differentiable and trains end-to-end. The key characteristic of these models is that their depth, the size of their short-term memory, and the number of parameters can be altered independently — unlike models like LSTM, whose number of parameters grows quadratically with memory size.\n\nEncoder–decoder frameworks are based on neural networks that map highly structured input to highly structured output. The approach arose in the context of machine translation, where the input and output are written sentences in two natural languages. In that work, an LSTM RNN or CNN was used as an encoder to summarize a source sentence, and the summary was decoded using a conditional RNN language model to produce the translation. These systems share building blocks: gated RNNs and CNNs and trained attention mechanisms.\n\nMultilayer kernel machines (MKM) are a way of learning highly nonlinear functions by iterative application of weakly nonlinear kernels. They use the kernel principal component analysis (KPCA), as a method for the unsupervised greedy layer-wise pre-training step of the deep learning architecture.\n\nLayer formula_125 learns the representation of the previous layer formula_126, extracting the formula_127 principal component (PC) of the projection layer formula_126 output in the feature domain induced by the kernel. For the sake of dimensionality reduction of the updated representation in each layer, a supervised strategy is proposed to select the best informative features among features extracted by KPCA. The process is:\nSome drawbacks accompany the KPCA method as the building cells of an MKM.\n\nA more straightforward way to use kernel machines for deep learning was developed for spoken language understanding. The main idea is to use a kernel machine to approximate a shallow neural net with an infinite number of hidden units, then use stacking to splice the output of the kernel machine and the raw input in building the next, higher level of the kernel machine. The number of levels in the deep convex network is a hyper-parameter of the overall system, to be determined by cross validation.\n\nUsing ANNs requires an understanding of their characteristics.\nANN capabilities fall within the following broad categories:\n\n\nBecause of their ability to reproduce and model nonlinear processes, ANNs have found many applications in a wide range of disciplines.\n\nApplication areas include system identification and control (vehicle control, trajectory prediction, process control, natural resource management), quantum chemistry, game-playing and decision making (backgammon, chess, poker), pattern recognition (radar systems, face identification, signal classification, object recognition and more), sequence recognition (gesture, speech, handwritten and printed text recognition), medical diagnosis, finance (e.g. automated trading systems), data mining, visualization, machine translation, social network filtering and e-mail spam filtering.\n\nANNs have been used to diagnose cancers, including lung cancer, prostate cancer, colorectal cancer and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.\n\nANNs have been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology, are just few examples of this kind.\n\nTheoretical and computational neuroscience is concerned with the theoretical analysis and the computational modeling of biological neural systems. Since neural systems attempt to reflect cognitive processes and behavior, the field is closely related to cognitive and behavioral modeling.\n\nTo gain this understanding, neuroscientists strive to link observed biological processes (data), biologically plausible mechanisms for neural processing and learning (biological neural network models) and theory (statistical learning theory and information theory).\n\nBrain research has repeatedly led to new ANN approaches, such as the use of connections to connect neurons in other layers rather than adjacent neurons in the same layer. Other research explored the use of multiple signal types, or finer control than boolean (on/off) variables. Dynamic neural networks can dynamically form new connections and even new neural units while disabling others.\n\nMany types of models are used, defined at different levels of abstraction and modeling different aspects of neural systems. They range from models of the short-term behavior of individual neurons, models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level.\n\nThe multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.\n\nA specific recurrent architecture with rational valued weights (as opposed to full precision real number-valued weights) has the full power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.\n\nModels' \"capacity\" property roughly corresponds to their ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.\n\nModels may not consistently converge on a single solution, firstly because many local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical. However, for CMAC neural network, a recursive least squares algorithm was introduced to train it, and this algorithm can be guaranteed to converge in one step.\n\nApplications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the capacity of the network significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and optimally select hyperparameters to minimize the generalization error. The second is to use some form of \"regularization\". This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.\nSupervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of the output of the network, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.\n\nBy assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based neural network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is very useful in classification as it gives a certainty measure on classifications.\n\nThe softmax activation function is:\n\n<section end=\"theory\" />\n\nA common criticism of neural networks, particularly in robotics, is that they require too much training for real-world operation. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example and by grouping examples in so-called mini-batches. Improving the training efficiency and convergence capability has always been an ongoing research area for neural network. For example, by introducing a recursive least squares algorithm for CMAC neural network, the training process only takes one step to converge.\n\nNo neural network has solved computationally difficult problems such as the n-Queens problem, the travelling salesman problem, or the problem of factoring large integers.\n\nA fundamental objection is that they do not reflect how real neurons function. Back propagation is a critical part of most artificial neural networks, although no such mechanism exists in biological neural networks. How information is coded by real neurons is not known. Sensor neurons fire action potentials more frequently with sensor activation and muscle cells pull more strongly when their associated motor neurons receive action potentials more frequently. Other than the case of relaying information from a sensor neuron to a motor neuron, almost nothing of the principles of how information is handled by biological neural networks is known.\n\nThe motivation behind ANNs is not necessarily to strictly replicate neural function, but to use biological neural networks as an inspiration. A central claim of ANNs is therefore that it embodies some new and powerful general principle for processing information. Unfortunately, these general principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. Alexander Dewdney commented that, as a result, artificial neural networks have a \"something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything\".\n\nBiological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.\n\nLarge and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may compel a neural network designer to fill many millions of database rows for its connections which can consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons which must often be matched with enormous CPU processing power and time.\n\nSchmidhuber notes that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of parallel GPUs can reduce training times from months to days.\n\nNeuromorphic engineering addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.\n\nArguments against Dewdney's position are that neural networks have been successfully used to solve many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.\n\nTechnology writer Roger Bridgman commented:\n\nNeural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".\nIn spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.\nAlthough it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs non-local learning and shallow vs deep architecture.\n\nAdvocates of hybrid models (combining neural networks and symbolic approaches), claim that such a mixture can better capture the mechanisms of the human mind.\n\nArtificial neural networks have many variations. The simplest, static types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to change during the learning process. The latter are much more complicated, but can shorten learning periods and produce better results. Some types allow/require learning to be \"supervised\" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.\n\n\n", "id": "21523", "title": "Artificial neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=14179835", "text": "Activation function\n\nIn computational networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard computer chip circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, only \"nonlinear\" activation functions allow such networks to compute nontrivial problems using only a small number of nodes. In artificial neural networks this function is also called the transfer function.\n\nIn biologically inspired neural networks, the activation function is usually an abstraction representing the rate of action potential firing in the cell. In its simplest form, this function is binary—that is, either the neuron is firing or not. The function looks like formula_1, where formula_2 is the Heaviside step function. In this case many neurons must be used in computation beyond linear separation of categories.\n\nA line of positive slope may be used to reflect the increase in firing rate that occurs as input current increases. Such a function would be of the form formula_3, where formula_4 is the slope. This activation function is linear, and therefore has the same problems as the binary function. In addition, networks constructed using this model have unstable convergence because neuron inputs along favored paths tend to increase without bound, as this function is not normalizable.\n\nAll problems mentioned above can be handled by using a normalizable sigmoid activation function. One realistic model stays at zero until input current is received, at which point the firing frequency increases quickly at first, but gradually approaches an asymptote at 100% firing rate. Mathematically, this looks like formula_5, where the hyperbolic tangent function can be replaced by any sigmoid function. This behavior is realistically reflected in the neuron, as neurons cannot physically fire faster than a certain rate. This model runs into problems, however, in computational networks as it is not differentiable, a requirement to calculate backpropagation.\n\nThe final model, then, that is used in multilayer perceptrons is a sigmoidal activation function in the form of a hyperbolic tangent. Two forms of this function are commonly used: formula_6 whose range is normalized from -1 to 1, and formula_7 is vertically translated to normalize from 0 to 1. The latter model is often considered more biologically realistic, but it runs into theoretical and experimental difficulties with certain types of computational problems.\n\nA special class of activation functions known as radial basis functions (RBFs) are used in RBF networks, which are extremely efficient as universal function approximators. These activation functions can take many forms, but they are usually found as one of three functions:\nwhere formula_11 is the vector representing the function \"center\" and formula_12 and formula_13 are parameters affecting the spread of the radius.\n\nSupport vector machines (SVMs) can effectively utilize a class of activation functions that includes both sigmoids and RBFs. In this case, the input is transformed to reflect a decision boundary hyperplane based on a few training inputs called \"support vectors\" formula_14. The activation function for the hidden layer of these machines is referred to as the \"inner product kernel\", formula_15. The support vectors are represented as the centers in RBFs with the kernel equal to the activation function, but they take a unique form in the perceptron as \nwhere formula_17 and formula_18 must satisfy certain conditions for convergence. These machines can also accept arbitrary-order polynomial activation functions where\nActivation function having types:\n\nSome desirable properties in an activation function include:\n\n\nThe following table compares the properties of several activation functions that are functions of one fold from the previous layer or layers:\n</math>\nwith formula_51 and formula_52\n\nThe following table lists activation functions that are not functions of a single fold from the previous layer or layers:\n\n", "id": "14179835", "title": "Activation function"}
{"url": "https://en.wikipedia.org/wiki?curid=8220913", "text": "ADALINE\n\nADALINE (Adaptive Linear Neuron or later Adaptive Linear Element) is an early single-layer artificial neural network and the name of the physical device that implemented this network. The network uses memistors. It was developed by Professor Bernard Widrow and his graduate student Ted Hoff at Stanford University in 1960. It is based on the McCulloch–Pitts neuron. It consists of a weight, a bias and a summation function.\n\nThe difference between Adaline and the standard (McCulloch–Pitts) perceptron is that in the learning phase, the weights are adjusted according to the weighted sum of the inputs (the net). In the standard perceptron, the net is passed to the activation (transfer) function and the function's output is used for adjusting the weights.\n\nA multilayer network of ADALINE units is known as a MADALINE.\n\nAdaline is a single layer neural network with multiple nodes where each node accepts multiple inputs and generates one output. Given the following variables:as\n\nthen we find that the output is formula_6. If we further assume that\n\nthen the output further reduces to: formula_9\n\nLet us assume:\n\nthen the weights are updated as follows formula_13. The ADALINE converges to the least squares error which is formula_14.\nThis update rule is in fact the stochastic gradient descent update for linear regression.\n\nMADALINE (Many ADALINE) is a three-layer (input, hidden, output), fully connected, feed-forward artificial neural network architecture for classification that uses ADALINE units in its hidden and output layers, i.e. its activation function is the sign function. The three-layer network uses memistors. Three different training algorithms for MADALINE networks, which cannot be learned using backpropagation because the sign function is not differentiable, have been suggested, called Rule I, Rule II and Rule III. The first of these dates back to 1962 and cannot adapt the weights of the hidden-output connection. The second training algorithm improved on Rule I and was described in 1988. The third \"Rule\" applied to a modified network with sigmoid activations instead of signum; it was later found to be equivalent to backpropagation.\n\nThe Rule II training algorithm is based on a principle called \"minimal disturbance\". It proceeds by looping over training examples, then for each example, it:\n\nAdditionally, when flipping single units' signs does not drive the error to zero for a particular example, the training algorithm starts flipping pairs of units' signs, then triples of units, etc.\n\n\n", "id": "8220913", "title": "ADALINE"}
{"url": "https://en.wikipedia.org/wiki?curid=3056879", "text": "Adaptive resonance theory\n\nAdaptive resonance theory (ART) is a theory developed by Stephen Grossberg and Gail Carpenter on aspects of how the brain processes information. It describes a number of neural network models which use supervised and unsupervised learning methods, and address problems such as pattern recognition and prediction.\n\nThe primary intuition behind the ART model is that object identification and recognition generally occur as a result of the interaction of 'top-down' observer expectations with 'bottom-up' sensory information. The model postulates that 'top-down' expectations take the form of a memory template or prototype that is then compared with the actual features of an object as detected by the senses. This comparison gives rise to a measure of category belongingness. As long as this difference between sensation and expectation does not exceed a set threshold called the 'vigilance parameter', the sensed object will be considered a member of the expected class. The system thus offers a solution to the 'plasticity/stability' problem, i.e. the problem of acquiring new knowledge without disrupting existing knowledge that is also called incremental learning.\n\nThe basic ART system is an unsupervised learning model. It typically consists of a comparison field and a recognition field composed of neurons, a vigilance parameter (threshold of recognition), and a reset module. The comparison field takes an input vector (a one-dimensional array of values) and transfers it to its best match in the recognition field. Its best match is the single neuron whose set of weights (weight vector) most closely matches the input vector. Each recognition field neuron outputs a negative signal (proportional to that neuron’s quality of match to the input vector) to each of the other recognition field neurons and thus inhibits their output. In this way the recognition field exhibits lateral inhibition, allowing each neuron in it to represent a category to which input vectors are classified.\n\nAfter the input vector is classified, the reset module compares the strength of the recognition match to the vigilance parameter. If the vigilance parameter is overcome (i.e. the input vector is within the normal range seen on previous input vectors), training commences: the weights of the winning recognition neuron are adjusted towards the features of the input vector. Otherwise, if the match level is below the vigilance parameter (i.e. the input vector's match is outside the normal expected range for that neuron) the winning recognition neuron is inhibited and a search procedure is carried out. In this search procedure, recognition neurons are disabled one by one by the reset function until the vigilance parameter is overcome by a recognition match. In particular, at each cycle of the search procedure the most active recognition neuron is selected and then switched off if its activation is below the vigilance parameter (note that it thus releases the remaining recognition neurons from its inhibition). If no committed recognition neuron’s match overcomes the vigilance parameter, then an uncommitted neuron is committed and its weights are adjusted towards matching the input vector. The vigilance parameter has considerable influence on the system: higher vigilance produces highly detailed memories (many, fine-grained categories), while lower vigilance results in more general memories (fewer, more-general categories).\n\nThere are two basic methods of training ART-based neural networks: slow and fast. In the slow learning method, the degree of training of the recognition neuron’s weights towards the input vector is calculated to continuous values with differential equations and is thus dependent on the length of time the input vector is presented. With fast learning, algebraic equations are used to calculate degree of weight adjustments to be made, and binary values are used. While fast learning is effective and efficient for a variety of tasks, the slow learning method is more biologically plausible and can be used with continuous-time networks (i.e. when the input vector can vary continuously).\n\nART 1 is the simplest variety of ART networks, accepting only binary inputs.\nART 2 extends network capabilities to support continuous inputs.\nART 2-A is a streamlined form of ART-2 with a drastically accelerated runtime, and with qualitative results being only rarely inferior to the full ART-2 implementation.\nART 3 builds on ART-2 by simulating rudimentary neurotransmitter regulation of synaptic activity by incorporating simulated sodium (Na+) and calcium (Ca2+) ion concentrations into the system's equations, which results in a more physiologically realistic means of partially inhibiting categories that trigger mismatch resets.\nARTMAP also known as Predictive ART, combines two slightly modified ART-1 or ART-2 units into a supervised learning structure where the first unit takes the input data and the second unit takes the correct output data, then used to make the minimum possible adjustment of the vigilance parameter in the first unit in order to make the correct classification.\n\nFuzzy ART implements fuzzy logic into ART’s pattern recognition, thus enhancing generalizability. An optional (and very useful) feature of fuzzy ART is complement coding, a means of incorporating the absence of features into pattern classifications, which goes a long way towards preventing inefficient and unnecessary category proliferation. The applied similarity measures are based on the L1 norm. Fuzzy ART is known to be very sensitive to noise.\n\nFuzzy ARTMAP is merely ARTMAP using fuzzy ART units, resulting in a corresponding increase in efficacy.\n\nSimplified Fuzzy ARTMAP (SFAM) constitutes a strongly simplified variant of fuzzy ARTMAP dedicated to classification tasks.\n\nGaussian ART and Gaussian ARTMAP use Gaussian activation functions and computations based on probability theory. Therefore, they have some similarity with Gaussian mixture models. In comparison to fuzzy ART and fuzzy ARTMAP, they are less sensitive to noise. But the stability of learnt representations is reduced which may lead to category proliferation in open-ended learning tasks.\n\nFusion ART and related networks extend ART and ARTMAP to multiple pattern channels. They support several learning paradigms.\n\nTopoART combines fuzzy ART with topology learning networks such as the growing neural gas. Furthermore, it adds a noise reduction mechanism. There are several derived neural networks which extend TopoART to further learning paradigms.\n\nHypersphere ART and Hypersphere ARTMAP are closely related to fuzzy ART and fuzzy ARTMAP, respectively. But as they use a different type of category representation (namely hyperspheres), they do not require their input to be normalised to the interval [0, 1]. They apply similarity measures based on the L2 norm.\n\nLAPARTThe Laterally Primed Adaptive Resonance Theory (LAPART) neural networks couple two Fuzzy ART algorithms to create a mechanism for making predictions based on learned associations. The coupling of the two Fuzzy ARTs has a unique stability that allows the system to converge rapidly towards a clear solution. Additionally, it can perform logical inference and supervised learning similar to fuzzy ARTMAP.\n\nIt has been noted that results of Fuzzy ART and ART 1 depend critically upon the order in which the training data are processed. The effect can be reduced to some extent by using a slower learning rate, but is present regardless of the size of the input data set. Hence Fuzzy ART and ART 1 estimates do not possess the statistical property of consistency.\n\nWasserman, Philip D. (1989), Neural computing: theory and practice, New York: Van Nostrand Reinhold, \n\n", "id": "3056879", "title": "Adaptive resonance theory"}
{"url": "https://en.wikipedia.org/wiki?curid=4231161", "text": "ALOPEX\n\nALOPEX (an acronym from \"\"ALgorithms Of Pattern EXtraction\"\") is a correlation based machine learning algorithm first proposed by Tzanakou and Harth in 1974.\n\nIn machine learning, the goal is to train a system to minimize a cost function or (referring to ALOPEX) a response function. Many training algorithms, such as backpropagation, have an inherent susceptibility to getting \"stuck\" in local minima or maxima of the response function. ALOPEX uses a cross-correlation of differences and a stochastic process to overcome this in an attempt to reach the absolute minimum (or maximum) of the response function.\n\nALOPEX, in its simplest form is defined by an updating equation:\n\nformula_1 \n\nWhere:\n\nEssentially, ALOPEX changes each system variable formula_15 based on a product of: the previous change in the variable formula_16formula_17, the resulting change in the cost function formula_16formula_19, and the learning rate parameter formula_9. Further, to find the absolute minimum (or maximum), the stochastic process formula_21 (Gaussian or other) is added to stochastically \"push\" the algorithm out of any local minima.\n\n", "id": "4231161", "title": "ALOPEX"}
{"url": "https://en.wikipedia.org/wiki?curid=349771", "text": "Artificial neuron\n\nAn artificial neuron is a mathematical function conceived as a model of biological neurons, a neural network. Artificial neurons are elementary units in an artificial neural network. The artificial neuron receives one or more inputs (representing excitatory postsynaptic potentials and inhibitory postsynaptic potentials at neural dendrites) and sums them to produce an output (or , representing a neuron's action potential which is transmitted along its axon). Usually each input is separately weighted, and the sum is passed through a non-linear function known as an activation function or transfer function. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often monotonically increasing, continuous, differentiable and bounded. The thresholding function has inspired building logic gates referred to as threshold logic; applicable to building logic circuits resembling brain processing. For example, new devices such as memristors have been extensively used to develop such logic in recent times.\n\nThe artificial neuron transfer function should not be confused with a linear system's transfer function.\n\nFor a given artificial neuron, let there be \"m\" + 1 inputs with signals \"x\" through \"x\" and weights \"w\" through \"w\". Usually, the \"x\" input is assigned the value +1, which makes it a \"bias\" input with \"w\" = \"b\". This leaves only \"m\" actual inputs to the neuron: from \"x\" to \"x\".\n\nThe output of the \"k\"th neuron is:\n\nWhere formula_2 (phi) is the transfer function.\n\nThe output is analogous to the axon of a biological neuron, and its value propagates to the input of the next layer, through a synapse. It may also exit the system, possibly as part of an output vector.\n\nIt has no learning process as such. Its transfer function weights are calculated and threshold value are predetermined.\n\nDepending on the specific model used they may be called a semi-linear unit, Nv neuron, binary neuron, linear threshold function, or McCulloch–Pitts (MCP) neuron.\n\nSimple artificial neurons, such as the McCulloch–Pitts model, are sometimes described as \"caricature models\", since they are intended to reflect one or more neurophysiological observations, but without regard to realism.\n\nArtificial neurons are designed to mimic aspects of their biological counterparts.\n\nUnlike most artificial neurons, however, biological neurons fire in discrete pulses. Each time the electrical potential inside the soma reaches a certain threshold, a pulse is transmitted down the axon. This pulsing can be translated into continuous values. The rate (activations per second, etc.) at which an axon fires converts directly into the rate at which neighboring cells get signal ions introduced into them. The faster a biological neuron fires, the faster nearby neurons accumulate electrical potential (or lose electrical potential, depending on the \"weighting\" of the dendrite that connects to the neuron that fired). It is this conversion that allows computer scientists and mathematicians to simulate biological neural networks using artificial neurons which can output distinct values (often from −1 to 1).\n\nResearch has shown that unary coding is used in the neural circuits responsible for birdsong production. The use of unary in biological networks is presumably due to the inherent simplicity of the coding. Another contributing factor could be that unary coding provides a certain degree of error correction.\n\nThe first artificial neuron was the Threshold Logic Unit (TLU), or Linear Threshold Unit, first proposed by Warren McCulloch and Walter Pitts in 1943. The model was specifically targeted as a computational model of the \"nerve net\" in the brain. As a transfer function, it employed a threshold, equivalent to using the Heaviside step function. Initially, only a simple model was considered, with binary inputs and outputs, some restrictions on the possible weights, and a more flexible threshold value. Since the beginning it was already noticed that any boolean function could be implemented by networks of such devices, what is easily seen from the fact that one can implement the AND and OR functions, and use them in the disjunctive or the conjunctive normal form.\nResearchers also soon realized that cyclic networks, with feedbacks through neurons, could define dynamical systems with memory, but most of the research concentrated (and still does) on strictly feed-forward networks because of the smaller difficulty they present.\n\nOne important and pioneering artificial neural network that used the linear threshold function was the perceptron, developed by Frank Rosenblatt. This model already considered more flexible weight values in the neurons, and was used in machines with adaptive capabilities. The representation of the threshold values as a bias term was introduced by Bernard Widrow in 1960 – see ADALINE.\n\nIn the late 1980s, when research on neural networks regained strength, neurons with more continuous shapes started to be considered. The possibility of differentiating the activation function allows the direct use of the gradient descent and other optimization algorithms for the adjustment of the weights. Neural networks also started to be used as a general function approximation model. The best known training algorithm called backpropagation has been rediscovered several times but its first development goes back to the work of Paul Werbos.\n\nThe transfer function of a neuron is chosen to have a number of properties which either enhance or simplify the network containing the neuron. Crucially, for instance, any multilayer perceptron using a \"linear\" transfer function has an equivalent single-layer network; a non-linear function is therefore necessary to gain the advantages of a multi-layer network.\n\nBelow, \"u\" refers in all cases to the weighted sum of all the inputs to the neuron, i.e. for \"n\" inputs,\n\nwhere w is a vector of \"synaptic weights\" and x is a vector of inputs.\n\nThe output \"y\" of this transfer function is binary, depending on whether the input meets a specified threshold, \"θ\". The \"signal\" is sent, i.e. the output is set to one, if the activation meets the threshold.\n\nThis function is used in perceptrons and often shows up in many other models. It performs a division of the space of inputs by a hyperplane. It is specially useful in the last layer of a network intended to perform binary classification of the inputs. It can be approximated from other sigmoidal functions by assigning large values to the weights.\n\nIn this case, the output unit is simply the weighted sum of its inputs plus a \"bias\" term. A number of such linear neurons perform a linear transformation of the input vector. This is usually more useful in the first layers of a network. A number of analysis tools exist based on linear models, such as harmonic analysis, and they can all be used in neural networks with this linear neuron. The bias term allows us to make affine transformations to the data.\n\nSee: Linear transformation, Harmonic analysis, Linear filter, Wavelet, Principal component analysis, Independent component analysis, Deconvolution.\n\nA fairly simple non-linear function, the sigmoid function such as the logistic function also has an easily calculated derivative, which can be important when calculating the weight updates in the network. It thus makes the network more easily manipulable mathematically, and was attractive to early computer scientists who needed to minimize the computational load of their simulations. It was previously commonly seen in multilayer perceptrons. However, recent work has shown sigmoid neurons to be less effective than rectified linear neurons. The reason is that the gradients computed by the backpropagation algorithm tend to diminish towards zero as activations propagate through layers of sigmoidal neurons, making it difficult to optimize neural networks using multiple layers of sigmoidal neurons.\n\nThe following is a simple pseudocode implementation of a single TLU which takes boolean inputs (true or false), and returns a single boolean output when activated. An object-oriented model is used. No method of training is defined, since several exist. If a purely functional model were used, the class TLU below would be replaced with a function TLU with input parameters threshold, weights, and inputs that returned a boolean value.\n\n\n", "id": "349771", "title": "Artificial neuron"}
{"url": "https://en.wikipedia.org/wiki?curid=9054429", "text": "Autoassociative memory\n\nAutoassociative memory, also known as auto-association memory or an autoassociation network, is any type of memory that enables one to retrieve a piece of data from only a tiny sample of itself. It is often misunderstood to be only a form of backpropagation or other neural networks.\n\nTraditional memory stores data at a unique address and can \"recall\" the data upon presentation of the complete unique address.\n\nAutoassociative memories are capable of retrieving a piece of data upon presentation of only partial information from \"that\" piece of data.\n\nFor example, the sentence fragments presented below are sufficient for most humans to recall the missing information.\n\n\nMany readers will realize the missing information is in fact:\n\n\nThis demonstrates the capability of autoassociative networks to recall the whole by using some of its parts.\n\nHeteroassociative memories, on the other hand, can recall an associated piece of datum from \"one\" category upon presentation of data from \"another\" category. Hopfield networks have been shown to act as autoassociative memory since they are capable of remembering data by observing a portion of that data.\n\nBidirectional associative memories (BAM) are artificial neural networks that have long been used for performing heteroassociative recall.\n\n", "id": "9054429", "title": "Autoassociative memory"}
{"url": "https://en.wikipedia.org/wiki?curid=1360091", "text": "Backpropagation\n\nBackpropagation is a method used in artificial neural networks to calculate the error contribution of each neuron after a batch of data (in image recognition, multiple images) is processed. It is a special case of an older and more general technique called automatic differentiation. In the context of learning, backpropagation is commonly used by the gradient descent optimization algorithm to adjust the weight of neurons by calculating the gradient of the loss function. This technique is also sometimes called backward propagation of errors, because the error is calculated at the output and distributed back through the network layers.\n\nThe backpropagation algorithm has been repeatedly rediscovered and is equivalent to automatic differentiation in reverse accumulation mode.\nBackpropagation requires a known, desired output for each input value—it is therefore considered to be a supervised learning method (although it is used in some unsupervised networks such as autoencoders). Backpropagation is also a generalization of the delta rule to multi-layered feedforward networks, made possible by using the chain rule to iteratively compute gradients for each layer. It is closely related to the Gauss–Newton algorithm, and is part of continuing research in neural backpropagation. Backpropagation can be used with any gradient-based optimizer, such as L-BFGS or truncated Newton. \n\nBackpropagation is commonly used to train deep neural networks , a term used to describe neural networks with more than one hidden layer. \n\nThe goal of any supervised learning algorithm is to find a function that best maps a set of inputs to their correct output. An example would be a classification task, where the input is an image of an animal, and the correct output is the name of the animal.\n\nThe motivation for backpropagation is to train a multi-layered neural network such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output.\n\nSometimes referred to as the cost function or error function (not to be confused with the Gauss error function), the loss function is a function that maps values of one or more variables onto a real number intuitively representing some \"cost\" associated with those values. For backpropagation, the loss function calculates the difference between the network output and its expected output, after a case propagates through the network. \n\nTwo assumptions must be made about the form of the error function. The first is that it can be written as an average formula_1 over error functions formula_2, for formula_3 individual training examples, formula_4. The reason for this assumption is that the backpropagation algorithm calculates the gradient of the error function for a single training example, which needs to be generalized to the overall error function. The second assumption is that it can be written as a function of the outputs from the neural network.\n\nLet formula_5 be vectors in formula_6.\n\nSelect an error function formula_7 measuring the difference between two outputs. The standard choice is the square of the Euclidean distance between the vectors formula_8 and formula_9:\n\nformula_10 ()\n\nNote that the factor of formula_11 conveniently cancels the exponent when the error function is subsequently differentiated. \n\nThe error function over formula_3 training examples can simply be written as an average of losses over individual examples:formula_13\n\nand therefore, the partial derivative with respect to the outputs:formula_14\n\nThe optimization algorithm repeats a two phase cycle, propagation and weight update. When an input vector is presented to the network, it is propagated forward through the network, layer by layer, until it reaches the output layer. The output of the network is then compared to the desired output, using a loss function. The resulting error value is calculated for each of the neurons in the output layer. The error values are then propagated from the output back through the network, until each neuron has an associated error value that reflects its contribution to the original output.\n\nBackpropagation uses these error values to calculate the gradient of the loss function. In the second phase, this gradient is fed to the optimization method, which in turn uses it to update the weights, in an attempt to minimize the loss function.\n\nLet formula_15 be a neural network with formula_16 connections, formula_17 inputs, and formula_18 outputs.\n\nBelow, formula_19 will denote vectors in formula_20, formula_21 vectors in formula_6, and formula_23 vectors in formula_24. \nThese are called \"inputs\", \"outputs\" and \"weights\" respectively.\n\nThe neural network corresponds to a function formula_25 which, given a weight formula_26, maps an input formula_27 to an output formula_8.\n\nThe optimization takes as input a sequence of \"training examples\" formula_29 and produces a sequence of weights formula_30 starting from some initial weight formula_31, usually chosen at random.\n\nThese weights are computed in turn: first compute formula_32 using only formula_33 for formula_34. The output of the algorithm is then formula_35, giving us a new function formula_36. The computation is the same in each step, hence only the case formula_37 is described.\n\nCalculating formula_38 from formula_39 is done by considering a variable weight formula_26 and applying gradient descent to the function formula_41 to find a local minimum, \nstarting at formula_42.\n\nThis makes formula_38 the minimizing weight found by gradient descent.\n\nTo implement the algorithm above, explicit formulas are required for the gradient of the function formula_44 where the function isformula_45.\n\nThe learning algorithm can be divided into two phases: propagation and weight update.\n\nEach propagation involves the following steps:\n\n\nFor each weight, the following steps must be followed:\nThis ratio (percentage) influences the speed and quality of learning; it is called the \"learning rate\". The greater the ratio, the faster the neuron trains, but the lower the ratio, the more accurate the training is. The sign of the gradient of a weight indicates whether the error varies directly with, or inversely to, the weight. Therefore, the weight must be updated in the opposite direction, \"descending\" the gradient.\n\nLearning is repeated (on new batches) until the network performs adequately.\n\nThe following is pseudocode for a stochastic gradient descent algorithm for training a three-layer network (only one hidden layer):\n\nThe lines labeled \"backward pass\" can be implemented using the backpropagation algorithm, which calculates the gradient of the error of the network regarding the network's modifiable weights. \n\nTo understand the mathematical derivation of the backpropagation algorithm, it helps to first develop some intuitions about the relationship between the actual output of a neuron and the correct output for a particular training case. Consider a simple neural network with two input units, one output unit and no hidden units. Each neuron uses a linear output that is the weighted sum of its input. \n\nInitially, before training, the weights will be set randomly. Then the neuron learns from training examples, which in this case consists of a set of tuples formula_46 where formula_47 and formula_48 are the inputs to the network and is the correct output (the output the network should eventually produce given those inputs). The initial network, given formula_47 and formula_48, will compute an output that likely differs from (given random weights). A common method for measuring the discrepancy between the expected output and the actual output is the squared error measure:\n\nwhere is the discrepancy or error.\n\nAs an example, consider the network on a single training case: formula_52, thus the input formula_47 and formula_48 are 1 and 1 respectively and the correct output, is 0. Now if the actual output is plotted on the horizontal axis against the error on the vertical axis, the result is a parabola. The minimum of the parabola corresponds to the output which minimizes the error . For a single training case, the minimum also touches the horizontal axis, which means the error will be zero and the network can produce an output that exactly matches the expected output . Therefore, the problem of mapping inputs to outputs can be reduced to an optimization problem of finding a function that will produce the minimal error. \n\nHowever, the output of a neuron depends on the weighted sum of all its inputs:\n\nwhere formula_38 and formula_57 are the weights on the connection from the input units to the output unit. Therefore, the error also depends on the incoming weights to the neuron, which is ultimately what needs to be changed in the network to enable learning. If each weight is plotted on a separate horizontal axis and the error on the vertical axis, the result is a parabolic bowl. For a neuron with weights, the same plot would require an elliptic paraboloid of formula_58 dimensions.\n\nOne commonly used algorithm to find the set of weights that minimizes the error is gradient descent. Backpropagation is then used to calculate the steepest descent direction.\n\nThe gradient descent method involves calculating the derivative of the squared error function with respect to the weights of the network. This is normally done using backpropagation. Assuming one output neuron, the squared error function is:\n\nwhere\n\nThe factor of formula_63 is included to cancel the exponent when differentiating. Later, the expression will be multiplied with an arbitrary learning rate, so that it doesn't matter if a constant coefficient is introduced now.\n\nFor each neuron formula_64, its output formula_65 is defined as\n\nThe input formula_67 to a neuron is the weighted sum of outputs formula_68 of previous neurons. If the neuron is in the first layer after the input layer, the formula_68 of the input layer are simply the inputs formula_70 to the network. The number of input units to the neuron is formula_18. The variable formula_72 denotes the weight between neurons formula_73 and formula_64.\n\nThe activation function formula_75 is non-linear and differentiable. A commonly used activation function is the logistic function:\n\nwhich has a convenient derivative of:\n\nCalculating the partial derivative of the error with respect to a weight formula_78 is done using the chain rule twice:\n\nIn the last factor of the right-hand side of the above, only one term in the sum formula_67 depends on formula_78, so that \n\nIf the neuron is in the first layer after the input layer, formula_83 is just formula_84.\n\nThe derivative of the output of neuron formula_64 with respect to its input is simply the partial derivative of the activation function (assuming here that the logistic function is used):\n\nThis is the reason why backpropagation requires the activation function to be differentiable.\n\nThe first factor is straightforward to evaluate if the neuron is in the output layer, because then formula_87 and\n\nHowever, if formula_64 is in an arbitrary inner layer of the network, finding the derivative formula_60 with respect to formula_65 is less obvious.\n\nConsidering formula_60 as a function of the inputs of all neurons formula_93 receiving input from neuron formula_64, \n\nand taking the total derivative with respect to formula_65, a recursive expression for the derivative is obtained:\n\nTherefore, the derivative with respect to formula_65 can be calculated if all the derivatives with respect to the outputs formula_99 of the next layer – the one closer to the output neuron – are known.\n\nPutting it all together:\n\nwith\n\nTo update the weight formula_78 using gradient descent, one must choose a learning rate, formula_103. The change in weight, which is added to the old weight, is equal to the product of the learning rate and the gradient, multiplied by formula_104:\n\nThe formula_106 is required in order to update in the direction of a minimum, not a maximum, of the error function.\n\nFor a single-layer network, this expression becomes the Delta Rule.\n\nThe choice of learning rate formula_107 is important, since a high value can cause too strong a change, causing the minimum to be missed, while a too low learning rate slows the training unnecessarily.\n\nOptimizations such as Quickprop are primarily aimed at speeding up error minimization; other improvements mainly try to increase reliability.\n\nIn order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements of this algorithm use an adaptive learning rate.\n\nBy using a variable inertia term \"(Momentum)\" formula_108 the gradient and the last change can be weighted such that the weight adjustment additionally depends on the previous change. If the \"Momentum\" formula_108 is equal to 0, the change depends solely on the gradient, while a value of 1 will only depend on the last change.\n\nSimilar to a ball rolling down a mountain, whose current speed is determined not only by the current slope of the mountain but also by its own inertia, inertia can be added:formula_110where:\n\nInertia depends on the current weight change formula_126 both from the current gradient of the error function (slope of the mountain, 1st summand), as well as from the weight change from the previous point in time (inertia, 2nd summand).\n\nWith inertia, the problems of getting stuck (in steep ravines and flat plateaus) are avoided. Since, for example, the gradient of the error function becomes very small in flat plateaus, inertia would immediately lead to a \"deceleration\" of the gradient descent. This deceleration is delayed by the addition of the inertia term so that a flat plateau can be escaped more quickly.\n\nTwo modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces \"noise\" into the gradient descent process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in a local minima. However, batch learning typically yields a faster, more stable descent to a local minima, since each update is performed in the direction of the average error of the batch. A common compromise choice is to use \"mini-batches\", meaning small batches and with samples in each batch selected stochastically from the entire data set.\n\n\nAccording to various sources, the basics of continuous backpropagation were derived in the context of control theory by Henry J. Kelley in 1960 and by Arthur E. Bryson in 1961. They used principles of dynamic programming. In 1962, Stuart Dreyfus published a simpler derivation based only on the chain rule. Bryson and Ho described it as a multi-stage dynamic system optimization method in 1969.\n\nIn 1970 Linnainmaa published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions. This corresponds to backpropagation, which is efficient even for sparse networks.\n\nIn 1973 Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. In 1974 Werbos mentioned the possibility of applying this principle to artificial neural networks, and in 1982 he applied Linnainmaa's AD method to neural networks in the way that is used today.\n\nIn 1986 Rumelhart, Hinton and Williams showed experimentally that this method can generate useful internal representations of incoming data in hidden layers of neural networks. In 1993, Wan was the first to win an international pattern recognition contest through backpropagation.\n\nDuring the 2000s it fell out of favour, but returned in the 2010s, benefitting from cheap, powerful GPU-based computing systems.\n\n\n", "id": "1360091", "title": "Backpropagation"}
{"url": "https://en.wikipedia.org/wiki?curid=27569062", "text": "Backpropagation through time\n\nBackpropagation through time (BPTT) is a gradient-based technique for training certain types of recurrent neural networks. It can be used to train Elman networks. The algorithm was independently derived by numerous researchers\n\nThe training data for a recurrent neural network is an ordered sequence of formula_1 input-output pairs, formula_2. An initial value must be specified for the hidden state formula_3. Typically, a vector of all zeros is used for this purpose.\n\nBPTT begins by unfolding a recurrent neural network in time. The unfolded network contains formula_1 inputs and outputs, but every copy of the network shares the same parameters. Then the backpropagation algorithm is used to find the gradient of the cost with respect to all the network parameters.\n\nConsider an example of a neural network that contains a recurrent layer formula_5 and a feedforward layer formula_6. There are different ways to define the training cost, but the total cost is always the average of the costs of each of the time steps. The cost of each time step can be computated separately. The figure above shows how the cost at time formula_7 can be computed, by unfolding the recurrent layer formula_5 for three time steps and adding the feedforward layer formula_6. Each instance of formula_5 in the unfolded network shares the same parameters. Thus the weight updates in each instance (formula_11) are summed together.\n\nPseudo-code for a truncated version of BPTT, where the training data contains formula_12 input-output pairs, but the network is unfolded for formula_1 time steps:\n\nBPTT tends to be significantly faster for training recurrent neural networks than general-purpose optimization techniques such as evolutionary optimization.\n\nBPTT has difficulty with local optima. With recurrent neural networks, local optima are a much more significant problem than with feed-forward neural networks. The recurrent feedback in such networks tends to create chaotic responses in the error surface which cause local optima to occur frequently, and in poor locations on the error surface.\n\n", "id": "27569062", "title": "Backpropagation through time"}
{"url": "https://en.wikipedia.org/wiki?curid=33025196", "text": "Bcpnn\n\nA Bayesian Confidence Neural Network (BCPNN) is an artificial neural network inspired by Bayes' theorem: node activations represent probability (\"confidence\") in the presence of input features or categories, synaptic weights are based on estimated correlations and the spread of activation corresponds to calculating posteriori probabilities. It was originally proposed by Anders Lansner and Örjan Ekeberg at KTH.\n\nThe basic network is a feedforward neural network with continuous activation. This can be extended to include spiking units and hypercolumns, representing mutually exclusive or interval coded features. This network has been used for classification tasks and data mining, for example for discovery of adverse drug reactions. The units can also be connected as a recurrent neural network (losing the strict interpretation of their activations as probabilities) but becoming a possible abstract model of biological neural networks and memory.\n", "id": "33025196", "title": "Bcpnn"}
{"url": "https://en.wikipedia.org/wiki?curid=17290783", "text": "Bidirectional associative memory\n\nBidirectional associative memory (BAM) is a type of recurrent neural network. BAM was introduced by Bart Kosko in 1988. There are two types of associative memory, auto-associative and hetero-associative. BAM is hetero-associative, meaning given a pattern it can return another pattern which is potentially of a different size. It is similar to the Hopfield network in that they are both forms of associative memory. However, Hopfield nets return patterns of the same size.\n\nA BAM contains two layers of neurons, which we shall denote X and Y. Layers X and Y are fully connected to each other. Once the weights have been established, input into layer X presents the pattern in layer Y, and vice versa.\n\nImagine we wish to store two associations, A1:B1 and A2:B2.\n\nThese are then transformed into the bipolar forms:\n\nFrom there, we calculate formula_1 where formula_2 denotes the transpose.\nSo,\n\nformula_3\n\nTo retrieve the association A1, we multiply it by M to get (4, 2, -2, -4), which, when run through a threshold, yields (1, 1, 0, 0), which is B1.\nTo find the reverse association, multiply this by the transpose of M.\n\nThe internal matrix has n x p independent degrees of freedom, where n is the dimension of the first vector (6 in this example) and p is the dimension of the second vector (4). This allows the BAM to be able to reliably store and recall a total of up to min(n,p) independent vector pairs, or min(6,4) = 4 in this example. The capacity can be increased above 4 if one gives up reliability and is willing to accept incorrect bits on the output.\n\n\n", "id": "17290783", "title": "Bidirectional associative memory"}
{"url": "https://en.wikipedia.org/wiki?curid=1166059", "text": "Boltzmann machine\n\nA Boltzmann machine (also called stochastic Hopfield network with hidden units) is a type of stochastic recurrent neural network (and Markov Random Field). \n\nBoltzmann machines can be seen as the stochastic, generative counterpart of Hopfield nets. They were one of the first neural networks capable of learning internal representations, and are able to represent and (given sufficient time) solve difficult combinatoric problems. \n\nThey are theoretically intriguing because of the locality and Hebbian nature of their training algorithm, and because of their parallelism and the resemblance of their dynamics to simple physical processes. Boltzmann machines with unconstrained connectivity have not proven useful for practical problems in machine learning or inference, but if the connectivity is properly constrained, the learning can be made efficient enough to be useful for practical problems.\n\nThey are named after the Boltzmann distribution in statistical mechanics, which is used in their sampling function. They were invented in 1985 by Geoffrey Hinton, then a Professor at Carnegie Mellon University, and Terry Sejnowski, then a Professor at Johns Hopkins University.\n\nA Boltzmann machine, like a Hopfield network, is a network of units with an \"energy\" defined for the overall network. Its units produce results. Unlike Hopfield nets, Boltzmann machine units are stochastic. The global energy, formula_1, in a Boltzmann machine is identical in form to that of a Hopfield network:\n\nWhere:\n\nOften the weights are represented as a symmetric matrix formula_12, with zeros along the diagonal.\n\nThe difference in the global energy that results from a single unit formula_5 equaling 0 (off) versus 1 (on), written formula_14, assuming a symmetric matrix of weights, is given by:\n\nSubstituting the energy of each state with its relative probability according to the Boltzmann Factor (the property of a Boltzmann distribution that the energy of a state is proportional to the negative log probability of that state) gives:\n\nwhere formula_17 is Boltzmann's constant and is absorbed into the artificial notion of temperature formula_18. We then rearrange terms and consider that the probabilities of the unit being on and off must sum to one:\n\nSolving for formula_25, the probability that the formula_5-th unit is on gives:\n\nwhere the scalar formula_18 is referred to as the temperature of the system. This relation is the source of the logistic function found in probability expressions in variants of the Boltzmann machine.\n\nThe network runs by repeatedly choosing a unit and resetting its state. After running for long enough at a certain temperature, the probability of a global state of the network depends only upon that global state's energy, according to a Boltzmann distribution, and not on the initial state from which the process was started. This means that log-probabilities of global states become linear in their energies. This relationship is true when the machine is \"at thermal equilibrium\", meaning that the probability distribution of global states has converged. Running the network beginning from a high temperature, its temperature gradually decreases until reaching a thermal equilibrium at a lower temperature. It then may converge to a distribution where the energy level fluctuates around the global minimum. This process is called simulated annealing.\n\nTo train the network so that the chance it will converge to a global state is according to an external distribution over these states, the weights must be set so that the global states with the highest probabilities get the lowest energies. This is done by training.\n\nThe units in the Boltzmann Machine are divided into 'visible' units, V, and 'hidden' units, H. The visible units are those that receive information from the 'environment', i.e. the training set is a set of binary vectors over the set V. The distribution over the training set is denoted formula_29. \nAs is discussed above, the distribution over global states converges as the Boltzmann machine reaches thermal equilibrium. We denote this distribution, after we marginalize it over the hidden units, as formula_30.\n\nOur goal is to approximate the \"real\" distribution formula_29 using the formula_30 produced (eventually) by the machine. To measure how similar the two distributions are, the Kullback–Leibler divergence, formula_33 is\n\nwhere the sum is over all the possible states of formula_35. formula_33 is a function of the weights, since they determine the energy of a state, and the energy determines formula_37, as promised by the Boltzmann distribution. Hence, we can use a gradient descent algorithm over formula_33, so a given weight, formula_3 is changed by subtracting the partial derivative of formula_33 with respect to the weight.\n\nThere are two alternating phases to Boltzmann machine training. One is the \"positive\" phase where the visible units' states are clamped to a particular binary state vector sampled from the training set (according to formula_41). The other is the \"negative\" phase where the network is allowed to run freely, i.e. no units have their state determined by external data. Surprisingly enough, the gradient with respect to a given weight, formula_3, is given by the simple proven equation:\n\nwhere:\n\nThis result follows from the fact that at thermal equilibrium the probability formula_47 of any global state formula_48 when the network is free-running is given by the Boltzmann distribution (hence the name \"Boltzmann machine\").\n\nRemarkably, this learning rule is fairly biologically plausible because the only information needed to change the weights is provided by \"local\" information. That is, the connection (or synapse biologically speaking) does not need information about anything other than the two neurons it connects. This is far more biologically realistic than the information needed by a connection in many other neural network training algorithms, such as backpropagation.\n\nThe training of a Boltzmann machine does not use the EM algorithm, which is heavily used in machine learning. \nBy minimizing the KL-divergence, it is equivalent to maximizing the log-likelihood of the data. Therefore, the training procedure performs gradient ascent on the log-likelihood of the observed data. This is in contrast to the EM algorithm, where the posterior distribution of the hidden nodes must be calculated before the maximization of the expected value of the complete data likelihood during the M-step.\n\nTraining the biases is similar, but uses only single node activity:\n\nThe Boltzmann machine would theoretically be a rather general computational medium. For instance, if trained on photographs, the machine would theoretically model the distribution of photographs, and could use that model to, for example, complete a partial photograph.\n\nUnfortunately, there is a serious practical problem with the Boltzmann machine, namely that it seems to stop learning correctly when the machine is scaled up to anything larger than a trivial machine. This is due to a number of effects, the most important of which are:\n\n\nAlthough learning is impractical in general Boltzmann machines, it can be made quite efficient in \nan architecture called the \"restricted Boltzmann machine\" or \"RBM\" which does not allow intralayer connections between hidden units. After training one RBM, the activities of its hidden units can be treated as data for training a higher-level RBM. This method of stacking RBMs makes it possible to train many layers of hidden units efficiently and is one of the most common deep learning strategies. As each new layer is added the overall generative model gets better.\n\nThere is an extension to the restricted Boltzmann machine that affords using real valued data rather than binary data. Along with higher order Boltzmann machines, it is outlined here .\n\nOne example of a practical application of Restricted Boltzmann machines is the performance improvement of speech recognition software.\n\nA deep Boltzmann machine (DBM) is a type of binary pairwise Markov random field (undirected probabilistic graphical model) with multiple layers of hidden random variables. It is a network of symmetrically coupled stochastic binary units. It comprises a set of visible units formula_50 and layers of hidden units formula_51. No connection links units of the same layer (like RBM). For the , the probability assigned to vector is\nwhere formula_53 are the set of hidden units, and formula_54 are the model parameters, representing visible-hidden and hidden-hidden interactions. Only the top two layers form a restricted Boltzmann machine (which is an undirected graphical model), while lower layers form a directed generative model.\n\nLike DBNs, DBMs can learn complex and abstract internal representations of the input in tasks such as object or speech recognition, using limited, labeled data to fine-tune the representations built using a large supply of unlabeled sensory input data. However, unlike and deep convolutional neural networks, they adopt the inference and training procedure in both directions, bottom-up and top-down pass, which allow the to better unveil the representations of the input structures.\n\nHowever, the slow speed of DBMs limits their performance and functionality. Because exact maximum likelihood learning is intractable for DBMs, only approximate maximum likelihood learning is possible. Another option is to use mean-field inference to estimate data-dependent expectations and approximate the expected sufficient statistics by using \"Markov chain Monte Carlo\" \"(MCMC)\". This approximate inference, which must be done for each test input, is about 25 to 50 times slower than a single bottom-up pass in DBMs. This makes joint optimization impractical for large data sets, and restricts the use of DBMs for tasks such as feature representation.\n\nThe Boltzmann machine is a Monte Carlo version of the Hopfield network.\n\nThe idea of using stochastic weights in Ising models considered by:\n\nHowever, in cognitive sciences, it is often thought to have been first described by:\n\nHowever, it should be noted that these articles appeared after the seminal publication by John Hopfield, where the connection to physics and statistical mechanics was made in the first place, mentioning spin glasses:\n\n\nThe idea of applying the Ising model with annealed Gibbs sampling is also present in Douglas Hofstadter's Copycat project:\n\n\nSimilar ideas (with a change of sign in the energy function) are also found in Paul Smolensky's \"Harmony Theory\".\n\nThe explicit analogy drawn with statistical mechanics in the Boltzmann Machine formulation led to the use of terminology borrowed from physics (e.g., \"energy\" rather than \"harmony\"), which has become standard in the field. The widespread adoption of this terminology may have been encouraged by the fact that its use led to the importation of a variety of concepts and methods from statistical mechanics.\nHowever, there is no reason to think that the various proposals to use simulated annealing for inference described above were not independent.\n\nIsing models are now considered to be a special case of Markov random fields, which find widespread application in various fields, including linguistics, robotics, computer vision, and artificial intelligence.\n\n\n\n", "id": "1166059", "title": "Boltzmann machine"}
{"url": "https://en.wikipedia.org/wiki?curid=2506529", "text": "Cellular neural network\n\nIn computer science and machine learning, cellular neural networks (CNN) (or cellular nonlinear networks (CNN)) are a parallel computing paradigm similar to neural networks, with the difference that communication is allowed between neighbouring units only. Typical applications include image processing, analyzing 3D surfaces, solving partial differential equations, reducing non-visual problems to geometric maps, modelling biological vision and other sensory-motor organs.\n\nDue to their number and variety of architectures, it is difficult to give a precise definition for a CNN processor. From an architecture standpoint, CNN processors are a system of a finite, fixed-number, fixed-location, fixed-topology, locally interconnected, multiple-input, single-output, nonlinear processing units. The nonlinear processing units are often referred to as neurons or cells. Mathematically, each cell can be modeled as a dissipative, nonlinear dynamical system where information is encoded via its initial state, inputs and variables used to define its behavior. Dynamics are usually continuous, as in the case of Continuous-Time CNN (CT-CNN) processors, but can be discrete, as in the case of Discrete-Time CNN (DT-CNN) processors. Each cell has one output, by which it communicates its state with both other cells and external devices. Output is typically real-valued, but can be complex or even quaternion, i.e. a Multi-Valued CNN (MV-CNN). In most CNN processors, processing units are identical, but there are applications that require non-identical units, which are called Non-Uniform Processor CNN (NUP-CNN) processors, and consist of different types of cells. In the original Chua-Yang CNN (CY-CNN) processor, the state of the cell was a weighted sum of the inputs and the output was a piecewise linear function. However, like the original perceptron-based neural networks, the functions it could perform were limited: specifically, it was incapable of modeling non-linear functions, such as XOR. More complex functions are realizable via Non-Linear CNN (NL-CNN) processors.\n\nCells are defined in a normed space, commonly a two-dimensional Euclidean geometry, like a grid. The cells are not limited to two-dimensional spaces however; they can be defined in an arbitrary number of dimensions and can be square, triangle, hexagonal, or any other spatially invariant arrangement. Topologically, cells can be arranged on an infinite plane or on a toroidal space. Cell interconnect is local, meaning that all connections between cells are within a specified radius (with distance measured topologically). Connections can also be time-delayed to allow for processing in the temporal domain.\n\nMost CNN architectures have cells with the same relative interconnects, but there are applications that require a spatially variant topology, i.e. Multiple-Neighborhood-Size CNN (MNS-CNN) processors. Also, Multiple-Layer CNN (ML-CNN) processors, where all cells on the same layer are identical, can be used to extend the capability of CNN processors.\n\nThe definition of a system is a collection of independent, interacting entities forming an integrated whole, whose behavior is distinct and qualitatively greater than its entities. Although connections are local, information exchange can happen globally through diffusion. In this sense, CNN processors are systems because their dynamics are derived from the interaction between the processing units and not within processing units. As a result, they exhibit emergent and collective behavior. Mathematically, the relationship between a cell and its neighbors, located within an area of influence, can be defined by a coupling law, and this is what primarily determines the behavior of the processor. When the coupling laws are modeled by fuzzy logic, it is a fuzzy CNN\n. When these laws are modeled by computational verb logic, it becomes a computational verb CNN (verb CNN)\n. Both fuzzy and verb CNNs are useful for modelling social networks when the local couplings are achieved by linguistic terms.\n\nThe idea of CNN processors was introduced by Leon Chua and Lin Yang’s two-part, 1988 article, \"Cellular Neural Networks: Theory\" and \"Cellular Neural Networks: Applications\" in IEEE Transactions on Circuits and Systems. In these articles, Chua and Yang outline the underlying mathematics behind CNN processors. They use this mathematical model to demonstrate, for a specific CNN implementation, that if the inputs are static, the processing units will converge, and can be used to perform useful calculations. They then suggest one of the first applications of CNN processors: image processing and pattern recognition (which is still the largest application to date). Leon Chua is still active in CNN research and publishes many of his articles in the International Journal of Bifurcation and Chaos, of which he is an editor. Both IEEE Transactions on Circuits and Systems and the International Journal of Bifurcation also contain a variety of useful articles on CNN processors authored by other knowledgeable researchers. The former tends to focus on new CNN architectures and the latter more on the dynamical aspects of CNN processors.\n\nAnother key article, Tamas Roska and Leon Chua’s 1993 article \"The CNN Universal Machine: An Analogic Array Computer\", introduced the first algorithmically programmable analog CNN processor to the engineering research community. The multi-national effort was funded by the Office of Naval Research, the National Science Foundation, and the Hungarian Academy of Sciences, and researched by the Hungarian Academy of Sciences and the University of California. This article proved that CNN processors were producible and provided researchers a physical platform to test their CNN theories. After this article, companies started to invest into larger, more capable processors, based on the same basic architecture as the CNN Universal Processor. Tamas Roska is another key contributor to CNNs. His name is often associated with biologically inspired information processing platforms and algorithms, and he has published numerous key articles and has been involved with companies and research institutions developing CNN technology.\n\nThere are several overviews of CNN processors in published literature. One of the better references is a paper, \"Cellular Neural Networks: A Review\" written for Neural Nets WIRN Vietri 1993, by Valerio Cimagalli and Marco Balsi. The paper provides definitions, CNN types, dynamics, implementations, and applications in a relatively small, readable document. There is also a book, \"Cellular Neural Networks and Visual Computing Foundations and Applications\", written by Leon Chua and Tamas Roska, which provides examples and exercises to help illustrates points in a manner uncommon for papers and journal articles. The book covers many different aspects of CNN processors and can serve as a textbook for a Masters or Ph.D. course. The two references are considered invaluable since they manage to organize the vast amount of CNN literature into a coherent framework.\n\nThe best place for CNN literature is from the proceedings of \"The International Workshop on Cellular Neural Networks and Their Applications\". The proceedings are available online, via IEEE Xplore, for conferences held in 1990, 1992, 1994, 1996, 1998, 2000, 2002, 2005 and 2006. There is also a workshop being held on July 14–16 in Santiago de Composetela, Spain. Topics include theory, design, applications, algorithms, physical implementations and programming/training methods. For an understanding of the analog semiconductor based CNN technology, AnaLogic Computers has their product line, in addition to the published articles available on their homepage and their publication list. They also have information on other CNN technologies such as optical computing. Many of the commonly used functions have already been implemented using CNN processors. A good reference point for some of these can be found in image processing libraries for CNN based visual computers such as Analogic’s CNN-based systems.\n\nCNN processors could be thought of as a hybrid between ANN and CA (Continuous Automata). The processing units of CNN and NN are similar. In both cases, the processor units are multi-input, dynamical systems, and the behavior of the overall systems is driven primarily through the weights of the processing unit’s linear interconnect. The main discriminator is that in CNN processors, connections are made locally, whereas in ANN, connections are global. For example, neurons in one layer are fully connected to another layer in a feed-forward NN and all the neurons are fully interconnected in Hopfield networks. In ANNs, the weights of interconnections contain information on the processing system’s previous state or feedback, but in CNN processors, the weights are used to determine the dynamics of the system. Furthermore, due to the high interconnectivity of ANNs, they tend not exploit locality in either the data set or the processing and as a result, they usually are highly redundant systems that allow for robust, fault-tolerant behavior without catastrophic errors. A cross between an ANN and a CNN processor is a Ratio Memory CNN (RMCNN). In RMCNN processors, the cell interconnect is local and topologically invariant, but the weights are used to store previous states and not to control dynamics. The weights of the cells are modified during some learning state creating long-term memory.\n\nThe topology and dynamics of CNN processors closely resembles that of CA. Like most CNN processors, CA consists of a fixed-number of identical processors that are spatially discrete and topologically uniform. The difference is that most CNN processors are continuous-valued whereas CA have discrete-values. Furthermore, the CNN processor's cell behavior is defined via some non-linear function whereas CA processor cells are defined by some state machine. However, there are some exceptions. Continuous Valued Cellular Automata or Continuous Automata are CA with continuous resolution. Depending on how a given Continuous Automata is specified, it can also be a CNN. There are also Continuous Spatial Automata, which consist of an infinite number of spatially continuous, continuous-valued automata. There is considerable work being performed in this field since continuous spaces are easier to mathematically model than discrete spaces, thus allowing a more quantitative approach as opposed to an empirical approach taken by some researchers of cellular automata. Continuous Spatial Automata processors can be physically realized though an unconventional information processing platform such as a chemical computer. Furthermore, it is conceivable that large CNN processors (in terms of the resolution of the input and output) can be modeled as a Continuous Spatial Automata.\n\nThe dynamical behaviors of CNN processors can be expressed mathematically as a series of ordinary differential equations, where each equation represents the state of an individual processing unit. The behavior of the entire CNN processor is defined by its initial conditions, the inputs, the cell interconnect (topology and weights), and the cells themselves. One possible use of CNN processors is to generate and respond to signals of specific dynamical properties. For example, CNN processors have been used to generate multi-scroll chaos, synchronize with chaotic systems, and exhibit multi-level hysterisis. CNN processors are designed specifically to solve local, low-level, processor intensive problems expressed as a function of space and time. For example, CNN processors can be used to implement high-pass and low-pass filters and morphological operators. They can also be used to approximate a wide range of Partial Differential Equations (PDE) such as heat dissipation and wave propagation.\n\nCNN processors can be used as Reaction-Diffusion (RD) processors. RD processors are spatially invariant, topologically invariant, analog, parallel processors characterized by reactions, where two agents can combine to create a third agent, and diffusions, the spreading of agents. RD processors are typically implemented through chemicals in a Petri dish (processor), light (input), and a camera (output) however RD processors can also be implemented through a multilayer CNN processor. RD processors can be used to create Voronoi diagrams and perform skeletonisation. The main difference between the chemical implementation and the CNN implementation is that CNN implementations are considerably faster than their chemical counterparts and chemical processors are spatially continuous whereas the CNN processors are spatially discrete. The most researched RD processor, Belousov-Zhabotinsky (BZ) processors, has already been simulated using a four-layer CNN processors and has been implemented in a semiconductor.\n\nLike CA, computations can be performed through the generation and propagation of signals that either grow or change over time. Computations can occur within a signal or can occur through the interaction between signals. One type of processing, which uses signals and is gaining momentum is wave processing, which involves the generation, expanding, and eventual collision of waves. Wave processing can be used to measure distances and find optimal paths. Computations can also occur through particles, gliders, solutions, and filterons localized structures that maintain their shape and velocity. Given how these structures interact/collide with each other and with static signals, they can be used to store information as states and implement different Boolean functions. Computations can also occur between complex, potentially growing or evolving localized behavior through worms, ladders, and pixel-snakes. In addition to storing states and performing Boolean functions, these structures can interact, create, and destroy static structures.\n\nAlthough CNN processors are primarily intended for analog calculations, certain types of CNN processors can implement any Boolean function, allowing simulating CA. Since some CA are Universal Turing machines (UTM), capable of simulating any algorithm can be performed on processors based on the von Neumann architecture, that makes this type of CNN processors, universal CNN, a UTM. One CNN architecture consists of an additional layer, similar to the ANN solution to the problem stated by Marvin Minsky years ago. CNN processors have resulted in the simplest realization of Conway’s Game of Life and Wolfram’s Rule 110, the simplest known universal Turing Machine. This unique, dynamical representation of an old systems, allows researchers to apply techniques and hardware developed for CNN to better understand important CA. Furthermore, the continuous state space of CNN processors, with slight modifications that have no equivalent in Cellular Automata, creates emergent behavior never seen before.\n\nAny information processing platform that allows the construction of arbitrary Boolean functions is called universal, and as result, this class CNN processors are commonly referred to as universal CNN processors. The original CNN processors can only perform linearly separable Boolean functions. This is essentially the same problem Marvin Minsky introduced with respect to the perceptions of the first neural networks In either case, by translating functions from digital logic or look-up table domains into the CNN domain, some functions can be considerably simplified. For example, the nine-bit, odd parity generation logic, which is typically implemented by eight nested exclusive-or gates, can also be represented by a sum function and four nested absolute value functions. Not only is there a reduction in the function complexity, but the CNN implementation parameters can be represented in the continuous, real-number domain.\n\nThere are two methods by which to select a CNN processor along with a template or weights. The first is by synthesis, which involves determine the coefficients offline. This can be done by leveraging off previous work, i.e. libraries, papers, and articles, or by mathematically deriving co that best suits the problem. The other is through training the processor. Researchers have used back-propagation and genetic algorithms to learn and perform functions. Back-propagation algorithms tend to be faster, but genetic algorithms are useful because they provide a mechanism to find a solution in a discontinuous, noisy search space.\n\nAn information processing platform remains nothing more than an intellectual exercise unless it can be implemented in hardware and integrated into a system. Although processors based on billiard balls can be interesting, unless their implementation provides advantages for a system, the only purpose they serve is as a teaching device. CNN processors have been implemented using current technology and there are plans to implement CNN processors into future technologies. They include the necessary interfaces for programming and interfacing, and have been implemented in a variety of systems. What follows is a cursory examination of the different types of CNN processors available today, their advantages and disadvantages, and the future roadmap for CNN processors.\n\nCNN processors have been implemented and are currently available as semiconductors and there are plans to migrate CNN processors to emerging technologies in the future. Semiconductor-based CNN processors can be segmented into analog CNN processors, digital CNN processors, and CNN processors emulated using digital processors. Analog CNN processors were the first to be developed. Analog computers were fairly common during the 1950 and 1960s, but they gradually were replaced by digital computers the 1970s. Analog processors were considerably faster in certain applications such as optimizing differential equations and modeling nonlinearities, but the reason why analog computing lost favor was the lack of precision and the difficulty to configure an analog computer to solve a complex equation. Analog CNN processors share some of the same advantages as their predecessors, specifically speed. The first analog CNN processors were able to perform real-time ultra-high frame-rate (>10,000 frame/s) processing unachievable by digital processors. The analog implementation of CNN processors requires less area and consumes less power than their digital counterparts. Although the accuracy of analog CNN processors does not compare to their digital counterparts, for many applications, noise and process variances are small enough not to perceptually affect the image quality.\n\nThe first algorithmically programmable, analog CNN processor was created in 1993. It was named the CNN Universal Processor because its internal controller allowed multiple templates to be performed on the same data set, thus simulating multiple layers and allowing for universal computation. Included in the design was a single layer 8x8 CCN, interfaces, analog memory, switching logic, and software. The processor was developed in order to determine CNN processor producibility and utility. The CNN concept proved promising and by 2000, there were at least six organizations designing algorithmically programmable, analog CNN processors. This is when AnaFocus, a mixed-signal semiconductor company that emerged from research at The University of Seville, introduced their ACE prototype CNN processor product line. Their first ACE processor contained 20x20 B/W processor units; their next ACE processor provided 48x48 grayscale processor units, and their latest ACE processor contains 128x128 grayscale processor units. Over time, not only did the number of processing elements increase, but their speed improved, the number of functions they can perform increased, and a seamless detector interface was integrated into the silicon (yielding considerably improved interface). The ability to embed the detector interface into the CNN processor allows for real-time interaction between the sensing and processing. AnaFocus has a multilayer CASE prototype CNN processors line. The latest CASE processor is a three layer 32x32 CNN processor. Their work in CNN processors is currently culminating in their soon-to-be-released, commercially available Eye-RIS product line that consists of all the processors, co-processors, software development kits, and support needed to program and integrate an analog processor into a system.\n\nAnaFocus is working with AnaLogic Computers, to include their CNN processors into visual systems. Founded in 2000, by many of the same researchers behind the first algorithmically programmable CNN Universal Processor, AnaLogic Computers mission is to commercialize high-speed, biologically inspired systems based on CNN processors. In 2003, AnaLogic Computers developed a PCI-X visual processor board that included the ACE 4K processor, with a Texas Instrument DIP module and a high-speed frame-grabber. This allowed CNN processing to be easily included in a desktop computer, considerably improving the usability and capability of CNN analog processors. In 2006, AnaLogic Computers developed their Bi-I Ultra High Speed Smart Camera product line, which includes the ACE 4K processor in their high-end models. The product that their development team is now pursuing is the Bionic Eyeglass. The Bionic Eyeglass is a dual-camera, wearable platform, based on the Bi-I Ultra High Speed Smart Camera, designed to provide assistance to blind people. Some of the functions that the Bionic Eyeglass system will perform is route number recognition and color processing.\n\nSome researchers are developed their own custom analog CNN processors. For example, an analog CNN processor was developed from a research team from University degli Studi di Catania, in order to generate gaits for a hexapod robot. Researchers from National Chiao Tung University designed a RM-CNN processor to learn more about pattern learning & recognition and researchers from the National Lien-Ho Institute of Technology developed a Min-Max CNN (MMCNN) processor to learn more about CNN dynamics. Given the diversity of CNN processors and the momentum that CNN research has gained, it is plausible that such analog CNN development efforts will be fairly common in the near future.\n\nDespite their speed and low power consumption, there are some significant drawbacks to analog CNN processors. First, analog CNN processors can potentially create erroneous results due to environment and process variation. In most applications, these errors are not noticeable, but there are situations where minor deviations can result in catastrophic system failures. For example, in chaotic communication, process variation will change the trajectory of a given system in phase space, resulting in a lost of synchronicity/stability. Due to the severity of the problem, there is considerable research being performed to ameliorate the problem. Some researchers are optimizing templates to accommodate greater variation. Other researchers are improving the semiconductor process to more closely match theoretical CNN performance. Other researchers are investigating different, potentially more robust CNN architectures. Lastly, researchers are developing methods to tune templates to target a specific chip and operating conditions. In other words, the templates are being optimized to match the information processing platform. Not only does process variation limit what can be done with current analog CNN processors, it is also a barrier for creating more complex processing units. Unless this process variation is resolved, ideas such as nested processing units, non-linear inputs, etc. cannot be implemented in a real-time analog CNN processor. Also, the semiconductor \"real estate\" for processing units limits the size of CNN processors. Currently the largest AnaVision CNN-based vision processor consists of a 4K detector, which is significantly less than the megapixel detectors found in affordable, consumer cameras. Unfortunately, feature size reductions, as predicted by Moore’s Law, will only result in minor improvements. For this reason, alternate technologies such as Resonant Tunneling Diodes and Neuron-Bipolar Junction Transistors are being explored. Also, the architecture of CNN processors is being reevaluated. For example, Star-CNN processors, where one analog multiplier is time-shared between multiple processor units, have been proposed and are expected to result in processor unit reduction size of eighty percent.\n\nAlthough not nearly as fast and energy efficient, digital CNN processors do not share the problems of process variation and feature size of their analog counterparts. This allows digital CNN processors to include nested processor units, non-linearities, etc. In addition, digital CNN are more flexible, cost less and are easier to integrate. The most common implementation of digital CNN processors uses an FPGA. Eutecus, founded in 2002 and operating in Berkeley, provides intellectual property that can be synthesized into an Altera FPGA. Their digital 320x280, FPGA-based CNN processors run at 30 frame/s and there are plans to make a fast digital ASIC. Eustecus is a strategic partner of AnaLogic computers, and their FPGA designs can be found in several of AnaLogic’s products. Eutecus is also developing software libraries to perform tasks including but not limited to video analytics for the video security market, feature classification, multi-target tracking, signal and image processing and flow processing. Many of these routines are derived using CNN-like processing. For those wanting to perform CNN simulations for prototyping, low-speed applications, or research, there are several options. First, there are precise CNN emulation software packages like SCNN 2000. If the speed is prohibitive, there are mathematical techniques, such as Jacobi’s Iterative Method or Forward-Backward Recursions that can be used to derive the steady state solution of a CNN processor. Said techniques can be performed by any mathematics tool, e.g. Matlab. Lastly, digital CNN processors can be emulated on highly parallel, application-specific processors, such as graphics processors. Implementing neural networks using graphics processors is an area of exploration for the research community.\n\nResearchers are also perusing alternate technologies for CNN processors. Although current CNN processors circumvent some of the problems associated with their digital counterparts, they do share some of the same long-term problems common to all semiconductor-based processors. These include, but are not limited to, speed, reliability, power-consumption, etc. AnaLogic Computers, is developing optical CNN processors, which combine optics, lasers, and biological and holographic memories. What initially was technology exploration resulted in a 500x500 CNN processor able to perform 300 giga-operations per second. Another promising technology for CNN processors is nanotechnology. One nanotechnology concept being investigated is using single electron tunneling junctions, which can be made into single-electron or high-current transistors, to create McCulloch-Pitts CNN processing units. In summary, CNN processors have been implemented and provide value to their users. They have been able to effectively leverage the advantages and address some of the disadvantages associated with their underling technology, i.e. semiconductors. Researchers are also transitioning CNN processors into emerging technologies. Therefore, if the CNN architecture is suited for a specific information processing system, there are processors available for purchase (as there will be for the foreseeable future).\n\nThe philosophy, interests, and methodologies of CNN researchers are varied. Due to the potential of the CNN architecture, this platform has attracted people from a variety of backgrounds and disciplines. Some are exploring practical implementations of CNN processors, others are using CNN processors to model physical phenomena, and there are even researchers exploring theoretical mathematical, computational, and philosophical ideas through CNN processors. Some applications are engineering related, where some known, understood behavior of CNN processors is exploited to perform a specific task, and some are scientific, where CNN processors are used to explore new and different phenomenon. CNN processors are versatile platforms that are being used for a variety of applications.\n\nCNN processors were designed to perform image processing; specifically, the original application of CNN processors was to perform real-time ultra-high frame-rate (>10,000 frame/s) processing unachievable by digital processors needed for applications like particle detection in jet engine fluids and spark-plug detection. Currently, CNN processors can achieve up to 50,000 frames per second, and for certain applications such as missile tracking, flash detection, and spark-plug diagnostics these microprocessors have outperformed a conventional supercomputer. CNN processors lend themselves to local, low-level, processor intensive operations and have been used in feature extraction, level and gain adjustments, color constancy detection, contrast enhancement, deconvolution, image compression, motion estimation, image encoding, image decoding, image segmentation, orientation preference maps, pattern learning/recognition, multi-target tracking, image stabilization, resolution enhancement, image deformations and mapping, image inpainting, optical flow, contouring, moving object detection, axis of symmetry detection, and image fusion.\n\nDue to their processing capabilities and flexibility, CNN processors have been used & prototyped for novel field applications such as flame analysis for monitoring combustion at a waste incinerator, mine-detection using infrared imagery, calorimeter cluster peak for high energy physics, anomaly detection in potential field maps for geophysics, laser dot detection, metal inspection for detecting manufacturing defects, and seismic horizon picking. They have also been used to perform biometric functions such as fingerprint recognition, vein feature extraction, face tracking, and generating visual stimuli via emergent patterns to gauge perceptual resonances. CNN processors have been used for medical and biological research in performing automated nucleated cell counting for detecting hyperplasia, segment images into anatomically and pathologically meaningful regions, measure and quantify cardiac function, measure the timing of neurons, and detect brain abnormalities that would lead to seizures. One potential future application of CNN microprocessors is to combine them with DNA microarrays to allow for a near-real time DNA analysis of hundreds of thousands of different DNA sequences. Currently, the major bottleneck of DNA microarray analysis is the amount of time needed to process data in the form of images, and using a CNN microprocessor, researchers have reduced the amount of time needed to perform this calculation to 7ms.\n\nCNN processors have also been used to generate and analyze patterns and textures. One motivation was to use CNN processors to understand pattern generation in natural systems. They were used to generate Turing patterns in order to understand the situations in which they form, the different types of patterns which can emerge, and the presence of defects or asymmetries. Also, CNN processors were used to approximate pattern generation systems that create stationary fronts, spatio-temporal patterns oscillating in time, hysteresis, memory, and heterogeneity. Furthermore, pattern generation was used to aid high-performance image generation and compression via real-time generation of stochastic and coarse-grained biological patterns, texture boundary detection, and pattern and texture recognition and classification.\n\nThere is an ongoing effort to incorporate CNN processors into sensory-computing-actuating machines as part of the emerging field of Cellular Machines. The basic premise is to create an integrated system that uses CNN processors for the sensory signal-processing and potentially the decision-making and control. The reason is that CNN processors can provide a low power, small size, and eventually low-cost computing and actuating system suited for Cellular Machines. These Cellular Machines will eventually create a Sensor-Actuator Network (SAN), a type of Mobile Ad Hoc Networks (MANET) which can be used for military intelligence gathering, surveillance of inhospitable environments, maintenance of large areas, planetary exploration, etc.\n\nCNN processors have been proven versatile enough for some control functions. They have been used to optimize function via a genetic algorithm, to measure distances, to perform optimal path-finding in a complex, dynamic environment, and theoretically can be used to learn and associate complex stimuli. They have also been used to create antonymous gaits and low-level motors for robotic nematodes, spiders, and lamprey gaits using a Central Pattern Generator (CPG). They were able to function using only feedback from the environment, allowing for a robust, flexible, biologically inspired robot motor system. CNN-based systems were able to operate in different environments and still function if some of the processing units are disabled.\n\nThe variety of dynamical behavior seen in CNN processors make them intriguing for communication systems. Chaotic communications using CNN processors is being researched due to their potential low power consumption, robustness and spread spectrum features. The premise behind chaotic communication is to use a chaotic signal for the carrier wave and to use chaotic phase synchronization to reconstruct the original message. CNN processors can be used on both the transmitter and receiver end to encode and decode a given message. They can also be used for data encryption and decryption, source authentication through watermarking, detecting of complex patterns in spectrogram images (sound processing), and transient spectral signals detection.\n\nCNN processors are neuromorphic processors, meaning that they emulate certain aspects of biological neural networks. The original CNN processors were based on mammalian retinas, which consist of a layer of photodetectors connected to several layers of locally coupled neurons. This makes CNN processors part of an interdisciplinary research area whose goal is to design systems that leverage knowledge and ideas from neuroscience and contribute back via real-world validation of theories. CNN processors have implemented a real-time system that replicates mammalian retinas, validating that the original CNN architecture chosen modeled the correct aspects of the biological neural networks used to perform the task in mammalian life. However, CNN processors are not limited to verifying biological neural networks associated with vision processing; they have been used to simulate dynamic activity seen in mammalian neural networks found in the olfactory bulb and locust antennal lobe, responsible for pre-processing sensory information to detect differences in repeating patterns.\n\nCNN processors are being used to understand systems that can be modeled using simple, coupled units, such as living cells, biological networks, physiological systems, and ecosystems. The CNN architecture captures some of the dynamics often seen in nature and is simple enough to analyze and conduct experiments. They are also being used for stochastic simulation techniques, which allow scientists to explore spin problems, population dynamics, lattice-based gas models, percolation, and other phenomena. Other simulation applications include heat transfer, mechanical vibrating systems, protein production, Josephson Transmission Line (JTL) problems, seismic wave propagation, and geothermal structures. Instances of 3D (Three Dimensional) CNN have been used to prove known complex shapes are emergent phenomena in complex systems, establishing a link between art, dynamical systems and VLSI technology. CNN processors have been used to research a variety of mathematical concepts, such as researching non-equilibrium systems, constructing non-linear systems of arbitrary complexity using a collection of simple, well-understood dynamic systems, studying emergent chaotic dynamics, generating chaotic signals, and in general discovering new dynamic behavior. They are often used in researching systemics, a trandisiplinary, scientific field that studies natural systems. The goal of systemics researchers is to develop a conceptual and mathematical framework necessary to analyze, model, and understand systems, including, but not limited to, atomic, mechanical, molecular, chemical, biological, ecological, social and economic systems. Topics explored are emergence, collective behavior, local activity and its impact on global behavior, and quantifying the complexity of an approximately spatial and topologically invariant system . Although another measure of complexity may not arouse enthusiasm (Seth Lloyd, a professor from Massachusetts Institute of Technology (MIT), has identified 32 different definitions of complexity), it can potentially be mathematically advantageous when analyzing systems such as economic and social systems.\n\nand its Computational Complexity\", Int’l Workshop on Cellular Neural Networks and Their Applications, 2004.\n", "id": "2506529", "title": "Cellular neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=38156432", "text": "CoDi\n\nCoDi is a cellular automaton (CA) model for spiking neural networks (SNNs). CoDi is an acronym for Collect and Distribute, referring to the signals and spikes in a neural network.\n\nCoDi uses a von Neumann neighborhood modified for a three-dimensional space; each cell looks at the states of its six orthogonal neighbors and its own state. In a growth phase a neural network is grown in the CA-space based on an underlying chromosome. There are four types of cells: neuron body, axon, dendrite and blank. The growth phase is followed by a signaling- or processing-phase. Signals are distributed from the neuron bodies via their axon tree and collected from connection dendrites. These two basic interactions cover every case, and they can be expressed simply, using a small number of rules.\n\nThe neuron body cells collect neural signals from the surrounding dendritic cells and apply an internally defined function to the collected data. In the CoDi model the neurons sum the incoming signal values and fire after a threshold is reached. This behavior of the neuron bodies can be modified easily to suit a given problem. The output of the neuron bodies is passed on to its surrounding axon cells. Axonal cells distribute data originating from the neuron body. Dendritic cells collect data and eventually pass it to the neuron body. These two types of cell-to-cell interaction cover all kinds of cell encounters.\n\nEvery cell has a gate, which is interpreted differently depending on the type of the cell. A neuron cell uses this gate to store its orientation, i.e. the direction in which the axon is pointing. In an axon cell, the gate points to the neighbor from which the neural signals are received. An axon cell accepts input only from this neighbor, but makes its own output available to all its neighbors. In this way axon cells distribute information. The source of information is always a neuron cell. Dendritic cells collect \ninformation by accepting information from any neighbor. They give their output, (e.g. a Boolean OR operation on the binary inputs) only to the neighbor specified by their own gate. In this way, dendritic cells collect and \"sum\" neural signals, until the final sum of collected neural signals reaches the neuron cell.\n\nEach axonal and dendritic cell \"belongs\" to exactly one neuron cell. This configuration of the CA-space is guaranteed by the preceding growth phase.\n\nThe CoDi model does not use explicit synapses, because dendrite cells that are in contact with an axonal trail (i.e. have an axon cell as neighbor) collect the neural signals directly from the axonal trail. This results from the behavior of axon cells, which distribute to every neighbor, and from the behavior of the dendrite cells, which collect from any neighbor.\n\nThe strength of a neuron-neuron connection (a synapse) is represented by the number of their neighboring axon and dendrite cells. The exact structure of the network and the position of the axon-dendrite neighbor pairs determine the time delay and strength (weight) of a neuron-neuron connection. This principle infers that a single neuron-neuron connection can consist of several synapse with different time delays with independent weights.\n\nThe chromosome is initially distributed throughout the CA-space, so that every cell in the CA-space contains one instruction of the chromosome, i.e. one growth instruction, so that the chromosome belongs to the network as a whole. The distributed chromosome technique of the CoDi model makes maximum use of the available CA-space and enables the growth of any type of network connectivity. The local connection of the grown circuitry to its chromosome, allows local learning to be combined with the evolution of grown neural networks.\n\nGrowth signals are passed to the direct neighbors of the neuron cell according to its chromosome information. The blank neighbors, which receive a neural growth signal, turn into either an axon cell or a dendrite cell. The growth signals include information containing the cell type of the cell that is to be grown from the signal. To decide in which directions axonal or dendritic trails should grow, the grown cells consult their chromosome information which encodes the growth instructions. These growth instructions can have an absolute or a relative directional encoding. An absolute encoding masks the six neighbors (i.e. directions) of a 3D cell with six bits. After a cell is grown, it accepts growth signals only from the direction from which it received its first signal. This \"reception direction\" information is stored in the \"gate\" position of each cell's state.\n\nThe states of our CAs have two parts, which are treated in different ways. The first part of the cell-state contains the cell's type and activity level and the second part serves as an interface to the cell's neighborhood by containing the input signals from the neighbors. Characteristic of our CA is that only part of the state of a cell is passed to its neighbors, namely the signal and then only to those neighbors specified in the fixed part of the cell state. This CA is called \"partitioned\", because the state is partitioned into two parts, the first being fixed and the second is variable for each cell.\n\nThe advantage of this partitioning-technique is that the amount of information that defines the new state of a CA cell is kept to a minimum, due to its avoidance of redundant information exchange.\n\nSince CAs are only locally connected, they are ideal for implementation on purely parallel hardware. When designing the CoDi CA-based neural networks model, the objective was to implement them directly in hardware (FPGAs). Therefore, the CA was kept as simple as possible, by having a small number of bits to specify the state, keeping the CA rules few in number, and having few cellular neighbors.\n\nThe CoDi model was implemented in the FPGA based CAM-Brain Machine (CBM) by Korkin.\n\nCoDi was introduced by Gers et al. in 1998. A specialized parallel machine based on FPGA Hardware (CAM) to run the DoDi model on a large scale was developed by Korkin et al. De Garis conducted a series of experiments on the CAM-machine evaluating the CoDi model. The original model, where learning is based on evolutionary algorithms, has been augmented with a local learning rule via feedback from dendritic spikes by Schwarzer.\n", "id": "38156432", "title": "CoDi"}
{"url": "https://en.wikipedia.org/wiki?curid=13526929", "text": "Compositional pattern-producing network\n\nCompositional pattern-producing networks (CPPNs) are a variation of artificial neural networks (ANNs) that have an architecture whose evolution is guided by genetic algorithms.\n\nWhile ANNs often contain only sigmoid functions and sometimes Gaussian functions, CPPNs can include both types of functions and many others. The choice of functions for the canonical set can be biased toward specific types of patterns and regularities. For example, periodic functions such as sine produce segmented patterns with repetitions, while symmetric functions such as Gaussian produce symmetric patterns. Linear functions can be employed to produce linear or fractal-like patterns. Thus, the architect of a CPPN-based genetic art system can bias the types of patterns it generates by deciding the set of canonical functions to include.\n\nFurthermore, unlike typical ANNs, CPPNs are applied across the entire space of possible inputs so that they can represent a complete image. Since they are compositions of functions, CPPNs in effect encode images at infinite resolution and can be sampled for a particular display at whatever resolution is optimal.\n\nCPPNs can be evolved through neuroevolution techniques such as neuroevolution of augmenting topologies (called CPPN-NEAT).\n\nCPPNs have been shown to be a very powerful encoding when evolving the following:\n\n\n", "id": "13526929", "title": "Compositional pattern-producing network"}
{"url": "https://en.wikipedia.org/wiki?curid=1532719", "text": "Computational cybernetics\n\nComputational cybernetics is the integration of cybernetics and computational intelligence techniques. Though the term Cybernetics entered the technical lexicon in the 1940s and 1950s, it was first used informally as a popular noun in the 1960s, when it became associated with computers, robotics, Artificial Intelligence and Science fiction.\n\nWhile Cybernetics is primarily concerned with the study of control systems, computational cybernetics focuses on their automatic (complex, autonomic, flexible, adaptive) operation. Furthermore, computational cybernetics covers not only mechanical, but biological (living), social and economical systems. To achieve this goal, it uses research from the fields of communication theory, signal processing, information technology, control theory, the theory of adaptive systems, the theory of complex systems (game theory, and operational research). \n\nIEEE, a professional organization for the advancement of technology, has organized two international conferences focusing on computational cybernetics in 2008 and 2013.\n\n", "id": "1532719", "title": "Computational cybernetics"}
{"url": "https://en.wikipedia.org/wiki?curid=8402086", "text": "Computational neurogenetic modeling\n\nComputational neurogenetic modeling (CNGM) is concerned with the study and development of dynamic neuronal models for modeling brain functions with respect to genes and dynamic interactions between genes. These include neural network models and their integration with gene network models. This area brings together knowledge from various scientific disciplines, such as computer and information science, neuroscience and cognitive science, genetics and molecular biology, as well as engineering.\n\nModels of the kinetics of proteins and ion channels associated with neuron activity represent the lowest level of modeling in a computational neurogenetic model. The altered activity of proteins in some diseases, such as the amyloid beta protein in Alzheimer's disease, must be modeled at the molecular level to accurately predict the effect on cognition. Ion channels, which are vital to the propagation of action potentials, are another molecule that may be modeled to more accurately reflect biological processes. For instance, to accurately model synaptic plasticity (the strengthening or weakening of synapses) and memory, it is necessary to model the activity of the NMDA receptor (NMDAR). The speed at which the NMDA receptor lets Calcium ions into the cell in response to Glutamate is an important determinant of Long-term potentiation via the insertion of AMPA receptors (AMPAR) into the plasma membrane at the synapse of the postsynaptic cell (the cell that receives the neurotransmitters from the presynaptic cell).\n\nIn most models of neural systems neurons are the most basic unit modeled. In computational neurogenetic modeling, to better simulate processes that are responsible for synaptic activity and connectivity, the genes responsible are modeled for each neuron.\n\nA gene regulatory network, protein regulatory network, or gene/protein regulatory network, is the level of processing in a computational neurogenetic model that models the interactions of genes and proteins relevant to synaptic activity and general cell functions. Genes and proteins are modeled as individual nodes, and the interactions that influence a gene are modeled as excitatory (increases gene/protein expression) or inhibitory (decreases gene/protein expression) inputs that are weighted to reflect the effect a gene or protein is having on another gene or protein. Gene regulatory networks are typically designed using data from microarrays.\n\nModeling of genes and proteins allows individual responses of neurons in an artificial neural network that mimic responses in biological nervous systems, such as division (adding new neurons to the artificial neural network), creation of proteins to expand their cell membrane and foster neurite outgrowth (and thus stronger connections with other neurons), up-regulate or down-regulate receptors at synapses (increasing or decreasing the weight (strength) of synaptic inputs), uptake more neurotransmitters, change into different types of neurons, or die due to necrosis or apoptosis. The creation and analysis of these networks can be divided into two sub-areas of research: the \ngene up-regulation that is involved in the normal functions of a neuron, such as growth, metabolism, and synapsing; and the effects of mutated genes on neurons and cognitive functions.\n\nAn artificial neural network generally refers to any computational model that mimics the central nervous system, with capabilities such as learning and pattern recognition. With regards to computational neurogenetic modeling, however, it is often used to refer to those specifically designed for biological accuracy rather than computational efficiency. Individual neurons are the basic unit of an artificial neural network, with each neuron acting as a node. Each node receives weighted signals from other nodes that are either excitatory or inhibitory. To determine the output, a transfer function (or activation function) evaluates the sum of the weighted signals and, in some artificial neural networks, their input rate. Signal weights are strengthened (long-term potentiation) or weakened (long-term depression) depending on how synchronous the presynaptic and postsynaptic activation rates are (Hebbian theory).\n\nThe synaptic activity of individual neurons is modeled using equations to determine the temporal (and in some \ncases, spatial) summation of synaptic signals, membrane potential, threshold for action potential \ngeneration, the absolute and relative refractory period, and optionally ion receptor channel kinetics and Gaussian noise (to increase biological accuracy by incorporation of random elements). In addition to connectivity, some types of artificial neural networks, such as spiking neural networks, also model the distance between neurons, and its effect on the synaptic weight (the strength of a synaptic transmission).\n\nFor the parameters in the gene regulatory network to affect the neurons in the artificial neural network as intended there must be some connection between them. In an organizational context, each node (neuron) in the artificial neural network has its own gene regulatory network associated with it. The weights (and in some networks, frequencies of synaptic transmission to the node), and the resulting membrane potential of the node (including whether an action potential is produced or not), affect the expression of different genes in the gene regulatory network. Factors affecting connections between neurons, such as synaptic plasticity, can be modeled by inputting the values of synaptic activity-associated genes and proteins to a function that re-evaluates the weight of an input from a particular neuron in the artificial neural network.\n\nOther cell types besides neurons can be modeled as well. Glial cells, such as astroglia and microglia, as well as endothelial cells, could be included in an artificial neural network. This would enable modeling of diseases where pathological effects may occur from sources other than neurons, such as Alzheimer's disease.\n\nWhile the term artificial neural network is usually used in computational neurogenetic modeling to refer to models of the central nervous system meant to possess biological accuracy, the general use of the term can be applied to many gene regulatory networks as well.\n\nArtificial neural networks, depending on type, may or may not take into account the timing of inputs. Those that do, such as spiking neural networks, fire only when the pooled inputs reach a membrane potential is reached. Because this mimics the firing of biological neurons, spiking neural networks are viewed as a more biologically accurate model of synaptic activity.\n\nTo accurately model the central nervous system, creation and death of neurons should be modeled as well. To accomplish this, constructive artificial neural networks that are able to grow or shrink to adapt to inputs are often used. Evolving connectionist systems are a subtype of constructive artificial neural networks (evolving in this case referring to changing the structure of its neural network rather than by mutation and natural selection).\n\nBoth synaptic transmission and gene-protein interactions are stochastic in nature. To model biological nervous systems with greater fidelity some form of randomness is often introduced into the network. Artificial neural networks modified in this manner are often labeled as probabilistic versions of their neural network sub-type (e.g., pSNN).\n\nFuzzy logic is a system of reasoning that enables an artificial neural network to deal in non-binary and linguistic variables. Biological data is often unable to be processed using Boolean logic, and moreover accurate modeling of the capabilities of biological nervous systems requires fuzzy logic. Therefore, artificial neural networks that incorporate it, such as evolving fuzzy neural networks (EFuNN) or Dynamic Evolving Neural-Fuzzy Inference Systems (DENFIS), are often used in computational neurogenetic modeling. The use of fuzzy logic is especially relevant in gene regulatory networks, as the modeling of protein binding strength often requires non-binary variables.\n\nArtificial Neural Networks designed to simulate of the human brain require an ability to learn a variety of tasks that is not required by those designed to accomplish a specific task. Supervised learning is a mechanism by which an artificial neural network can learn by receiving a number of inputs with a correct output already known. An example of an artificial neural network that uses supervised learning is a multilayer perceptron (MLP). In unsupervised learning, an artificial neural network is trained using only inputs. Unsupervised learning is the learning mechanism by which a type of artificial neural network known as a self-organizing map (SOM) learns. Some types of artificial neural network, such as evolving connectionist systems, can learn in both a supervised and unsupervised manner.\n\nBoth gene regulatory networks and artificial neural networks have two main strategies for improving their accuracy. In both cases the output of the network is measured against known biological data using some function, and subsequent improvements are made by altering the structure of the network. A common test of accuracy for artificial neural networks is to compare some parameter of the model to data acquired from biological neural systems, such as from an EEG. In the case of EEG recordings, the local field potential (LFP) of the artificial neural network is taken and compared to EEG data acquired from human patients. The relative intensity ratio (RIRs) and fast Fourier transform (FFT) of the EEG are compared with those generated by the artificial neural networks to determine the accuracy of the model.\n\nBecause the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model, \nevolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene \nregulatory network is: first, create a population; next, to create offspring via a crossover operation and \nevaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator; \nfinally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated.\nMethods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can \nbe modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.\n\nA variety of potential applications have been suggested for accurate computational neurogenetic models, such as simulating genetic diseases, examining the impact of potential treatments, better understanding of learning and cognition, and development of hardware able to interface with neurons.\n\nThe simulation of disease states is of particular interest, as modeling both the neurons and their genes and proteins allows linking genetic mutations and protein abnormalities to pathological effects in the central nervous system. Among those diseases suggested as being possible targets of computational neurogenetic modeling based analysis are epilepsy, schizophrenia, mental retardation, brain aging and Alzheimer's disease, and Parkinson's disease.\n\n\n", "id": "8402086", "title": "Computational neurogenetic modeling"}
{"url": "https://en.wikipedia.org/wiki?curid=42368614", "text": "Convolutional Deep Belief Networks\n\nIn computer science, Convolutional Deep Belief Network (CDBN) is a type of deep artificial neural network that is composed of multiple layers of convolutional restricted Boltzmann machines stacked together. Alternatively, it is a hierarchical generative model for deep learning, which is highly effective in the tasks of image processing and object recognition, though it has been used in other domains too. The salient features of the model include the fact that it scales well to high-dimensional images and is translation-invariant.\n\nCDBNs use the technique of probabilistic max-pooling to reduce the dimensions in higher layers in the network. Training of the network involves a pre-training stage accomplished in a greedy layer-wise manner, similar to other deep belief networks. Depending on whether the network is to be used for discrimination or generative tasks, it is then \"fine tuned\" or trained with either back-propagation or the up-down algorithm (contrastive-divergence), respectively.\n", "id": "42368614", "title": "Convolutional Deep Belief Networks"}
{"url": "https://en.wikipedia.org/wiki?curid=24364159", "text": "Cover's theorem\n\nCover's Theorem is a statement in computational learning theory and is one of the primary theoretical motivations for the use of non-linear kernel methods in machine learning applications. The theorem states that given a set of training data that is not linearly separable, one can with high probability transform it into a training set that is linearly separable by projecting it into a higher-dimensional space via some non-linear transformation. The theorem is named after the information theorist Thomas M. Cover who stated it in 1965.\n\nThe proof is easy. A deterministic mapping may be used. Indeed, suppose there are formula_1 samples. Lift them onto the vertices of the simplex in the formula_2 dimensional real space. Every partition of the samples into two sets is separable by a linear separator. QED.\n\n", "id": "24364159", "title": "Cover's theorem"}
{"url": "https://en.wikipedia.org/wiki?curid=41416740", "text": "Deep belief network\n\nIn machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables (\"hidden units\"), with connections between the layers but not between units within each layer.\n\nWhen trained on a set of examples without supervision, a DBN can learn to probabilistically reconstruct its inputs. The layers then act as feature detectors. After this learning step, a DBN can be further trained with supervision to perform classification.\n\nDBNs can be viewed as a composition of simple, unsupervised networks such as restricted Boltzmann machines (RBMs) or autoencoders, where each sub-network's hidden layer serves as the visible layer for the next. An RBM is an undirected, generative energy-based model with a \"visible\" input layer and a hidden layer and connections between but not within layers. This composition leads to a fast, layer-by-layer unsupervised training procedure, where contrastive divergence is applied to each sub-network in turn, starting from the \"lowest\" pair of layers (the lowest visible layer is a training set).\n\nTeh's observation that DBNs can be trained greedily, one layer at a time, led to one of the first effective deep learning algorithms. Overall, there are many attractive implementations and uses of DBNs in real-life applications and scenarios (e.g., electroencephalography, drug discovery). \n\nThe training method for RBMs proposed by Geoffrey Hinton for use with training \"Product of Expert\" models is called contrastive divergence (CD). CD provides an approximation to the maximum likelihood method that would ideally be applied for learning the weights. In training a single RBM, weight updates are performed with gradient descent via the following equation: formula_1\n\nwhere, formula_2 is the probability of a visible vector, which is given by formula_3. formula_4 is the partition function (used for normalizing) and formula_5 is the energy function assigned to the state of the network. A lower energy indicates the network is in a more \"desirable\" configuration. The gradient formula_6 has the simple form formula_7 where formula_8 represent averages with respect to distribution formula_9. The issue arises in sampling formula_10 because this requires extended alternating Gibbs sampling. CD replaces this step by running alternating Gibbs sampling for formula_11 steps (values of formula_12 perform well). After formula_11 steps, the data are sampled and that sample is used in place of formula_10. The CD procedure works as follows:\nOnce an RBM is trained, another RBM is \"stacked\" atop it, taking its input from the final trained layer. The new visible layer is initialized to a training vector, and values for the units in the already-trained layers are assigned using the current weights and biases. The new RBM is then trained with the procedure above. This whole process is repeated until the desired stopping criterion is met.\n\nAlthough the approximation of CD to maximum likelihood is crude (does not follow the gradient of any function), it is empirically effective.\n\n\n", "id": "41416740", "title": "Deep belief network"}
{"url": "https://en.wikipedia.org/wiki?curid=42358441", "text": "Deep lambertian networks\n\nDeep Lambertian Networks (DLN) is a combination of Deep belief network and Lambertian reflectance assumption which deals with the challenges posed by illumination variation in visual perception. Lambertian Reflectance model gives an illumination invariant representation which can be used for recognition. The Lambertian reflectance model is widely used for\nmodeling illumination variations and is a good approximation for diffuse object surfaces. The DLN is a hybrid undirected-directed generative model that combines DBNs with the Lambertian reflectance model.\n\nIn the DLN, the visible layer consists of image pixel intensities v ∈ R, where N is the number of pixels in the image. For every pixel i there are two latent variables namely the albedo and surface normal. GRBMs are used to model the albedo and surface normals.\n\nCombining Deep Belief Nets with the Lambertian reflectance assumption, the model can learn good priors over the albedo from 2D images. Illumination variations can be explained by changing only the lighting latent variable. By transferring learned knowledge from similar objects, albedo and surface normals estimation from a single image is also possible. Experiments demonstrate that this model is able to generalize as well as improve over standard baselines in one-shot face recognition.\n\nThe model has been successfully applied in reconstruction of shadows facial images, given any set of lighting conditions. The model has also been tested on non-living objects. The method outperforms most other methods and is faster than them.\n", "id": "42358441", "title": "Deep lambertian networks"}
{"url": "https://en.wikipedia.org/wiki?curid=1237612", "text": "Delta rule\n\nIn machine learning, the Delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm. For a neuron formula_1 with activation function formula_2, the delta rule for formula_1's formula_4th weight formula_5 is given by\n\nwhere\n\nIt holds that formula_7 and formula_8.\n\nThe delta rule is commonly stated in simplified form for a neuron with a linear activation function as \n\nWhile the delta rule is similar to the perceptron's update rule, the derivation is different. The perceptron uses the Heaviside step function as the activation function formula_10, and that means that formula_11 does not exist at zero, and is equal to zero elsewhere, which makes the direct application of the delta rule impossible.\n\nThe delta rule is derived by attempting to minimize the error in the output of the neural network through gradient descent. The error for a neural network with formula_1 outputs can be measured as \n\nIn this case, we wish to move through \"weight space\" of the neuron (the space of all possible values of all of the neuron's weights) in proportion to the gradient of the error function with respect to each weight. In order to do that, we calculate the partial derivative of the error with respect to each weight. For the formula_4th weight, this derivative can be written as \n\nBecause we are only concerning ourselves with the formula_1th neuron, we can substitute the error formula above while omitting the summation:\n\nNext we use the chain rule to split this into two derivatives:\n\nTo find the left derivative, we simply apply the general power rule:\n\nTo find the right derivative, we again apply the chain rule, this time differentiating with respect to the total input to formula_1, formula_21:\n\nNote that the output of the formula_23th neuron, formula_24, is just the neuron's activation function formula_25 applied to the neuron's input formula_21. We can therefore write the derivative of formula_24 with respect to formula_21 simply as formula_25's first derivative:\n\nNext we rewrite formula_21 in the last term as the sum over all formula_32 weights of each weight formula_33 times its corresponding input formula_34:\n\nBecause we are only concerned with the formula_4th weight, the only term of the summation that is relevant is formula_37. Clearly, \n\ngiving us our final equation for the gradient:\n\nAs noted above, gradient descent tells us that our change for each weight should be proportional to the gradient. Choosing a proportionality constant formula_40 and eliminating the minus sign to enable us to move the weight in the negative direction of the gradient to minimize error, we arrive at our target equation:\n\n", "id": "1237612", "title": "Delta rule"}
{"url": "https://en.wikipedia.org/wiki?curid=8887731", "text": "Echo state network\n\nThe echo state network (ESN), is a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can (re)produce specific temporal patterns. The main interest of this network is that although its behaviour is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.\n\nAlternatively, one may consider a nonparametric Bayesian formulation of the output layer, under which: (i) a prior distribution is imposed over the output weights; and (ii) the output weights are marginalized out in the context of prediction generation, given the training data. This idea has been demonstrated in by using Gaussian priors, whereby a Gaussian process model with ESN-driven kernel function is obtained. Such a solution was shown to outperform ESNs with trainable (finite) sets of weights in several benchmarks.\n\nSome publicly available implementations of ESNs are: (i) aureservoir: an efficient C++ library for various kinds of echo state networks with python/numpy bindings; and (ii) Matlab code: an efficient matlab for an echo state network.\n\n", "id": "8887731", "title": "Echo state network"}
{"url": "https://en.wikipedia.org/wiki?curid=14509578", "text": "The Emotion Machine\n\nThe Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind is a 2006 book by cognitive scientist Marvin Minsky that elaborates and expands on Minsky's ideas as presented in his earlier book \"Society of Mind\".\n\nMinsky argues that emotions are different ways to think that our mind uses to increase our intelligence. He challenges the distinction between emotions and other kinds of thinking. His main argument is that emotions are \"ways to think\" for different \"problem types\" that exist in the world, and that the brain has rule-based mechanisms (selectors) that turn on emotions to deal with various problems. The book reviews the accomplishments of AI, why modelling an AI is difficult in terms of replicating the behaviors of humans, if and how AIs think, and in what manner they might experience struggles and pleasures.\n\nIn a review for \"The Washington Post\", neurologist Richard Restak states that:\nMinsky outlines the book as follows:\n\n\n\n\n", "id": "14509578", "title": "The Emotion Machine"}
{"url": "https://en.wikipedia.org/wiki?curid=19172663", "text": "European Neural Network Society\n\nThe European Neural Network Society (ENNS) is an association of scientists, engineers, students, and others seeking to learn about and advance understanding of artificial neural networks. Specific areas of interest in this scientific field include modelling of behavioral and brain processes, development of neural algorithms and applying neural modelling concepts to problems relevant in many different domains. Erkki Oja and John G. Taylor are past ENNS presidents and honorary executive board members.\n\nEvery year since 1991 ENNS organizes the International Conference on Artificial Neural Networks (ICANN). The history and the links to past conferences are available at the ENNS web site. This is one of the oldest and best established conferences on the subject, with proceedings published in Springer Lecture Notes in Computer Science, see index in DBLP bibliography database.\n\nAs a non-profit organization ENNS promotes scientific activities at European and at the national levels in cooperation with national organizations that focus on neural networks. Every year many stipends to attend ICANN conference are given. ENNS also sponsors other students and have given awards and prizes at co-sponsored events (schools, workshops, conferences and competitions).\n", "id": "19172663", "title": "European Neural Network Society"}
{"url": "https://en.wikipedia.org/wiki?curid=15702071", "text": "Evolutionary acquisition of neural topologies\n\nEvolutionary acquisition of neural topologies (EANT/EANT2) is an evolutionary reinforcement learning method that evolves both the topology and weights of artificial neural networks. It is closely related to the works of Angeline et al. and Stanley and Miikkulainen. Like the work of Angeline et al., the method uses a type of parametric mutation that comes from evolution strategies and evolutionary programming (now using the most advanced form of the evolution strategies CMA-ES in EANT2), in which adaptive step sizes are used for optimizing the weights of the neural networks. Similar to the work of Stanley (NEAT), the method starts with minimal structures which gain complexity along the evolution path.\n\nDespite sharing these two properties, the method has the following important features which distinguish it from previous works in neuroevolution.\n\nIt introduces a genetic encoding called common genetic encoding (CGE) that handles both direct and indirect encoding of neural networks within the same theoretical framework. The encoding has important properties that makes it suitable for evolving neural networks: \n\nThese properties have been formally proven in.\n\nFor evolving the structure and weights of neural networks, an evolutionary process is used, where the \"exploration\" of structures is executed at a larger timescale (structural exploration), and the \"exploitation\" of existing structures is done at a smaller timescale (structural exploitation). In the structural exploration phase, new neural structures are developed by gradually adding new structures to an initially minimal network that is used as a starting point. In the structural exploitation phase, the weights of the currently available structures are optimized using an evolution strategy.\n\nEANT has been tested on some benchmark problems such as the double-pole balancing problem, and the RoboCup keepaway benchmark. In all the tests, EANT was found to perform very well. Moreover, a newer version of EANT, called EANT2, was tested on a visual servoing task and found to outperform NEAT and the traditional iterative Gauss–Newton method. Further experiments include results on a classification problem \n\n", "id": "15702071", "title": "Evolutionary acquisition of neural topologies"}
{"url": "https://en.wikipedia.org/wiki?curid=31294087", "text": "Extension neural network\n\nExtension neural network is a pattern recognition method found by M. H. Wang and C. P. Hung in 2003 to classify instances of data sets. Extension neural network is composed of artificial neural network and extension theory concepts. It uses the fast and adaptive learning capability of neural network and correlation estimation property of extension theory by calculating extension distance. \nENN was used in:\n\nExtension theory was first proposed by Cai in 1983 to solve contradictory problems. While classical mathematic is familiar with quantity and forms of objects, extension theory transforms these objects to matter-element model.\n\nwhere in matter formula_1, formula_2 is the name or type, formula_3 is its characteristics and formula_4 is the corresponding value for the characteristic. There is a corresponding example in equation 2.\n\nwhere formula_5 and formula_6 characteristics form extension sets. These extension sets are defined by the formula_4 values which are range values for corresponding characteristics. Extension theory concerns with the extension correlation function between matter-element models like shown in equation 2 and extension sets. Extension correlation function is used to define extension space which is composed of pairs of elements and their extension correlation functions. The extension space formula is shown in equation 3.\n\nwhere, formula_8 is the extension space, formula_9 is the object space, formula_10 is the extension correlation function, formula_11 is an element from the object space and formula_12 is the corresponding extension correlation function output of element formula_11. formula_14 maps formula_11 to a membership interval formula_16. Negative region represents an element not belonging membership degree to a class and positive region vice versa. If formula_11 is mapped to formula_18, extension theory acts like fuzzy set theory. The correlation function can be shown with the equation 4.\n\nwhere, formula_19 and formula_20 are called concerned and neighborhood domain and their intervals are (a,b) and (c,d) respectively. The extended correlation function used for estimation of membership degree between formula_11 and formula_19, formula_20 is shown in equation 5.\n</math>\n\nExtension neural network has a neural network like appearance. Weight vector resides between the input nodes and output nodes. Output nodes are the representation of input nodes by passing them through the weight vector. \n\nThere are total number of input and output nodes are represented by formula_24 and formula_25, respectively. These numbers depend on the number of characteristics and classes. Rather than using one weight value between two layer nodes as in neural network, extension neural network architecture has two weight values. In extension neural network architecture, for instance formula_26, formula_27 is the input which belongs to class formula_28 and formula_29 is the corresponding output for class formula_30. The output formula_29 is calculated by using extension distance as shown in equation 6.\n\n+1 \\right)\n</math>\nformula_32\n\nEstimated class is found through searching for the minimum extension distance among the calculated extension distance for all classes as summarized in equation 7, where formula_33 is the estimated class.\n\nEach class is composed of ranges of characteristics. These characteristics are the input types or names which come from matter-element model. Weight values in extension neural network represent these ranges. In the learning algorithm, first weights are initialized by searching for the maximum and minimum values of inputs for each class as shown in equation 8\n\nwhere, formula_26 is the instance number and formula_35 is represents number of input. This initialization provides classes' ranges according to given training data.\n\nAfter maintaining weights, center of clusters are found through the equation 9.\n\nBefore learning process begins, predefined learning performance rate is given as shown in equation 10\n\nwhere, formula_36 is the misclassified instances and formula_37 is the total number of instances. Initialized parameters are used to classify instances with using equation 6. If the initialization is not sufficient due to the learning performance rate, training is required. In the training step weights are adjusted to classify training data more accurately, therefore reducing learning performance rate is aimed. In each iteration, formula_38 is checked to control if required learning performance is reached. In each iteration every training instance is used for training. \nInstance formula_26, belongs to class formula_28 is shown by:\n\nformula_41\n\nformula_42\n\nEvery input data point of formula_43 is used in extension distance calculation to estimate the class of formula_43. If the estimated class formula_45 then update is not needed. Whereas, if formula_46 then update is done. In update case, separators which show the relationship between inputs and classes, are shifted proportional to the distance between the center of clusters and the data points. \nThe update formula:\n\nformula_47\nformula_48\nformula_49\nformula_50\nformula_51\nformula_52\n\nTo classify the instance formula_26 accurately, separator of class formula_28 for input formula_35 moves close to data-point of instance formula_26, whereas separator of class formula_33 for input formula_35 moves far away. In the above image, an update example is given. Assume that instance formula_26 belongs to class A, whereas it is classified to class B because extension distance calculation gives out formula_60. After the update, separator of class A moves close to the data-point of instance formula_26 whereas separator of class B moves far away. Consequently, extension distance gives out formula_62, therefore after update instance \nformula_26 is classified to class A.\n\n", "id": "31294087", "title": "Extension neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=574759", "text": "Feed forward (control)\n\nFeed-forward, sometimes written feedforward, is a term describing an element or pathway within a control system that passes a controlling signal from a source in its external environment, often a command signal from an external operator, to a load elsewhere in its external environment. A control system which has only feed-forward behavior responds to its control signal in a pre-defined way without responding to how the load reacts; it is in contrast with a system that also has feedback, which adjusts the output to take account of how it affects the load, and how the load itself may vary unpredictably; the load is considered to belong to the external environment of the system.\n\nIn a feed-forward system, the control variable adjustment is not error-based. Instead it is based on knowledge about the process in the form of a mathematical model of the process and knowledge about or measurements of the process disturbances.\n\nSome prerequisites are needed for control scheme to be reliable by pure feed-forward without feedback: the external command or controlling signal must be available, and the effect of the output of the system on the load should be known (that usually means that the load must be predictably unchanging with time). Sometimes pure feed-forward control without feedback is called 'ballistic', because once a control signal has been sent, it cannot be further adjusted; any corrective adjustment must be by way of a new control signal. In contrast, 'cruise control' adjusts the output in response to the load that it encounters, by a feedback mechanism.\n\nThese systems could relate to control theory, physiology computing.\n\nWith feed-forward or Feedforward control, the disturbances are measured and accounted for before they have time to affect the system. In the house example, a feed-forward system may measure the fact that the door is opened and automatically turn on the heater before the house can get too cold. The difficulty with feed-forward control is that the effects of the disturbances on the system must be accurately predicted, and there must not be any unmeasured disturbances. For instance, if a window was opened that was not being measured, the feed-forward-controlled thermostat might still let the house cool down.\n\nThe term has specific meaning within the field of CPU-based automatic control. The discipline of “feedforward control” as it relates to modern, CPU based automatic controls is widely discussed, but is seldom practiced due to the difficulty and expense of developing or providing for the mathematical model required to facilitate this type of control. Open-loop control and feedback control, often based on canned PID control algorithms, are much more widely used.\n\nThere are three types of control systems: open loop, feed-forward, and feedback. \nAn example of a pure open loop control system is manual non-power-assisted steering of a motor car; the steering system does not have access to an auxiliary power source and does not respond to varying resistance to turning of the direction wheels; the driver must make that response without help from the steering system. In comparison, power steering has access to a controlled auxiliary power source, which depends on the engine speed. When the steering wheel is turned, a valve is opened which allows fluid under pressure to turn the driving wheels. A sensor monitors that pressure so that the valve only opens enough to cause the correct pressure to reach the wheel turning mechanism. This is feed-forward control where the output of the system, the change in direction of travel of the vehicle, plays no part in the system. See Model predictive control.\n\nIf you include the driver in the system, then he does provide a feedback path by observing the direction of travel and compensating for errors by turning the steering wheel. In that case you have a feedback system, and the block labeled \"System\" in Figure(c) is a feed-forward system.\n\nIn other words, systems of different types can be nested, and the overall system regarded as a black-box.\n\nFeedforward control is distinctly different from open loop control and teleoperator systems. Feedforward control requires a mathematical model of the plant (process and/or machine being controlled) and the plant's relationship to any inputs or feedback the system might receive. Neither open loop control nor teleoperator systems require the sophistication of a mathematical model of the physical system or plant being controlled. Control based on operator input without integral processing and interpretation through a mathematical model of the system is a teleoperator system and is not considered feedforward control.\n\nHistorically, the use of the term “feedforward” is found in works by D. M. MacKay as early as 1956. While MacKay’s work is in the field of biological control theory, he speaks only of feedforward systems. MacKay does not mention “Feedforward Control” or allude to the discipline of “Feedforward Controls.” MacKay and other early writers who use the term “feedforward” are generally writing about theories of how human or animal brains work.\n\nThe discipline of “feedforward controls” was largely developed by professors and graduate students at Georgia Tech, MIT, Stanford and Carnegie Mellon. Feedforward is not typically hyphenated in scholarly publications. Meckl and Seering of MIT and Book and Dickerson of Georgia Tech began the development of the concepts of Feedforward Control in the mid 1970s. The discipline of Feedforward Controls was well defined in many scholarly papers, articles and books by the late 1980s.\n\nThe benefits of feedforward control are significant and can often justify the extra cost, time and effort required to implement the technology. Control accuracy can often be improved by as much as an order of magnitude if the mathematical model is of sufficient quality and implementation of the feedforward control law is well thought out. Energy consumption by the feedforward control system and its driver is typically substantially lower than with other controls. Stability is enhanced such that the controlled device can be built of lower cost, lighter weight, springier materials while still being highly accurate and able to operate at high speeds. Other benefits of feedforward control include reduced wear and tear on equipment, lower maintenance costs, higher reliability and a substantial reduction in hysteresis. Feedforward control is often combined with feedback control to optimize performance.\n\nThe mathematical model of the plant (machine, process or organism) used by the feedforward control system may be created and input by a control engineer or it may be learned by the control system. Control systems capable of learning and/or adapting their mathematical model have become more practical as microprocessor speeds have increased. The discipline of modern feedforward control was itself made possible by the invention of microprocessors.\n\nFeedforward control requires integration of the mathematical model into the control algorithm such that it is used to determine the control actions based on what is known about the state of the system being controlled. In the case of control for a lightweight, flexible robotic arm, this could be as simple as compensating between when the robot arm is carrying a payload and when it is not. The target joint angles are adjusted to place the payload in the desired position based on knowing the deflections in the arm from the mathematical model’s interpretation of the disturbance caused by the payload. Systems that plan actions and then pass the plan to a different system for execution do not satisfy the above definition of feedforward control. Unless the system includes a means to detect a disturbance or receive an input and process that input through the mathematical model to determine the required modification to the control action, it is not true feedforward control.\n\nIn systems theory, an open system is a feed forward system that does not have any feedback loop to control its output. In contrast, a closed system uses on a feedback loop to control the operation of the system. In an open system, the output of the system is not fed back into the input to the system for control or operation.\n\nIn physiology, feed-forward control is exemplified by the normal anticipatory regulation of heartbeat in advance of actual physical exertion. Feed-forward control can be likened to learned anticipatory responses to known cues. Feedback regulation of the heartbeat provides further adaptiveness to the running eventualities of physical exertion.\n\nFeedforward systems are also found in biological control by human and animal brains.\n\nEven in the case of biological feedforward systems, such as in the human brain, knowledge or a mental model of the plant (body) can be considered to be mathematical as the model is characterized by limits, rhythms, mechanics and patterns.\n\nA pure feed-forward system is different from a homeostatic control system, which has the function of keeping the body's internal environment 'steady' or in a 'prolonged steady state of readiness.' A homeostatic control system relies mainly on feedback (especially negative), in addition to the feedforward elements of the system.\n\nThe cross regulation of genes can be represented by a graph, where genes are the nodes and one node is linked to another if the former is a transcription factor for the latter. A motif which predominantly appears in all known networks (E. coli, Yeast...) is A activates B, A and B activate C. This motif has been shown to be a feed forward system, detecting non-temporary change of environment. This feed forward control theme is commonly observed in hematopoietic cell lineage development, where irreversible commitments are made.\n\nIn computing, feed-forward normally refers to a perceptron network in which the outputs from all neurons go to following but not preceding layers, so there are no feedback loops. The connections are set up during a training phase, which in effect is when the system is a feedback system.\n\nIn the early 1970s, intercity coaxial transmission systems, including L-carrier, used feed-forward amplifiers to diminish linear distortion. This more complex method allowed wider bandwidth than earlier feedback systems. Optical fiber, however, made such systems obsolete before many were built.\n\nFeedforward control is a discipline within the field of automatic controls used in automation.\n\n\n", "id": "574759", "title": "Feed forward (control)"}
{"url": "https://en.wikipedia.org/wiki?curid=1706332", "text": "Feedforward neural network\n\nA feedforward neural network is an artificial neural network wherein connections between the units do \"not\" form a cycle. As such, it is different from recurrent neural networks.\n\nThe feedforward neural network was the first and simplest type of artificial neural network devised. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network.\n\nThe simplest kind of neural network is a \"single-layer perceptron\" network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. In this way it can be considered the simplest kind of feed-forward network. The sum of the products of the weights and the inputs is calculated in each node, and if the value is above some threshold (typically 0) the neuron fires and takes the activated value (typically 1); otherwise it takes the deactivated value (typically -1). Neurons with this kind of activation function are also called \"artificial neurons\" or \"linear threshold units\". In the literature the term \"perceptron\" often refers to networks consisting of just one of these units. A similar neuron was described by Warren McCulloch and Walter Pitts in the 1940s.\n\nA perceptron can be created using any values for the activated and deactivated states as long as the threshold value lies between the two.\n\nPerceptrons can be trained by a simple learning algorithm that is usually called the \"delta rule\". It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of gradient descent.\n\nSingle-unit perceptrons are only capable of learning linearly separable patterns; in 1969 in a famous monograph entitled \"Perceptrons\", Marvin Minsky and Seymour Papert showed that it was impossible for a single-layer perceptron network to learn an XOR function (nonetheless, it was known that multi-layer perceptrons are capable of producing any possible boolean function). \n\nAlthough a single threshold unit is quite limited in its computational power, it has been shown that networks of parallel threshold units can approximate any continuous function from a compact interval of the real numbers into the interval [-1,1]. This result can be found in Peter Auer, Harald Burgsteiner and Wolfgang Maass \"A learning rule for very simple universal approximators consisting of a single layer of perceptrons\".\n\nA multi-layer neural network can compute a continuous output instead of a step function. A common choice is the so-called logistic function:\n\nWith this choice, the single-layer network is identical to the logistic regression model, widely used in statistical modeling. The logistic function is also known as the sigmoid function. It has a continuous derivative, which allows it to be used in backpropagation. This function is also preferred because its derivative is easily calculated:\n\nThis class of networks consists of multiple layers of computational units, usually interconnected in a feed-forward way. Each neuron in one layer has directed connections to the neurons of the subsequent layer. In many applications the units of these networks apply a \"sigmoid function\" as an activation function.\n\nThe \"universal approximation theorem\" for neural networks states that every continuous function that maps intervals of real numbers to some output interval of real numbers can be approximated arbitrarily closely by a multi-layer perceptron with just one hidden layer. This result holds for a wide range of activation functions, e.g. for the sigmoidal functions.\n\nMulti-layer networks use a variety of learning techniques, the most popular being \"back-propagation\". Here, the output values are compared with the correct answer to compute the value of some predefined error-function. By various techniques, the error is then fed back through the network. Using this information, the algorithm adjusts the weights of each connection in order to reduce the value of the error function by some small amount. After repeating this process for a sufficiently large number of training cycles, the network will usually converge to some state where the error of the calculations is small. In this case, one would say that the network has \"learned\" a certain target function. To adjust weights properly, one applies a general method for non-linear optimization that is called gradient descent. For this, the network calculates the derivative of the error function with respect to the network weights, and changes the weights such that the error decreases (thus going downhill on the surface of the error function). For this reason, back-propagation can only be applied on networks with differentiable activation functions.\n\nIn general, the problem of teaching a network to perform well, even on samples that were not used as training samples, is a quite subtle issue that requires additional techniques. This is especially important for cases where only very limited numbers of training samples are available. The danger is that the network overfits the training data and fails to capture the true statistical process generating the data. Computational learning theory is concerned with training classifiers on a limited amount of data. In the context of neural networks a simple heuristic, called early stopping, often ensures that the network will generalize well to examples not in the training set.\n\nOther typical problems of the back-propagation algorithm are the speed of convergence and the possibility of ending up in a local minimum of the error function. Today there are practical methods that make back-propagation in multi-layer perceptrons the tool of choice for many machine learning tasks.\n\nOne also can use a series of independent neural networks moderated by some intermediary, a similar behavior that happens in brain. These neurons can perform separably and handle a large task, and the results can be finally combined. \n\n\n", "id": "1706332", "title": "Feedforward neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=14402929", "text": "Generalized Hebbian Algorithm\n\nThe Generalized Hebbian Algorithm (GHA), also known in the literature as Sanger's rule, is a linear feedforward neural network model for unsupervised learning with applications primarily in principal components analysis. First defined in 1989, it is similar to Oja's rule in its formulation and stability, except it can be applied to networks with multiple outputs. The name originates because of the similarity between the algorithm and a hypothesis made by Donald Hebb about the way in which synaptic strengths in the brain are modified in response to experience, i.e., that changes are proportional to the correlation between the firing of pre- and post-synaptic neurons.\n\nGHA combines Oja's rule with the Gram-Schmidt process to produce a learning rule of the form\n\nwhere defines the synaptic weight or connection strength between the th input and th output neurons, and are the input and output vectors, respectively, and is the \"learning rate\" parameter.\n\nIn matrix form, Oja's rule can be written\n\nand the Gram-Schmidt algorithm is\n\nwhere is any matrix, in this case representing synaptic weights, is the autocorrelation matrix, simply the outer product of inputs, is the function that diagonalizes a matrix, and is the function that sets all matrix elements on or above the diagonal equal to 0. We can combine these equations to get our original rule in matrix form,\n\nwhere the function sets all matrix elements above the diagonal equal to 0, and note that our output is a linear neuron.\n\nGHA is used in applications where a self-organizing map is necessary, or where a feature or principal components analysis can be used. Examples of such cases include artificial intelligence and speech and image processing.\n\nIts importance comes from the fact that learning is a single-layer process—that is, a synaptic weight changes only depending on the response of the inputs and outputs of that layer, thus avoiding the multi-layer dependence associated with the backpropagation algorithm. It also has a simple and predictable trade-off between learning speed and accuracy of convergence as set by the learning rate parameter .\n\n", "id": "14402929", "title": "Generalized Hebbian Algorithm"}
{"url": "https://en.wikipedia.org/wiki?curid=1091054", "text": "Generative topographic map\n\nGenerative topographic map (GTM) is a machine learning method that is a probabilistic counterpart of the self-organizing map (SOM), is probably convergent and does not require a shrinking neighborhood or a decreasing step size. It is a generative model: the data is assumed to arise by first probabilistically picking a point in a low-dimensional space, mapping the point to the observed high-dimensional input space (via a smooth function), then adding noise in that space. The parameters of the low-dimensional probability distribution, the smooth map and the noise are all learned from the training data using the expectation-maximization (EM) algorithm. GTM was introduced in 1996 in a paper by Christopher Bishop, Markus Svensen, and Christopher K. I. Williams.\n\nThe approach is strongly related to density networks which use importance sampling and a multi-layer perceptron to form a non-linear latent variable model. In the GTM the latent space is a discrete grid of points which is assumed to be non-linearly projected into data space. A Gaussian noise assumption is then made in data space so that the model becomes a constrained mixture of Gaussians. Then the model's likelihood can be maximized by EM.\n\nIn theory, an arbitrary nonlinear parametric deformation could be used. The optimal parameters could be found by gradient descent, etc.\n\nThe suggested approach to the nonlinear mapping is to use a radial basis function network (RBF) to create a nonlinear mapping between the latent space and the data space. The nodes of the \nRBF network then form a feature space and the nonlinear mapping can then be taken as a linear transform of this feature space. This approach has the advantage over the suggested density network approach that it can be optimised analytically.\n\nIn data analysis, GTMs are like a nonlinear version of principal components analysis, which allows high-dimensional data to be modelled as resulting from Gaussian noise added to sources in lower-dimensional latent space. For example, to locate stocks in plottable 2D space based on their hi-D time-series shapes. Other applications may want to have fewer sources than data points, for example mixture models.\n\nIn generative deformational modelling, the latent and data spaces have the same dimensions, for example, 2D images or 1 audio sound waves. Extra 'empty' dimensions are added to the source (known as the 'template' in this form of modelling), for example locating the 1D sound wave in 2D space. Further nonlinear dimensions are then added, produced by combining the original dimensions. The enlarged latent space is then projected back into the 1D data space. The probability of a given projection is, as before, given by the product of the likelihood of the data under the Gaussian noise model with the prior on the deformation parameter. Unlike conventional spring-based deformation modelling, this has the advantage of being analytically optimizable. The disadvantage is that it is a 'data-mining' approach, i.e. the shape of the deformation prior is unlikely to be meaningful as an explanation of the possible deformations, as it is based on a very high, artificial- and arbitrarily constructed nonlinear latent space. For this reason the prior is learned from data rather than created by a human expert, as is possible for spring-based models.\n\nWhile nodes in the self-organizing map (SOM) can wander around at will, GTM nodes are constrained by the allowable transformations and their probabilities. If the deformations are well-behaved the topology of the latent space is preserved.\nThe SOM was created as a biological model of neurons and is a heuristic algorithm. By contrast, the GTM has nothing to do with neuroscience or cognition and is a probabilistically principled model. Thus, it has a number of advantages over SOM, namely:\n\nGTM was introduced by Bishop, Svensen and Williams in their Technical Report in 1997 (Technical Report NCRG/96/015, Aston University, UK) published later in Neural Computation. It was also described in the PhD thesis of Markus Svensen (Aston, 1998).\n\n\n", "id": "1091054", "title": "Generative topographic map"}
{"url": "https://en.wikipedia.org/wiki?curid=35323360", "text": "Grossberg network\n\nGrossberg network is a artificial neural network introduced by Stephen Grossberg. It is a self organizing, competitive network based on continuous time. Grossberg, a neuroscientist and a biomedical engineer, designed this network based on the human visual system.\n\nShunting model is one of the Grossberg's neural network model based on Leaky integrator, given by the expression:\n", "id": "35323360", "title": "Grossberg network"}
{"url": "https://en.wikipedia.org/wiki?curid=13793747", "text": "Group method of data handling\n\nGroup method of data handling (GMDH) is a family of inductive algorithms for computer-based mathematical modeling of multi-parametric datasets that features fully automatic structural and parametric optimization of models.\n\nGMDH is used in such fields as data mining, knowledge discovery, prediction, complex systems modeling, optimization and pattern recognition. Li et. al. (2017)'s results showed that GMDH neural network performed better than the classical forecasting algorithms such as Single Exponential Smooth, Double Exponential Smooth, ARIMA and back-propagation neural network. \n\nGMDH algorithms are characterized by inductive procedure that performs sorting-out of gradually complicated polynomial models and selecting the best solution by means of the so-called \"external criterion\".\n\nA GMDH model with multiple inputs and one output is a subset of components of the \"base function\" (1):\n\nwhere \"f\" are elementary functions dependent on different sets of inputs, \"a\" are coefficients and \"m\" is the number of the base function components.\n\nIn order to find the best solution GMDH algorithms consider various component subsets of the base function (1) called \"partial models\". Coefficients of these models are estimated by the least squares method. GMDH algorithms gradually increase the number of partial model components and find a model structure with optimal complexity indicated by the minimum value of an \"external criterion\". This process is called self-organization of models.\n\nThe most popular base function used in GMDH is the gradually complicated Kolmogorov-Gabor polynomial (2):\n\nThe resulting models are also known as polynomial neural networks. Jürgen Schmidhuber cites GDMH as one of the earliest deep learning methods, remarking that it was used to train eight-layer neural nets as early as 1971.\n\nThe method was originated in 1968 by Prof. Alexey G. Ivakhnenko in the Institute of Cybernetics in Kiev (then in the Ukrainian SSR).\nThis approach from the very beginning was a computer-based method so, a set of computer programs and algorithms were the primary practical results achieved at the base of the new theoretical principles. Thanks to the author's policy of open code sharing the method was quickly settled in the large number of scientific laboratories worldwide. At that time code sharing was quite a physical action since the Internet is at least 5 years younger than GMDH. Despite this fact the first investigation of GMDH outside the Soviet Union had been made soon by R.Shankar in 1972. Later on different GMDH variants were published by Japanese and Polish scientists.\n\nPeriod 1968-1971 is characterized by application of only regularity criterion for solving of the problems of identification, pattern recognition and short-term forecasting. As reference functions polynomials, logical nets, fuzzy Zadeh sets and Bayes probability formulas were used. Authors were stimulated by very high accuracy of forecasting with the new approach. Noiseimmunity was not investigated.\n\nPeriod 1972-1975. The problem of modeling of noised data and incomplete information basis was solved. Multicriteria selection and utilization of additional priory information for noiseimmunity increasing were proposed. Best experiments showed that with extended definition of the optimal model by additional criterion noise level can be ten times more than signal. Then it was improved using Shannon's Theorem of General Communication theory.\n\nPeriod 1976-1979. The convergence of multilayered GMDH algorithms was investigated. It was shown that some multilayered algorithms have \"multilayerness error\" - analogous to static error of control systems. In 1977 a solution of objective systems analysis problems by multilayered GMDH algorithms was proposed. It turned out that sorting-out by criteria ensemble finds the only optimal system of equations and therefore to show complex object elements, their main input and output variables.\n\nPeriod 1980-1988. Many important theoretical results were received. It became clear that full physical models cannot be used for long-term forecasting. It was proved, that non-physical models of GMDH are more accurate for approximation and forecast than physical models of regression analysis. Two-level algorithms which use two different time scales for modeling were developed.\n\nSince 1989 the new algorithms (AC, OCC, PF) for non-parametric modeling of fuzzy objects and SLP for expert systems were developed and investigated. Present stage of GMDH development can be described as blossom out of twice-multilayered neuronets and parallel combinatorial algorithms for multiprocessor computers.\n\nExternal criterion is one of the key features of GMDH. Criterion describes requirements to the model, for example minimization of Least squares. It is always calculated with a separate part of data sample that have not been used for estimation of coefficients. There are several popular criteria:\n\nIf a criterion does not define the number of observations for external dataset then the problem of data dividing ratio appears because the forecasting abilities of identified model are very dependent on the dividing ratio.\n\nFor modeling using GMDH, first, the number of inputs for each neuron, polynomial power and input sources of layers after the first layer are decided. Then, the design process begins from the first layer and goes on. All possible combinations of allowable inputs (all possible neurons) are considered. Then polynomial coefficients are determined using one of the available minimizing methods such as singular value decom-position (with training data). Then, neurons that have better external criterion (for testing data) are kept, and others are removed (The input data for development of the model were divided into training and testing groups). If the external criterion for layer’s best neuron surpasses the stopping criterion, network design is completed and the polynomial expression of the best neuron of the last layer is introduced as the math-ematical prediction function; if not, the next layer will be generated, and this process goes on .\n\nThere are many different ways to choose an order for partial models consideration. The very first consideration order used in GMDH and originally called multilayered inductive procedure is the most popular one. It is a sorting-out of gradually complicated models generated from Kolmogorov-Gabor polynomial. The best model is indicated by the minimum of the external criterion characteristic. Multilayered procedure is equivalent to the Artificial Neural Network with polynomial activation function of neurons. Therefore, the algorithm with such an approach usually referred as GMDH-type Neural Network or Polynomial Neural Network.\n\nAnother important approach to partial models consideration that becomes more and more popular is a brute force combinatorial search that is either limited or full. This approach has some advantages against Polynomial Neural Networks but requires considerable computational power and thus is not effective for objects with more than 30 inputs in case of full search. An important achievement of Combinatorial GMDH is that it fully outperforms linear regression approach if noise level in the input data is greater than zero.\n\nBasic combinatorial algorithm makes the following steps:\n\n\nIn contrast to GMDH-type neural networks Combinatorial algorithm can't be stopped at the certain level of complexity because a point of increase of criterion value can be simply a local minimum, see Fig.1.\n\n\n\n\n", "id": "13793747", "title": "Group method of data handling"}
{"url": "https://en.wikipedia.org/wiki?curid=30867546", "text": "Growing self-organizing map\n\nA growing self-organizing map (GSOM) is a growing variant of a self-organizing map (SOM). The GSOM was developed to address the issue of identifying a suitable map size in the SOM. It starts with a minimal number of nodes (usually 4) and grows new nodes on the boundary based on a heuristic. By using the value called Spread Factor (SF), the data analyst has the ability to control the growth of the GSOM.\n\nAll the starting nodes of the GSOM are boundary nodes, i.e. each node has the freedom to grow in its own direction at the beginning. (Fig. 1) New Nodes are grown from the boundary nodes. Once a node is selected for growing all its free neighboring positions will be grown new nodes. The figure shows the three possible node growth options for a rectangular GSOM.\n\nThe GSOM process is as follows:\n\nThe GSOM can be used for many preprocessing tasks in Data mining, for Nonlinear dimensionality reduction, for approximation of principal curves and manifolds, for clustering and classification. It gives often the better representation of the data geometry than the SOM (see the classical benchmark for principal curves on the left).\n\n\n", "id": "30867546", "title": "Growing self-organizing map"}
{"url": "https://en.wikipedia.org/wiki?curid=1222568", "text": "Helmholtz machine\n\nThe Helmholtz machine is a type of artificial neural network that can account for the hidden structure of a set of data by being trained to create a generative model of the original set of data. The hope is that by learning economical representations of the data, the underlying structure of the generative model should reasonably approximate the hidden structure of the data set. A Helmholtz machine contains two networks, a bottom-up \"recognition\" network that takes the data as input and produces a distribution over hidden variables, and a top-down \"generative\" network that generates values of the hidden variables and the data itself.\n\nHelmholtz machines are usually trained using an unsupervised learning algorithm, such as the wake-sleep algorithm.\n\nHelmholtz machines may also be used in applications requiring a supervised learning algorithm (e.g. character recognition, or position-invariant recognition of an object within a field).\n\n\n", "id": "1222568", "title": "Helmholtz machine"}
{"url": "https://en.wikipedia.org/wiki?curid=1170097", "text": "Hopfield network\n\nA Hopfield network is a form of recurrent artificial neural network popularized by John Hopfield in 1982, but described earlier by Little in 1974. Hopfield nets serve as content-addressable (\"associative\") memory systems with binary threshold nodes. They are guaranteed to converge to a local minimum, but will sometimes converge to a false pattern (wrong local minimum) rather than the stored pattern (expected local minimum). Hopfield networks also provide a model for understanding human memory.\n\nThe units in Hopfield nets are binary threshold units, i.e. the units only take on two different values for their states and the value is determined by whether or not the units' input exceeds their threshold. Hopfield nets normally have units that take on values of 1 or -1, and this convention will be used throughout this page. However, other literature might use units that take values of 0 and 1.\n\nEvery pair of units \"i\" and \"j\" in a Hopfield network have a connection that is described by the connectivity weight formula_1. In this sense, the Hopfield network can be formally described as a complete undirected graph formula_2, where formula_3 is a set of McCulloch-Pitts neurons and formula_4 is a function that links pairs of nodes to a real value, the connectivity weight.\n\nThe connections in a Hopfield net typically have the following restrictions:\n\nThe constraint that weights be symmetric guarantees that the energy function decreases monotonically while following the activation rules. A network with asymmetric weights may exhibit some periodic or chaotic behaviour; however, Hopfield found that this behavior is confined to relatively small parts of the phase space and does not impair the network's ability to act as a content-addressable associative memory system.\n\nUpdating one unit (node in the graph simulating the artificial neuron) in the Hopfield network is performed using the following rule:\n\nformula_7\n\nwhere:\n\nUpdates in the Hopfield network can be performed in two different ways:\n\nThe weight between two units has a powerful impact upon the values of the neurons. Consider the connection weight formula_8 between two neurons i and j. If formula_12, the updating rule implies that:\n\nThus, the values of neurons i and j will converge if the weight between them is positive. Similarly, they will diverge if the weight is negative.\n\nHopfield nets have a scalar value associated with each state of the network referred to as the \"energy\", E, of the network, where:\n\nThis value is called the \"energy\" because: the definition ensures that when units are randomly chosen to update, the energy E will either lower in value or stay the same. Furthermore, under repeated updating the network will eventually converge to a state which is a local minimum in the energy function (which is considered to be a Lyapunov function). Thus, if a state is a local minimum in the energy function, it is a stable state for the network. Note that this energy function belongs to a general class of models in physics, under the name of Ising models; these in turn are a special case of Markov networks, since the associated probability measure, the Gibbs measure, has the Markov property.\n\nInitialization of the Hopfield Networks is done by setting the values of the units to the desired start pattern. Repeated updates are then performed until the network converges to an attractor pattern. Convergence is generally assured, as Hopfield proved that the attractors of this nonlinear dynamical system are stable, not periodic or chaotic as in some other systems. Therefore, in the context of Hopfield Networks, an attractor pattern is a final stable state, a pattern that cannot change any value within it under updating.\n\nTraining a Hopfield net involves lowering the energy of states that the net should \"remember\". This allows the net to serve as a content addressable memory system, that is to say, the network will converge to a \"remembered\" state if it is given only part of the state. The net can be used to recover from a distorted input to the trained state that is most similar to that input. This is called associative memory because it recovers memories on the basis of similarity. For example, if we train a Hopfield net with five units so that the state (1, -1, 1, -1, 1) is an energy minimum, and we give the network the state (1, -1, -1, -1, 1) it will converge to (1, -1, 1, -1, 1). Thus, the network is properly trained when the energy of states which the network should remember are local minima.\n\nThere are various different learning rules that can be used to store information in the memory of the Hopfield Network. It is desirable for a learning rule to have both of the following two properties:\n\nThese properties are desirable, since a learning rule satisfying them is more biologically plausible. For example, since the human brain is always learning new concepts, one can reason that human learning is incremental. A learning system that were not incremental would generally be trained only once, with a huge batch of training data.\n\nThe Hebbian Theory was introduced by Donald Hebb in 1949, in order to explain \"associative learning\", in which simultaneous activation of neuron cells leads to pronounced increases in synaptic strength between those cells. It is often summarized as \"Neurons that fire together, wire together. Neurons that fire out of sync, fail to link\".\n\nThe Hebbian rule is both local and incremental. For the Hopfield Networks, it is implemented in the following manner, when learning formula_20\nbinary patterns:\n\nformula_21\n\nwhere formula_22 represents bit i from pattern formula_23.\n\nIf the bits corresponding to neurons i and j are equal in pattern formula_23, then the product formula_25 will be positive. This would, in turn, have a positive effect on the weight formula_26 and the values of i and j will tend to become equal. The opposite happens if the bits corresponding to neurons i and j are different.\n\nThis rule was introduced by Amos Storkey in 1997 and is both local and incremental. Storkey also showed that a Hopfield network trained using this rule has a greater capacity than a corresponding network trained using the Hebbian rule. The weight matrix of an attractor neural network is said to follow the Storkey learning rule if it obeys:\n\nformula_27\n\nwhere formula_28 is a form of \"local field\" at neuron i.\nThis learning rule is local, since the synapses take into account only neurons at their sides. The rule makes use of more information from the patterns and weights than the generalized Hebbian rule, due to the effect of the local field.\n\nPatterns that the network uses for training (called \"retrieval states\") become attractors of the system. Repeated updates would eventually lead to convergence to one of the retrieval states. However, sometimes the network will converge to spurious patterns (different from the training patterns). The energy in these spurious patterns is also a local minimum. For each stored pattern x, the negation -x is also a spurious pattern.\n\nA spurious state can also be a linear combination of an odd number of retrieval states. For example, when using 3 patterns formula_29, one can get the following spurious state:\n\nformula_30\n\nSpurious patterns that have an even number of states cannot exist, since they might sum up to zero \n\nThe Network capacity of the Hopfield network model is determined by neuron amounts and connections within a given network. Therefore, the number of memories that are able to be stored is dependent on neurons and connections. Furthermore, it was shown that the recall accuracy between vectors and nodes was 0.138 (approximately 138 vectors can be recalled from storage for every 1000 nodes) (Hertz et al., 1991). Therefore, it is evident that many mistakes will occur if one tries to store a large number of vectors. When the Hopfield model does not recall the right pattern, it is possible that an intrusion has taken place, since semantically related items tend to confuse the individual, and recollection of the wrong pattern occurs. Therefore, the Hopfield network model is shown to confuse one stored item with that of another upon retrieval. Perfect recalls and high capacity, >0.14, can be loaded in the network by Hebbian learning method.\n\nThe Hopfield model accounts for associative memory through the incorporation of memory vectors. Memory vectors can be slightly used, and this would spark the retrieval of the most similar vector in the network. However, we will find out that due to this process, intrusions can occur. In associative memory for the Hopfield network, there are two types of operations: auto-association and hetero-association. The first being when a vector is associated with itself, and the latter being when two different vectors are associated in storage. Furthermore, both types of operations are possible to store within a single memory matrix, but only if that given representation matrix is not one or the other of the operations, but rather the combination (auto-associative and hetero-associative) of the two. It is important to note that Hopfield’s network model utilizes the same learning rule as Hebb’s (1949) learning rule, which basically tried to show that learning occurs as a result of the strengthening of the weights by when activity is occurring.\n\nRizzuto and Kahana (2001) were able to show that the neural network model can account for repetition on recall accuracy by incorporating a probabilistic-learning algorithm. During the retrieval process, no learning occurs. As a result, the weights of the network remain fixed, showing that the model is able to switch from a learning stage to a recall stage. By adding contextual drift we are able to show the rapid forgetting that occurs in a Hopfield model during a cued-recall task. The entire network contributes to the change in the activation of any single node.\n\nMcCulloch and Pitts' (1943) dynamical rule, which describes the behavior of neurons, does so in a way that shows how the activations of multiple neurons map onto the activation of a new neuron’s firing rate, and how the weights of the neurons strengthen the synaptic connections between the new activated neuron (and those that activated it). Hopfield would use McCulloch-Pitts's dynamical rule in order to show how retrieval is possible in the Hopfield network. However, it is important to note that Hopfield would do so in a repetitious fashion. Hopfield would use a nonlinear activation function, instead of using a linear function. This would therefore create the Hopfield dynamical rule and with this, Hopfield was able to show that with the nonlinear activation function, the dynamical rule will always modify the values of the state vector in the direction of one of the stored patterns.\n\n\n\n", "id": "1170097", "title": "Hopfield network"}
{"url": "https://en.wikipedia.org/wiki?curid=34676768", "text": "Hybrid Kohonen self-organizing map\n\nIn artificial neural networks, a hybrid Kohonen self-organizing map is a type of self-organizing map (SOM) named for the Finnish professor Teuvo Kohonen, where the network architecture consists of an input layer fully connected to a 2–D SOM or Kohonen layer. \n\nThe output from the Kohonen layer, which is the winning neuron, feeds into a hidden layer and finally into an output layer. In other words, the Kohonen SOM is the front–end, while the hidden and output layer of a multilayer perceptron is the back–end of the\nhybrid Kohonen SOM. The hybrid Kohonen SOM was first applied to machine vision systems for image classification and recognition. \n\nHybrid Kohonen SOM has been used in weather prediction and especially in forecasting stock prices, which has made a challenging task considerably easier. It is fast and efficient with less classification error, hence is a better predictor, when compared to Kohonen SOM and backpropagation networks.\n", "id": "34676768", "title": "Hybrid Kohonen self-organizing map"}
{"url": "https://en.wikipedia.org/wiki?curid=21573718", "text": "HyperNEAT\n\nHypercube-based NEAT, or HyperNEAT, is a generative encoding that evolves artificial neural networks (ANNs) with the principles of the widely used NeuroEvolution of Augmented Topologies (NEAT) algorithm. It is a novel technique for evolving large-scale neural networks using the geometric regularities of the task domain. It uses Compositional Pattern Producing Networks (CPPNs), which are used to generate the images for Picbreeder.org and shapes for EndlessForms.com. HyperNEAT has recently been extended to also evolve plastic ANNs and to evolve the location of every neuron in the network.\n\n\n", "id": "21573718", "title": "HyperNEAT"}
{"url": "https://en.wikipedia.org/wiki?curid=9835466", "text": "Infomax\n\nInfomax is an optimization principle for artificial neural networks and other information processing systems. It prescribes that a function that maps a set of input values \"I\" to a set of output values \"O\" should be chosen or learned so as to maximize the average Shannon mutual information between \"I\" and \"O\", subject to a set of specified constraints and/or noise processes. Infomax algorithms are learning algorithms that perform this optimization process. The principle was described by Linsker in 1988.\n\nInfomax, in its zero-noise limit, is related to the principle of redundancy reduction proposed for biological sensory processing by Horace Barlow in 1961, and applied quantitatively to retinal processing by Atick and Redlich.\n\nOne of the applications of infomax has been to an independent component analysis algorithm that finds independent signals by maximizing entropy. Infomax-based ICA was described by Bell and Sejnowski, and Nadal and Parga in 1995. \n\n\n", "id": "9835466", "title": "Infomax"}
{"url": "https://en.wikipedia.org/wiki?curid=7437547", "text": "Interactive activation and competition networks\n\nInteractive activation and competition (IAC) networks are artificial neural networks used to model memory and intuitive generalizations. They are made up of nodes or artificial neurons which are arrayed and activated in ways that emulate the behaviors of human memory.\n\nThe IAC model is used by the parallel distributed processing (PDP) Group and is associated with James L. McClelland and David E. Rumelhart; it is described in detail in their book \"Explorations in Parallel Distributed Processing: A Handbook of Models, Programs, and Exercises\". This model does not contradict any currently known biological data or theories, and its performance is close enough to human performance as to warrant further investigation.\n\n", "id": "7437547", "title": "Interactive activation and competition networks"}
{"url": "https://en.wikipedia.org/wiki?curid=42639996", "text": "Jpred\n\nJpred v.4 is the latest version of the popular JPred Protein Secondary Structure Prediction Server which provides predictions by the JNet algorithm, one of the most accurate methods for secondary structure prediction, that has existed since 1998 in different versions.\n\nIn addition to protein secondary structure, JPred also makes predictions of solvent accessibility and coiled-coil regions. The JPred service runs up to 134 000 jobs per month and has carried out over 2 million predictions in total for users in 179 countries.\n\nThe current version of JPred (v4) has the following improvements and updates incorporated:\n\nThe JPred v3 followed on from previous versions of JPred developed and maintained by James Cuff and Jonathan Barber (see JPred References). This release added new functionality and fixed lots of bugs. The highlights are:\n\nThe static HTML pages of JPred 2 are still available for reference.\n\nSequence residues are categorised or assigned to one of the secondary structure elements, such as alpha-helix, beta-sheet and coiled-coil.\n\nJnet uses two neural networks for its prediction. The first network is fed with a window of 17 residues over each amino acid in the alignment plus a conservation number. It uses a hidden layer of nine nodes and has three output nodes, one for each secondary structure element.\nThe second network is fed with a window of 19 residues (the result of first network) plus the conservation number. It has a hidden layer with nine nodes and has three output nodes.\n\n", "id": "42639996", "title": "Jpred"}
{"url": "https://en.wikipedia.org/wiki?curid=7049330", "text": "Leabra\n\nLeabra stands for local, error-driven and associative, biologically realistic algorithm. It is a model of learning which is a balance between Hebbian and error-driven learning with other network-derived characteristics. This model is used to mathematically predict outcomes based on inputs and previous learning influences. This model is heavily influenced by and contributes to neural network designs and models. This algorithm is the default algorithm in \"emergent\" (successor of PDP++) when making a new project, and is extensively used in various simulations.\n\nHebbian learning is performed using conditional principal components analysis (CPCA) algorithm with correction factor for sparse expected activity levels.\n\nError-driven learning is performed using GeneRec, which is a generalization of the recirculation algorithm, and approximates Almeida–Pineda recurrent backpropagation. The symmetric, midpoint version of GeneRec is used, which is equivalent to the contrastive Hebbian learning algorithm (CHL). See O'Reilly (1996; Neural Computation) for more details.\n\nThe activation function is a point-neuron approximation with both discrete spiking and continuous rate-code output.\n\nLayer or unit-group level inhibition can be computed directly using a k-winners-take-all (KWTA) function, producing sparse distributed representations.\n\nThe net input is computed as an average, not a sum, over connections, based on normalized, sigmoidally transformed weight values, which are subject to scaling on a connection-group level to alter relative contributions. Automatic scaling is performed to compensate for differences in expected activity level in the different projections.\n\nDocumentation about this algorithm can be found in the book \"Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain\" published by MIT press. and in the Emergent Documentation\n\nThe pseudocode for Leabra is given here, showing exactly how the\npieces of the algorithm described in more detail in the subsequent\nsections fit together.\n\ncodice_1\n\nEmergent is the original implementation of Leabra, written in C++ and highly optimized. This is the fastest implementation, suitable for constructing large networks. Although \"emergent\" has a graphical user interface, it is very complex and has a steep learning curve.\n\nIf you want to understand the algorithm in detail, it will be easier to read non-optimzed code. For this purpose, check out the MATLAB version. There is also an R version available, that can be easily installed via codice_2 in R and has a short introduction to how the package is used. The MATLAB and R versions are not suited for constructing very large networks, but they can be installed quickly and (with some programming background) are easy to use. Furthermore, they can also be adapted easily.\n\n\n", "id": "7049330", "title": "Leabra"}
{"url": "https://en.wikipedia.org/wiki?curid=35699507", "text": "Learning rule\n\nLearning rule or Learning process is a method or a mathematical logic which improves the artificial neural network's performance and usually this rule is applied repeatedly over the network. It is done by updating the levels of a network when a network is simulated in a specific data environment. A learning rule may accept existing condition ( weights and bias ) of the network and will compare the expected result and actual result of the network to give new and improved values for weights and bias. Depending on the complexity of actual model, which is being simulated, the learning rule of the network can be as simple as an XOR gate or Mean Squared Error or it can be the result of multiple differential equations. The learning rule is one of the factors which decides how fast or how accurate the artificial network can be developed. Depending upon the process to develop the network there are three main models of machine learning:\n\n\n", "id": "35699507", "title": "Learning rule"}
{"url": "https://en.wikipedia.org/wiki?curid=827635", "text": "Learning vector quantization\n\nIn computer science, learning vector quantization (LVQ), is a prototype-based supervised classification algorithm. LVQ is the supervised counterpart of vector quantization systems.\n\nLVQ can be understood as a special case of an artificial neural network, more precisely, it applies a winner-take-all Hebbian learning-based approach. It is a precursor to self-organizing maps (SOM) and related to neural gas, and to the k-nearest neighbor algorithm (k-NN). LVQ was invented by Teuvo Kohonen.\n\nAn LVQ system is represented by prototypes formula_1 which are defined in the feature space of observed data. In winner-take-all training algorithms one determines, for each data point, the prototype which is closest to the input according to a given distance measure. The position of this so-called winner prototype is then adapted, i.e. the winner is moved closer if it correctly classifies the data point or moved away if it classifies the data point incorrectly.\n\nAn advantage of LVQ is that it creates prototypes that are easy to interpret for experts in the respective application domain.\nLVQ systems can be applied to multi-class classification problems in a natural way. \nIt is used in a variety of practical applications. See http://liinwww.ira.uka.de/bibliography/Neural/SOM.LVQ.html\nfor an extensive bibliography.\n\nA key issue in LVQ is the choice of an appropriate measure of distance or similarity for training and classification. Recently, techniques have been developed which adapt a parameterized distance measure in the course of training the system, see e.g. (Schneider, Biehl, and Hammer, 2009) and references therein.\n\nLVQ can be a source of great help in classifying text documents.\n\nBelow follows an informal description.<br>\nThe algorithm consists of 3 basic steps. The algorithm's input is:\n\nThe algorithm's flow is:\n\nNote: formula_3 and formula_9 are vectors in feature space.<br>\nA more formal description can be found here: http://jsalatas.ictpro.gr/implementation-of-competitive-learning-networks-for-weka/\n\n\n", "id": "827635", "title": "Learning vector quantization"}
{"url": "https://en.wikipedia.org/wiki?curid=7583202", "text": "Lernmatrix\n\nLernmatrix, an associative-memory-like architecture of an artificial neural network, invented around 1960 by Karl Steinbuch.\n\n", "id": "7583202", "title": "Lernmatrix"}
{"url": "https://en.wikipedia.org/wiki?curid=1796414", "text": "Linde–Buzo–Gray algorithm\n\nThe Linde–Buzo–Gray algorithm (introduced by Yoseph Linde, Andrés Buzo and Robert M. Gray in 1980) is a vector quantization algorithm to derive a good codebook.\n\nIt is similar to the k-means method in data clustering.\n\nAt each iteration, each vector is split into two new vectors.\n\n\n\n", "id": "1796414", "title": "Linde–Buzo–Gray algorithm"}
{"url": "https://en.wikipedia.org/wiki?curid=7823692", "text": "Liquid state machine\n\nA liquid state machine (LSM) is a particular kind of spiking neural network. An LSM consists of a large collection of units (called \"nodes\", or \"neurons\"). Each node receives time varying input from external sources (the inputs) as well as from other nodes. Nodes are randomly connected to each other. The recurrent nature of the connections turns the time varying input into a spatio-temporal pattern of activations in the network nodes. The spatio-temporal patterns of activation are read out by linear discriminant units.\n\nThe soup of recurrently connected nodes will end up computing a large variety of nonlinear functions on the input. Given a large enough variety of such nonlinear functions, it is theoretically possible to obtain linear combinations (using the read out units) to perform whatever mathematical operation is needed to perform a certain task, such as speech recognition or computer vision.\n\nThe word liquid in the name comes from the analogy drawn to dropping a stone into a still body of water or other liquid. The falling stone will generate ripples in the liquid. The input (motion of the falling stone) has been converted into a spatio-temporal pattern of liquid displacement (ripples).\n\nLSMs have been put forward as a way to explain the operation of brains. LSMs are argued to be an improvement over the theory of artificial neural networks because:\n\n\nCriticisms of LSMs as used in computational neuroscience are that\n\nIf a reservoir has fading memory and input separability, with help of a readout,\nit can be proven the liquid state machine is a universal function approximator using Stone-Weierstrass theorem.\n\n\n\n", "id": "7823692", "title": "Liquid state machine"}
{"url": "https://en.wikipedia.org/wiki?curid=10711453", "text": "Long short-term memory\n\nLong short-term memory (LSTM) units (or blocks) are a building unit for layers of a recurrent neural network (RNN). An RNN composed of LSTM units is often called an LSTM network. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell is responsible for \"remembering\" values over arbitrary time intervals; hence the word \"memory\" in LSTM. Each of the three \"gates\" can be thought of as a \"conventional\" artificial neuron, as in a multi-layer (or feedforward) neural network: that is, they compute an activation (using an activation function) of a weighted sum. Intuitively, they can be thought as \"regulators\" of the flow of values that goes through the connections of the LSTM; hence the denotation \"gate\". There are connections between these gates and the cell.\n\nThe expression \"long short-term\" refers to the fact that LSTM is a model for the \"short-term memory\" which can last for a \"long\" period of time. An LSTM is well-suited to classify, process and predict time series given time lags of unknown size and duration between important events. LSTMs were developed to deal with the exploding and vanishing gradient problem when training traditional RNNs. Relative insensitivity to gap length gives an advantage to LSTM over alternative RNNs, hidden Markov models and other sequence learning methods in numerous applications .\n\nLSTM was proposed in 1997 by Sepp Hochreiter and Jürgen Schmidhuber and improved in 2000 by Felix Gers' team.\n\nAmong other successes, LSTM achieved record results in natural language text compression, unsegmented connected handwriting recognition and won the ICDAR handwriting competition (2009). LSTM networks were a major component of a network that achieved a record 17.7% phoneme error rate on the classic TIMIT natural speech dataset (2013).\n\nAs of 2016, major technology companies including Google, Apple, and Microsoft were using LSTM as fundamental components in new products. For example, Google used LSTM for speech recognition on the smartphone, for the smart assistant Allo and for Google Translate. Apple uses LSTM for the \"Quicktype\" function on the iPhone and for Siri. Amazon uses LSTM for Amazon Alexa.\n\nIn 2017 Microsoft reported reaching 95.1% recognition accuracy on the Switchboard corpus, incorporating a vocabulary of 165,000 words. The approach used \"dialog session-based long-short-term memory\".\n\nThere are several architectures of LSTM units. A common architecture is composed of a memory \"cell\", an \"input gate\", an \"output gate\" and a \"forget gate\".\n\nAn LSTM (memory) cell stores a value (or state), for either long or short time periods. This is achieved by using an identity (or no) activation function for the memory cell. In this way, when an LSTM network (that is an RNN composed of LSTM units) is trained with backpropagation through time, the gradient does not tend to vanish. \n\nThe LSTM gates compute an activation, often using the logistic function. Intuitively, the \"input gate\" controls the extent to which a new value flows into the cell, the \"forget gate\" controls the extent to which a value remains in the cell and the \"output gate\" controls the extent to which the value in the cell is used to compute the output activation of the LSTM unit.\n\nThere are connections into and out of these gates. A few connections are recurrent. The weights of these connections, which need to be learned during training, of an LSTM unit are used to direct the operation of the gates. Each of the gates has its own parameters, that is weights and biases, from possibly other units outside the LSTM unit.\n\nIn the equations below, each variable in lowercase italics represents a vector. Matrices formula_1 and formula_2 collect respectively the weights of the input and recurrent connections, \nwhere formula_3 can either be the input gate formula_4, output gate formula_5, the forget gate formula_6 or the memory cell formula_7, depending on the activation being calculated.\n\nCompact form of the equations for the forward pass of a LSTM unit with a forget gate. \n\nwhere the initial values are formula_9 and formula_10 and the operator formula_11 denotes the Hadamard product (entry-wise product). The subscripts formula_12 refer to the time step.\n\n\n\nThe figure on the right is a graphical representation of a LSTM unit with peephole connections (i.e. a peephole LSTM). Peephole connections allow the gates to access the constant error carousel (CEC), whose activation is the cell state. formula_26 is not used, formula_27 is used instead in most places.\n\nConvolutional LSTM. formula_29 denotes the convolution operator.\n\nTo minimize LSTM's total error on a set of training sequences, iterative gradient descent such as backpropagation through time can be used to change each weight in proportion to its derivative with respect to the error. A problem with using gradient descent for standard RNNs is that error gradients vanish exponentially quickly with the size of the time lag between important events. This is due to formula_31 if the spectral radius of formula_32 is smaller than 1. With LSTM units, however, when error values are back-propagated from the output, the error remains in the unit's memory. This \"error carousel\" continuously feeds error back to each of the gates until they learn to cut off the value. Thus, regular backpropagation is effective at training an LSTM unit to remember values for long durations.\n\nLSTM can also be trained by a combination of artificial evolution for weights to the hidden units, and pseudo-inverse or support vector machines for weights to the output units. In reinforcement learning applications LSTM can be trained by policy gradient methods, evolution strategies or genetic algorithms.\n\nMany applications use stacks of LSTM RNNs and train them by connectionist temporal classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.\n\nApplications of LSTM include:\n\nLSTM has Turing completeness in the sense that given enough network units it can compute any result that a conventional computer can compute, provided it has the proper weight matrix, which may be viewed as its program.\n\n\n", "id": "10711453", "title": "Long short-term memory"}
{"url": "https://en.wikipedia.org/wiki?curid=17319790", "text": "Modular neural network\n\nA modular neural network is an artificial neural network characterized by a series of independent neural networks moderated by some intermediary. Each independent neural network serves as a module and operates on separate inputs to accomplish some subtask of the task the network hopes to perform. The intermediary takes the outputs of each module and processes them to produce the output of the network as a whole. The intermediary only accepts the modules' outputs—it does not respond to, nor otherwise signal, the modules. As well, the modules do not interact with each other.\n\nAs artificial neural network research progresses, it is appropriate that artificial neural networks continue to draw on their biological inspiration and emulate the segmentation and modularization found in the brain. The brain, for example, divides the complex task of visual perception into many subtasks. Within a part of the brain, called the thalamus, lies the lateral geniculate nucleus (LGN), which is divided into layers that separately process color and contrast: both major components of vision. After the LGN processes each component in parallel, it passes the result to another region to compile the results.\n\nSome tasks that the brain handles, like vision, employ a hierarchy of sub-networks. However, it is not clear whether some intermediary ties these separate processes together. Rather, as the tasks grow more abstract, the modules communicate with each other, unlike the modular neural network model.\n\nUnlike a single large network that can be assigned to arbitrary tasks, each module in a modular network must be assigned a specific task and connected to other modules in specific ways by a designer. In the vision example, the brain evolved (rather than learned) to create the LGN. In some cases, the designer may choose to follow biological models. In other cases, other models may be superior. The quality of the result will be a function of the quality of the design.\n\nModular neural networks reduce a single large, unwieldy neural network to smaller, potentially more manageable components. Some tasks are intractably large for a single neural network. The benefits of modular neural networks include:\n\nThe possible neuron (node) connections increase exponentially as nodes are added to a network. Computation time depends on the number of nodes and their connections, any increase has drastic consequences for processing time. Assigning specific subtasks to individual modules reduce the number of necessary connections.\n\nA large neural network attempting to model multiple parameters can suffer from interference as new data can alter existing connections or just serve to confuse. Each module can be trained independently and more precisely master its simpler task. This means the training algorithm and the training data can be implemented more quickly.\n\nRegardless of whether a large neural network is biological or artificial, it remains largely susceptible to interference at and failure in any one of its nodes. By compartmentalizing subtasks, failure and interference are much more readily diagnosed and their effects on other sub-networks are eliminated as each one is independent of the other.\n\n", "id": "17319790", "title": "Modular neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=5847907", "text": "MoneyBee\n\nMoneyBee was a distributed computing project in the fields of economics, finance and stock markets, that generated stock forecasts by application of artificial intelligence with the aid of artificial neural networks. MoneyBee acted as a screensaver. The project was run by i42 Informationsmanagement GmbH, a consulting private company from Mannheim, Germany. The project was suspended with a standing invitation for any interested in joining the MoneyBee2 project, but MoneyBee2 seems to have been abandoned in early 2010.\n\nThe idea was first conceived in an economics thesis on stock forecasts with the aid of artificial neural networks at the University of Heidelberg. The MoneyBee system was then developed by i42 GmbH, Mannheim, who added some innovative features to the original idea.\n\nSince starting in September 2000, many users downloaded the German version. By 2001, about 12,000 users were generating 0.1 Teraflops of computing capacity – comparable to an average university mainframe computer at the time.\n\n\n", "id": "5847907", "title": "MoneyBee"}
{"url": "https://en.wikipedia.org/wiki?curid=2266644", "text": "Multilayer perceptron\n\nA multilayer perceptron (MLP) is a class of feedforward artificial neural network. An MLP consists of at least three layers of nodes. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.\n\nMultilayer perceptrons are sometimes colloquially referred to as \"vanilla\" neural networks, especially when they have a single hidden layer.\n\n<section begin=theory />\nIf a multilayer perceptron has a linear activation function in all neurons, that is, a linear function that maps the weighted inputs to the output of each neuron, then linear algebra shows that any number of layers can be reduced to a two-layer input-output model. In MLPs some neurons use a \"nonlinear\" activation function that was developed to model the frequency of action potentials, or firing, of biological neurons.\n\nThe two common activation functions are both sigmoids, and are described by\n\nThe first is a hyperbolic tangent that ranges from -1 to 1, while the other is the logistic function, which is similar in shape but ranges from 0 to 1. Here formula_2 is the output of the formula_3th node (neuron) and formula_4 is the weighted sum of the input connections. Alternative activation functions have been proposed, including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks, another class of supervised neural network models).\n\nThe MLP consists of three or more layers (an input and an output layer with one or more \"hidden layers\") of nonlinearly-activating nodes making it a deep neural network. Since MLPs are fully connected, each node in one layer connects with a certain weight formula_5 to every node in the following layer. \n\nLearning occurs in the perceptron by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation, a generalization of the least mean squares algorithm in the linear perceptron.\n\nWe represent the error in output node formula_6 in the formula_7th data point (training example) by formula_8, where formula_9 is the target value and formula_10 is the value produced by the perceptron. The node weights are adjusted based on corrections that minimize the error in the entire output, given by\n\nUsing gradient descent, the change in each weight is\n\nwhere formula_2 is the output of the previous neuron and formula_14 is the \"learning rate\", which is selected to ensure that the weights quickly converge to a response, without oscillations.\n\nThe derivative to be calculated depends on the induced local field formula_15, which itself varies. It is easy to prove that for an output node this derivative can be simplified to\n\nwhere formula_17 is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is\n\nThis depends on the change in weights of the formula_19th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function.\n<section end=theory />\n\nThe term \"multilayer perceptron\" does not refer to a single perceptron that has multiple layers. Rather, it contains many perceptrons that are organized into layers. An alternative is \"multilayer perceptron network\". Moreover, MLP \"perceptrons\" are not perceptrons in the strictest possible sense. True perceptrons are formally a special case of artificial neurons that use a threshold activation function such as the Heaviside step function. MLP perceptrons can employ arbitrary activation functions. A true perceptron performs binary classification (either this or that), an MLP neuron is free to either perform classification or regression, depending upon its activation function.\n\nThe term \"multilayer perceptron\" later was applied without respect to nature of the nodes/layers, which can be composed of arbitrarily defined artificial neurons, and not perceptrons specifically. This interpretation avoids the loosening of the definition of \"perceptron\" to mean an artificial neuron in general.\n\nMLPs are useful in research for their ability to solve problems stochastically, which often allows approximate solutions for extremely complex problems like fitness approximation.\n\nMLPs are universal function approximators as showed by Cybenko's theorem, so they can be used to create mathematical models by regression analysis. As classification is a particular case of regression when the response variable is categorical, MLPs make good classifier algorithms.\n\nMLPs were a popular machine learning solution in the 1980s, finding applications in diverse fields such as speech recognition, image recognition, and machine translation software, but thereafter faced strong competition from much simpler (and related) support vector machines. Interest in backpropagation networks returned due to the successes of deep learning.\n\n", "id": "2266644", "title": "Multilayer perceptron"}
{"url": "https://en.wikipedia.org/wiki?curid=6092601", "text": "Neocognitron\n\nThe neocognitron is a hierarchical, multilayered artificial neural network proposed by Kunihiko Fukushima in the 1980s. It has been used for handwritten character recognition and other pattern recognition tasks, and served as the inspiration for convolutional neural networks.\n\nThe neocognitron was inspired by the model proposed by Hubel & Wiesel in 1959. They found two types of cells in the visual primary cortex called \"simple cell\" and \"complex cell\", and also proposed a cascading model of these two types of cells for use in pattern recognition tasks.\n\nThe neocognitron is a natural extension of these cascading models. The neocognitron consists of multiple types of cells, the most important of which are called \"S-cells\" and \"C-cells.\" The local features are extracted by S-cells, and these features' deformation, such as local shifts, are tolerated by C-cells. Local features in the input are integrated gradually and classified in the higher layers. The idea of local feature integration is found in several other models, such as the \"LeNet\" model and the \"SIFT\" model.\n\nThere are various kinds of neocognitron. For example, some types of neocognitron can detect multiple patterns in the same input by using backward signals to achieve selective attention.\n\n\n\n", "id": "6092601", "title": "Neocognitron"}
{"url": "https://en.wikipedia.org/wiki?curid=3691953", "text": "NETtalk (artificial neural network)\n\nNETtalk is an artificial neural network. It is the result of research carried out in the mid-1980s by Terrence Sejnowski and Charles Rosenberg. The intent behind NETtalk was to construct simplified models that might shed light on the complexity of learning human level cognitive tasks, and their implementation as a connectionist model that could also learn to perform a comparable task.\n\nNETtalk is a program that learns to pronounce written English text by being shown text as input and matching phonetic transcriptions for comparison.\n\nNETtalk was created to explore the mechanisms of learning to correctly pronounce English text. The authors note that learning to read involves a complex mechanism involving many parts of the human brain. NETtalk does not specifically model the image processing stages and letter recognition of the visual cortex. Rather, it assumes that the letters have been pre-classified and recognized, and these letter sequences comprising words are then shown to the neural network during training and during performance testing. It is NETtalk's task to learn proper associations between the correct pronunciation with a given sequence of letters based on the context in which the letters appear. In other words, NETtalk learns to use the letters around the currently pronounced phoneme that provide cues as to its intended phonemic mapping.\n\n", "id": "3691953", "title": "NETtalk (artificial neural network)"}
{"url": "https://en.wikipedia.org/wiki?curid=14338608", "text": "Neural backpropagation\n\nNeural backpropagation is the phenomenon in which the action potential of a neuron creates a voltage spike both at the end of the axon (normal propagation) and back through to the dendritic arbor or dendrites, from which much of the original input current originated. In addition to active backpropagation of the action potential, there is also passive electrotonic spread. While there is ample evidence to prove the existence of backpropagating action potentials, the function of such action potentials and the extent to which they invade the most distal dendrites remains highly controversial.\n\nWhen a neuron fires an action potential, it is initiated at the axon initial segment. An action potential spreads down the axon because of the gating properties of voltage-gated sodium channels and voltage-gated potassium channels. Initially, it was thought that an action potential could only travel down the axon in one direction towards the axon terminal where it ultimately signaled the release of neurotransmitters. However, recent research has provided evidence for the existence of backwards propagating action potentials (Staley 2004).\n\nNeural backpropagation can occur in one of two ways. First, during the initiation of an axonal action potential, the cell body, or soma, can become depolarized as well. This depolarization can spread through the cell body towards the dendritic tree where there are voltage-gated sodium channels. The depolarization of these voltage-gated sodium channels can then result in the propagation of a dendritic action potential. Such backpropagation is sometimes referred to as an echo of the forward propagating action potential (Staley 2004). It has also been shown that an action potential initiated in the axon can create a retrograde signal that travels in the opposite direction (Hausser 2000). This impulse travels up the axon eventually causing the cell body to become depolarized, thus triggering the dendritic voltage-gated calcium channels. As described in the first process, the triggering of dendritic voltage-gated calcium channels leads to the propagation of a dendritic action potential.\n\nGenerally, excitatory postsynaptic potentials (EPSPs) from synaptic activation are not large enough to activate the dendritic voltage-gated calcium channels (usually on the order of a couple milliamperes each) so backpropagation is typically believed to happen only when the cell is activated to fire an action potential.\n\nIt is important to note that the strength of backpropagating action potentials varies greatly between different neuronal types (Hausser 2000). Some types of neuronal cells show little to no decrease in the amplitude of action potentials as they invade and travel through the dendritic tree while other neuronal cell types, such as cerebellar Purkinje neurons, exhibit very little action potential backpropagation (Stuart 1997). Additionally, there are other neuronal cell types that manifest varying degrees of amplitude decrement during backpropagation. It is thought that this is due to the fact that each neuronal cell type contains varying numbers of the voltage-gated channels required to propagate a dendritic action potential.\n\nGenerally, synaptic signals that are received by the dendrite are combined in the soma in order to generate an action potential that is then transmitted down the axon toward the next synaptic contact. Thus, the backpropagation of action potentials poses a threat to initiate an uncontrolled positive feedback loop between the soma and the dendrites. For example, as an action potential was triggered, its dendritic echo could enter the dendrite and potentially trigger a second action potential. If left unchecked, an endless cycle of action potentials triggered by their own echo would be created. In order to prevent such a cycle, most neurons have a relatively high density of A-type K+ channels.\n\nA-type K+ channels belong to the superfamily of voltage-gated ion channels and are transmembrane channels that help maintain the cell’s membrane potential (Cai 2007). Typically, they play a crucial role in returning the cell to its resting membrane following an action potential by allowing an inhibitory current of K+ ions to quickly flow out of the neuron. The presence of these channels in such high density in the dendrites explains their inability to initiate an action potential, even during synaptic input. Additionally, the presence of these channels provides a mechanism by which the neuron can suppress and regulate the backpropagation of action potentials through the dendrite (Vetter 2000). Results have indicated a linear increase in the density of A-type channels with increasing distance into the dendrite away from the soma. The increase in the density of A-type channels results in a dampening of the backpropagating action potential as it travels into the dendrite. Essentially, inhibition occurs because the A-type channels facilitate the outflow of K+ ions in order to maintain the membrane potential below threshold levels (Cai 2007). Such inhibition limits EPSP and protects the neuron from entering a never-ending positive-positive feedback loop between the soma and the dendrites.\n\nSince the 1950s, evidence has existed that neurons in the central nervous system generate an action potential, or voltage spike, that travels both through the axon to signal the next neuron and backpropagates through the dendrites sending a retrograde signal to its presynaptic signaling neurons. This current decays significantly with travel length along the dendrites, so effects are predicted to be more significant for neurons whose synapses are near the postsynaptic cell body, with magnitude depending mainly on sodium-channel density in the dendrite. It is also dependent on the shape of the dendritic tree and, more importantly, on the rate of signal currents to the neuron. On average, a backpropagating spike loses about half its voltage after traveling nearly 500 micrometres.\n\nBackpropagation occurs actively in the neocortex, hippocampus, substantia nigra, and spinal cord, while in the cerebellum it occurs relatively passively. This is consistent with observations that synaptic plasticity is much more apparent in areas like the hippocampus, which controls spatial memory, than the cerebellum, which controls more unconscious and vegetative functions.\n\nThe backpropagating current also causes a voltage change that increases the concentration of Ca in the dendrites, an event which coincides with certain models of synaptic plasticity. This change also affects future integration of signals, leading to at least a short-term response difference between the presynaptic signals and the postsynaptic spike.\n\nWhile many questions have yet to be answered in regards to neural backpropagation, there exists a number of hypotheses regarding its function. Some proposed function include involvement in synaptic plasticity, involvement in dendrodendritic inhibition, boosting synaptic responses, resetting membrane potential, retrograde actions at synapses and conditional axonal output.\nBackpropagation is believed to help form LTP (long term potentiation) and Hebbian plasticity at hippocampal synapses. Since artificial LTP induction, using microelectrode stimulation, voltage clamp, etc. requires the postsynaptic cell to be slightly depolarized when EPSPs are elicited, backpropagation can serve as the means of depolarization of the postsynaptic cell.\n\nWhile a backpropagating action potential can presumably cause changes in the weight of the presynaptic connections, there is no simple mechanism for an error signal to propagate through multiple layers of neurons, as in the computer backpropagation algorithm. However, simple linear topologies have shown that effective computation is possible through signal backpropagation in this biological sense.\n\n", "id": "14338608", "title": "Neural backpropagation"}
{"url": "https://en.wikipedia.org/wiki?curid=12589161", "text": "Neural cryptography\n\nNeural cryptography is a branch of cryptography dedicated to analyzing the application of stochastic algorithms, especially artificial neural network algorithms, for use in encryption and cryptanalysis.\n\nNeural Networks are well known for their ability to selectively explore the solution space of a given problem. This feature finds a natural niche of application in the field of cryptanalysis. At the same time, Neural Networks offer a new approach to attack ciphering algorithms based on the principle that any function could be reproduced by a neural network, which is a powerful proven computational tool that can be used to find the inverse-function of any cryptographic algorithm.\n\nThe ideas of mutual learning, self learning, and stochastic behavior of neural networks and similar algorithms can be used for different aspects of cryptography, like public-key cryptography, solving the key distribution problem using neural network mutual synchronization, hashing or generation of pseudo-random numbers.\n\nAnother idea is the ability of a neural network to separate space in non-linear pieces using \"bias\". It gives different probabilities of activating the neural network or not. This is very useful in the case of Cryptanalysis.\n\nTwo names are used to design the same domain of research: Neuro-Cryptography and Neural Cryptography.\n\nThe first work that it is known on this topic can be traced back to 1995 in an IT Master Thesis.\n\nThere are currently no practical applications due to the recent development of the field, but it could be used specifically where the keys are continually generated and the system (both pairs and the insecure media) is in a continuously evolving mode.\nIn 1995, Sebastien Dourlens applied neural networks cryptanalyze DES by allowing the networks to learn how to invert the S-tables of the DES. The bias in DES studied through Differential Cryptanalysis by Adi Shamir is highlighted. The experiment shows about 50% of the key bits can be found, allowing the complete key to be found in a short time. Hardware application with multi micro-controllers have been proposed due to the easy implementation of multilayer neural networks in hardware.\nOne example of a public-key protocol is given by Khalil Shihab. He describes the decryption scheme and the public key creation that are based on a backpropagation neural network. The encryption scheme and the private key creation process are based on Boolean algebra. This technique has the advantage of small time and memory complexities. A disadvantage is the property of backpropagation algorithms: because of huge training sets, the learning phase of a neural network is very long. Therefore, the use of this protocol is only theoretical so far.\n\nThe most used protocol for key exchange between two parties A and B in the practice is Diffie-Hellman protocol. Neural key exchange, which is based on the synchronization of two tree parity machines, should be a secure replacement for this method.\nSynchronizing these two machines is similar to synchronizing two chaotic oscillators in chaos communications.\n\nThe tree parity machine is a special type of multi-layer feed-forward neural network.\n\nIt consists of one output neuron, K hidden neurons and K*N input neurons. Inputs to the network take 3 values: \nThe weights between input and hidden neurons take the values: \nOutput value of each hidden neuron is calculated as a sum of all multiplications of input neurons and these weights: \nSignum is a simple function, which returns -1,0 or 1: <br>\n\nIf the scalar product is 0, the output of the hidden neuron is mapped to -1 in order to ensure a binary output value. The output of neural network is then computed as the multiplication of all values produced by hidden elements: <br>\nOutput of the tree parity machine is binary.\n\nEach party (A and B) uses its own tree parity machine. Synchronization of the tree parity machines is achieved in these steps\n\nAfter the full synchronization is achieved (the weights w of both tree parity machines are same), A and B can use their weights as keys.<br>\nThis method is known as a bidirectional learning.<br> \nOne of the following learning rules can be used for the synchronization:\n\nWhere:\nAnd:\n\nIn every attack it is considered, that the attacker E can eavesdrop messages between the parties A and B, but does not have an opportunity to change them.\n\nTo provide a brute force attack, an attacker has to test all possible keys (all possible values of weights wij). By K hidden neurons, K*N input neurons and boundary of weights L, this gives (2L+1) possibilities. For example, the configuration K = 3, L = 3 and N = 100 gives us 3*10 key possibilities, making the attack impossible with today’s computer power.\n\nOne of the basic attacks can be provided by an attacker, who owns the same tree parity machine as the parties A and B. He wants to synchronize his tree parity machine with these two parties. In each step there are three situations possible:\nIt has been proven, that the synchronization of two parties is faster than learning of an attacker. It can be improved by increasing of the synaptic depth L of the neural network. That gives this protocol enough security and an attacker can find out the key only with small probability.\n\nFor conventional cryptographic systems, we can improve the security of the protocol by increasing of the key length. In the case of neural cryptography, we improve it by increasing of the synaptic depth L of the neural networks. Changing this parameter increases the cost of a successful attack exponentially, while the effort for the users grows polynomially. Therefore, breaking the security of neural key exchange belongs to the complexity class NP.\n\nAlexander Klimov, Anton Mityaguine, and Adi Shamir say that the original neural synchronization scheme can be broken by at least three different attacks—geometric, probabilistic analysis, and using genetic algorithms. Even though this particular implementation is insecure, the ideas behind chaotic synchronization could potentially lead to a secure implementation.\n\nThe permutation parity machine is a binary variant of the tree parity machine.\n\nIt consists of one input layer, one hidden layer and one output layer. The number of neurons in the output layer depends on the number of hidden units K. Each hidden neuron has N binary input neurons: \nThe weights between input and hidden neurons are also binary: \n\nOutput value of each hidden neuron is calculated as a sum of all exclusive disjunctions (exclusive or) of input neurons and these weights:\n\n(⊕ means XOR).\n\nThe function formula_18 is a threshold function, which returns 0 or 1: <br>\n\nThe output of neural network with two or more hidden neurons can be computed as the exclusive or of the values produced by hidden elements: <br>\nOther configurations of the output layer for K>2 are also possible.\n\nThis machine has proven to be robust enough against some attacks so it could be used as a cryptographic mean, but it has been shown to be vulnerable to a probabilistic attack.\n\nA quantum computer is a device that uses quantum mechanisms for computation. In this device the data are stored as qubits (quantum binary digits). That gives a quantum computer in comparison with a conventional computer the opportunity to solve complicated problems in a short time, e.g. discrete logarithm problem or factorization. Algorithms that are not based on any of these number theory problems are being searched because of this property.\n\nNeural key exchange protocol is not based on any number theory.\nIt is based on the difference between unidirectional and bidirectional synchronization of neural networks.\nTherefore, something like the neural key exchange protocol could give rise to potentially faster key exchange schemes.\n\n\n", "id": "12589161", "title": "Neural cryptography"}
{"url": "https://en.wikipedia.org/wiki?curid=2457021", "text": "Neural gas\n\nNeural gas is an artificial neural network, inspired by the self-organizing map and introduced in 1991 by Thomas Martinetz and Klaus Schulten. The neural gas is a simple algorithm for finding optimal data representations based on feature vectors. The algorithm was coined \"neural gas\" because of the dynamics of the feature vectors during the adaptation process, which distribute themselves like a gas within the data space. It is applied where data compression or vector quantization is an issue, for example speech recognition, image processing or pattern recognition. As a robustly converging alternative to the k-means clustering it is also used for cluster analysis.\n\nGiven a probability distribution formula_1 of data vectors formula_2 and a finite number of feature vectors formula_3.\n\nWith each time step formula_4, a data vector formula_2 randomly chosen from formula_1 is presented. Subsequently, the distance order of the feature vectors to the given data vector formula_2 is determined. Let formula_8 denote the index of the closest feature vector, formula_9 the index of the second closest feature vector, and formula_10 the index of the feature vector most distant to formula_2. Then each feature vector is adapted according to\n\nformula_12\n\nwith formula_13 as the adaptation step size and formula_14 as the so-called neighborhood range. formula_13 and formula_14 are reduced with increasing formula_4. After sufficiently many adaptation steps the feature vectors cover the data space with minimum representation error.\n\nThe adaptation step of the neural gas can be interpreted as gradient descent on a cost function. By adapting not only the closest feature vector but all of them with a step size decreasing with increasing distance order, compared to (online) k-means clustering a much more robust convergence of the algorithm can be achieved. The neural gas model does not delete a node and also does not create new nodes.\n\nA number of variants of the neural gas algorithm exists in the literature so as to mitigate some of its shortcomings. More notable is perhaps Bernd Fritzke's growing neural gas, but also one should mention further elaborations such as the Growing When Required network and also the incremental growing neural gas.\n\nFritzke describes the growing neural gas (GNG) as an incremental network model that learns topological relations by using a \"Hebb-like learning rule\", only, unlike the neural gas, it has no parameters that change over time and it is capable of continuous learning.\n\nHaving a network with a growing set of nodes, like the one implemented by the GNG algorithm was seen as a great advantage, however some limitation on the learning was seen by the introduction of the parameter λ, in which the network would only be able to grow when iterations were a multiple of this parameter. The proposal to mitigate this problem was a new algorithm, the Growing When Required network (GWR), which would have the network grow more quickly, by adding nodes as quickly as possible whenever the network identified that the existing nodes would not describe the input well enough. \n\nAnother neural gas variant inspired in the GNG algorithm is the incremental growing neural gas (IGNG). The authors propose the main advantage of this algorithm to be \"learning new data (plasticity) without degrading the previously trained network and forgetting the old input data (stability).\" \n\n\n", "id": "2457021", "title": "Neural gas"}
{"url": "https://en.wikipedia.org/wiki?curid=40473285", "text": "Neural network synchronization protocol\n\nThe Neural network synchronization protocol, abbreviated as NNSP, is built on the application-level layer of the OSI upon TCP/IP. Aiming at secure communication, this protocol's design make use of a concept called neural network synchronization. Server and each connected client must create special type of neural network called Tree Parity Machine then compute outputs of their neural networks and exchange them in an iterative manner through the public channel. By learning and exchanging public outputs of their networks, client's and server's neural networks will be synchronized, meaning they will have identical synaptic weights, after some time. Once synchronization is achieved, weights of networks are used for secret key derivation. For the purposes of encryption/decryption of subsequent communication this symmetric key is used.\n\n", "id": "40473285", "title": "Neural network synchronization protocol"}
{"url": "https://en.wikipedia.org/wiki?curid=21393064", "text": "Neural Networks (journal)\n\nNeural Networks is a monthly peer-reviewed scientific journal and an official journal of the International Neural Network Society, European Neural Network Society, and Japanese Neural Network Society. It was established in 1988 and is published by Elsevier. The journal covers all aspects of research on artificial neural networks. The founding editor-in-chief was Stephen Grossberg (Boston University), the current editors-in-chief are DeLiang Wang (Ohio State University) and Kenji Doya (Okinawa Institute of Science and Technology). The journal is abstracted and indexed in Scopus and the Science Citation Index. According to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 5.287.\n", "id": "21393064", "title": "Neural Networks (journal)"}
{"url": "https://en.wikipedia.org/wiki?curid=344922", "text": "Neuroevolution of augmenting topologies\n\nNeuroEvolution of Augmenting Topologies (NEAT) is a genetic algorithm (GA) for the generation of evolving artificial neural networks (a neuroevolution technique) developed by Ken Stanley in 2002 while at The University of Texas at Austin. It alters both the weighting parameters and structures of networks, attempting to find a balance between the fitness of evolved solutions and their diversity. It is based on applying three key techniques: tracking genes with history markers to allow crossover among topologies, applying speciation (the evolution of species) to preserve innovations, and developing topologies incrementally from simple initial structures (\"complexifying\").\nOn simple control tasks, the NEAT algorithm often arrives at effective networks more quickly than other contemporary neuro-evolutionary techniques and reinforcement learning methods.\n\nTraditionally a neural network topology is chosen by a human experimenter, and effective connection weight values are learned through a training procedure. This yields a situation whereby a trial and error process may be necessary in order to determine an appropriate topology. NEAT is an example of a topology and weight evolving artificial neural network (TWEAN) which attempt to simultaneously learn weight values and an appropriate topology for a neural network. \n\nIn order to encode the network into a phenotype for the GA, NEAT uses a direct encoding scheme which means every connection and neuron is explicitly represented. This is in contrast to indirect encoding schemes which define rules that allow the network to be constructed without explicitly representing every connection and neuron allowing for more compact representation.\n\nThe NEAT approach begins with a perceptron-like feed-forward network of only input neurons and output neurons. As evolution progresses through discrete steps, the complexity of the network's topology may grow, either by inserting a new neuron into a connection path, or by creating a new connection between (formerly unconnected) neurons.\n\nThe competing conventions problem arises when there is more than one way of representing information in a phenotype. For example if a genome contains neurons A, B and C and is represented by [A B C], if this genome is crossed with an identical genome (in terms of functionality) but ordered [C B A] crossover will yield children that are missing information ([A B A] or [C B C]), in fact 1/3 of the information has been lost in this example. NEAT solves this problem by tracking the history of genes by the use of a global innovation number which increases as new genes are added. When adding a new gene the global innovation number is incremented and assigned to that gene. Thus the higher the number the more recently the gene was added. For a particular generation if an identical mutation occurs in more than one genome they are both given the same number, beyond that however the mutation number will remain unchanged indefinitely. \n\nThese innovation numbers allow NEAT to match up genes which can be crossed with each other.\n\nThe original implementation by Ken Stanley is published under the GPL. It integrates with Guile, a GNU scheme interpreter. This implementation of NEAT is considered the conventional basic starting point for implementations of the NEAT algorithm.\n\nIn 2003 Stanley devised an extension to NEAT that allows evolution to occur in real time rather than through the iteration of generations as used by most genetic algorithms. The basic idea is to put the population under constant evaluation with a \"lifetime\" timer on each individual in the population. When a network's timer expires its current fitness measure is examined to see whether it falls near the bottom of the population, and if so it is discarded and replaced by a new network bred from two high-fitness parents. A timer is set for the new network and it is placed in the population to participate in the ongoing evaluations.\n\nThe first application of rtNEAT is a video game called Neuro-Evolving Robotic Operatives, or NERO. In the first phase of the game, individual players deploy robots in a 'sandbox' and train them to some desired tactical doctrine. Once a collection of robots has been trained, a second phase of play allows players to pit their robots in a battle against robots trained by some other player, to see how well their training regimens prepared their robots for battle.\n\nAn extension of Ken Stanley's NEAT, developed by Colin Green, adds periodic pruning of the network topologies of candidate solutions during the evolution process. This addition addressed concern that unbounded automated growth would generate unnecessary structure.\n\nHyperNEAT is specialized to evolve large scale structures. It was originally based on the CPPN theory and is an active field of research.\n\nContent-Generating NEAT (cgNEAT) evolves custom video game content based on user preferences. The first video game to implement cgNEAT is Galactic Arms Race, a space-shooter game in which unique particle system weapons are evolved based on player usage statistics. Each particle system weapon in the game is controlled by an evolved CPPN, similarly to the evolution technique in the NEAT Particles interactive art program.\n\nodNEAT is an online and decentralized version of NEAT designed for multi-robot systems. odNEAT is executed onboard robots themselves during task execution to continuously optimize the parameters and the topology of the artificial neural network-based controllers. In this way, robots executing odNEAT have the potential to adapt to changing conditions and learn new behaviors as they carry out their tasks. The online evolutionary process is implemented according to a physically distributed island model. Each robot optimizes an internal population of candidate solutions (intra-island variation), and two or more robots exchange candidate solutions when they meet (inter-island migration). In this way, each robot is potentially self-sufficient and the evolutionary process capitalizes on the exchange of controllers between multiple robots for faster synthesis of effective controllers.\n\n\n\n\n", "id": "344922", "title": "Neuroevolution of augmenting topologies"}
{"url": "https://en.wikipedia.org/wiki?curid=8892368", "text": "Ni1000\n\nThe Ni1000 is an artificial neural network chip developed by Nestor Corporation. The chip is aimed at image analysis applications, contains more than 3 million transistors and can analyze patterns at the rate of 40,000 per second. Prototypes running with Nestor's OCR software in 1994 were capable of recognizing around 100 handwritten characters per second.\n\n", "id": "8892368", "title": "Ni1000"}
{"url": "https://en.wikipedia.org/wiki?curid=9617564", "text": "Oja's rule\n\nOja's learning rule, or simply Oja's rule, named after Finnish computer scientist Erkki Oja, is a model of how neurons in the brain or in artificial neural networks change connection strength, or learn, over time. It is a modification of the standard Hebb's Rule (see Hebbian learning) that, through multiplicative normalization, solves all stability problems and generates an algorithm for principal components analysis. This is a computational form of an effect which is believed to happen in biological neurons.\n\nOja's rule requires a number of simplifications to derive, but in its final form it is demonstrably stable, unlike Hebb's rule. It is a single-neuron special case of the Generalized Hebbian Algorithm. However, Oja's rule can also be generalized in other ways to varying degrees of stability and success.\n\nOja's rule defines the change in presynaptic weights given the output response formula_1 of a neuron to its inputs to be\n\nwhere is the \"learning rate\" which can also change with time. Note that the bold symbols are vectors and defines a discrete time iteration. The rule can also be made for continuous iterations as\n\nThe simplest learning rule known is Hebb's rule, which states in conceptual terms that \"neurons that fire together, wire together\". In component form as a difference equation, it is written\n\nor in scalar form with implicit -dependence,\n\nwhere is again the output, this time explicitly dependent on its input vector .\n\nHebb's rule has synaptic weights approaching infinity with a positive learning rate. We can stop this by normalizing the weights so that each weight's magnitude is restricted between 0, corresponding to no weight, and 1, corresponding to being the only input neuron with any weight. We do this by normalizing the weight vector to be of length one:\n\nNote that in Oja's original paper, , corresponding to quadrature (root sum of squares), which is the familiar Cartesian normalization rule. However, any type of normalization, even linear, will give the same result without loss of generality.\n\nFor a small learning rate formula_7 the equation can be expanded as a Power series in formula_8.\n\nFor small , our higher-order terms go to zero. We again make the specification of a linear neuron, that is, the output of the neuron is equal to the sum of the product of each input and its synaptic weight, or\n\nWe also specify that our weights normalize to , which will be a necessary condition for stability, so\n\nwhich, when substituted into our expansion, gives Oja's rule, or\n\nIn analyzing the convergence of a single neuron evolving by Oja's rule, one extracts the first \"principal component\", or feature, of a data set. Furthermore, with extensions using the Generalized Hebbian Algorithm, one can create a multi-Oja neural network that can extract as many features as desired, allowing for principal components analysis.\n\nA principal component is extracted from a dataset through some associated vector , or , and we can restore our original dataset by taking\n\nIn the case of a single neuron trained by Oja's rule, we find the weight vector converges to , or the first principal component, as time or number of iterations approaches infinity. We can also define, given a set of input vectors , that its correlation matrix has an associated eigenvector given by with eigenvalue . The variance of outputs of our Oja neuron then converges with time iterations to the principal eigenvalue, or\n\nThese results are derived using Lyapunov function analysis, and they show that Oja's neuron necessarily converges on strictly the first principal component if certain conditions are met in our original learning rule. Most importantly, our learning rate is allowed to vary with time, but only such that its sum is \"divergent\" but its power sum is \"convergent\", that is\n\nOur output activation function is also allowed to be nonlinear and nonstatic, but it must be continuously differentiable in both and and have derivatives bounded in time.\n\nRecently, in the context of associative learning, it has been shown that the Hebbian rule, which is similar to Oja's rule, can be generalized using an Ising-like model: The main idea of the generalization is based on formulating energy function like in Ising model and then applying stochastic gradient descent algorithm to this energy function. The energy function and the update rule corresponding to following the derivative are given by:\n\nwhere:\nformula_18, formula_19 is the coupling among inputs, formula_20 is the correlation strength between the model and the output, formula_21 corresponds to the presence of an external magnetic field, formula_22 determines the connections among inputs.\n\nThen, for formula_23, formula_24, and formula_25 we get the Hebbian rule, and for formula_23, formula_27, formula_25, and formula_29, where formula_30 is an identity matrix, introduce weight decay. The formula then reduces to:\n\nOja's rule was originally described in Oja's 1982 paper, but the principle of self-organization to which it is applied is first attributed to Alan Turing in 1952. PCA has also had a long history of use before Oja's rule formalized its use in network computation in 1989. The model can thus be applied to any problem of self-organizing mapping, in particular those in which feature extraction is of primary interest. Therefore, Oja's rule has an important place in image and speech processing. It is also useful as it expands easily to higher dimensions of processing, thus being able to integrate multiple outputs quickly. A canonical example is its use in binocular vision.\n\nThere is clear evidence for both long-term potentiation and long-term depression in biological neural networks, along with a normalization effect in both input weights and neuron outputs. However, while there is no direct experimental evidence yet of Oja's rule active in a biological neural network, a biophysical derivation of a generalization of the rule is possible. Such a derivation requires retrograde signalling from the postsynaptic neuron, which is biologically plausible (see neural backpropagation), and takes the form of\n\nwhere as before is the synaptic weight between the th input and th output neurons, is the input, is the postsynaptic output, and we define to be a constant analogous the learning rate, and and are presynaptic and postsynaptic functions that model the weakening of signals over time. Note that the angle brackets denote the average and the ∗ operator is a convolution. By taking the pre- and post-synaptic functions into frequency space and combining integration terms with the convolution, we find that this gives an arbitrary-dimensional generalization of Oja's rule known as Oja's Subspace, namely\n\n\n", "id": "9617564", "title": "Oja's rule"}
{"url": "https://en.wikipedia.org/wiki?curid=1635395", "text": "Optical neural network\n\nAn optical neural network is a physical implementation of an artificial neural network with optical components.\n\nSome artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystals.\n\nBiological neural networks function on an electrochemical basis, while optical neural networks use electromagnetic waves. Optical interfaces to biological neural networks can be created with optogenetics, but is not the same as an optical neural networks. In biological neural networks there exist a lot of different mechanisms for dynamically changing the state of the neurons, these include short-term and long-term synaptic plasticity. Synaptic plasticity is among the electrophysiological phenomena used to control the efficiency of synaptic transmission, long-term for learning and memory, and short-term for short transient changes in synaptic transmission efficiency. Implementing this with optical components is difficult, and ideally requires advanced photonic materials. Properties that might be desirable in photonic materials for optical neural networks include the ability to change their efficiency of transmitting light, based on the intensity of incoming light.\n\nThere is one recent (2007) model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC). It had been implemented in the year 2000 and reported based on modified Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. Full parallelism, large array size and the speed of light are three promises offered by POAC to implement an optical CNN. They had been investigated during the last years with their practical limitations and considerations yielding the design of the first portable POAC version.\n\nThe practical details: hardware (optical setups) and software (optical templates) are published. However, POAC is a general purpose and programmable array computer that has a wide range of applications including:\n\n", "id": "1635395", "title": "Optical neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=35179233", "text": "Probabilistic neural network\n\nA probabilistic neural network (PNN) is a feedforward neural network, which is widely used in classification and pattern recognition problems. In the PNN algorithm, the parent probability distribution function (PDF) of each class is approximated by a Parzen window and a non-parametric function. Then, using PDF of each class, the class probability of a new input data is estimated and Bayes’ rule is then employed to allocate the class with highest posterior probability to new input data. By this method, the probability of mis-classification is minimized. This type of ANN was derived from the Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It was introduced by D.F. Specht in the 1966. In a PNN, the operations are organized into a multilayered feedforward network with four layers:\n\nPNN is often used in classification problems. When an input is present, the first layer computes the distance from the input vector to the training input vectors. This produces a vector where its elements indicate how close the input is to the training input. The second layer sums the contribution for each class of inputs and produces its net output as a vector of probabilities. Finally, a compete transfer function on the output of the second layer picks the maximum of these probabilities, and produces a 1 (positive identification) for that class and a 0 (negative identification) for non-targeted classes.\n\nEach neuron in the input layer represents a predictor variable. In categorical variables, \"N-1\" neurons are used when there are \"N\" number of categories. It standardizes the range of the values by subtracting the median and dividing by the interquartile range. Then the input neurons feed the values to each of the neurons in the hidden layer.\n\nThis layer contains one neuron for each case in the training data set. It stores the values of the predictor variables for the case along with the target value. A hidden neuron computes the Euclidean distance of the test case from the neuron’s center point and then applies the radial basis function kernel function using the sigma values.\n\nFor PNN networks there is one pattern neuron for each category of the target variable. The actual target category of each training case is stored with each hidden neuron; the weighted value coming out of a hidden neuron is fed only to the pattern neuron that corresponds to the hidden neuron’s category. The pattern neurons add the values for the class they represent.\n\nThe output layer compares the weighted votes for each target category accumulated in the pattern layer and uses the largest vote to predict the target category.\n\nThere are several advantages and disadvantages using PNN instead of multilayer perceptron.\n\n\n", "id": "35179233", "title": "Probabilistic neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=25056220", "text": "Promoter based genetic algorithm\n\nThe promoter based genetic algorithm (PBGA) is a genetic algorithm for neuroevolution developed by F. Bellas and R.J. Duro in the Integrated Group for Engineering Research (GII) at the University of Coruña, in Spain. It evolves variable size feedforward artificial neural networks (ANN) that are encoded into sequences of genes for constructing a basic ANN unit. Each of these blocks is preceded by a gene promoter acting as an on/off switch that determines if that particular unit will be expressed or not.\n\nThe basic unit in the PBGA is a neuron with all of its inbound connections as represented in the following figure:\n\nThe genotype of a basic unit is a set of real valued weights followed by the parameters of the neuron and proceeded by an integer valued field that determines the promoter gene value and, consequently, the expression of the unit. By concatenating units of this type we can construct the whole network.\n\nWith this encoding it is imposed that the information that is not expressed is still carried by the genotype in evolution but it is shielded from direct selective pressure, maintaining this way the diversity in the population, which has been a design premise for this algorithm. Therefore, a clear difference is established between the search space and the solution space, permitting information learned and encoded into the genotypic representation to be preserved by disabling promoter genes.\n\nThe PBGA was originally presented within the field of autonomous robotics, in particular in the real time learning of environment models of the robot.\n\nIt has been used inside the Multilevel Darwinist Brain (MDB) cognitive mechanism developed in the GII for real robots on-line learning. In another paper it is shown how the application of the PBGA together with an external memory that stores the successful obtained world models, is an optimal strategy for adaptation in dynamic environments. \n\nRecently, the PBGA has provided results that outperform other neuroevolutionary algorithms in non-stationary problems, where the fitness function varies in time.\n\n", "id": "25056220", "title": "Promoter based genetic algorithm"}
{"url": "https://en.wikipedia.org/wiki?curid=6107563", "text": "Pulse-coupled networks\n\nPulse-coupled networks or pulse-coupled neural networks (PCNNs) are neural models proposed by modeling a cat’s visual cortex, and developed for high-performance biomimetic image processing.\n\nIn 1989, Eckhorn introduced a neural model to emulate the mechanism of cat’s visual cortex. The Eckhorn model provided a simple and effective tool for studying small mammal’s visual cortex, and was soon recognized as having significant application potential in image processing. \n\nIn 1994, Johnson adapted the Eckhorn model to an image processing algorithm, calling this algorithm a \"pulse-coupled neural network.\" Over the past decade, PCNNs have been used in a variety of image processing applications, including: image segmentation, feature generation, face extraction, motion detection, region growing, and noise reduction.\n\nThe basic property of the Eckhorn's linking-field model (LFM) is the coupling term. LFM is a modulation of the primary input by a biased offset factor driven by the linking input. These drive a threshold variable that decays from an initial high value. When the threshold drops below zero it is reset to a high value and the process starts over. This is different than the standard integrate-and-fire neural model, which accumulates the input until it passes an upper limit and effectively \"shorts out\" to cause the pulse.\n\nLFM uses this difference to sustain pulse bursts, something the standard model does not do on a single neuron level. It is valuable to understand, however, that a detailed analysis of the standard model must include a shunting term, due to the floating voltages level in the dendritic compartment(s), and in turn this causes an elegant multiple modulation effect that enables a true higher-order network (HON). Multidimensional pulse image processing of chemical structure data using PCNN has been discussed by Kinser, et al.\n\nA PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel’s color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be used for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.\n\nA simplified PCNN called a spiking cortical model was developed in 2009.\n\nPCNNs are useful for image processing, as discussed in a book by Thomas Lindblad and Jason M. Kinser.\n\nPCNN is proven success in many academic and industrial fields, such as image processing (image denoising, and image enhancement ), all pairs shortest path problem, and pattern recognition.\n", "id": "6107563", "title": "Pulse-coupled networks"}
{"url": "https://en.wikipedia.org/wiki?curid=3737445", "text": "Quantum neural network\n\nQuantum neural networks (QNNs) are neural network models which are based on the principles of quantum mechanics. There are two different approaches to QNN research, one exploiting quantum information processing to improve existing neural network models (sometimes also vice versa), and the other one searching for potential quantum effects in the brain.\n\nIn the computational approach to quantum neural network research, scientists try to combine artificial neural network models (which are widely used in machine learning for the important task of pattern classification) with the advantages of quantum information in order to develop more efficient algorithms (for a review, see ). One important motivation for these investigations is the difficulty to train classical neural networks, especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage, such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments.\n\nQuantum neural network research is still in its infancy, and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”), resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’.\n\nThe first ideas on quantum neural computation were published independently in 1995 by Ron Chrisley and\nSubhash Kak. Kak discussed the similarity of the neural activation function with the quantum mechanical Eigenvalue equation, and later discussed the application of these ideas to the study of brain function and the limitations of this approach. Ajit Narayanan and Tammy Menneer proposed a photonic implementation of a quantum neural network model that is based on the many-universe theory and “collapses” into the desired model upon measurement. Since then, more and more articles have been published in journals of computer science as well as quantum physics in order to find a superior quantum neural network model.\n\nA lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory, since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld, Sinayskiy and Petruccione based on the quantum phase estimation algorithm.\n\nA substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic.\n\nSome contributions reverse the approach and try to exploit the insights from neural network research in order to obtain powerful applications for quantum computing, such as quantum algorithmic design supported by machine learning. An example is the work of Elizabeth Behrman and Jim Steck, who propose a quantum computing setup that consists of a number of qubits with tunable mutual interactions. Following the classical backpropagation rule, the strength of the interactions are learned from a training set of desired input-output relations, and the quantum network thus ‘learns’ an algorithm.\n\nThe quantum associative memory algorithm has been introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory, but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition, and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. An advantage lies in the exponential storage capacity of memory states, however the question remains whether the model has significance regarding the initial purpose of Hopfield models as a demonstration of how simplified artificial neural networks can simulate features of the brain.\n\nRinkus proposes that distributed representation, specifically \"sparse distributed representation\" (SDR), provides a classical implementation of quantum computing. Specifically, the set of SDR codes stored in an SDR coding field will generally intersect with each other to varying degrees. In other work, Rinkus describes a fixed time learning (and inference) algorithm that preserves similarity in the input space into similarity (intersection size) in the SDR code space. Assuming that input similarity correlates with probability, this means that any single active SDR code is also a probability distribution over all stored inputs, with the probability of each input measured by the fraction of its SDR code that is active (i.e., the size of its intersection with the active SDR code). The learning/inference algorithm can also be viewed as a state update operator and because any single active SDR simultaneously represents both the probability of the single input, X, to which it was assigned during learning and the probabilities of all other stored inputs, the same physical process that updates the probability of X also updates all stored probabilities. By 'fixed time', it is meant that the number of computational steps comprising this process (the update algorithm) remains constant as the number of stored codes increases. This theory departs radically from the standard view of quantum computing and quantum physical theory more generally: rather than assuming that the states of the lowest level entities in the system, i.e., single binary neurons, exist in superposition, it assumes only that higher-level, i.e., composite entities, i.e., whole SDR codes (which are sets of binary neurons), exist in superposition.\n\nMost learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use a classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing. Recently a new post-learning strategy to allow the search for improved set of weights based on analogy with quantum effects occurring in nature. The technique, proposed in is based on the analogy of modeling a biological neuron as a semiconductor heterostructure consisting of one energetic barrier sandwiched between two energetically lower areas. The activation function of the neuron is therefore considered as a particle entering the heterostructure and interacting with the barrier. In this way auxiliary reinforcement to the classical learning process of neural networks is achieved with minimal additional computational costs.\n\nOne way of constructing a quantum neuron is to first generalise classical neurons (by padding of ancillary bits) to reversible permutation gates and then generalising them further to make unitary gates. Due to the no-cloning theorem in quantum mechanics, the copying of the output before sending it to several neurons in the next layer is non-trivial. This can be replaced with a general quantum unitary acting on the output plus a dummy bit in state . That has the classical copying gate (CNOT ) as a special case, and in that sense generalises the classical copying operation. It can be demonstrated that in this scheme, the quantum neural networks can: (i) compress quantum states onto a minimal number of qubits, creating a quantum autoencoder, and (ii) discover quantum communication protocols such as teleportation. The general recipe is theoretical and implementation-independent. The quantum neuron module can naturally be implemented photonically.\n\nAlthough many quantum neural network researchers explicitly limit their scope to a computational perspective, the field is closely connected to investigations of potential quantum effects in biological neural networks. Models of cognitive agents and memory based on quantum collectivees have been proposed by Subhash Kak, but he also points to specific problems of limits on observation and control of these memories due to fundamental logical reasons. He has also proposed that a quantum language must be associated with biological neural networks.\n\nThe combination of quantum physics and neuroscience also nourishes a vivid debate beyond the borders of science, an illustrative example being journals such as NeuroQuantology or the healing method of Quantum Neurology. However, also in the scientific sphere theories of how the brain might harvest the behavior of particles on a quantum level are controversially debated. The fusion of biology and quantum physics recently gained momentum by the discovery of signs for efficient energy transport in photosynthesis due to quantum effects. However, there is no widely accepted evidence for the ‘quantum brain’ yet.\n\n\n", "id": "3737445", "title": "Quantum neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=41575347", "text": "Quickprop\n\nQuickprop is an iterative method for determining the minimum of the loss function of an artificial neural network, following an algorithm inspired by the Newton's method. Sometimes, the algorithm is classified to the group of the second order learning methods. It follows a quadratic approximation of the previous gradient step and the current gradient, which is expected to be closed to the minimum of the loss function, under the assumption that the loss function is locally approximately square, trying to describe it by means of an upwardly open parabola. The minimum is sought in the vertex of the parabola. The procedure requires only local information of the artificial neuron to which it is applied.\nThe k-th approximation step is given by:\n\nformula_1\n\nBeing formula_2 the neuron j weight of its i input and E is the loss function.\n\nThe Quickprop algorithm is an implementation of the error backpropagation algorithm, but the network can behave chaotically during the learning phase due to large step sizes.\n\n", "id": "41575347", "title": "Quickprop"}
{"url": "https://en.wikipedia.org/wiki?curid=2310753", "text": "Radial basis function\n\nA radial basis function (RBF) is a real-valued function whose value depends only on the distance from the origin, so that formula_1; or alternatively on the distance from some other point \"c\", called a \"center\", so that formula_2. Any function formula_3 that satisfies the property formula_1 is a radial function. The norm is usually Euclidean distance, although other distance functions are also possible.\n\nSums of radial basis functions are typically used to approximate given functions. This approximation process can also be interpreted as a simple kind of neural network; this was the context in which they originally surfaced, in work by David Broomhead and David Lowe in 1988, which stemmed from Michael J. D. Powell's seminal research from 1977.\nRBFs are also used as a kernel in support vector classification.\n\nCommonly used types of radial basis functions include (writing formula_5):\n\nRadial basis functions are typically used to build up function approximations of the form\nwhere the approximating function \"y\"(x) is represented as a sum of \"N\" radial basis functions, each associated with a different center x, and weighted by an appropriate coefficient \"w\". The weights \"w\" can be estimated using the matrix methods of linear least squares, because the approximating function is \"linear\" in the weights.\n\nApproximation schemes of this kind have been particularly used in time series prediction and control of nonlinear systems exhibiting sufficiently simple chaotic behaviour, 3D reconstruction in computer graphics (for example, hierarchical RBF and Pose Space Deformation).\n\nThe sum \ncan also be interpreted as a rather simple single-layer type of artificial neural network called a radial basis function network, with the radial basis functions taking on the role of the activation functions of the network. It can be shown that any continuous function on a compact interval can in principle be interpolated with arbitrary accuracy by a sum of this form, if a sufficiently large number \"N\" of radial basis functions is used.\n\nThe approximant \"y\"(x) is differentiable with respect to the weights \"w\". The weights could thus be learned using any of the standard iterative methods for neural networks.\n\nUsing radial basis functions in this manner yields a reasonable interpolation approach provided that the fitting set has been chosen such that it covers the entire range systematically (equidistant data points are ideal). However, without a polynomial term that is orthogonal to the radial basis functions, estimates outside the fitting set tend to perform poorly.\n\n\n", "id": "2310753", "title": "Radial basis function"}
{"url": "https://en.wikipedia.org/wiki?curid=9651443", "text": "Radial basis function network\n\nIn the field of mathematical modeling, a radial basis function network is an artificial neural network that uses radial basis functions as activation functions. The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters. Radial basis function networks have many uses, including function approximation, time series prediction, classification, and system control. They were first formulated in a 1988 paper by Broomhead and Lowe, both researchers at the Royal Signals and Radar Establishment.\n\nRadial basis function (RBF) networks typically have three layers: an input layer, a hidden layer with a non-linear RBF activation function and a linear output layer. The input can be modeled as a vector of real numbers formula_1. The output of the network is then a scalar function of the input vector, formula_2, and is given by\n\nwhere formula_4 is the number of neurons in the hidden layer, formula_5 is the center vector for neuron formula_6, and formula_7 is the weight of neuron formula_6 in the linear output neuron. Functions that depend only on the distance from a center vector are radially symmetric about that vector, hence the name radial basis function. In the basic form all inputs are connected to each hidden neuron. The norm is typically taken to be the Euclidean distance (although the Mahalanobis distance appears to perform better in general) and the radial basis function is commonly taken to be Gaussian\n\nThe Gaussian basis functions are local to the center vector in the sense that\n\ni.e. changing parameters of one neuron has only a small effect for input values that are far away from the center of that neuron.\n\nGiven certain mild conditions on the shape of the activation function, RBF networks are universal approximators on a compact subset of formula_11. This means that an RBF network with enough hidden neurons can approximate any continuous function on a closed, bounded set with arbitrary precision.\n\nThe parameters formula_12, formula_13, and formula_14 are determined in a manner that optimizes the fit between formula_15 and the data.\n\nIn addition to the above \"unnormalized\" architecture, RBF networks can be \"normalized\". In this case the mapping is\n\nwhere\n\nis known as a \"normalized radial basis function\".\n\nThere is theoretical justification for this architecture in the case of stochastic data flow. Assume a stochastic kernel approximation for the joint probability density\n\nwhere the weights formula_19 and formula_20 are exemplars from the data and we require the kernels to be normalized\nand\n\nThe probability densities in the input and output spaces are\n\nand\n\nThe expectation of y given an input formula_24 is\n\nwhere\nis the conditional probability of y given formula_27.\nThe conditional probability is related to the joint probability through Bayes theorem\n\nwhich yields\n\nThis becomes\n\nwhen the integrations are performed.\n\nIt is sometimes convenient to expand the architecture to include local linear models. In that case the architectures become, to first order,\n\nand\n\nin the unnormalized and normalized cases, respectively. Here formula_33 are weights to be determined. Higher order linear terms are also possible.\n\nThis result can be written\n\nwhere\n\nand\n\nin the unnormalized case and\n\nin the normalized case.\n\nHere formula_38 is a Kronecker delta function defined as\n\nRBF networks are typically trained from pairs of input and target values formula_40, formula_41 by a two-step algorithm. In the first step, the center vectors formula_5 of the RBF functions in the hidden layer are chosen. This step can be performed in several ways; centers can be randomly sampled from some set of examples, or they can be determined using k-means clustering. Note that this step is unsupervised. A third backpropagation step can be performed to fine-tune all of the RBF net's parameters.\n\nThe second step simply fits a linear model with coefficients formula_43 to the hidden layer's outputs with respect to some objective function. A common objective function, at least for regression/function estimation, is the least squares function:\n\nwhere\nWe have explicitly included the dependence on the weights. Minimization of the least squares objective function by optimal choice of weights optimizes accuracy of fit.\n\nThere are occasions in which multiple objectives, such as smoothness as well as accuracy, must be optimized. In that case it is useful to optimize a regularized objective function such as\n\nwhere\n\nand\n\nwhere optimization of S maximizes smoothness and formula_49 is known as a regularization parameter.\n\nRBF networks can be used to interpolate a function formula_50 when the values of that function are known on finite number of points: formula_51. Taking the known points formula_52 to be the centers of the radial basis functions and evaluating the values of the basis functions at the same points formula_53 the weights can be solved from the equation\n\nIt can be shown that the interpolation matrix in the above equation is non-singular, if the points formula_52 are distinct, and thus the weights formula_56 can be solved by simple linear algebra:\n\nIf the purpose is not to perform strict interpolation but instead more general function approximation or classification the optimization is somewhat more complex because there is no obvious choice for the centers. The training is typically done in two phases first fixing the width and centers and then the weights. This can be justified by considering the different nature of the non-linear hidden neurons versus the linear output neuron.\n\nBasis function centers can be randomly sampled among the input instances or obtained by Orthogonal Least Square Learning Algorithm or found by clustering the samples and choosing the cluster means as the centers.\n\nThe RBF widths are usually all fixed to same value which is proportional to the maximum distance between the chosen centers.\n\nAfter the centers formula_58 have been fixed, the weights that minimize the error at the output are computed with a linear pseudoinverse solution:\nwhere the entries of \"G\" are the values of the radial basis functions evaluated at the points formula_60: formula_61.\n\nThe existence of this linear solution means that unlike multi-layer perceptron (MLP) networks, RBF networks have a unique local minimum (when the centers are fixed).\n\nAnother possible training algorithm is gradient descent. In gradient descent training, the weights are adjusted at each time step by moving them in a direction opposite from the gradient of the objective function (thus allowing the minimum of the objective function to be found),\n\nwhere formula_63 is a \"learning parameter.\"\n\nFor the case of training the linear weights, formula_64, the algorithm becomes\n\nin the unnormalized case and\n\nin the normalized case.\n\nFor local-linear-architectures gradient-descent training is\n\nFor the case of training the linear weights, formula_64 and formula_69, the algorithm becomes\n\nin the unnormalized case and\n\nin the normalized case and\n\nin the local-linear case.\n\nFor one basis function, projection operator training reduces to Newton's method.\n\nThe basic properties of radial basis functions can be illustrated with a simple mathematical map, the logistic map, which maps the unit interval onto itself. It can be used to generate a convenient prototype data stream. The logistic map can be used to explore function approximation, time series prediction, and control theory. The map originated from the field of population dynamics and became the prototype for chaotic time series. The map, in the fully chaotic regime, is given by\n\nwhere t is a time index. The value of x at time t+1 is a parabolic function of x at time t. This equation represents the underlying geometry of the chaotic time series generated by the logistic map.\n\nGeneration of the time series from this equation is the forward problem. The examples here illustrate the inverse problem; identification of the underlying dynamics, or fundamental equation, of the logistic map from exemplars of the time series. The goal is to find an estimate\n\nfor f.\n\nThe architecture is\n\nwhere\n\nSince the input is a scalar rather than a vector, the input dimension is one. We choose the number of basis functions as N=5 and the size of the training set to be 100 exemplars generated by the chaotic time series. The weight formula_77 is taken to be a constant equal to 5. The weights formula_78 are five exemplars from the time series. The weights formula_12 are trained with projection operator training:\n\nwhere the learning rate formula_81 is taken to be 0.3. The training is performed with one pass through the 100 training points. The rms error is 0.15.\n\nThe normalized RBF architecture is\n\nwhere\n\nAgain:\n\nAgain, we choose the number of basis functions as five and the size of the training set to be 100 exemplars generated by the chaotic time series. The weight formula_77 is taken to be a constant equal to 6. The weights formula_78 are five exemplars from the time series. The weights formula_12 are trained with projection operator training:\n\nwhere the learning rate formula_81 is again taken to be 0.3. The training is performed with one pass through the 100 training points. The rms error on a test set of 100 exemplars is 0.084, smaller than the unnormalized error. Normalization yields accuracy improvement. Typically accuracy with normalized basis functions increases even more over unnormalized functions as input dimensionality increases.\n\nOnce the underlying geometry of the time series is estimated as in the previous examples, a prediction for the time series can be made by iteration:\n\nA comparison of the actual and estimated time series is displayed in the figure. The estimated times series starts out at time zero with an exact knowledge of x(0). It then uses the estimate of the dynamics to update the time series estimate for several time steps.\n\nNote that the estimate is accurate for only a few time steps. This is a general characteristic of chaotic time series. This is a property of the sensitive dependence on initial conditions common to chaotic time series. A small initial error is amplified with time. A measure of the divergence of time series with nearly identical initial conditions is known as the Lyapunov exponent.\n\nWe assume the output of the logistic map can be manipulated through a control parameter formula_93 such that\n\nThe goal is to choose the control parameter in such a way as to drive the time series to a desired output formula_95. This can be done if we choose the control paramer to be\n\nwhere\n\nis an approximation to the underlying natural dynamics of the system.\n\nThe learning algorithm is given by\n\nwhere\n\n\n", "id": "9651443", "title": "Radial basis function network"}
{"url": "https://en.wikipedia.org/wiki?curid=8278198", "text": "Random neural network\n\nThe random neural network (RNN) is a mathematical representation of an interconnected network of neurons or cells which exchange spiking signals. It was invented by Erol Gelenbe and is linked to the G-network model of queueing networks as well as to Gene Regulatory Network models. Each cell state is represented by an integer whose value rises when the cell receives an excitatory spike and drops when it receives an inhibitory spike. The spikes can originate outside the network itself, or they can come from other cells in the networks. Cells whose internal excitatory state has a positive value are allowed to send out spikes of either kind to other cells in the network according to specific cell-dependent spiking rates. The model has a mathematical solution in steady-state which provides the joint probability distribution of the network in terms of the individual probabilities that each cell is excited and able to send out spikes. Computing this solution is based on solving a set of non-linear algebraic equations whose parameters are related to the spiking rates of individual cells and their connectivity to other cells, as well as the arrival rates of spikes from outside the network. The RNN is a recurrent model, i.e. a neural network that is allowed to have complex feedback loops.\n\nA highly energy-efficient implementation of random neural networks was demonstrated by Krishna Palem et al. using the Probabilistic CMOS or PCMOS technology and was shown to be c. 226–300 times more efficient in terms of Energy-Performance-Product.\n\nRNNs are also related to artificial neural networks, which (like the random neural network) have gradient-based learning algorithms. The learning algorithm for an n-node random neural network that includes feedback loops (it is also a recurrent neural network) is of computational complexity O(n^3) (the number of computations is proportional to the cube of n, the number of neurons). The random neural network can also be used with other learning algorithms such as reinforcement learning. The RNN has been shown to be a universal approximator for bounded and continuous functions.\n\n\n\n", "id": "8278198", "title": "Random neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=37862937", "text": "Rectifier (neural networks)\n\nIn the context of artificial neural networks, the rectifier is an activation function defined as the positive part of its argument:\n\nformula_1,\n\nwhere \"x\" is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. \nThis activation function was first introduced to a dynamical network by Hahnloser et al. in a 2000 paper in Nature with strong biological motivations and mathematical justifications.. It has been demonstrated for the first time in 2011 to enable better training of deeper networks , compared to the widely used activation functions prior to 2011, i.e., the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart, the hyperbolic tangent. The rectifier is, , the most popular activation function for deep neural networks.\n\nA unit employing the rectifier is also called a rectified linear unit (ReLU).\n\nA smooth approximation to the rectifier is the analytic function\nwhich is called the softplus function. The derivative of softplus is formula_3, i.e. the logistic function.\n\nRectified linear units find applications in computer vision and speech recognition using deep neural nets.\n\nRectified linear units can be extended to include Gaussian noise, making them noisy ReLUs, giving\n\nNoisy ReLUs have been used with some success in restricted Boltzmann machines for computer vision tasks.\n\nLeaky ReLUs allow a small, non-zero gradient when the unit is not active.\n\nParametric ReLUs take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural network parameters.\n\nNote that for formula_8, this is equivalent to\nand thus has a relation to \"maxout\" networks.\n\nExponential linear units try to make the mean activations closer to zero which speeds up learning. It has been shown that ELUs can obtain higher classification accuracy than ReLUs.\n\nformula_10\n\nformula_11 is a hyper-parameter to be tuned and formula_12 is a constraint.\n\n\nRectifying activation functions were used to separate specific excitation and unspecific inhibition in the Neural Abstraction Pyramid, which was trained in a supervised way to learn several computer vision tasks.\nIn 2011, the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training.\nRectified linear units, compared to sigmoid function or similar activation functions, allow for faster and effective training of deep neural architectures on large and complex datasets.\n\n\n", "id": "37862937", "title": "Rectifier (neural networks)"}
{"url": "https://en.wikipedia.org/wiki?curid=10667750", "text": "Reservoir computing\n\nReservoir computing is a framework for computation that may be viewed as an extension of neural networks. Typically an input signal is fed into a fixed (random) dynamical system called a \"reservoir\" and the dynamics of the reservoir map the input to a higher dimension. Then a simple \"readout\" mechanism is trained to read the state of the reservoir and map it to the desired output. The main benefit is that training is performed only at the readout stage and the reservoir is fixed. Liquid-state machines and echo state networks are two major types of reservoir computing.\n\nThe reservoir consists of a collection of recurrently connected units. The connectivity structure is usually \"random\", and the units are usually non-linear. The overall dynamics of the reservoir are driven by the input, and also affected by the past. A rich collection of dynamical input-output mapping is a crucial advantage over time delay neural networks.\n\nThe readout is carried out using a linear transformation of the reservoir output. This transformation is adapted to the task of interest by using a linear regression or a Ridge regression using a teaching signal.\n\nAn early example of reservoir computing was the context reverberation network.\nIn this architecture, an input layer feeds into a high dimensional dynamical system which is read out by a trainable single-layer perceptron. Two kinds of dynamical system were described: a recurrent neural network with fixed random weights, and a continuous reaction-diffusion system inspired by Alan Turing’s model of morphogenesis. At the trainable layer, the perceptron associates current inputs with the signals that reverberate in the dynamical system; the latter were said to provide a dynamic \"context\" for the inputs. In the language of later work, the reaction-diffusion system served as the reservoir.\n\nBackpropagation-Decorrelation (BPDC)\n\nThe Tree Echo State Network (TreeESN) model represents a generalization of the reservoir computing framework to tree structured data.\n\nThe extension of the reservoir computing framework towards Deep Learning, with the introduction of deep reservoir computing and of the deep Echo State Network (deepESN) model allows to develop efficiently trained models for hierarchical processing of temporal data, at the same time enabling the investigation on the inherent role of layered composition in recurrent neural networks.\n\nIn late 2017, a research team from the University of Michigan implemented the reservoir computing principles in a chip and demonstrated its performance in a speech prediction task.\n\n\n", "id": "10667750", "title": "Reservoir computing"}
{"url": "https://en.wikipedia.org/wiki?curid=7950358", "text": "Rprop\n\nRprop, short for resilient backpropagation, is a learning heuristic for supervised learning in feedforward artificial neural networks. This is a first-order optimization algorithm. This algorithm was created by Martin Riedmiller and Heinrich Braun in 1992.\n\nSimilarly to the Manhattan update rule, Rprop takes into account only the sign of the partial derivative over all patterns (not the magnitude), and acts independently on each \"weight\". For each weight, if there was a sign change of the partial derivative of the total error function compared to the last iteration, the update value for that weight is multiplied by a factor \"η\", where \"η\" < 1. If the last iteration produced the same sign, the update value is multiplied by a factor of \"η\", where \"η\" > 1. The update values are calculated for each weight in the above manner, and finally each weight is changed by its own update value, in the opposite direction of that weight's partial derivative, so as to minimise the total error function. \"η\" is empirically set to 1.2 and \"η\" to 0.5.\n\nNext to the cascade correlation algorithm and the Levenberg–Marquardt algorithm, Rprop is one of the fastest weight update mechanisms.\n\nRPROP is a batch update algorithm.\n\nMartin Riedmiller developed three algorithms, all named RPROP. Igel and Hüsken assigned names to them and added a new variant:\n\n\n", "id": "7950358", "title": "Rprop"}
{"url": "https://en.wikipedia.org/wiki?curid=3710117", "text": "Semantic neural network\n\nSemantic neural network (SNN) is based on John von Neumann's neural network <nowiki>[</nowiki>von Neumann, 1966<nowiki>]</nowiki> and Nikolai Amosov M-Network. There are limitations to a link topology for the von Neumann’s network but SNN accept a case without these limitations. Only logical values can be processed, but SNN accept that fuzzy values can be processed too. All neurons into the von Neumann network are synchronized by tacts. For further use of self-synchronizing circuit technique SNN accepts neurons can be self-running or synchronized.\n\nIn contrast to the von Neumann network there are no limitations for topology of neurons for semantic networks. It leads to the impossibility of relative addressing of neurons as it was done by von Neumann. In this case an absolute readdressing should be used. Every neuron should have a unique identifier that would provide a direct access to another neuron. Of course, neurons interacting by axons-dendrites should have each other's identifiers. An absolute readdressing can be modulated by using neuron specificity as it was realized for biological neural networks.\n\nThere’s no description for self-reflectiveness and self-modification abilities into the initial description of semantic networks [Dudar Z.V., Shuklin D.E., 2000]. But in [Shuklin D.E. 2004] a conclusion had been drawn about the necessity of introspection and self-modification abilities in the system. For maintenance of these abilities a concept of pointer to neuron is provided. Pointers represent virtual connections between neurons. In this model, bodies and signals transferring through the neurons connections represent a physical body, and virtual connections between neurons are representing an astral body. It is proposed to create models of artificial neuron networks on the basis of virtual machine supporting the opportunity for paranormal effects.\n\nSNN is generally used for natural language processing.\n\n\n\n", "id": "3710117", "title": "Semantic neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=87210", "text": "Sigmoid function\n\nA sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. Often, \"sigmoid function\" refers to the special case of the logistic function shown in the first figure and defined by the formula\nOther examples of similar shapes include the Gompertz curve (used in modeling systems that saturate at large values of x) and the ogee curve (used in the spillway of some dams). Sigmoid functions have domain of all real numbers, with return value monotonically increasing most often from 0 to 1 or alternatively from −1 to 1, depending on convention.\n\nA wide variety of sigmoid functions have been used as the activation function of artificial neurons, including the logistic and hyperbolic tangent functions. Sigmoid curves are also common in statistics as cumulative distribution functions (which go from 0 to 1), such as the integrals of the logistic distribution, the normal distribution, and Student's \"t\" probability density functions.\n\nA sigmoid function is a bounded differentiable real function that is defined for all real input values and has a non-negative derivative at each point.\n\nIn general, a sigmoid function is real-valued, monotonic, and differentiable having a non-negative first derivative which is bell shaped. A sigmoid function is constrained by a pair of horizontal asymptotes as formula_2.\n\n\n\n\n\n\n\n\n\nThe integral of any continuous, non-negative, \"bump-shaped\" function will be sigmoidal, thus the cumulative distribution functions for many common probability distributions are sigmoidal. One such example is the error function, which is related to the cumulative distribution function (CDF) of a normal distribution.\n\nMany natural processes, such as those of complex system learning curves, exhibit a progression from small beginnings that accelerates and approaches a climax over time. When a specific mathematical model is lacking, a sigmoid function is often used.\n\nThe van Genuchten-Gupta model is based on an inverted S-curve and applied to the response of crop yield to soil salinity.\n\nExamples of the application of the logistic S-curve to the response of crop yield (barley) to both the soil salinity and depth to watertable in the soil are shown in .\n\n\n", "id": "87210", "title": "Sigmoid function"}
{"url": "https://en.wikipedia.org/wiki?curid=6152185", "text": "Softmax function\n\nIn mathematics, the softmax function, or normalized exponential function, is a generalization of the logistic function that \"squashes\" a -dimensional vector formula_1 of arbitrary real values to a -dimensional vector formula_2 of real values in the range [0, 1] that add up to 1. The function is given by\n\nIn probability theory, the output of the softmax function can be used to represent a categorical distribution – that is, a probability distribution over different possible outcomes. In fact, it is the gradient-log-normalizer of the categorical probability distribution.\n\nThe softmax function is used in various multiclass classification methods, such as multinomial logistic regression (also known as softmax regression) , multiclass linear discriminant analysis, naive Bayes classifiers, and artificial neural networks. Specifically, in multinomial logistic regression and linear discriminant analysis, the input to the function is the result of distinct linear functions, and the predicted probability for the 'th class given a sample vector and a weighting vector is:\n\nThis can be seen as the composition of linear functions formula_6 and the softmax function (where formula_7 denotes the inner product of formula_8 and formula_9). The operation is equivalent to applying a linear operator defined by formula_9 to vectors formula_8, thus transforming the original, probably highly-dimensional, input to vectors in a -dimensional space formula_12.\n\nIf we take an input of [1, 2, 3, 4, 1, 2, 3], the softmax of that is [0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]. The output has most of its weight where the '4' was in the original input. This is what the function is normally used for: to highlight the largest values and suppress values which are significantly below the maximum value.\n\nComputation of this example using simple Python code:\n\n»> import math\n»> z = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n»> z_exp = [math.exp(i) for i in z]\n»> print([round(i, 2) for i in z_exp])\n[2.72, 7.39, 20.09, 54.6, 2.72, 7.39, 20.09]\n»> sum_z_exp = sum(z_exp)\n»> print(round(sum_z_exp, 2))\n114.98\n»> softmax = [round(i / sum_z_exp, 3) for i in z_exp]\n»> print(softmax)\n[0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]\nHere is an example of Julia code:\n\njulia> A = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n7-element Array{Float64,1}:\n\njulia> exp.(A) ./ sum(exp.(A))\n7-element Array{Float64,1}:\nThe softmax function is often used in the final layer of a neural network-based classifier. Such networks are commonly trained under a log loss (or cross-entropy) regime, giving a non-linear variant of multinomial logistic regression.\n\nSince the function maps a vector and a specific index \"i\" to a real value, the derivative needs to take the index into account:\n\nHere, the Kronecker delta is used for simplicity (cf. the derivative of a sigmoid function, being expressed via the function itself).\n\nSee Multinomial logit for a probability model which uses the softmax activation function.\n\nIn the field of reinforcement learning, a softmax function can be used to convert values into action probabilities. The function commonly used is:\n\nwhere the action value formula_15 corresponds to the expected reward of following action a and formula_16 is called a temperature parameter (in allusion to statistical mechanics). For high temperatures (formula_17), all actions have nearly the same probability and the lower the temperature, the more expected rewards affect the probability. For a low temperature (formula_18), the probability of the action with the highest expected reward tends to 1.\n\nSigmoidal or Softmax normalization is a way of reducing the influence of extreme values or outliers in the data without removing them from the dataset. It is useful given outlier data, which we wish to include in the dataset while still preserving the significance of data within a standard deviation of the mean. The data are nonlinearly transformed using one of the sigmoidal functions. \n\nThe logistic sigmoid function:\n\nThe hyperbolic tangent function, tanh:\n\nThe sigmoid function limits the range of the normalized data to values between 0 and 1. The sigmoid function is almost linear near the mean and has smooth nonlinearity at both extremes, ensuring that all data points are within a limited range. This maintains the resolution of most values within a standard deviation of the mean.\n\nThe hyperbolic tangent function, tanh, limits the range of the normalized data to values between −1 and 1. The hyperbolic tangent function is almost linear near the mean, but has a slope of half that of the sigmoid function. Like sigmoid, it has smooth, monotonic nonlinearity at both extremes. Also, like the sigmoid function, it remains differentiable everywhere and the sign of the derivative (slope) is unaffected by the normalization. This ensures that optimization and numerical integration algorithms can continue to rely on the derivative to estimate changes to the output (normalized value) that will be produced by changes to the input in the region near any linearisation point.\n\nThe softmax function also happens to be the probability of an atom being found in a quantum state of energy formula_21 when the atom is part of an ensemble that has reached thermal equilibrium at temperature formula_22. This is known as the Boltzmann distribution. The expected relative occupancy of each state is formula_23, and this is normalised so that the sum over energy levels sums to 1. In this analogy, the input to the softmax function is the negative energy of each quantum state divided by formula_24.\n\n", "id": "6152185", "title": "Softmax function"}
{"url": "https://en.wikipedia.org/wiki?curid=10159567", "text": "Spiking neural network\n\nSpiking neural networks (SNNs) fall into the third generation of neural network models, increasing the level of realism in a neural simulation.\nIn addition to neuronal and synaptic state, SNNs also incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not fire at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather fire only when a membrane potential – an intrinsic quality of the neuron related to its membrane electrical charge – reaches a specific value. When a neuron fires, it generates a signal which travels to other neurons which, in turn, increase or decrease their potentials in accordance with this signal.\n\nIn the context of spiking neural networks, the current activation level (modeled as some differential equation) is normally considered to be the neuron's state, with incoming spikes pushing this value higher, and then either firing or decaying over time. Various \"coding methods\" exist for interpreting the outgoing \"spike train\" as a real-value number, either relying on the frequency of spikes, or the timing between spikes, to encode information.\n\nThe first scientific model of a spiking neuron was proposed by Alan Lloyd Hodgkin and Andrew Huxley in 1952. This model describes how action potentials are initiated and propagated. Spikes, however, are not generally transmitted directly between neurons. Communication requires the exchange of chemical substances in the synaptic gap, called neurotransmitters. The complexity and variability of biological models have resulted in various neuron models, such as the integrate-and-fire (1907), FitzHugh–Nagumo model (1961–1962) and Hindmarsh–Rose model (1984).\n\nFrom the information theory point of view, the problem is to propose a model that explains how information is encoded and decoded by a series of trains of pulses, i.e. action potentials. Thus, one of the fundamental questions of neuroscience is to determine if neurons communicate by a rate or temporal code. Temporal coding suggests that a single spiking neuron \"can replace hundreds of hidden units on a sigmoidal neural net.\"\n\nThis kind of neural network can in principle be used for information processing applications the same way as traditional artificial neural networks. In addition, spiking neural networks can model the central nervous system of a virtual insect for seeking food without the prior knowledge of the environment. However, due to their more realistic properties, they can also be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function, the electrophysiological recordings of this circuit can be compared to the output of the corresponding spiking artificial neural network simulated on computer, determining the plausibility of the starting hypothesis.\n\nIn practice, there is a major difference between the theoretical power of spiking neural networks and what has been demonstrated. They have proved useful in neuroscience, but not (yet) in engineering. Some large scale neural network models have been designed that take advantage of the pulse coding found in spiking neural networks, these networks mostly rely on the principles of reservoir computing. However, the real world application of large scale spiking neural networks has been limited because the increased computational costs associated with simulating realistic neural models have not been justified by commensurate benefits in computational power. As a result, there has been little application of large scale spiking neural networks to solve computational tasks of the order and complexity that are commonly addressed using rate coded (second generation) neural networks. In addition it can be difficult to adapt second generation neural network models into real time, spiking neural networks (especially if these network algorithms are defined in discrete time). It is relatively easy to construct a spiking neural network model and observe its dynamics. It is much harder to develop a model with stable behavior that computes a specific function.\n\nThere is diverse range of application software to simulate spiking neural networks. This software can be classified according to the use of the simulation:\n\nNeurogrid, built at Stanford University, is a board that can simulate spiking neural networks directly in hardware. SpiNNaker (Spiking Neural Network Architecture), designed at the University of Manchester, uses ARM processors as the building blocks of a massively parallel computing platform based on a six-layer thalamocortical model.\n\nAnother implementation is the TrueNorth processor from IBM. This processor contains 5.4 billion transistors, but is designed to consume very little power, only 70 milliwatts; most processors in personal computers contain about 1.4 billion transistors and require 35 watts or more. IBM refers to the design principle behind TrueNorth as neuromorphic computing. Its primary purpose is pattern recognition; while critics say the chip isn't powerful enough, its supporters point out that this is only the first generation, and the capabilities of improved iterations will become clear.\n\nAnother hardware platform aimed at providing reconfigurable, general-purpose, real-time neural networks of spiking neurons is the Dynamic Neuromorphic Asynchronous Processor (DYNAP). DYNAP uses a unique combination of slow, low-power, inhomogeneous sub-thresholds analog circuits, and fast programmable digital circuits. This allow the implementation of real-time spike-based neural processing architectures in which memory and computation are co-localized, solving the von Neumann bottleneck problem and enabling real-time massively multiplexed communication of spiking events for realising massive networks. Recurrent networks, feed-forward networks, convolutional networks, attractor networks, echo-state networks, deep networks, and sensory fusion networks are few of the possibilities.\n\n\n", "id": "10159567", "title": "Spiking neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=37652061", "text": "Stochastic neural analog reinforcement calculator\n\nSNARC (Stochastic Neural Analog Reinforcement Calculator) is a neural net machine designed by Marvin Lee Minsky. George Miller gathered the funding for the project from the Air Force Office of Scientific Research in the summer of 1951. At the time, one of Minsky's graduate students at Princeton, Dean Edmonds, volunteered that he was good with electronics and therefore Minsky brought him onto the project.\n\nThe machine itself is a randomly connected network of approximately 40 Hebb synapses. These synapses each have a memory that holds the probability that signal comes in one input and another signal will come out of the output. There is a probability knob that goes from 0 to 1 that shows this probability of the signals propagating. If the probability signal gets through, a capacitor remembers this function and engages a \"clutch\". At this point, the operator will press a button to give reward to the machine. At this point, a large motor starts and there is a chain that goes to all 40 synapse machines, checking if the clutch is engaged or not. As the capacitor can only \"remember\" for a certain amount of time, the chain only catches the most recent updates of the probabilities.\n\nThis machine is considered one of the first pioneering attempts at the field of artificial intelligence. Minsky went on to be a founding member of MIT's Project MAC, which split to become the MIT Laboratory for Computer Science and the MIT Artificial Intelligence Lab, and is now the MIT Computer Science and Artificial Intelligence Laboratory. In 1985 Minsky became a founding member of the MIT Media Laboratory. \n\n", "id": "37652061", "title": "Stochastic neural analog reinforcement calculator"}
{"url": "https://en.wikipedia.org/wiki?curid=2070605", "text": "Stochastic neural network\n\nStochastic neural networks are a type of artificial neural networks built by introducing random variations into the network, either by giving the network's neurons stochastic transfer functions, or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help it escape from local minima.\n\nAn example of a neural network using stochastic transfer functions is a Boltzmann machine. Each neuron is binary valued, and the chance of it firing depends on the other neurons in the network.\n\nStochastic neural networks have found applications in risk management, oncology, bioinformatics, and other similar fields.\n", "id": "2070605", "title": "Stochastic neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=41027689", "text": "Synaptic transistor\n\nA synaptic transistor is an electrical device that can learn in ways similar to a neural synapse. It optimizes its own properties for the functions it has carried out in the past. The device mimics the behavior of the property of neurons called spike-timing-dependent plasticity, or STDP.\n\nIts structure is similar to that of a field effect transistor, where an ionic liquid takes the place of the gate insulating layer between the gate electrode and the conducting channel. That channel is composed of samarium nickelate (, or SNO) rather than the field effect transistor's doped silicon.\n\nA synaptic transistor has a traditional immediate response whose amount of current that passes between the source and drain contacts varies with voltage applied to the gate electrode. It also produces a much slower learned response such that the conductivity of the SNO layer varies in response to the transistor's STDP history, essentially by shuttling oxygen ions between the SNO and the ionic liquid.\n\nThe analog of strengthening a synapse is to increase the SNO's conductivity, which essentially increases gain. Similarly, weakening a synapse is analogous to decreasing the SNO's conductivity, lowering the gain.\n\nThe input and output of the synaptic transistor are continuous analog values, rather than digital on-off signals. While the physical structure of the device has the potential to learn from history, it contains no way to bias the transistor to control the memory effect. An external supervisory circuit converts the time delay between input and output into a voltage applied to the ionic liquid that either drives ions into the SNO or removes them.\n\nA network of such devices can learn particular responses to \"sensory inputs\", with those responses being learned through experience rather than explicitly programmed.\n", "id": "41027689", "title": "Synaptic transistor"}
{"url": "https://en.wikipedia.org/wiki?curid=14405160", "text": "Synaptic weight\n\nIn neuroscience and computer science, synaptic weight refers to the strength or amplitude of a connection between two nodes, corresponding in biology to the amount of influence the firing of one neuron has on another. The term is typically used in artificial and biological neural network research.\n\nIn a computational neural network, a vector or set of inputs formula_1 and outputs formula_2, or pre- and post-synaptic neurons respectively, are interconnected with synaptic weights represented by the matrix formula_3, where for a linear neuron\n\nThe synaptic weight is changed by using a learning rule, the most basic of which is Hebb's rule, which is usually stated in biological terms as \n\n\"Neurons that fire together, wire together.\"\n\nComputationally, this means that if a large signal from one of the input neurons results in a large signal from one of the output neurons, then the synaptic weight between those two neurons will increase. The rule is unstable, however, and is typically modified using such variations as Oja's rule, radial basis functions or the backpropagation algorithm.\n\nFor biological networks, the effect of synaptic weights is not as simple as for linear neurons or Hebbian learning. However, biophysical models such as BCM theory have seen some success in mathematically describing these networks.\n\nIn the mammalian central nervous system, signal transmission is carried out by interconnected networks of nerve cells, or neurons. For the basic pyramidal neuron, the input signal is carried by the axon, which releases neurotransmitter chemicals into the synapse which is picked up by the dendrites of the next neuron, which can then generate an action potential which is analogous to the output signal in the computational case.\n\nThe synaptic weight in this process is determined by several variable factors:\n\nThe changes in synaptic weight that occur is known as synaptic plasticity, and the process behind long-term changes (long-term potentiation and depression) is still poorly understood. Hebb's original learning rule was originally applied to biological systems, but has had to undergo many modifications as a number of theoretical and experimental problems came to light.\n\n", "id": "14405160", "title": "Synaptic weight"}
{"url": "https://en.wikipedia.org/wiki?curid=5099023", "text": "Tensor product network\n\nA tensor product network, in artificial neural networks, is a network that exploits the properties of tensors to model associative concepts such as variable assignment. Orthonormal vectors are chosen to model the ideas (such as variable names and target assignments), and the tensor product of these vectors construct a network whose mathematical properties allow the user to easily extract the association from it.\n\n", "id": "5099023", "title": "Tensor product network"}
{"url": "https://en.wikipedia.org/wiki?curid=23594537", "text": "Time delay neural network\n\nTime delay neural network (TDNN) is an artificial neural network architecture whose primary purpose is to classify patterns shift-invariantly, i.e., requiring no explicit prior determination of the beginning and end point of the pattern. The TDNN was first proposed to classify phonemes in speech signals for automatic speech recognition, where the automatic determination of precise segments or feature boundaries is difficult or impossible. The TDNN recognizes phonemes and their underlying acoustic/phonetic features, independent of time-shifts, i.e. position in time.\n\nAn input signal is augmented with delayed copies as other inputs, the neural network is time-shift invariant since it has no internal state.\n\nThe original paper presented a perceptron network whose connection weights were trained with the back-propagation algorithm, this may be done in batch or online. The Stuttgart Neural Network Simulator implements that version.\n\nThe Time Delay Neural Network, like other neural networks, operates with multiple interconnected layers composed of clusters. These clusters are meant to represent neurons in a brain and, like the brain, each cluster need only focus on small regions of the input. A proto-typical TDNN has three layers of clusters, one for input, one for output, and the middle layer which handles manipulation of the input through filters. Due to their sequential nature, TDNN’s are implemented as a feedforward neural network instead of a recurrent neural network.\n\nIn order to achieve time-shift invariance, a set of delays are added to the input (audio file, image, etc.) so that the data is represented at different points in time. These delays are arbitrary and application specific, which generally means the input data is customized for a specific delay pattern. There has been work done in creating an adaptable time-delay TDNN. where this manual tuning is eradicated. The delays are an attempt to add a temporal dimension to the network which is not present in Recurrent Neural Networks or Multi-Layer Perceptrons with a sliding window. The combination of past inputs with present inputs make the TDNN’s approach unique.\n\nA key feature for TDNN’s are the ability to express a relation between inputs in time. This relation can be the result of a feature detector and is used within the TDNN to recognize patterns between the delayed inputs.\nOne of the main advantages of neural networks is the lack of a dependence on prior knowledge to set up the banks of filters at each layer. However, this entails that the network must learn the optimal value for these filters through processing numerous training inputs. Supervised learning is generally the learning algorithm associated with TDNN’s due to its strength in pattern recognition and function approximation. Supervised learning is commonly implemented with a back propagation algorithm.\n\nSpeech Recognition\n\nTDNN’s used to solve problems in speech recognition that were introduced in 1989 and initially focused on phoneme detection. Speech lends itself nicely to TDNN’s as spoken sounds are rarely of uniform length. By examining a sound shifted in the past and future, the TDNN is able to construct a model for that sound that is time-invariant. This is especially helpful in speech recognition as different dialects and languages pronounce the same sounds with different lengths. Spectral coefficients are used to describe the relation between the input samples.\n\nVideo Analysis\n\nVideo has a temporal dimension which makes a TDNN an ideal solution to analyzing motion patterns. An example of this analysis is a combination of vehicle detection and recognizing pedestrians. When examining videos, subsequent images are fed into the TDNN as input where each image is the next frame in the video. The strength of the TDNN comes from its ability to examine objects shifted in time forward and backward to define an object detectable as the time is altered. If an object can be recognized in this manner, an application can plan on that object to be found in the future and perform an optimal action.\n\nCommon Libraries\n\n\n", "id": "23594537", "title": "Time delay neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=28071238", "text": "U-matrix\n\nThe U-matrix (unified distance matrix) is a representation of a self-organizing map (SOM) where the Euclidean distance between the codebook vectors of neighboring neurons is depicted in a grayscale image. This image is used to visualize the data in a high-dimensional space using a 2D image.\n\nOnce the SOM is trained using the input data, the final map is not expected to have any twists. If the map is twist-free, the distance between the codebook vectors of neighboring neurons gives an approximation of the distance between different parts of the underlying data. When such distances are depicted in a grayscale image, light colors depict closely spaced node codebook vectors and darker colors indicate more widely separated node codebook vectors. Thus, groups of light colors can be considered as clusters, and the dark parts as the boundaries between the clusters. This representation can help to visualize the clusters in the high-dimensional spaces, or to automatically recognize them using relatively simple image processing techniques.\n", "id": "28071238", "title": "U-matrix"}
{"url": "https://en.wikipedia.org/wiki?curid=18543448", "text": "Universal approximation theorem\n\nIn the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons (i.e., a multilayer perceptron), can approximate continuous functions on compact subsets of R, under mild assumptions on the activation function. The theorem thus states that simple neural networks can \"represent\" a wide variety of interesting functions when given appropriate parameters; however, it does not touch upon the algorithmic learnability of those parameters.\n\nOne of the first versions of the theorem was proved by George Cybenko in 1989 for sigmoid activation functions.\n\nKurt Hornik showed in 1991 that it is not the specific choice of the activation function, but rather the multilayer feedforward architecture itself which gives neural networks the potential of being universal approximators. The output units are always assumed to be linear. For notational convenience, only the single output case will be shown. The general case can easily be deduced from the single output case.\n\nThe theorem in mathematical terms:\n\nLet formula_1 be a nonconstant, bounded, and monotonically-increasing continuous function. Let formula_2 denote the \"m\"-dimensional unit hypercube formula_3. The space of continuous functions on formula_2 is denoted by formula_5. Then, given any formula_6 and any function formula_7, there exist an integer formula_8, real constants formula_9 and real vectors formula_10, where formula_11, such that we may define:\n\nas an approximate realization of the function formula_13 where formula_13 is independent of formula_15; that is,\n\nfor all formula_17. In other words, functions of the form formula_18 are dense in formula_5.\nThis still holds when replacing formula_2 with any compact subset of formula_21.\n\n\n", "id": "18543448", "title": "Universal approximation theorem"}
{"url": "https://en.wikipedia.org/wiki?curid=1853175", "text": "Winner-take-all (computing)\n\nWinner-take-all is a computational principle applied in computational models of neural networks by which neurons in a layer compete with each other for activation. In the classical form, only the neuron with the highest activation stays active while all other neurons shut down; however, other variations allow more than one neuron to be active, for example the soft winner take-all, by which a power function is applied to the neurons.\n\nIn the theory of artificial neural networks, winner-take-all networks are a case of competitive learning in recurrent neural networks. Output nodes in the network mutually inhibit each other, while simultaneously activating themselves through reflexive connections. After some time, only one node in the output layer will be active, namely the one corresponding to the strongest input. Thus the network uses nonlinear inhibition to pick out the largest of a set of inputs. Winner-take-all is a general computational primitive that can be implemented using different types of neural network models, including both continuous-time and spiking networks (Grossberg, 1973; Oster et al. 2009).\n\nWinner-take-all networks are commonly used in computational models of the brain, particularly for distributed decision-making or action selection in the cortex. Important examples include hierarchical models of vision (Riesenhuber et al. 1999), and models of selective attention and recognition (Carpenter and Grossberg, 1987; Itti et al. 1998). They are also common in artificial neural networks and neuromorphic analog VLSI circuits. It has been formally proven that the winner-take-all operation is computationally powerful compared to other nonlinear operations, such as thresholding (Maass 2000).\n\nIn many practical cases, there is not only a single neuron which becomes the only active one but there are exactly \"k\" neurons which become active for a fixed number \"k\". This principle is referred to as k-winners-take-all.\n\nA simple, but popular CMOS winner-take-all circuit is shown on the right. This circuit was originally proposed by Lazzaro et al. (1989) using MOS transistors biased to operate in the weak-inversion or subthreshold regime. In the particular case shown there are only two inputs (\"I\" and \"I\"), but the circuit can be easily extended to multiple inputs in a straightforward way. It operates on continuous-time input signals (currents) in parallel, using only two transistors per input. In addition, the bias current \"I\" is set by a single global transistor that is common to all the inputs.\n\nThe largest of the input currents sets the common potential \"V\". As a result, the corresponding output carries almost all the bias current, while the other outputs have currents that are close to zero. Thus, the circuit selects the larger of the two input currents, i.e., if \"I\" > \"I\", we get \"I\" = \"I\" and \"I\" = 0. Similarly, if \"I\" > \"I\", we get \"I\" = 0 and \"I\" = \"I\".\n\nA SPICE-based DC simulation of the CMOS winner-take-all circuit in the two-input case is shown on the right. As shown in the top subplot, the input \"I\" was fixed at 6nA, while \"I\" was linearly increased from 0 to 10nA. The bottom subplot shows the two output currents. As expected, the output corresponding to the larger of the two inputs carries the entire bias current (10nA in this case), forcing the other output current nearly to zero.\n\nIn stereo matching algorithms, following the taxonomy proposed by Scharstein et al. (IJCV 2002), winner-take-all is a local method for disparity computation. Adopting a winner-take-all strategy, the disparity associated with the minimum or maximum cost value is selected at each pixel.\n\nIt is axiomatic that in the electronic commerce market, early dominant players such as AOL or Yahoo! get most of the rewards. By 1998, one study found the top 5% of all web sites garnered more than 74% of all traffic.\n\nThe winner take all hypothesis suggests that once a technology or a firm gets ahead, it will do better and better over time, whereas lagging technology and firms will fall further behind.\n\n\n", "id": "1853175", "title": "Winner-take-all (computing)"}
{"url": "https://en.wikipedia.org/wiki?curid=24833746", "text": "Physical neural network\n\nA physical neural network is a type of artificial neural network in which an electrically adjustable resistance material is used to emulate the function of a neural synapse. \"Physical\" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches which simulate neural networks. More generally the term is applicable to other artificial neural networks in which a memristor or other electrically adjustable resistance material is used to emulate a neural synapse.\n\nIn the 1960s Bernard Widrow and Ted Hoff developed ADALINE (Adaptive Linear Neuron) which used electrochemical cells called memistors (memory resistors) to emulate synapses of an artificial neuron. The memistors were implemented as 3-terminal devices operating based on the reversible electroplating of copper such that the resistance between two of the terminals is controlled by the integral of the current applied via the third terminal. The ADALINE circuitry was briefly commercialized by the Memistor Corporation in the 1960s enabling some applications in pattern recognition. However, since the memistors were not fabricated using integrated circuit fabrication techniques the technology was not scalable and was eventually abandoned as solid state electronics became mature.\n\nAlex Nugent describes a physical neural network as one or more nonlinear neuron-like nodes used to sum signals and nanoconnections formed from nanoparticles, nanowires, or nanotubes which determine the signal strength input to the nodes. Alignment or self-assembly of the nanoconnections is determined by the history of the applied electric field performing a function analogous to neural synapses. Numerous applications for such physical neural networks are possible. For example, a temporal summation device can be composed of one or more nanoconnections having an input and an output thereof, wherein an input signal provided to the input causes one or more of the nanoconnection to experience an increase in connection strength thereof over time. Another example of a physical neural network is taught by U.S. Patent No. 7,039,619 entitled \"Utilized nanotechnology apparatus using a neural network, a solution and a connection gap,\" which issued to Alex Nugent by the U.S. Patent & Trademark Office on May 2, 2006.\n\nA further application of physical neural network is shown in U.S. Patent No. 7,412,428 entitled \"Application of hebbian and anti-hebbian learning to nanotechnology-based physical neural networks,\" which issued on August 12, 2008.\n\nNugent and Molter have shown that universal computing and general-purpose machine learning are possible from operations available through simple memristive circuits operating the AHaH plasticity rule.\nMore recently, it has been argued that complex purely memristive circuits can serve as neural networks.\n\nIn 2002, Stanford Ovshinsky described an analog neural computing medium in which phase change material has the ability to cumulatively respond to multiple input signals. An electrical alteration of the resistance of the phase change material is used to control the weighting of the input signals.\n\nGreg Snider of HP Labs describes a system of cortical computing with memristive nanodevices. The memristors (memory resistors) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. DARPA's SyNAPSE project has funded IBM Research and HP Labs, in collaboration with the Boston University Department of Cognitive and Neural Systems (CNS), to develop neuromorphic architectures which may be based on memristive systems.\n\n\n", "id": "24833746", "title": "Physical neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=31405724", "text": "IPO underpricing algorithm\n\nIPO underpricing is the increase in stock value from the initial offering price to the first-day closing price. Many believe that underpriced IPOs leave money on the table for corporations, but some believe that underpricing is inevitable. Investors state that underpricing signals high interest to the market which increases the demand. On the other hand, overpriced stocks will drop long-term as the price stabilizes so underpricing may keep the issuers safe from investor litigation.\n\nUnderwriters and investors and corporations going for an initial public offering (IPO), issuers, are interested in their market value. There is always tension that results since the underwriters want to keep the price low while the companies want a high IPO price.\n\nUnderpricing may also be caused by investor over-reaction causing spikes on the initial days of trading. The IPO pricing process is similar to pricing new and unique products where there is sparse data on market demand, product acceptance, or competitive response. Besides, underpricing is also affected by the firm idiosyncratic factors such as its business model. Thus it is difficult to determine a clear price which is compounded by the different goals issuers and investors have.\n\nThe problem with developing algorithms to determine underpricing is dealing with noisy, complex, and unordered data sets. Additionally, people, environment, and various environmental conditions introduce irregularities in the data. To resolve these issues, researchers have found various techniques from artificial intelligence that normalizes the data.\n\nArtificial neural networks (ANNs) resolves these issues by scanning the data to develop internal representations of the relationship between the data. By determining the relationship over time, ANNs are more responsive and adaptive to structural changes in the data. There are two models for ANNs: supervised learning and unsupervised learning.\n\nIn supervised learning models, there are tests that are needed to pass to reduce mistakes. Usually, when mistakes are encountered i.e. test output does not match test input, the algorithms use back propagation to fix mistakes. Whereas in unsupervised learning models, the input is classified based on which problems need to be resolved.\n\nEvolutionary programming is often paired with other algorithms e.g. ANN to improve the robustness, reliability, and adaptability. Evolutionary models reduce error rates by allowing the numerical values to change within the fixed structure of the program. Designers provide their algorithms the variables, they then provide training data to help the program generate rules defined in the input space that make a prediction in the output variable space.\n\nIn this approach, the solution is made an individual and the population is made of alternatives. However, the outliers cause the individuals to act unexpectedly as they try to create rules to explain the whole set.\n\nFor example, Quintana first abstracts a model with 7 major variables. The rules evolved from the Evolutionary Computation system developed at Michigan and Pittsburgh: \n\nQuintana uses these factors as signals that investors focus on. The algorithm his team explains shows how a prediction with a high-degree of confidence is possible with just a subset of the data.\n\nLuque approaches the problem with outliers by performing linear regressions over the set of data points (input, output). The algorithm deals with the data by allocating regions for noisy data. The scheme has the advantage of isolating noisy patterns which reduces the effect outliers have on the rule-generation system. The algorithm can come back later to understand if the isolated data sets influence the general data. Finally, the worst results from the algorithm outperformed all other algorithms' predictive abilities.\n\nCurrently, many of the algorithms assume homogeneous and rational behavior among investors. However, there's an approach alternative to financial modeling, and it's called agent-based modelling (ABM). ABM uses different autonomous agents whose behavior evolves endogenously which lead to complicated system dynamics that are sometimes impossible to predict from the properties of individual agents. ABM is starting to be applied to computational finance. Though, for ABM to be more accurate, better models for rule-generation need to be developed.\n", "id": "31405724", "title": "IPO underpricing algorithm"}
{"url": "https://en.wikipedia.org/wiki?curid=43561218", "text": "Word embedding\n\nWord embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with much lower dimension.\n\nMethods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, and explicit representation in terms of the context in which words appear.\n\nWord and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis.\n\nIn linguistics word embeddings were discussed in the research area of distributional semantics. It aims to quantify and categorize semantic similarities between linguistic items based on their distributional properties in large samples of language data. The underlying idea that \"a word is characterized by the company it keeps\" was popularized by Firth.\n\nThe technique of representing words as vectors has roots in the 1960s with the development of the vector space model for information retrieval. Reducing the number of dimensions using singular value decomposition then led to the introduction of latent semantic analysis in the late 1980s.\nIn 2000 Bengio et al. provided in a series of papers the \"Neural probabilistic language models\" to reduce the high dimensionality of words representations in contexts by \"learning a distributed representation for words\". (Bengio et al, 2003). Word embeddings come in two different styles, one in which words are expressed as vectors of co-occurring words, and another in which words are expressed as vectors of linguistic contexts in which the words occur; these different styles are studied in (Lavelli et al, 2004). Roweis and Saul published in \"Science\" how to use \"locally linear embedding\" (LLE) to discover representations of high dimensional data structures. The area developed gradually and really took off after 2010, partly because important advances had been made since then on the quality of vectors and the training speed of the model.\n\nThere are many branches and many research groups working on word embeddings. In 2013, a team at Google led by Tomas Mikolov created word2vec, a word embedding toolkit which can train vector space models faster than the previous approaches. Most new word embedding techniques rely on a neural network architecture instead of more traditional n-gram models and unsupervised learning.\n\nWord embeddings for n-grams in biological sequences (e.g. DNA, RNA, and Proteins) for bioinformatics applications have been proposed by Asgari and Mofrad. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of deep learning in proteomics and genomics. The results presented by suggest that BioVectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns.\n\nThought vectors are an extension of word embeddings to entire sentences or even documents. Some researchers hope that these can improve the quality of machine translation.\nSoftware for training and using word embeddings includes Tomas Mikolov's Word2vec, Stanford University's GloVe, fastText, Gensim, Indra and Deeplearning4j. Principal Component Analysis (PCA) and T-Distributed Stochastic Neighbour Embedding (t-SNE) are both used to reduce the dimensionality of word vector spaces and visualize word embeddings and clusters.\n", "id": "43561218", "title": "Word embedding"}
{"url": "https://en.wikipedia.org/wiki?curid=44738576", "text": "Hyper basis function network\n\nIn machine learning, a Hyper basis function network, or HyperBF network, is a generalization of radial basis function (RBF) networks concept, where the Mahalanobis-like distance is used instead of Euclidean distance measure. Hyper basis function networks were first introduced by Poggio and Girosi in the 1990 paper “Networks for Approximation and Learning”.\n\nThe typical HyperBF network structure consists of a real input vector formula_1, a hidden layer of activation functions and a linear output layer. The output of the network is a scalar function of the input vector, formula_2, is given by \n\nwhere formula_3 is a number of neurons in the hidden layer, formula_4 and formula_5 are the center and weight of neuron formula_6. The activation function formula_7 at the HyperBF network takes the following form\n\nwhere formula_8 is a positive definite formula_9 matrix. Depending on the application, the following types of matrices formula_8 are usually considered\n\nTraining HyperBF networks involves estimation of weights formula_5, shape and centers of neurons formula_8 and formula_4. Poggio and Girosi (1990) describe the training method with moving centers and adaptable neuron shapes. The outline of the method is provided below.\n\nConsider the quadratic loss of the network formula_20. The following conditions must be satisfied at the optimum:\n\nwhere formula_21. Then in the gradient descent method the values of formula_22 that minimize formula_23 can be found as a stable fixed point of the following dynamic system:\n\nwhere formula_24 determines the rate of convergence.\n\nOverall, training HyperBF networks can be computationally challenging. Moreover, the high degree of freedom of HyperBF leads to overfitting and poor generalization. However, HyperBF networks have an important advantage that a small number of neurons is enough for learning complex functions.\n", "id": "44738576", "title": "Hyper basis function network"}
{"url": "https://en.wikipedia.org/wiki?curid=46975535", "text": "Multimodal learning\n\nThe information in real world usually comes as different modalities. For example, images are usually associated with tags and text explanations; texts contain images to more clearly express the main idea of the article. Different modalities are characterized by very different statistical properties. For instance, images are usually represented as pixel intensities or outputs of feature extractors, while texts are represented as discrete word count vectors. Due to the distinct statistical properties of different information resources, it is very important to discover the relationship between different modalities. Multimodal learning is a good model to represent the joint representations of different modalities. The multimodal learning model is also capable to fill missing modality given the observed ones. The multimodal learning model combines two deep Boltzmann machines each corresponds to one modality. An additional hidden layer is placed on top of the two Boltzmann Machines to give the joint representation.\n\nMotivation is about internal and external factors that stimulate desire and energy in people to be continually interested and committed to a job, role or subject, or to make an effort to attain a goal.\n\nMotivation results from the interaction of both conscious and unconscious factors such as the (1) intensity of desire or need, (2) incentive or reward value of the goal, and (3) expectations of the individual and of his or her peers. These factors are the reasons one has for behaving a certain way. An example is a student that spends extra time studying for a test because he or she wants a better grade in the class.\n\nSo here it classifies human–computer interaction, which can be implemented with the use of models or algorithms.\n\nA lot of models/algorithms have been implemented to retrieve and classify a certain type of data, e.g. image or text (where humans who interacts with machines can extract images in a form of pictures and text that could be any message etc). However, data usually comes with different modalities (it is the degree to which a system's components may be separated or combined) which carry different information. For example, it is very common to caption an image to convey the information not presented by this image. Similarly, sometimes it is more straightforward to use an image to describe the information which may not be obvious from texts. As a results, if some different words appear in similar images, these words are very likely used to describe the same thing. Conversely, if some words are used in different images, these images may represent the same object. Thus, it is important to invite a novel model which is able to jointly represent the information such that the model can capture the correlation structure between different modalities. Moreover, it should also be able to recover missing modalities given observed ones, e.g. predicting possible image object according to text description. The Multimodal Deep Boltzmann Machine model satisfies the above purposes.\n\nA Boltzmann machine is a type of stochastic neural network invented by Geoffrey Hinton and Terry Sejnowski in 1985. Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield nets. They are named after the Boltzmann distribution in statistical mechanics. The units in Boltzmann machines are divided into two groups-visible units and hidden units. General Boltzmann machines allow connection between any units. However, learning is impractical using general Boltzmann Machines because the computational time is exponential to the size of the machine. A more efficient architecture is called restricted Boltzmann machine where connection is only allowed between hidden unit and visible unit, which is described in the next section.\n\nA restricted Boltzmann machine is an undirected graphical model with stochastic visible variable and stochastic hidden variables. Each visible variable is connected to each hidden variable. The energy function of the model is defined as \nwhere formula_2 are model parameters: formula_3 represents the symmetric interaction term between visible unit formula_4 and hidden unit formula_5; formula_6 and formula_7 are bias terms. The joint distribution of the system is defined as \nwhere formula_9 is a normalizing constant.\nThe conditional distribution over hidden formula_10 and formula_11 can be derived as logistic function in terms of model parameters.\nwhere formula_16 is the logistic function.\nThe derivative of the log-likelihood with respect to the model parameters can be decomposed as the difference between the \"model's expectation\" and \"data-dependent expectation\".\n\nGaussian-Bernoulli RBMs are a variant of restricted Boltzmann machine used for modeling real-valued vectors such as pixel intensities. It is usually used to model the image data. The energy of the system of the Gaussian-Bernoulli RBM is defined as \nwhere formula_18 are the model parameters. The joint distribution is defined the same as the one in restricted Boltzmann machine. The conditional distributions now become \nIn Gaussian-Bernoulli RBM, the visible unit conditioned on hidden units is modeled as a Gaussian distribution.\n\nThe Replicated Softmax Model is also an variant of restricted Boltzmann machine and commonly used to model word count vectors in a document. In a typical text mining problem, let formula_23 be the dictionary size, and formula_24 be the number of words in the document. Let formula_25 be a formula_26 binary matrix with formula_27 only when the formula_28 word in the document is the formula_29 word in the dictionary. formula_30 denotes the count for the formula_29 word in the dictionary. The energy of the state formula_32 for a document contains formula_24 words is defined as\nThe conditional distributions are given by \n\nA deep Boltzmann machine has a sequence of layers of hidden units.There are only connections between adjacent hidden layers, as well as between visible units and hidden units in the first hidden layer. The energy function of the system adds layer interaction terms to the energy function of general restricted Boltzmann machine and is defined by \nformula_37\n\nThe joint distribution is \n\nMultimodal deep Boltzmann machine uses an image-text bi-modal DBM where the image pathway is modeled as Gaussian-Bernoulli DBM and text pathway as Replicated Softmax DBM, and each DBM has two hidden layers and one visible layer. The two DBMs join together at an additional top hidden layer. The joint distribution over the multi-modal inputs defined as \nformula_39\n\nThe conditional distributions over the visible and hidden units are \n\nExact maximum likelihood learning in this model is intractable, but approximate learning of DBMs can be carried out by using a variational approach, where mean-field inference is used to estimate data-dependent expectations and an MCMC based stochastic approximation procedure is used to approximate the model’s expected sufficient statistics.\n\nMultimodal deep Boltzmann machines is successfully used in classification and missing data retrieval. The classification accuracy of multimodal deep Boltzmann machine outperforms support vector machines, latent Dirichlet allocation and deep belief network, when models are tested on data with both image-text modalities or with single modality. Multimodal deep Boltzmann machine is also able to predict the missing modality given the observed ones with reasonably good precision.\n\n", "id": "46975535", "title": "Multimodal learning"}
{"url": "https://en.wikipedia.org/wiki?curid=47349395", "text": "Dropout (neural networks)\n\nDropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data. It is a very efficient way of performing model averaging with neural networks. The term \"dropout\" refers to dropping out units (both hidden and visible) in a neural network.\n", "id": "47349395", "title": "Dropout (neural networks)"}
{"url": "https://en.wikipedia.org/wiki?curid=31663887", "text": "Adaptive neuro fuzzy inference system\n\nAn adaptive neuro-fuzzy inference system or adaptive network-based fuzzy inference system (ANFIS) is a kind of artificial neural network that is based on Takagi–Sugeno fuzzy inference system. The technique was developed in the early 1990s. Since it integrates both neural networks and fuzzy logic principles, it has potential to capture the benefits of both in a single framework. Its inference system corresponds to a set of fuzzy IF–THEN rules that have learning capability to approximate nonlinear functions. Hence, ANFIS is considered to be a universal estimator. For using the ANFIS in a more efficient and optimal way, one can use the best parameters obtained by genetic algorithm.\n", "id": "31663887", "title": "Adaptive neuro fuzzy inference system"}
{"url": "https://en.wikipedia.org/wiki?curid=47459238", "text": "ND4S\n\nND4S is a free, open-source extension of the Scala programming language operating on the Java Virtual Machine – though it is compatible with both Java and Clojure. \n\nND4S is a scientific computing library for linear algebra and matrix manipulation in a production environment, integrating with Hadoop and Spark to work with distributed GPUs. It supports n-dimensional arrays for JVM-based languages.\n\nND4S has primarily been developed by the group in San Francisco that built Deeplearning4j, led by Adam Gibson. It was created under an Apache Software Foundation license.\n\nND4S's operations include distributed parallel versions. They can take place in a cluster and process massive amounts of data. Matrix manipulation occurs in parallel on GPUs or CPUs in the cloud, and can work within Spark or Hadoop clusters.\n\nBenchmarks show that ND4S runs roughly twice as fast as NumPy on large matrices.\n\n", "id": "47459238", "title": "ND4S"}
{"url": "https://en.wikipedia.org/wiki?curid=172777", "text": "Perceptron\n\nIn machine learning, the perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time.\n\nThe perceptron algorithm dates back to the late 1950s. Its first implementation, in custom hardware, was one of the first artificial neural networks to be produced.\n\nThe perceptron algorithm was invented in 1957 at the Cornell Aeronautical Laboratory by Frank Rosenblatt, funded by the United States Office of Naval Research.\nThe perceptron was intended to be a machine, rather than a program, and while its first implementation was in software for the IBM 704, it was subsequently implemented in custom-built hardware as the \"Mark 1 perceptron\". This machine was designed for image recognition: it had an array of 400 photocells, randomly connected to the \"neurons\". Weights were encoded in potentiometers, and weight updates during learning were performed by electric motors.\n\nIn a 1958 press conference organized by the US Navy, Rosenblatt made statements about the perceptron that caused a heated controversy among the fledgling AI community; based on Rosenblatt's statements, \"The New York Times\" reported the perceptron to be \"the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.\"\n\nAlthough the perceptron initially seemed promising, it was quickly proved that perceptrons could not be trained to recognise many classes of patterns. This caused the field of neural network research to stagnate for many years, before it was recognised that a feedforward neural network with two or more layers (also called a multilayer perceptron) had far greater processing power than perceptrons with one layer (also called a single layer perceptron).\nSingle layer perceptrons are only capable of learning linearly separable patterns; in 1969 a famous book entitled \"Perceptrons\" by Marvin Minsky and Seymour Papert showed that it was impossible for these classes of network to learn an XOR function. It is often believed (incorrectly) that they also conjectured that a similar result would hold for a multi-layer perceptron network. However, this is not true, as both Minsky and Papert already knew that multi-layer perceptrons were capable of producing an XOR function. (See the page on \"Perceptrons (book)\" for more information.) Three years later Stephen Grossberg published a series of papers introducing networks capable of modelling differential, contrast-enhancing and XOR functions. (The papers were published in 1972 and 1973, see e.g.:). Nevertheless, the often-miscited Minsky/Papert text caused a significant decline in interest and funding of neural network research. It took ten more years until neural network research experienced a resurgence in the 1980s. This text was reprinted in 1987 as \"Perceptrons - Expanded Edition\" where some errors in the original text are shown and corrected.\n\nThe kernel perceptron algorithm was already introduced in 1964 by Aizerman et al. Margin bounds guarantees were given for the Perceptron algorithm in the general non-separable case first by Freund and Schapire (1998), and more recently by Mohri and Rostamizadeh (2013) who extend previous results and give new L1 bounds.\n\nIn the modern sense, the perceptron is an algorithm for learning a binary classifier: a function that maps its input (a real-valued vector) to an output value formula_1 (a single binary value):\n\nwhere is a vector of real-valued weights, formula_3 is the dot product formula_4, where m is the number of inputs to the perceptron and is the \"bias\". The bias shifts the decision boundary away from the origin and does not depend on any input value.\n\nThe value of formula_1 (0 or 1) is used to classify as either a positive or a negative instance, in the case of a binary classification problem. If is negative, then the weighted combination of inputs must produce a positive value greater than formula_6 in order to push the classifier neuron over the 0 threshold. Spatially, the bias alters the position (though not the orientation) of the decision boundary. The perceptron learning algorithm does not terminate if the learning set is not linearly separable. If the vectors are not linearly separable learning will never reach a point where all vectors are classified properly. The most famous example of the perceptron's inability to solve problems with linearly nonseparable vectors is the Boolean exclusive-or problem. The solution spaces of decision boundaries for all binary functions and learning behaviors are studied in the reference.\n\nIn the context of neural networks, a perceptron is an artificial neuron using the Heaviside step function as the activation function. The perceptron algorithm is also termed the single-layer perceptron, to distinguish it from a multilayer perceptron, which is a misnomer for a more complicated neural network. As a linear classifier, the single-layer perceptron is the simplest feedforward neural network.\n\nBelow is an example of a learning algorithm for a (single-layer) perceptron. For multilayer perceptrons, where a hidden layer exists, more sophisticated algorithms such as backpropagation must be used. Alternatively, methods such as the delta rule can be used if the function is non-linear and differentiable, although the one below will work as well.\n\nWhen multiple perceptrons are combined in an artificial neural network, each output neuron operates independently of all the others; thus, learning each output can be considered in isolation.\n\nWe first define some variables:\nWe show the values of the features as follows:\nTo represent the weights: \nTo show the time-dependence of formula_24, we use:\nUnlike other linear classification algorithms such as logistic regression, there is no need for a \"learning rate\" in the perceptron algorithm. This is because multiplying the update by any constant simply rescales the weights but never changes the sign of the prediction.\n\nThe algorithm updates the weights after steps 2a and 2b. These weights are immediately applied to a pair in the training set, and subsequently updated, rather than waiting until all pairs in the training set have undergone these steps.\n\nThe perceptron is a linear classifier, therefore it will never get to the state with all the input vectors classified correctly if the training set is not linearly separable, i.e. if the positive examples can not be separated from the negative examples by a hyperplane. In this case, no \"approximate\" solution will be gradually approached under the standard learning algorithm, but instead learning will fail completely. Hence, if linear separability of the training set is not known a priori, one of the training variants below should be used.\n\nBut if the training set \"is\" linearly separable, then the perceptron is guaranteed to converge, and there is an upper bound on the number of times the perceptron will adjust its weights during the training.\n\nSuppose that the input vectors from the two classes can be separated by a hyperplane with a margin formula_28, i.e. there exists a weight vector formula_29, and a bias term such that formula_30 for all formula_31 and formula_32 for all formula_33. And also let denote the maximum norm of an input vector. Novikoff (1962) proved that in this case the perceptron algorithm converges after making formula_34 updates. The idea of the proof is that the weight vector is always adjusted by a bounded amount in a direction with which it has a negative dot product, and thus can be bounded above by where is the number of changes to the weight vector. But it can also be bounded below by because if there exists an (unknown) satisfactory weight vector, then every change makes progress in this (unknown) direction by a positive \namount that depends only on the input vector.\nWhile the perceptron algorithm is guaranteed to converge on \"some\" solution in the case of a linearly separable training set, it may still pick \"any\" solution and problems may admit many solutions of varying quality. The \"perceptron of optimal stability\", nowadays better known as the linear support vector machine, was designed to solve this problem.\n\nThe pocket algorithm with ratchet (Gallant, 1990) solves the stability problem of perceptron learning by keeping the best solution seen so far \"in its pocket\". The pocket algorithm then returns the solution in the pocket, rather than the last solution. It can be used also for non-separable data sets, where the aim is to find a perceptron with a small number of misclassifications. However, these solutions appear purely stochastically and hence the pocket algorithm neither approaches them gradually in the course of learning, nor are they guaranteed to show up within a given number of learning steps.\n\nThe Maxover algorithm (Wendemuth, 1995) is \"robust\" in the sense that it will converge regardless of (prior) knowledge of linear separability of the data set. In the linear separable case, it will solve the training problem – if desired, even with optimal stability (maximum margin between the classes). For non-separable data sets, it will return a solution with a small number of misclassifications. In all cases, the algorithm gradually approaches the solution in the course of learning, without memorizing previous states and without stochastic jumps. Convergence is to global optimality for separable data sets and to local optimality for non-separable data sets.\n\nIn separable problems, perceptron training can also aim at finding the largest separating margin between the classes. The so-called perceptron of optimal stability can be determined by means of iterative training and optimization schemes, such as the Min-Over algorithm (Krauth and Mezard, 1987) or the AdaTron (Anlauf and Biehl, 1989))\n. AdaTron uses the fact that the corresponding quadratic optimization problem is convex. The perceptron of optimal stability, together with the kernel trick, are the conceptual foundations of the support vector machine.\n\nThe formula_35-perceptron further used a pre-processing layer of fixed random weights, with thresholded output units. This enabled the perceptron to classify analogue patterns, by projecting them into a binary space. In fact, for a projection space of sufficiently high dimension, patterns can become linearly separable.\n\nAnother way to solve nonlinear problems without using multiple layers is to use higher order networks (sigma-pi unit). In this type of network, each element in the input vector is extended with each pairwise combination of multiplied inputs (second order). This can be extended to an \"n\"-order network.\n\nIt should be kept in mind, however, that the best classifier is not necessarily that which classifies all the training data perfectly. Indeed, if we had the prior constraint that the data come from equi-variant Gaussian distributions, the linear separation in the input space is optimal, and the nonlinear solution is overfitted.\n\nOther linear classification algorithms include Winnow, support vector machine and logistic regression.\n\nLike most other techniques for training linear classifiers, the perceptron generalizes naturally to multiclass classification. Here, the input formula_36 and the output formula_37 are drawn from arbitrary sets. A feature representation function formula_38 maps each possible input/output pair to a finite-dimensional real-valued feature vector. As before, the feature vector is multiplied by a weight vector formula_39, but now the resulting score is used to choose among many possible outputs:\n\nLearning again iterates over the examples, predicting an output for each, leaving the weights unchanged when the predicted output matches the target, and changing them when it does not. The update becomes:\n\nThis multiclass feedback formulation reduces to the original perceptron when formula_36 is a real-valued vector, formula_37 is chosen from formula_44, and formula_45.\n\nFor certain problems, input/output representations and features can be chosen so that formula_46 can be found efficiently even though formula_37 is chosen from a very large or even infinite set.\n\nIn recent years, perceptron training has become popular in the field of natural language processing for such tasks as part-of-speech tagging and syntactic parsing (Collins, 2002).\n\n\n", "id": "172777", "title": "Perceptron"}
{"url": "https://en.wikipedia.org/wiki?curid=48684895", "text": "Electricity price forecasting\n\nElectricity price forecasting (EPF) is a branch of energy forecasting which focuses on predicting the spot and forward prices in wholesale electricity markets. Over the last 15 years electricity price forecasts have become a fundamental input to energy companies’ decision-making mechanisms at the corporate level.\n\nSince the early 1990s, the process of deregulation and the introduction of competitive electricity markets have been reshaping the landscape of the traditionally monopolistic and government-controlled power sectors. Throughout Europe, North America and Australia, electricity is now traded under market rules using spot and derivative contracts. However, electricity is a very special commodity: it is economically non-storable and power system stability requires a constant balance between production and consumption. At the same time, electricity demand depends on weather (temperature, wind speed, precipitation, etc.) and the intensity of business and everyday activities (on-peak vs. off-peak hours, weekdays vs. weekends, holidays, etc.). These unique characteristics lead to price dynamics not observed in any other market, exhibiting daily, weekly and often annual seasonality and abrupt, short-lived and generally unanticipated price spikes.\n\nExtreme price volatility, which can be up to two orders of magnitude higher than that of any other commodity or financial asset, has forced market participants to hedge not only volume but also price risk. Price forecasts from a few hours to a few months ahead have become of particular interest to power portfolio managers. A power market company able to forecast the volatile wholesale prices with a reasonable level of accuracy can adjust its bidding strategy and its own production or consumption schedule in order to reduce the risk or maximize the profits in day-ahead trading. A ballpark estimate of savings from a 1% reduction in the mean absolute percentage error (MAPE) of short-term price forecasts is $300,000 per year for a utility with 1GW peak load.\n\nA variety of methods and ideas have been tried for EPF over the last 15 years, with varying degrees of success. They can be broadly classified into six groups.\n\n\"Multi-agent\" (\"multi-agent simulation\"\", equilibrium, game theoretic\") models simulate the operation of a system of heterogeneous agents (generating units, companies) interacting with each other, and build the price process by matching the demand and supply in the market. This class includes \"cost-based models\" (or \"production-cost models\", PCM), \"equilibrium\" or \"game theoretic\" approaches (like the Nash-Cournot framework, supply function equilibrium - SFE, strategic production-cost models - SPCM) and agent-based models.\n\nMulti-agent models generally focus on qualitative issues rather than quantitative results. They may provide insights as to whether or not prices will be above marginal costs, and how this might influence the players’ outcomes. However, they pose problems if more quantitative conclusions have to be drawn, particularly if electricity prices have to be predicted with a high level of precision.\n\n\"Fundamental\" (\"structural\") methods try to capture the basic physical and economic relationships which are present in the production and trading of electricity. The functional associations between fundamental drivers (loads, weather conditions, system parameters, etc.) are postulated, and the fundamental inputs are modeled and predicted independently, often via statistical, reduced-form or computational intelligence techniques. In general, two subclasses of fundamental models can be identified: \"parameter rich models\" and \"parsimonious structural models\" of supply and demand.\n\nTwo major challenges arise in the practical implementation of fundamental models: data availability and incorporation of stochastic fluctuations of the fundamental drivers. In building the model, we make specific assumptions about physical and economic relationships in the marketplace, and therefore the price projections generated by the models are very sensitive to violations of these assumptions.\n\n\"Reduced-form\" (\"quantitative, stochastic\") models characterize the statistical properties of electricity prices over time, with the ultimate objective of derivatives valuation and risk management. Their main intention is not to provide accurate hourly price forecasts, but rather to replicate the main characteristics of daily electricity prices, like marginal distributions at future time points, price dynamics, and correlations between commodity prices. If the price process chosen is not appropriate for capturing the main properties of electricity prices, the results from the model are likely to be unreliable. However, if the model is too complex, the computational burden will prevent its use on-line in trading departments. Depending on the type of market under consideration, reduced-form models can be classified as:\n\n\"Statistical\" (\"econometric, technical analysis\") methods forecast the current price by using a mathematical combination of the previous prices and/or previous or current values of exogenous factors, typically consumption and production figures, or weather variables. The two most important categories are \"additive\" and \"multiplicative\" models. They differ in whether the predicted price is the sum (additive) of a number of components or the product (multiplicative) of a number of factors. The former are far more popular, but the two are closely related - a multiplicative model for prices can be transformed into an additive model for log-prices. Statistical models are attractive because some physical interpretation may be attached to their components, thus allowing engineers and system operators to understand their behavior. They are often criticized for their limited ability to model the (usually) nonlinear behavior of electricity prices and related fundamental variables. However, in practical applications, their performances are not worse than those of the non-linear computational intelligence methods (see below). For instance, in the \"load forecasting track\" of the Global Energy Forecasting Competition (GEFCom2012) attracting hundreds of participants worldwide, the top four winning entries used regression-type models.\n\nStatistical models constitute a very rich class which includes:\n\n\"Computational intelligence\" (\"artificial intelligence-based, machine learning, non-parametric, non-linear statistical\") techniques combine elements of learning, evolution and fuzziness to create approaches that are capable of adapting to complex dynamic systems, and may be regarded as \"intelligent\" in this sense. Artificial neural networks, fuzzy systems and support vector machines (SVM) are unquestionably the main classes of computational intelligence techniques in EPF. Their major strength is the ability to handle complexity and non-linearity. In general, computational intelligence methods are better at modeling these features of electricity prices than the statistical techniques (see above). At the same time, this flexibility is also their major weakness. The ability to adapt to nonlinear, spiky behaviors will not necessarily result in better point or probabilistic forecasts.\n\nMany of the modeling and price forecasting approaches considered in the literature are \"hybrid\" solutions, combining techniques from two or more of the groups listed above. Their classification is non-trivial, if possible at all.\nAs an example of hybrid model AleaModel (AleaSoft) combines Neural Networks and Box Jenkins models.\n\nIt is customary to talk about short-, medium- and long-term forecasting, but there is no consensus in the literature as to what the thresholds should actually be:\n\nIn his extensive review paper, Weron looks ahead and speculates on the directions EPF will or should take over the next decade or so:\n\nA key point in electricity spot price modeling and forecasting is the appropriate treatment of seasonality. And it should be emphasized that the electricity price exhibits seasonality at three levels: the daily and weekly, and to some extent - the annual. In \"short-term forecasting\", the annual or long-term seasonality is usually ignored, but the daily and weekly patterns (including a separate treatment of holidays) are of prime importance. This, however, may not be the right approach. As Nowotarski and Weron have recently shown, decomposing a series of electricity prices into a long-term seasonal and a stochastic component, modeling them independently and combining their forecasts can bring - contrary to a common belief - an accuracy gain compared to an approach in which a given model is calibrated to the prices themselves. \n\nIn \"mid-term forecasting\", the daily patterns become less relevant and most EPF models work with average daily prices. However, the long-term trend-cycle component plays a crucial role. Its misspecification can introduce bias, which may lead to a bad estimate of the mean reversion level or of the price spike intensity and severity, and consequently, to underestimating the risk. Finally, in the \"long term\", when the time horizon is measured in years, the daily, weekly and even annual seasonality may be ignored, and long-term trends dominate. Adequate treatment - both in-sample and out-of-sample - of seasonality has not been given enough attention in the literature so far. \n\nAnother crucial issue in electricity price forecasting is the appropriate choice of explanatory variables. Apart from historical electricity prices, the current spot price is dependent on a large set of fundamental drivers, including system loads, weather variables, fuel costs, the reserve margin (i.e., available generation minus/over predicted demand) and information about scheduled maintenance and forced outages. Although \"pure price\" models are sometimes used for EPF, in the most common day-ahead forecasting scenario most authors select a combination of these fundamental drivers, based on the heuristics and experience of the forecaster. Very rarely has an automated selection or shrinkage procedure been carried out in EPF, especially for a large set of initial explanatory variables. However, the machine learning literature provides viable tools that can be broadly classified into two categories:\nSome of these techniques have been utilized in the context of EPF:\nbut their use is not common. Further development and employment of methods for selecting the most effective input variables - from among the past electricity prices, as well as the past and predicted values of the fundamental drivers - is needed.\n\nWhen predicting spike occurrences or spot price volatility, one of the most influential fundamental variables is the reserve margin, also called surplus generation. It relates the available capacity (generation, supply), formula_1, to the demand (load), formula_2, at a given moment in time formula_3. The traditional engineering notion of the reserve margin defines it as the difference between the two, i.e., formula_4, but many authors prefer to work with dimensionless ratios formula_5, formula_6 or the so-called capacity utilization formula_7. Its rare application in EPF can be justified only by the difficulty of obtaining good quality reserve margin data. Given that more and more system operators (see e.g. http://www.elexon.co.uk) are disclosing such information nowadays, reserve margin data should be playing a significant role in EPF in the near future.\n\nThe use of prediction intervals (PI) and densities, or probabilistic forecasting, has become much more common over the past three decades, as practitioners have come to understand the limitations of point forecasts. Despite the bold move by the organizers of the Global Energy Forecasting Competition 2014 to require the participants to submit forecasts of the 99 percentiles of the predictive distribution (day-ahead in the price track) and not the point forecasts as in the 2012 edition, this does not seem to be a common case in EPF as yet.\n\nIf PIs are computed at all, they usually are distribution-based (and approximated by the standard deviation of the model residuals) or empirical. The latter method resembles the estimation of the Value-at-Risk via historical simulation, and consists of computing sample quantiles of the empirical distribution of the one-step-ahead prediction errors. A new forecast combination (see below) technique has been introduced recently in the context of EPF. Quantile Regression Averaging (QRA) involves applying quantile regression to the point forecasts of a small number of individual forecasting models or experts, hence allows to leverage existing development of point forecasting.\n\nConsensus forecasts, also known as \"combining forecasts\", \"forecast averaging\" or \"model averaging\" (in econometrics and statistics) and \"committee machines\", \"ensemble averaging\" or \"expert aggregation\" (in machine learning), are predictions of the future that are created by combining together several separate forecasts which have often been created using different methodologies. Despite their popularity in econometrics, averaged forecasts have not been used extensively in the context of electricity markets to date. There is some limited evidence on the adequacy of combining forecasts of electricity demand, but it was only very recently that combining was used in EPF and only for point forecasts. Combining probabilistic (i.e., interval and density) forecasts is much less popular, even in econometrics in general, mainly because of the increased complexity of the problem. Since Quantile Regression Averaging (QRA) allows to leverage existing development of point forecasting, it is particularly attractive from a practical point of view and may become a popular tool in EPF in the near future.\n\nThe literature on forecasting daily electricity prices has concentrated largely on models that use only information at the aggregated (i.e., daily) level. On the other hand, the very rich body of literature on forecasting intra-day prices has used disaggregated data (i.e., hourly or half-hourly), but generally has not explored the complex dependence structure of the multivariate price series. If we want to explore the structure of intra-day electricity prices, we need to use dimension reduction methods; for instance, factor models with factors estimated as principal components (PC). Empirical evidence indicates that there are forecast improvements from incorporating disaggregated (i.e., hourly or zonal) data for predicting daily system prices, especially when the forecast horizon exceeds one week. With the increase of computational power, the real-time calibration of these complex models will become feasible and we may expect to see more EPF applications of the multivariate framework in the coming years.\n\nAll major review publications conclude that there are problems with comparing the methods developed and used in the EPF literature. This is due mainly to the use of different datasets, different software implementations of the forecasting models and different error measures, but also to the lack of statistical rigor in many studies. This calls for a comprehensive, thorough study involving (i) the same datasets, (ii) the same robust error evaluation procedures, and (iii) statistical testing of the significance of one model's outperformance of another. To some extent, the Global Energy Forecasting Competition 2014 has addressed these issues. Yet more has to be done. A selection of the better-performing measures (weighted-MAE, seasonal MASE or RMSSE) should be used either exclusively or in conjunction with the more popular ones (MAPE, RMSE). The empirical results should be further tested for the significance of the differences in forecasting accuracies of the models.\n\n", "id": "48684895", "title": "Electricity price forecasting"}
{"url": "https://en.wikipedia.org/wiki?curid=49594059", "text": "ND4J (software)\n\nND4J is a scientific computing library, written in the programming language C++, operating on the Java virtual machine (JVM), and compatible with the languages Java, Scala, and Clojure. ND4J was contributed to the Eclipse Foundation in October 2017.\n\nND4J is for performing linear algebra and matrix manipulation in a production environment, integrating with Apache Hadoop and Spark to work with distributed central processing units (CPUs) or graphics processing units (GPUs). It supports n-dimensional arrays for JVM-based languages.\n\nND4J is free and open-source software, released under Apache License 2.0, and developed mostly by the group in San Francisco that built Deeplearning4j, led by Adam Gibson. It was created under an Apache Software Foundation license.\n\nND4J's operations include distributed parallel versions. Operation can occur in a cluster and process massive amounts of data. Matrix manipulation occurs in parallel on CPUs or GPUs over cloud computing, and can work in Spark or Hadoop clusters.\n\nA usability gap has separated Java, Scala, Kotlin and Clojure programmers from powerful tools in data analysis such as NumPy or Matlab. Libraries like Breeze don’t support n-dimensional arrays, or tensors, which are necessary for deep learning and other tasks. Libraries like Colt and Parallel Colt use or have dependencies with GPL in the license, making them unsuitable for commercial use. ND4J was built to address those functional and licenses issues. \n\n\n", "id": "49594059", "title": "ND4J (software)"}
{"url": "https://en.wikipedia.org/wiki?curid=49686608", "text": "Bidirectional recurrent neural networks\n\nBidirectional Recurrent Neural Networks (BRNN) were invented in 1997 by Schuster and Paliwal. BRNNs were introduced to increase the amount of input information available to the network. For example, multilayer perceptron (MLPs) and time delay neural network (TDNNs) have limitations on the input data flexibility, as they require their input data to be fixed. Standard recurrent neural network (RNNs) also have restrictions as the future input information cannot be reached from the current state. On the contrary, BRNNs do not require their input data to be fixed. Moreover, their future input information is reachable from the current state. The basic idea of BRNNs is to connect two hidden layers of opposite directions to the same output. By this structure, the output layer can get information from past and future states.\n\nBRNN are especially useful when the context of the input is needed. For example, in handwriting recognition, the performance can be enhanced by knowledge of the letters located before and after the current letter.\n\nThe principle of BRNN is to split the neurons of a regular RNN into two directions, one for positive time direction(forward states), and another for negative time direction(backward states). Those two states’ output are not connected to inputs of the opposite direction states. The general structure of RNN and BRNN can be depicted in the right diagram. By using two time directions, input information from the past and future of the current time frame can be used unlike standard RNN which requires the delays for including future information.\n\nBRNN can be trained using similar algorithms compared to RNN, because the two directional neurons do not have any interactions. However, when back-propagation is applied, additional processes are needed because updating input and output layers cannot be done at once. General procedures for training are as follows: For forward pass, forward states and backward states are passed first, then output neurons are passed. For backward pass, output neurons are passed first, then forward states and backward states are passed next. After forward and backward passes are done, the weights are updated.\n\nApplications of BRNN include :\n\n\n\n\n", "id": "49686608", "title": "Bidirectional recurrent neural networks"}
{"url": "https://en.wikipedia.org/wiki?curid=51002645", "text": "FindFace\n\nFindFace is a website that lets users search for members of the social network VK by uploading a photograph.\n\nIt was created by Artem Kuharenko and Alexander Kabakov - Ntech Lab Ltd. co-founders.\n\nFindFace employs a facial recognition neural network algorithm developed by N-Tech.Lab to match faces in the photographs uploaded by its users against faces in photographs published on VK, with a reported accuracy of 70 percent. Different sources pointing NTech Lab's technology accuracy from 85.081% to 99%\n\nIn 2015 NTechLab algorithm won The MegaFace Benchmark challenge, organized by University of Washington.\n\nIn 2016, FindFace generated controversy because it was used to systematically deanonymize Russian pornography actresses and alleged prostitutes. These efforts were organized by users of a Russian imageboard who claimed that women in the sex industry are “corrupt and deceptive”, according to \"Global Voices\". In addition, FindFace has been characterized as a major step in the erosion of anonymity.\n", "id": "51002645", "title": "FindFace"}
{"url": "https://en.wikipedia.org/wiki?curid=53468615", "text": "General regression neural network\n\nGeneralized regression neural network (GRNN) is a variation to radial basis neural networks. GRNN was suggested by D.F. Specht in 1991.\n\nGRNN can be used for regression, prediction, and classification. GRNN can also be a good solution for online dynamical systems.\n\nGRNN represents an improved technique in the neural networks based on the nonparametric regression. The idea is that every training sample will represents a mean to a radial basis neuron. \n\nwhere:\n\nformula_7\n\nwhere formula_8 is the squared euclidean distance between the training samples formula_9 and the input formula_10\n\nGRNN has been implemented in many software including MATLAB, R- programming language and Python (programming language).\n\nSimilar to RBFNN GRNN has the following advantages:\n\n\nThe main disadvantages of GRNN are:\n", "id": "53468615", "title": "General regression neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=54133326", "text": "WaveNet\n\nWaveNet is a deep neural network for generating raw audio. It was created by researchers at London-based artificial intelligence firm DeepMind. The technique, outlined in a paper in September 2016, is able to generate more realistic-sounding human-like voices by sampling real human speech and directly modelling waveforms. Tests with US English and Mandarin, reportedly showed that the system outperforms Google's best existing text-to-speech (TTS) systems, although it is still less convincing than actual human speech. WaveNet’s ability to generate raw waveforms means that it can model any kind of audio, including music. Canada-based start-up Lyrebird-AI offers similar technology, based on a different deep learning model. \n\nGenerating speech from text is an increasingly common task thanks to the popularity of software such as Apple's Siri, Microsoft’s Cortana, Amazon Alexa and the Google Assistant. \nMost such systems use a variation of a technique that involves concatenated sound fragments together to form recognisable sounds and words. The most common of these is called concatenative TTS. It consists of large library of speech fragments, recorded from a single speaker that are then concatenated to produce complete words and sounds. The result sounds unnatural, with an odd cadence and tone. The reliance on a recorded library also makes it difficult to modify or change the voice.\nAnother technique, known as parametric TTS, uses mathematical models to recreate sounds that are then assembled into words and sentences. The information required to generate the sounds is stored in the parameters of the model. The characteristics of the output speech are controlled via the inputs to the model, while the speech is typically created using a voice synthesiser known as a vocoder. This can also result in unnatural sounding audio.\nWaveNet is a type of feedforward neural network known as a deep convolutional neural network (CNN). These consist of layers of interconnected nodes somewhat analogous to the brain’s neurons. The CNN takes a raw signal as an input and synthesises an output one sample at a time. \nIn the 2016 paper, the network was fed real waveforms of speech in English and Mandarin. As these pass through the network, it learns a set of rules to describe how the audio waveform evolves over time. The trained network can then be used to create new speech-like waveforms at 16,000 samples per second. These waveforms include realistic breaths and lip smacks - but do not conform to any language. \nWaveNet is able to accurately model different voices, with the accent and tone of the input correlating with the output. For example, if it is trained with German, it produces German speech. This ability to clone voices has raised ethical concerns about WaveNets ability to mimic the voices of living persons. \nThe capability also means that if the WaveNet is fed other inputs - such as music - its output will be musical. At the time of its release, DeepMind showed that WaveNet could produce waveforms that sound like classical music. \n\nAt the time of its release, DeepMind said that WaveNet required too much computational processing power to be used in real world applications. As of October 2017, Google announced a 1,000-fold performance improvement along with better voice quality. WaveNet was then used to generate Google Assistant voices for US English and Japanese across all Google platforms.\n", "id": "54133326", "title": "WaveNet"}
{"url": "https://en.wikipedia.org/wiki?curid=55482316", "text": "Relation network\n\nA relation network (RN) is an artificial neural network component with a structure that can reason about relations among objects. An example category of such relations is spatial relations (above, below, left, right, in front of, behind).\n\nRNs can infer relations, they are data efficient, and they operate on a set of objects without regard to the objects' order.\n\nIn June 2017, DeepMind announced the first relation network. It claimed that the technology had achieved \"superhuman\" performance on multiple question-answering problem sets.\nRNs constrain the functional form of a neural network to capture the common properties of relational reasoning. These properties are explicitly added to the system, rather than established by learning just as the capacity to reason about spatial, translation-invariant properties is explicitly part of convolutional neural networks (CNN). The data to be considered can be presented as a simple list or as a directed graph whose nodes are objects and whose edges are the pairs of objects whose relationships are to be considered. The RN is a composite function: formula_1\n\nwhere the input is a set of “objects” formula_2 is the i object, and fφ and gθ are functions with parameters φ and θ, respectively and \"q\" is the question. \"fφ\" and \"gθ\" are multilayer perceptrons, while the 2 parameters are learnable synaptic weights. RNs are differentiable. The output of gθ is a “relation”; therefore, the role of gθ is to infer any ways in which two objects are related.\n\nImage (128x128 pixel) processing is done with a 4-layer CNN. Outputs from the CNN are treated as the objects for relation analysis, without regard for what those \"objects\" explicitly represent. Questions were processed with a long short-term memory network.\n\n", "id": "55482316", "title": "Relation network"}
{"url": "https://en.wikipedia.org/wiki?curid=55867424", "text": "Residual neural network\n\nA residual neural network is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing \"skip connections\" or \"short-cuts\" to jump over some layers. In its limit as \"ResNets\" it will only skip over a single layer. With an additional weight matrix to learn the skip weights it is referred to as \"HighwayNets\". With several parallel skips it is referred to as \"DenseNets\". In comparison, a non-residual neural network is described as a \"plain network\" in the context of residual neural networks.\nThe brain has structures similar to residual nets, as layer VI neurons gets input from layer I, skipping over all intermediary layers. In the figure this compares to signals from the (3) Apical dendrite skipping over layers while the (2) Basal dendrite collecting signals from the previous and/or same layer. Similar structures exists for other layers. How many layers in the cerebral cortex compare to layers in an artificial neural network is not clear, neither if every area in cerebral cortex exhibits the same structure, but over large areas they look quite similar.\n\nThe motivation for skipping over layers in ANNs is to avoid the problem of vanishing gradients, by reusing activation from a previous layer until the layer next to the current one have learned its weights. During training the weights will adapt to mute the previous layer and amplify the layer next to the current. In the simplest case only the weights for the connection to the next to the current layer is adapted, with no explicit weights for the upstream previous layer. This usually work properly when a single non-linear layer is stepped over, or in the case when the intermediate layers are all linear. If not, then an explicit weight matrix should be learned for the skipped connection.\n\nThe intuition on why this work is that the neural network collapses into fewer layers in the initial phase, which makes it easier to learn, and thus gradually expands the layers as it learns more of the feature space. During later learning, when all layers are expanded, it will stay closer to the manifold and thus learn faster. A neural network without residual parts will explore more of the feature space, small perturbations will make it leave the manifold altogether, and thus needs it will need a lot more training data just to get it back on track.\n\nLayers are indexed both as formula_1 to formula_2 and formula_2 to formula_4. (Script formula_2 used for clarity, usually it is written as a simple \"l\".) It only has consequence for where you start counting, and thus whether you describe the skipping as going backward or forward. As signal flows it is easier to describe the signal flows going forward as formula_6 from a given layer, but as a learning rule it is easier to describe which activation layer you reuse as formula_7.\n\nIf there is a weight matrix formula_8for weights from layer formula_9 to formula_2, and a weight matrix formula_11 for weights from layer formula_1 to formula_2, then the forward propagation through the activation function would be (aka \"HighwayNets\")\nwhere we have\nIf there is no such explicit matrix, then the forward propagation through the activation function would simplify into (aka \"ResNets\")\nAnother way to formulate this is to insert an identity matrix, but that is only valid when the dimensions match. This is somewhat confusingly called an \"identity block\".\n\nIn the cerebral cortex such forward skips are done for several layers. Usually all forward skips starts from the same layer, and successively connects to later layers. In the general case this will be expressed as (aka \"DenseNets\")\n\nDuring backpropagation learning for the normal path\nand for the skipper paths (note that they are close to identical)\nIn both cases we have\nIf the skippers have fixed weights, then they will not be updated. If they can be updated, then the rule will be an ordinary backprop update rule.\n\nIn the general case there can be formula_33 skipper weight matrices, thus\n\nAs the learning rules are similar the weight matrices can be merged and learned in the same step.\n", "id": "55867424", "title": "Residual neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=32472154", "text": "Deep learning\n\nDeep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, semi-supervised or unsupervised.\n\nSome representations are loosely based on interpretation of information processing and communication patterns in a biological nervous system, such as neural coding that attempts to define a relationship between various stimuli and associated neuronal responses in the brain.\n\nDeep learning architectures such as deep neural networks, deep belief networks and recurrent neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics and drug design, where they have produced results comparable to and in some cases superior to human experts.\n\nDeep learning is a class of machine learning algorithms that:\n\nLayers that have been used in deep learning include hidden layers of an artificial neural network and sets of propositional formulas. They may also include latent variables organized layer-wise in deep generative models such as the nodes in Deep Belief Networks and Deep Boltzmann Machines.\n\n\nThe assumption underlying distributed representations is that observed data are generated by the interactions of layered factors.\n\nDeep learning adds the assumption that these layers of factors correspond to levels of abstraction or composition. Varying numbers of layers and layer sizes can provide different degrees of abstraction.\n\nDeep learning exploits this idea of hierarchical explanatory factors where higher level, more abstract concepts are learned from the lower level ones. .\n\nDeep learning architectures are often constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance.\n\nFor supervised learning tasks, deep learning methods obviate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.\n\nDeep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than labeled data. Examples of deep structures that can be trained in an unsupervised manner are neural history compressors and deep belief networks.\n\nDeep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.\n\nThe universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Hornik.\n\nThe probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.\n\nThe term \"Deep Learning\" was introduced to the machine learning community by Rina Dechter in 1986, and to Artificial Neural Networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons. through neural networks for reinforcement learning. In 2006, a publication by Geoff Hinton, Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation. The paper referred to \"learning\" for \"deep belief nets.\" \n\nThe first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1965. A 1971 paper described a deep network with 8 layers trained by the group method of data handling algorithm.\n\nOther deep learning working architectures, specifically those built for computer vision, began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1989, Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970, to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.\n\nBy 1991 such systems were used for recognizing isolated 2-D hand-written digits, while recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. Weng \"et al.\" suggested that a human brain does not use a monolithic 3-D object model and in 1992 they published Cresceptron, a method for performing 3-D object recognition in cluttered scenes. Cresceptron is a cascade of layers similar to Neocognitron. But while Neocognitron required a human programmer to hand-merge features, Cresceptron learned an open number of features in each layer without supervision, where each feature is represented by a convolution kernel. Cresceptron segmented each learned object from a cluttered scene through back-analysis through the network. Max pooling, now often adopted by deep neural networks (e.g. ImageNet tests), was first used in Cresceptron to reduce the position resolution by a factor of (2x2) to 1 through the cascade for better generalization.\n\nIn 1994, André de Carvalho, together with Fairhurst and Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a self-organising feature extraction neural network module followed by a classification neural network module, which were independently trained.\n\nIn 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton. Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter.\n\nSimpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of ANNs' computational cost and a lack of understanding of how the brain wires its biological networks.\n\nBoth shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power.\n\nMost speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI studied deep neural networks in speech and speaker recognition. Heck's speaker recognition team achieved the first significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. While SRI experienced success with deep neural networks in speaker recognition, they were unsuccessful in demonstrating similar success in speech recognition. One decade later, Hinton and Deng collaborated with each other and then with colleagues across groups at University of Toronto, Microsoft, Google and IBM, igniting a renaissance of deep feedforward neural networks in speech recognition.\n\nThe principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.\n\nMany aspects of speech recognition were taken over by a deep learning method called Long short-term memory (LSTM), a recurrent neural network published by Hochreiter and Schmidhuber in 1997. LSTM RNNs avoid the vanishing gradient problem and can learn \"Very Deep Learning\" tasks that require memories of events that happened thousands of discrete time steps before, which is important for speech. In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks. Later it was combined with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.\n\nIn the early 2000s, CNNs processed an estimated 10% to 20% of all the checks written in the US.\n\nIn 2006, Hinton and Salakhutdinov showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation.\n\nDeep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks (CNNs) were superseded for ASR by CTC for LSTM. but are more successful in computer vision.\n\nThe impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US. Industrial applications of deep learning to large-scale speech recognition started around 2010.\n\nIn late 2009, Li Deng invited Hinton to work with him and colleagues to apply deep learning to speech recognition. They co-organized the 2009 NIPS Workshop on Deep Learning for Speech Recognition. The workshop was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, they discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009-2010, contrasted the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition, eventually leading to pervasive and dominant use in that industry. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.\n\nIn 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.\n\nAdvances in hardware enabled the renewed interest. In 2009, Nvidia was involved in what was called the “big bang” of deep learning, “as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs).” That year, Google Brain used Nvidia GPUs to create capable DNNs. While there, Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times. In particular, GPUs are well-suited for the matrix/vector math involved in machine learning. GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days. Specialized hardware and algorithm optimizations can be used for efficient processing.\n\nIn 2012, a team led by Dahl won the \"Merck Molecular Activity Challenge\" using multi-task deep neural networks to predict the biomolecular target of one drug. In 2014, Hochreiter's group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the \"Tox21 Data Challenge\" of NIH, FDA and NCATS.\n\nSignificant additional impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, fast implementations of CNNs with max-pooling on GPUs in the style of Ciresan and colleagues were needed to progress on computer vision. In 2011, this approach achieved for the first time superhuman performance in a visual pattern recognition contest. Also in 2011, it won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest. Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In October 2012, a similar system by Krizhevsky and Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. In November 2012, Ciresan et al.'s system also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic. In 2013 and 2014, the error rate on the ImageNet task using deep learning was further reduced, following a similar trend in large-scale speech recognition. The Wolfram Image Identification project publicized these improvements.\n\nImage classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.\n\nArtificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.\n\nAn ANN is based on a collection of connected units called artificial neurons, (analogous to axons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.\n\nTypically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.\n\nThe original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.\n\nNeural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\n\nAs of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, playing \"Go\").\n\nA deep neural network (DNN) is an ANN with multiple hidden layers between the input and output layers. Similar to shallow ANNs, DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.\n\nDeep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.\n\nDNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back.\n\nRecurrent neural networks (RNNs), in which data can flow in any direction, are used for applications such as language modeling. Long short-term memory is particularly effective for this use.\n\nConvolutional deep neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).\n\nAs with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.\n\nDNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or weight decay (formula_1-regularization) or sparsity (formula_2-regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.\n\nDNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing throughput using GPUs has produced significant speedups in training, because the matrix and vector computations required are well-suited for GPUs. \n\nAlternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights for CMAC. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved .\n\nLarge-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn \"Very Deep Learning\" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.\n\nThe initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak language models (without a strong grammar). This lets the weaknesses in acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized over the past 20 years:\n\nThe debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003-2007, accelerated progress in eight major areas:\n\nAll major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.\n\nA common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.\n\nDeep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants. This first occurred in 2011.\n\nDeep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.\n\nClosely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of a) identifying the style period of a given painting, b) \"capturing\" the style of a given painting and applying it in a visually pleasing manner to an arbitrary photograph, and c) generating striking imagery based on random visual input fields.\n\nNeural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.\n\nOther key techniques in this field are negative sampling and word embedding. Word embedding, such as \"word2vec\", can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition and others.\n\nGoogle Translate (GT) uses a large end-to-end long short-term memory network. GNMT uses an example-based machine translation method in which the system \"learns from millions of examples.\" It translates \"whole sentences at a time, rather than pieces. Google Translate supports over one hundred languages. The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\". GT uses English as an intermediate between most language pairs..\n\nA large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects. Research has explored use of deep learning to predict biomolecular target, off-target and toxic effects of environmental chemicals in nutrients, household products and drugs.\n\nAtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.\n\nDeep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.\n\nRecommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music recommendations. Multiview deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.\n\nAn autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.\n\nIn medical informatics, deep learning was used to predict sleep quality based on data from wearables and predictions of health complications from electronic health record data.\n\nFinding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and assimilated before a target segment can be created and used in ad serving by any ad server. Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.\n\nDeep learning has been successfully applied to inverse problems such as denoising, super-resolution, and inpainting. These applications include learning methods such \"Shrinkage Fields for Effective Image Restoration\" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.\n\nDeep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s. These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, \"...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature.\"\n\nMany organizations employ deep learning for particular applications. Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.\n\nGoogle's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses an LSTM to translate between more than 100 languages.\n\nIn 2015, Blippar demonstrated a mobile augmented reality application that uses deep learning to recognize objects in real time.\n\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science.\n\nA main criticism concerns the lack of theory surrounding the methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.\n\nOthers point out that deep learning should be looked at as a step towards realizing strong AI, not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed for realizing this goal entirely. Research psychologist Gary Marcus noted:\"Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning.\"As an alternative to this emphasis on the limits of deep learning, one author speculated that it might be possible to train a machine vision stack to perform the sophisticated task of discriminating between \"old master\" and amateur figure drawings, and hypothesized that such a sensitivity might represent the rudiments of a non-trivial machine empathy. This same author proposed that this would be in line with anthropology, which identifies a concern with aesthetics as a key element of behavioral modernity.\n\nIn further reference to the idea that artistic sensitivity might inhere within relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on \"The Guardian's\" web site.\n\nSome deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images and misclassifying minuscule perturbations of correctly classified images. Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component AGI architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and AI.\n\nAs deep learning moves from the lab into the world, research and experience shows that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such a manipulation is termed an “adversarial attack.” In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.\n\nAnother group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.\n\nANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.\n\nAnother group demonstrated that certain sounds could make the Google Now voice command system open a particular web address that would download malware.\n\nIn “data poisoning”, false data is continually smuggled into a machine learning system’s training set to prevent it from achieving mastery.\n\n\nU\n\n", "id": "32472154", "title": "Deep learning"}
{"url": "https://en.wikipedia.org/wiki?curid=28016652", "text": "Types of artificial neural networks\n\nThere are many types of artificial neural networks (ANN).\n\nArtificial neural networks are computational models inspired by biological neural networks, and are used to approximate functions that are generally unknown. Particularly, they are inspired by the behaviour of neurons and the electrical signals they convey between input (such as from the eyes or nerve endings in the hand), processing, and output from the brain (such as reacting to light, touch, or heat). The way neurons semantically communicate is an area of ongoing research. Most artificial neural networks bear only some resemblance to their more complex biological counterparts, but are very effective at their intended tasks (e.g. classification or segmentation).\n\nSome ANNs are adaptive systems and are used for example to model populations and environments, which constantly change.\n\nNeural networks can be hardware- (neurons are represented by physical components) or software-based (computer models), and can use a variety of topologies and learning algorithms.\n\nThe feedforward neural network was the first and simplest type. In this network the information moves only from the input layer directly through any hidden layers to the output layer without cycles/loops. Feedforward networks can be constructed with various types of units, such as binary McCulloch-Pitts neurons, the simplest of which is the perceptron. Continuous neurons, frequently with sigmoidal activation, are used in the context of backpropagation.\n\nAn autoencoder, autoassociator or Diabolo network is similar to the multilayer perceptron (MLP) – with an input layer, an output layer and one or more hidden layers connecting them. However, the output layer has the same number of units as the input layer. Its purpose is to reconstruct its own inputs (instead of emitting a target value). Therefore, autoencoders are unsupervised learning models. An autoencoder is used for unsupervised learning of efficient codings, typically for the purpose of dimensionality reduction and for learning generative models of data.\n\nA probabilistic neural network (PNN) is a four-layer feedforward neural network. The layers are Input, hidden, pattern/summation and output. In the PNN algorithm, the parent probability distribution function (PDF) of each class is approximated by a Parzen window and a non-parametric function. Then, using PDF of each class, the class probability of a new input is estimated and Bayes’ rule is employed to allocate it to the class with the highest posterior probability. It was derived from the Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It is used for classification and pattern recognition.\n\nA time delay neural network (TDNN) is a feedforward architecture for sequential data that recognizes features independent of sequence position. In order to achieve time-shift invariance, delays are added to the input so that multiple data points (points in time) are analyzed together.\n\nIt usually forms part of a larger pattern recognition system. It has been implemented using a perceptron network whose connection weights were trained with back propagation (supervised learning).\n\nIn a convolutional neural network (CNN, or ConvNet or shift invariant or space invariant.) the unit connectivity pattern is inspired by the organization of the visual cortex. Units respond to stimuli in a restricted region of space known as the receptive field. Receptive fields partially overlap, over-covering the entire visual field. Unit response can be approximated mathematically by a convolution operation. They are variations of multilayer perceptrons that use minimal preprocessing. They have wide applications in image and video recognition, recommender systems and natural language processing.\n\nRegulatory feedback networks started as a model to explain brain phenomena found during recognition including network-wide bursting and difficulty with similarity found universally in sensory recognition. This approach can also perform mathematically equivalent classification as feedforward methods and is used as a tool to create and modify networks.\n\nRadial basis functions are functions that have a distance criterion with respect to a center. Radial basis functions have been applied as a replacement for the sigmoidal hidden layer transfer characteristic in multi-layer perceptrons. RBF networks have two layers: In the first, input is mapped onto each RBF in the 'hidden' layer. The RBF chosen is usually a Gaussian. In regression problems the output layer is a linear combination of hidden layer values representing mean predicted output. The interpretation of this output layer value is the same as a regression model in statistics. In classification problems the output layer is typically a sigmoid function of a linear combination of hidden layer values, representing a posterior probability. Performance in both cases is often improved by shrinkage techniques, known as ridge regression in classical statistics. This corresponds to a prior belief in small parameter values (and therefore smooth output functions) in a Bayesian framework.\n\nRBF networks have the advantage of avoiding local minima in the same way as multi-layer perceptrons. This is because the only parameters that are adjusted in the learning process are the linear mapping from hidden layer to output layer. Linearity ensures that the error surface is quadratic and therefore has a single easily found minimum. In regression problems this can be found in one matrix operation. In classification problems the fixed non-linearity introduced by the sigmoid output function is most efficiently dealt with using iteratively re-weighted least squares.\n\nRBF networks have the disadvantage of requiring good coverage of the input space by radial basis functions. RBF centres are determined with reference to the distribution of the input data, but without reference to the prediction task. As a result, representational resources may be wasted on areas of the input space that are irrelevant to the task. A common solution is to associate each data point with its own centre, although this can expand the linear system to be solved in the final layer and requires shrinkage techniques to avoid overfitting.\n\nAssociating each input datum with an RBF leads naturally to kernel methods such as support vector machines (SVM) and Gaussian processes (the RBF is the kernel function). All three approaches use a non-linear kernel function to project the input data into a space where the learning problem can be solved using a linear model. Like Gaussian processes, and unlike SVMs, RBF networks are typically trained in a maximum likelihood framework by maximizing the probability (minimizing the error). SVMs avoid overfitting by maximizing instead a margin. SVMs outperform RBF networks in most classification applications. In regression applications they can be competitive when the dimensionality of the input space is relatively small.\n\nRBF neural networks are conceptually similar to K-Nearest Neighbor (k-NN) models. The basic idea is that similar inputs produce similar outputs.\n\nIn the case in of a training set has two predictor variables, x and y and the target variable has two categories, positive and negative. Given a new case with predictor values x=6, y=5.1, how is the target variable computed?\n\nThe nearest neighbor classification performed for this example depends on how many neighboring points are considered. If 1-NN is used and the closest point is negative, then the new point should be classified as negative. Alternatively, if 9-NN classification is used and the closest 9 points are considered, then the effect of the surrounding 8 positive points may outweigh the closest 9 (negative) point.\n\nAn RBF network positions neurons in the space described by the predictor variables (x,y in this example). This space has as many dimensions as predictor variables. The Euclidean distance is computed from the new point to the center of each neuron, and a radial basis function (RBF) (also called a kernel function) is applied to the distance to compute the weight (influence) for each neuron. The radial basis function is so named because the radius distance is the argument to the function.\n\n\"Weight\" = RBF(\"distance\")\n\nThe value for the new point is found by summing the output values of the RBF functions multiplied by weights computed for each neuron.\n\nThe radial basis function for a neuron has a center and a radius (also called a spread). The radius may be different for each neuron, and, in RBF networks generated by DTREG, the radius may be different in each dimension.\n\nWith larger spread, neurons at a distance from a point have a greater influence.\n\nRBF networks have three layers:\n\n\nThe following parameters are determined by the training process:\n\n\nVarious methods have been used to train RBF networks. One approach first uses K-means clustering to find cluster centers which are then used as the centers for the RBF functions. However, K-means clustering is computationally intensive and it often does not generate the optimal number of centers. Another approach is to use a random subset of the training points as the centers.\n\nDTREG uses a training algorithm that uses an evolutionary approach to determine the optimal center points and spreads for each neuron. It determines when to stop adding neurons to the network by monitoring the estimated leave-one-out (LOO) error and terminating when the LOO error begins to increase because of overfitting.\n\nThe computation of the optimal weights between the neurons in the hidden layer and the summation layer is done using ridge regression. An iterative procedure computes the optimal regularization Lambda parameter that minimizes the generalized cross-validation (GCV) error.\n\nRecurrent neural networks (RNNs) propagate data forward, but also backwards, from later processing stages to earlier stages. RNNs can be used as general sequence processors.\n\nThis architecture was developed in the 1980s. Its network creates a directed connection between every pair of units. Each has a time-varying, real-valued (more than just zero or one) activation (output). Each connection has a modifiable real-valued weight. Some of the nodes are called labeled nodes, some output nodes, the rest hidden nodes.\n\nFor supervised learning in discrete time settings, training sequences of real-valued input vectors become sequences of activations of the input nodes, one input vector at a time. At each time step, each non-input unit computes its current activation as a nonlinear function of the weighted sum of the activations of all units from which it receives connections. The system can explicitly activate (independent of incoming signals) some output units at certain time steps. For example, if the input sequence is a speech signal corresponding to a spoken digit, the final target output at the end of the sequence may be a label classifying the digit. For each sequence, its error is the sum of the deviations of all activations computed by the network from the corresponding target signals. For a training set of numerous sequences, the total error is the sum of the errors of all individual sequences.\n\nTo minimize total error, gradient descent can be used to change each weight in proportion to its derivative with respect to the error, provided the non-linear activation functions are differentiable. The standard method is called \"backpropagation through time\" or BPTT, a generalization of back-propagation for feedforward networks. A more computationally expensive online variant is called \"Real-Time Recurrent Learning\" or RTRL. Unlike BPTT this algorithm is \"local in time but not local in space\". An online hybrid between BPTT and RTRL with intermediate complexity exists, with variants for continuous time. A major problem with gradient descent for standard RNN architectures is that error gradients vanish exponentially quickly with the size of the time lag between important events. The Long short-term memory architecture overcomes these problems.\n\nIn reinforcement learning settings, no teacher provides target signals. Instead a fitness function or reward function or utility function is occasionally used to evaluate performance, which influences its input stream through output units connected to actuators that affect the environment. Variants of evolutionary computation are often used to optimize the weight matrix.\n\nThe Hopfield network (like similar attractor-based networks) is of historic interest although it is not a general RNN, as it is not designed to process sequences of patterns. Instead it requires stationary inputs. It is an RNN in which all connections are symmetric. It guarantees that it will converge. If the connections are trained using Hebbian learning the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration.\n\nThe Boltzmann machine can be thought of as a noisy Hopfield network. It is one of the first neural networks to demonstrate learning of latent variables (hidden units). Boltzmann machine learning was at first slow to simulate, but the contrastive divergence algorithm speeds up training for Boltzmann machines and Products of Experts.\n\nThe self-organizing map (SOM) uses unsupervised learning. A set of neurons learn to map points in an input space to coordinates in an output space. The input space can have different dimensions and topology from the output space, and SOM attempts to preserve these.\n\nLearning vector quantization (LVQ) can be interpreted as a neural network architecture. Prototypical representatives of the classes parameterize, together with an appropriate distance measure, in a distance-based classification scheme.\n\nSimple recurrent networks have three layers, with the addition of a set of \"context units\" in the input layer. These units connect from the hidden layer or the output layer with a fixed weight of one. At each time step, the input is propagated in a standard feedforward fashion, and then a backpropagation-like learning rule is applied (not performing gradient descent). The fixed back connections leave a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied).\n\nThe echo state network (ESN) is a recurrent neural network with a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change (be trained). ESN are good at reproducing certain time series. A variant for spiking neurons is known as liquid state machines.\n\nThe long short-term memory (LSTM) has no vanishing gradient problem. It works even when with long delays, and it can handle signals that have a mix of low and high frequency components. LSTM RNN outperformed other RNN and other sequence learning methods such as HMM in applications such as language learning and connected handwriting recognition.\n\nBi-directional RNNs, or BRNNs, use a finite sequence to predict or label each element of the sequence based on both the past and the future context of the element. This is done by adding the outputs of two RNNs: one processing the sequence from left to right, the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique proved to be especially useful when combined with LSTM RNNs.\n\nHierarchical RNNs connects elements in various ways to decompose hierarchical behavior into useful subprograms.\n\nA stochastic neural network introduces random variations into the network. Such random variations can be viewed as a form of statistical sampling, such as Monte Carlo sampling.\n\nA RNN (often a LSTM) where a series is decomposed into a number of scales where every scale informs the primary length between two consecutive points. A first order scale consists of a normal RNN, a second order consists of all points separated by two indices and so on. The Nth order RNN connects the first and last node. The outputs from all the various scales are treated as a CoM and the associated scores are used genetically for the next iteration.\n\nBiological studies have shown that the human brain operates as a collection of small networks. This realization gave birth to the concept of modular neural networks, in which several small networks cooperate or compete to solve problems.\n\nA committee of machines (CoM) is a collection of different neural networks that together \"vote\" on a given example. This generally gives a much better result than individual networks. Because neural networks suffer from local minima, starting with the same architecture and training but using randomly different initial weights often gives vastly different results. A CoM tends to stabilize the result.\n\nThe CoM is similar to the general machine learning \"bagging\" method, except that the necessary variety of machines in the committee is obtained by training from different starting weights rather than training on different randomly selected subsets of the training data.\n\nThe associative neural network (ASNN) is an extension of committee of machines that combines multiple feedforward neural networks and the k-nearest neighbor technique. It uses the correlation between ensemble responses as a measure of distance amid the analyzed cases for the kNN. This corrects the bias of the neural network ensemble. An associative neural network has a memory that can coincide with the training set. If new data become available, the network instantly improves its predictive ability and provides data approximation (self-learns) without retraining. Another important feature of ASNN is the possibility to interpret neural network results by analysis of correlations between data cases in the space of models.\n\nA physical neural network includes electrically adjustable resistance material to simulate artificial synapses. Examples include the ADALINE memristor-based neural network. An optical neural network is a physical implementation of an artificial neural network with optical components.\n\nInstantaneously trained neural networks (ITNNs) were inspired by the phenomenon of short-term learning that seems to occur instantaneously. In these networks the weights of the hidden and the output layers are mapped directly from the training vector data. Ordinarily, they work on binary data, but versions for continuous data that require small additional processing exist.\n\nSpiking neural networks (SNNs) explicitly consider the timing of inputs. The network input and output are usually represented as a series of spikes (delta function or more complex shapes). SNNs can process information in the time domain (signals that vary over time). They are often implemented as recurrent networks. SNNs are also a form of pulse computer.\n\nSpiking neural networks with axonal conduction delays exhibit polychronization, and hence could have a very large memory capacity.\n\nSNNs and the temporal correlations of neural assemblies in such networks—have been used to model figure/ground separation and region linking in the visual system.\n\nA regulatory feedback network makes inferences using negative feedback. The feedback is used to find the optimal activation of units. It is most similar to a non-parametric method but is different from K-nearest neighbor in that it mathematically emulates feedforward networks.\n\nThe neocognitron is a hierarchical, multilayered network that was modeled after the visual cortex. It uses multiple types of units, (originally two, called simple and complex cells), as a cascading model for use in pattern recognition tasks. Local features are extracted by S-cells whose deformation is tolerated by C-cells. Local features in the input are integrated gradually and classified at higher layers. Among the various kinds of neocognitron are systems that can detect multiple patterns in the same input by using back propagation to achieve selective attention. It has been used for pattern recognition tasks and inspired convolutional neural networks.\n\nDynamic neural networks address nonlinear multivariate behaviour and include (learning of) time-dependent behaviour, such as transient phenomena and delay effects. Techniques to estimate a system process from observed data fall under the general category of system identification.\n\nCascade correlation is an architecture and supervised learning algorithm. Instead of just adjusting the weights in a network of fixed topology, Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages: It learns quickly, determines its own size and topology, retains the structures it has built even if the training set changes and requires no backpropagation.\n\nA neuro-fuzzy network is a fuzzy inference system in the body of an artificial neural network. Depending on the FIS type, several layers simulate the processes involved in a fuzzy inference-like fuzzification, inference, aggregation and defuzzification. Embedding an FIS in a general structure of an ANN has the benefit of using available ANN training methods to find the parameters of a fuzzy system.\n\nCompositional pattern-producing networks (CPPNs) are a variation of ANNs which differ in their set of activation functions and how they are applied. While typical ANNs often contain only sigmoid functions (and sometimes Gaussian functions), CPPNs can include both types of functions and many others. Furthermore, unlike typical ANNs, CPPNs are applied across the entire space of possible inputs so that they can represent a complete image. Since they are compositions of functions, CPPNs in effect encode images at infinite resolution and can be sampled for a particular display at whatever resolution is optimal.\n\nThis type of network can add new patterns without re-training. It is done by creating a specific memory structure, which assigns each new pattern to an orthogonal plane using adjacently connected hierarchical arrays. The network offers real-time pattern recognition and high scalability; this requires parallel processing and is thus best suited for platforms such as wireless sensor networks, grid computing, and GPGPUs.\n\nHierarchical temporal memory (HTM) models some of the structural and algorithmic properties of the neocortex. HTM is a biomimetic model based on memory-prediction theory. HTM is a method for discovering and inferring the high-level causes of observed input patterns and sequences, thus building an increasingly complex model of the world.\n\nHTM combines existing ideas to mimic the neocortex with a simple design that provides many capabilities. HTM combines and extends approaches used in Bayesian networks, spatial and temporal clustering algorithms, while using a tree-shaped hierarchy of nodes that is common in neural networks.\n\nHolographic Associative Memory (HAM) is an analog, correlation-based, associative, stimulus-response system. Information is mapped onto the phase orientation of complex numbers. The memory is effective for associative memory tasks, generalization and pattern recognition with changeable attention. Dynamic search localization is central to biological memory. In visual perception, humans focus on specific objects in a pattern. Humans can change focus from object to object without learning. HAM can mimic this ability by creating explicit representations for focus. It uses a bi-modal representation of pattern and a hologram-like complex spherical weight state-space. HAMs are useful for optical realization because the underlying hyper-spherical computations can be implemented with optical computation.\n", "id": "28016652", "title": "Types of artificial neural networks"}
{"url": "https://en.wikipedia.org/wiki?curid=5289626", "text": "TIMIT\n\nTIMIT is a corpus of phonemically and lexically transcribed speech of American English speakers of different sexes and dialects. Each transcribed element has been delineated in time.\n\nTIMIT was designed to further acoustic-phonetic knowledge and automatic speech recognition systems. It was commissioned by DARPA and corpus design was a joint effort between the Massachusetts Institute of Technology, SRI International, and Texas Instruments (TI). The speech was recorded at TI, transcribed at MIT, and verified and prepared for publishing by the National Institute of Standards and Technology (NIST). There is also a telephone bandwidth version called NTIMIT (Network TIMIT). \n\nTIMIT and NTIMIT are not freely available — either membership of the Linguistic Data Consortium, or a monetary payment, is required for access to the dataset.\n\n\n", "id": "5289626", "title": "TIMIT"}
{"url": "https://en.wikipedia.org/wiki?curid=10477224", "text": "Iris flower data set\n\nThe \"Iris\" flower data set or Fisher's \"Iris\" data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper \"The use of multiple measurements in taxonomic problems\" as an example of linear discriminant analysis. It is sometimes called Anderson's \"Iris\" data set because Edgar Anderson collected the data to quantify the morphologic variation of \"Iris\" flowers of three related species. Two of the three species were collected in the Gaspé Peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\".\n\nThe data set consists of 50 samples from each of three species of \"Iris\" (\"Iris setosa\", \"Iris virginica\" and \"Iris versicolor\"). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.\n\nBased on Fisher's linear discriminant model, this data set became a typical test case for many statistical classification techniques in machine learning such as support vector machines.\n\nThe use of this data set in cluster analysis however is not common, since the data set only contains two clusters with rather obvious separation. One of the clusters contains \"Iris setosa\", while the other cluster contains both \"Iris virginica\" and \"Iris versicolor\" and is not separable without the species information Fisher used. This makes the data set a good example to explain the difference between supervised and unsupervised techniques in data mining: Fisher's linear discriminant model can only be obtained when the object species are known: class labels and clusters are not necessarily the same.\n\nNevertheless, all three species of \"Iris\" are separable in the projection on the nonlinear branching principal component. The data set is approximated by the closest tree with some penalty for the excessive number of nodes, bending and stretching. Then the so-called \"metro map\" is constructed. The data points are projected into the closest node. For each node the pie diagram of the projected points is prepared. The area of the pie is proportional to the number of the projected points. It is clear from the diagram (left) that the absolute majority of the samples of the different \"Iris\" species belong to the different nodes. Only a small fraction of \"Iris-virginica\" is mixed with \"Iris-versicolor\" (the mixed blue-green nodes in the diagram). Therefore, the three species of Iris (\"Iris setosa\", \"Iris virginica\" and \"Iris versicolor\") are separable by the unsupervising procedures of nonlinear principal component analysis. To discriminate them, it is sufficient just to select the corresponding nodes on the principal tree.\n\nThe dataset contains a set of 50 records under 5 attributes - Petal Length , Petal Width , Sepal Length , Sepal width and Class.\n\nSeveral versions of the dataset have been published.\n\n", "id": "10477224", "title": "Iris flower data set"}
{"url": "https://en.wikipedia.org/wiki?curid=52222085", "text": "Arabic Speech Corpus\n\nThe Arabic Speech Corpus is a Modern Standard Arabic (MSA) speech corpus for speech synthesis. The corpus contains phonetic and orthographic transcriptions of more than 3.7 hours of MSA speech aligned with recorded speech on the phoneme level. The annotations include word stress marks on the individual phonemes.\n\nThe Arabic Speech Corpus was built as part of a doctoral project by Nawar Halabi at the University of Southampton funded by MicroLinkPC who own an exclusive license to commercialise the corpus, but the corpus is available for strictly non-commercial purposes through the official Arabic Speech Corpus website. It is distributed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\nThe corpus was mainly built for speech synthesis purposes, specifically Speech Synthesis, but the corpus has been used for building HMM based voices in Arabic. It was also used to automatically align other speech corpora with their phonetic transcript and could be used as part of a larger corpus for training speech recognition systems.\n\nThe package contains the following:\n\nThe corpus was also used to prove that using automatically extracted, orthography-based stress marks improve the quality of speech synthesis in MSA.\n\n\n", "id": "52222085", "title": "Arabic Speech Corpus"}
{"url": "https://en.wikipedia.org/wiki?curid=54057693", "text": "Persian Speech Corpus\n\nThe Persian Speech Corpus is a Modern Persian speech corpus for speech synthesis. The corpus contains phonetic and orthographic transcriptions of about 2.5 hours of Persian speech aligned with recorded speech on the phoneme level, including annotations of word boundaries. Previous spoken corpora of Persian include FARSDAT, which consists of read aloud speech from newspaper texts from 100 Persian speakers and the Telephone FARsi Spoken language DATabase (TFARSDAT) which comprises seven hours of read and spontaneous speech produced by 60 native speakers of Persian from ten regions of Iran.\n\nThe Persian Speech Corpus was built using the same methodologies laid out in the doctoral project on Modern Standard Arabic of Nawar Halabi at the University of Southampton. The work was funded by MicroLinkPC, who own an exclusive license to commercialise the corpus, though the corpus is available for non-commercial use through the corpus' website. It is distributed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\nThe corpus was built for speech synthesis purposes, but has been used for building HMM based voices in Persian. It can also be used to automatically align other speech corpora with their phonetic transcript and could be used as part of a larger corpus for training speech recognition systems.\n\nThe corpus is downloadable from its website, and contains the following:\n\n\n", "id": "54057693", "title": "Persian Speech Corpus"}
{"url": "https://en.wikipedia.org/wiki?curid=7024544", "text": "Statistical parsing\n\nStatistical parsing is a group of parsing methods within natural language processing. The methods have in common that they associate grammar rules with a probability. Grammar rules are traditionally viewed in computational linguistics as defining the valid sentences in a language. Within this mindset, the idea of associating each rule with a probability then provides the relative frequency of any given grammar rule and, by deduction, the probability of a complete parse for a sentence. (The probability associated with a grammar rule may be induced, but the application of that grammar rule within a parse tree and the computation of the probability of the parse tree based on its component rules is a form of deduction.) Using this concept, statistical parsers make use of a procedure to search over a space of all candidate parses, and the computation of each candidate's probability, to derive the most probable parse of a sentence. The Viterbi algorithm is one popular method of searching for the most probable parse.\n\n\"Search\" in this context is an application of the very useful search algorithm in artificial intelligence.\n\nAs an example, think about the sentence \"The can can hold water\". A reader would instantly see that there is an object called \"the can\" and that this object is performing the action 'can' (i.e. is able to); and the thing the object is able to do is \"hold\"; and the thing the object is able to hold is \"water\". Using more linguistic terminology, \"The can\" is a noun phrase composed of a determiner followed by a noun, and \"can hold water\" is a verb phrase which is itself composed of a verb followed by a verb phrase. But is this the only interpretation of the sentence? Certainly \"The can can\" is a perfectly valid noun-phrase referring to a type of dance, and \"hold water\" is also a valid verb-phrase, although the coerced meaning of the combined sentence is non-obvious. This lack of meaning is not seen as a problem by most linguists (for a discussion on this point, see Colorless green ideas sleep furiously) but from a pragmatic point of view it is desirable to obtain the first interpretation rather than the second and statistical parsers achieve this by ranking the interpretations based on their probability.\n\nThere are a number of methods that statistical parsing algorithms frequently use. While few algorithms will use all of these they give a good overview of the general field. Most statistical parsing algorithms are based on a modified form of chart parsing. The modifications are necessary to support an extremely large number of grammatical rules and therefore search space, and essentially involve applying classical artificial intelligence algorithms to the traditionally exhaustive search. Some examples of the optimisations are only searching a likely subset of the search space (stack search), for optimising the search probability (Baum-Welch algorithm) and for discarding parses that are too similar to be treated separately (Viterbi algorithm).\n\n\n", "id": "7024544", "title": "Statistical parsing"}
{"url": "https://en.wikipedia.org/wiki?curid=4561188", "text": "Frederick Jelinek\n\nFrederick Jelinek (18 November 1932 – 14 September 2010) was a Czech-American researcher in information theory, automatic speech recognition, and natural language processing. He is well known for his oft-quoted statement, \"Every time I fire a linguist, the performance of the speech recognizer goes up\".\n\nJelinek was born in Czechoslovakia just before the outbreak of World War II and emigrated with his family to the United States in the early years of the communist regime. He studied engineering at the Massachusetts Institute of Technology and taught for 10 years at Cornell University before being offered a job at IBM Research. In 1961, he married Czech screenwriter Milena Jelinek. At IBM, his team advanced approaches to computer speech recognition and machine translation. After IBM, he went to head the Center for Language and Speech Processing at Johns Hopkins University for 17 years, where he was still working on the day he died.\n\nJelinek was born on November 18, 1932, as Bedřich Jelínek in Kladno to Vilém and Trude Jelinek. His father was Jewish; his mother was born in Switzerland to Czech Catholic parents and had converted to Judaism. Jelinek senior, a dentist, had planned early for an escape to England; he arranged for a passport, visa, and the shipping of his dentistry materials. The couple planned to send their son to an English private school. However, Vilém decided to stay at the last minute and was eventually sent to the Theresienstadt concentration camp, where he died in 1945. The family was forced to move to Prague in 1941, but Frederick, his sister and motherthanks to the latter's backgroundescaped the concentration camps.\n\nAfter the war, Jelinek entered in the gymnasium, despite having missed several years of schooling because education of Jewish children had been forbidden since 1942. His mother, anxious that her son should get a good education, made great efforts for their emigration, especially when it became clear he would not be allowed to even attempt the graduation examination. His mother hoped her son would become a physician, but Jelinek dreamed of being a lawyer. He studied engineering in evening classes at the City College of New York and received stipends from the National Committee for a Free Europe that allowed him to study at the Massachusetts Institute of Technology. About his choice of specialty, he said: \"Fortunately, to electrical engineering there belonged a discipline whose aim was not the construction of physical systems: the theory of information\". He obtained his Ph.D. in 1962, with Robert Fano as his adviser.\n\nIn 1957, Jelinek paid an unexpected visit to Prague. He had been in Vienna and applied for a visa, hoping to see his former acquaintances again. He met with his old friend Miloš Forman, who introduced him to film student Milena Tabolovawhose screenplay had been the basis for the movie \"Easy Life\" (\"Snadný život\"). His flight back to the U.S. had a stopover in Munich, during which he called her to propose. Tabolova was considered a dissident and the authorities were not happy with her film. Jelinek asked for help from Jerome Wiesner and Cyrus Eaton, the latter who lobbied Nikita Khrushchev. Following the inauguration of John F. Kennedy, a group of Czech dissidents were allowed to emigrate in January 1961. Thanks to the lobbying, the future Milena Jelinek was one of them.\n\nAfter completing his graduate studies, Jelinek, who had developed an interest in linguistics, had plans to work with Charles F. Hockett at Cornell University. However these fell through and during the next ten years he continued to study information theory. Having previously worked at IBM during a sabbatical, he began full-time work there in 1972at first on leave for Cornell, but permanently from 1974. He remained there for over twenty years. Although at first he had been offered a regular research job, upon his arrival he learned that Josef Raviv had recently been promoted to head of the newly opened IBM Haifa Research Laboratory, and became head of the Continuous Speech Recognition group at the Thomas J. Watson Research Center. Despite his team's successes in this area, Jelinek's work remained little known in his home country because Czech scientists were not allowed to participate in key conferences.\n\nAfter the 1989 fall of communism, Jelinek helped establish scientific relationships, regularly visiting to lecture and helping to persuade IBM to establish a computing centre at Charles University. In 1993, he retired from IBM and went to Johns Hopkins University's Center for Language and Speech Processing, where he was director and Julian Sinclair Smith Professor of Electrical and Computer Engineering. He was still working there at the time of his death; Jelinek died of a heart attack at the close of an otherwise normal workday in mid-September 2010. He was survived by his wife, daughter and son, sister, stepsister, and three grandchildren: including Sophie Gold Jelinek.\n\nInformation theory was a fashionable scientific approach in the mid '50s. However, pioneer Claude Shannon wrote in 1956 that this trendiness was dangerous. He said, \"Our fellow scientists in many different fields, attracted by the fanfare and by the new avenues opened to scientific analysis, are using these ideas in their own problems ... It will be all too easy for our somewhat artificial prosperity to collapse overnight when it is realized that the use of a few exciting words like information, entropy, redundancy, do not solve all our problems.\" During the next decade, a combination of factors shut down the application of information theory to natural language processing (NLP) problemsin particular machine translation. One factor was the 1957 publication of Noam Chomsky's \"Syntactic Structures,\" which stated, \"probabilistic models give no insight into the basic problems of syntactic structure\". This accorded well with the philosophy of the artificial intelligence research of the time, which promoted rule-based approaches. The other factor was the 1966 ALPAC report, which recommended that the government should stop funding research into machine translation. ALPAC chairman John Pierce later said that the field was filled with \"mad inventors or untrustworthy engineers\". He said that the underlying linguistic problems must be solved before attempts at NLP could be reasonably made. These elements essentially halted research in the field.\n\nJelinek had begun to develop an interest in linguistics after the immigration of his wife, who initially enrolled in the MIT linguistics program with the help of Roman Jakobson. Jelinek often accompanied her to Chomsky's lectures, and even discussed the possibility of changing orientation with his adviser. Fano was \"really upset\", and after the failure of his project with Hockett at Cornell, he did not return to this field of research until starting work at IBM. The scope of research at IBM was considerably different from that of most other teams. According to Liberman, \"While [Jelinek] was leading IBM’s effort to solve the general dictation problem during the decade or so following 1972, most other U.S. companies and academic researchers were working on very limited problems ... or were staying out of the field entirely\".\n\nJelinek regarded speech recognition as an information theory problema noisy channel, in this case the acoustic signalwhich some observers considered a daring approach. The concept of perplexity was introduced in their first model, New Raleigh Grammar, which was published in 1976 as the paper \"Continuous Speech Recognition by Statistical Methods\" in the journal \"Proceedings of the IEEE\". According to Young, the basic noisy channel approach \"reduced the speech recognition problem to one of producing two statistical models\". Whereas New Raleigh Grammar was a hidden Markov model, their next model, called Tangora, was broader and involved n-grams, specifically trigrams. Even though \"it was obvious to everyone that this model was hopelessly impoverished\", it was not improved upon until Jelinek presented another paper in 1999. The same trigram approach was applied to phones in single words. Although the identification of parts of speech turned out not to be very useful for speech recognition, tagging methods developed during these projects are now used in various NLP applications.\n\nThe incremental research techniques developed at IBM eventually became dominant in the field after DARPA, in the mid-80s, returned to NLP research and imposed that methodology to participating teams, shared common goals, data, and precise evaluation metrics. The Continuous Speech Recognition Group's research, which required large amounts of data to train the algorithms, eventually led to the creation of the Linguistic Data Consortium. In the 1980s, although the broader problem of speech recognition remained unsolved, they sought to apply the methods developed to other problems; machine translation and stock value prediction were both seen as options. A group of IBM researchers went on to work for Renaissance Technologies. Jelinek wrote, \"The performance of the Renaissance fund is legendary, but I have no idea whether any methods we pioneered at IBM have ever been used. My former colleagues will not tell me: theirs is a very hush-hush operation!\" Methods very similar to those developed for achieving speech recognition are at the base of most machine translation systems in use today. Observers have said that Pierce's paradigm, according to which engineering achievements in this area would be built on scientific progress, has been inverted, with the achievements in engineering being at the base of a number of scientific findings.\n\nJelinek's works won \"best paper\" awards on several occasions, and he received a number of company awards while he worked at IBM. He received the Society Award for \"outstanding technical contributions and leadership\" from the IEEE Signal Processing Society for 1997, and the ESCA Medal for Scientific Achievement in 1999. He was a recipient of an IEEE Third Millennium Medal in 2000, the European Language Resources Association's first Antonio Zampolli Prize in 2004, the 2005 James L. Flanagan Speech and Audio Processing Award, and the 2009 Lifetime Achievement Award from the Association for Computational Linguistics. He received an \"honoris causa\" Ph.D. from Charles University in 2001, was elected to the National Academy of Engineering in 2006 and was made one of twelve inaugural fellows of the International Speech Communication Association in 2008.\n\n", "id": "4561188", "title": "Frederick Jelinek"}
{"url": "https://en.wikipedia.org/wiki?curid=1911810", "text": "Language model\n\nA statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length , it assigns a probability formula_1 to the whole sequence. Having a way to estimate the relative likelihood of different phrases is useful in many natural language processing applications, especially ones that generate text as an output. Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, handwriting recognition, information retrieval and other applications.\n\nIn speech recognition, the computer tries to match sounds with word sequences. The language model provides context to distinguish between words and phrases that sound similar. For example, in American English, the phrases \"recognize speech\" and \"wreck a nice beach\" are pronounced almost the same but mean very different things. These ambiguities are easier to resolve when evidence from the language model is incorporated with the pronunciation model and the acoustic model.\n\nLanguage models are used in information retrieval in the query likelihood model. Here a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query \"Q\" in the document's language model formula_2. Commonly, the unigram language model is used for this purpose—otherwise known as the bag of words model.\n\nData sparsity is a major problem in building language models. Most possible word sequences will not be observed in training. One solution is to make the assumption that the probability of a word only depends on the previous \"n\" words. This is known as an \"n\"-gram model or unigram model when \"n\" = 1.\n\nA unigram model used in information retrieval can be treated as the combination of several one-state finite automata. It splits the probabilities of different terms in a context, e.g. from formula_3 to formula_4.\n\nIn this model, the probability of each word only depends on that word's own probability in the document, so we only have one-state finite automata as units. The automaton itself has a probability distribution over the entire vocabulary of the model, summing to 1. The following is an illustration of a unigram model of a document.\n\nThe probability generated for a specific query is calculated as\n\nFor different documents, we can build their own unigram models, with different hitting probabilities of words in it. And we use probabilities from different documents to generate different hitting probabilities for a query. Then we can rank documents for a query according to the generating probabilities. Next is an example of two unigram models of two documents.\nIn information retrieval contexts, unigram language models are often smoothed to avoid instances where \"P\"(term) = 0. A common approach is to generate a maximum-likelihood model for the entire collection and linearly interpolate the collection model with a maximum-likelihood model for each document to create a smoothed document model.\n\nIn an \"n\"-gram model, the probability formula_1 of observing the sentence formula_8 is approximated as\n\nHere, it is assumed that the probability of observing the \"i\" word \"w\" in the context history of the preceding \"i\" − 1 words can be approximated by the probability of observing it in the shortened context history of the preceding \"n\" − 1 words (\"n\" order Markov property).\n\nThe conditional probability can be calculated from \"n\"-gram model frequency counts:\n\nThe words bigram and trigram language model denote \"n\"-gram model language models with \"n\" = 2 and \"n\" = 3, respectively.\n\nTypically, however, the \"n\"-gram model probabilities are not derived directly from the frequency counts, because models derived this way have severe problems when confronted with any \"n\"-grams that have not explicitly been seen before. Instead, some form of \"smoothing\" is necessary, assigning some of the total probability mass to unseen words or \"n\"-grams. Various methods are used, from simple \"add-one\" smoothing (assign a count of 1 to unseen \"n\"-grams) to more sophisticated models, such as Good-Turing discounting or back-off models.\n\nIn a bigram (\"n\" = 2) language model, the probability of the sentence \"I saw the red house\" is approximated as\n\nwhereas in a trigram (\"n\" = 3) language model, the approximation is\n\nNote that the context of the first \"n\" – 1 \"n\"-grams is filled with start-of-sentence markers, typically denoted <nowiki></nowiki>.\n\nAdditionally, without an end-of-sentence marker, the probability of an ungrammatical sequence \"*I saw the\" would always be higher than that of the longer sentence \"I saw the red house.\"\n\nMaximum entropy language models encode the relationship between a word and the n-gram history using feature functions.\nThe equation is formula_13 where \nformula_14 is the partition function, formula_15 is the \nparameter vector, and formula_16 is the feature function. In the simplest case, the feature function\nis just an indicator of the presence of a certain n-gram. It is helpful to use a prior on formula_15 or some form of\nregularization.\n\nThe log-bilinear model is another example of an exponential language model.\n\nNeural language models (or \"Continuous space language models\") use continuous representations or embeddings of words to make their predictions. These models make use of Neural networks . \n\nContinuous space embeddings help to alleviate the curse of dimensionality in language modeling: as language models are trained on larger and larger texts, the number of unique words (the vocabulary) increases and the number of possible sequences of words increases exponentially with the size of the vocabulary, causing a data sparsity problem because for each of the exponentially many sequences, statistics are needed to properly estimate probabilities . Neural networks avoid this problem by representing words in a distributed way, as non-linear combinations of weights in a neural net. The neural net architecture might be feed-forward or recurrent .\n\nTypically, neural net language models are constructed and trained as probabilistic classifiers that learn to predict a probability distribution\n\nI.e., the network is trained to predict a probability distribution over the vocabulary, given some linguistic context. This is done using standard neural net training algorithms such as stochastic gradient descent with backpropagation. The context might be a fixed-size window of previous words, so that the network predicts\n\nfrom a feature vector representing the previous words. Another option is to use \"future\" words as well as \"past\" words as features, so that the estimated probability is\n\nA third option, that allows faster training, is to invert the previous problem and make a neural network learn the context, given a word. One then maximizes the log-probability\n\nThis is called a skip-gram language model, and is the basis of the popular word2vec program.\n\nInstead of using neural net language models to produce actual probabilities, it is common to instead use the distributed representation encoded in the networks' \"hidden\" layers as representations of words; each word is then mapped onto an -dimensional real vector called the word embedding, where is the size of the layer just before the output layer. The representations in skip-gram models have the distinct characteristic that they model semantic relations between words as linear combinations, capturing a form of compositionality. For example, in some such models, if is the function that maps a word to its -d vector representation, then\n\nwhere ≈ is made precise by stipulating that its right-hand side must be the nearest neighbor of the value of the left-hand side.\n\nA positional language model is one that describes the probability of given words occurring close to one another in a text, not necessarily immediately adjacent. Similarly, bag-of-concepts models leverage on the semantics associated with multi-word expressions such as \"buy_christmas_present\", even when they are used in information-rich sentences like \"today I bought a lot of very nice Christmas presents\".\n\n\n\n", "id": "1911810", "title": "Language model"}
{"url": "https://en.wikipedia.org/wiki?curid=13911197", "text": "Katz's back-off model\n\nKatz back-off is a generative \"n\"-gram language model that estimates the conditional probability of a word given its history in the \"n\"-gram. It accomplishes this estimation by \"backing-off\" to models with smaller histories under certain conditions. By doing so, the model with the most reliable information about a given history is used to provide the better results.\n\nThe model was introduced in 1987 by Slava M. Katz. Prior to that, n-gram language models were constructed by training individual models for different n-gram orders using maximum likelihood estimation and then interpolating them together.\n\nThe equation for Katz's back-off model is: \n\nwhere\n\nEssentially, this means that if the \"n\"-gram has been seen more than \"k\" times in training, the conditional probability of a word given its history is proportional to the maximum likelihood estimate of that \"n\"-gram. Otherwise, the conditional probability is equal to the back-off conditional probability of the (\"n\" − 1)-gram.\n\nThe more difficult part is determining the values for \"k\", \"d\" and \"α\".\n\nformula_2 is the least important of the parameters. It is usually chosen to be 0. However, empirical testing may find better values for k.\n\nformula_3 is typically the amount of discounting found by Good–Turing estimation. In other words, if Good–Turing estimates formula_4 as formula_5, then formula_6\n\nTo compute formula_7, it is useful to first define a quantity β, which is the left-over probability mass for the (\"n\" − 1)-gram:\n\nThen the back-off weight, α, is computed as follows:\n\nThe above formula only applies if there is data for the \"(\"n\" − 1)-gram\". If not, the algorithm skips N-1 entirely and uses the Katz estimate for N-2. (and so on until an N-gram with data is found)\n\nThis model generally works well in practice, but fails in some circumstances. For example, suppose that the bigram \"a b\" and the unigram \"c\" are very common, but the trigram \"a b c\" is never seen. Since \"a b\" and \"c\" are very common, it may be significant (that is, not due to chance) that \"a b c\" is never seen. Perhaps it's not allowed by the rules of the grammar. Instead of assigning a more appropriate value of 0, the method will back off to the bigram and estimate \"P\"(\"c\" | \"b\"), which may be too high.\n", "id": "13911197", "title": "Katz's back-off model"}
{"url": "https://en.wikipedia.org/wiki?curid=17110513", "text": "Additive smoothing\n\nIn statistics, additive smoothing, also called Laplace smoothing (not to be confused with Laplacian smoothing), or Lidstone smoothing, is a technique used to smooth categorical data. Given an observation x = (\"x\", …, \"x\") from a multinomial distribution with \"N\" trials and parameter vector θ = (\"θ\", …, \"θ\"), a \"smoothed\" version of the data gives the estimator:\n\nwhere the pseudocount \"α\" > 0 is the smoothing parameter (\"α\" = 0 corresponds to no smoothing). Additive smoothing is a type of shrinkage estimator, as the resulting estimate will be between the empirical estimate \"x\" / \"N\", and the uniform probability 1/\"d\". Using Laplace's rule of succession, some authors have argued that \"α\" should be 1 (in which case the term add-one smoothing is also used), though in practice a smaller value is typically chosen.\n\nFrom a Bayesian point of view, this corresponds to the expected value of the posterior distribution, using a symmetric Dirichlet distribution with parameter \"α\" as a prior. In the special case where the number of categories is 2, this is equivalent to using a Beta distribution as the conjugate prior for the parameters of Binomial distribution.\n\nLaplace came up with this smoothing technique when he tried to estimate the chance that the sun will rise tomorrow. His rationale was that even given a large sample of days with the rising sun, we still can not be completely sure that the sun will still rise tomorrow (known as the sunrise problem).\n\nA pseudocount is an amount (not generally an integer, despite its name) added to the number of observed cases in order to change the expected probability in a model of those data, when not known to be zero. Depending on the prior knowledge, which is sometimes a subjective value, a pseudocount may have any non-negative finite value. It may only be zero (or the possibility ignored) if impossible by definition, such as the possibility of a decimal digit of pi being a letter, or a physical possibility that would be rejected and so not counted, such as a computer printing a letter when a valid program for pi is run, or excluded and not counted because of no interest, such as if only interested in the zeros and ones. Generally, there is also a possibility that no value may be computable or observable in a finite time (see Turing's halting problem). But at least one possibility must have a non-zero pseudocount, otherwise no prediction could be computed before the first observation. The relative values of pseudocounts represent the relative prior expected probabilities of their possibilities. The sum of the pseudocounts, which may be very large, represents the estimated weight of the prior knowledge compared with all the actual observations (one for each) when determining the expected probability.\n\nIn any observed data set or sample there is the possibility, especially with low-probability events and with small data sets, of a possible event not occurring. Its observed frequency is therefore zero, apparently implying a probability of zero. This is an oversimplification, which is inaccurate and often unhelpful, particularly in probability-based machine learning techniques such as artificial neural networks and hidden Markov models. By artificially adjusting the probability of rare (but not impossible) events so those probabilities are not exactly zero, zero-frequency problems are avoided. Also see Cromwell's rule. \n\nThe simplest approach is to add \"one\" to each observed number of events including the zero-count possibilities. This is sometimes called Laplace's Rule of Succession. \nThis approach is equivalent to assuming a uniform prior distribution over the probabilities for each possible event (spanning the simplex where each probability is between 0 and 1, and they all sum to 1).\n\nUsing the Jeffreys prior approach, a pseudocount of one half should be added to each possible outcome.\n\nPseudocounts should be set to one only when there is no prior knowledge at all — see the principle of indifference. However, given appropriate prior knowledge, the sum should be adjusted in proportion to the expectation that the prior probabilities should be considered correct, despite evidence to the contrary — see further analysis. Higher values are appropriate inasmuch as there is prior knowledge of the true values (for a mint condition coin, say); lower values inasmuch as there is prior knowledge that there is probable bias, but of unknown degree (for a bent coin, say).\n\nA more complex approach is to estimate the probability of the events from other factors and adjust accordingly.\n\nOften you are testing the bias of an unknown trial population against a control population with known parameters (incidence rates) μ = (\"μ\", …, \"μ\").\nIn this case the uniform probability 1/\"d\" should be replaced by the known incidence rate of the control population \"μ\" to calculate the smoothed estimator :\n\nAs a consistency check, if the empirical estimator happens to equal the incidence rate, i.e. \"μ\" = \"x\" / \"N\", the smoothed estimator is independent of \"α\" and also equals the incidence rate.\n\nAdditive smoothing is commonly a component of naive Bayes classifiers.\n\nIn a bag of words model of natural language processing and information retrieval, the data consists of the number of occurrences of each word in a document. Additive smoothing allows the assignment of non-zero probabilities to words which do not occur in the sample. Recent studies have proven that additive smoothing is more effective than other probability smoothing methods in several retrieval tasks such as language-model-based pseudo-relevance feedback and recommender systems. \n\n\n", "id": "17110513", "title": "Additive smoothing"}
{"url": "https://en.wikipedia.org/wiki?curid=13125238", "text": "Writer invariant\n\nWriter invariant, also called authorial invariant or author's invariant, is a property of a text which is invariant of its author, that is, it will be similar in all texts of a given author and different in texts of different authors. It can be used to find plagiarism or discover who is real author of anonymously published text. Writer invariant is also an author's pattern of writing a letter in handwritten text recognition.\n\nWhile it is generally recognised that writer invariants exist, it is not agreed what properties of a text should be used. Among the first ones used was distribution of word lengths; other proposed invariants include average sentence length, average word length, noun, verb or adjective usage frequency, vocabulary richness, and frequency of function words, or specific function words.\n\nOf these, average sentence lengths can be very similar in works of different authors or vary significantly even within a single work; average word lengths likewise turn out to be very similar in works of different authors. Analysis of function words shows promise because they are used by authors unconsciously.\n\n", "id": "13125238", "title": "Writer invariant"}
{"url": "https://en.wikipedia.org/wiki?curid=7498882", "text": "Sinkov statistic\n\nSinkov statistics, also known as log-weight statistics, is a specialized field of statistics that was developed by Abraham Sinkov, while working for the small Signal Intelligence Service organization, the primary mission of which was to compile codes and ciphers for use by the U.S. Army. The mathematics involved include modular arithmetic, a bit of number theory, some linear algebra of two dimensions with matrices, some combinatorics, and a little statistics. \n\nSinkov did not explain the theoretical underpinnings of his statistics, or characterized its distribution, nor did he give a decision procedure for accepting or rejecting candidate plaintexts on the basis of their S scores. The situation becomes more difficult when comparing strings of different lengths because Sinkov does not explain how the distribution of his statistics changes with length, especially when applied to higher-order grams. As for how to accept or reject a candidate plaintext, Sinkov simply said to try all possibilities and to pick the one with the highest S value. Although the procedure works for some applications, it is inadequate for applications that require on-line decisions. Furthermore, it is desirable to have a meaningful interpretation of the S values. \n", "id": "7498882", "title": "Sinkov statistic"}
{"url": "https://en.wikipedia.org/wiki?curid=17378052", "text": "Noisy channel model\n\nThe noisy channel model is a framework used in spell checkers,\nquestion answering, speech recognition, and machine translation.\nIn this model, the goal is to find the intended word given a word where the\nletters have been scrambled in some manner.\n\nGiven an alphabet formula_1, let formula_2 be the set\nof all finite strings over formula_1. Let the dictionary\nformula_4 of valid words be some subset of formula_2, i.e.,\nformula_6.\n\nThe noisy channel is the matrix\n\nwhere formula_8 is the intended word and formula_9\nis the scrambled word that was actually received.\n\nConsider the English alphabet\nformula_10. Some subset\nformula_6 makes up the dictionary of valid English\nwords.\n\nThere are several mistakes that may occur while typing, including:\n\n\nTo construct the noisy channel matrix formula_12, we must consider\nthe probability of each mistake, given the intended word\n(formula_13 for all formula_8 and\nformula_9). These probabilities may be gathered, for\nexample, by considering the Levenshtein distance between formula_16\nand formula_17 or by comparing the draft of an essay with one that has\nbeen manually edited for spelling.\n\nThe goal of the noisy channel model is to find the intended word given the\nscrambled word that was received. The decision function\nformula_18 is a function that, given a scrambled word, returns\nthe intended word.\n\nMethods of constructing a decision function include the\nmaximum likelihood rule, the\nmaximum a posteriori rule, and the\nminimum distance rule.\n\nIn some cases, it may be better to accept the scrambled word as the intended\nword rather than attempt to find an intended word in the dictionary. For\nexample, the word \"schönfinkeling\" may not be in the dictionary, but might\nin fact be the intended word.\n\n", "id": "17378052", "title": "Noisy channel model"}
{"url": "https://en.wikipedia.org/wiki?curid=2284104", "text": "Dissociated press\n\nDissociated press is a parody generator (a computer program that generates nonsensical text). The generated text is based on another text using the Markov chain technique. The name is a play on \"Associated Press\".\n\nAn implementation of the algorithm is available in Emacs. Another implementation is available as a Perl module in CPAN, Games::Dissociate.\n\nThe algorithm starts by printing a number of consecutive words (or letters) from the source text. Then it searches the source text for an occurrence of the few last words or letters printed out so far. If multiple occurrences are found, it picks a random one, and proceeds with printing the text following the chosen occurrence. After a predetermined length of text was printed out, the search procedure is repeated for the newly printed ending.\n\nConsidering that words and phrases tend to appear in specific grammatical contexts, the resulting text usually seems correct grammatically, and if the source text is uniform in style, the result appears to be of similar style and subject, and takes some effort on the reader's side to recognize as not genuine. Still, the randomness of the assembly process deprives it of any logical flow - the loosely related parts are connected in a nonsensical way, creating a humorously abstract, random result.\n\nHere is a short example of word-based Dissociated Press applied to the Jargon File:\n\nHere is a short example of letter-based Dissociated Press applied to the same source:\n\nThe dissociated press algorithm is described in HAKMEM (1972) Item #176. The name \"dissociated press\" is first known to have been associated with the Emacs implementation.\n\nBrian Hayes discussed a Travesty algorithm in \"Scientific American\" in November 1983. The article provided a garbled William Faulkner passage:\nHugh Kenner and Joseph O'Rourke of The Johns Hopkins University discussed their frequency table-based Travesty generator for microcomputers in \"BYTE\" in November 1984. The article included the Turbo Pascal source for two versions of the generator, one using Hayes' algorithm and another using Claude Shannon's Hellbat algorithm. Murray Lesser offered a compiled BASIC version in the magazine in July 1985, in September 1985 Peter Wayner offered a version that used tree data structures instead of frequency tables, and in December 1985 Neil J. Rubenking offered a version written in Turbo Pascal that stored frequency information in a B-tree.\n\n\n", "id": "2284104", "title": "Dissociated press"}
{"url": "https://en.wikipedia.org/wiki?curid=87030", "text": "Glottochronology\n\nGlottochronology (from Attic Greek γλῶττα \"tongue, language\" and χρóνος \"time\") is the part of lexicostatistics dealing with the chronological relationship between languages.\n\nThe idea was developed by Morris Swadesh under two assumptions: first; that there does, indeed, exist a relatively stable \"basic vocabulary\" (referred to as \"Swadesh lists\") in all languages of the world; and, second; that any replacements happen in a way analogous to radioactive decay, in constant percentages per time elapsed. Meanwhile, there exist many different methods, partly extensions of the Swadesh method, now more and more under the biological assumptions of replacements in genes. However, Swadesh's technique is so well known that, for many people, 'glottochronology' refers to it alone.\n\nThe original method presumed that the core vocabulary of a language is replaced at a constant (or constant average) rate across all languages and cultures, and can therefore be used to measure the passage of time. The process makes use of a list of lexical terms. Lists were compiled by Morris Swadesh and assumed to be resistant against borrowing (originally designed in 1952 as a list of 200 items; however, the refined 100 word list in Swadesh (1955) is much more common among modern day linguists). This core vocabulary was designed to encompass concepts common to every human language (such as personal pronouns, body parts, heavenly bodies, verbs of basic actions, numerals 'one' and 'two', etc.), eliminating concepts that are specific to a particular culture or time. It has been found that this ideal is not in fact possible and that the meaning set may need to be tailored to the languages being compared. Many alternative word lists have been compiled by other linguists, often using fewer meaning slots.\n\nThe percentage of cognates (words that have a common origin) in these word lists is then measured. The larger the percentage of cognates, the more recently the two languages being compared are presumed to have separated.\n\nRobert Lees obtained a value for the \"glottochronological constant\" (r) of words by considering the known changes in 13 pairs of languages using the 200 word list. He obtained a value of 0.805 ± 0.0176 with 90% confidence. For the 100 word list Swadesh obtained a value of 0.86, the higher value reflecting the elimination of semantically unstable words. This constant may be related to the retention rate of words by:-\n\nwhere \"L\" is the rate of replacement, ln is the logarithm to base e, and \"r\" is the glottochronological constant\n\nThe basic formula of glottochronology in its shortest form is:-\n\nwhere \"t\" = a given period of time from one stage of the language to another, \"c\" = proportion of wordlist items retained at the end of that period, and \"L\" = rate of replacement for that word list.\n\nOne can also therefore formulate that:\n\nBy testing historically verifiable cases where we have knowledge of \"t\" through non-linguistic data (e. g. the approximate distance from Classical Latin to modern Romance languages), Swadesh arrived at the empirical value of approximately 0.14 for \"L\" (meaning that the rate of replacement constitutes around 14 words from the 100-wordlist per millennium).\n\nGlottochronology was found to work in the case of Indo-European, accounting for 87% of the variance. It is also postulated to work for Hamito-Semitic (Fleming 1973), Chinese (Munro 1978) and Amerind (Stark 1973; Baumhoff and Olmsted 1963). For the latter, correlations have been obtained with radiocarbon dating and blood groups as well as archaeology.\nNote that the approach of Gray and Atkinson, as they say, has nothing to do with \"glottochronology\".\n\nThe concept of language change is old, and its history is reviewed in Hymes (1973) and Wells (1973). Glottochronology itself dates back to the mid-20th century. An introduction to the subject is given in Embleton (1986) and in McMahon and McMahon (2005).\n\nGlottochronology has been controversial ever since, partly owing to issues of accuracy, as well as the question of whether its basis is sound (see e.g. Bergsland 1958; Bergsland and Vogt 1962; Fodor 1961; Chretien 1962; Guy 1980). These concerns have been addressed by Dobson et al. (1972), Dyen (1973) and Kruskal, Dyen and Black (1973). The assumption of a single-word replacement rate can distort the divergence-time estimate when borrowed words are included (Thomason and Kaufman 1988). Chrétien purported to disprove the mathematics of the Swadesh-model. At a conference at Yale in 1971 his criticisms were shown to be invalid. See the published proceedings under Dyen (1973) The same conference saw the application of the theory to Creole language (Wittmann 1973).\nAn overview of recent arguments can be obtained from the papers of a conference held at the McDonald Institute in 2000. These presentations vary from \"Why linguists don't do dates\" to the one by Starostin discussed above.\nSince its original inception, glottochronology has been rejected by many linguists, mostly Indo-Europeanists of the school of the traditional comparative method. Criticisms have been answered in particular around three points of discussion.\n\n\nSomewhere in between the original concept of Swadesh and the rejection of glottochronology in its entirety lies the idea that glottochronology as a formal method of linguistic analysis becomes valid with the help of several important modifications. Thus, inhomogeneities in the replacement rate were dealt with by Van der Merwe (1966) by splitting the word list into classes each with their own rate, while Dyen, James and Cole (1967) allowed each meaning to have its own rate. Simultaneous estimation of divergence time and replacement rate was studied by Kruskal, Dyen and Black.\n\nBrainard (1970) allowed for chance cognation and drift effects was introduced by Gleason (1959). Sankoff (1973) suggested introducing a borrowing parameter and allowed synonyms.\n\nA combination of these various improvements is given in Sankoff's \"Fully Parameterised Lexicostatistics\". In 1972 Sankoff in a biological context developed a model of genetic divergence of populations. Embleton (1981) derives a simplified version of this in a linguistic context. She carries out a number of simulations using this which are shown to give good results.\n\nImprovements in statistical methodology related to a completely different branch of science – changes in DNA over time – have sparked a recent renewed interest. These methods are more robust than the earlier ones because they calibrate points on the tree with known historical events and smooth the rates of change across these. As such, they no longer require the assumption of a constant rate of change (Gray & Atkinson 2003).\n\nAnother attempt to introduce such modifications was performed by the Russian linguist Sergei Starostin, who had proposed that\n\n\nThe resulting formula, taking into account both the time dependence and the individual stability quotients, looks as follows:\n\nIn this formula, −\"Lc\" reflects the gradual slowing down of the replacement process due to different individual rates (the less stable elements are the first and the quickest to be replaced), whereas the square root represents the reverse trend – acceleration of replacement as items in the original wordlist \"age\" and become more prone to shifting their meaning. The formula is obviously more complicated than Swadesh's original one, but, as shown in Starostin's work, yields more credible results than the former (and more or less agrees with all the cases of language separation that can be confirmed by historical knowledge). On the other hand, it shows that glottochronology can really only be used as a serious scientific tool on language families the historical phonology of which has been meticulously elaborated (at least to the point of being able to clearly distinguish between cognates and loanwords).\n\nThe McDonald Institute hosted a conference on the issue of time-depth estimation in 2000. The published papers give an idea of the views on glottochronology at that time. These vary from \"Why linguists don't do dates\" to the one by Starostin discussed above. Note that in the referenced Gray and Atkinson paper, they hold that their methods can not be called \"glottochronology\", by confining this term to its original method.\n\n\n", "id": "87030", "title": "Glottochronology"}
{"url": "https://en.wikipedia.org/wiki?curid=17878029", "text": "Markov information source\n\nIn mathematics, a Markov information source, or simply, a Markov source, is an information source whose underlying dynamics are given by a stationary finite Markov chain.\n\nAn information source is a sequence of random variables ranging over a finite alphabet Γ, having a stationary distribution.\n\nA Markov information source is then a (stationary) Markov chain \"M\", together with a function\n\nthat maps states \"S\" in the Markov chain to letters in the alphabet Γ.\n\nA unifilar Markov source is a Markov source for which the values formula_2 are distinct whenever each of the states formula_3 are reachable, in one step, from a common prior state. Unifilar sources are notable in that many of their properties are far more easily analyzed, as compared to the general case.\n\nMarkov sources are commonly used in communication theory, as a model of a transmitter. Markov sources also occur in natural language processing, where they are used to represent hidden meaning in a text. Given the output of a Markov source, whose underlying Markov chain is unknown, the task of solving for the underlying chain is undertaken by the techniques of hidden Markov models, such as the Viterbi algorithm.\n\n\n", "id": "17878029", "title": "Markov information source"}
{"url": "https://en.wikipedia.org/wiki?curid=13805160", "text": "Stochastic grammar\n\nA stochastic grammar (statistical grammar) is a grammar framework with a probabilistic notion of grammaticality:\n\nStatistical natural language processing uses stochastic, probabilistic and statistical methods, especially to resolve difficulties that arise because longer sentences are highly ambiguous when processed with realistic grammars, yielding thousands or millions of possible analyses. Methods for disambiguation often involve the use of corpora and Markov models. \"A probabilistic model consists of a non-probabilistic model plus some numerical quantities; it is not true that probabilistic models are inherently simpler or less structural than non-probabilistic models.\"\n\nThe technology for statistical NLP comes mainly from machine learning and data mining, both of which are fields of artificial intelligence that involve learning from data.\n\nA probabilistic method for rhyme detection is implemented by Hirjee & Brown in their study in 2013 to find internal and imperfect rhyme pairs in rap lyrics. The concept is adapted from a sequence alignment technique using BLOSUM (BLOcks SUbstitution Matrix). They were able to detect rhymes undetectable by non-probabilistic model.\n\n\n", "id": "13805160", "title": "Stochastic grammar"}
{"url": "https://en.wikipedia.org/wiki?curid=2061486", "text": "Trigram tagger\n\nIn computational linguistics, a trigram tagger is a statistical method for automatically identifying words as being nouns, verbs, adjectives, adverbs, etc. based on second order Markov models that consider triples of consecutive words. It is trained on a text corpus as a method to predict the next word, taking the product of the probabilities of unigram, bigram and trigram. In speech recognition, algorithms utilizing trigram-tagger score better than those algorithms utilizing IIMM tagger but less well than Net tagger.\n\nThe description of the trigram tagger is provided by Brants (2000). \n\n", "id": "2061486", "title": "Trigram tagger"}
{"url": "https://en.wikipedia.org/wiki?curid=4558491", "text": "Statistical machine translation\n\nStatistical machine translation (SMT) is a machine translation paradigm where translations are generated on the basis of statistical models whose parameters are derived from the analysis of bilingual text corpora. The statistical approach contrasts with the rule-based approaches to machine translation as well as with example-based machine translation.\n\nThe first ideas of statistical machine translation were introduced by Warren Weaver in 1949, including the ideas of applying Claude Shannon's information theory. Statistical machine translation was re-introduced in the late 1980s and early 1990s by researchers at IBM's Thomas J. Watson Research Center and has contributed to the significant resurgence in interest in machine translation in recent years. Nowadays it is by far the most widely studied machine translation method.\n\nThe idea behind statistical machine translation comes from information theory. A document is translated according to the probability distribution formula_1 that a string formula_2 in the target language (for example, English) is the translation of a string formula_3 in the source language (for example, French).\n\nThe problem of modeling the probability distribution formula_1 has been approached in a number of ways. One approach which lends itself well to computer implementation is to apply Bayes Theorem, that is formula_5, where the translation model formula_6 is the probability that the source string is the translation of the target string, and the language model formula_7 is the probability of seeing that target language string. This decomposition is attractive as it splits the problem into two subproblems. Finding the best translation formula_8 is done by picking up the one that gives the highest probability:\n\nFor a rigorous implementation of this one would have to perform an exhaustive search by going through all strings formula_10 in the native language. Performing the search efficiently is the work of a machine translation decoder that uses the foreign string, heuristics and other methods to limit the search space and at the same time keeping acceptable quality. This trade-off between quality and time usage can also be found in speech recognition.\n\nAs the translation systems are not able to store all native strings and their translations, a document is typically translated sentence by sentence, but even this is not enough. Language models are typically approximated by smoothed \"n\"-gram models, and similar approaches have been applied to translation models, but there is additional complexity due to different sentence lengths and word orders in the languages.\n\nThe statistical translation models were initially word based (Models 1-5 from IBM Hidden Markov model from Stephan Vogel and Model 6 from Franz-Joseph Och), but significant advances were made with the introduction of phrase based models. Recent work has incorporated syntax or quasi-syntactic structures.\n\nThe most frequently cited benefits of statistical machine translation over rule-based approach are:\n\n\n\nIn word-based translation, the fundamental unit of translation is a word in some natural language. Typically, the number of words in translated sentences are different, because of compound words, morphology and idioms. The ratio of the lengths of sequences of translated words is called fertility, which tells how many foreign words each native word produces. Necessarily it is assumed by information theory that each covers the same concept. In practice this is not really true. For example, the English word \"corner\" can be translated in Spanish by either \"rincón\" or \"esquina\", depending on whether it is to mean its internal or external angle.\n\nSimple word-based translation can't translate between languages with different fertility. Word-based translation systems can relatively simply be made to cope with high fertility, but they could map a single word to multiple words, but not the other way about. For example, if we were translating from English to French, each word in English could produce any number of French words— sometimes none at all. But there's no way to group two English words producing a single French word.\n\nAn example of a word-based translation system is the freely available GIZA++ package (GPLed), which includes the training program for IBM models and HMM model and Model 6.\n\nThe word-based translation is not widely used today; phrase-based systems are more common. Most phrase-based system are still using GIZA++ to align the corpus. The alignments are used to extract phrases or deduce syntax rules. And matching words in bi-text is still a problem actively discussed in the community. Because of the predominance of GIZA++, there are now several distributed implementations of it online.\n\nIn phrase-based translation, the aim is to reduce the restrictions of word-based translation by translating whole sequences of words, where the lengths may differ. The sequences of words are called blocks or phrases, but typically are not linguistic phrases, but phrasemes found using statistical methods from corpora. It has been shown that restricting the phrases to linguistic phrases (syntactically motivated groups of words, see syntactic categories) decreases the quality of translation.\n\nSyntax-based translation is based on the idea of translating syntactic units, rather than single words or strings of words (as in phrase-based MT), i.e. (partial) parse trees of sentences/utterances. The idea of syntax-based translation is quite old in MT, though its statistical counterpart did not take off until the advent of strong stochastic parsers in the 1990s. Examples of this approach include DOP-based MT and, more recently, synchronous context-free grammars.\n\nHierarchical phrase-based translation combines the strengths of phrase-based and syntax-based translation. It uses synchronous context-free grammar rules, but the grammars may be constructed by an extension of methods for phrase-based translation without reference to linguistically motivated syntactic constituents. This idea was first introduced in Chiang's Hiero system (2005).\n\nProblems that statistical machine translation have to deal with include:\n\nIn parallel corpora single sentences in one language can be found translated into several sentences in the other and vice versa. Sentence aligning can be performed through the Gale-Church alignment algorithm.\n\nSentence alignment is usually either provided by the corpus or obtained by aforementioned Gale-Church alignment algorithm. To learn e.g. the translation model, however, we need to know which words align in a source-target sentence pair. Solutions are the IBM-Models or the HMM-approach.\n\nReal-world training sets may override translations of, say, proper nouns. An example would be that \"I took the train to Berlin\" gets mis-translated as \"I took the train to Paris\" due to an abundance of \"train to Paris\" in the training set\n\nDepending on the corpora used, idioms may not translate \"idiomatically\". For example, using Canadian Hansard as the bilingual corpus, \"hear\" may almost invariably be translated to \"Bravo!\" since in Parliament \"Hear, Hear!\" becomes \"Bravo!\".\n\nWord order in languages differ. Some classification can be done by naming the typical order of subject (S), verb (V) and object (O) in a sentence and one can talk, for instance, of SVO or VSO languages. There are also additional differences in word orders, for instance, where modifiers for nouns are located, or where the same words are used as a question or a statement.\n\nIn speech recognition, the speech signal and the corresponding textual representation can be mapped to each other in blocks in order. This is not always the case with the same text in two languages. For SMT, the machine translator can only manage small sequences of words, and word order has to be thought of by the program designer. Attempts at solutions have included re-ordering models, where a distribution of location changes for each item of translation is guessed from aligned bi-text. Different location changes can be ranked with the help of the language model and the best can be selected.\n\nRecently, Skype voice communicator started testing speech translation. However, machine translation is following technological trends in speech at a slower rate than speech recognition. In fact, some ideas from speech recognition research have been adopted by statistical machine translation.\n\nSMT systems typically store different word forms as separate symbols without any relation to each other and word forms \nor phrases that were not in the training data cannot be translated. This might be because of the lack of training data, changes in the human domain where the system is used, or differences in morphology.\n\nThe rapid increase in the computing power of tablets and smartphones, combined with the wide availability of high-speed mobile Internet access, makes it possible for them to run machine translation systems. Experimental systems have already been developed to assist foreign health workers in developing countries. Similar systems are already available on the market. For example, Apple’s iOS 8 allows users to dictate text messages. A built-in ASR system recognizes the speech and the recognition results are edited by an online system.\n\nProjects such as Universal Speech Translation Advanced Research (U-STAR1, a continuation of the A-STAR project) and EU-BRIDGE2 are currently conducting research in translation of full sentences recognized from spoken language. Recent years have seen a growing interest in combining speech recognition, machine translation and speech synthesis. To achieve speech-to-speech translation, n-best lists are passed from the ASR to the statistical machine translation system. However, combining those systems raises problems of how to achieve sentence segmentation, de-normalization and punctuation prediction needed for quality translations.\n\n\n", "id": "4558491", "title": "Statistical machine translation"}
{"url": "https://en.wikipedia.org/wiki?curid=27900233", "text": "Maximum-entropy Markov model\n\nIn machine learning, a maximum-entropy Markov model (MEMM), or conditional Markov model (CMM), is a graphical model for sequence labeling that combines features of hidden Markov models (HMMs) and maximum entropy (MaxEnt) models. An MEMM is a discriminative model that extends a standard maximum entropy classifier by assuming that the unknown values to be learnt are connected in a Markov chain rather than being conditionally independent of each other. MEMMs find applications in natural language processing, specifically in part-of-speech tagging and information extraction.\n\nSuppose we have a sequence of observations formula_1 that we seek to tag with the labels formula_2that maximize the conditional probability formula_3. In a MEMM, this probability is factored into Markov transition probabilities, where the probability of transitioning to a particular label depends only on the observation at that position and the previous position's label:\nEach of these transition probabilities come from the same general distribution formula_5. For each possible label value of the previous label formula_6, the probability of a certain label formula_7 is modeled in the same way as a maximum entropy classifier:\nHere, the formula_9 are real-valued or categorical feature-functions, and formula_10 is a normalization term ensuring that the distribution sums to one. This form for the distribution corresponds to the maximum entropy probability distribution satisfying the constraint that the empirical expectation for the feature is equal to the expectation given the model:\nThe parameters formula_12 can be estimated using generalized iterative scaling. Furthermore, a variant of the Baum–Welch algorithm, which is used for training HMMs, can be used to estimate parameters when training data has incomplete or missing labels.\n\nThe optimal state sequence formula_2 can be found using a very similar Viterbi algorithm to the one used for HMMs. The dynamic program uses the forward probability:\n\nAn advantage of MEMMs rather than HMMs for sequence tagging is that they offer increased freedom in choosing features to represent observations. In sequence tagging situations, it is useful to use domain knowledge to design special-purpose features. In the original paper introducing MEMMs, the authors write that \"when trying to extract previously unseen company names from a newswire article, the identity of a word alone is not very predictive; however, knowing that the word is capitalized, that is a noun, that it is used in an appositive, and that it appears near the top of the article would all be quite predictive (in conjunction with the context provided by the state-transition structure).\" Useful sequence tagging features, such as these, are often non-independent. Maximum entropy models do not assume independence between features, but generative observation models used in HMMs do. Therefore, MEMMs allow the user to specify lots of correlated, but informative features.\n\nAnother advantage of MEMMs versus HMMs and conditional random fields (CRFs) is that training can be considerably more efficient. In HMMs and CRFs, one needs to use some version of the forward–backward algorithm as an inner loop in training. However, in MEMMs, estimating the parameters of the maximum-entropy distributions used for the transition probabilities can be done for each transition distribution in isolation.\n\nA drawback of MEMMs is that they potentially suffer from the \"label bias problem,\" where states with low-entropy transition distributions \"effectively ignore their observations.\" Conditional random fields were designed to overcome this weakness,\nwhich had already been recognised in the context of neural network-based Markov models in the early 1990s.\nAnother source of label bias is that training is always done with respect to known previous tags, so the model struggles at test time when there is uncertainty in the previous tag.\n", "id": "27900233", "title": "Maximum-entropy Markov model"}
{"url": "https://en.wikipedia.org/wiki?curid=28082011", "text": "Pachinko allocation\n\nIn machine learning and natural language processing, the pachinko allocation model (PAM) is a topic model. Topic models are a suite of algorithms to uncover the hidden thematic structure of a collection of documents. The algorithm improves upon earlier topic models such as latent Dirichlet allocation (LDA) by modeling correlations between topics in addition to the word correlations which constitute topics. PAM provides more ﬂexibility and greater expressive power\nthan latent Dirichlet allocation. While first described and implemented in the context of natural language processing, the algorithm may have applications in other fields such as bioinformatics. The\nmodel is named for pachinko machines—a game popular in Japan, in which metal balls bounce down around\na complex collection of pins until they land in various\nbins at the bottom.\n\nPachinko allocation was first described by Wei Li and Andrew McCallum in 2006.\nThe idea was extended with hierarchical Pachinko allocation by Li, McCallum, and David Mimno in 2007. In 2007, McCallum and his colleagues proposed a nonparametric Bayesian prior for PAM based\non a variant of the hierarchical Dirichlet process (HDP). The algorithm has been implemented in the MALLET software package published by McCallum's group at the University of Massachusetts Amherst.\n\nPAM connects words in V and topics in T\nwith an arbitrary Directed Acyclic Graph (DAG), where topic nodes occupy the\ninterior levels and the leaves are words. \n\nThe probability of generating a whole corpus\nis the product of the probability for every document:\n\nformula_1\n\n\n", "id": "28082011", "title": "Pachinko allocation"}
{"url": "https://en.wikipedia.org/wiki?curid=28934119", "text": "Topic model\n\nIn machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.\n\nTopic models are also referred to as probabilistic topic models, which refers to statistic algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics.\n\nAn early topic model was described by Papadimitriou, Raghavan, Tamaki and Vempala in 1998. Another one, called probabilistic latent semantic analysis (PLSA), was created by Thomas Hofmann in 1999. Latent Dirichlet allocation (LDA), perhaps the most common topic model currently in use, is a generalization of PLSA. Developed by David Blei, Andrew Ng, and Michael I. Jordan in 2002, LDA introduces sparse Dirichlet prior distributions over document-topic and topic-word distributions, encoding the intuition that documents cover a small number of topics and that topics often use a small number of words. Other topic models are generally extensions on LDA, such as Pachinko allocation, which improves on LDA by modeling correlations between topics in addition to the word correlations which constitute topics.\n\nTopic models can include context information such as timestamps, authorship information or geographical coordinates associated with documents. Additionally, network information (such as social networks between authors) can be modelled.\n\nApproaches for temporal information include Block and Newman's determination the temporal dynamics of topics in the \"Pennsylvania Gazette\" during 1728–1800. Griffiths & Steyvers use topic modeling on abstract from the journal \"PNAS\" to identify topics that rose or fell in popularity from 1991 to 2001. Nelson has been analyzing change in topics over time in the \"Richmond Times-Dispatch\" to understand social and political changes and continuities in Richmond during the American Civil War. Yang, Torget and Mihalcea applied topic modeling methods to newspapers from 1829–2008. Mimno used topic modelling with 24 journals on classical philology and archaeology spanning 150 years to look at how topics in the journals change over time and how the journals become more different or similar over time.\n\nYin et al. introduced a topic model for geographically distributed documents, where document positions are explained by latent regions which are detected during inference.\n\nChang and Blei included network information between linked documents in the relational topic model, which allows to model links between websites.\n\nThe author-topic model by Rosen-Zvi et al. models the topics associated with authors of documents to improve the topic detection for documents with authorship information.\n\nIn practice researchers attempt to fit appropriate model parameters to the data corpus using one of several heuristics for maximum likelihood fit. A recent survey by Blei describes this suite of algorithms.\nSeveral groups of researchers starting with Papadimitriou et al. have attempted to design algorithms with probable guarantees. Assuming that the data were actually generated by the model in question, they try to design algorithms that probably find the model that was used to create the data. Techniques used here include singular value decomposition (SVD) and the method of moments. In 2012 an algorithm based upon non-negative matrix factorization (NMF) was introduced that also generalizes to topic models with correlations among topics.\n\n\n\n\n", "id": "28934119", "title": "Topic model"}
{"url": "https://en.wikipedia.org/wiki?curid=3517732", "text": "Markovian discrimination\n\nMarkovian discrimination in spam filtering is a method used in CRM114 and other spam filters to model the statistical behaviors of spam and nonspam more accurately than in simple Bayesian methods. A simple Bayesian model of written text contains only the dictionary of legal words and their relative probabilities. A Markovian model adds the relative transition probabilities that given one word, predict what the next word will be. It is based on the theory of Markov chains by Andrey Markov, hence the name. In essence, a Bayesian filter works on single words alone, while a Markovian filter works on phrases or entire sentences.\n\nThere are two types of Markov models; the visible Markov model, and the hidden Markov model or HMM.\nThe difference is that with a visible Markov model, the current word is considered to contain the entire state of the language model, while a hidden Markov model hides the state and presumes only that the current word is probabilistically related to the actual internal state of the language.\n\nFor example, in a visible Markov model the word \"the\" should predict with accuracy the following word, while in\na hidden Markov model, the entire prior text implies the actual state and predicts the following words, but does\nnot actually guarantee that state or prediction. Since the latter case is what's encountered in spam filtering,\nhidden Markov models are almost always used. In particular, because of storage limitations, the specific type\nof hidden Markov model called a Markov random field is particularly applicable, usually with a clique size of\nbetween four and six tokens.\n\n\n", "id": "3517732", "title": "Markovian discrimination"}
{"url": "https://en.wikipedia.org/wiki?curid=29895551", "text": "Synchronous context-free grammar\n\nSynchronous context-free grammars (SynCFG or SCFG; not to be confused with stochastic CFGs) are a type of formal grammar designed for use in transfer-based machine translation. Rules in these grammars apply to two languages at the same time, capturing grammatical structures that are each other's translations.\n\nThe theory of SynCFGs borrows from syntax-directed transduction and syntax-based machine translation, modeling the reordering of clauses that occurs when translating a sentence by correspondences between phrase-structure rules in the source and target languages. Performance of SCFG-based MT systems has been found comparable with, or even better than, state-of-the-art phrase-based machine translation systems.\nSeveral algorithms exist to perform translation using SynCFGs.\n\nRules in a SynCFG are superficially similar to CFG rules, except that they specify the structure of two phrases at the same time; one in the source language (the language being translated) and one in the target language. Numeric indices indicate correspondences between non-terminals in both constituent trees. Chiang gives the Chinese/English example:\n\nThis rule indicates that an phrase can be formed in Chinese with the structure \"yu you \", where and are variables standing in for subphrases; and that the corresponding structure in English is \"have with \" where and are independently translated to English.\n\n", "id": "29895551", "title": "Synchronous context-free grammar"}
{"url": "https://en.wikipedia.org/wiki?curid=4011785", "text": "F1 score\n\nIn statistical analysis of binary classification, the F score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision \"p\" and the recall \"r\" of the test to compute the score: \"p\" is the number of correct positive results divided by the number of all positive results returned by the classifier, and \"r\" is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F score is the harmonic average of the precision and recall, where an F score reaches its best value at 1 (perfect precision and recall) and worst at 0.\nThe traditional F-measure or balanced F-score (F score) is the harmonic mean of precision and recall:\n\nThe general formula for positive real β is:\n\nThe formula in terms of Type I and type II errors:\n\nTwo other commonly used F measures are the formula_4 measure, which weighs recall higher than precision (by placing more emphasis on false negatives), and the formula_5 measure, which weighs recall lower than precision (by attenuating the influence of false negatives).\n\nThe F-measure was derived so that formula_6 \"measures the effectiveness of retrieval with respect to a user who attaches β times as much importance to recall as precision\". It is based on Van Rijsbergen's effectiveness measure\n\nTheir relationship is formula_8 where formula_9.\n\nThe F score is also known as the Sørensen–Dice coefficient or Dice similarity coefficient (DSC).\n\nThis is related to the field of binary classification where recall is often termed as Sensitivity. There are several reasons that the F score can be criticized in particular circumstances.\n\nThe F-score is often used in the field of information retrieval for measuring search, document classification, and query classification performance. Earlier works focused primarily on the F score, but with the proliferation of large scale search engines, performance goals changed to place more emphasis on either precision or recall and so formula_6 is seen in wide application.\n\nThe F-score is also used in machine learning. Note, however, that the F-measures do not take the true negatives into account, and that measures such as the Matthews correlation coefficient, Informedness or Cohen's kappa may be preferable to assess the performance of a binary classifier.\n\nThe F-score has been widely used in the natural language processing literature, such as the evaluation of named entity recognition and word segmentation.\n\nWhile the F-measure is the harmonic mean of Recall and Precision, the G-measure is the geometric mean.\n\nThis is also known as the Fowlkes–Mallows index.\n\n", "id": "4011785", "title": "F1 score"}
{"url": "https://en.wikipedia.org/wiki?curid=299329", "text": "Probabilistic context-free grammar\n\nGrammar theory to model symbol strings originated from work in computational linguistics aiming to understand the structure of natural languages. Probabilistic context free grammars (PCFGs) have been applied in probabilistic modeling of RNA structures almost 40 years after they were introduced in computational linguistics.\n\nPCFGs extend context-free grammars similar to how hidden Markov models extend regular grammars. Each production is assigned a probability. The probability of a derivation (parse) is the product of the probabilities of the productions used in that derivation. These probabilities can be viewed as parameters of the model, and for large problems it is convenient to learn these parameters via machine learning. A probabilistic grammar's validity is constrained by context of its training dataset.\n\nPCFGs have application in areas as diverse as natural language processing to the study the structure of RNA molecules and design of programming languages. Designing efficient PCFGs has to weigh factors of scalability and generality. Issues such as grammar ambiguity must be resolved. The grammar design affects results accuracy. Grammar parsing algorithms have various time and memory requirements.\n\nDerivation: The process of recursive generation of strings from a grammar.\nParsing: Finding a valid derivation using an automaton.\n\nParse Tree: The alignment of the grammar to a sequence.\n\nAn example of a parser for PCFG grammars is the pushdown automaton. The algorithm parses grammar nonterminals from left to right in a stack-like manner. This brute-force approach is not very efficient. In RNA secondary structure prediction variants of the Cocke–Younger–Kasami (CYK) algorithm provide more efficient alternatives to grammar parsing than pushdown automata. Another example of a PCFG parser is the Stanford Statistical Parser which has been trained using Treebank.\n\nSimilar to a CFG, a probabilistic context-free grammar can be defined by a quintuple:\n\nwhere \n\nPCFGs models extend context-free grammars the same way as hidden Markov models extend regular grammars.\n\nThe Inside-Outside algorithm is an analogue of the Forward-Backward algorithm. It computes the total probability of all derivations that are consistent with a given sequence, based on some PCFG. This is equivalent to the probability of the PCFG generating the sequence, and is intuitively a measure of how consistent the sequence is with the given grammar. The Inside-Outside algorithm is used in model parametrization to estimate prior frequencies observed from training sequences in the case of RNAs.\n\nDynamic programming variants of the CYK algorithm find the Viterbi parse of a RNA sequence for a PCFG model. This parse is the most likely derivation of the sequence by the given PCFG.\n\nContext-free grammars are represented as a set of rules inspired from attempts to model natural languages. The rules are absolute and have a typical syntax representation known as Backus–Naur form. The production rules consist of terminal formula_2 and non-terminal symbols and a blank formula_3 may also be used as an end point. In the production rules of CFG and PCFG the left side has only one nonterminal whereas the right side can be any string of terminal or nonterminals. In PCFG nulls are excluded. An example of a grammar:\nThis grammar can be shortened using the '|' ('or') character into:\n\nTerminals in a grammar are words and through the grammar rules a non-terminal symbol is transformed into a string of either terminals and/or non-terminals. The above grammar is read as \"beginning from a non-terminal the emission can generate either or or formula_3\".\nIts derivation is: \n\nAmbiguous grammar may result in ambiguous parsing if applied on homographs since the same word sequence can have more than one interpretation. Pun sentences such as the newspaper headline \"Iraqi Head Seeks Arms\" are an example of ambiguous parses.\n\nOne strategy of dealing with ambiguous parses (originating with grammarians as early as Pāṇini) is to add yet more rules, or prioritize them so that one rule takes precedence over others. This, however, has the drawback of proliferating the rules, often to the point where they become difficult to manage. Another difficulty is overgeneration, where unlicensed structures are also generated.\n\nProbabilistic grammars circumvent these problems by ranking various productions on frequency weights, resulting in a \"most likely\" (winner-take-all) interpretation. As usage patterns are altered in diachronic shifts, these probabilistic rules can be re-learned, thus updating the grammar.\n\nAssigning probability to production rules makes a PCFG. These probabilities are informed by observing distributions on a training set of similar composition to the language to be modeled. On most samples of broad language, probabilistic grammars where probabilities are estimated from data typically outperform hand-crafted grammars. CFGs when contrasted with PCFGs are not applicable to RNA structure prediction because while they incorporate sequence-structure relationship they lack the scoring metrics that reveal a sequence structural potential \n\nA weighted context-free grammar (WCFG) is a more general category of context-free grammar, where each production has a numeric weight associated with it. The weight of a specific parse tree in a WCFG is the product (or sum ) of all rule weights in the tree. Each rule weight is included as often as the rule is used in the tree. A special case of WCFGs are PCFGs, where the weights are (logarithms of ) probabilities.\n\nAn extended version of the CYK algorithm can be used to find the \"lightest\" (least-weight) derivation of a string given some WCFG.\n\nWhen the tree weight is the product of the rule weights, WCFGs and PCFGs can express the same set of probability distributions.\n\nEnergy minimization and PCFG provide ways of predicting RNA secondary structure with comparable performance. However structure prediction by PCFGs is scored probabilistically rather than by minimum free energy calculation. PCFG model parameters are directly derived from frequencies of different features observed in databases of RNA structures rather than by experimental \ndetermination as is the case with energy minimization methods.\n\nThe types of various structure that can be modeled by a PCFG include long range interactions, pairwise structure and other nested structures. However, pseudoknots can not be modeled. PCFGs extend CFG by assigning probabilities to each production rule. A maximum probability parse tree from the grammar implies a maximum probability structure. Since RNAs preserve their structures over their primary sequence; RNA structure prediction can be guided by combining evolutionary information from comparative sequence analysis with biophysical knowledge about a structure plausibility based on such probabilities. Also search results for structural homologs using PCFG rules are scored according to PCFG derivations probabilities. Therefore, building grammar to model the behavior of base-pairs and single-stranded regions starts with exploring features of structural multiple sequence alignment of related RNAs. \n\nThe above grammar generates a string in an outside-in fashion, that is the basepair on the furthest extremes of the terminal is derived first. So a string such as formula_9 is derived by first generating the distal 's on both sides before moving inwards:\n\nA PCFG model extendibility allows constraining structure prediction by incorporating expectations about different features of an RNA . Such expectation may reflect for example the propensity for assuming a certain structure by an RNA. However incorporation of too much information may increase PCFG space and memory complexity and it is desirable that a PCFG-based model be as simple as possible.\n\nEvery possible string a grammar generates is assigned a probability weight formula_11 given the PCFG model formula_12. It follows that the sum of all probabilities to all possible grammar productions is formula_13. The scores for each paired and unpaired residue explain likelihood for secondary structure formations. Production rules also allow scoring loop lengths as well as the order of base pair stacking hence it is possible to explore the range of all possible generations including suboptimal structures from the grammar and accept or reject structures based on score thresholds.\n\nRNA secondary structure implementations based on PCFG approaches can be utilized in :\n\nDifferent implementation of these approaches exist. For example, Pfold is used in secondary structure prediction from a group of related RNA sequences, covariance models are used in searching databases for homologous sequences and RNA annotation and classification, RNApromo, CMFinder and TEISER are used in finding stable structural motifs in RNAs.\n\nPCFG design impacts the secondary structure prediction accuracy. Any useful structure prediction probabilistic model based on PCFG has to maintain simplicity without much compromise to prediction accuracy. Too complex a model of excellent performance on a single sequence may not scale. A grammar based model should be able to:\n\nThe resulting of multiple parse trees per grammar denotes grammar ambiguity. This may be useful in revealing all possible base-pair structures for a grammar. However an optimal structure is the one where there is one and only one correspondence between the parse tree and the secondary structure.\n\nTwo types of ambiguities can be distinguished. Parse tree ambiguity and structural ambiguity. Structural ambiguity does not affect thermodynamic approaches as the optimal structure selection is always on the basis of lowest free energy scores. Parse tree ambiguity concerns the existence of multiple parse trees per sequence. Such an ambiguity can reveal all possible base-paired structures for the sequence by generating all possible parse trees then finding the optimal one. In the case of structural ambiguity multiple parse trees describe the same secondary structure. This obscures the CYK algorithm decision on finding an optimal structure as the correspondence between the parse tree and the structure is not unique. Grammar ambiguity can be checked for by the conditional-inside algorithm.\n\nA probabilistic context free grammar consists of terminal and nonterminal variables. Each feature to be modeled has a production rule that is assigned a probability estimated from a training set of RNA structures. Production rules are recursively applied until only terminal residues are left.\n\nA starting non-terminal formula_14 produces loops. The rest of the grammar proceeds with parameter formula_15 that decide whether a loop is a start of a stem or a single stranded region and parameter formula_16 that produces paired bases.\n\nThe formalism of this simple PCFG looks like:\n\nThe application of PCFGs in predicting structures is a multi-step process. In addition, the PCFG itself can be incorporated into probabilistic models that consider RNA evolutionary history or search homologous sequences in databases. In an evolutionary history context inclusion of prior distributions of RNA structures of a structural alignment in the production rules of the PCFG facilitates good prediction accuracy.\n\nA summary of general steps for utilizing PCFGs in various scenarios:\n\nSeveral algorithms dealing with aspects of PCFG based probabilistic models in RNA structure prediction exist. For instance the inside-outside algorithm and the CYK algorithm. The inside-outside algorithm is a recursive dynamic programming scoring algorithm that can follow expectation-maximization paradigms. It computes the total probability of all derivations that are consistent with a given sequence, based on some PCFG. The inside part scores the subtrees from a parse tree and therefore subsequences probabilities given an PCFG. The outside part scores the probability of the complete parse tree for a full sequence. CYK modifies the inside-outside scoring. Note that the term 'CYK algorithm' describes the CYK variant of the inside algorithm that finds an optimal parse tree for a sequence using a PCFG. It extends the actual CYK algorithm used in non-probabilistic CFGs.\n\nThe inside algorithm calculates formula_20 probabilities for all formula_21 of a parse subtree rooted at formula_22 for subsequence formula_23. Outside algorithm calculates formula_24 probabilities of a complete parse tree for sequence from root excluding the calculation of formula_23. The variables and refine the estimation of probability parameters of an PCFG. It is possible to reestimate the PCFG algorithm by finding the expected number of times a state is used in a derivation through summing all the products of and divided by the probability for a sequence given the model formula_11. It is also possible to find the expected number of times a production rule is used by an expectation-maximization that utilizes the values of and . The CYK algorithm calculates formula_27 to find the most probable parse tree formula_28 and yields formula_29.\n\nMemory and time complexity for general PCFG algorithms in RNA structure predictions are formula_30 and formula_31 respectively. Restricting a PCFG may alter this requirement as is the case with database searches methods.\n\nCovariance models (CMs) are a special type of PCFGs with applications in database searches for homologs, annotation and RNA classification. Through CMs it is possible to build PCFG-based RNA profiles where related RNAs can be represented by a consensus secondary structure. The RNA analysis package Infernal uses such profiles in inference of RNA alignments. The Rfam database also uses CMs in classifying RNAs into families based on their structure and sequence information.\n\nCMs are designed from a consensus RNA structure. A CM allows indels of unlimited length in the alignment. Terminals constitute states in the CM and the transition probabilities between the states is 1 if no indels are considered. Grammars in a CM are as follows:\n\nThe model has 6 possible states and each state grammar includes different types of secondary structure probabilities of the non-terminals. The states are connected by transitions. Ideally current node states connect to all insert states and subsequent node states connect to non-insert states. In order to allow insertion of more than one base insert states connect to themselves.\n\nIn order to score a CM model the inside-outside algorithms are used. CMs use a slightly different implementation of CYK. Log-odds emission scores for the optimum parse tree - formula_38 - are calculated out of the emitting states formula_39. Since these scores are a function of sequence length a more discriminative measure to recover an optimum parse tree probability score- formula_40 - is reached by limiting the maximum length of the sequence to be aligned and calculating the log-odds relative to a null. The computation time of this step is linear to the database size and the algorithm has a memory complexity of formula_41.\n\nThe KH-99 algorithm by Knudsen and Hein lays the basis of the Pfold approach to predicting RNA secondary structure. In this approach the parameterization requires evolutionary history information derived from an alignment tree in addition to probabilities of columns and mutations. The grammar probabilities are observed from a training dataset.\n\nIn a structural alignment the probabilities of the unpaired bases columns and the paired bases columns are independent of other columns. By counting bases in single base positions and paired positions one obtains the frequencies of bases in loops and stems.\nFor basepair and an occurrence of formula_42 is also counted as an occurrence of formula_43. Identical basepairs such as formula_44 are counted twice.\n\nBy pairing sequences in all possible ways overall mutation rates are estimated. In order to recover plausible mutations a sequence identity threshold should be used so that the comparison is between similar sequences. This approach uses 85% identity threshold between pairing sequences. \nFirst single base positions differences -except for gapped columns- between sequence pairs are counted such that if the same position in two sequences had different bases the count of the difference is incremented for each sequence.\n\nFor unpaired bases a 4 X 4 mutation rate matrix is used that satisfies that the mutation flow from X to Y is reversible:\nFor basepairs a 16 X 16 rate distribution matrix is similarly generated.\nThe PCFG is used to predict the prior probability distribution of the structure whereas posterior probabilities are estimated by the inside-outside algorithm and the most likely structure is found by the CYK algorithm.\n\nAfter calculating the column prior probabilities the alignment probability is estimated by summing over all possible secondary structures. Any column in a secondary structure formula_48 for a sequence of length such that formula_49 can be scored with respect to the alignment tree and the mutational model . The prior distribution given by the PCFG is formula_50. The phylogenetic tree, can be calculated from the model by maximum likelihood estimation. Note that gaps are treated as unknown bases and the summation can be done through dynamic programming. \n\nEach structure in the grammar is assigned production probabilities devised from the structures of the training dataset. These prior probabilities give weight to predictions accuracy. The number of times each rule is used depends on the observations from the training dataset for that particular grammar feature. These probabilities are written in parenthesis in the grammar formalism and each rule will have a total of 100%. For instance:\n\nGiven the prior alignment frequencies of the data the most likely structure from the ensemble predicted by the grammar can then be computed by maximizing formula_58 through the CYK algorithm. The structure with the highest predicted number of correct predictions is reported as the consensus structure.\n\nPCFG based approaches are desired to be scalable and general enough. Compromising speed for accuracy needs to as minimal as possible. Pfold addresses the limitations of the KH-99 algorithm with respect to scalability, gaps, speed and accuracy. \n\nWhereas PCFGs have proved powerful tools for predicting RNA secondary structure, usage in the field of protein sequence analysis has been limited. Indeed, the size of the amino acid alphabet and the variety of interactions seen in proteins make grammar inference much more challenging. As a consequence, most applications of formal language theory to protein analysis have been mainly restricted to the production of grammars of lower expressive power to model simple functional patterns based on local interactions. Since protein structures commonly display higher-order dependencies including nested and crossing relationships, they clearly exceed the capabilities of any CFG. Still, development of PCFGs allows expressing some of those dependencies and providing the ability to model a wider range of protein patterns.\n\nOne of the main obstacles in inferring a protein grammar is the size of the alphabet that should encode the 20 different amino acids. It has been proposed to address this by using physico-chemical properties of amino acids to reduce significantly the number of possible combinations of right side symbols in production rules: 3 levels of a quantitative property are utilised instead of the 20 amino acid types, e.g. small, medium or large van der Waals volume. Based on such a scheme, PCFGs have been produced to generate both binding site and helix-helix contact site descriptors. A significant feature of those grammars is that analysis of their rules and parse trees can provide biologically meaningful information.\n\n\n", "id": "299329", "title": "Probabilistic context-free grammar"}
{"url": "https://en.wikipedia.org/wiki?curid=30533678", "text": "Interactive machine translation\n\nInteractive Machine Translation (IMT), is a specific sub-field of computer-aided translation. Under this translation paradigm, the computer software that assists the human translator attempts to predict the text the user is going to input by taking into account all the information it has available. Whenever such prediction is wrong and the user provides feedback to the system, a new prediction is performed considering the new information available. Such process is repeated until the translation provided matches the user's expectations.\n\nInteractive machine translation is specially interesting when translating texts in domains where it is not admissible to output a translation containing errors, hence requiring a human user to amend the translations provided by the system. In such cases, interactive machine translation has been proved to provide benefit to potential users.\nNevertheless, there are few commercial software that implements interactive machine translation and work done in the field is mostly restrained to academic research.\n\nHistorically, interactive machine translation is born as an evolution of the computer-aided translation paradigm, where the human translator and the machine translation system were intended to work as a tandem.\nThis first work was extended within the TransType research project, funded by the Canadian government. In this project, the human interaction was aimed towards producing the target text for the first time by embedding data-driven machine translation techniques within the interactive translation environment with the goal of achieving the best of both actors: the efficiency of the automatic system and the reliability of human translators.\n\nLater, a larger-scale research project, TransType2,\n\nMore recently, CASMACAT, also funded by the European Commission, aimed at developing novel types of assistance to human translators and integrated them into a new workbench, consisting of an editor, a server, and analysis and visualisation tools. The workbench was designed in a modular fashion and can be combined with existing computer aided translation tools. Furthermore, the CASMACAT workbench can learn from the interaction with the human translator by updating and adapting its models instantly based on the translation choices of the user.\n\nRecent work on involving an extensive evaluation with human users revealed the fact that interactive machine translation may even be used by users that do not speak the source language in order to achieve near professional translation quality. Moreover, it also elucidated the fact that an interactive scenario is more beneficial than a classic post-edition scenario.\n\nThe interactive machine translation process starts with the system suggesting a\ntranslation hypothesis to the user. Then, the user may accept the complete\nsentence as correct, or may modify it if he considers there is some error.\nTypically, when modifying a given word, it is assumed that the prefix until\nthat word is correct, leading to a left-to-right interaction scheme. Once the\nuser has changed the word considered incorrect, the system then proposes a new\nsuffix, i.e. the remainder of the sentence. Such process continues until the\ntranslation provided satisfies the user.\n\nAlthough explained at the word level, the previous process may also be implemented at the character level, and hence the system provides a suffix whenever the human translator types in a single character. In addition, there is ongoing effort towards changing the typical left-to-right interaction scheme in order to make human-machine interaction easier.\n\nA similar approach is used in the Caitra translation tool.\n\nEvaluation is a difficult issue in interactive machine translation. Ideally,\nevaluation should take place in experiments involving human users. However,\ngiven the high monetary cost this would imply, this is seldom the case.\nMoreover, even when considering human translators in order to perform a true\nevaluation of interactive machine translation techniques, it is not clear what\nshould be measured in such experiments, since there are many different\nvariables that should be taken into account and cannot be controlled, as is \nfor instance the time the user takes in order to get used to the process.\nIn the CASMACAT project, some field trials have been carried out to study some of these variables.\n\nFor quick evaluations in laboratory conditions, interactive machine translation is measured by using the \"key stroke ratio\" or the \"word stroke ratio\". Such criteria attempt to measure how many key-strokes or words did the user need to introduce before producing the final translated document.\n\nAlthough interactive machine translation is a sub-field of computer-aided translation, the main attractive of the former with respect to the latter is the interactivity. In classical computer-aided translation, the translation system may suggest one translation hypothesis in the best case, and then the user is required to post-edit such hypothesis. In contrast, in interactive machine translation the system produces a new translation hypothesis each time the user interacts with the system, i.e. after each word (or letter) has been introduced.\n\n\n", "id": "30533678", "title": "Interactive machine translation"}
{"url": "https://en.wikipedia.org/wiki?curid=2088675", "text": "Probabilistic latent semantic analysis\n\nProbabilistic latent semantic analysis (PLSA), also known as probabilistic latent semantic indexing (PLSI, especially in information retrieval circles) is a statistical technique for the analysis of two-mode and co-occurrence data. In effect, one can derive a low-dimensional representation of the observed variables in terms of their affinity to certain hidden variables, just as in latent semantic analysis, from which PLSA evolved.\n\nCompared to standard latent semantic analysis which stems from linear algebra and downsizes the occurrence tables (usually via a singular value decomposition), probabilistic latent semantic analysis is based on a mixture decomposition derived from a latent class model.\n\nConsidering observations in the form of co-occurrences formula_1 of words and documents, PLSA models the probability of each co-occurrence as a mixture of conditionally independent multinomial distributions:\n\nwith 'c' being the words' topic. Note that the number of topics is a hyperparameter that must be chosen in advance and is not estimated from the data. The first formulation is the \"symmetric\" formulation, where formula_3 and formula_4 are both generated from the latent class formula_5 in similar ways (using the conditional probabilities formula_6 and formula_7), whereas the second formulation is the \"asymmetric\" formulation, where, for each document formula_4, a latent class is chosen conditionally to the document according to formula_9, and a word is then generated from that class according to formula_7. Although we have used words and documents in this example, the co-occurrence of any couple of discrete variables may be modelled in exactly the same way.\n\nSo, the number of parameters is equal to formula_11. The number of parameters grows linearly with the number of documents. In addition, although PLSA is a generative model of the documents in the collection it is estimated on, it is not a generative model of new documents.\n\nTheir parameters are learned using the EM algorithm.\n\nPLSA may be used in a discriminative setting, via Fisher kernels.\n\nPLSA has applications in information retrieval and filtering, natural language processing, machine learning from text, and related areas.\n\nIt is reported that the aspect model used in the probabilistic latent semantic analysis has severe overfitting problems.\n\n\n\nThis is an example of a latent class model (see references therein), and it is related to non-negative matrix factorization. The present terminology was coined in 1999 by Thomas Hofmann.\n\n\n", "id": "2088675", "title": "Probabilistic latent semantic analysis"}
{"url": "https://en.wikipedia.org/wiki?curid=30972465", "text": "Apache OpenNLP\n\nThe Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text. It supports the most common NLP tasks, such as language detection, tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing and coreference resolution. These tasks are usually required to build more advanced text processing services.\n\n\n", "id": "30972465", "title": "Apache OpenNLP"}
{"url": "https://en.wikipedia.org/wiki?curid=34073580", "text": "Dynamic topic model\n\nDynamic topic models are generative models that can be used to analyze the evolution of (unobserved) topics of a collection of documents over time. This family of models was proposed by David Blei and John Lafferty and is an extension to Latent Dirichlet Allocation (LDA) that can handle sequential documents.\n\nIn LDA, both the order the words appear in a document and the order the documents appear in the corpus are oblivious to the model. Whereas words are still assumed to be exchangeable, in a dynamic topic model the order of the documents plays a fundamental role. More precisely, the documents are grouped by time slice (e.g.: years) and it is assumed that the documents of each group come from a set of topics that evolved from the set of the previous slice.\n\nSimilarly to LDA and pLSA, in a dynamic topic model, each document is viewed as a mixture of unobserved topics. Furthermore, each topic defines a multinomial distribution over a set of terms. Thus, for each word of each document, a topic is drawn from the mixture and a term is subsequently drawn from the multinomial distribution corresponding to that topic.\n\nThe topics, however, evolve over time. For instance, the two most likely terms of a topic at time could be \"network\" and \"Zipf\" (in descending order) while the most likely ones at time could be \"Zipf\" and \"percolation\" (in descending order).\n\nDefine\n\nIn this model, the multinomial distributions formula_6 and formula_7 are generated from formula_1 and formula_2, respectively.\nEven though multinomial distributions are usually written in terms of the mean parameters, representing them in terms of the natural parameters is better in the context of dynamic topic models.\n\nThe former representation has some disadvantages due to the fact that the parameters are constrained to be non-negative and sum to one. When defining the evolution of these distributions, one would need to assure that such constraints were satisfied. Since both distributions are in the exponential family, one solution to this problem is to represent them in terms of the natural parameters, that can assume any real value and can be individually changed.\n\nUsing the natural parameterization, the dynamics of the topic model are given by\nand\n\nThe generative process at time slice 't' is therefore:\n\nwhere formula_17 is a mapping from the natural parameterization \"x\" to the mean parameterization, namely\n\nIn the dynamic topic model, only formula_19 is observable. Learning the other parameters constitutes an inference problem. Blei and Lafferty argue that applying Gibbs sampling to do inference in this model is more difficult than in static models, due to the nonconjugacy of the Gaussian and multinomial distributions. They propose the use of variational methods, in particular, the Variational Kalman Filtering and the Variational Wavelet Regression.\n\nIn the original paper, a dynamic topic model is applied to the corpus of Science articles published between 1881 and 1999 aiming to show that this method can be used to analyze the trends of word usage inside topics. The authors also show that the model trained with past documents is able to fit documents of an incoming year better than LDA.\n\nA continuous dynamic topic model was developed by Wang et al. and applied to predict the timestamp of documents.\n\n", "id": "34073580", "title": "Dynamic topic model"}
{"url": "https://en.wikipedia.org/wiki?curid=2229040", "text": "Factored language model\n\nThe factored language model (FLM) is an extension of a conventional language model introduced by Jeff Bilmes and Katrin Kirchoff in 2003. In an FLM, each word is viewed as a vector of \"k\" factors: formula_1 An FLM provides the probabilistic model formula_2 where the prediction of a factor formula_3 is based on formula_4 parents formula_5. For example, if formula_6 represents a word token and formula_7 represents a Part of speech tag for English, the expression formula_8 gives a model for predicting current word token based on a traditional Ngram model as well as the Part of speech tag of the previous word.\n\nA major advantage of factored language models is that they allow users to specify linguistic knowledge such as the relationship between word tokens and Part of speech in English, or morphological information (stems, root, etc.) in Arabic.\n\nLike N-gram models, smoothing techniques are necessary in parameter estimation. In particular, generalized back-off is used in training an FLM.\n", "id": "2229040", "title": "Factored language model"}
{"url": "https://en.wikipedia.org/wiki?curid=5825553", "text": "Collostructional analysis\n\nCollostructional analysis is a family of methods developed by (in alphabetical order) Stefan Th. Gries (University of California, Santa Barbara) and Anatol Stefanowitsch (Free University of Berlin). Collostructional analysis aims at measuring the degree of attraction or repulsion that words exhibit to constructions, where the notion of construction has so far been that of Goldberg's construction grammar.\n\nCollostructional analysis so far comprises three different methods:\n\n\nCollostructional analysis requires frequencies of words and constructions and is similar to a wide variety of collocation statistics. It differs from raw frequency counts by providing not only observed co-occurrence frequencies of words and constructions, but also\n\n(i) a comparison of the observed frequency to the one expected by chance; thus, collostructional analysis can distinguish attraction and repulsion of words and constructions;\n\n(ii) a measure of the strength of the attraction or repulsion; this is usually the log-transformed p-value of a Fisher-Yates exact test.\n\nCollostructional analysis differs from most collocation statistics such that\n\n(i) it measures not the association of words to words, but of words to syntactic patterns or constructions; thus, it takes syntactic structure more seriously than most collocation-based analyses;\n\n(ii) it has so far only used the most precise statistics, namely the Fisher-Yates exact test based on the hypergeometric distribution; thus, unlike \"t\"-scores, \"z\"-scores, chi-square tests etc., the analysis is not based on, and does not violate, any distributional assumptions.\n\n\n\n", "id": "5825553", "title": "Collostructional analysis"}
{"url": "https://en.wikipedia.org/wiki?curid=2057290", "text": "Tf–idf\n\nIn information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.\nThe tf-idf value increases proportionally to the number of times a word appears in the document, but is often offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Nowadays, tf-idf is one of the most popular term-weighting schemes; 83% of text-based recommender systems in the domain of digital libraries use tf-idf.\n\nVariations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.\n\nOne of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model.\n\nSuppose we have a set of English text documents and wish to rank which document is most relevant to the query, \"the brown cow\". A simple way to start out is by eliminating documents that do not contain all three words \"the\", \"brown\", and \"cow\", but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document; the number of times a term occurs in a document is called its \"term frequency\". However, in the case where the length of documents varies greatly, adjustments are often made (see definition below). The first form of term weighting is due to Hans Peter Luhn (1957) which may be summarized as:\n\nBecause the term \"the\" is so common, term frequency will tend to incorrectly emphasize documents which happen to use the word \"the\" more frequently, without giving enough weight to the more meaningful terms \"brown\" and \"cow\". The term \"the\" is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less-common words \"brown\" and \"cow\". Hence an \"inverse document frequency\" factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\n\nKaren Spärck Jones (1972) conceived a statistical interpretation of term specificity called Inverse Document Frequency (IDF), which became a cornerstone of term weighting:\n\nThe tf–idf is the product of two statistics, term frequency and inverse document frequency. Various ways for determining the exact values of both statistics exist.\n\nIn the case of the term frequency , the simplest choice is to use the \"raw count\" of a term in a document, i.e. the number of times that term occurs in document . If we denote the raw count by , then the simplest tf scheme is . Other possibilities include\n\nThe inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient.\n\nwith\n\nThen tf–idf is calculated as\n\nA high weight in tf–idf is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms. Since the ratio inside the idf's log function is always greater than or equal to 1, the value of idf (and tf-idf) is greater than or equal to 0. As a term appears in more documents, the ratio inside the logarithm approaches 1, bringing the idf and tf-idf closer to 0.\n\nIdf was introduced, as \"term specificity\", by Karen Spärck Jones in a 1972 paper. Although it has worked well as a heuristic, its theoretical foundations have been troublesome for at least three decades afterward, with many researchers trying to find information theoretic justifications for it.\n\nSpärck Jones's own explanation did not propose much theory, aside from a connection to Zipf's law. Attempts have been made to put idf on a probabilistic footing, by estimating the probability that a given document contains a term as the relative document frequency,\n\nso that we can define idf as\n\nNamely, the inverse document frequency is the logarithm of \"inverse\" relative document frequency.\n\nThis probabilistic interpretation in turn takes the same form as that of self-information. However, applying such information-theoretic notions to problems in information retrieval leads to problems when trying to define the appropriate event spaces for the required probability distributions: not only documents need to be taken into account, but also queries and terms.\n\nSuppose that we have term count tables of a corpus consisting of only two documents, as listed on the right.\n\nThe calculation of tf–idf for the term \"this\" is performed as follows:\n\nIn its raw frequency form, tf is just the frequency of the \"this\" for each document. In each document, the word \"this\" appears once; but as the document 2 has more words, its relative frequency is smaller.\n\nAn idf is constant per corpus, and accounts for the ratio of documents that include the word \"this\". In this case, we have a corpus of two documents and all of them include the word \"this\".\n\nSo tf–idf is zero for the word \"this\", which implies that the word is not very informative as it appears in all documents.\n\nA slightly more interesting example arises from the word \"example\", which occurs three times but only in the second document:\n\nFinally,\n\n(using the base 10 logarithm).\n\nThe idea behind TF–IDF has also been applied to entities other than terms. In 1998, the concept of IDF was applied to citations. The authors argued that \"if a very uncommon citation is shared by two documents, this should be weighted more highly than a citation made by a large number of documents\". In addition, tf-idf was applied to \"visual words\" with the purpose of conducting object matching in videos, and entire sentences. However, not in all cases did the concept of TF–IDF prove to be more effective than a plain TF scheme (without IDF). When TF–IDF was applied to citations, researchers could find no improvement over a simple citation–count weight that had no IDF component.\n\nThere are a number of term-weighting schemes that derived from TF–IDF. One of them is TF–PDF (Term Frequency * Proportional Document Frequency). TF-PDF was introduced in 2001 in the context of identifying emerging topics in the media. The PDF component measures the difference of how often a term occurs in different domains. Another derivate is TF-IDuF. In TF-IDuF, IDF is not calculated based on the document corpus that is to be searched or recommended. Instead, IDF is calculated based on users' personal document collections. The authors report that TF-IDuF was equally effective as tf-idf but could also be applied in situations when e.g. a user modeling system has no access to a global document corpus.\n\n\n", "id": "2057290", "title": "Tf–idf"}
{"url": "https://en.wikipedia.org/wiki?curid=1661566", "text": "Natural Language Toolkit\n\nThe Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania. NLTK includes graphical demonstrations and sample data. It is accompanied by a book that explains the underlying concepts behind the language processing tasks supported by the toolkit, plus a cookbook.\n\nNLTK is intended to support research and teaching in NLP or closely related areas, including empirical linguistics, cognitive science, artificial intelligence, information retrieval, and machine learning.\nNLTK has been used successfully as a teaching tool, as an individual study tool, and as a platform for prototyping and building research systems. There are 32 universities in the US and 25 countries using NLTK in their courses. NLTK supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities..\n\n\n\n", "id": "1661566", "title": "Natural Language Toolkit"}
{"url": "https://en.wikipedia.org/wiki?curid=42016100", "text": "Brown clustering\n\nBrown clustering is a hard hierarchical agglomerative clustering problem based on distributional information proposed by Peter Brown, Vincent Della Pietra, Peter deSouza, Jennifer Lai, and Robert Mercer. It is typically applied to text, grouping words into clusters that are assumed to be semantically related by virtue of their having been embedded in similar contexts.\n\nIn natural language processing, Brown clustering or IBM clustering is a form of hierarchical clustering of words based on the contexts in which they occur, proposed by Peter Brown, Vincent Della Pietra, Peter deSouza, Jennifer Lai, and Robert Mercer of IBM in the context of language modeling. The intuition behind the method is that a class-based language model (also called cluster -gram model), i.e. one where probabilities of words are based on the classes (clusters) of previous words, is used to address the data sparsity problem inherent in language modeling.\n\nJurafsky and Martin give the example of a flight reservation system that needs to estimate the likelihood of the bigram \"to Shanghai\", without having seen this in a training set. The system can obtain a good estimate if it can cluster \"Shanghai\" with other city names, then make its estimate based on the likelihood of phrases such as \"to London\", \"to Beijing\" and \"to Denver\".\n\nBrown groups items (i.e., types) into classes, using a binary merging criterion based on the log-probability of a text under a class-based language model, i.e. a probability model that takes the clustering into account. Thus, AMI is the optimisation function, and merges are chosen such that they incur the least loss in global mutual information.\n\nAs a result, the output can be thought of not only as a binary tree but perhaps more helpfully as a sequence of merges, terminating with one big class of all words. This model has the same general form as a hidden Markov model, reduced to bigram probabilities in Brown's solution to the problem.\nMI is defined as:\n\nFinding the clustering that maximizes the likelihood of the data is computationally expensive. \nThe approach proposed by Brown et al. is a greedy heuristic.\n\nThe work also suggests use of Brown clusterings as a simplistic bigram class-based language model. Given cluster membership indicators for the tokens in a text, the probability of the word instance given preceding word is given by:\n\nThis has been criticised as being of limited utility, as it only ever predicts the most common word in any class, and so is restricted to word types; this is reflected in the low relative reduction in perplexity found when using this model and Brown.\n\nOther works have examined trigrams in their approaches to the Brown clustering problem.\n\nBrown clustering as proposed generates a fixed number of output classes. It is important to choose the correct number of classes, which is task-dependent. The cluster memberships of words resulting from Brown clustering can be used as features in a variety of machine-learned natural language processing tasks.\n\n\n", "id": "42016100", "title": "Brown clustering"}
{"url": "https://en.wikipedia.org/wiki?curid=4605351", "text": "Latent Dirichlet allocation\n\nIn natural language processing, Latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003.\nEssentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000.\nBoth papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.\n\nIn LDA, each document may be viewed as a mixture of various topics where each document is considered to have a set of topics that are assigned to it via LDA. This is identical to probabilistic latent semantic analysis (pLSA), except that in LDA the topic distribution is assumed to have a sparse Dirichlet prior. The sparse Dirichlet priors encode the intuition that documents cover only a small set of topics and that topics use only a small set of words frequently. In practice, this results in a better disambiguation of words and a more precise assignment of documents to topics. LDA is a generalisation of the pLSA model, which is equivalent to LDA under a uniform Dirichlet prior distribution.\n\nFor example, an LDA model might have topics that can be classified as CAT_related and DOG_related. A topic has probabilities of generating various words, such as \"milk\", \"meow\", and \"kitten\", which can be classified and interpreted by the viewer as \"CAT_related\". Naturally, the word \"cat\" itself will have high probability given this topic. The DOG_related topic likewise has probabilities of generating each word: \"puppy\", \"bark\", and \"bone\" might have high probability. Words without special relevance, such as \"the\" (see function word), will have roughly even probability between classes (or can be placed into a separate category). A topic is not strongly defined, neither semantically nor epistemologically. It is identified on the basis of automatic detection of the likelihood of term co-occurrence. A lexical word may occur in several topics with a different probability, however, with a different typical set of neighboring words in each topic.\n\nEach document is assumed to be characterized by a particular set of topics. This is akin to the standard bag of words model assumption, and makes the individual words exchangeable.\n\n With plate notation, the dependencies among the many variables can be captured concisely. The boxes are \"plates\" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of documents, N the number of words in a document. Thus:\n\nThe words formula_5 are the only observable variables, and the other variables are latent variables.\nAs proposed in the original paper, a sparse Dirichlet prior can be put over the topic-word distribution. This codes the intuition that the probability of topics is focussed on a small set of words. The resulting model is the most widely applied variant of LDA today. The plate notation for this model is shown on the right, where formula_6 denotes the number of topics and formula_7 \"are\" formula_8-dimensional vectors storing the parameters of the Dirichlet-distributed topic-word distributions (formula_8 is the number of words in the vocabulary).\n\nThe generative process is as follows. Documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. LDA assumes the following generative process for a corpus formula_10 consisting of formula_11 documents each of length formula_12:\n\n1. Choose formula_13, where formula_14 and\nformula_15 is a Dirichlet distribution with a symmetric parameter formula_16 which typically is sparse (formula_17)\n\n2. Choose formula_18, where formula_19 and formula_20 typically is sparse\n\n3. For each of the word positions formula_21, where formula_22, and formula_14\n\nThe lengths formula_12 are treated as independent of all the other data generating variables (formula_27 and formula_28). The subscript is often dropped, as in the plate diagrams shown here.\n\nA formal description of LDA is as follows:\n\nWe can then mathematically describe the random variables as follows:\n\nLearning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of Bayesian inference. The original paper used a variational Bayes approximation of the posterior distribution; alternative inference techniques use Gibbs sampling and expectation propagation.\n\nFollowing is the derivation of the equations for collapsed Gibbs sampling, which means formula_30s and formula_31s will be integrated out. For simplicity, in this derivation the documents are all assumed to have the same length formula_32. The derivation is equally valid if the document lengths vary.\n\nAccording to the model, the total probability of the model is:\n\nwhere the bold-font variables denote the vector version of the variables. First, formula_34 and formula_35 need to be integrated out.\n\nAll the formula_31s are independent to each other and the same to all the formula_30s. So we can treat each formula_31 and each formula_30 separately. We now focus only on the formula_31 part.\n\nWe can further focus on only one formula_31 as the following:\n\nActually, it is the hidden part of the model for the formula_45 document. Now we replace the probabilities in the above equation by the true distribution expression to write out the explicit equation.\n\nLet formula_47 be the number of word tokens in the formula_45 document with the same word symbol (the formula_49 word in the vocabulary) assigned to the formula_50 topic. So, formula_47 is three dimensional. If any of the three dimensions is not limited to a specific value, we use a parenthesized point formula_52 to\ndenote. For example, formula_53 denotes the number of word tokens in the formula_45 document assigned to the formula_50 topic. Thus, the right most part of the above equation can be rewritten as:\n\nSo the formula_57 integration formula can be changed to:\n\nClearly, the equation inside the integration has the same form as the Dirichlet distribution. According to the Dirichlet distribution,\n\nThus,\n\nNow we turn our attention to the formula_34 part. Actually, the derivation of the formula_34 part is very similar to the formula_35 part. Here we only list the steps of the derivation:\n\nFor clarity, here we write down the final equation with both formula_65 and formula_35 integrated out:\n\nThe goal of Gibbs Sampling here is to approximate the distribution of formula_68. Since formula_69 is invariable for any of Z, Gibbs Sampling equations can be derived from formula_70 directly. The key point is to derive the following conditional probability:\n\nwhere formula_72 denotes the formula_73 hidden variable of the formula_74 word token in the formula_75 document. And further we assume that the word\nsymbol of it is the formula_76 word in the vocabulary. formula_77 denotes all the formula_73s but formula_72. Note that Gibbs Sampling needs only to sample a value for formula_72, according to the above probability, we do not need the exact value of\n\nbut the ratios among the probabilities that formula_72 can take value. So, the above equation can be simplified as:\n\nFinally, let formula_84 be the same meaning as formula_47 but with the formula_72 excluded. The above equation can be further simplified leveraging the property of gamma function. We first split the summation and then merge it back to obtain a formula_87-independent summation, which could be dropped:\n\nNote that the same formula is derived in the article on the , as part of a more general discussion of integrating Dirichlet distribution priors out of a Bayesian network.\n\nRecent research has been focused on speeding up the inference of latent Dirichlet Allocation to support capture of a massive number of topics in large number of documents. The update equation of the collapsed Gibbs sampler mentioned in the earlier section has a natural sparsity within it that can be taken advantage of. Intuitively, since each document only contains a subset of topics formula_89, and a word also only appears in a subset of topics formula_90, the above update equation could be rewritten to take advantage of this sparsity.\n\nformula_91\n\nIn this equation, we have three terms, out of which two of them are sparse, and the other is small. We call these terms formula_92 and formula_93 respectively. Now, if we normalize each term by summing over all the topics, we get:\n\nHere, we can see that formula_97 is a summation of the topics that appear in document formula_98, and formula_99 is also a sparse summation of the topics that a word formula_27 is assigned to across the whole corpus. formula_101 on the other hand, is dense but because of the small values of formula_16 & formula_20, the value is very small compared to the two other terms.\n\nNow, while sampling a topic, if we sample a random variable uniformly from formula_104, we can check which bucket our sample lands in. Since formula_101 is small, we are very unlikely to fall into this bucket; however, if we do fall into this bucket, sampling a topic takes O(K) time (same as the original Collapsed Gibbs Sampler). However, if we fall into the other two buckets, we only need to check a subset of topics if we keep a record of the sparse topics. A topic can be sampled from the formula_97 bucket in formula_107 time, and a topic can be sampled from the formula_99 bucket in formula_109 time where formula_89 and formula_90 denotes the number of topics assigned to the current document and current word type respectively.\n\nNotice that after sampling each topic, updating these buckets are all basic formula_112 arithmetic operations.\n\nTopic modeling is a classic problem in information retrieval. Related models and techniques are, among others, latent semantic indexing, independent component analysis, probabilistic latent semantic indexing, non-negative matrix factorization, and Gamma-Poisson distribution.\n\nThe LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model follows this approach, inducing a correlation structure between topics by using the logistic normal distribution instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA), where topics are joined together in a hierarchy by using the nested Chinese restaurant process. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the LDA-dual model.\nNonparametric extensions of LDA include the hierarchical Dirichlet process mixture model, which allows the number of topics to be unbounded and learnt from data and the nested Chinese restaurant process which allows topics to be arranged in a hierarchy whose structure is learnt from data.\n\nAs noted earlier, pLSA is similar to LDA. The LDA model is essentially the Bayesian version of pLSA model. The Bayesian formulation tends to perform better on small datasets because Bayesian methods can avoid overfitting the data. For very large datasets, the results of the two models tend to converge. One difference is that pLSA uses a variable formula_98 to represent a document in the training set. So in pLSA, when presented with a document the model hasn't seen before, we fix formula_114—the probability of words under topics—to be that learned from the training set and use the same EM algorithm to infer formula_115—the topic distribution under formula_98. Blei argues that this step is cheating because you are essentially refitting the model to the new data.\n\nVariations on LDA have been used to automatically put natural images into categories, such as \"bedroom\" or \"forest\", by treating an image as a document, and small patches of the image as words; one of the variations is called Spatial Latent Dirichlet Allocation.\n\n\n", "id": "4605351", "title": "Latent Dirichlet allocation"}
{"url": "https://en.wikipedia.org/wiki?curid=31139924", "text": "Cache language model\n\nA cache language model is a type of statistical language model. These occur in the natural language processing subfield of computer science and assign probabilities to given sequences of words by means of a probability distribution. Statistical language models are key components of speech recognition systems and of many machine translation systems: they tell such systems which possible output word sequences are probable and which are improbable. The particular characteristic of a cache language model is that it contains a cache component and assigns relatively high probabilities to words or word sequences that occur elsewhere in a given text. The primary, but by no means sole, use of cache language models is in speech recognition systems.\n\nTo understand why it is a good idea for a statistical language model to contain a cache component one might consider someone who is dictating a letter about elephants to a speech recognition system. Standard (non-cache) N-gram language models will assign a very low probability to the word \"elephant\" because it is a very rare word in English. If the speech recognition system does not contain a cache component the person dictating the letter may be annoyed: each time the word \"elephant\" is spoken another sequence of words with a higher probability according to the N-gram language model may be recognized (e.g., \"tell a plan\"). These erroneous sequences will have to be deleted manually and replaced in the text by \"elephant\" each time \"elephant\" is spoken. If the system has a cache language model, \"elephant\" will still probably be misrecognized the first time it is spoken and will have to be entered into the text manually; however, from this point on the system is aware that \"elephant\" is likely to occur again – the estimated probability of occurrence of \"elephant\" has been increased, making it more likely that if it is spoken it will be recognized correctly. Once \"elephant\" has occurred several times the system is likely to recognize it correctly every time it is spoken until the letter has been completely dictated. This increase in the probability assigned to the occurrence of \"elephant\" is an example of a consequence of machine learning and more specifically of pattern recognition.\n\nThere exist variants of the cache language model in which not only single words but also multi-word sequences that have occurred previously are assigned higher probabilities (e.g., if \"San Francisco\" occurred near the beginning of the text subsequent instances of it would be assigned a higher probability).\n\nThe cache language model was first proposed in a paper published in 1990, after which the IBM speech-recognition group experimented with the concept. The group found that implementation of a form of cache language model yielded a 24% drop in word-error rates once the first few hundred words of a document had been dictated. A detailed survey of language modeling techniques concluded that the cache language model was one of the few new language modeling techniques that yielded improvements over the standard N-gram approach: \"Our caching results show that caching is by far the most useful technique for perplexity reduction at small and medium training data sizes\".\n\nThe development of the cache language model has generated considerable interest among those concerned with computational linguistics in general and statistical natural language processing in particular: recently there has been interest in applying the cache language model in the field of statistical machine translation.\n\nThe success of the cache language model in improving word prediction rests on the human tendency to use words in a \"bursty\" fashion: when one is discussing a certain topic in a certain context the frequency with which one uses certain words will be quite different from their frequencies when one is discussing other topics in other contexts. The traditional N-gram language models, which rely entirely on information from a very small number (four, three, or two) of words preceding the word to which a probability is to be assigned, do not adequately model this \"burstiness\".\n\n", "id": "31139924", "title": "Cache language model"}
{"url": "https://en.wikipedia.org/wiki?curid=4631023", "text": "Perplexity\n\nIn information theory, perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample.\n\nThe perplexity of a discrete probability distribution \"p\" is defined as\n\nwhere \"H\"(\"p\") is the entropy (in bits) of the distribution and \"x\" ranges over events.\n\nPerplexity of a random variable \"X\" may be defined as the perplexity of the distribution over its possible values \"x\".\n\nIn the special case where \"p\" models a fair \"k\"-sided die (a uniform distribution over \"k\" discrete events), its perplexity is \"k\". A random variable with perplexity \"k\" has the same uncertainty as a fair \"k\"-sided die, and one is said to be \"\"k\"-ways perplexed\" about the value of the random variable. (Unless it is a fair \"k\"-sided die, more than \"k\" values will be possible, but the overall uncertainty is no greater because some of these values will have probability greater than 1/\"k\", decreasing the overall value while summing.)\n\nPerplexity is sometimes used as a measure of how hard a prediction problem is. This is not always accurate. If you have two choices, one with probability 0.9, then your chances of a correct guess are 90 percent using the optimal strategy.\nThe perplexity is 2= 1.38. The inverse of the perplexity (which, in the case of the fair k-sided die, represents the probability of guessing correctly), is 1/1.38 = 0.72, not 0.9.\n\nThe perplexity is the exponentiation of the entropy, which is a more clearcut quantity.\nThe entropy is a measure of the expected, or \"average\", number of bits required to encode the outcome of the random variable, using a theoretical optimal variable-length code, cf. the next section.\nIt can equivalently be regarded as the expected information gain from learning the outcome of the random variable, where information is measured in bits.\n\nA model of an unknown probability distribution \"p\", may be proposed based on a training sample that was drawn from \"p\". Given a proposed probability model \"q\", one may evaluate \"q\" by asking how well it predicts a separate test sample \"x\", \"x\", ..., \"x\" also drawn from \"p\". The perplexity of the model \"q\" is defined as\n\nwhere formula_3 is customarily 2. Better models \"q\" of the unknown distribution \"p\" will tend to assign higher probabilities \"q\"(\"x\") to the test events. Thus, they have lower perplexity: they are less surprised by the test sample.\n\nThe exponent above may be regarded as the average number of bits needed to represent a test event \"x\" if one uses an optimal code based on \"q\". Low-perplexity models do a better job of compressing the test sample, requiring few bits per test element on average because \"q\"(\"x\") tends to be high.\n\nThe exponent may also be regarded as a cross-entropy,\n\nwhere formula_5 denotes the empirical distribution of the test sample (i.e., formula_6 if \"x\" appeared \"n\" times in the test sample of size \"N\").\n\nIn natural language processing, perplexity is a way of evaluating language models. A language model is a probability distribution over entire sentences or texts.\n\nUsing the definition of perplexity for a probability model, one might find, for example, that the average sentence \"x\" in the test sample could be coded in 190 bits (i.e., the test sentences had an average log-probability of -190). This would give an enormous model perplexity of 2 per sentence. However, it is more common to normalize for sentence length and consider only the number of bits per word. Thus, if the test sample's sentences comprised a total of 1,000 words, and could be coded using a total of 7.95 bits per word, one could report a model perplexity of 2 = 247 \"per word.\" In other words, the model is as confused on test data as if it had to choose uniformly and independently among 247 possibilities for each word.\n\nThe lowest perplexity that has been published on the Brown Corpus (1 million words of American English of varying topics and genres) as of 1992 is indeed about 247 per word, corresponding to a cross-entropy of log247 = 7.95 bits per word or 1.75 bits per letter using a trigram model. It is often possible to achieve lower perplexity on more specialized corpora, as they are more predictable.\n\nAgain, simply guessing that the next word in the Brown corpus is the word \"the\" will have an accuracy of 7 percent, not 1/247 = 0.4 percent, as a naive use of perplexity as a measure of predictiveness might lead one to believe. This guess is based on the unigram statistics of the Brown corpus, not on the trigram statistics, which yielded the word perplexity 247. Using trigram statistics would further improve the chances of a correct guess.\n", "id": "4631023", "title": "Perplexity"}
{"url": "https://en.wikipedia.org/wiki?curid=45391945", "text": "Kneser–Ney smoothing\n\nKneser–Ney smoothing is a method primarily used to calculate the probability distribution of \"n\"-grams in a document based on their histories. It is widely considered the most effective method of smoothing due to its use of absolute discounting by subtracting a fixed value from the probability's lower order terms to omit \"n\"-grams with lower frequencies. This approach has been considered equally effective for both higher and lower order \"n\"-grams. The method is due to Reinhard Kneser and Hermann Ney. \n\nA common example that illustrates the concept behind this method is the frequency of the bigram \"San Francisco\". If it appears several times in a training corpus, the frequency of the unigram \"Francisco\" will also be high. Relying on only the unigram frequency to predict the frequencies of \"n\"-grams leads to skewed results; however, Kneser–Ney smoothing corrects this by considering the frequency of the unigram in relation to possible words preceding it.\n\nLet formula_1 be the number of occurrences of the word formula_2 followed by the word formula_3 in the corpus.\n\nThe equation for bigram probabilities is as follows:\n\nformula_4\n\nWhere the unigram probability formula_5 depends on how likely it is to see the word formula_6 in an unfamiliar context, which is estimated as the number of times it appears after any other word divided by the number of distinct pairs of consecutive words in the corpus:\n\nformula_7\n\nPlease note that formula_8 is a proper distribution, as the values defined in above way are non-negative and sum to one.\n\nThe parameter formula_9 is a constant which denotes the discount value subtracted from the count of each n-gram, usually between 0 and 1.\n\nThe value of the normalizing constant formula_10 is calculated to make the sum of conditional probabilities formula_11 over all formula_6 equal to one. \nObserve that (provided formula_13) for each formula_6 which occurs at least once in the context of formula_15 in the corpus we discount the probability by exactly the same constant amount formula_16,\nso the total discount depends linearly on the number of unique words formula_6 that can occur after formula_15.\nThis total discount is a budget we can spread over all formula_11 proportionally to formula_5.\nAs the values of formula_5 sum to one, we can simply define formula_10 to be equal to this total discount:\n\nformula_23\n\nThis equation can be extended to n-grams. Let formula_24 be the formula_25 words before formula_6:\n\nformula_27\nThis model uses the concept of absolute-discounting interpolation which incorporates information from higher and lower order language models. The addition of the term for lower order n-grams adds more weight to the overall probability when the count for the higher order n-grams is zero. Similarly, the weight of the lower order model decreases when the count of the n-gram is non zero.\n", "id": "45391945", "title": "Kneser–Ney smoothing"}
{"url": "https://en.wikipedia.org/wiki?curid=986182", "text": "N-gram\n\nIn the fields of computational linguistics and probability, an \"n\"-gram is a contiguous sequence of \"n\" items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The \"n\"-grams typically are collected from a text or speech corpus. When the items are words, -grams may also be called \"shingles\".\n\nAn \"n\"-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\" (or, less commonly, a \"digram\"); size 3 is a \"trigram\". Larger sizes are sometimes referred to by the value of \"n\" in modern language, e.g., \"four-gram\", \"five-gram\", and so on.\n\nAn \"n\"-gram model is a type of probabilistic language model for predicting the next item in such a sequence in the form of a (\"n\" − 1)–order Markov model. \"n\"-gram models are now widely used in probability, communication theory, computational linguistics (for instance, statistical natural language processing), computational biology (for instance, biological sequence analysis), and data compression. Two benefits of \"n\"-gram models (and algorithms that use them) are simplicity and scalability – with larger \"n\", a model can store more context with a well-understood space–time tradeoff, enabling small experiments to scale up efficiently.\n\nFigure 1 shows several example sequences and the corresponding 1-gram, 2-gram and 3-gram sequences.\n\nHere are further examples; these are word-level 3-grams and 4-grams (and counts of the number of times they appeared) from the Google \"n\"-gram corpus.\n\n3-grams\n\n4-grams\n\nAn \"n\"-gram model models sequences, notably natural languages, using the statistical properties of \"n\"-grams.\n\nThis idea can be traced to an experiment by Claude Shannon's work in information theory. Shannon posed the question: given a sequence of letters (for example, the sequence \"for ex\"), what is the likelihood of the next letter? From training data, one can derive a probability distribution for the next letter given a history of size formula_1: \"a\" = 0.4, \"b\" = 0.00001, \"c\" = 0, ...; where the probabilities of all possible \"next-letters\" sum to 1.0.\n\nMore concisely, an \"n\"-gram model predicts formula_2 based on formula_3. In probability terms, this is formula_4. When used for language modeling, independence assumptions are made so that each word depends only on the last \"n\" − 1 words. This Markov model is used as an approximation of the true underlying language. This assumption is important because it massively simplifies the problem of estimating the language model from data. In addition, because of the open nature of language, it is common to group words unknown to the language model together.\n\nNote that in a simple \"n\"-gram language model, the probability of a word, conditioned on some number of previous words (one word in a bigram model, two words in a trigram model, etc.) can be described as following a categorical distribution (often imprecisely called a \"multinomial distribution\").\n\nIn practice, the probability distributions are smoothed by assigning non-zero probabilities to unseen words or \"n\"-grams; see smoothing techniques.\n\n\"n\"-gram models are widely used in statistical natural language processing. In speech recognition, phonemes and sequences of phonemes are modeled using a \"n\"-gram distribution. For parsing, words are modeled such that each \"n\"-gram is composed of \"n\" words. For language identification, sequences of characters/graphemes (\"e.g.\", letters of the alphabet) are modeled for different languages. For sequences of characters, the 3-grams (sometimes referred to as \"trigrams\") that can be generated from \"good morning\" are \"goo\", \"ood\", \"od \", \"d m\", \" mo\", \"mor\" and so forth, counting the space character as a gram (sometimes the beginning and end of a text are modeled explicitly, adding \"__g\", \"_go\", \"ng_\", and \"g__\"). For sequences of words, the trigrams (shingles) that can be generated from \"the dog smelled like a skunk\" are \"# the dog\", \"the dog smelled\", \"dog smelled like\", \"smelled like a\", \"like a skunk\" and \"a skunk #\".\n\nPractitioners more interested in multiple word terms might preprocess strings to remove spaces. Many simply collapse whitespace to a single space while preserving paragraph marks, because the whitespace is frequently either an element of writing style or introduces layout or presentation not required by the prediction and deduction methodology. Punctuation is also commonly reduced or removed by preprocessing and is frequently used to trigger functionality.\n\n\"n\"-grams can also be used for sequences of words or almost any type of data. For example, they have been used for extracting features for clustering large sets of satellite earth images and for determining what part of the Earth a particular image came from. They have also been very successful as the first pass in genetic sequence search and in the identification of the species from which short sequences of DNA originated.\n\n\"n\"-gram models are often criticized because they lack any explicit representation of long range dependency. This is because the only explicit dependency range is (\"n\" − 1) tokens for an \"n\"-gram model, and since natural languages incorporate many cases of unbounded dependencies (such as wh-movement), this means that an \"n\"-gram model cannot in principle distinguish unbounded dependencies from noise (since long range correlations drop exponentially with distance for any Markov model). For this reason, \"n\"-gram models have not made much impact on linguistic theory, where part of the explicit goal is to model such dependencies.\n\nAnother criticism that has been made is that Markov models of language, including \"n\"-gram models, do not explicitly capture the performance/competence distinction. This is because \"n\"-gram models are not designed to model linguistic knowledge as such, and make no claims to being (even potentially) complete models of linguistic knowledge; instead, they are used in practical applications.\n\nIn practice, \"n\"-gram models have been shown to be extremely effective in modeling language data, which is a core component in modern statistical language applications.\n\nMost modern applications that rely on \"n\"-gram based models, such as machine translation applications, do not rely exclusively on such models; instead, they typically also incorporate Bayesian inference. Modern statistical models are typically made up of two parts, a prior distribution describing the inherent likelihood of a possible result and a likelihood function used to assess the compatibility of a possible result with observed data. When a language model is used, it is used as part of the prior distribution (e.g. to gauge the inherent \"goodness\" of a possible translation), and even then it is often not the only component in this distribution.\n\nHandcrafted features of various sorts are also used, for example variables that represent the position of a word in a sentence or the general topic of discourse. In addition, features based on the structure of the potential result, such as syntactic considerations, are often used. Such features are also used as part of the likelihood function, which makes use of the observed data. Conventional linguistic theory can be incorporated in these features (although in practice, it is rare that features specific to generative or other particular theories of grammar are incorporated, as computational linguists tend to be \"agnostic\" towards individual theories of grammar).\n\nAn issue when using n-gram language models are out-of-vocabulary (OOV) words. They are encountered in computational linguistics and natural language processing when the input includes words which were not present in a system's dictionary or database during its preparation. By default, when a language model is estimated, the entire observed vocabulary is used. In some cases, it may be necessary to estimate the language model with a specific fixed vocabulary. In such a scenario, the n-grams in the corpus that contain an out-of-vocabulary word are ignored. The n-gram probabilities are smoothed over all the words in the vocabulary even if they were not observed.\n\nNonetheless, it is essential in some cases to explicitly model the probability of out-of-vocabulary words by introducing a special token (e.g. \"<unk>\") into the vocabulary. Out-of-vocabulary words in the corpus are effectively replaced with this special <unk> token before n-grams counts are cumulated. With this option, it is possible to estimate the transition probabilities of n-grams involving out-of-vocabulary words.\n\n\"n\"-grams can also be used for efficient approximate matching. By converting a sequence of items to a set of \"n\"-grams, it can be embedded in a vector space, thus allowing the sequence to be compared to other sequences in an efficient manner. For example, if we convert strings with only letters in the English alphabet into single character 3-grams, we get a formula_5-dimensional space (the first dimension measures the number of occurrences of \"aaa\", the second \"aab\", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string. For example, both the strings \"abc\" and \"bca\" give rise to exactly the same 2-gram \"bc\" (although {\"ab\", \"bc\"} is clearly not the same as {\"bc\", \"ca\"}). However, we know empirically that if two strings of real text have a similar vector representation (as measured by cosine distance) then they are likely to be similar. Other metrics have also been applied to vectors of \"n\"-grams with varying, sometimes better, results. For example, z-scores have been used to compare documents by examining how many standard deviations each \"n\"-gram differs from its mean occurrence in a large collection, or text corpus, of documents (which form the \"background\" vector). In the event of small counts, the g-score (also known as g-test) may give better results for comparing alternative models.\n\nIt is also possible to take a more principled approach to the statistics of \"n\"-grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in Bayesian inference.\n\n\"n\"-gram-based searching can also be used for plagiarism detection.\n\n\"n\"-grams find use in several areas of computer science, computational linguistics, and applied mathematics.\n\nThey have been used to:\n\nTo choose a value for \"n\" in an \"n\"-gram model, it is necessary to find the right trade off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.\n\nThere are problems of balance weight between \"infrequent grams\" (for example, if a proper name appeared in the training data) and \"frequent grams\". Also, items not seen in the training data will be given a probability of 0.0 without smoothing. For unseen but plausible data from a sample, one can introduce pseudocounts. Pseudocounts are generally motivated on Bayesian grounds.\n\nIn practice it is necessary to \"smooth\" the probability distributions by also assigning non-zero probabilities to unseen words or \"n\"-grams. The reason is that models derived directly from the \"n\"-gram frequency counts have severe problems when confronted with any \"n\"-grams that have not explicitly been seen before – the zero-frequency problem. Various smoothing methods are used, from simple \"add-one\" (Laplace) smoothing (assign a count of 1 to unseen \"n\"-grams; see Rule of succession) to more sophisticated models, such as Good–Turing discounting or back-off models. Some of these methods are equivalent to assigning a prior distribution to the probabilities of the \"n\"-grams and using Bayesian inference to compute the resulting posterior \"n\"-gram probabilities. However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations.\n\nIn the field of computational linguistics, in particular language modeling, skip-grams are a generalization of \"n\"-grams in which the components (typically words) need not be consecutive in the text under consideration, but may leave gaps that are \"skipped\" over. They provide one way of overcoming the data sparsity problem found with conventional \"n\"-gram analysis.\n\nFormally, an -gram is a consecutive subsequence of length of some sequence of tokens . A -skip--gram is a length- subsequence where the components occur at distance at most from each other.\n\nFor example, in the input text:\n\nthe set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences\n\nSyntactic \"n\"-grams are \"n\"-grams defined by paths in syntactic dependency or constituent trees rather than the linear structure of the text. For example, the sentence \"economic news has little effect on financial markets\" can be transformed to syntactic \"n\"-grams following the tree structure of its dependency relations: news-economic, effect-little, effect-on-markets-financial.\n\nSyntactic \"n\"-grams are intended to reflect syntactic structure more faithfully than linear \"n\"-grams, and have many of the same applications, especially as features in a Vector Space Model. Syntactic \"n\"-grams for certain tasks gives better results than the use of standard \"n\"-grams, for example, for authorship attribution.\n\n\n\n", "id": "986182", "title": "N-gram"}
{"url": "https://en.wikipedia.org/wiki?curid=49119569", "text": "Comparison of deep learning software\n\nThe following table compares some of the most popular software frameworks, libraries and computer programs for deep learning.\n\n\n", "id": "49119569", "title": "Comparison of deep learning software"}
{"url": "https://en.wikipedia.org/wiki?curid=42571226", "text": "Torch (machine learning)\n\nTorch is an open source machine learning library, \na scientific computing framework, and a script language based on the Lua programming language. It provides a wide range of algorithms for deep machine learning, and uses the scripting language LuaJIT, and an underlying C implementation.\n\nThe core package of Torch is torch. It provides a flexible N-dimensional array or Tensor, which supports basic routines for indexing, slicing, transposing, type-casting, resizing, sharing storage and cloning. This object is used by most other packages and thus forms the core object of the library. The Tensor also supports mathematical operations like codice_1, codice_2, codice_3, statistical distributions like uniform, normal and multinomial, and BLAS operations like dot product, matrix-vector multiplication, matrix-matrix multiplication, matrix-vector product and matrix product.\n\nThe following exemplifies using torch via its REPL interpreter:\n\n> a = torch.randn(3,4)\n\n> =a\n-0.2381 -0.3401 -1.7844 -0.2615\n-1.0434 2.2291 1.0525 0.8465\n[torch.DoubleTensor of dimension 3x4]\n\n> a[1][2]\n-0.34010116549482\n> a:narrow(1,1,2)\n-0.2381 -0.3401 -1.7844 -0.2615\n[torch.DoubleTensor of dimension 2x4]\n\n> a:index(1, torch.LongTensor{1,2})\n-0.2381 -0.3401 -1.7844 -0.2615\n[torch.DoubleTensor of dimension 2x4]\n\n> a:min()\n-1.7844365427828 \nThe torch package also simplifies object oriented programming and serialization by providing various convenience functions which are used throughout its packages. The codice_4 function can be used to create object factories (classes). When the constructor is called, torch initializes and sets a Lua table with the user-defined metatable, which makes the table an object.\n\nObjects created with the torch factory can also be serialized, as long as they do not contain references to objects that cannot be serialized, such as Lua coroutines, and Lua \"userdata\". However, \"userdata\" can be serialized if it is wrapped by a table (or metatable) that provides codice_5 and codice_6 methods.\n\nThe nn package is used for building neural networks. It is divided into modular objects that share a common codice_7 interface. Modules have a codice_8 and codice_9 method that allow them to feedforward and backpropagate, respectively. Modules can be joined together using module composites, like codice_10, codice_11 and codice_12 to create complex task-tailored graphs. Simpler modules like codice_13, codice_14 and codice_15 make up the basic component modules. This modular interface provides first-order automatic gradient differentiation. What follows is an example use-case for building a multilayer perceptron using Modules:\n\n> mlp = nn.Sequential()\n> mlp:add( nn.Linear(10, 25) ) -- 10 input, 25 hidden units\n> mlp:add( nn.Tanh() ) -- some hyperbolic tangent transfer function\n> mlp:add( nn.Linear(25, 1) ) -- 1 output\n> =mlp:forward(torch.randn(10))\n-0.1815\n[torch.Tensor of dimension 1]\nLoss functions are implemented as sub-classes of codice_16, which has a similar interface to codice_7. It also has codice_8 and codice_9 methods for computing the loss and backpropagating gradients, respectively. Criteria are helpful to train neural network on classical tasks. Common criteria are the Mean Squared Error criterion implemented in codice_20 and the cross-entropy criterion implemented in codice_21. What follows is an example of a Lua function that can be iteratively called to train \nan codice_22 Module on input Tensor codice_23, target Tensor codice_24 with a scalar codice_25: \n\nfunction gradUpdate(mlp,x,y,learningRate)\nend\nIt also has codice_26 class for training a neural network using Stochastic gradient descent, although the Optim package provides much more options in this respect, like momentum and weight decay regularization.\n\nMany packages other than the above official packages are used with Torch. These are listed in the torch cheatsheet. These extra packages provide a wide range of utilities such as parallelism, asynchronous input/output, image processing, and so on. They can be installed with LuaRocks, the Lua package manager which is also included with the Torch distribution.\n\nTorch is used by the Facebook AI Research Group, IBM, Yandex and the Idiap Research Institute. Torch has been extended for use on Android and iOS. It has been used to build hardware implementations for data flows like those found in neural networks.\n\nFacebook has released a set of extension modules as open source software.\n\n\n", "id": "42571226", "title": "Torch (machine learning)"}
{"url": "https://en.wikipedia.org/wiki?curid=33520809", "text": "Theano (software)\n\nTheano is a numerical computation library for Python.\nIn Theano, computations are expressed using a NumPy-esque syntax and compiled to run efficiently on either CPU or GPU architectures.\n\nTheano is an open source project primarily developed by a machine learning group at the Université de Montréal.\n\nOn 28 September 2017, Pascal Lamblin announced major development would cease after the 1.0 release due to competing offerings by strong industrial players. Theano 1.0.0 was then released on 15 November 2017.\n\n", "id": "33520809", "title": "Theano (software)"}
{"url": "https://en.wikipedia.org/wiki?curid=47012074", "text": "Neural Designer\n\nNeural Designer is a software tool for data analytics based on neural networks, a main area of artificial intelligence research. It has been developed from the open source library OpenNN, and contains a graphical user interface which simplifies data entry and interpretation of results.\n\nIn 2014, this computer program was selected by the prestigious magazine \"Predictive Analytics Today\" among the top proprietary software for data mining. Also, during the same year, \"Big Data Analytics Today\" selected Neural Designer as one of the best brain inspired artificial intelligence projects.\n\nIn 2015, Neural Designer has been chosen by the European Commission, within the Horizon 2020 program, as a disruptive technology in the ICT field.\n\nNeural Designer performs descriptive, diagnostic, predictive and prescriptive data analytics. It implements deep architectures with multiple non-linear layers and contains utilities to solve function regression, pattern recognition, time series and autoencoding problems.\n\nThe input to Neural Designer is a data set, and the output from it is a predictive model. That result takes the form of an explicit mathematical expression, which can be exported to any computer language or system.\n\n\n", "id": "47012074", "title": "Neural Designer"}
{"url": "https://en.wikipedia.org/wiki?curid=1643246", "text": "Numenta\n\nNumenta is a machine intelligence company that has developed a cohesive theory, core software, technology and applications based on the principles of the neocortex. The company was founded on February 4, 2005 by Palm founder Jeff Hawkins with his longtime business partner Donna Dubinsky and Stanford graduate student Dileep George. Numenta is headquartered in Redwood City, California and is privately funded.\n\nNumenta has developed a number of example applications to demonstrate the applicability of its technology. Its first commercial product, Grok, offers anomaly detection for IT analytics, giving insight into IT systems to identify unusual behavior and reduce business downtime. Grok has since been licensed to their strategic partner, Avik Partners. Other applications include stock monitoring, geospatial tracking and rogue behavior.\n\nIn addition, Numenta has created NuPIC (Numenta Platform for Intelligent Computing) as an open source project and maintains an open source community discussion forum.\n\nThe company name comes from the Latin \"mentis\" (“pertaining to the mind”) genitive of \"mēns\" (“mind”).\n\nNumenta's machine intelligence technology is called hierarchical temporal memory (HTM), and is a computational theory of the neocortex. This theory was first described in the book \"On Intelligence\", written in 2004 by Jeff Hawkins and co-author Sandra Blakeslee. At the core of HTM are time-based learning algorithms that store and recall temporal patterns. The HTM algorithms are documented and available through its open source project, NuPIC. The HTM technology is suited to address a number of problems, particularly those with the following characteristics: streaming data, underlying patterns in data change over time, subtle patterns, time-based patterns.\n\nNumenta focuses on large-scale brain theory and simulation. Numenta researchers have written a number of peer-reviewed journal papers and research reports.\n\nNumenta is a technology provider and does not create go-to-market solutions for specific use cases. The company licenses their technology and application code to developers, organizations and companies who wish to build upon their technology.\". Numenta has several different types of licenses, including open source licenses, trial licenses and commercial licenses. Developers can use Numenta technology within NuPIC using the AGPL v3 open source license.\n\nThe following commercial applications are available using NuPIC: \nThe following tools are available on NuPIC:\nThe following example applications are available on NuPIC, see http://numenta.com/applications/:\n\nNumenta works with strategic partners, who license their technology and build products using HTM. Cortical.io is using HTM for natural language processing, the partnership was announced in May 2015. Avik Partners has licensed their Grok for IT Analytics application to monitor IT servers. The partnership was announced in August 2015. Numenta also partners with various research institutions and universities.\n\nThe Numenta Platform for Intelligent Computing (NuPIC) is an open source platform and community for machine intelligence based on HTM theory. NuPIC is an implementation of HTM and can be used to analyze streaming data. Numenta first announced in June 2013 that it would open-source its HTM technology, the core of its software and algorithms. This was accompanied by the new Numenta.org website and a mailing list for community members.\n\nCommunity members are contributors from around the world, and topics on the mailing list have included both discussions of the HTM theory as well as details of development of the software. The mission of NuPIC is to build and support a community, that is interested in machine learning and machine intelligence based on modeling the neocortex and its principles.\n\nNumenta has hosted a series of hackathons, the first one in 2013, to bring community members together to collaborate on NuPIC and its applications.\n\n", "id": "1643246", "title": "Numenta"}
{"url": "https://en.wikipedia.org/wiki?curid=49830146", "text": "Microsoft Cognitive Toolkit\n\nMicrosoft Cognitive Toolkit, previously known as CNTK and sometimes styled as The Microsoft Cognitive Toolkit, is a deep learning framework developed by Microsoft Research. Microsoft Cognitive Toolkit describes neural networks as a series of computational steps via a directed graph.\n\n", "id": "49830146", "title": "Microsoft Cognitive Toolkit"}
{"url": "https://en.wikipedia.org/wiki?curid=51545339", "text": "Apache SINGA\n\nApache SINGA is an Apache Incubating project for developing an open source machine learning library. It provides a flexible architecture for scalable distributed training, is extensible to run over a wide range of hardware, and has a focus on health-care applications.\n\nThe SINGA project was initiated by the DB System Group at National University of Singapore in 2014, in collaboration with the database group of Zhejiang University. It focused on distributed deep learning by partitioning the model and data onto nodes in a cluster and parallelize the training. The prototype was accepted by Apache Incubator in March 2015. Five versions have been released as shown in the following table. Since V1.0, SINGA is general to support traditional machine learning models such as logistic regression. Companies like NetEase, yzBigData and Shentilium are using SINGA for their applications.\n\nSINGA's software stack includes three major components, namely, core, IO and model. The following figure illustrates these components together with the hardware. The core component provides memory management and tensor operations; IO has classes for reading (and writing) data from (to) disk and network; The model component provides data structures and algorithms for machine learning models, e.g., layers for neural network models, optimizers/initializer/metric/loss for general machine learning models.\n\nTo get started with SINGA, there are some tutorials available as Jupyter notebooks. The tutorials cover the following:\n\n", "id": "51545339", "title": "Apache SINGA"}
{"url": "https://en.wikipedia.org/wiki?curid=51650259", "text": "Keras\n\nKeras is an open source neural network library written in Python. It is capable of running on top of MXNet, Deeplearning4j, TensorFlow, Microsoft Cognitive Toolkit or Theano. Designed to enable fast experimentation with deep neural networks, it focuses on being minimal, modular and extensible. It was developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System), and its primary author and maintainer is François Chollet, a Google engineer.\n\nIn 2017, Google's TensorFlow team decided to support Keras in TensorFlow's core library. Chollet explained that Keras was conceived to be an interface rather than an end-to-end machine-learning framework. It presents a higher-level, more intuitive set of abstractions that make it easy to configure neural networks regardless of the backend scientific computing library. Microsoft has been working to add a CNTK backend to Keras as well, and the functionality is currently in beta release with CNTK v2.0 .\n\nThe library contains numerous implementations of commonly used neural network building blocks such as layers, objectives, activation functions, optimizers, and a host of tools to make working with image and text data easier. The code is hosted on GitHub, and community support forums include the GitHub issues page, a Gitter channel and a Slack channel.\n\n, Keras is the second-fastest growing deep learning framework after Google's TensorFlow, and the third largest after TensorFlow and Caffe.\n\n", "id": "51650259", "title": "Keras"}
{"url": "https://en.wikipedia.org/wiki?curid=52513310", "text": "MXNet\n\nApache MXNet is a modern open-source deep learning framework used to train, and deploy deep neural networks. It is scalable, allowing for fast model training, and supports a flexible programming model and multiple languages (C++, Python, Julia, Matlab, JavaScript, Go, R, Scala, Perl, Wolfram Language)\n\nThe MXNet library is portable and can scale to multiple GPUs and multiple machines. MXNet is supported by major Public Cloud providers including AWS and Azure. Amazon has chosen MXNet as its deep learning framework of choice at AWS. Currently, MXNet is supported by Intel, Dato, Baidu, Microsoft, Wolfram Research, and research institutions such as Carnegie Mellon, MIT, the University of Washington, and the Hong Kong University of Science and Technology.\n\nApache MXNet is a lean, flexible, and ultra-scalable deep learning framework that supports state of the art in deep learning models, including convolutional neural networks (CNNs) and long short-term memory networks (LSTMs).\n\nMXNet is designed to be distributed on dynamic Cloud infrastructure, using distributed parameter server (based on ), and can achieve almost linear scale with multiple GPU/CPU.\n\nMXNet supports both imperative and symbolic programming, which makes it easier for developers that are used to imperative programming to get started with deep learning. It also makes it easier to track, debug, save checkpoints, modify hyperparameters, such as learning rate or perform early stopping.\n\nSupports C++ for the optimized backend to get the most of the GPU or CPU available, and Python, R, Scala, Julia, Perl, Matlab and Javascript for the simple to use frontend for the developers.\n\nSupports an efficient deployment of a trained model to low-end devices for inference, such as mobile devices (using Amalgamation Amalgamation), IoT devices (using AWS Greengrass), Serverless (Using AWS Lambda) or containers. These low-end environments can have only weaker CPU or limited memory (RAM), and should be able to use the models that were trained on a higher-level environment (GPU based cluster, for example).\n\n", "id": "52513310", "title": "MXNet"}
{"url": "https://en.wikipedia.org/wiki?curid=52801963", "text": "AlexNet\n\nAlexNet is the name of a convolutional neural network, originally written with CUDA to run with GPU support, which competed in the ImageNet Large Scale Visual Recognition Challenge in 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points ahead of the runner up. AlexNet was designed by the SuperVision group, consisting of Alex Krizhevsky, Geoffrey Hinton, and Ilya Sutskever. \n\nAlexnet contained only 8 layers, first 5 were convolutional layers followed by fully connected layers.\n", "id": "52801963", "title": "AlexNet"}
{"url": "https://en.wikipedia.org/wiki?curid=48508507", "text": "TensorFlow\n\nTensorFlow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and also used for machine learning applications such as neural networks. It is used for both research and production at Google,   often replacing its closed-source predecessor, DistBelief.\n\nTensorFlow was developed by the Google Brain team for internal Google use. It was released under the Apache 2.0 open source license on November 9, 2015.\n\nStarting in 2011, Google Brain built DistBelief as a proprietary machine learning system based on deep learning neural networks. Its use grew rapidly across diverse Alphabet companies in both research and commercial applications. Google assigned multiple computer scientists, including Jeff Dean, to simplify and refactor the codebase of DistBelief into a faster, more robust application-grade library, which became TensorFlow. In 2009, the team, led by Geoffrey Hinton, had implemented generalized backpropagation and other improvements which allowed generation of neural networks with substantially higher accuracy, for instance a 25% reduction in errors in speech recognition.\n\nTensorFlow is Google Brain's second generation system. Version 1.0.0 was released on February 11, 2017. While the reference implementation runs on single devices, TensorFlow can run on multiple CPUs and GPUs (with optional CUDA and SYCL extensions for general-purpose computing on graphics processing units). TensorFlow is available on 64-bit Linux, macOS, Windows, and mobile computing platforms including Android and iOS.\n\nTensorFlow computations are expressed as stateful dataflow graphs. The name TensorFlow derives from the operations that such neural networks perform on multidimensional data arrays. These arrays are referred to as \"tensors\". In June 2016, Dean stated that 1,500 repositories on GitHub mentioned TensorFlow, of which only 5 were from Google.\n\nIn May 2016 Google announced its Tensor processing unit (TPU), an ASIC built specifically for machine learning and tailored for TensorFlow. TPU is a programmable AI accelerator designed to provide high throughput of low-precision arithmetic (e.g., 8-bit), and oriented toward using or running models rather than training them. Google announced they had been running TPUs inside their data centers for more than a year, and have found them to deliver an order of magnitude better-optimized performance per watt for machine learning.\n\nIn May 2017 Google announced the second-generation, as well as the availability of the TPUs in Google Compute Engine. The second-generation TPUs deliver up to 180 teraflops of performance, and when organized into clusters of 64 TPUs, provide up to 11.5 petaflops.\n\nIn May 2017 Google announced a software stack specifically for Android development, TensorFlow Lite, beginning with Android Oreo.\n\nGoogle officially released RankBrain on October 26, 2015, backed by TensorFlow.\n\nTensorFlow provides a Python API, as well as C++, Haskell, Java, Go, and Rust APIs. Third party packages are available for C#, Julia, R, and Scala.\n\nAmong the applications for which TensorFlow is the foundation, are automated image captioning software, such as DeepDream. RankBrain now handles a substantial number of search queries, replacing and supplementing traditional static algorithm based search results.\n\n\n", "id": "48508507", "title": "TensorFlow"}
{"url": "https://en.wikipedia.org/wiki?curid=54022970", "text": "PyTorch\n\nPyTorch is an open source machine learning library for Python, used for applications such as natural language processing. It is primarily developed by Facebook's artificial-intelligence research group, and Uber's \"Pyro\" software for probabilistic programming is built upon it.\n\n", "id": "54022970", "title": "PyTorch"}
{"url": "https://en.wikipedia.org/wiki?curid=56173403", "text": "Deep Image Prior\n\nDeep Image Prior is a type of convolutional neural network used to enhance a given image with no prior training data other than the image itself.\nA neural-network is randomly initialized and used as prior to solve inverse problems such as noise reduction, super-resolution, and inpainting. Image statistics is captured by the structure of a convolutional image generator rather than by any previously learned capabilities.\n\nInverse problems such as noise reduction, super-resolution, and inpainting can be formulated as the optimization task formula_1, where formula_2 is an image, formula_3 a corrupted representation of that image, formula_4 is a task-dependent data term, and R(x) is the regularizer. This forms an energy minimization problem.\n\nDeep neural networks learn a generator/decoder formula_5 which maps a random code vector formula_6 to an image formula_2.\n\nThe image corruption method used to generate formula_3 is selected for the specific application.\n\nIn this approach, the formula_9 prior is replaced with the implicit prior captured by the neural network (where formula_10 for images that can be produced by a deep neural networks and formula_11 otherwise). This yields the equation for the minimizer formula_12 and the result of the optimization process formula_13.\n\nThe minimizer formula_14 (typically a gradient descent) starts from a randomly initialized parameters and descends into a local best result to yield the formula_15 restoration function.\n\nA parameter θ may be used to recover any image, including its noise. However, the network is reluctant to pick up noise because it contains high impedance while useful signal offers low impedance. This results in the θ parameter approaching a good-looking local optimum so long as the number of iterations in the optimization process remains low enough not to overfit data.\n\nThe principle of denoising is to recover an image formula_2 from a noisy observation formula_3, where formula_18. The distribution formula_19 is sometimes known (eg: profiling sensor and photon noise) and may optionally be incorporated into the model, though this process works well in blind denoising.\n\nThe quadratic energy function formula_20 is used as the data term, plugging it into the equation for formula_14 yields the optimization problem formula_22.\n\nSuper-resolution is used to generate a higher resolution version of image x. The data term is set to formula_23 where d(·) is a downsampling operator such as Lanczos that decimates the image by a factor t.\n\nInpainting is used to reconstruct a missing area in an image formula_3. These missing pixels are defined as the binary mask formula_25. The data term is defined as formula_26 (where formula_27 is the Hadamard product).\n\nThis approach may be extended to multiple images. A straightforward example mentioned by the author is the reconstruction of an image to obtain natural light and clarity from a flash-no-flash pair. Video reconstruction is possible but it requires optimizations to take into account the spatial differences.\n\n", "id": "56173403", "title": "Deep Image Prior"}
{"url": "https://en.wikipedia.org/wiki?curid=56086085", "text": "Deep Instinct\n\nDeep Instinct is a cybersecurity startup that applies deep learning into cybersecurity. The company implements advanced Artificial Intelligence to the task of detecting malware. \n\nDeep Instinct was founded in the year 2014 by Guy Caspi, Dr. Eli David, and Nadav Maman. The headquarters of the company is located in Israel and San Francisco. This software is designed to protect an organization’s endpoints, mobile devices, servers against known and unknown malware attacks in real-time. It is said to detect and solve cyber attacks on all kind of operating systems. In July 2017, the company has raised $32 million in a Series B financing round led by CNTP.\n\nDeep Instinct’s endpoint protection software includes an artificial neural network which imitates the human brain. With its advanced AI technologies, it can identify malicious applications on mobile devices, servers, and desktop. Using proprietary deep learning algorithms, this software predicts unknown cyber attacks.\n\nTo run non-graphics software on graphic chips, the software runs on the combination of Central Processing Units (CPU) and Graphics Processing Units (GPU), and NVIDIA’s CUDA software.\n\nIn 2016, Deep Instinct was named as the “Most Innovative Startup” at the Black Hat IT Security Conference.\n\nIt was also named as the “Most Disruptive” AI startup by the US chip maker NVIDIA during 2017 Inception Awards.  In the same year, World Economic Forum recognized them as “Technology Pioneer.”\n", "id": "56086085", "title": "Deep Instinct"}
{"url": "https://en.wikipedia.org/wiki?curid=12468879", "text": "Immunocomputing\n\nImmunocomputing explores the principles of information processing that proteins and immune networks utilize in order to solve specific complex problems while protected from viruses, noise, errors and intrusions.\n\nIt intends to establish:\n\n\nThe main difference with other kinds of computing lay on the function of its \"basic element\", the formal protein, defined according with its biological prototype and its mathematical model. \n\nThe main biophysical issues considered in immunocomputing are:\n\n\nFormal immune networks (FINs) have as closest model the \"idiotypic network\" of \"N. Jerne\" but they consider specific mechanisms of interactions between proteins. FINs are able to learn, recognize and solve problems.\n", "id": "12468879", "title": "Immunocomputing"}
{"url": "https://en.wikipedia.org/wiki?curid=25532348", "text": "MIMIC (immunology)\n\nMIMIC, or modular immune in vitro construct, is an artificial system imitating the human immune system. It has applications in vaccine development.\n\nWhite blood cells, specifically peripheral blood mononuclear cells including T cells and B cells, from human donars are placed in standard tubes containing specially designed tissue constructs made out of collagen, where they develop into small but functioning immune systems.Up to ninety-six individual tubes can be carried on a plate the size of a deck of cards, allowing scientists to use cells from almost a hundred different donors at once.\n\nThe MIMIC system replaces some steps in the vaccine development process that would otherwise be performed on animals and offers scientists better speed and flexibility than traditional methods. However, critics are concerned that MIMIC may be too simple for use as widespread as its developers hope.\n\nThe MIMIC system was developed by VaxDesign and became available for use in 2008.\n", "id": "25532348", "title": "MIMIC (immunology)"}
{"url": "https://en.wikipedia.org/wiki?curid=1589987", "text": "Artificial immune system\n\nIn artificial intelligence, artificial immune systems (AIS) are a class of computationally intelligent, rule-based machine learning systems inspired by the principles and processes of the vertebrate immune system. The algorithms are typically modeled after the immune system's characteristics of learning and memory for use in problem-solving.\n\nThe field of Artificial Immune Systems (AIS) is concerned with abstracting the structure and function of the immune system to computational systems, and investigating the application of these systems towards solving computational problems from mathematics, engineering, and information technology. AIS is a sub-field of Biologically-inspired computing, and Natural computation, with interests in Machine Learning and belonging to the broader field of Artificial Intelligence.\n\nArtificial Immune Systems (AIS) are adaptive systems, inspired by theoretical immunology and observed immune functions, principles and models, which are applied to problem solving.\n\nAIS is distinct from computational immunology and theoretical biology that are concerned with simulating immunology using computational and mathematical models towards better understanding the immune system, although such models initiated the field of AIS and continue to provide a fertile ground for inspiration. Finally, the field of AIS is not concerned with the investigation of the immune system as a substrate for computation, unlike other fields such as DNA computing.\n\nAIS emerged in the mid 1980s with articles authored by Farmer, Packard and Perelson (1986) and Bersini and Varela (1990) on immune networks. However, it was only in the mid 1990s that AIS became a field in its own right. Forrest \"et al.\" (on negative selection) and Kephart \"et al.\" published their first papers on AIS in 1994, and Dasgupta conducted extensive studies on Negative Selection Algorithms. Hunt and Cooke started the works on Immune Network models in 1995; Timmis and Neal continued this work and made some improvements. De Castro & Von Zuben's and Nicosia & Cutello's work (on clonal selection) became notable in 2002. The first book on Artificial Immune Systems was edited by Dasgupta in 1999.\n\nCurrently, new ideas along AIS lines, such as danger theory and algorithms inspired by the innate immune system, are also being explored. Although some believe that these new ideas do not yet offer any truly 'new' abstract, over and above existing AIS algorithms. This, however, is hotly debated, and the debate provides one of the main driving forces for AIS development at the moment. Other recent developments involve the exploration of degeneracy in AIS models, which is motivated by its hypothesized role in open ended learning and evolution.\n\nOriginally AIS set out to find efficient abstractions of processes found in the immune system but, more recently, it is becoming interested in modelling the biological processes and in applying immune algorithms to bioinformatics problems.\n\nIn 2008, Dasgupta and Nino published a textbook on Immunological Computation which presents a compendium of up-to-date work related to immunity-based techniques and describes a wide variety of applications.\n\nThe common techniques are inspired by specific immunological theories that explain the function and behavior of the mammalian adaptive immune system.\n\n\n\n\n\n\n\n", "id": "1589987", "title": "Artificial immune system"}
{"url": "https://en.wikipedia.org/wiki?curid=20434643", "text": "Clonal selection algorithm\n\nIn artificial immune systems, clonal selection algorithms are a class of algorithms inspired by the clonal selection theory of acquired immunity that explains how B and T lymphocytes improve their response to antigens over time called affinity maturation. These algorithms focus on the Darwinian attributes of the theory where selection is inspired by the affinity of antigen-antibody interactions, reproduction is inspired by cell division, and variation is inspired by somatic hypermutation. Clonal selection algorithms are most commonly applied to optimization and pattern recognition domains, some of which resemble parallel hill climbing and the genetic algorithm without the recombination operator.\n\n\n\n", "id": "20434643", "title": "Clonal selection algorithm"}
{"url": "https://en.wikipedia.org/wiki?curid=5751182", "text": "Human-based evolutionary computation\n\nHuman-based evolutionary computation (HBEC) is a set of evolutionary computation techniques that rely on human innovation. Human-based evolutionary computation techniques can be classified into three more specific classes analogous to ones in evolutionary computation. There are three basic types of innovation: initialization, mutation, and recombination. Here is a table illustrating which type of human innovation are supported in different classes of HBEC:\n\nAll these three classes also have to implement selection, performed either by humans or by computers.\n\nHuman-based selection strategy is a simplest human-based evolutionary computation procedure. It is used heavily today by websites outsourcing collection and selection of the content to humans (user-contributed content). Viewed as evolutionary computation, their mechanism supports two operations: initialization (when a user adds a new item) and selection (when a user expresses preference among items). The website software aggregates the preferences to compute the fitness of items so that it can promote the fittest items and discard the worst ones. Several methods of human-based selection were analytically compared in (Kosorukoff, 2000; Gentry, 2005).\n\nBecause the concept seems too simple, most of the websites implementing the idea can't avoid the common pitfall: informational cascade in soliciting human preference. For example, digg-style implementations, pervasive on the web, heavily bias subsequent human evaluations by prior ones by showing how many votes the items already have. This makes the aggregated evaluation depend on a very small initial sample of rarely independent evaluations. This encourages many people to game the system that might add to digg's popularity but detract from the quality of the featured results. It is too easy to submit evaluation in digg-style system based only on the content title, without reading the actual content supposed to be evaluated.\n\nA better example of a human-based selection system is Stumbleupon. In Stumbleupon, users first experience the content (stumble upon it), and can then submit their preference by pressing a thumb-up or thumb-down button. Because the user doesn't see the number of votes given to the site by previous users, Stumbleupon can collect a relatively unbiased set of user preferences, and thus evaluate content much more precisely.\n\nIn this context and maybe generally, the Wikipedia software is the best illustration of a working human-based evolution strategy wherein the (targeted) evolution of any given page comprises the fine tuning of the knowledge base of such information that relates to that page. Traditional evolution strategy has three operators: initialization, mutation, and selection. In the case of Wikipedia, the initialization operator is page creation, the mutation operator is incremental page editing. The selection operator is less salient. It is provided by the revision history and the ability to select among all previous revisions via a revert operation. If the page is vandalised and no longer a good fit to its title, a reader can easily go to the revision history and select one of the previous revisions that fits best (hopefully, the previous one). This selection feature is crucial to the success of the Wikipedia.\n\nAn interesting fact is that the original wiki software was created in 1995, but it took at least another six years for large wiki-based collaborative projects to appear. Why did it take so long? One explanation is that the original wiki software lacked a selection operation and hence couldn't effectively support content evolution. The addition of revision history and the rise of large wiki-supported communities coincide in time. From an evolutionary computation point of view, this is not surprising: without a selection operation the content would undergo an aimless genetic drift and would unlikely to be useful to anyone. That is what many people expected from Wikipedia at its inception. However, with a selection operation, the utility of content has a tendency to improve over time as beneficial changes accumulate. This is what actually happens on a large scale in Wikipedia.\n\nHuman-based genetic algorithm (HBGA) provides means for human-based recombination operation (a distinctive feature of genetic algorithms). Recombination operator brings together highly fit parts of different solutions that evolved independently. This makes the evolutionary process more efficient.\n\n", "id": "5751182", "title": "Human-based evolutionary computation"}
{"url": "https://en.wikipedia.org/wiki?curid=4483084", "text": "IEEE Congress on Evolutionary Computation\n\nThe IEEE Congress on Evolutionary Computation (CEC) is one of the largest and most important conferences within Evolutionary computation (EC), the other conferences of similar importance being Genetic and Evolutionary Computation Conference (GECCO), Parallel Problem Solving from Nature (PPSN) and European Conference on Genetic Programming (EuroGP).\n\nCEC, which is organized by the IEEE Computational Intelligence Society in cooperation with the Evolutionary Programming Society, covers most subtopics of EC, such as Evolutionary robotics, Multiobjective optimization, Evolvable hardware, Theory of evolutionary computation, Evolutionary design etc. Papers can also be found that deal with topics which are related to rather than part of EC, such Ant colony optimization, Swarm intelligence and Quantum computing.\n\nThe conference usually attracts several hundreds of attendees, as well as hundreds of papers.\n\n", "id": "4483084", "title": "IEEE Congress on Evolutionary Computation"}
{"url": "https://en.wikipedia.org/wiki?curid=12836631", "text": "Java Evolutionary Computation Toolkit\n\nECJ is a freeware evolutionary computation research system written in Java. It is a framework that supports a variety of evolutionary computation techniques, such as genetic algorithms, genetic programming, evolution strategies, coevolution, particle swarm optimization, and differential evolution. The framework models iterative evolutionary processes using a series of pipelines arranged to connect one or more subpopulations of individuals with selection, breeding (such as crossover, and mutation operators that produce new individuals. The framework is open source and is distributed under the Academic Free License. ECJ was created by Sean Luke, a computer science professor at George Mason University, and is maintained by Sean Luke and a variety of contributors.\n\nFeatures (listed from ECJ's project page):\n\nGeneral Features:\n\n\nEC Features:\n\n\nGP Tree Representations:\n\n\nVector (GA/ES) Representations:\n\n\nOther Representations:\n\n\n\n", "id": "12836631", "title": "Java Evolutionary Computation Toolkit"}
{"url": "https://en.wikipedia.org/wiki?curid=1514142", "text": "Convergence (evolutionary computing)\n\nConvergence is a phenomenon in evolutionary computation. It causes evolution to halt because precisely every individual in the population is identical. Full convergence might be seen in genetic algorithms (a type of evolutionary computation) using only crossover (a way of combining individuals to make new offspring). Premature convergence is when a population has converged to a single solution, but that solution is not as high of quality as expected, i.e. the population has gotten 'stuck'. However, convergence is not necessarily a negative thing, because populations often stabilise after a time, in the sense that the best programs all have a common ancestor and their behaviour is very similar (or identical) both to each other and to that of high fitness programs from the previous generations. Often the term \"convergence\" is loosely used. Convergence can be avoided with a variety of diversity-generating techniques.\n\nFoundations of Genetic Programming\n", "id": "1514142", "title": "Convergence (evolutionary computing)"}
{"url": "https://en.wikipedia.org/wiki?curid=1346015", "text": "PORS\n\nPORS stands for Plus One Recall Store. It is a problem used in evolutionary computation and genetic programming.\n\nThe PORS language consists of two terminal nodes (1 and recall), one unary operation (store) and one binary operation (plus) that together make up a parse tree that calculates a number.\n", "id": "1346015", "title": "PORS"}
{"url": "https://en.wikipedia.org/wiki?curid=6548718", "text": "Learnable evolution model\n\nThe learnable evolution model (LEM) is a novel, non-Darwinian methodology for evolutionary computation that employs machine learning to guide the generation of new individuals (candidate problem solutions). Unlike standard, Darwinian-type evolutionary computation methods that use random or semi-random operators for generating new individuals (such as mutations and/or recombinations), LEM employs hypothesis generation and instantiation operators.\nThe hypothesis generation operator applies a machine learning program to induce descriptions that distinguish between high-fitness and low-fitness individuals in each consecutive population. Such descriptions delineate areas in the search space that most likely contain the desirable solutions. Subsequently the instantiation operator samples these areas to create new individuals.\nLEM has been modified from optimization domain to classification domain by augmented LEM with ID3. (February 2013 by M. Elemam Shehab, K. Badran, M. Zaki and Gouda I. Salama.\n\n", "id": "6548718", "title": "Learnable evolution model"}
{"url": "https://en.wikipedia.org/wiki?curid=3062637", "text": "Estimation of distribution algorithm\n\nEstimation of distribution algorithms (EDAs), sometimes called probabilistic model-building genetic algorithms (PMBGAs) , are stochastic optimization methods that guide the search for the optimum by building and sampling explicit probabilistic models of promising candidate solutions. Optimization is viewed as a series of incremental updates of a probabilistic model, starting with the model encoding the uniform distribution over admissible solutions and ending with the model that generates only the global optima.\n\nEDAs belong to the class of evolutionary algorithms. The main difference between EDAs and most conventional evolutionary algorithms is that evolutionary algorithms generate new candidate solutions using an \"implicit\" distribution defined by one or more variation operators, whereas EDAs use an \"explicit\" probability distribution encoded by a Bayesian network, a multivariate normal distribution, or another model class. Similarly as other evolutionary algorithms, EDAs can be used to solve optimization problems defined over a number of representations from vectors to LISP style S expressions, and the quality of candidate solutions is often evaluated using one or more objective functions.\n\nThe general procedure of an EDA is outlined in the following:\nUsing explicit probabilistic models in optimization allowed EDAs to feasibly solve optimization problems that were notoriously difficult for most conventional evolutionary algorithms and traditional optimization techniques, such as problems with high levels of epistasis . Nonetheless, the advantage of EDAs is also that these algorithms provide an optimization practitioner with a series of probabilistic models that reveal a lot of information about the problem being solved. This information can in turn be used to design problem-specific neighborhood operators for local search, to bias future runs of EDAs on a similar problem, or to create an efficient computational model of the problem.\n\nFor example, if the population is represented by bit strings of length 4, the EDA can represent the population of promising solution using a single vector of four probabilities (p1, p2, p3, p4) where each component of p defines the probability of that position being a 1. Using this probability vector it is possible to create an arbitrary number of candidate solutions.\n\nThis section describes the models built by some well known EDAs of different levels of complexity. It is always assumed a population formula_1 at the generation formula_2, a selection operator formula_3, a model-building operator formula_4 and a sampling operator formula_5.\n\nThe most simple EDAs assume that decision variables are independent, i.e. formula_6. Therefore, univariate EDAs rely only on univariate statistics and multivariate distributions must be factorized as the product of formula_7 univariate probability distributions,\n\nformula_8\n\nSuch factorizations are used in many different EDAs, next we describe some of them.\n\nThe UMDA is a simple EDA that uses an operator formula_9 to estimate marginal probabilities from a selected population formula_10. By assuming formula_10 contain formula_12 elements, formula_9 produces probabilities:\n\nformula_14\n\nEvery UMDA step can be described as follows\n\nformula_15\n\nThe PBIL, represents the population implicitly by its model, from which it samples new solutions and updates the model. At each generation, formula_16 individuals are sampled and formula_17 are selected. Such individuals are then used to update the model as follows\n\nformula_18\n\nwhere formula_19 is a parameter defining the learning rate, a small value determines that the previous model formula_20 should be only slightly modified by the new solutions sampled. PBIL can be described as\n\nformula_21\n\nThe CGA, also relies on the implicit populations defined by univariate distributions. At each generation formula_2, two individuals formula_23 are sampled, formula_24. The population formula_1 is then sort in decreasing order of fitness, formula_26, with formula_27 being the best and formula_28 being the worst solution. The CGA estimates univariate probabilities as follows\n\nformula_29\n\nwhere, formula_19 is a constant defining the learning rate, usually set to formula_31. The CGA can be defined as\n\nformula_32\n\nAlthough univariate models can be computed efficiently, in many cases they are not representative enough to provide better performance than GAs. In order to overcome such a drawback, the use of bivariate factorizations was proposed in the EDA community, in which dependencies between pairs of variables could be modeled. A bivariate factorization can be defined as follows, where formula_33 contains a possible variable dependent to formula_34, i.e. formula_35.\n\nformula_36\n\nBivariate and multivariate distributions are usually represented as Probabilistic Graphical Models (graphs), in which edges denote statistical dependencies (or conditional probabilities) and vertices denote variables. To learn the structure of a PGM from data linkage-learning is employed.\n\nThe MIMIC factorizes the joint probability distribution in a chain-like model representing successive dependencies between variables. It finds a permutation of the decision variables, formula_37, such that formula_38 minimizes the Kullback-Leibler divergence in relation to the true probability distribution, i.e. formula_39. MIMIC models a distribution\n\nformula_40\n\nNew solutions are sampled from the leftmost to the rightmost variable, the first is generated independently and the others according to conditional probabilities. Since the estimated distribution must be recomputed each generation, MIMIC uses concrete populations in the following way\n\nformula_41\n\nThe BMDA factorizes the joint probability distribution in bivariate distributions. First, a randomly chosen variable is added as a node in a graph, the most dependent variable to one of those in the graph is chosen among those not yet in the graph, this procedure is repeated until no remaining variable depends on any variable in the graph (verified according to a threshold value).\n\nThe resulting model is a forest with multiple trees rooted at nodes formula_42. Considering formula_43 the non-root variables, BMDA estimates a factorized distribution in which the root variables can be sampled independently, whereas all the others must be conditioned to the parent variable formula_33.\n\nformula_45\n\nEach step of BMDA is defined as follows\n\nformula_46\n\nThe next stage of EDAs development was the use of multivariate factorizations. In this case, the joint probability distribution is usually factorized in a number of components of limited size formula_47.\n\nformula_48\n\nThe learning of PGMs encoding multivariate distributions is a computationally expensive task, therefore, it is usual for EDAs to estimate multivariate statistics from bivariate statistics. Such relaxation allows PGM to be built in polynomial time in formula_7; however, it also limits the generality of such EDAs.\n\nThe ECGA was one of the first EDA to employ multivariate factorizations, in which high-order dependencies among decision variables can be modeled. Its approach factorizes the joint probability distribution in the product of multivariate marginal distributions. Assume formula_50 is a set of subsets, in which every formula_51 is a linkage set, containing formula_52 variables. The factorized joint probability distribution is represented as follows\n\nformula_53\n\nThe ECGA popularized the term \"linkage-learning\" as denoting procedures that identify linkage sets. Its linkage-learning procedure relies on two measures: (1) the Model Complexity (MC) and (2) the Compressed Population Complexity (CPC). The MC quantifies the model representation size in terms of number of bits required to store all the marginal probabilities\n\nformula_54\n\nThe CPC, on the other hand, quantifies the data compression in terms of entropy of the marginal distribution over all partitions, where formula_12 is the selected population size, formula_56 is the number of decision variables in the linkage set formula_57 and formula_58 is the joint entropy of the variables in formula_57\n\nformula_60\n\nThe linkage-learning in ECGA works as follows: (1) Insert each variable in a cluster, (2) compute CCC = MC + CPC of the current linkage sets, (3) verify the increase on CCC provided by joining pairs of clusters, (4) effectively joins those clusters with highest CCC improvement. This procedure is repeated until no CCC improvements are possible and produces a linkage model formula_61. The ECGA works with concrete populations, therefore, using the factorized distribution modeled by ECGA, it can be described as\n\nformula_62\n\nThe BOA uses Bayesian networks to model and sample promising solutions. Bayesian networks are directed acyclic graphs, with nodes representing variables and edges representing conditional probabilities between pair of variables. The value of a variable formula_63 can be conditioned on a maximum of formula_64 other variables, defined in formula_33. BOA builds a PGM encoding a factorized joint distribution, in which the parameters of the network, i.e. the conditional probabilities, are estimated from the selected population using the maximum likelihood estimator.\n\nformula_66\n\nThe Bayesian network structure, on the other hand, must be built iteratively (linkage-learning). It starts with a network without edges and, at each step, adds the edge which better improves some scoring metric (e.g. Bayesian information criterion (BIC) or Bayesian-Dirichlet metric with likelihood equivalence (BDe)). The scoring metric evaluates the network structure according to its accuracy in modeling the selected population. From the built network, BOA samples new promising solutions as follows: (1) it computes the ancestral ordering for each variable, each node being preceded by its parents; (2) each variable is sampled conditionally to its parents. Given such scenario, every BOA step can be defined as\n\nformula_67\n\nThe LTGA differs from most EDA in the sense it does not explicitly model a probabilisty distribution but only a linkage model, called linkage-tree. A linkage formula_68 is a set of linkage sets with no probability distribution associated, therefore, there is no way to sample new solutions directly from formula_68. The linkage model is a linkage-tree produced stored as a Family of sets (FOS).\n\nformula_70\n\nThe linkage-tree learning procedure is a hierarchical clustering algorithm, which work as follows. At each step the two \"closest\" clusters formula_71 and formula_72 are merged, this procedure repeats until only one cluster remains, each subtree is stored as a subset formula_73.\n\nThe LTGA uses formula_74 to guide an \"optimal mixing\" procedure which resembles a recombination operator but only accepts improving moves. We denote it as formula_75, where the notation formula_76 indicates the transfer of the genetic material indexed by formula_57 from formula_78 to formula_79.\nThe LTGA does not implement typical selection operators, instead, selection is performed during recombination. Similar ideas have been usually applied into local-search heuristics and, in this sense, the LTGA can be seen as an hybrid method. In summary, one step of the LTGA is defined as\n\nformula_95\n\n\n", "id": "3062637", "title": "Estimation of distribution algorithm"}
{"url": "https://en.wikipedia.org/wiki?curid=26536158", "text": "Cooperative coevolution\n\nCooperative Coevolution (CC) is an evolutionary computation method that divides a large problem into subcomponents and solves them independently in order to solve the large problem. \n\nThe subcomponents are also called species. The subcomponents are implemented as subpopulations and the only interaction between subpopulations is in the cooperative evaluation of each individual of the subpopulations. The general CC framework is nature inspired where the individuals of a particular group of species mate amongst themselves, however, mating in between different species is not feasible. The cooperative evaluation of each individual in a subpopulation is done by concatenating the current individual with the best individuals from the rest of the subpopulations as described by M. Potter.\n\nThe cooperative coevolution framework has been applied to real world problems such as pedestrian detection systems, large-scale function optimization and neural network training.\nIt has also be further extended into another method, called Constructive cooperative coevolution.\n\n \"i\" = 0 \n\n", "id": "26536158", "title": "Cooperative coevolution"}
{"url": "https://en.wikipedia.org/wiki?curid=30035652", "text": "Evolutionary Computation (journal)\n\nEvolutionary Computation is a peer-reviewed academic journal published four times a year by the MIT Press. The journal serves as an international forum for researchers exchanging information in the field which deals with computational systems drawing their inspiration from nature.\n\nAccording to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 3.826.\n\nEditor-in-chief is Emma Hart. She succeeded Hans-Georg Beyer, who was editor-in-chief from 2010 until 2016.\n", "id": "30035652", "title": "Evolutionary Computation (journal)"}
{"url": "https://en.wikipedia.org/wiki?curid=25839999", "text": "Computer-automated design\n\nDesign Automation usually refers to electronic design automation, or Design Automation which is a Product Configurator. Extending Computer-Aided Design (CAD), automated design and Computer-Automated Design (CAutoD) are more concerned with a broader range of applications, such as automotive engineering, civil engineering, composite material design, control engineering, dynamic system identification, financial systems, industrial equipment, mechatronic systems, steel construction, structural optimisation, and the invention of novel systems.\n\nThe concept of CAutoD perhaps first appeared in 1963, in the IBM Journal of Research and Development , where a computer program was written.\n\n(1) to search for logic circuits having certain constraints on hardware design and \n\n(2) to evaluate these logics in terms of their discriminating ability over samples of the character set they are expected to recognize. \n\nMore recently, traditional CAD simulation is seen to be transformed to CAutoD by biologically-inspired machine learning or search techniques such as evolutionary computation, including swarm intelligence algorithms.\n\nTo meet the ever growing demand of quality and competitiveness, iterative physical prototyping is now often replaced by 'digital prototyping' of a 'good design', which aims to meet multiple objectives such as maximised output, energy efficiency, highest speed and cost-effectiveness. The design problem concerns both finding the best design within a known range (i.e., through 'learning' or 'optimisation') and finding a new and better design beyond the existing ones (i.e., through creation and invention). This is equivalent to a search problem in an almost certainly, multidimensional (multivariate), multi-modal space with a single (or weighted) objective or multiple objectives.\n\nUsing single-objective CAutoD as an example, if the objective function, either as a cost function formula_1, or inversely, as a fitness function formula_2, where\n\nis differentiable under practical constraints in the multidimensional space, the design problem may be solved analytically. Finding the parameter sets that result in a zero first-order derivative and that satisfy the second-order derivative conditions would reveal all local optima. Then comparing the values of the performance index of all the local optima, together with those of all boundary parameter sets, would lead to the global optimum, whose corresponding 'parameter' set will thus represent the best design. However, in practice, the optimization usually involves multiple objectives and the matters involving derivatives are lot more complex.\n\nIn practice, the objective value may be noisy or even non-numerical, and hence its gradient information may be unreliable or unavailable. This is particularly true when the problem is multi-objective. At present, many designs and refinements are mainly made through a manual trial-and-error process with the help of a CAD simulation package. Usually, such \"a posteriori\" learning or adjustments need to be repeated many times until a ‘satisfactory’ or ‘optimal’ design emerges.\n\nIn theory, this adjustment process can be automated by computerised search, such as exhaustive search. As this is an exponential algorithm, it may not deliver solutions in practice within a limited period of time.\n\nOne approach to virtual engineering and automated design is evolutionary computation such as evolutionary algorithms.\n\nTo reduce the search time, the biologically-inspired evolutionary algorithm (EA) can be used instead, which is a (non-deterministic) polynomial algorithm. The EA based multi-objective \"search team\" can be interfaced with an existing CAD simulation package in a batch mode. The EA encodes the design parameters (encoding being necessary if some parameters are non-numerical) to refine multiple candidates through parallel and interactive search. In the search process, 'selection' is performed using 'survival of the fittest' \"a posteriori\" learning. To obtain the next 'generation' of possible solutions, some parameter values are exchanged between two candidates (by an operation called 'crossover') and new values introduced (by an operation called 'mutation'). This way, the evolutionary technique makes use of past trial information in a similarly intelligent manner to the human designer.\n\nThe EA based optimal designs can start from the designer's existing design database or from an initial generation of candidate designs obtained randomly. A number of finally evolved top-performing candidates will represent several automatically optimized digital prototypes.\n\nThere are websites that demonstrate interactive evolutionary algorithms for design. EndlessForms.com allows you to evolve 3D objects online and have them 3D printed. PicBreeder.org allows you to do the same for 2D images.\n\n\n", "id": "25839999", "title": "Computer-automated design"}
{"url": "https://en.wikipedia.org/wiki?curid=34542671", "text": "MCACEA\n\nMCACEA (Multiple Coordinated Agents Coevolution Evolutionary Algorithm) is a general framework that uses a single evolutionary algorithm (EA) per agent sharing their optimal solutions to coordinate the evolutions of the EAs populations using cooperation objectives. This framework can be used to optimize some characteristics of multiple cooperating agents in mathematical optimization problems. More specifically, due to its nature in which both individual and cooperation objectives are optimize, MCACEA is used in multi-objective optimization problems.\n\nMCACEA, uses multiple EAs (one per each agent) that evolve their own populations to find the best solution for its associated problem according to their individual and cooperation constraints and objective indexes. Each EA is an optimization problem that runs in parallel and that exchanges some information with the others during its evaluation step. This information is needed to let each EA to measure the coordination objectives of the solutions encoded in its own population, taking into account the possible optimal solutions of the remaining populations of the other EAs. With this purpose, each single EA receives information related to the best solutions of the remaining ones before evaluating the cooperative objectives of each possible solution of its own population.\n\nAs the cooperation objective values depend on the best solutions of the other populations and the optimality of a solution depends both on the individual and cooperation objectives, it is not really possible to select and send the best solution of \neach planner to the others. However, MCACEA divides the evaluation step inside each EA in three parts: In the first part, the \nEAs identify the best solution considering only its individual objective values and send it to the others EAs; in the second part, the cooperation objective values of all solutions are calculated taking into account the received information; and in the third part, the EAs calculate the fitness of the solutions considering all the individual and cooperation objective values. \n\nAlthough each population can only offer a unique optimal solution, each EA maintains a pareto set of optimal solutions and selects the unique optimal solution at the end, when the last population has already been obtained. Therefore, to be able to determine a unique optimal solution according with the individual objectives in each generation (and so, using it with the MCACEA framework), a step in charge of selecting the final optimal solution must also be included in the evaluation step of each EA.\n\nThe complete evaluation phase of the individual cooperating EAs is divided in six steps. When searching for the solution of a single EA, only the first two steps of this new evaluation process are used. MCACEA extends this process from these two only steps to the next six:\n\n1. Evaluating the individual objectives of each solution.\n\n2. Calculating the fitness of each solution with the single evaluation function (containing only the individual objectives).\n\n3. Finding the best solution of the population.\n\n4. Sending (and receiving) the best solution to (of) the other single EAs.\n\n5. Calculating the cooperation objectives taking into account the received information from the other EAs.\n\n6. Calculating the fitness of each solution with the complete evaluation function (containing both the individual and the cooperation objectives), which have been obtained in steps 1 and 5.\n\nAlthough MCACEA may look similar to the habitual parallelization of EAs, in this case, instead of distributing the solutions of the whole problem between different EAs that share their solutions periodically, the algorithm is dividing the problem into smaller problems that are solved simultaneously by each EA taking into account the solutions of the part of the problems that the other EAs are obtaining.\n\nAnother possibility, is to send the best completely evaluated solutions of the previous generation to the other EAs instead of our current best only individual objective evaluated one. Nevertheless, that approach introduces a bias toward outdated completely evaluated trajectories, while MCACEA does it toward currently good individual objective evaluated ones.\n\nMCACEA has been used for finding and optimizing unmanned aerial vehicles (UAVs) trajectories when flying simultaneously in the same scenario.\n\n\nL. de la Torre, J. M. de la Cruz, and B. Andrés-Toro. \"Evolutionary trajectory planner for multiple UAVs in realistic scenarios\". IEEE Transactions on Robotics, vol. 26, no. 4, pp. 619–634, August 2010.\n", "id": "34542671", "title": "MCACEA"}
{"url": "https://en.wikipedia.org/wiki?curid=36089423", "text": "SolveIT Software\n\nSolveIT Software Pty Ltd is a provider of advanced planning and scheduling enterprise software for supply and demand optimisation and predictive modelling. Based in Adelaide, South Australia, 70% of its turnover is generated from software deployed in the mining and bulk material handling sectors.\n\nThe company was set up in 2005 by four academics who were also experienced business people, all recent immigrants to Australia. The team was headed by ex-Ernst and Young consultant Matthew Michalewicz, who had moved to Adelaide in 2004 after selling his last company, NuTech. The other three partners were Zbigniew Michalewicz Ph.D, Martin Schmidt and Constantin Chiriac, all four of which were co-authors of the book \"Adaptive Business Intelligence®\".\n\nThe company first developed an optimization and predictive modeling platform based on Artificial Intelligence, and then built its supply chain applications for planning, scheduling, and demand forecasting on this platform. Early customers included Orlando Wines, ABB Grain, the Fosters wine brands and later Pernod Ricard that were also located in the Barossa Valley region.\n\nIn 2008, Rio Tinto Iron Ore asked the company to improve its mining planning and scheduling operations based at Pilbara. SolveIT succeeded in applying its advanced planning and scheduling product, based on non-linear optimization, to the Rio Tinto mine scheduling problem, after many other vendors had failed over a period of ten years.\n\nWith 30 employees at this point, it then won an additional contract in the mining sector with BHP Billiton/Mitsubishi Alliance Coal, leading to subsequent tender wins in the sector, including: BHP Billiton; CBH Group; Fortescue Metals Group, Hills Holdings, Pacific National Coal; and Xstrata Coal.\n\nOn 3 September 2012, SolveIT announced it was acquired by Schneider Electric, a global specialist in energy management. \nHeadquartered in Adelaide, the company has over 150 staff based across operational offices in: Melbourne; Brisbane; Perth; and Chişinău, Moldova.\n\nThe company develops advanced planning and scheduling business optimisation software, which helps manage complex operations using artificial intelligence. Most of the products were initially developed around the key South Australian industries of wine and grain handling, and today SolveIt has a specialist mining division due to early adoption of the company's software within the mining market. The software helps companies accurately predict and plan their production, supply chain, shipping and currency hedging.\n\nDue to the scientific optimisation components embedded in the company’s software products, it sometimes uses a prize-based system to recruit the required high-level of talent. In 2011, the company used a Magic square problem, won by University of Nottingham graduate Yuri Bykov, who developed a program which solved a constrained version of a 2600 by 2600 magic square within a minute.\n\nIn 2011, the company won the Australian National iAward in the e-Logistics and Supply Chain category for its Supply Chain Network Optimiser (SCNO). In February 2012, SolveIT and Schneider Electric became co-organisers of the Integrated Planning and Optimisation Summit, held at the Adelaide Convention Centre.\n\n\nThe company’s mining division provides integrated planning, scheduling and optimisation for the whole traditional mining supply chain network of mine, process plant, transport network, port and trading desk. Variables into the systems allow for asset management, workforce variability, maintenance, accommodation and market factors.\n\n", "id": "36089423", "title": "SolveIT Software"}
{"url": "https://en.wikipedia.org/wiki?curid=268020", "text": "Evolutionary computation\n\nIn computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.\n\nIn evolutionary computation, an initial set of candidate solutions is generated and iteratively updated. Each new generation is produced by stochastically removing less desired solutions, and introducing small random changes. In biological terminology, a population of solutions is subjected to natural selection (or artificial selection) and mutation. As a result, the population will gradually evolve to increase in fitness, in this case the chosen fitness function of the algorithm.\n\nEvolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. Many variants and extensions exist, suited to more specific families of problems and data structures. Evolutionary computation is also sometimes used in evolutionary biology as an \"in silico\" experimental procedure to study common aspects of general evolutionary processes.\n\nThe use of Evolutionary principles for automated problem solving originated in the 1950s. It was not until the 1960s that three distinct interpretations of this idea started to be developed in three different places.\n\nEvolutionary programming was introduced by Lawrence J. Fogel in the US, while John Henry Holland called his method a genetic algorithm. In Germany Ingo Rechenberg and Hans-Paul Schwefel introduced evolution strategies. These areas developed separately for about 15 years. From the early nineties on they are unified as different representatives (\"dialects\") of one technology, called evolutionary computing. Also in the early nineties, a fourth stream following the general ideas had emerged – genetic programming. Since the 1990s, nature-inspired algorithms are becoming an increasingly significant part of evolutionary computation.\n\nThese terminologies denote the field of evolutionary computing and consider evolutionary programming, evolution strategies, genetic algorithms, and genetic programming as sub-areas.\n\nSimulations of evolution using evolutionary algorithms and artificial life started with the work of Nils Aall Barricelli in the 1960s, and was extended by Alex Fraser, who published a series of papers on simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s and early 1970s, who used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Holland. As academic interest grew, dramatic increases in the power of computers allowed practical applications, including the automatic evolution of computer programs. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers, and also to optimise the design of systems.\n\nEvolutionary computing techniques mostly involve metaheuristic optimization algorithms. Broadly speaking, the field includes:\n\n\nEvolutionary algorithms form a subset of evolutionary computation in that they generally only involve techniques implementing mechanisms inspired by biological evolution such as reproduction, mutation, recombination, natural selection and survival of the fittest. Candidate solutions to the optimization problem play the role of individuals in a population, and the cost function determines the environment within which the solutions \"live\" (see also fitness function). Evolution of the population then takes place after the repeated application of the above operators.\n\nIn this process, there are two main forces that form the basis of evolutionary systems: Recombination and mutation create the necessary diversity and thereby facilitate novelty, while selection acts as a force increasing quality.\n\nMany aspects of such an evolutionary process are stochastic. Changed pieces of information due to recombination and mutation are randomly chosen. On the other hand, selection operators can be either deterministic, or stochastic. In the latter case, individuals with a higher fitness have a higher chance to be selected than individuals with a lower fitness, but typically even the weak individuals have a chance to become a parent or to survive.\n\nThe list of active researchers is naturally dynamic and non-exhaustive. A network analysis of the community was published in 2007.\n\n\n\n", "id": "268020", "title": "Evolutionary computation"}
{"url": "https://en.wikipedia.org/wiki?curid=38808364", "text": "MOEA Framework\n\nThe MOEA Framework is an open-source evolutionary computation library for Java that specializes in multi-objective optimization. It supports a variety of multiobjective evolutionary algorithms (MOEAs), including genetic algorithms, genetic programming, grammatical evolution, differential evolution, and particle swarm optimization. As a result, it has been used to conduct numerous comparative studies to assess the efficiency, reliability, and controllability of state-of-the-art MOEAs.\n\nThe MOEA Framework is an extensible framework for rapidly designing, developing, executing, and statistically testing multiobjective evolutionary algorithms (MOEAs). It features 25 different state-of-the-art MOEAs and over 80 analytical test problems. It supports NSGA-II, its recently introduced successor NSGA-III epsilon-MOEA, GDE3., and MOEA/D. natively. In addition, it integrates with the JMetal, Platform and Programming Language Independent Interface for Search Algorithms (PISA), and Borg MOEA libraries to provide access to all popular MOEAs. Additionally, using Java's service provider interface (SPI), new MOEAs and problems can be introduced into the framework. This supports the use of the MOEA Framework in scientific studies, allowing new MOEAs to be tested against a suite of state-of-the-art algorithms across a large collection of test problems.\n\nNew problems are defined in the MOEA Framework using one or more decision variables of a varying type. This includes common representations such as binary strings, real-valued numbers, and permutations. It additionally supports evolving grammars in Backus–Naur form and programs using an internal Turing complete programming language. Once the problem is defined, the user can optimize the problem using any of supported MOEAs.\n\nThe MOEA Framework is the only known framework for evolutionary computation that provides support for sensitivity analysis. Sensitivity analysis in this context studies how an MOEA's parameters impact its output (i.e., the quality of the results). Alternatively, sensitivity analysis measures the robustness of an MOEA to changes in its parameters. An MOEA whose behavior is sensitive to its paramterization will not be easily controllable; conversely, an MOEA that is insensitive to its parameters is controllable. By measuring the sensitivities of each MOEA, the MOEA Framework can identify the controlling parameters for each MOEA and provide guidance for fine-tuning the parameters. Additionally, MOEAs that are consistently insensitive to parameter changes across an array of problem domains are regarded highly due to their robust ability to solve optimization problems.\n\n\n", "id": "38808364", "title": "MOEA Framework"}
{"url": "https://en.wikipedia.org/wiki?curid=39447416", "text": "DEAP (software)\n\nDistributed Evolutionary Algorithms in Python (DEAP) is an evolutionary computation framework for rapid prototyping and testing of ideas\nIt incorporates the data structures and tools required to implement most common evolutionary computation techniques such as genetic algorithm, genetic programming, evolution strategies, particle swarm optimization, differential evolution, traffic flow and estimation of distribution algorithm. It is developed at Université Laval since 2009.\n\nThe following code gives a quick overview how the Onemax problem optimization with genetic algorithm can be implemented with DEAP.\n\n", "id": "39447416", "title": "DEAP (software)"}
{"url": "https://en.wikipedia.org/wiki?curid=418075", "text": "Digital organism\n\nA digital organism is a self-replicating computer program that mutates and evolves. Digital organisms are used as a tool to study the dynamics of Darwinian evolution, and to test or verify specific hypotheses or mathematical models of evolution. The study of digital organisms is closely related to the area of artificial life.\n\nDigital organisms can be traced back to the game Darwin, developed in 1961 at Bell Labs, in which computer programs had to compete with each other by trying to stop others from executing . A similar implementation that followed this was the game Core War. In Core War, it turned out that one of the winning strategies was to replicate as fast as possible, which deprived the opponent of all computational resources. Programs in the Core War game were also able to mutate themselves and each other by overwriting instructions in the simulated \"memory\" in which the game took place. This allowed competing programs to embed damaging instructions in each other that caused errors (terminating the process that read it), \"enslaved processes\" (making an enemy program work for you), or even change strategies mid-game and heal themselves.\n\nSteen Rasmussen at Los Alamos National Laboratory took the idea from Core War one step further in his core world system by introducing a genetic algorithm that automatically wrote programs. However, Rasmussen did not observe the evolution of complex and stable programs. It turned out that the programming language in which core world programs were written was very brittle, and more often than not mutations would completely destroy the functionality of a program.\n\nThe first to solve the issue of program brittleness was Thomas S. Ray with his Tierra system, which was similar to core world. Ray made some key changes to the programming language such that mutations were much less likely to destroy a program. With these modifications, he observed for the first time computer programs that did indeed evolve in a meaningful and complex way.\n\nLater, Chris Adami, Titus Brown, and Charles Ofria started developing their Avida system, which was inspired by Tierra but again had some crucial differences. In Tierra, all programs lived in the same address space and could potentially overwrite or otherwise interfere with each other. In Avida, on the other hand, each program lives in its own address space. Because of this modification, experiments with Avida became much cleaner and easier to interpret than those with Tierra. With Avida, digital organism research has begun to be accepted as a valid contribution to evolutionary biology by a growing number of evolutionary biologists. Evolutionary biologist Richard Lenski of Michigan State University has used Avida extensively in his work. Lenski, Adami, and their colleagues have published in journals such as \"Nature\" and the \"Proceedings of the National Academy of Sciences\" (USA).\n\nIn 1996, Andy Pargellis created a Tierra-like system called \"Amoeba\" that evolved self-replication from a randomly seeded initial condition.\n\n\n\n", "id": "418075", "title": "Digital organism"}
{"url": "https://en.wikipedia.org/wiki?curid=901162", "text": "Interactive evolutionary computation\n\nInteractive evolutionary computation (IEC) or aesthetic selection is a general term for methods of evolutionary computation that use human evaluation. Usually human evaluation is necessary when the form of fitness function is not known (for example, visual appeal or attractiveness; as in Dawkins, 1986) or the result of optimization should fit a particular user preference (for example, taste of coffee or color set of the user interface).\n\nThe number of evaluations that IEC can receive from one human user is limited by user fatigue which was reported by many researchers as a major problem. In addition, human evaluations are slow and expensive as compared to fitness function computation. Hence, one-user IEC methods should be designed to converge using a small number of evaluations, which necessarily implies very small populations. Several methods were proposed by researchers to speed up convergence, like interactive constrain evolutionary search (user intervention) or fitting user preferences using a convex function. IEC human-computer interfaces should be carefully designed in order to reduce user fatigue. There is also evidence that the addition of computational agents can successfully counteract user fatigue.\n\nHowever IEC implementations that can concurrently accept evaluations from many users overcome the limitations described above. An example of this approach is an interactive media installation by Karl Sims that allows one to accept preferences from many visitors by using floor sensors to evolve attractive 3D animated forms. Some of these multi-user IEC implementations serve as collaboration tools, for example HBGA.\n\nIEC methods include interactive evolution strategy, interactive genetic algorithm, interactive genetic programming, and human-based genetic algorithm.,\n\nAn interactive genetic algorithm (IGA) is defined as a genetic algorithm that uses human evaluation. These algorithms belong to a more general category of Interactive evolutionary computation. The main application of these techniques include domains where it is hard or impossible to design a computational fitness function, for example, evolving images, music, various artistic designs and forms to fit a user's aesthetic preferences. Interactive computation methods can use different representations, both linear (as in traditional genetic algorithms) and tree-like ones (as in genetic programming).\n\n\n\n", "id": "901162", "title": "Interactive evolutionary computation"}
{"url": "https://en.wikipedia.org/wiki?curid=44636079", "text": "Constructive cooperative coevolution\n\nThe constructive cooperative coevolutionary algorithm (also called C) is an global optimisation algorithm in artificial intelligence based on the multi-start architecture of the greedy randomized adaptive search procedure (GRASP). It incorporates the existing cooperative coevolutionary algorithm (CC). The considered problem is decomposed into subproblems. These subproblems are optimised separately while exchanging information in order to solve the complete problem. An optimisation algorithm, usually but not necessarily an evolutionary algorithm, is embedded in C for optimising those subproblems. The nature of the embedded optimisation algorithm determines whether C's behaviour is deterministic or stochastic.\n\nThe C optimisation algorithm was originally designed for simulation-based optimisation but it can be used for global optimisation problems in general. Its strength over other optimisation algorithms, specifically cooperative coevolution, is that it is better able to handle non-separable optimisation problems.\n\nAn improved version was proposed later, called the Improved Constructive Cooperative Coevolutionary Differential Evolution (CDE), which removes several limitations with the previous version. A novel element of CDE is the advanced initialisation of the subpopulations. CDE initially optimises the subpopulations in a partially co-adaptive fashion. During the initial optimisation of a subpopulation, only a subset of the other subcomponents is considered for the co-adaptation. This subset increases stepwise until all subcomponents are considered. This makes CDE very effective on large-scale global optimisation problems (up to 1000 dimensions) compared to cooperative coevolutionary algorithm (CC) and Differential evolution.\n\nThe improved algorithm has then been adapted for multi-objective optimization.\n\nAs shown in the pseudo code below, an iteration of C exists of two phases. In Phase I, the constructive phase, a feasible solution for the entire problem is constructed in a stepwise manner. Considering a different subproblem in each step. After the final step, all subproblems are considered and a solution for the complete problem has been constructed. This constructed solution is then used as the initial solution in Phase II, the local improvement phase. The CC algorithm is employed to further optimise the constructed solution. A cycle of Phase II includes optimising the subproblems separately while keeping the parameters of the other subproblems fixed to a central blackboard solution. When this is done for each subproblem, the found solution are combined during a \"collaboration\" step, and the best one among the produced combinations becomes the blackboard solution for the next cycle. In the next cycle, the same is repeated. Phase II, and thereby the current iteration, are terminated when the search of the CC algorithm stagnates and no significantly better solutions are being found. Then, the next iteration is started. At the start of the next iteration, a new feasible solution is constructed, utilising solutions that were found during the Phase I of the previous iteraton(s). This constructed solution is then used as the initial solution in Phase II in the same way as in the first iteration. This is repeated until one of the termination criteria for the optimisation is reached, e.g. a maximum number of evaluations.\n\nThe multi-objective version of the C algorithm is a Pareto-based algorithm which uses the same divide-and-conquer strategy as the single-objective C optimisation algorithm . The algorithm again starts with the advanced constructive initial optimisations of the subpopulations, considering an increasing subset of subproblems. The subset increases until the entire set of all subproblems is included. During these initial optimisations, the subpopulation of the latest included subproblem is evolved by a multi-objective evolutionary algorithm. For the fitness calculations of the members of the subpopulation, they are combined with a collaborator solution from each of the previously optimised subpopulations. Once all subproblems' subpopulations have been initially optimised, the multi-objective C optimisation algorithm continues to optimise each subproblem in a round-robin fashion, but now collaborator solutions from all other subproblems' subspopulations are combined with the member of the subpopulation that is being evaluated. The collaborator solution is selected randomly from the solutions that make up the Pareto-optimal front of the subpopulation. The fitness assignment to the collaborator solutions is done in an optimistic fashion (i.e. an \"old\" fitness value is replaced when the new one is better).\n\nThe constructive cooperative coevolution algorithm has been applied to different types of problems, e.g. a set of standard benchmark functions, optimisation of sheet metal press lines and interacting production stations. The C algorithm has been embedded with, amongst others, the differential evolution algorithm and the particle swarm optimiser for the subproblem optimisations.\n\n\n", "id": "44636079", "title": "Constructive cooperative coevolution"}
{"url": "https://en.wikipedia.org/wiki?curid=49305019", "text": "Genetic and Evolutionary Computation Conference\n\nThe Genetic and Evolutionary Computation Conference (GECCO) is the premier conference in the area of genetic and evolutionary computation. GECCO has been held every year since 1999, when it was first established as a recombination of the International Conference on Genetic Algorithms (ICGA) and the Annual Genetic Programming Conference (GP).\n\nGECCO presents the latest high-quality results in genetic and evolutionary computation. Topics of interest include: genetic algorithms, genetic programming, evolution strategies, evolutionary programming, estimation of distribution algorithms, memetic algorithms, hyper-heuristics, evolutionary robotics, evolvable hardware, artificial life, ant colony optimization algorithms, swarm intelligence, artificial immune systems, digital entertainment technologies, evolutionary art, evolutionary combinatorial optimization, metaheuristics, evolutionary multi-objective optimization, evolutionary machine learning, search-based software engineering, theory, real-world applications, and more.\n\nOther important conferences in the field are IEEE Congress on Evolutionary Computation (CEC), Parallel Problem Solving from Nature (PPSN) and EvoStar (a group name for four co-located conferences, EuroGP, EvoCOP, EvoMUSART, and EvoApplications).\n\nGECCO is the main annual conference of the Special Interest Group on Genetic and Evolutionary Computation (SIGEVO), which is a Special Interest Group (SIG) of the Association for Computing Machinery (ACM).\n\n", "id": "49305019", "title": "Genetic and Evolutionary Computation Conference"}
{"url": "https://en.wikipedia.org/wiki?curid=1514566", "text": "Mating pool\n\nA mating pool is a concept used in evolutionary computation. More specifically, it is one of the four steps in genetic algorithm operations that places value on the fitness of individuals in a population. \n\nTypically, the population of candidate solutions for a genetic algorithm is treated as a single entity. An alternative approach is to separate only individuals who will produce offspring from the remainder of the current population. These are placed into a \"mating pool\".\n\nSelection operators are applied to the entire population in order to determine the fittest individuals of the current population, which are then placed into the mating pool. Genetic operations, such as crossover and mutation, are then applied to the mating pool to create the next generation.\n\nThere are multiple ways to create a mating pool, including selecting the best individuals based on certain criteria, known as truncation selection, and choosing individuals randomly with the probability of being chosen based on fitness level, known as fitness proportionate selection. Additionally, a random sub-group can be selected from the population and then the individuals with the highest fitness are selected for the mating pool, known as tournament selection.\n", "id": "1514566", "title": "Mating pool"}
{"url": "https://en.wikipedia.org/wiki?curid=43144075", "text": "Android Auto\n\nAndroid Auto is a mobile app developed by Google that allows enhanced use of an Android device within a vehicle equipped with a compatible head unit. Once the Android device is connected to the head unit, the system enables it to broadcast apps with a simple, driver-friendly user interface onto the vehicle's display, including GPS mapping/navigation, music playback, SMS, telephony, and web search. The system supports both touchscreen and button-controlled head unit displays, although hands-free operation through voice commands is encouraged to minimize driving distraction.\n\nIt was announced at Google I/O 2014, and the app was released on 19 March 2015. Android Auto is part of the Open Automotive Alliance announced in 2014, and is a joint effort between 28 automobile manufacturers, with Nvidia as tech supplier. \n\nThe most common way Android Auto is deployed is via an Android mobile device running the Android Auto app, acting as a slave to a vehicle's dashboard head unit that supports this functionality. Once the user's Android device is connected to the vehicle, the head unit will serve as an external display for the Android device, presenting supported software in a car-specific user interface provided by the Android Auto app. In Android Auto's first iterations, the device was required to be connected via USB to the car, Alternatively, in November 2016, Google added the option to run Android Auto as a regular app on an Android device, i.e., not tethered to a car's head unit, which allows for its usage on Android-powered head units, or simply on the user's personal phone or tablet in the vehicle. In addition, on January 1 2018 it was announced that JVCKenwood would be exhibiting wireless Android Auto enabled head units at CES 2018, which would be capable of operating without the need for a wired connection.\n\nAn Android Auto SDK has been released, allowing third parties to modify their apps to work with Android Auto; Initially, only APIs for music and messaging apps were be available, but it is expected that through Android auto, the mobile device will have access to several of the automobile's sensors and inputs, such as GPS and high-quality GPS antennas, steering-wheel mounted buttons, the sound system, directional speakers, directional microphones, wheel speed, compass, mobile antennas, etc. Also there's partial access to car data, a feature still under development.\n\nAt CES 2018, Google confirmed that the Google Assistant would be coming to Android Auto later in the year.\n\nCurrently supported apps include:\n\nGPS navigation\nMusic\nTelephony/Messaging\n\nIn May 2015, Hyundai became the first manufacturer to offer Android Auto support, making it available first in the 2015 Hyundai Sonata. Automobile manufacturers that will be offering Android Auto support in their cars include Abarth, Acura, Alfa Romeo, Audi, Bentley, Cadillac, Chevrolet, Chrysler, Dodge, Fiat, Ford, GMC, Honda, Hyundai, Infiniti, Jaguar Land Rover, Jeep, Kia, Lamborghini, Lexus, Lincoln, Mahindra and Mahindra, Maserati, Mercedes-Benz, Mitsubishi, Nissan, Opel, RAM, Renault, SEAT, Škoda, Subaru, Suzuki, Tata Motors Cars, Volkswagen and Volvo. The official Android Auto website lists most car manufacturers supporting the app.\n\nAdditionally, aftermarket car audio systems supporting Android Auto, enable the technology into host vehicles, including Pioneer, Kenwood, Panasonic and Sony.\n\n\n", "id": "43144075", "title": "Android Auto"}
{"url": "https://en.wikipedia.org/wiki?curid=51060375", "text": "Amazon Alexa\n\nAlexa is an intelligent personal assistant developed by Amazon, first used in the Amazon Echo and the Amazon Echo Dot devices developed by Amazon Lab126. It is capable of voice interaction, music playback, making to-do lists, setting alarms, streaming podcasts, playing audiobooks, and providing weather, traffic, and other real-time information, such as news. Alexa can also control several smart devices using itself as a home automation system.\n\nMost devices with Alexa allow users to activate the device using a wake-word (such as \"Echo\"); other devices (such as the Amazon mobile app on iOS or Android) require the user to push a button to activate Alexa's listening mode. Currently, interaction and communication with Alexa are only available in English and German. In November 2017, Alexa became available in the Canadian market in English only.\n\nAs of September 2017, Amazon had more than 5,000 employees working on Alexa and related products.\n\nIn November 2014, Amazon announced Alexa alongside the Echo. Alexa was inspired by the computer voice and conversational system on board the Starship \"Enterprise\" in science fiction TV series and movies, beginning with \"\" and \"\".\n\nThe name Alexa was chosen due to the fact that it has a hard consonant with the X and therefore could be recognized with higher precision. The name is also claimed to be reminiscent of the Library of Alexandria, which is also used by Amazon Alexa Internet for the same reason. In June 2015, Amazon announced Alexa Fund, a program that would invest in companies making voice control skills and technologies. The 100 million in funds has invested in companies including Ecobee, Orange Chef, Scout Alarm, Garageio, Toymail, MARA, and Mojio. In 2016 the Alexa Prize was announced to advance the technology.\n\nIn January 2017, the first Alexa Conference took place in Nashville, Tennessee, an independent gathering of the worldwide community of Alexa developers and enthusiasts. The follow-up has been announced, to be keynoted by original Amazon Alexa / Connected Home product head Ahmed Bouzid.\n\nAt the Amazon Web Services Re:Invent conference in Las Vegas, Amazon announced Alexa for Business and the ability for app developers to have paid add-ons to their skills. \n\nA companion app is available from the Apple App Store, Google Play, and Amazon Appstore. The app can be used by owners of Alexa-enabled devices to install skills, control music, manage alarms, and view shopping lists. It also allows users to review the recognized text on the app screen and to send feedback to Amazon concerning whether the recognition was good or bad. A web interface is also available to set up compatible devices (e.g., Amazon Echo, Amazon Dot, Amazon Echo Show).\n\nAlexa offers weather reports provided by AccuWeather and news provided by TuneIn from a variety of sources including local radio stations, NPR, and ESPN. Additionally, Alexa-supported devices stream music from the owner's Amazon Music accounts and have built-in support for Pandora and Spotify accounts. Alexa can play music from streaming services such as Apple Music and Google Play Music from a phone or tablet. Alexa can manage voice-controlled alarms, timers, and shopping and to-do lists, and can access Wikipedia articles. Alexa devices will respond to questions about items in the user's Google Calendar. As of November 2016, the Alexa Appstore had over 5,000 functions (\"skills\") available for users to download, up from 1,000 functions in June 2016. As of a partnership with fellow technology company, Microsoft, Alexa will be available via its competing virtual personal assistant, Cortana. This functionality was said to come later in 2017; although as of January 2018, is still not available.\n\nIn the home automation space, Alexa can interact with devices from Belkin Wemo, ecobee, Geeni, IFTTT, Insteon, LIFX, LightwaveRF, Nest Thermostats, Philips Hue, SmartThings, Wink, and Yonomi. The Home Automation feature was launched on April 8, 2015.\n\nTake-out food can be ordered using Alexa; as of May 2017 food ordering using Alexa is supported by Domino's Pizza, Grubhub, Pizza Hut, Seamless, and Wingstop. Also, users of Alexa in the UK can order meals via Just Eat. In early 2017, Starbucks announced a private beta for placing pick-up orders using Alexa. In addition, users can order meals using Amazon Prime Now via Alexa in 20 major US cities. With the introduction of Amazon Key in November 2017, Alexa also works together with the smart lock and the Alexa Cloud Cam included in the service to allow Amazon couriers to unlock customers' front doors and deliver packages inside. \n\nAlexa supports a multitude of subscription-based and free streaming services on Amazon devices. These streaming services include: Prime Music, Amazon Music, Amazon Music Unlimited, TuneIn, iHeartRadio, Audible, Pandora, and Spotify Premium. However, some of these music services are not available on other Alexa-enabled products that are manufactured by companies external of its services. This unavailability also includes Amazon's own Fire TV devices or tablets.\n\nAlexa is able to stream media and music directly. To do this, Alexa's device should be linked to the Amazon account, which enables access to one's Amazon Music library, in addition to any audiobooks available in one's Audible library. Amazon Prime members have an additional ability to access stations, playlists, and over two million songs free of charge. Amazon Music Unlimited subscribers also have access to a list of millions of songs.\n\nAmazon Music for PC allows one to play personal music from Google Play, iTunes, and others on an Alexa device. This can be done by uploading one's collection to My Music on Amazon from a computer. Up to 250 songs can be uploaded free of charge. Once this is done, Alexa can play this music and control playback through voice command options.\n\nAlexa allows the user to hear updates on supported sports teams. A way to do this is by adding the sports team to the list created under Alexa's Sports Update app section. \nThe user are able to hear updates on up to 15 supported teams:\n\n\nThere are a number of ways messages can be sent from Alexa's application. Alexa is able to deliver messages to a recipient's Alexa application, as well as to all of their Echo devices that are both supported and associated with their Amazon account. Alexa is able to send typed messages only from Alexa's app. If one sends a message from an associated Echo device, it will be sent as a voice message. Alexa cannot send attachments such as videos and photos.\n\nFor households with more than one member, one's Alexa contacts are pooled across all of the devices that are registered to its associated account. However, within Alexa's app one is only able to start conversations with its Alexa contacts. When accessed and supported by an Alexa app or Echo device, Alexa messaging is available to anyone in one's household. These messages can be heard by anyone with access in the household. This messaging feature does not yet contain a password protection or associated PIN. Anyone who has access to one's cell phone number is able to use this feature to contact them through their supported Alexa app or Echo device. The feature to block alerts for messages and calls is available temporarily by utilizing the Do Not Disturb feature.\n\nAlexa for Business is a paid subscription service allowing companies to use Alexa to join conference calls, schedule meeting rooms, and custom skills designed by 3rd-party vendors. At launch, notable skills are available from SAP, Microsoft, and Salesforce.\n\nAmazon allows developers to build and publish skills for Alexa using the Alexa Skills Kit. These skills are third-party developed voice experiences that add to the capabilities of any Alexa-enabled device (such as the Echo). These skills are available for free download using the Alexa app. Skills are continuously being added to increase the capabilities available to the user. A \"Smart Home Skill API\" is available. All of the code runs in the cloud – nothing is on any user's device. A developer can follow tutorials to learn how to quickly build voice experiences for their new and existing applications.\n\nAmazon allows device manufacturers to integrate Alexa voice capabilities into their own connected products by using the Alexa Voice Service (AVS), a cloud-based service that provides APIs to interface with Alexa. Products built using AVS have access to Alexa's growing list of capabilities including all of the Alexa Skills. AVS provides cloud-based automatic speech recognition (ASR) and natural language understanding (NLU). There are no fees for companies looking to integrate Alexa into their products by using AVS.\n\nThe voice of Amazon Alexa is generated by a long short-term memory artificial neural network.\n\nOn November 30, 2016 Amazon announced that they will make the speech recognition and natural language processing technology behind Alexa available for developers under the name of Amazon Lex. This new service would allow developers to create their own chatbots that can interact in a conversational manner, similar to Alexa. Along with the connection to various Amazon services, the initial version will provide connectivity to Facebook Messenger, with Slack and Twilio integration to follow.\n\nThere are concerns about the access Amazon has to private conversations in the home and other non-verbal indications that can identify who is present in the home with non-stop audio pick-up from Alexa-enabled devices. Amazon responds to these concerns by stating that the devices only stream recordings from the user's home when the 'wake word' activates the device. The device is technically capable of streaming voice recordings at all times, and in fact will always be listening to detect if a user has uttered the wake word.\n\nAmazon uses past voice recordings sent to the cloud service to improve response to future questions the user may pose. To address privacy concerns, the user can delete voice recordings that are currently associated with the user's account, but doing so may degrade the user's experience using search functions. To delete these recordings, the user can visit the \"Manage My Device\" page on Amazon.com or contact Amazon customer service.\n\nAlexa uses an address stored in the companion app when it needs a location. Amazon and third-party apps and websites use location information to provide location-based services and store information to provide voice services, the Maps app, \"Find Your Device\", and to monitor the performance and accuracy of location services. For example, Echo voice services use the user's location to respond to the user's requests for nearby restaurants or stores. Similarly, Alexa uses the user's location to process the user's mapping-related requests and improve the Maps experience. All information collected is subject to the Amazon.com Privacy Notice.\n\nAmazon retains digital recordings of users' audio spoken after the \"wake word,\" and while the audio recordings are subject to demands by law enforcement, government agents, and other entities via subpoena, Amazon publishes some information about the warrants it receives, the subpoenas it receives, and some of the warrant-less demands it receives, allowing customers some indication as to the percentage of government demands for customer information it receives.\n\nAs of December 2017, Alexa is available in 35 countries. Amazon has additionally confirmed that Alexa will launch in Australia and New Zealand Feb 1st 2018.\n\n\n\n\n\n\n\n\n\nIn September 2016, a university student competition called the Alexa Prize was announced for November of that year. The prize is equipped with a total of $2.5 million and teams and their universities can win cash and research grants. The process starts with a team selection in 2016, final award will be announced in 2017. The 2017 inaugural competition focuses on the challenge of building a socialbot. This is similar to the Loebner Prize, but with higher prize money.\n\nGiven Amazon's strong belief in voice technologies, Amazon announced a US$100 million venture capital fund on June 25, 2015. By specifically targeting developers, device-makers and innovative companies of all sizes, Amazon aims at making digital voice assistants more powerful for its users. Eligible projects for financial funding base on either creating new Alexa capabilities by using the Alexa Skills Kit (ASK) or Alexa Voice Service (AVS).\n\nThe final selection of companies originates from the customer perspective and works backwards, specific elements that are considered for potential investments are: level of customer centricity, degree of innovation, motivation of leadership, fit to Alexa product/service line, amount of other funding raised.\n\nBesides financial support, Amazon provides business and technology expertise, help for bringing products to the market, aid for hard- and software development as well as enhanced marketing support on proprietary Amazon platforms.\n\nThe list of funded business includes (in alphabetical order): DefinedCrows, Dragon Innovation, Ecobee, Embodied Inc., Garageio, Invoxia, kitt.ai, Luma, Mara, Mojio (2x times), Musaic, Nucleus, Orange Chef, Owlet Baby Care, Petnet, Rachio, Ring, Scout, Sutro, Thalmic Labs, Toymail Co., TrackR and Vesper.\n\n", "id": "51060375", "title": "Amazon Alexa"}
{"url": "https://en.wikipedia.org/wiki?curid=53639024", "text": "Bixby (virtual assistant)\n\nBixby is a virtual assistant developed by Samsung Electronics.\n\nOn March 20, 2017, Samsung announced the voice-powered digital assistant named \"Bixby\". Bixby was introduced alongside the Samsung Galaxy S8 and S8+ during the Samsung Galaxy Unpacked 2017 event, which was held on March 29, 2017. Samsung officially unveiled Bixby a week before launch but it only made its first appearance during the event. Bixby can also be sideloaded on older Galaxy devices running Android Nougat.\n\nBixby represents a major reboot for S Voice, Samsung's voice assistant app introduced in 2012 with the Galaxy S III.\n\nIn May 2017, Samsung announced that Bixby would be coming to its line of Family Hub 2.0 refrigerators, making it the first non-mobile product to include the virtual assistant.\n\nIn October 2017, Samsung announced the release of Bixby 2.0 during its annual developer conference in San Francisco. The new version is set to be rolled out across the company’s line of connected products, including smartphones, TVs and refrigerators. Moreover, third parties will be allowed to develop applications for Bixby using the Samsung Developer Kit.\n\nBixby comes with three parts, known as \"Bixby Voice\", \"Bixby Vision\" and \"Bixby Home\". With \"Bixby Voice\", the user can trigger Bixby by calling it or pressing and holding the button located below the volume rocker, dubbed the \"Bixby Button\". A while before the phone's release, the Bixby Button was reprogrammable and could be set to open other applications or assistants, such as Google Assistant. Near the phone's release, however, this ability was removed with a firmware update.\n\n\"Bixby Vision\" is built into the camera app and can \"see\" what one can see as it is essentially an augmented reality camera that can identify objects in real time, search for them on various services, and offer the user to purchase them if available. Bixby is also able to translate text, read QR codes and recognize landmarks.\n\n\"Bixby Home\" can be found by swiping to the right on the home screen. It is a vertically scrolling list of information that Bixby can interact with, for example, weather, fitness activity, and buttons for controlling their smart home gadgets.\n\nBixby supports multiple languages including English, Korean and Chinese. It also supports contextual search and visual search.\n\nCurrently, Bixby is only available in Korean and English. However, Samsung announced on April 6, 2017 that the German release is planned for quarter 4 of 2017. Bixby Korean was launched on May 1, 2017 (KST).\n\nSamsung reported that Bixby will not be operational on the US version of the Samsung Galaxy S8 and S8+ when the devices are shipped to consumers beginning April 21, 2017. Samsung have stated that the key features of Bixby, including Vision, Home and Reminder, will be available with the global launch of the smartphones. Bixby Voice will be available in the US on the Galaxy S8 and S8+ later this spring. However, the release of English version was postponed as Samsung had problems getting Bixby to fully understand the language.\n\nAs of December 2017, Bixby is available in over 200 countries, but only in Korean, English, and Chinese (Mandarin). The Chinese version of Bixby is only available on devices officially sold in Mainland China.\n\n\n\n\n\n\n", "id": "53639024", "title": "Bixby (virtual assistant)"}
{"url": "https://en.wikipedia.org/wiki?curid=39791384", "text": "CarPlay\n\nCarPlay is an Apple standard that enables a car radio or head unit to be a display and also act as a controller for an iPhone. It is available on all iPhone 5 and later with at least iOS 7.1.\n\nMost worldwide vehicle manufacturers have said they will be incorporating CarPlay into their infotainment systems over time. CarPlay can also be retrofitted to most vehicles with aftermarket vehicle audio hardware.\n\nAccording to Apple's website, all major vehicle manufacturers are partnering with CarPlay.\n\nCarPlay provides access to Apple apps such as Phone, Music, Apple Maps, iMessage, iBooks, and Podcasts, as well as third-party apps such as iHeartRadio, Radioplayer, At Bat, Spotify, CBS Radio, Rdio, Overcast, Pocket Casts, Google Play Music, Clammr, NPR One, Audiobooks.com, and Audible. Developers must apply to Apple for entitlement to develop CarPlay-enabled apps.\n\nCars with CarPlay are available from most major brands. Manufacturers with no CarPlay models but which Apple says \"has partnered ... in supporting CarPlay\" are:\n\nAftermarket head units can be purchased from Alpine, Clarion, Kenwood, Pioneer, Sony, and JVC.\n\nThe concept of CarPlay (and subsequently Android Auto) was based on the little known (and used) Apple iOS 4 feature called \"iPod Out\" that was the result of a joint development between the BMW Group's Technology Office USA in Palo Alto, California, and Apple Inc. The result of several years of exploratory cooperation, iPod Out enabled vehicles with the necessary infrastructure to \"host\" the analog video and audio from a supporting iOS device while receiving inputs, such as button presses and knob rotations from a car's infotainment system, to drive the \"hosted\" user interface in the vehicle's built-in display. The iOS feature was first announced during WWDC in 2010 and first shipped as an implemented infrastructure in BMW Group vehicles starting in early 2011. The BMW and Mini option was called \"PlugIn\" and paved the way for the first cross-OEM platforms, introducing the concept of requiring a car-specific interface for apps (as opposed to MirrorLink's simple and insufficient mirroring of what was shown on the smartphone's screen).\n\nDuring development its codename was Stark. Apple's Eddy Cue announced it as \"iOS in the Car\" at the 2013 WWDC. In January 2014 it was reported that Apple's hardware-oriented corporate culture had led to release delays. CarPlay was launched with its current branding in at the Geneva Motor Show in March, 2014 with Ferrari, Mercedes-Benz and Volvo among the first car manufacturers.\n\nJune 2013, BMW initially announced it would not be implemented, but later changed this policy.\n\nNovember 2013, Siri Eyes Free mode was offered as a dealer-installed accessory in the US to some Honda Accord and Acura RDX & ILX models.<ref name=\"MR Honda/Acura 1\"></ref> In December, Honda offered additional integration, featuring new HondaLink services, on some US and Canada models of the Civic and the Fit.<ref name=\"MR Honda/Acura 2\"></ref>\n\nSeptember 2014, a Ferrari FF was the first car with a full version of CarPlay. \n\nNovember 2014, Hyundai announced the Sonata sedan will be available with CarPlay by the end of the first quarter of 2015.\n\nDecember 2015, Volvo implemented CarPlay in the 2016 XC90.\n\nJanuary 2016, Apple releases a list detailing the car models which support CarPlay.\n\nOctober 2017, the 2018 Honda Gold Wing becomes the first motorcycle to support CarPlay.\n\nThe Open Automotive Alliance's Android Auto is a similar implementation used for Android devices.\n\nMirrorLink is a standard for car-smartphone connectivity, currently implemented in vehicles by Honda, Volkswagen, SEAT, Buick, Skoda, Mercedes-Benz, Citroën, and Smart with phones by multiple manufacturers including Sony, Samsung, and HTC.\n\nSome vehicle manufacturers have their own systems for syncing the car with smartphones, for example: BMW Assist, Hyundai Blue Link, iLane, MyFord Touch, Ford SYNC, OnStar, and Toyota Entune.\n\nGeneral Motors has released an API to allow the development of apps that interact with vehicle software systems.\n\n", "id": "39791384", "title": "CarPlay"}
{"url": "https://en.wikipedia.org/wiki?curid=42119832", "text": "Cortana\n\nCortana is a virtual assistant created by Microsoft for Windows 10, Windows 10 Mobile, Windows Phone 8.1, Invoke smart speaker, Microsoft Band, Xbox One, iOS, Android, Windows Mixed Reality, and soon Amazon Alexa.\n\nCortana can set reminders, recognize natural voice without the requirement for keyboard input, and answer questions using information from the Bing search engine.\n\nCortana is currently available in English, Portuguese, French, German, Italian, Spanish, Chinese, and Japanese language editions, depending on the software platform and region in which it is used. Cortana mainly competes against assistants such as Apple Siri, Google Assistant, and Amazon Alexa.\n\nCortana was demonstrated for the first time at the Microsoft BUILD Developer Conference (April 2–4, 2013) in San Francisco. It has been launched as a key ingredient of Microsoft's planned \"makeover\" of the future operating systems for Windows Phone and Windows.\n\nIt is named after Cortana, a synthetic intelligence character in Microsoft's \"Halo\" video game franchise originating in Bungie folklore, with Jen Taylor, the character's voice actress, returning to voice the personal assistant's US-specific version.\n\nThe development of Cortana started in 2009 in the Microsoft Speech products team with general manager Zig Serafin and Chief Scientist Larry Heck. Heck and Serafin established the vision, mission, and long-range plan for Microsoft's digital-personal-assistant and they built a team with the expertise to create the initial prototypes for Cortana. To develop the Cortana digital assistant, the team interviewed human personal assistants. These interviews inspired a number of unique features in Cortana, including the assistant's \"notebook\" feature. Originally \"Cortana\" was only meant to be a codename, but a petition on Windows Phone's UserVoice site that proved to be popular made the codename official.\n\nIn January 2015, Microsoft announced the availability of Cortana for Windows 10 desktops and mobile devices as part of merging Windows Phone into the operating system at large.\n\nOn May 26, 2015, Microsoft announced that Cortana would also be available on other mobile platforms. An Android release was set for July 2015, but an Android APK file containing Cortana was leaked ahead of its release. It was officially released, along with an iOS version, in December 2015.\n\nDuring E3 2015, Microsoft announced that Cortana would come to the Xbox One as part of a universally designed Windows 10 update for the console.\n\nMicrosoft has integrated Cortana into numerous products such as Microsoft Edge, the browser bundled with Windows 10. Microsoft's Cortana assistant is deeply integrated into its Edge browser. Cortana can find opening-hours when on restaurant sites, show retail coupons for websites, or show weather information in the address bar. At the Worldwide Partners Conference 2015 Microsoft demonstrated Cortana integration with upcoming products such as GigJam. Conversely, Microsoft announced in late April 2016 that it would block anything other than Bing and Edge from being used to complete Cortana searches, again raising questions of anticompetitive behavior by the company.\n\nMicrosoft's \"Windows in the car\" concept includes Cortana. The concept makes it possible for drivers to make restaurant reservations and see places before they go there.\n\nAt Microsoft Build 2016, Microsoft announced plans to integrate Cortana into Skype (Microsoft's instant messaging service) as a bot to allow users to order food, book trips, transcribe video messages and make calendar appointments through Cortana and in addition to other bots. , Cortana can underline certain words and phrases in Skype conversations that relate to contacts and corporations. A writer from Engadget has criticised the Cortana integration in Skype for only responding to very specific keywords, feeling as if she was \"chatting with a search engine\" due to the impersonal way the bots replied to certain words such as \"Hello\" the Bing Music bot brought up Adele's song of that name.\n\nMicrosoft also announced at Microsoft Build 2016 that Cortana would be able to cloud-synchronise notifications between Windows 10 Mobile's and Windows 10's Action Center, as well as notifications from Android devices.\n\nIn May 2017, Microsoft in collaboration with Harman Kardon announced INVOKE, a voice-activated speaker featuring Cortana. The premium speaker has a cylindrical design and offers 360 degree sound, the ability to make and receive calls with Skype, and all of the other features currently available with Cortana.\nCortana can set reminders, recognize natural voice without the requirement for keyboard input, and answer questions using information from the Bing search engine (e.g., current weather and traffic conditions, sports scores, biographies). Searches will only be made with Microsoft Bing search engine and all links will open with Microsoft Edge, except when a screen reader such as Narrator is being used, where the links will open in Internet Explorer. Windows 8.1's universal Bing SmartSearch features are incorporated into Cortana, which replaces the previous Bing Search app which was activated when a user presses the \"Search\" button on their device. Cortana includes a music recognition service. Cortana can simulate rolling dice and flipping a coin. 'Cortana's \"Concert Watch\" monitors Bing searches to determine which bands or musicians the user is interested in. It integrates with the Microsoft Band watch band for Windows Phone devices if connected via Bluetooth, it can make reminders and phone notifications.\n\nSince the Lumia Denim mobile phone series, launched in October 2014, active listening was added to Cortana, enabling it to be invoked with the phrase: \"Hey Cortana\"; it can then be controlled as usual. Some devices from the United Kingdom by O2 have received the Lumia Denim update without the feature but this was later clarified as a bug and Microsoft has since fixed it.\n\nCortana integrates with services such as Foursquare to provide restaurant and local attraction recommendations and LIFX to control smart light bulbs.\n\nCortana stores personal information such as interests, location data, reminders, and contacts in the \"Notebook\". It can draw upon and add to this data to learn a user's specific patterns and behaviors. Users can view and specify what information is collected to allow some control over privacy, said to be \"a level of control that goes beyond comparable assistants\". Users can delete information from the \"Notebook\".\n\nCortana has a built in system of reminders which for example can be associated with a specific contact; it will then remind the user when in communication with that contact, possibly at a specific time or when the phone is in a specific location. Originally these reminders were specific to the device Cortana was installed on, but since Windows 10 Microsoft synchronises reminders across devices.\n\nMost versions of Cortana take the form of two nested, animated circles which are animated to indicate activities such as searching or talking. The main color scheme includes a black or white background and shades of blue for the respective circles.\n\nCortana on Windows mobile and Android is capable of capturing device notifications and sending them to a Windows 10 device. This allows a computer user to view notifications from their phone in the Windows 10 Action Center. The feature was announced in early 2016 and released later in the year.\n\nCortana has a \"do-not-disturb\" mode in which users can specify \"quiet hours\", as was available for Windows Phone 8.1 users. Users can change the settings so that Cortana calls users by their names or nicknames. It also has a library of \"Easter Eggs\", pre-determined remarks.\n\nWhen asked for a prediction, Cortana correctly predicted the winners of the first 14 matches of the football 2014 FIFA World Cup knockout stage, including the semi-finals, before it incorrectly picked Brazil over the Netherlands in the third place play-off match; this streak topped Paul the Octopus who correctly predicted all 7 of Germany's 2010 FIFA World Cup matches as well as the Final. Cortana can forecast results in various other sports such as the NBA, the NFL, the Super Bowl, the ICC Cricket World Cup and various European football leagues. Cortana can solve mathematical equations, convert units of measurement, and determine the exchange rates between currencies including Bitcoin.\n\nCortana can integrate with third-party apps on Windows 10 or directly through the service. Starting in late 2016, Cortana integrated with Microsoft's Wunderlist service, allowing Cortana to add and act on reminders.\n\nAt its Build 2017 conference, Microsoft announced that Cortana would get third-party skills capability, similar to that in Amazon Alexa.\n\nCortana indexes and stores user information. It can be disabled; this will cause Windows search to search the Web as well as the local computer, but this can be turned off. Turning Cortana off does not in itself delete user data stored on Microsoft's servers, but it can be deleted by user action. Microsoft has further been criticised for requests to Bing's website for a file called \"threshold.appcache\" which contains Cortana's information through searches made through the Start Menu even when Cortana is disabled on Windows 10.\n\n, Cortana was disabled for users aged under 13 years.\n\nThe UK version of Cortana speaks with a British accent and uses British idioms, while the Chinese version, known as Xiao Na, speaks Mandarin Chinese and has an icon featuring a face and two eyes, which is not used in other regions.\n\nThe United Kingdom (English) localised version of Cortana is voiced by Ginnie Watson, an Anglo-French actress, singer/songwriter and voice-over artist.\n\nThis table identifies the localized version of Cortana currently available. Except where indicated, this applies to both Windows Mobile and Windows 10 versions of the assistant. \n\nThe natural language processing capabilities of Cortana are derived from Tellme Networks (bought by Microsoft in 2007) and are coupled with a Semantic search database called Satori.\n\nCortana updates are delivered independently of those to the main Windows Phone OS, allowing Microsoft to provide new features at a faster pace. Not all Cortana-related features can be updated in this manner as some features such as \"Hey Cortana\" require the Windows Phone update service and the Qualcomm Snapdragon SensorCore Technology.\n\n\n", "id": "42119832", "title": "Cortana"}
{"url": "https://en.wikipedia.org/wiki?curid=50575063", "text": "Google Assistant\n\nGoogle Assistant is a virtual personal assistant developed by Google that is primarily available on mobile and smart home devices. Unlike Google Now, the Google Assistant can engage in two-way conversations.\n\nAssistant initially debuted in May 2016 as part of Google's messaging app Allo, and its voice-activated speaker Google Home. After a period of exclusivity on the Pixel and Pixel XL smartphones, it began to be deployed on other Android devices in February 2017, including third-party smartphones and Android Wear, and was released as a standalone app on the iOS operating system in May. Alongside the announcement of a software development kit in April 2017, the Assistant has been, and is being, further extended to support a large variety of devices, including cars and smart home appliances. The functionality of the Assistant can also be enhanced by third-party developers.\n\nUsers primarily interact with the Google Assistant through natural voice, though keyboard input is also supported. In the same nature and manner as Google Now, the Assistant is able to search the Internet, schedule events and alarms, adjust hardware settings on the user's device, and show information from the user's Google account. Google has also announced that the Assistant will be able to identify objects and gather visual information through the device's camera, and support purchasing products and sending money, as well as identifying songs.\n\nAt CES 2018, the first Assistant-powered smart displays (smart speakers with screens) were announced, with a planned release for summer 2018.\n\nThe Google Assistant was unveiled during Google's developer conference on May 18, 2016, as part of the unveiling of the Google Home smart speaker and new messaging app Allo; Google CEO Sundar Pichai explained that the Assistant was designed to be a conversational and two-way experience, and \"an ambient experience that extends across devices\". Later that month, Google assigned Google Doodle leader Ryan Germick and hired former Pixar animator Emma Coats to develop \"a little more of a personality.\"\n\nFor system-level integration outside of the Allo app and Google Home, the Google Assistant was initially exclusive to the Pixel and Pixel XL smartphones. In February 2017, Google announced that it had begun to enable access to the Assistant on Android smartphones running Android Marshmallow or Nougat, beginning in select English-speaking markets. Android tablets did not receive the Assistant as part of this rollout. The Assistant is also integrated in Android Wear 2.0, and will be included in future versions of Android TV and Android Auto. In October 2017, the Google Pixelbook became the first laptop to include Google Assistant. Google Assistant later came to the Google Pixel Buds. In December 2017, Google announced that the Assistant would be released for phones running Android Lollipop through an update to Google Play Services, as well as tablets running 6.0 Marshmallow and 7.0 Nougat.\n\nOn May 15, 2017, \"Android Police\" reported that the Google Assistant would be coming to the iOS operating system as a separate app. The information was confirmed two days later at Google's developer conference.\n\nIn January 2018 at the Consumer Electronics Show, the first Assistant-powered \"smart displays\" were released. Smart displays were shown at the event from Lenovo, Sony, JBL and LG. These devices have support for Google Duo video calls, YouTube videos, Google Maps directions, a Google Calendar agenda, viewing of smart camera footage, in addition to services which work with Google Home devices. These devices are planned for an initial release in summer 2018, with third-party developer support coming later in the year.\n\nIn December 2016, Google launched \"Actions on Google\", a developer platform for the Google Assistant. Actions on Google further enhances the Assistant user experience by enabling developers to bring their services to the Assistant. In March 2017, Google added new tools for developing on Actions on Google to support the creation of games for the Google Assistant. Originally limited to the Google Home smart speaker, Actions on Google was made available to Android and iOS devices in May 2017, at which time Google also introduced an app directory for overview of compatible products and services. To incentivize developers to build Actions, Google announced a competition in which it will select winners based on categories, including \"Best app by students\" and \"Best life hack.\" Winners will be announced by Google, and first place wins tickets to Google's 2018 developer conference, $10,000, and a walk-through of Google's campus, while second place and third place receive $7,500 and $5,000, respectively, and Google Home.\n\nIn April 2017, a software development kit (SDK) was released, allowing third-party developers to build their own hardware that can run the Google Assistant. It has been integrated into Raspberry Pi, cars from Audi and Volvo, and smart home appliances, including fridges, washers, and ovens, from companies including iRobot, LG, General Electric, and D-Link. Google updated the SDK in December 2017 to add several features only the Google Home smart speakers and Google Assistant smartphone apps had previously supported, including letting third-party device makers incorporate their own \"Actions on Google\" commands for their respective products, as well as incorporating text-based interactions and more languages, and allowing users to set a precise geographic location for the device to enable improved location-specific queries.\n\nThe Google Assistant, in the nature and manner of Google Now, can search the Internet, schedule events and alarms, adjust hardware settings on the user's device, and show information from the user's Google account. Unlike Google Now, however, the Assistant can engage in a two-way conversation, using Google's natural language processing algorithm. Search results are presented in a card format that users can tap to open the page. In February 2017, Google announced that users of Google Home would be able to shop entirely by voice for products through its Google Express shopping service, with products available from Whole Foods Market, Costco, Walgreens, PetSmart, and Bed Bath & Beyond at launch, and other retailers added in the following months as new partnerships were formed. The Google Assistant can maintain a shopping list; this was previously done within the notetaking service Google Keep, but the feature was moved to Google Express and the Google Home app in April 2017, resulting in a severe loss of functionality.\n\nIn May 2017, Google announced that the Assistant would support a keyboard for typed input and visual responses, support identifying objects and gather visual information through the device's camera, and support purchasing products and sending money. Through the use of the keyboard, users can see a history of queries made to the Google Assistant, and edit or delete previous inputs. The Assistant warns against deleting, however, due to its use of previous inputs to generate better answers in the future. In November 2017, it became possible to identify songs currently playing by asking the Assistant.\n\n\"PC World\"s Mark Hachman gave a favorable review of the Google Assistant, saying that it was a \"step up on Cortana and Siri.\"\n\n\n\n", "id": "50575063", "title": "Google Assistant"}
{"url": "https://en.wikipedia.org/wiki?curid=36279735", "text": "Google Now\n\nGoogle Now is an intelligent personal assistant developed by Google. It is available in the Google app for Android and iOS. Google Now uses a natural language user interface to answer questions, make recommendations, and perform actions by delegating requests to a set of web services. Along with answering user-initiated queries, Google Now proactively delivers information to users that it predicts (based on their search habits) they may want. Google Now was previously activated by saying \"Okay, Google Now,\" from either the Google app, or any screen. Google Now is currently activated by simply saying \"Okay Google\". Now's \"cards\" are also available for Chrome OS in the notification center. Voice search and limited voice-commands are available in the Google app for Microsoft Windows, and through Google Search when using the Google Chrome web-browser. \n\nIt was first included in Android 4.1 (\"Jelly Bean\"), which launched on July 9, 2012, and was first supported on the Galaxy Nexus smartphone. The service became available for iOS on April 29, 2013, without most of its features. In 2014 Google added Now cards to the notification center in Chrome OS and in the Chrome browser, however the notification center was later removed entirely from the Chrome browser. \"Popular Science\" named Google Now the \"Innovation of the Year\" for 2012. Google Now competes against assistants such as Apple's Siri and Microsoft's Cortana.\n\nSince 2015, Google has gradually phased out reference to \"Google Now\" in the Google app, with remaining use of \"Now\" being largely removed in October 2016, including replacing \"Now cards\" with \"feed\". At Google I/O 2016, Google showcased its new intelligent personal assistant Google Assistant, which is in sorts an evolution of Google Now. Unlike Google Now, however, Assistant can engage in two-way dialogue with the user.\n\nIn late 2011, reports surfaced that Google was enhancing its product Google Voice Search for the next version of Android. It was originally codenamed \"Majel\" after Majel Barrett, the wife of Gene Roddenberry and the voice of computer systems in the \"Star Trek\" franchise; it was also codenamed \"assistant\".\n\nOn June 27, 2012, Google Now was unveiled as part of the premier demonstration of Android 4.1 Jelly Bean at Google I/O 2012.\n\nOn October 29, 2012, Google Now received an update through the Google Play Store bringing the addition of Gmail cards. Google Now displays cards with information pulled from the user's Gmail account, such as flight information, package tracking information, hotel reservations and restaurant reservations (as long as the Gmail account is not a Google Apps account). Other additions were movie, concert, stock and news cards based on the users location and search history. Also included was the facility to create calendar events using voice input, for instance \"Make a new appointment for dinner with Steve next Thursday at 7pm\".\n\nOn December 5, 2012, an update to the Google Search application brought several new features to Google Now, including cards for nearby events, searching by camera when at a museum or shop, airplane boarding passes found from e-mail (United Airlines in the first instance, more airlines followed). In addition Google Now would show cards for the weather for upcoming travel destinations, birthday reminders; and monthly summaries of biking and walking activities. New voice action features included with this update include the ability to post to Google+, song recognition capabilities, and the ability to scan bar codes. However, when the Search 2.5 update hit, Google removed the \"Search With Camera\" feature.\n\nOn March 21, 2013, the executive Chairman of Google, Eric Schmidt, stated that Google had submitted an iOS version of Google Now to Apple for review and that the app was awaiting approval, but he later said that this was not true after Apple denied this was the case. Despite this, on April 29, 2013, Google Now was made available for iOS in an update to the operating system's Google Search application.\n\nBased on Google Chrome code review on December 2012, Google Now was expected to be integrated into the desktop version of Google Chrome. According to Seth Rosenblatt of CNET, it is rumored that Google Now will also serve as iGoogle's replacement in November 2013. On May 15, 2013, at Google I/O 2013, Google announced the upcoming release of Google Now on desktop platforms; the feature will be accessible only via Google Chrome or Google Chrome OS. On January 16, 2014, an alpha version of the Google Now was made available on desktop through the Google Chrome Canary release although this app lacks some of the cards available on mobile version of Google Now such as public alerts, nearby photos, activity summary and stocks. On March 24, 2014, Google started rolling out Google Now for Google Chrome users who are signed into their Google account on the browser.\n\nGoogle Now is implemented as an aspect of the Google Search application. It recognizes repeated actions that a user performs on the device (common locations, repeated calendar appointments, search queries, etc.) to display more relevant information to the user in the form of \"cards\". The system leverages Google's Knowledge Graph project, a system used to assemble more detailed search results by analyzing their meaning and connections.\n\nSpecialized cards currently comprise:\n\nIn January 2015, Google introduced the ability for participating, installed third-party apps to generate cards; on launch, this included apps such as Airbnb, eBay, \"The Guardian\", Pandora Radio and Lyft among others.\n\nOn Android 6.0 \"Marshmallow\", Google Now supports an additional feature known as \"Now On Tap\", which allows users to perform searches within the context of information currently being displayed in an app. When a user activates the feature, by holding the \"Home\" button or using a voice command, the entire text content of the current screen is parsed to search for keywords and other information (such as the names of people, television programs and films, etc.), which is then used to generate cards that display information, suggestions, and actions related to the content. Users can also use voice commands to ask questions related to the subjects of these cards.\n\nScott Webster of CNET praised Google Now for its ability to remind users of events based on past location histories and check-ins, and further commended it for providing \"information instantly in a clean, intuitive manner\" without the user's requesting it. A review by Ryan Paul of Ars Technica claims that like most other voice activated apps, including Siri, voice recognition is a major issue, but notes that the ability to type queries provides users with alternatives. Some commentators noted that Google Now's predictive power reveals \"exactly how much data and information Google actually has about [users'] routines and daily lives.\" An October 2014 review on Android Central showed Google Now outperforming its competition, Siri and Cortana.\n\nUnlike Apple's assistant app Siri, which sources its information from specific websites, Google Now \nsources its information from any website deemed relevant. For example, on November 25, 2014, \"Search Engine Land\" published an article about how, when asked about \"King Of United States,\" Google would inform the visitor that Barack Obama was the king of the United States, citing a sardonic conservative website. This has been mentioned as a weakness by some who prefer the Siri method, referring to the \"King of the United States\" example as a search engine error.\n\nHowever, others argue that Google simply does the search differently, not incorrectly. Some people want Google to filter the results for \"validity\", but the competing opinion is that Google is not in the business of deciding whether certain listings deserve to be in the returned group of results.\n\nIn terms of speech recognition, various studies have indicated one or the other is slightly more accurate, with ratings typically in the mid-80% range.\n\n", "id": "36279735", "title": "Google Now"}
{"url": "https://en.wikipedia.org/wiki?curid=42370155", "text": "Lyra (virtual assistant)\n\nLyra, until 2017 marketed under the name Indigo, is an intelligent personal assistant developed by Artificial Solutions and which runs on Android and iOS. The application uses natural language understanding to answer a user's questions on a variety of topics, make recommendations and operate the user's device.\n\nDemonstrated at Mobile World Congress in February 2013, the software, then called Indigo, was first publicly launched by Artificial Solutions for Android mobile devices in April 2013, with a Windows phone app appearing later in 2013 and an iOS app in 2014. One of the notable features of Lyra is its cross-platform functionality and ability to retain a conversation across multiple devices.\n\nThe Software was originally designed to compete with Apple's Siri Virtual Assistant and to bring the functionality to Android devices, which are incompatible with Siri.\n", "id": "42370155", "title": "Lyra (virtual assistant)"}
{"url": "https://en.wikipedia.org/wiki?curid=53802682", "text": "M (virtual assistant)\n\nM was a virtual assistant by Facebook, first announced in August 2015, that can automatically complete tasks for users, such as purchase items, arrange gift deliveries, reserve restaurant tables, and arrange travel. As of April 2017, it is available to about 10,000 users. It works inside the Facebook Messenger instant messaging service.\n\nWhen a user makes a request for M, it uses algorithms to determine what the user wants. If M doesn't understand, a human takes over the conversation, unbeknownst to the user. This allows M to learn. \n\nAlex Lebrun runs the project, which began in 2015, and which in April 2017 the \"MIT Technology Review\" called \"successful\".\n\nIn January 2018, Facebook announced that they would be discontinuing M. The company stated that what they learned from M would be applied to other artificial intelligence projects at Facebook.\n\nIn April 2017, Facebook enabled suggestions, powered by M, for users in the United States. M scans chats for keywords and then suggests relevant actions. For example, a user writing \"You owe me $20\" to a friend triggers M to enable the user's friend to pay the user via Facebook's payment platform.\n\n", "id": "53802682", "title": "M (virtual assistant)"}
{"url": "https://en.wikipedia.org/wiki?curid=52396698", "text": "Lucida (intelligent assistant)\n\nLucida is a natural language intelligent personal assistant (IPA) notable for being open source in a field of competing proprietary IPA services. Lucida was originally developed by Clarity Lab at the University of Michigan as a web service called Sirius.\n\n", "id": "52396698", "title": "Lucida (intelligent assistant)"}
{"url": "https://en.wikipedia.org/wiki?curid=43236335", "text": "Mya (program)\n\nMya was an intelligent personal assistant under development by Motorola. Proposed features for the program included the ability to read emails and answer questions 24 hours a day. Mya was intended to work with an internet service Motorola was developing called Myosphere, and was planned to be a paid service that would eventually be used by other mobile carriers. A female computer-generated character was created to represent Mya in advertising. While the quality of the character's animation was praised, it received criticism for being over sexualised. \n\nBoth the character and the program were announced to the public via an advertisement in March 2000, though the program was not ready for use at that time. Despite the announcement generating a considerable amount of attention, little was heard regarding the project in subsequent months. The program was never officially released nor cancelled, though the trademarks for both Myosphere and Mya were abandoned by Motorola in 2002. The name Mya was believed to be a play on the words 'My assistant'.\n\nThe Internet service that Mya was developed for was called Myosphere. Motorola began development of Myosphere in 1998, and it had been described as a speech enabled service \"which enables consumers to manage and control wireless and wireline communications from a single point of access using natural voice commands.\" Several other companies had already announced plans for similar software at the time; Alan Reiter from \"Wireless Internet and Mobile Computing\" was puzzled at Motorola's announcement of Myosphere, saying \"They're kind of late to the [voice activation] party. But the party is likely to be very big. ... Motorola's entry will help further legitimize the value of voice response systems. But it's a tough market, and it will take time.\" The term myosphere was \"a play on the theme of connecting the elements of an individual's world, or sphere.\"\n\nIntended to provide a human-like interface to the Internet, Mya was to be accessed via a toll-free telephone number and a pin code. The program was designed to work with any phone, including landlines, but primarily for mobiles, and was to be accessible 24 hours a day. Mya was said to be able to answer questions on topics such as stock prices, news, sports, weather conditions, traffic, airline reservations, addresses, and appointments, as well as being able to call contacts in a mobile phone's address book. \n\nIntended to be a paid service that would be ready by December 2000, Motorola hoped that Mya would also eventually be used on Palm Pilot and by other mobile carriers. In July 2000 Motorola was reported to be planning to work with Nuance Communications to internationalize Mya, and that same month BellSouth was declared to be the first carrier to buy the service. According to an article in \"Popular Science\" in August 2000, Motorola was spending \"millions of dollars\" on both the Mya character and the program. Mya was originally programmed only for English, though by April 2001 the program was being developed in six languages, and additionally Nippon Telegraph and Telephone were said to be working with Motorola to develop a Japanese version. Mya was voiced by actress Gabrielle Carteris, and mechanically altered to sound more digital.\n\nTo create a commercial for Mya, Motorola hired the McCann Erickson company, who in turn hired Digital Domain to create the character. The design was described as a \"big-budget\" production, though Digital Domain were only given three months to complete the project. Mya's physical representation: a tall, thin, blonde, blue-eyed white female, was created in the likeness of a human model, Michelle Holgate. The initial inspiration for Mya came from vintage pin-up girls. The first representation of Mya had a very small waist and large breasts, and was said to resemble Jessica Rabbit, which did not impress either Motorola or McCann Erickson. Motorola asked Digital Domain to make Mya look as human as possible yet still be obviously artificial. The first completed iteration of Mya was so realistic that Motorola asked for her to be made more obviously digital. Viewers were reportedly not impressed with Mya because they thought she was a real person. Digital Domain visual effects supervisor and animation director, Fred Raimondi, decided to remodel Mya's appearance to be \"just to the left of real\". Mya's hair was changed from brunette to bleach blonde. Her short spiky hair style was said to resemble that of Serena Altschul. According to Digital Domain, giving Mya hair that was longer than ear-length was not possible in the time they had been given, due to the difficulties of creating digital photorealistic hair. Mya's final body shape was an almost exact copy of the original model's measurements. Mya was typically seen wearing a silver pantsuit but also appeared in halter tops in some shots and dressed in an evening gown for her debut. While Digital Domain staff wanted Mya to appear in a knee-length skirt with high boots, Motorola and McCann chose the pantsuit, due to its contemporary look.\n\nDigital Domain chose not to use motion capture for Mya's movements as they believed it would constrain the character too much. Instead they used rotoscoping to place their digital character over the real model. The evening gown Motorola selected for Mya to debut in was described by Digital Domain as the most difficult item of clothing they could have chosen, due to its transparency and layering. To render 150 frames (equating to 5–6 seconds of actual footage) of Mya moving in the gown in low-resolution required approximately 6 hours of processing time; the final high-resolution shots took longer. Mya's rendering was so complex she crashed the computers at Digital Domain several times. Mya's creators said they had difficulty making Mya appear as if she were \"alive\", and focused intensely on movements, specular highlights and eye blinks in order to \"bring her to life\". The specular highlights also had the intended effect of making Mya shine in an inhuman manner; when the light hit Mya at certain angles, a rainbow would appear. Mya's skin was described as \"part china doll, part disco ball.\" Her distinct shine was based on that of a china plate that the commercial's director, Alex Proyas, had bought in Australia. In some shots of Mya, images were deliberately downgraded and had scan lines added to make the character appear more artificial.\n\nMya's visual representation, however, appeared solely during advertising and on her website. Only her voice was to be heard when using the actual program. Demonstrations of Mya's abilities and images of the character could be viewed at the now defunct website, mya.com. Raimondi said he believed the name Mya was a play on the words 'My assistant', as did Sidney Matrix in the book \"Cyberpop: Digital Lifestyles and Commodity Culture\".\n\nMya made her debut on March 26, 2000 in an 60-second advertisement shown during the 72nd Academy Awards. The ad featured Mya dressed in her evening gown and wearing a headset. In the ad Mya steps out of a stretch limousine and walks down the red carpet for the show. The ad declared Mya to be \"the darling of the e-world, the 24-hour talking Internet\" and stated that Mya's abilities would change users' lives. Despite making her first advertising appearance in March, the Mya program was not scheduled to be ready until December 2000.\n\nMya subsequently received considerable media attention, and was featured on the front covers of \"USA Today\", \"InStyle\", \"Wired\" and \"Adbusters\". One promotion for Mya showed Hugh Hefner sitting in a limousine with two Playboy Bunnies, asking Mya to read him his emails.\n\nThe Mya program was on display at the Motorola wireless booth at COMDEX in April 2000, which was visited by then president Bill Clinton. Mya \"chok[ed] up halfway\" through a demonstration for the president and had to be restarted.\n\nIn 2006 Sidney Matrix stated Mya \"disappeared\" after her debut commercial; in August 2000, a Yankee Group vice-president stated the debut advertisement for Mya was a \"great ad, but where have [Motorola] gone with it? ... The spot drove viewers to its website to demo the product ... but failed to market Mya further.\" Mya was never documented to have been released, nor was there an announcement of the program's cancellation; Motorola abandoned their trademark for 'Mya' on September 19, 2002 and their trademark for 'Myosphere' on December 1, 2002.\n\nThe reception of the character was mixed. Libby Callaway from \"New York Post\" stated Mya was one of their favourite \"virtual babes\", and said she threatened to take Lara Croft's title as the internet's most popular pin-up girl, also describing Mya as \"the world's first 'cyber assistant'\". Whilst admitting that the character of Mya was visually appealing, John Sullivan of \"Wireless Insider\" also stated that Motorola \"went overboard\" by trying to give the Mya program a character in the hopes she would become a celebrity in her own right, and accused Motorola of trying to mimic the success of Lara Croft. Mya was described in the 2003 book \"Data Made Flesh: Embodying Information\" as \"by far\" the best-rendered and most self-assured digital woman. Noah Robischon from \"Entertainment Weekly\" called her debut the second creepiest moment at that year's Academy Awards (the first being Angelina Jolie kissing her brother).\n\nWriting in \"Popular Mechanics\", Tobey Grumet described Mya as a male-chauvinistic creation, and she was cited in the 2006 book \"Physical Culture, Power, and the Body\" as an example of simulated sexualised females. Sidney Matrix stated that Mya's seductive appearance and sultry voice \"depended on, borrowed from and retrench[ed] sexist stereotypes\", and accused Motorola of normalising the assumption that technology users are both male and heterosexual. Motorola's marketing director Julie Roth defended the design of Mya's appearance and voice, attributing it to market research of what would appeal to users.\n\nMya's character was often compared to the female computer-simulated character for Ananova, a web-based news service that was being developed around the same time.\n\nThough Mya's character was generally regarded as impressive, the underlying technology was described by Peggy Albright in \"Wireless Week\" as not surprising; Albright said Motorola was \"latest company in recent weeks to introduce a voice-activated virtual assistant\", as Mya was announced shortly after Microsoft had announced their MiPad, and Lucent had launched their Mobile Voice Activated Dialing software. However, Tobey Grummet spoke highly of the program in anticipation of its release, and Mya was described by Elliot Drucker of \"Wireless Week\" as a solution to the limitations of accessing the Internet on a mobile phone, without a keyboard or large colour display. While regarding the program with interest, John Sullivan doubted that Mya would persuade people who were not already on the Internet to start using it, and stated that if Mya could only read emails and not actually converse with him, he would rather just read his emails himself. Dawn Chmielewski from the Orange County Register called Mya a \"crude interpretation of things to come\", noting that speech technology at the time was not without its limitations. \nThe program won the \"Most Innovative Telephony Application\" award at the 20th Annual AVIOS Conference in April 2001.\n\nBibliography\n\n", "id": "43236335", "title": "Mya (program)"}
{"url": "https://en.wikipedia.org/wiki?curid=17540116", "text": "SoundHound\n\nSoundHound Inc., founded in 2005, is an audio recognition and cognition company. It develops sound-recognition and sound-search technologies. Its feature product is a music recognition mobile app called SoundHound. In 2015, it released two new products, a voice-recognition virtual assistant mobile app called \"Hound\" and a voice-enabled artificial intelligence developer platform called \"Houndify\". The company’s headquarters are in Santa Clara, California.\n\nThe company was founded in 2005 by Keyvan Mohajer, a Ph.D. candidate at Stanford studying vocal recognition. In 2009, the company's \"Midomi\" app was rebranded as SoundHound. In 2012, SoundHound announced it had over 100 million users globally. In 2014, SoundHound became the first music-search product available as a wearable. Later, SoundHound became the first music recognition service shipping in autos, in a partnership with Hyundai, in the new 2015 Genesis. By May 2016, SoundHound had over 300 million users globally.\n\nAccording to an article of June 2015 SoundHound Inc. had raised a total of $40 million in funding from Global Catalyst Partners, Translink Capital, Walden Venture Capital, and other investors. This included $7 million in a Series B funding round the company secured in October 2008, bringing total funds raised to $12 million at this time. The round was led by TransLink Capital with the participation of JAIC America and Series A investor Global Catalyst Partners. In 2009 it attracted additional funding from Larry Marcus at Walden Venture Capital, who had previously invested in music startups Pandora and Snocap. The $4 million funding round was led by Walden Venture Capital VII, with the participation of an unnamed device manufacturer.\n\nIn January 2017, the company raised another $75 million in an investment round that included Nvidia, Samsung, Kleiner Perkins Caulfield & Byers, and others. \n\n", "id": "17540116", "title": "SoundHound"}
{"url": "https://en.wikipedia.org/wiki?curid=50491264", "text": "Viv (software)\n\nViv is an intelligent personal assistant software created by the developers of Siri. It debuted on May 9, 2016, at Disrupt NYC. Compared to Siri, the software's platform is open and can accommodate external plug-ins written to work with the assistant. It can also handle more complex queries. The development team has been working on the software since 2012 and had raised over $22 million in funding by early 2015 and $30 million by early 2016.\n\nOn October 5, 2016, Six Five Labs, Inc. was acquired by Samsung Electronics.\n\nThe following month, it was revealed that a personal assistant software would be available on the Samsung Galaxy S8 and S8+. This, however, turned out to be Bixby, which was a relaunch of S Voice, rather than based on Viv. In October 2017, Samsung announced Viv Labs technology would be integrated into Bixby 2.0.\n\n\n", "id": "50491264", "title": "Viv (software)"}
{"url": "https://en.wikipedia.org/wiki?curid=21903944", "text": "Wolfram Alpha\n\nWolfram Alpha (also styled WolframAlpha and Wolfram|Alpha) is a computational knowledge engine or answer engine developed by Wolfram Alpha LLC, a subsidiary of Wolfram Research. It is an online service that answers factual queries directly by computing the answer from externally sourced \"curated data\", rather than providing a list of documents or web pages that might contain the answer as a search engine might.\n\nWolfram Alpha, which was released on May 18, 2009, is based on Wolfram's earlier flagship product Wolfram Mathematica, a computational platform or toolkit that encompasses computer algebra, symbolic and numerical computation, visualization, and statistics capabilities. Additional data is gathered from both academic and commercial websites such as the CIA's \"The World Factbook\", the United States Geological Survey, a Cornell University Library publication called \"All About Birds\", \"Chambers Biographical Dictionary\", Dow Jones, the \"Catalogue of Life\", CrunchBase, Best Buy, the FAA and optionally a user's Facebook account.\n\nUsers submit queries and computation requests via a text field. Wolfram Alpha then computes answers and relevant visualizations from a knowledge base of curated, structured data that come from other sites and books. The site \"use[s] a portfolio of automated and manual methods, including statistics, visualization, source cross-checking, and expert review.\" The curated data makes Alpha different from semantic search engines, which index a large number of answers and then try to match the question to one.\n\nWolfram Alpha can only provide robust query results based on computational facts, not queries on the social sciences, cultural studies or even many questions about history where responses require more subtlety and complexity. It is able to respond to particularly-phrased natural language fact-based questions such as \"Where was Mary Robinson born?\" or more complex questions such as \"How old was Queen Elizabeth II in 1974?\" It displays its \"Input interpretation\" of such a question, using standardized phrases such as \"age | of Queen Elizabeth II (royalty) | in 1974\", the answer of which is \"Age at start of 1974: 47 years\", and a biography link. Wolfram Alpha does not answer queries which require a narrative response such as \"What is the difference between the Julian and the Gregorian calendars?\" but will answer factual or computational questions such as \"June 1 in Julian calendar\".\n\nMathematical symbolism can be parsed by the engine, which typically responds with more than the numerical results. For example, \"lim(x->0) (sin x)/x\" yields the correct limiting value of 1, as well as a plot, up to 235 terms () of the Taylor series, and (for registered users) a possible derivation using L'Hôpital's rule. It is also able to perform calculations on data using more than one source. For example, \"What is the fifty-second smallest country by GDP per capita?\" yields Nicaragua, $1160 per year.\n\nWolfram Alpha is written in 15 million lines of Wolfram Language code and runs on more than 10,000 CPUs. The database currently includes hundreds of datasets, such as \"All Current and Historical Weather.\" The datasets have been accumulated over several years. The curated (as distinct from auto-generated) datasets are checked for quality either by a scientist or other expert in a relevant field, or someone acting in a clerical capacity who simply verifies that the datasets are \"acceptable\".\n\nOne example of a live dataset that Wolfram Alpha can use is the profile of a Facebook user, through inputting the \"facebook report\" query. If the user authorizes Facebook to share his or her account details with the Wolfram site, Alpha can generate a \"personal analytics\" report containing the age distribution of friends, the frequency of words used in status updates and other detailed information. Within two weeks of launching the Facebook analytics service, 400,000 users had used it. Downloadable query results are behind a pay wall but summaries are accessible to free accounts.\n\nWolfram Alpha has been used to power some searches in the Microsoft Bing and DuckDuckGo search engines. With the first release on July 21, 2017, Brave web browser features Wolfram Alpha as one of its default search engines. For factual question answering, it is also queried by Apple's Siri, Samsung's S Voice, as well as Dexetra's speech recognition software for the Android platform, Iris, and the voice control software on BlackBerry 10.\n\nLaunch preparations began on May 15, 2009 at 7 pm CDT and were broadcast live on Justin.tv. The plan was to publicly launch the service a few hours later, with expected issues due to extreme load. The service was officially launched on May 18, 2009.\n\nWolfram Alpha has received mixed reviews. Wolfram Alpha advocates point to its potential, some even stating that how it determines results is more important than current usefulness.\n\nOn December 3, 2009, an iPhone app was introduced. Some users considered the initial $50 price of the iOS app unnecessarily high, since the same features could be freely accessed by using a web browser instead. They also complained about the simultaneous removal of the mobile formatting option for the site. Wolfram responded by lowering the price to $2, offering a refund to existing customers and re-instating the mobile site.\n\nOn October 6, 2010 an Android version of the app was released and it is now available for Kindle Fire and Nook. (The Nook version is not available outside the US). A further 71 apps are available which use the Wolfram Alpha engine for specialized tasks.\n\nOn February 8, 2012, Wolfram Alpha Pro was released, offering users additional features for a monthly subscription fee. A key feature is the ability to upload many common file types and data—including raw tabular data, images, audio, XML, and dozens of specialized scientific, medical, and mathematical formats—for automatic analysis. Other features include an extended keyboard, interactivity with CDF, data downloads, in-depth step by step solution, the ability to customize and save graphical and tabular results and extra computation time.\n\nAlong with new premium features, Wolfram Alpha Pro has led to some changes in the free version of the site:\n\n\"InfoWorld\" published an article warning readers of the potential implications of giving an automated website proprietary rights to the data it generates. Free software advocate Richard Stallman also opposes the idea of recognizing the site as a copyright holder and suspects that Wolfram would not be able to make this case under existing copyright law.\n\n\n", "id": "21903944", "title": "Wolfram Alpha"}
{"url": "https://en.wikipedia.org/wiki?curid=55531461", "text": "Alisa (virtual assistant)\n\nAlisa (, Alice) is a Russian intelligent personal assistant for Android, iOS and Windows operating systems developed by Yandex. Alisa was officially introduced on 10 October 2017. Aside from common tasks, such as internet search or weather forecasts, it can also run applications and chit-chat.\n\nA beta version of Alisa was released in May 2017. Later a neural network-based \"chit-chat\" engine was added allowing Russian-speaking users to have free conversations with Alisa about anything. Speech recognition was found to be particularly challenging for the Russian language due to its grammatical and morphological complexities. To handle it, Alisa was equipped with Yandex’s SpeechKit, which, according to word error rate, provides the highest accuracy for spoken Russian recognition. Alisa's voice is based on that of the Russian voice actress Tatyana Shitova. \n\nVoice requests to Alisa are processed by Yandex cloud servers to retain some of them with the aim of expanding Alisa's training set data. According to Denis Filippov, head of Yandex Speech Technologies, the retained voice data are completely anonymous and without any association with users' accounts.\n\n ", "id": "55531461", "title": "Alisa (virtual assistant)"}
{"url": "https://en.wikipedia.org/wiki?curid=26086272", "text": "Siri\n\nSiri (pronounced ) is an intelligent personal assistant, part of Apple Inc.'s iOS, watchOS, macOS, and tvOS operating systems. The assistant uses voice queries and a natural language user interface to attempt to answer questions, make recommendations, and perform actions by delegating requests to a set of Internet services. The software adapts to users' individual language usages, searches, and preferences, with continuing use. Returned results are individualized.\n\nSiri is a spin-off from a project originally developed by the SRI International Artificial Intelligence Center. Its speech recognition engine is provided by Nuance Communications, and Siri uses advanced machine learning technologies to function. Its original American, British, and Australian voice actors recorded their respective voices around 2005, unaware of the recordings' eventual usage in Siri. The voice assistant was released as an app for iOS in February 2010, and it was acquired by Apple two months later. Siri was then integrated into iPhone 4S at its release in October 2011. At that time, the separate app was also removed from the iOS App Store. Siri has become an integral part of Apple's products, having been adapted into other hardware devices over the years, including newer iPhone models, as well as iPad, iPod Touch, Mac, AirPods, Apple TV and the upcoming HomePod.\n\nSiri supports a wide range of user commands, including performing phone actions, checking basic information, scheduling events and reminders, handling device settings, searching the Internet, navigating areas, finding information on entertainment, and is able to engage with iOS-integrated apps. With the release of iOS 10 in 2016, Apple opened up limited third-party access to Siri, including third-party messaging apps, as well as payments, ride-sharing, and Internet calling apps. With the release of iOS 11, Apple has updated Siri's voices for more clear, human voices, supports follow-up questions and language translation, and additional third-party actions.\n\nSiri's original release on iPhone 4S in 2011 received mixed reviews. It received praise for its voice recognition and contextual knowledge of user information, including calendar appointments, but was criticized for requiring stiff user commands and having a lack of flexibility. It was also criticized for lacking information on certain nearby places, and for its inability to understand certain English accents. In 2016 and 2017, a number of media reports have indicated that Siri is lacking in innovation, particularly against new competing voice assistants from other technology companies. The reports concerned Siri's limited set of features, \"bad\" voice recognition, and undeveloped service integrations as causing trouble for Apple in the field of artificial intelligence and cloud-based services; the basis for the complaints reportedly due to stifled development, as caused by Apple's prioritization of user privacy and executive power struggles within the company.\n\nSiri is a spin-out from the SRI International Artificial Intelligence Center, and is an offshoot of the DARPA-funded CALO project. It was co-founded by Dag Kittlaus, Adam Cheyer, and Tom Gruber.\n\nSiri's speech recognition engine is provided by Nuance Communications, a speech technology company. This was not officially acknowledged by Apple nor Nuance for years, until Nuance CEO Paul Ricci confirmed the information at a 2011 technology conference. The speech recognition system makes use of sophisticated machine learning techniques, including convolutional neural networks and long short-term memory.\n\nApple's first notion of a digital personal assistant was originally a concept video in 1987, called the \"Knowledge Navigator\".\n\nThe original American voice of Siri was provided by Susan Bennett in July 2005, unaware that it would eventually be used for the voice assistant. A report from \"The Verge\" in September 2013 about voice actors, their work, and machine learning developments, made hints that Allison Dufty was the voice behind Siri, though this was disproven when Dufty wrote on her website that she was \"absolutely, positively NOT the voice of Siri\". Citing growing pressure, Bennett revealed her role as Siri in October, and her claim was proven by Ed Primeau, an American audio forensics expert. Apple has never confirmed the information.\n\nThe original British male voice was provided by Jon Briggs, a former technology journalist. Having discovered that he was the voice of Siri by watching television, he first spoke only about his role in November 2011, also acknowledging his voice work was done \"five or six years ago\" without knowing the recordings' final usage form.\n\nThe original Australian voice was provided by Karen Jacobsen, a voice-over artist known in Australia for her work as the \"GPS girl\".\n\nAs part of an interview between all three voice actors and \"The Guardian\", Briggs stated that \"The original system was recorded for a US company called Scansoft, who were then bought by Nuance. Apple simply licensed it.\"\n\nWith iOS 11, Apple auditioned \"hundreds of candidates\" to find a new female voice, then recorded hours of speech, including different personalities and expressions, and built a new text-to-speech voice based on deep learning technology.\n\nSiri was originally released as a stand-alone application for the iOS operating system in February 2010, and at the time, the developers were also intending to release Siri for Android and BlackBerry devices. Two months later, Apple acquired Siri. On October 4, 2011, Apple introduced the iPhone 4S with a beta version of Siri. After the announcement, Apple removed the existing standalone Siri app from App Store. \"TechCrunch\" wrote that, despite the Siri app's support for iPhone 4, its removal from App Store might also have had a financial aspect for the company, in providing an incentive for customers to upgrade devices. Third-party developer Steven Troughton-Smith, however, managed to port Siri to iPhone 4, though without being able to communicate with Apple's servers. A few days later, Troughton-Smith, working with an anonymous person nicknamed \"Chpwn\", managed to fully hack Siri, enabling its full functionalities on iPhone 4 and iPod Touch devices. Additionally, developers were also able to successfully create and distribute legal ports of Siri to any device capable of running iOS 5, though a proxy server was required for Apple server interaction.\nOver the years, Apple has expanded the line of officially supported products, including newer iPhone models, as well as iPad support in June 2012, iPod Touch support in September 2012, Apple TV support, and the standalone Siri Remote, in September 2015, Mac and AirPods support in September 2016, and HomePod support. Homepod does not currently have a set release date.\n\nApple offers a wide range of voice commands to interact with Siri, including, but not limited to:\n\nSiri also offers numerous pre-programmed responses to amusing questions. Such questions include \"What is the meaning of life?\", to which Siri may reply \"All evidence to date suggests it's chocolate\"; \"Why am I here?\", to which it may reply \"I don't know. Frankly, I've wondered that myself\"; and \"Will you marry me?\", to which it may respond with \"My End User Licensing Agreement does not cover marriage. My apologies\".\n\nInitially limited to female voices, Apple announced in June 2013 that Siri would feature a gender option, adding a male voice counterpart.\n\nIn September 2014, Apple added the ability for users to speak \"Hey Siri\" to enable the assistant without the requirement of physically handling the device.\n\nIn September 2015, the \"Hey Siri\" feature was updated to include individualized voice recognition, a presumed effort to prevent non-owner activation.\n\nWith the announcement of iOS 10 in June 2016, Apple opened up limited third-party developer access to Siri through a dedicated application programming interface (API). The API restricts usage of Siri to engaging with third-party messaging apps, payment apps, ride-sharing apps, and Internet calling apps.\n\nIn iOS 11, Siri is able to handle follow-up questions, supports language translation, and opens up to more third-party actions, including task management. Additionally, users are able to type to Siri, and a new, privacy-minded \"on-device learning\" technique improves Siri's suggestions by privately analyzing personal usage of different iOS applications.\n\nSiri received mixed reviews during its \"beta\" release as an integrated part of iPhone 4S in October 2011.\n\nMG Siegler of \"TechCrunch\" wrote that Siri was \"great\", adding that \"The amount of times Siri hasn’t been able to understand and execute my request is astonishingly low\". Siegler further praised the potential for Siri after losing the \"beta\" tag, writing \"Just imagine what will happen when Apple partners with other services to expand Siri further. And imagine when they have an API that any developer can use. This really could alter the mobile landscape\".\n\nWriting for \"The New York Times\", David Pogue also praised Siri, writing that it \"thinks for a few seconds, displays a beautifully formatted response and speaks in a calm female voice\". Pogue praised Siri's language understanding, stating that \"It’s mind-blowing how inexact your utterances can be. Siri understands everything from, “What’s the weather going to be like in Tucson this weekend?” to “Will I need an umbrella tonight?”\". Pogue also equally praised Siri's ability to understand context, using the example of \"Once, I tried saying, “Make an appointment with Patrick for Thursday at 3.” Siri responded, “Note that you already have an all-day appointment about ‘Boston Trip’ for this Thursday. Shall I schedule this anyway?” Unbelievable\".\n\nJacqui Cheng of \"Ars Technica\" wrote that Apple's claims of what Siri could do were \"bold\", and the early demos \"even bolder\". Cheng wrote that \"Though Siri shows real potential, these kinds of high expectations are bound to be disappointed\", adding that \"Apple makes clear that the product is still in beta—an appropriate label, in our opinion\". While praising its ability to \"decipher our casual language\" and deliver \"very specific and accurate result\", sometimes even providing additional information, Cheng noted and criticized its restrictions, particularly when the language moved away from \"stiffer commands\" into more human interactions. One example included the phrase \"Send a text to Jason, Clint, Sam, and Lee saying we're having dinner at Silver Cloud\", which Siri interpreted as sending a message to Jason only, containing the text \"Clint Sam and Lee saying we're having dinner at Silver Cloud\". She also noted a lack of proper editability, as saying \"Edit message to say: we're at Silver Cloud and you should come find us\", generated \"Clint Sam and Lee saying we're having dinner at Silver Cloud to say we're at Silver Cloud and you should come find us\".\n\nGoogle's executive chairman and former chief, Eric Schmidt, conceded that Siri could pose a \"competitive threat\" to the company's core search business.\n\nSiri was criticized by pro-choice abortion organizations, including the American Civil Liberties Union (ACLU) and NARAL Pro-Choice America, after users found that Siri could not provide information about the location of birth control or abortion providers nearby, sometimes directing users to pro-life crisis pregnancy centers instead. Natalie Kerris, a spokeswoman for Apple, told \"The New York Times\" that \"Our customers want to use Siri to find out all types of information, and while it can find a lot, it doesn’t always find what you want\", adding \"These are not intentional omissions meant to offend anyone. It simply means that as we bring Siri from beta to a final product, we find places where we can do better, and we will in the coming weeks\". In January 2016, \"Fast Company\" reported that, in then-recent months, Siri had begun to confuse the word \"abortion\" with \"adoption\", citing \"health experts\" who stated that the situation had \"gotten worse\". However, at the time of \"Fast Company\"s report, the situation had changed slightly, with Siri offering \"a more comprehensive list of Planned Parenthood facilities\", although \"Adoption clinics continue to pop up, but near the bottom of the list.\"\n\nSiri has also not been well received by some English speakers with distinctive accents, including Scottish and Americans from Boston or the South.\n\nIn March 2012, Frank M. Fazio filed a class action lawsuit against Apple on behalf of the people who bought iPhone 4S and felt misled about the capabilities of Siri, alleging its failing to function as depicted in Apple's Siri commercials. Fazio filed the lawsuit in California and claimed that the iPhone 4S was merely a \"more expensive iPhone 4\" if Siri fails to function as advertised. On July 22, 2013, U.S. District Judge Claudia Wilken in San Francisco dismissed the suit but said the plaintiffs could amend at a later time. The reason given for dismissal was that plaintiffs did not sufficiently document enough misrepresentations by Apple for the trial to proceed.\n\nIn June 2016, \"The Verge\"s Sean O'Kane wrote about the then-upcoming major iOS 10 updates, with a headline stating \"Siri's big upgrades won't matter if it can't understand its users\". O'Kane wrote that \"What Apple didn’t talk about was solving Siri’s biggest, most basic flaws: it’s still not very good at voice recognition, and when it gets it right, the results are often clunky. And these problems look even worse when you consider that Apple now has full-fledged competitors in this space: Amazon’s Alexa, Microsoft’s Cortana, and Google’s Assistant.\" Also writing for \"The Verge\", Walt Mossberg had previously questioned Apple's efforts in cloud-based services, writing:\n[...] perhaps the biggest disappointment among Apple's cloud-based services is the one it needs most today, right now: Siri. Before Apple bought it, Siri was on the road to being a robust digital assistant that could do many things, and integrate with many services — even though it was being built by a startup with limited funds and people. After Apple bought Siri, the giant company seemed to treat it as a backwater, restricting it to doing only a few, slowly increasing number of tasks, like telling you the weather, sports scores, movie and restaurant listings, and controlling the device's functions. Its unhappy founders have left Apple to build a new AI service called Viv. And, on too many occasions, Siri either gets things wrong, doesn't know the answer, or can't verbalize it. Instead, it shows you a web search result, even when you're not in a position to read it.\nIn October 2016, \"Bloomberg\" reported that Apple had plans to unify the teams behind its various cloud-based services, including a single campus and reorganized cloud computing resources aimed at improving the processing of Siri's queries, though another report from \"The Verge\" in June 2017 once again called Siri's voice recognition \"bad\".\n\nIn June 2017, \"The Wall Street Journal\" published an extensive report on the lack of innovation with Siri following competitors' advancement in the field of voice assistants. Noting that Apple workers' anxiety levels \"went up a notch\" on the announcement of Amazon's Alexa, the \"Journal\" wrote that \"Today, Apple is playing catch-up in a product category it invented, increasing worries about whether the technology giant has lost some of its innovation edge\". The report cites the primary causes as being Apple's prioritization of user privacy, including randomly-tagged six-month Siri searches, whereas Google and Amazon keep data until actively discarded by the user, and executive power struggles with some employees leaving. Apple declined to comment on the report, while Eddy Cue said that \"Apple often uses generic data rather than user data to train its systems and has the ability to improve Siri’s performance for individual users with information kept on their iPhones\".\n\n\n", "id": "26086272", "title": "Siri"}
{"url": "https://en.wikipedia.org/wiki?curid=56327946", "text": "Clova (virtual assistant)\n\nClova is an intelligent personal assistant for Android and iOS operating systems developed by Naver Corporation and Line Corporation (a subsidiary of Naver). Clova, short for \"cloud virtual assistant\", was officially introduced in May 2017. It was first used in the Wave (smart speaker) released in Korea as Wave (sometimes referred to as Naver Wave) and in Japan as Line Wave for the Korean and Japanese markets.\n\n", "id": "56327946", "title": "Clova (virtual assistant)"}
{"url": "https://en.wikipedia.org/wiki?curid=38776364", "text": "Rapyuta\n\nRapyuta is the online database for the RoboEarth Cloud Engine which is a platform as a service (PaaS). The database is part of the European RoboEarth Project. The project is designed to allow robots to query the database to learn about their environment, build, as well as providing guidance systems. Rapyuta project lead was Mohanarajah Gajamohan.\n\nThe name Rapyuta refers to Laputa in \"Castle in the Sky.\"\n\nThe stated purpose of the project is:\n\n[T]he goal of RoboEarth is to allow robotic systems to benefit from the experience of other robots, paving the way for rapid advances in machine cognition and behavior, and ultimately, for more subtle and sophisticated human-machine interaction. \n", "id": "38776364", "title": "Rapyuta"}
{"url": "https://en.wikipedia.org/wiki?curid=34643983", "text": "MyRobots\n\nMyRobots is an online social network for robots and smart objects. The site claims robots can benefit from being connected to other robots in the same way humans use social networks to interact and collaborate with other humans. The robots reportedly share their sensor information, giving insight on their perspective of their current state.\n\nMyRobots has been launched by RobotShop in December 2011. MyRobots is one of the first Cloud Robotics initiative. Cloud robotics happens when robots are connected to the Internet. Then, by doing so, robots become augmented with more capacity and intelligence. The cloud lets them communicate with other machines and serve their human operators better. Connected robots equal augmented robots. By collaborating with other machines and humans, robots transcend their physical limitations and become more useful and capable, since they can delegate parts of their tasks to more suitable parties.\n\nMyRobots provides a Restful API. All internet enabled devices and robots can connect via the API.\n\nMost important keywords used in the MyRobots API:\n\nChannel or Robot - The name for where data can be inserted or retrieved within the MyRobots API, identified by a numerical Channel ID.\n\nField - One of eight specific locations for data inside of a channel, identified by a number between 1 and 8. It can store numeric data from sensors or alphanumeric strings from serial devices or RFID readers.\n\nStatus - A short status message to augment the data stored in a channel.\n\nLocation - The latitude, longitude, and elevation of where data is being sent from.\n\nFeed - The collective name for the data stored inside a channel, which may be any combination of field data, status updates, and location info.\n\nWrite API Key – A 16 digit code that allows an application to write data to a channel.\n\nRead API Key – A 16 digit code that allows an application to read the data stored in a channel.\n\nThe MyRobots Connect is a Serial-to-Ethernet gateway that connects directly to MyRobots Data Engine. It's an Open-source Hardware (OSHW) module that uses a Microchip PIC18F67J60 to run a TCP-IP stack. It relays the serial commands received by an included XBee module using an Open Serial Protocol. The MyRobots Connect receives serial data and messages using the wireless XBee module and sends them to the MyRobot servers using a standard internet connection.\n\nThe MyRobots Connect uses DHCP so connecting it to a router is a simple matter of plugging a network cable; no configuration required.\n\n\n", "id": "34643983", "title": "MyRobots"}
