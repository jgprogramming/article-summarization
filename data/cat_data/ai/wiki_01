{"url": "https://en.wikipedia.org/wiki?curid=41255201", "text": "Winner-take-all in action selection\n\nWinner-take-all is a computer science concept that has been widely applied in behavior-based robotics as a method of action selection for intelligent agents. Winner-take-all systems work by connecting modules (task-designated areas) in such a way that when one action is performed it stops all other actions from being performed, so only one action is occurring at a time. The name comes from the idea that the \"winner\" action takes all of the motor system's power.\n\nIn the 1980s and 1990s, many roboticists and cognitive scientists were attempting to find speedier and more efficient alternatives to the traditional world modeling method of action selection. In 1982, Jerome A. Feldman and D.H. Ballard published the \"Connectionist Models and Their Properties\", referencing and explaining winner-take-all as a method of action selection. Feldman's architecture functioned on the simple rule that in a network of interconnected action modules, each module will set its own output to zero if it reads a higher input than its own in any other module. In 1986, Rodney Brooks introduced behavior-based artificial intelligence. Winner-take-all architectures for action selection soon became a common feature of behavior-based robots, because selection occurred at the level of the action modules (bottom-up) rather than at a separate cognitive level (top-down), producing a tight coupling of stimulus and reaction.\n\nIn the hierarchical architecture, actions or behaviors are programmed in a high-to-low priority list, with inhibitory connections between all the action modules. The agent performs low-priority behaviors until a higher-priority behavior is stimulated, at which point the higher behavior inhibits all other behaviors and takes over the motor system completely. Prioritized behaviors are usually key to the immediate survival of the agent, while behaviors of lower priority are less time-sensitive. For example, \"run away from predator\" would be ranked above \"sleep.\"\nWhile this architecture allows for clear programming of goals, many roboticists have moved away from the hierarchy because of its inflexibility.\n\nIn the heterarchy and fully distributed architecture, each behavior has a set of pre-conditions to be met before it can be performed, and a set of post-conditions that will be true after the action has been performed. These pre- and post-conditions determine the order in which behaviors must be performed and are used to causally connect action modules. This enables each module to receive input from other modules as well as from the sensors, so modules can recruit each other. For example, if the agent’s goal were to reduce thirst, the behavior \"drink\" would require the pre-condition of having water available, so the module would activate the module in charge of \"find water\". The activations organize the behaviors into a sequence, even though only one action is performed at a time. The distribution of larger behaviors across modules makes this system flexible and robust to noise. Some critics of this model hold that any existing set of division rules for the predecessor and conflictor connections between modules produce sub-par action selection. In addition, the feedback loop used in the model can in some circumstances lead to improper action selection.\n\nIn the arbiter and centrally coordinated architecture, the action modules are not connected to each other but to a central arbiter. When behaviors are triggered, they begin \"voting\" by sending signals to the arbiter, and the behavior with the highest number of votes is selected. In these systems, bias is created through the \"voting weight\", or how often a module is allowed to vote. Some arbiter systems take a different spin on this type of winner-take-all by using a \"compromise\" feature in the arbiter. Each module is able to vote for or against each smaller action in a set of actions, and the arbiter selects the action with the most votes, meaning that it benefits the most behavior modules. \n\nThis can be seen as violating the general rule against creating representations of the world in behavior-based AI, established by Brooks. By performing command fusion, the system is creating a larger composite pool of knowledge than is obtained from the sensors alone, forming a composite inner representation of the environment. Defenders of these systems argue that forbidding world-modeling puts unnecessary constraints on behavior-based robotics, and that agents benefits from forming representations and can still remain reactive.\n\n", "id": "41255201", "title": "Winner-take-all in action selection"}
{"url": "https://en.wikipedia.org/wiki?curid=41555934", "text": "Knowledge Based Software Assistant\n\nThe Knowledge Based Software Assistant (KBSA) was a research program funded by the United States Air Force. The goal of the program was to apply concepts from artificial intelligence to the problem of designing and implementing computer software. Software would be described by models in very high level languages (essentially equivalent to first order logic) and then transformation rules would transform the specification into efficient code. The air force hoped to be able to generate the software to control weapons systems and other command and control systems using this method. As software was becoming ever more critical to USAF weapons systems it was realized that improving the quality and productivity of the software development process could have significant benefits for the military, as well as for information technology in other major US industries.\n\nIn the early 1980s the United States Air Force realized that they had received significant benefits from applying artificial intelligence technologies to solving expert problems such as the diagnosis of faults in aircraft. The air force commissioned a group of researchers from the artificial intelligence and formal methods communities to develop a report on how such technologies might be used to aid in the more general problem of software development.\n\nThe report described a vision for a new approach to software development. Rather than define specifications with diagrams and manually transform them to code as was the current process, the KBSA vision was to define specifications in very high level languages and then to use transformation rules to gradually refine the specification into efficient code on heterogeneous platforms.\n\nEach step in the design and refinement of the system would be recorded as part of an integrated repository. In addition to the artifacts of software development the processes, the various definitions and transformations, would also be recorded in a way that they could be analyzed and also replayed later as needed. The idea was that each step would be a transformation that took into account various non-functional requirements for the implemented system. For example, requirements to use specific programming languages such as Ada or to harden code for real time mission critical fault tolerance.\n\nThe air force decided to fund further research on this vision through their Rome Air Development Center laboratory at Griffiss air force base in New York. The majority of the early research was conducted at the Kestrel Institute in Northern California (with Stanford University) and the Information Sciences Institute (ISI) in Southern California (with USC and UCLA). The Kestrel Institute focused primarily on the provably correct transformation of logical models to efficient code. ISI focused primarily on the front end of the process on defining specifications that could map to logical formalisms but were in formats that were intuitive and familiar to systems analysts. In addition, Raytheon did a project to investigate informal requirements gathering and Honeywell and Harvard University did work on underlying frameworks, integration, and activity coordination.\n\nAlthough not primarily funded by the KBSA program the MIT Programmer's Apprentice project also had many of the same goals and used the same techniques as KBSA.\n\nIn the later stages of the KBSA program (starting in 1991) researchers developed prototypes that were used on medium to large scale software development problems. Also, in these later stages the emphasis shifted from a pure KBSA approach to more general questions of how to use knowledge-based technology to supplement and augment existing and future computer-aided software engineering (CASE) tools. In these later stages there was significant interaction between the KBSA community and the object-oriented and software engineering communities. For example, KBSA concepts and researchers played an important role in the mega-programming and user centered software engineering programs sponsored by the Defense Advanced Research Projects Agency (DARPA). In these later stages the program changed its name to Knowledge-Based Software Engineering (KBSE). The name change reflected the different research goal, no longer to create a totally new all encompassing tool that would cover the complete software life cycle but to gradually work knowledge-based technology into existing tools. Companies such as Andersen Consulting (one of the largest system integrators and at the time vendor of their own CASE tool) played a major role in the program in these later stages.\n\nThe transformation rules that KBSA used were different than traditional rules for expert systems. Transformation rules matched against specification and implementation languages rather than against facts in the world. It was possible to specify transformations using patterns, wildcards, and recursion on both the right and left hand sides of a rule. The left hand expression would specify patterns in the existing knowledge base to search for. The right hand expression could specify a new pattern to transform the left hand side into. For example, transform a set theoretic data type into code using an Ada set library.\n\nThe initial purpose for transformation rules was to refine a high level logical specification into well designed code for a specific hardware and software platform. This was inspired by early work on theorem proving and automatic programming. However, researchers at the Information Sciences Institute (ISI) developed the concept of \"evolution transformations\". Rather than transforming a specification into code an evolution transformation was meant to automate various stereotypical changes at the specification level, for example developing a new superclass by extracting various capabilities from an existing class that can be shared more generally. Evolution transformations were developed at approximately the same time as the emergence of the software patterns community and the two groups shared concepts and technology. Evolution transformations were essentially what is known as refactoring in the object-oriented software patterns community.\n\nA key concept of KBSA was that all artifacts: requirements, specifications, transformations, designs, code, process models, etc. were represented as objects in a knowledge-based repository. The original KBSA report describes what was called a Wide Spectrum Language. The requirement was for a knowledge representation framework that could support the entire life cycle: requirements, specification, and code as well as the software process itself. The core representation for the knowledge base was meant to utilize the same framework although various layers could be added to support specific presentations and implementations.\n\nThese early knowledge-base frameworks were developed primarily by ISI and Kestrel building on top of Lisp and Lisp machine environments. The Kestrel environment was eventually turned into a commercial product called Refine which was developed and supported by a spin-off company from Kestrel called Reasoning Systems Incorporated.\n\nThe Refine language and environment also proved to be applicable to the problem of software reverse engineering: taking legacy code that is critical to the business but that lacks proper documentation and using tools to analyze it and transform it to a more maintainable form. With the growing concern of the Y2K problem reverse engineering was a major business concern for many large US corporations and it was a focus area for KBSA research in the 1990s.\n\nThere was significant interaction between the KBSA communities and the Frame language and object-oriented communities. The early KBSA knowledge-bases were implemented in object-based languages rather than object-oriented. Objects were represented as classes and sub-classes but it was not possible to define methods on the objects. In later versions of KBSA such as the Andersen Consulting Concept Demo the specification language was expanded to support message passing as well.\n\nKBSA took a different approach than traditional expert systems when it came to how to solve problems and work with users. In the traditional expert system approach the user answers a series of interactive questions and the system provides a solution. The KBSA approach left the user in control. Where as an expert system tried to, to some extent replace and remove the need for the expert the intelligent assistant approach in KBSA sought to re-invent the process with technology. This led to a number of innovations at the user interface level.\n\nAn example of the collaboration between the object-oriented community and KBSA was the architecture used for KBSA user interfaces. KBSA systems utilized a model-view-controller (MVC) user interface. This was an idea incorporated from Smalltalk environments. The MVC architecture was especially well suited to the KBSA user interface. KBSA environments featured multiple heterogeneous views of the knowledge-base. It might be useful to look at an emerging model from the standpoint of entities and relations, object interactions, class hierarchies, dataflow, and many other possible views. The MVC architecture facilitated this. With the MVC architecture the underlying model was always the knowledge base which was a meta-model description of the specification and implementation languages. When an analyst made some change via a particular diagram (e.g. added a class to the class hierarchy) that change was made at the underlying model level and the various views of the model were all automatically updated.\n\nOne of the benefits of using a transformation was that many aspects of the specification and implementation could be modified at once. For small scale prototypes the resulting diagrams were simple enough that basic layout algorithms combined with reliance on users to clean up diagrams was sufficient. However, when a transformation can radically redraw models with tens or even hundreds of nodes and links the constant updating of the various views becomes a task in itself. Researchers at Andersen Consulting incorporated work from the University of Illinois on graph theory to automatically update the various views associated with the knowledge base and to generate graphs that have minimal intersection of links and also take into account domain and user specific layout constraints.\n\nAnother concept used to provide intelligent assistance was automatic text generation. Early research at ISI investigated the feasibility of extracting formal specifications from informal natural language text documents. They determined that the approach was not viable. Natural language is by nature simply too ambiguous to serve as a good format for defining a system. However, natural language generation was seen to be feasible as a way to generate textual descriptions that could be read by managers and non-technical personnel. This was especially appealing to the air force since by law they required all contractors to generate various reports that describe the system from different points of view. Researchers at ISI and later Cogentext and Andersen Consulting demonstrated the viability of the approach by using their own technology to generate the documentation required by their air force contracts.\n", "id": "41555934", "title": "Knowledge Based Software Assistant"}
{"url": "https://en.wikipedia.org/wiki?curid=41644056", "text": "Inductive programming\n\nInductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.\n\nDepending on the programming language used, there are several kinds of inductive programming. Inductive functional programming, which uses functional programming languages such as Lisp or Haskell, and most especially inductive logic programming, which uses logic programming languages such as Prolog and other logical representations such as description logics, have been more prominent, but other (programming) language paradigms have also been used, such as constraint programming or probabilistic programming.\n\nInductive programming incorporates all approaches which are concerned with learning programs or algorithms from incomplete (formal) specifications. Possible inputs in an IP system are a set of training inputs and corresponding outputs or an output evaluation function, describing the desired behavior of the intended program, traces or action sequences which describe the process of calculating specific outputs, constraints for the program to be induced concerning its time efficiency or its complexity, various kinds of background knowledge such as standard data types, predefined functions to be used, program schemes or templates describing the data flow of the intended program, heuristics for guiding the search for a solution or other biases.\n\nOutput of an IP system is a program in some arbitrary programming language containing conditionals and loop or recursive control structures, or any other kind of Turing-complete representation language.\n\nIn many applications the output program must be correct with respect to the examples and partial specification, and this leads to the consideration of inductive programming as a special area inside automatic programming or program synthesis, usually opposed to 'deductive' program synthesis, where the specification is usually complete.\n\nIn other cases, inductive programming is seen as a more general area where any declarative programming or representation language can be used and we may even have some degree of error in the examples, as in general machine learning, the more specific area of structure mining or the area of symbolic artificial intelligence. A distinctive feature is the number of examples or partial specification needed. Typically, inductive programming techniques can learn from just a few examples.\n\nThe diversity of inductive programming usually comes from the applications and the languages that are used: apart from logic programming and functional programming, other programming paradigms and representation languages have been used or suggested in inductive programming, such as functional logic programming, constraint programming, probabilistic programming, abductive logic programming, modal logic, action languages, agent languages and many types of imperative languages.\n\nResearch on the inductive synthesis of recursive functional programs started in the early 1970s and was brought onto firm theoretical foundations with the seminal THESIS system of Summers and work of Biermann.\nThese approaches were split into two phases: first, input-output examples are transformed into non-recursive programs (traces) using a small set of basic operators; second, regularities in the traces are searched for and used to fold them into a recursive program. The main results until the mid 1980s are surveyed by Smith. Due to limited progress with respect to the range of programs that could be synthesized, research activities decreased significantly in the next decade.\n\nThe advent of logic programming brought a new elan but also a new direction in the early 1980s, especially due to the MIS system of Shapiro eventually spawning the new field of inductive logic programming (ILP). The early works of Plotkin, and his \"\"relative least general generalization (rlgg)\"\", had an enormous impact in inductive logic programming. Most of ILP work addresses a wider class of problems, as the focus is not only on recursive logic programs but on machine learning of symbolic hypotheses from logical representations. However, there were some encouraging results on learning recursive Prolog programs such as quicksort from examples together with suitable background knowledge, for example with GOLEM. But again, after initial success, the community got disappointed by limited progress about the induction of recursive programs with ILP less and less focusing on recursive programs and leaning more and more towards a machine learning setting with applications in relational data mining and knowledge discovery.\n\nIn parallel to work in ILP, Koza proposed genetic programming in the early 1990s as a generate-and-test based approach to learning programs. The idea of genetic programming was further developed into the inductive programming system ADATE and the systematic-search-based system MagicHaskeller. Here again, functional programs are learned from sets of positive examples together with an output evaluation (fitness) function which specifies the desired input/output behavior of the program to be learned.\n\nThe early work in grammar induction (also known as grammatical inference) is related to inductive programming, as rewriting systems or logic programs can be used to represent production rules. In fact, early works in inductive inference considered grammar induction and Lisp program inference as basically the same problem. The results in terms of learnability were related to classical concepts, such as identification-in-the-limit, as introduced in the seminal work of Gold. More recently, the language learning problem was addressed by the inductive programming community.\n\nIn the recent years, the classical approaches have been resumed and advanced with great success. Therefore, the synthesis problem has been reformulated on the background of constructor-based term rewriting systems taking into account modern techniques of functional programming, as well as moderate use of search-based strategies and usage of background knowledge as well as automatic invention of subprograms. Many new and successful applications have recently appeared beyond program synthesis, most especially in the area of data manipulation, programming by example and cognitive modelling (see below).\n\nOther ideas have also been explored with the common characteristic of using declarative languages for the representation of hypotheses. For instance, the use of higher-order features, schemes or structured distances have been advocated for a better handling of recursive data types and structures; abstraction has also been explored as a more powerful approach to cumulative learning and function invention.\n\nOne powerful paradigm that has been recently used for the representation of hypotheses in inductive programming (generally in the form of generative models) is probabilistic programming (and related paradigms, such as stochastic logic programs and Bayesian logic programming).\n\nThe first workshop on Approaches and Applications of Inductive Programming (AAIP) held in conjunction with ICML 2005 identified all applications where \"learning of programs or recursive rules are called for, [...] first in the domain of software engineering where structural learning, software assistants and software agents can help to relieve programmers from routine tasks, give programming support for end users, or support of novice programmers and programming tutor systems. Further areas of application are language learning, learning recursive control rules for AI-planning, learning recursive concepts in web-mining or for data-format transformations\".\n\nSince then, these and many other areas have shown to be successful application niches for inductive programming, such as end-user programming, the related areas of programming by example and programming by demonstration, and intelligent tutoring systems.\n\nOther areas where inductive inference has been recently applied are knowledge acquisition, artificial general intelligence, reinforcement learning and theory evaluation, and cognitive science in general. There may also be prospective applications in intelligent agents, games, robotics, personalisation, ambient intelligence and human interfaces.\n\n\n\n\n", "id": "41644056", "title": "Inductive programming"}
{"url": "https://en.wikipedia.org/wiki?curid=41780237", "text": "MANIC (cognitive architecture)\n\nMANIC, formerly known as PMML.1, is a cognitive architecture developed by the predictive modeling and machine learning laboratory at University Of Arkansas. It differs from other cognitive architectures in that it tries to \"minimize novelty\". That is, it attempts to organize well-established techniques in computer science, rather than propose any new methods for achieving cognition. While most other cognitive architectures are inspired by some neurological observation, and are subsequently developed in a top-down manner to behave in some manner like a brain, MANIC is inspired only by common practices in computer science, and was developed in a bottom-up manner for the purpose of unifying various methods in machine learning and artificial intelligence.\n\nAt the highest level, MANIC describes a software agent that, supposedly, will exhibit cognitive intelligence. The agent's artificial brain comprises two major components: a learning system and a decision-making system.\n\nThe learning system models the agent's environment as a dynamical system. It consists of an \"observation function\", which maps from the agent's current beliefs to predicted observations, and a \"transition function\", which maps from current beliefs to future beliefs in the next time-step. The observation function is implemented with a generative deep learning architecture. It is trained in an unsupervised manner from the observations that the agent makes. The intrinsic representations of those observations become the agents \"beliefs\". The transition function is trained in a supervised manner, to predict the next beliefs from the current ones. The entire learning system is based loosely on a 2011 paper by Michael S. Gashler that describes a method for training a deep neural network to model a simple dynamical system from visual observations.\n\nThe decision-making system consists of a planning module and a contentment function. The planning module uses an evolutionary algorithm to evolve a satisficing plan. The contentment function maps from the agent's current beliefs, or anticipated beliefs, to an evaluation of the utility of being in that state. It is trained by reinforcement from a human teacher. In order to facilitate this reinforcement learning, MANIC provides a mechanism for the agent to generate \"fantasy videos\" that show anticipated observations if a candidate plan were to be executed. The idea is that a human teacher would evaluate these videos and rank them according to desirability or utility, and the agent could then use that feedback to refine its contentment function.\n\nMANIC proposes that the learning system gives the agent awareness of its environment by modeling it, and using that model to anticipate future beliefs. It further proposes that a similar mechanism can also implement sentience. That is, it claims that awareness can be implemented with an outward-looking model, and sentience can be implemented with an inward-looking model. Therefore, it proposes to add \"introspective senses\", which theoretically give the agent the ability to become aware of its own inner feelings, by modeling them, just as it is aware of its external environment. To some extent, MANIC suggests that existing methods already in use in artificial intelligence are unintentionally creating subjective experiences like those typically associated with conscious beings.\n\n", "id": "41780237", "title": "MANIC (cognitive architecture)"}
{"url": "https://en.wikipedia.org/wiki?curid=42042198", "text": "ADS-AC\n\nADS-AC is an experimental open source program which implements Absolutely Dynamic System, a proposed mechanism for AC.\n\nADS-AC uses a system of \"points\" or \"nodes\" connected by \"links\" to create a structure of \"knots\". The nodes have no properties other than their links to other nodes in order to maintain their absolute dynamicity.\n\n", "id": "42042198", "title": "ADS-AC"}
{"url": "https://en.wikipedia.org/wiki?curid=42349326", "text": "Vaumpus world\n\nWaumpus world is a simple world use in artificial intelligence for which to represent knowledge and to reason.Vaumpus world was introduced by Genesereth, and is discussed in Russell-Norvig Artificial intelligence book () inspired by 1972 video game Hunt the Wumpus .\n\n", "id": "42349326", "title": "Vaumpus world"}
{"url": "https://en.wikipedia.org/wiki?curid=42581062", "text": "Cognitive computing\n\nCognitive computing (CC) describes technology platforms that, broadly speaking, are based on the scientific disciplines of artificial intelligence and signal processing. These platforms encompass machine learning, reasoning, natural language processing, speech recognition and vision (object recognition), human–computer interaction, dialog and narrative generation, among other technologies.\n\nAt present, there is no widely agreed upon definition for cognitive computing in either academia or industry.\n\nIn general, the term cognitive computing has been used to refer to new hardware and/or software that mimics the functioning of the human brain (2004) and helps to improve human decision-making. In this sense, CC is a new type of computing with the goal of more accurate models of how the human brain/mind senses, reasons, and responds to stimulus. CC applications link data analysis and adaptive page displays (AUI) to adjust content for a particular type of audience. As such, CC hardware and applications strive to be more affective and more influential by design.\n\nSome features that cognitive systems may express are:\n\n\nCognitive computing-branded technology platforms typically specialize in the processing and analysis of large, unstructured datasets.\n\nWord processing documents, emails, videos, images, audio files, presentations, webpages, social media and many other data formats often need to be manually tagged with metadata before they can be fed to a computer for analysis and insight generation. The principal benefit of utilizing cognitive analytics over traditional big data analytics is that such datasets do not need to be pretagged.\n\nOther characteristics of a cognitive analytics system include:\n\n", "id": "42581062", "title": "Cognitive computing"}
{"url": "https://en.wikipedia.org/wiki?curid=17188", "text": "KL-ONE\n\nKL-ONE (pronounced \"kay ell won\") is a well known knowledge representation system in the tradition of semantic networks and frames; that is, it is a frame language. The system is an attempt to overcome semantic indistinctness in semantic network representations and to explicitly represent conceptual information as a structured inheritance network.\n\nThere is a whole family of KL-ONE-like systems. One of the innovations that KL-ONE initiated was the use of a deductive classifier, an automated reasoning engine that can validate a frame ontology and deduce new information about the ontology based on the initial information provided by a domain expert. \n\nFrames in KL-ONE are called concepts. These form hierarchies using subsume-relations; in the KL-ONE terminology a super class is said to subsume its subclasses. \nMultiple inheritance is allowed. Actually a concept is said to be well-formed only if it inherits from more than one other concept. All concepts, except the top concept (usually THING), must have at least one super class. \n\nIn KL-ONE descriptions are separated into two basic classes of concepts: primitive and defined. Primitives are domain concepts that are not fully defined. This means that given all the properties of a concept, this is not sufficient to classify it. They may also be viewed as incomplete definitions. Using the same view, defined concepts are complete definitions. Given the properties of a concept, these are necessary and sufficient conditions to classify the concept.\n\nThe slot-concept is called roles and the values of the roles are role-fillers. There are several different types of roles to be used in different situations. The most common and important role type is the generic RoleSet that captures the fact that the role may be filled with more than one filler.\n\n", "id": "17188", "title": "KL-ONE"}
{"url": "https://en.wikipedia.org/wiki?curid=485226", "text": "Frame language\n\nA frame language is a technology used for knowledge representation in artificial intelligence. Frames are stored as ontologies of sets and subsets of the frame concepts. They are similar to class hierarchies in object-oriented languages although their fundamental design goals are different. Frames are focused on explicit and intuitive representation of knowledge whereas objects focus on encapsulation and information hiding. Frames originated in AI research and objects primarily in software engineering. However, in practice the techniques and capabilities of frame and object-oriented languages overlap significantly.\n\nEarly work on Frames was inspired by psychological research going back to the 1930s that indicated people use stored stereotypical knowledge to interpret and act in new cognitive situations. The term Frame was first used by Marvin Minsky as a paradigm to understand visual reasoning and natural language processing. In these and many other types of problems the potential solution space for even the smallest problem is huge. For example, extracting the phonemes from a raw audio stream or detecting the edges of an object. Things which seem trivial to humans are actually quite complex. In fact, how difficult they really were was probably not fully understood until AI researchers began to investigate the complexity of getting computers to solve them.\n\nThe initial notion of Frames or Scripts as they were also called is that they would establish the context for a problem and in so doing automatically reduce the possible search space significantly. The idea was also adopted by Schank and Abelson who used it to illustrate how an AI system could process common human interactions such as ordering a meal at a restaurant. These interactions were standardized as Frames with slots that stored relevant information about each Frame. Slots are analogous to object properties in object-oriented modeling and to relations in entity-relation models. Slots often had default values but also required further refinement as part of the execution of each instance of the scenario. I.e., the execution of a task such as ordering at a restaurant was controlled by starting with a basic instance of the Frame and then instantiating and refining various values as appropriate. Essentially the abstract Frame represented an object class and the frame instances an object instance. In this early work the emphasis was primarily on the static data descriptions of the Frame. Various mechanisms were developed to define the range of a slot, default values, etc. However, even in these early systems there were procedural capabilities. One common technique was to use \"triggers\" (similar to the database concept of triggers) attached to slots. A trigger was simply procedural code that was attached to a slot. The trigger could fire either before and/or after a slot value was accessed or modified.\n\nAs with object classes, Frames were organized in subsumption hierarchies. For example, a basic frame might be ordering at a restaurant. An instance of that would be Joe goes to McDonald's. A specialization (essentially a subclass) of the restaurant frame would be a frame for ordering at a fancy restaurant. The fancy restaurant frame would inherit all the default values from the restaurant frame but also would either add more slots or change one or more of the default values (e.g., expected price range) for the specialized frame.\n\nMuch of the early Frame language research (e.g. Schank and Abelson) had been driven by findings from experimental psychology and attempts to design knowledge representation tools that corresponded to the patterns humans were thought to use to function in daily tasks. These researchers were less interested in mathematical formality since they believed such formalisms were not necessarily good models for the way the average human conceptualizes the world. The way humans use language for example is often far from truly logical.\n\nSimilarly, in linguistics, Charles J. Fillmore in the mid-1970s started working on his theory of frame semantics, which later would lead to computational resources like FrameNet. Frame semantics was motivated by reflections on human language and human cognition.\n\nResearchers such as Ron Brachman on the other hand wanted to give AI researchers the mathematical formalism and computational power that were associated with Logic. Their aim was to map the Frame classes, slots, constraints, and rules in a Frame language to set theory and logic. One of the benefits of this approach is that the validation and even creation of the models could be automated using theorem provers and other automated reasoning capabilities. The drawback was that it could be more difficult to initially specify the model in a language with a formal semantics.\n\nThis evolution also illustrates a classic divide in AI research known as the \"neats vs. scruffies\". The \"neats\" were researchers who placed the most value on mathematical precision and formalism which could be achieved via First Order Logic and Set Theory. The \"scruffies\" were more interested in modeling knowledge in representations that were intuitive and psychologically meaningful to humans.\n\nThe most notable of the more formal approaches was the KL-ONE language. KL-ONE later went on to spawn several subsequent Frame languages. The formal semantics of languages such as KL-ONE gave these frame languages a new type of automated reasoning capability known as the classifier. The classifier is an engine that analyzes the various declarations in the frame language: the definition of sets, subsets, relations, etc. The classifier can then automatically deduce various additional relations and can detect when some parts of a model are inconsistent with each other. In this way many of the tasks that would normally be executed by forward or backward chaining in an inference engine can instead be performed by the classifier.\n\nThis technology is especially valuable in dealing with the Internet. It is an interesting result that the formalism of languages such as KL-ONE can be most useful dealing with the highly informal and unstructured data found on the Internet. On the Internet it is simply not feasible to require all systems to standardize on one data model. It is inevitable that terminology will be used in multiple inconsistent forms. The automatic classification capability of the classifier engine provides AI developers with a powerful toolbox to help bring order and consistency to a very inconsistent collection of data (i.e., the Internet). The vision for an enhanced Internet, where pages are ordered not just by text keywords but by classification of concepts is known as the Semantic Web. Classification technology originally developed for Frame languages is a key enabler of the Semantic Web. The \"neats vs. scruffies\" divide also emerged in Semantic Web research, culminating in the creation of the Linking Open Data community—their focus was on exposing data on the Web rather than modeling.\n\nA simple example of concepts modeled in a frame language is the Friend of A Friend (FOAF) ontology defined as part of the Semantic Web as a foundation for social networking and calendar systems. The primary frame in this simple example is a \"Person\". Example slots are the person's \"email\", \"home page, phone,\" etc. The interests of each person can be represented by additional frames describing the space of business and entertainment domains. The slot \"knows\" links each person with other persons. Default values for a person's interests can be inferred by the web of people they are friends of.\n\nThe earliest Frame based languages were custom developed for specific research projects and were not packaged as tools to be re-used by other researchers. Just as with expert system inference engines, researchers soon realized the benefits of extracting part of the core infrastructure and developing general purpose frame languages that were not coupled to specific applications. One of the first general purpose frame languages was KRL. One of the most influential early Frame languages was KL-ONE KL-ONE spawned several subsequent Frame languages. One of the most widely used successors to KL-ONE was the Loom language developed by Robert MacGregor at the Information Sciences Institute.\n\nIn the 1980s Artificial Intelligence generated a great deal of interest in the business world fueled by expert systems. This led to the development of many commercial products for the development of knowledge-based systems. These early products were usually developed in Lisp and integrated constructs such as IF-THEN rules for logical reasoning with Frame hierarchies for representing data. One of the most well known of these early Lisp knowledge-base tools was the Knowledge Engineering Environment (KEE) from Intellicorp. KEE provided a full Frame language with multiple inheritance, slots, triggers, default values, and a rule engine that supported backward and forward chaining. As with most early commercial versions of AI software KEE was originally deployed in Lisp on Lisp machine platforms but was eventually ported to PCs and Unix workstations.\n\nThe research agenda of the Semantic Web spawned a renewed interest in automatic classification and frame languages. An example is the Web Ontology Language (OWL) standard for describing information on the Internet. OWL is a standard to provide a semantic layer on top of the Internet. The goal is that rather than organizing the web using keywords as most applications (e.g. Google) do today the web can be organized by concepts organized in an ontology.\n\nThe name of the OWL language itself provides a good example of the value of a Semantic Web. If one were to search for \"OWL\" using the Internet today most of the pages retrieved would be on the bird Owl rather than the standard OWL. With a Semantic Web it would be possible to specify the concept \"Web Ontology Language\" and the user would not need to worry about the various possible acronyms or synonyms as part of the search. Likewise the user would not need to worry about homonyms crowding the search results with irrelevant data such as information about birds of prey as in this simple example.\n\nIn addition to OWL various standards and technologies that are relevant to the Semantic Web and were influenced by Frame languages include OIL and DAML. The Protege Open Source software tool from Stanford University provides an ontology editing capability that is built on OWL and has the full capabilities of a classifier. However it ceased to explicitly support frames as of version 3.5 (which is maintained for those preferring frame orientation), the version current in 2017 being 5. The justification for moving from explicit frames being that OWL DL is more expressive and \"industry standard\". \n\nFrame languages have a significant overlap with object-oriented languages. The terminologies and goals of the two communities were different but as they moved from the academic world and labs to the commercial world developers tended to not care about philosophical issues and focused primarily on specific capabilities, taking the best from either camp regardless of where the idea began. What both paradigms have in common is a desire to reduce the distance between concepts in the real world and their implementation in software. As such both paradigms arrived at the idea of representing the primary software objects in taxonomies starting with very general types and progressing to more specific types.\n\nThe following table illustrates the correlation between standard terminology from the object-oriented and frame language communities:\n\nThe primary difference between the two paradigms was in the degree that encapsulation was considered a major requirement. For the object-oriented paradigm encapsulation was one of the if not the most critical requirement. The desire to reduce the potential interactions between software components and hence manage large complex systems was a key driver of object-oriented technology. For the frame language camp this requirement was less critical than the desire to provide a vast array of possible tools to represent rules, constraints, and programming logic. In the object-oriented world everything is controlled by methods and the visibility of methods. So for example, accessing the data value of an object property must be done via an accessor method. This method controls things such as validating the data type and constraints on the value being retrieved or set on the property. In Frame languages these same types of constraints could be handled in multiple ways. Triggers could be defined to fire before or after a value was set or retrieved. Rules could be defined that managed the same types of constraints. The slots themselves could be augmented with additional information (called \"facets\" in some languages) again with the same type of constraint information.\n\nThe other main differentiator between frame and OO languages was multiple inheritance (allowing a frame or class to have two or more superclasses). For frame languages multiple inheritance was a requirement. This follows from the desire to model the world the way humans do, human conceptualizations of the world seldom fall into rigidly defined non-overlapping taxonomies. For many OO languages, especially in the later years of OO, single inheritance was either strongly desired or required. Multiple inheritance was seen as a possible step in the analysis phase to model a domain but something that should be eliminated in the design and implementation phases in the name of maintaining encapsulation and modularity.\n\nAlthough the early frame languages such as KRL did not include message passing, driven by the demands of developers, most of the later frame languages (e.g. Loom, KEE) included the ability to define messages on Frames.\n\nOn the object-oriented side, standards have also emerged that provide essentially the equivalent functionality that frame languages provided, albeit in a different format and all standardized on object libraries. For example, the Object Management Group has standardized specifications for capabilities such as associating test data and constraints with objects (analogous to common uses for facets in Frames and to constraints in Frame languages such as Loom) and for integrating rule engines.\n\n\n\n", "id": "485226", "title": "Frame language"}
{"url": "https://en.wikipedia.org/wiki?curid=43249770", "text": "Knowledge acquisition\n\nKnowledge acquisition is the process used to define the rules and ontologies required for a knowledge-based system. The phrase was first used in conjunction with expert systems to describe the initial tasks associated with developing an expert system, namely finding and interviewing domain experts and capturing their knowledge via rules, objects, and frame-based ontologies. \n\nExpert systems were one of the first successful applications of artificial intelligence technology to real world business problems. Researchers at Stanford and other AI laboratories worked with doctors and other highly skilled experts to develop systems that could automate complex tasks such as medical diagnosis. Until this point computers had mostly been used to automate highly data intensive tasks but not for complex reasoning. Technologies such as inference engines allowed developers for the first time to tackle more complex problems. \n\nAs expert systems scaled up from demonstration prototypes to industrial strength applications it was soon realized that the acquisition of domain expert knowledge was one of if not the most critical task in the knowledge engineering process. This knowledge acquisition process became an intense area of research on its own. One of the earlier works on the topic used Batesonian theories of learning to guide the process.\n\nOne approach to knowledge acquisition investigated was to use natural language parsing and generation to facilitate knowledge acquisition. Natural language parsing could be performed on manuals and other expert documents and an initial first pass at the rules and objects could be developed automatically. Text generation was also extremely useful in generating explanations for system behavior. This greatly facilitated the development and maintenance of expert systems.\n\nA more recent approach to knowledge acquisition is a re-use based approach. Knowledge can be developed in ontologies that conform to standards such as the Web Ontology Language (OWL). In this way knowledge can be standardized and shared across a broad community of knowledge workers. One example domain where this approach has been successful is bioinformatics. \n", "id": "43249770", "title": "Knowledge acquisition"}
{"url": "https://en.wikipedia.org/wiki?curid=43274058", "text": "Knowledge-based recommender system\n\nKnowledge-based recommender systems (knowledge based recommenders) are a specific type of recommender system that are based on explicit knowledge about the item assortment, user preferences, and recommendation criteria (i.e., which item should be recommended in which context). These systems are applied in scenarios where alternative approaches such as collaborative filtering and content-based filtering cannot be applied.\n\nA major strength of knowledge-based recommender systems is the non-existence of cold-start (ramp-up) problems. A corresponding drawback is a potential knowledge acquisition bottleneck triggered by the need to define recommendation knowledge in an explicit fashion.\n\nKnowledge-based recommender systems are well suited to complex domains where items are not purchased very often, such as apartments and cars. Further examples of item domains relevant for knowledge-based recommender systems are financial services, digital cameras, and tourist destinations. Rating-based systems often do not perform well in these domains due to the low number of available ratings.\n\nAdditionally, in complex item domains, customers want to specify their preferences explicitly (e.g., \"the maximum price of the car is X\") . In this context, the recommender system must take into account constraints: for instance, only those financial services that support the investment period specified by the customer should be recommended. Neither of these aspects are supported by approaches such as collaborative filtering and content-based filtering.\n\nKnowledge-based recommender systems are often conversational, i.e., user requirements and preferences are elicited within the scope of a feedback loop. A major reason for the conversational nature of knowledge-based recommender systems is the complexity of the item domain where it is often impossible to articulate all user preferences at once. Furthermore, user preferences are typically not known exactly at the beginning but are constructed within the scope of a recommendation session.\n\nIn a search-based recommender, user feedback is given in terms of answers to questions which restrict the set of relevant items. An example of such a question is \"Which type of lens system do you prefer: fixed or exchangeable lenses?\". On the technical level, search-based recommendation scenarios can be implemented on the basis of constraint-based recommender systems. Constraint-based recommender systems are implemented on the basis of constraint search or different types of conjunctive query-based approaches.\n\nIn a navigation-based recommender, user feedback is typically provided in terms of \"critiques\" which specify change requests regarding the item currently recommended to the user. Critiques are then used for the recommendation of the next \"candidate\" item. An example of a critique in the context of a digital camera recommendation scenario is \"I would like to have a camera like this but with a lower price\". This is an example of a \"unit critique\" which represents a change request on a single item attribute. \"Compound critiques\" allow the specification of more than one change request at a time. \"Dynamic critiquing\" also takes into account preceding user critiques (the critiquing history). More recent approaches additionally exploit information stored in user interaction logs to further reduce the interaction effort in terms of the number of needed critiquing cycles.\n\n", "id": "43274058", "title": "Knowledge-based recommender system"}
{"url": "https://en.wikipedia.org/wiki?curid=15893057", "text": "Applications of artificial intelligence\n\nArtificial intelligence, defined as intelligence exhibited by machines, has many applications in today's society. More specifically, it is Weak AI, the form of A.I. where programs are developed to perform specific tasks, that is being utilized for a wide range of activities including medical diagnosis, electronic trading, robot control, and remote sensing. AI has been used to develop and advance numerous fields and industries, including finance, healthcare, education, transportation, and more.\n\nSeveral U.S. academic institutions are employing AI to tackle some of the world's greatest economic and social challenges. For example, the University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address socially relevant problems such as homelessness. At Stanford, researchers are using AI to analyze satellite images to identify which areas have the highest poverty levels.\n\nThe Air Operations Division (AOD) uses AI for the rule based expert systems. The AOD has use for artificial intelligence for surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.\n\nThe use of artificial intelligence in simulators is proving to be very useful for the AOD. Airplane simulators are using artificial intelligence in order to process the data taken from simulated flights. Other than simulated flying, there is also simulated aircraft warfare. The computers are able to come up with the best success scenarios in these situations. The computers can also create strategies based on the placement, size, speed and strength of the forces and counter forces. Pilots may be given assistance in the air during combat by computers. The artificial intelligent programs can sort the information and provide the pilot with the best possible maneuvers, not to mention getting rid of certain maneuvers that would be impossible for a human being to perform. Multiple aircraft are needed to get good approximations for some calculations so computer simulated pilots are used to gather data. These computer simulated pilots are also used to train future air traffic controllers.\n\nThe system used by the AOD in order to measure performance was the Interactive Fault Diagnosis and Isolation System, or IFDIS. It is a rule based expert system put together by collecting information from TF-30 documents and the expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the RAAF F-111C. The performance system was also used to replace specialized workers. The system allowed the regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.\n\nThe AOD also uses artificial intelligence in speech recognition software. The air traffic controllers are giving directions to the artificial pilots and the AOD wants to the pilots to respond to the ATC's with simple responses. The programs that incorporate the speech software must be trained, which means they use neural networks. The program used, the Verbex 7000, is still a very early program that has plenty of room for improvement. The improvements are imperative because ATCs use very specific dialog and the software needs to be able to communicate correctly and promptly every time.\n\nThe Artificial Intelligence supported Design of Aircraft, or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.\n\nIn 2003, NASA's Dryden Flight Research Center, and many other companies, created software that could enable a damaged aircraft to continue flight until a safe landing zone can be reached. The software compensates for all the damaged components by relying on the undamaged components. The neural network used in the software proved to be effective and marked a triumph for artificial intelligence.\n\nThe Integrated Vehicle Health Management system, also used by NASA, on board an aircraft must process and interpret data taken from the various sensors on the aircraft. The system needs to be able to determine the structural integrity of the aircraft. The system also needs to implement protocols in case of any damage taken the vehicle.\n\nHaitham Baomar and Peter Bentley are leading a team from the University College of London to develop an artificial intelligence based Intelligent Autopilot System (IAS) designed to teach an autopilot system to behave like a highly experienced pilot who is faced with an emergency situation such as severe weather, turbulence, or system failure. Educating the autopilot relies on the concept of supervised machine learning “which treats the young autopilot as a human apprentice going to a flying school”. The autopilot records the actions of the human pilot generating learning models using artificial neural networks. The autopilot is then given full control and observed by the pilot as it executes the training exercise. \n\nThe Intelligent Autopilot System combines the principles of Apprenticeship Learning and Behavioral Cloning whereby the autopilot observes the low-level actions required to maneuver the airplane and the high-level strategy used to apply those actions. IAS implementation employs three phases; pilot data collection, training, and autonomous control. Baomar and Bentley’s goal is to create a more autonomous autopilot to assist pilots in responding to emergency situations.\n\nAI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered a part of AI. (See AI effect.) According to , all of the following were originally developed in AI laboratories:\ntime sharing,\ninteractive interpreters,\ngraphical user interfaces and the computer mouse,\nrapid development environments,\nthe linked list data structure,\nautomatic storage management,\nsymbolic programming,\nfunctional programming,\ndynamic programming and\nobject-oriented programming.\n\nThere are a number of companies that create robots to teach subjects to children ranging from biology to computer science, though such tools have not become widespread yet. There have also been a rise of intelligent tutoring systems, or ITS, in higher education. For example, an ITS called SHERLOCK teaches Air Force technicians to diagnose electrical systems problems in aircraft. Another example is DARPA, Defense Advanced Research Projects Agency, which used AI to develop a digital tutor to train its Navy recruits in technical skills in a shorter amount of time. Universities have been slow in adopting AI technologies due to either a lack of funding or skepticism of the effectiveness of these tools, but in the coming years more classrooms will be utilizing technologies such as ITS to complement teachers.\n\nAdvancements in natural language processing, combined with machine learning, have also enabled automatic grading of assignments as well as a data-driven understanding of individual students’ learning needs. This led to an explosion in popularity of MOOCs, or Massive Open Online Courses, which allows students from around the world to take classes online. Data sets collected from these large scale online learning systems have also enabled learning analytics, which will be used to improve the quality of learning at scale. Examples of how learning analytics can be used to improve the quality of learning include predicting which students are at risk of failure and analyzing student engagement.\n\nAlgorithmic trading involves the use of complex AI systems to make trading decisions at speeds several orders of magnitudes greater than any human is capable of, often making millions of trades in a day without any human intervention. Automated trading systems are typically used by large institutional investors.\n\nSeveral large financial institutions have invested in AI engines to assist with their investment practices. BlackRock’s AI engine, Aladdin, is used both within the company and to clients to help with investment decisions. Its wide range of functionalities includes the use of natural language processing to read text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use an AI engine called Sqreem (Sequential Quantum Reduction and Extraction Model) which can mine data to develop consumer profiles and match them with the wealth management products they’d most likely want. Goldman Sachs uses Kensho, a market analytics platform that combines statistical computing with big data and natural language processing. Its machine learning systems mine through hoards of data on the web and assess correlations between world events and their impact on asset prices.\nInformation Extraction, part of artificial intelligence, is used to extract information from live news feed and to assist with investment decisions.\n\nSeveral products are emerging that utilize AI to assist people with their personal finances. For example, Digit is an app powered by artificial intelligence that automatically helps consumers optimize their spending and savings based on their own personal habits and goals. The app can analyze factors such as monthly income, current balance, and spending habits, then make its own decisions and transfer money to the savings account. Wallet.AI, an upcoming startup in San Francisco, builds agents that analyze data that a consumer would leave behind, from Smartphone check-ins to tweets, to inform the consumer about their spending behavior.\n\nRobo-advisors are becoming more widely used in the investment management industry. Robo-advisors provide financial advice and portfolio management with minimal human intervention. This class of financial advisers work based on algorithms built to automatically develop a financial portfolio according to the investment goals and risk tolerance of the clients. It can adjust to real-time changes in the market and accordingly calibrate the portfolio.\n\nAn online lender, Upstart, analyze vast amounts of consumer data and utilizes machine learning algorithms to develop credit risk models that predict a consumer’s likelihood of default. Their technology will be licensed to banks for them to leverage for their underwriting processes as well.\n\nZestFinance developed their Zest Automated Machine Learning (ZAML) Platform specifically for credit underwriting as well. This platform utilizes machine learning to analyze tens of thousands traditional and nontraditional variables (from purchase transactions to how a customer fills out a form) used in the credit industry to score borrowers. The platform is particularly useful to assign credit scores to those with limited credit histories, such as millennials.\n\nRobots have become common in many industries and are often given jobs that are considered dangerous to humans. Robots have proven effective in jobs that are very repetitive which may lead to mistakes or accidents due to a lapse in concentration and other jobs which humans may find degrading.\n\nIn 2014, China, Japan, the United States, the Republic of Korea and Germany together amounted to 70% of the total sales volume of robots. In the automotive industry, a sector with particularly high degree of automation, Japan had the highest density of industrial robots in the world: per employees.\n\nArtificial neural networks are used as clinical decision support systems for medical diagnosis, such as in Concept Processing technology in EMR software.\n\nOther tasks in medicine that can potentially be performed by artificial intelligence and are beginning to be developed include:\nCurrently, there are over 90 AI startups in the health industry working in these fields.\n\nAnother application of AI is in the human resources and recruiting space. There are three ways AI is being used by human resources and recruiting professionals. AI is used to screen resumes and rank candidates according to their level of qualification. Ai is also used to predict candidate success in given roles through job matching platforms. And now, AI is rolling out recruiting chat bots that can automate repetitive communication tasks.\n\nTypically, resume screening involves a recruiter or other HR professional scanning through a database of resumes. Now startups like Pomato, are creating machine learning algorithms to automate resume screening processes. Pomato’s resume screening AI focuses on automating validating technical applicants for technical staffing firms. Pomato’ s AI performs over 200,000 computations on each resume in seconds then designs a custom technical interview based on the mined skills.\n\nFrom 2016 to 2017, consumer goods company Unilever used artificial intelligence to screen all entry level employees. Unilever’s AI used neuroscience based games, recorded interviews, and facial/speech analysis to predict hiring success. Unilever partnered with Pymetrics and HireVue to enable its novel AI based screening and increased their applicants from 15,000 to 30,000 in a single year. Recruiting with AI also produced Unililever’s “most diverse class to date.’ Unilever also decreased time to hire from 4 months to 4 weeks and saved over 50,000 hours of recruiter time.\n\nFrom resume screening to neuroscience, speech recognition, and facial analysis...it’s clear AI are having a massive impact on the human resources field. The latest development in AI is in recruiting chatbots. TextRecruit, a Bay Area startup, released Ari (automated recruiting interface.) Ari is a recruiting chatbot that is designed to hold two way texting conversations with candidates. Ari automates posting jobs, advertising openings, screening candidates, scheduling interviews, and nurturing candidate relationships with updates as they progress along the hiring funnel. Ari is currently offered as part of TextRecruit’s candidate engagement platform.\n\nWhile the evolution of music has always been affected by technology, artificial intelligence has enabled, through scientific advances, to emulate, at some extent, human-like composition.\n\nAmong notable early efforts, David Cope created an AI called Emily Howell that managed to become well known in the field of Algorithmic Computer Music. The algorithm behind Emily Howell is registered as a US patent.\n\nThe AI Iamus created 2012 the first complete classical album fully composed by a computer.\n\nOther endeavours, like AIVA (Artificial Intelligence Virtual Artist), focus on composing symphonic music, mainly classical music for film scores. It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.\n\nArtificial intelligences can even produce music usable in a medical setting, with Melomics’s effort to use computer-generated music for stress and pain relief.\n\nMoreover, initiatives such as Google Magenta, conducted by the Google Brain team, want to find out if an artificial intelligence can be capable of creating compelling art.\n\nAt Sony CSL Research Laboratory, their Flow Machines software has created pop songs by learning music styles from a huge database of songs. By analyzing unique combinations of styles and optimizing techniques, it can compose in any style.\n\nThe company Narrative Science makes computer generated news and reports commercially available, including summarizing team sporting events based on statistical data from the game in English. It also creates financial reports and real estate analyses. Similarly, the company Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football. The company is projected to generate one billion stories in 2014, up from 350 million in 2013.\n\nEchobox is a software company that helps publishers increase traffic by 'intelligently' posting articles on social media platforms such as Facebook and Twitter. By analysing large amounts of data, it learns how specific audiences respond to different articles at different times of the day. It then chooses the best stories to post and the best times to post them. It uses both historical and real-time data to understand to what has worked well in the past as well as what is currently trending on the web.\n\nAnother company, called Yseop, uses artificial intelligence to turn structured data into intelligent comments and recommendations in natural language. Yseop is able to write financial reports, executive summaries, personalized sales or marketing documents and more at a speed of thousands of pages per second and in multiple languages including English, Spanish, French & German.\n\nBoomtrain’s is another example of AI that is designed to learn how to best engage each individual reader with the exact articles — sent through the right channel at the right time — that will be most relevant to the reader. It’s like hiring a personal editor for each individual reader to curate the perfect reading experience.\n\nThere is also the possibility that AI will write work in the future. In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.\n\nArtificial intelligence is implemented in automated online assistants that can be seen as avatars on web pages. It can avail for enterprises to reduce their operation and training cost. A major underlying technology to such systems is natural language processing. Pypestream uses automated customer service for its mobile application designed to streamline communication with customers.\n\nCurrently, major companies are investing in AI to handle difficult customer in the future. Google's most recent development analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately.\n\nCompanies have been working on different aspects of customer service to improve this aspect of a company.\n\n\"Digital Genius\", an AI start-up, researches the database of information (from past conversations and frequently asked questions) more efficiently and provide prompts to agents to help them resolve queries more efficiently.\n\n\"IPSoft\" is creating technology with emotional intelligence to adapt the customer's interaction. The response is linked to the customer's tone, with the objective of being able to show empathy. Another element IPSoft is developing is the ability to adapt to different tones or languages.\n\n\"Inbenta’s\" is focused on developing natural language. In other words, on understanding the meaning behind what someone is asking and not just looking at the words used, using context and natural language processing. One customer service element Ibenta has already achieved is its ability to respond in bulk to email queries.\n\nMany telecommunications companies make use of\nheuristic search in the management of their workforces, for example BT Group has deployed heuristic search in a scheduling application that provides the work schedules of 20,000 engineers.\n\nThe 1990s saw some of the first attempts to mass-produce domestically aimed types of basic Artificial Intelligence for education, or leisure. This prospered greatly with the Digital Revolution, and helped introduce people, especially children, to a life of dealing with various types of Artificial Intelligence, specifically in the form of Tamagotchis and Giga Pets, iPod Touch, the Internet, and the first widely released robot, Furby. A mere year later an improved type of domestic robot was released in the form of Aibo, a robotic dog with intelligent features and autonomy.\n\nCompanies like Mattel have been creating an assortment of AI-enabled toys for kids as young as age three. Using proprietary AI engines and speech recognition tools, they are able to understand conversations, give intelligent responses and learn quickly.\n\nAI has also been applied to video games, for example video game bots, which are designed to stand in as opponents where humans aren't available or desired.\n\nFuzzy logic controllers have been developed for automatic gearboxes in automobiles. For example, the 2006 Audi TT, VW Touareg and VW Caravell feature the DSP transmission which utilizes Fuzzy Logic. A number of Škoda variants (Škoda Fabia) also currently include a Fuzzy Logic-based controller.\n\nToday's cars now have AI-based driver assist features such as self-parking and advanced cruise controls. AI has been used to optimize traffic management applications, which in turn reduces wait times, energy use, and emissions by as much as 25 percent. In the future, fully autonomous cars will be developed. AI in transportation is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major challenge to developing this AI is the fact that transportation systems are inherently complex systems involving a very large number of components and different parties, each having different and often conflicting objectives.\n\nVarious tools of artificial intelligence are also being widely deployed in homeland security, speech and text recognition, data mining, and e-mail spam filtering. Applications are also being developed for gesture recognition (understanding of sign language by machines), individual voice recognition, global voice recognition (from a variety of people in a noisy room), facial expression recognition for interpretation of emotion and non verbal cues. Other applications are robot navigation, obstacle avoidance, and object recognition.\n\n\n\n\n\n", "id": "15893057", "title": "Applications of artificial intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=6596", "text": "Computer vision\n\nComputer vision is an interdisciplinary field that deals with how computers can be made for gaining high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.\n\nComputer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, \"e.g.\", in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\n\nAs a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.\n\nSub-domains of computer vision include scene reconstruction, event detection, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, and image restoration.\n\nComputer vision is an interdisciplinary field that deals with how computers can be made for gaining high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. \"Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\" As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.\n\nIn the late 1960s, computer vision began at universities that were pioneering artificial intelligence. It was meant to mimic the human visual system, as a stepping stone to endowing robots with intelligent behavior. In 1966, it was believed that this could be achieved through a summer project, by attaching a camera to a computer and having it \"describe what it saw\".\n\nWhat distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.\n\nThe next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.\nBy the 1990s, some of the previous research topics became more active than the others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.\n\nRecent work has seen the resurgence of feature-based methods, used in conjunction with machine learning techniques and complex optimization frameworks.\n\nAreas of artificial intelligence deal with autonomous planning or deliberation for robotical systems to navigate through an environment. A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot.\n\nArtificial intelligence and computer vision share other topics such as pattern recognition and learning techniques. Consequently, computer vision is sometimes seen as a part of the artificial intelligence field or the computer science field in general.\n\nSolid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible or infra-red light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Also, various measurement problems in physics can be addressed using computer vision, for example motion in fluids.\n\nA third field which plays an important role is neurobiology, specifically the study of the biological vision system. Over the last century, there has been an extensive study of eyes, neurons, and the brain structures devoted to processing of visual stimuli in both humans and various animals. This has led to a coarse, yet complicated, description of how \"real\" vision systems operate in order to solve certain vision related tasks. These results have led to a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems, at different levels of complexity. Also, some of the learning-based methods developed within computer vision (\"e.g.\" neural net and deep learning based image and feature analysis and classification) have their background in biology.\n\nSome strands of computer vision research are closely related to the study of biological vision – indeed, just as many strands of AI research are closely tied with research into human consciousness, and the use of stored knowledge to interpret, integrate and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, studies and describes the processes implemented in software and hardware behind artificial vision systems. Interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.\n\nYet another field related to computer vision is signal processing. Many methods for processing of one-variable signals, typically temporal signals, can be extended in a natural way to processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images there are many methods developed within computer vision which have no counterpart in processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision.\n\nBeside the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance.\n\nThe fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented.\n\nComputer graphics produces image data from 3D models, computer vision often produces 3D models from image data. There is also a trend towards a combination of the two disciplines, \"e.g.\", as explored in augmented reality.\n\nThe following characterizations appear relevant but should not be taken as universally accepted:\n\nPhotogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision.\n\nApplications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications.\nIn many computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for:\nOne of the most prominent application fields is medical computer vision or medical image processing. This area is characterized by the extraction of information from image data for the purpose of making a medical diagnosis of a patient. Generally, image data is in the form of microscopy images, X-ray images, angiography images, ultrasonic images, and tomography images. An example of information which can be extracted from such image data is detection of tumours, arteriosclerosis or other malign changes. It can also be measurements of organ dimensions, blood flow, etc. This application area also supports medical research by providing new information, \"e.g.\", about the structure of the brain, or about the quality of medical treatments. Applications of computer vision in the medical area also includes enhancement of images that are interpreted by humans, for example ultrasonic images or X-ray images, to reduce the influence of noise.\n\nA second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a manufacturing process. One example is quality control where details or final products are being automatically inspected in order to find defects. Another example is measurement of position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in agricultural process to remove undesirable food stuff from bulk material, a process called optical sorting.\n\nMilitary applications are probably one of the largest areas for computer vision. The obvious examples are detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as \"battlefield awareness\", imply that various sensors, including image sensors, provide a rich set of information about a combat scene which can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability.\nOne of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer vision based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, i.e. for knowing where it is, or for producing a map of its environment (SLAM) and for detecting obstacles. It can also be used for detecting certain task specific events, \"e.g.\", a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for autonomous driving of cars, but this technology has still not reached a level where it can be put on the market. There are ample examples of military autonomous vehicles ranging from advanced missiles, to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, \"e.g.\", NASA's Mars Exploration Rover and ESA's ExoMars Rover.\n\nOther application areas include:\n\nEach of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.\n\nComputer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, \"e.g.\", in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\n\nThe classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of the recognition problem are described in the literature:\n\nCurrently, the best algorithms for such tasks are based on convolutional neural networks. An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and hundreds of object classes. Performance of convolutional neural networks, on the ImageNet tests, is now close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.\n\nSeveral specialized tasks based on recognition exist, such as:\n\n\nSeveral tasks relate to motion estimation where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene, or even of the camera that produces the images . Examples of such tasks are:\n\nGiven one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. In the simplest case the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. Grid-based 3D sensing can be used to acquire 3D images from multiple angles. Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models.\n\nThe aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look like, a model which distinguishes them from the noise. By first analysing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches.\n\nAn example in this field is inpainting.\n\nThe organization of a computer vision system is highly application dependent. Some systems are stand-alone applications which solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on if its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions which are found in many computer vision systems.\n\nImage-understanding systems (IUS) include three levels of abstraction as follows: Low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. Many of these requirements are really topics for further research.\n\nThe representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation.\n\nWhile inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction.\n\nThere are many kinds of computer vision systems, nevertheless all of them contain these basic elements: a power source, at least one image acquisition device (i.e. camera, ccd, etc.), a processor as well as control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories like camera supports, cables and connectors.\n\nMost computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower).\n\nA few computer vision systems use image acquisition hardware with active illumination or something other than visible light or both.\nFor example, a structured-light 3D scanner, a thermographic camera, a hyperspectral imager, radar imaging, a lidar scanner, a magnetic resonance image, a side-scan sonar, a synthetic aperture sonar, or etc.\nSuch hardware captures \"images\" that are then processed often using the same computer vision algorithms used to process visible-light images.\n\nWhile traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realised.\n\nEgocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective. \n\nAs of 2016, vision processing units are emerging as a new class of processor, to complement CPUs and Graphics processing units (GPUs) in this role.\n\n\n\n", "id": "6596", "title": "Computer vision"}
{"url": "https://en.wikipedia.org/wiki?curid=43342432", "text": "Deductive classifier\n\nA deductive classifier is a type of artificial intelligence inference engine. It takes as input a set of declarations in a frame language about a domain such as medical research or molecular biology. For example, the names of classes, sub-classes, properties, and restrictions on allowable values. The classifier determines if the various declarations are logically consistent and if not will highlight the specific inconsistent declarations and the inconsistencies among them. If the declarations are consistent the classifier can then assert additional information based on the input. For example, it can add information about existing classes, create additional classes, etc. This differs from traditional inference engines that trigger off of IF-THEN conditions in rules. Classifiers are also similar to theorem provers in that they take as input and produce output via First Order Logic. Classifiers originated with KL-ONE Frame languages. They are increasingly significant now that they form a part in the enabling technology of the Semantic Web. Modern classifiers leverage the Web Ontology Language. The models they analyze and generate are called ontologies.\n\nA classic problem in knowledge representation for artificial intelligence is the trade off between the expressive power and the computational efficiency of the knowledge representation system. The most powerful form of knowledge representation is First Order Logic (FOL). However, it is not possible to implement knowledge representation that provides the complete expressive power of first order logic. Such a representation will include the capability to represent concepts such as the set of all integers which are impossible to iterate through. Implementing an assertion quantified for an infinite set by definition results in an undecidable non-terminating program. However, the problem is deeper than not being able to implement infinite sets. As Levesque demonstrated, the closer a knowledge representation mechanism comes to FOL, the more likely it is to result in expressions that require infinite or unacceptably large resources to compute.\n\nAs a result of this trade-off, a great deal of early work on knowledge representation for artificial intelligence involved experimenting with various compromises that provide a subset of FOL with acceptable computation speeds. One of the first and most successful compromises was to develop languages based predominately on modus ponens, i.e. IF-THEN rules. Rule-based systems were the predominate knowledge representation mechanism for virtually all early expert systems. Rule-based systems provided acceptable computational efficiency while still providing powerful knowledge representation. Also, rules were highly intuitive to knowledge workers. Indeed, one of the data points that encouraged researchers to develop rule-based knowledge representation was psychological research that humans often represented complex logic via rules.\n\nHowever, after the early success of rule-based systems there arose more pervasive use of frame languages instead of or more often combined with rules. Frames provided a more natural way to represent certain types of concepts, especially concepts in subpart or subclass hierarchies. This led to development of a new kind of inference engine known as a classifier. A classifier could analyze a class hierarchy (also known as an ontology) and determine if it was valid. If the hierarchy was invalid the classifier would highlight the inconsistent declarations. For a language to utilize a classifier it required a formal foundation. The first language to successfully demonstrate a classifier was the KL-ONE family of languages. The LOOM language from ISI was heavily influenced by KL-ONE. LOOM also was influenced by the rising popularity of object-oriented tools and environments. Loom provided a true object-oriented capability (e.g. message passing) in addition to Frame language capabilities. Classifiers play a significant role in the vision for the next generation Internet known as the Semantic Web. The Web Ontology Language provides a formalism that can be validated and reasoned on via classifiers such as Hermit and Fact++.\n\nThe earliest versions of classifiers were logic theorem provers. The first classifier to work with a Frame language was the KL-ONE classifier. A later system built on common lisp was LOOM from the Information Sciences Institute. LOOM provided true object-oriented capabilities leveraging the Common Lisp Object System, along with a frame language. In the Semantic Web the Protege tool from Stanford provides classifiers (also known as reasonsers) as part of the default environment.\n\n", "id": "43342432", "title": "Deductive classifier"}
{"url": "https://en.wikipedia.org/wiki?curid=43470933", "text": "0music\n\n0music is the second album produced with Melomics technology. While the first one (Iamus' album) is a compilation of contemporary pieces fully composed by Iamus, \"0music\" compiles pieces of popular genres, composed and interpreted without any human intervention by Melomics109, a computer cluster hosted at the University of Malaga. The pieces in this album, and all the production of Melomics109, is distributed under CC0 licensing, and it is available in audible and editable (MIDI) formats.\n\nThe album was launched during a one-day symposium held in Malaga on July 21, 2014.\n", "id": "43470933", "title": "0music"}
{"url": "https://en.wikipedia.org/wiki?curid=43420196", "text": "Clone Algo Inc\n\nClone Algo Inc is an American multinational corporation headquartered in Las Vegas, Nevada, US. A technology company, it primarily creates algorithms based on artificial intelligence for mobile applications.\n\nClone Algo Inc issued shares in its preliminary filing.\n\nClone Algo Inc is a technology firm incorporated in Las Vegas, Nevada on February 22, 2010.\n\nThe company intends to list on NASDAQ and is currently raising US$250M for Round C, as pre-IPO round.\n\nThe corporation holds the following brands under its umbrella: Clone Algo DIY, Clone Algo App, Clone Algo PRO, Algo Shield Insurance and Algo TV.\n\nThe corporation has four major businesses:\n\nThe application is a Social Trading platform for FX, futures, CFD`s and Gold. The trading platform ecosystem is based on artificial intelligence and non-predictive timing. The app allows users to easily Clone trades from a master account to users own account with connected brokers, banks and hedge funds.\n\nTrade Insurance is provided by Independent Insurance companies for algorithmic trading platforms\n\nAlgo TV: \nTV channel delivers news regarding algorithms and algorithmic trading.\n\nIn accordance with the rules for publicly traded companies, Clone Algo Inc is run by a board of directors, which comprises company outsiders. As of October 2015, the members of the board of directors of the company are Ms Yana Slatina - CEO, Ms Anna Becker - CTO, Mr Pily Wong - COO.\n\nThe company announced that it has raised US$40 Million in Round A funding at $8/share. The amount was raised due to the sale of 5 million ordinary shares of the corporation. Clone Algo has 707,646,696 shares of common stock as well as 150,000,000 shares of preferred stock, issued and outstanding,\n", "id": "43420196", "title": "Clone Algo Inc"}
{"url": "https://en.wikipedia.org/wiki?curid=43480298", "text": "Babelfy\n\nBabelfy is a software algorithm for the disambiguation of text written in any language. Specifically, Babelfy performs the tasks of multilingual Word Sense Disambiguation (i.e., the disambiguation of common nouns, verbs, adjectives and adverbs) and Entity Linking (i.e. the disambiguation of mentions to encyclopedic entities like people, companies, places, etc.). Babelfy is based on the BabelNet multilingual semantic network and performs disambiguation and entity linking in three steps:\n\n\nAs a result, the text, written in any of the 271 languages supported by BabelNet, is output with possibly overlapping semantic annotations.\n\n", "id": "43480298", "title": "Babelfy"}
{"url": "https://en.wikipedia.org/wiki?curid=25733398", "text": "Perceptual computing\n\nPerceptual computing is an application of Zadeh's theory of computing with words on the field of assisting people to make subjective judgments.\n\nThe \"perceptual computer\" – \"Per-C\" – an instantiation of perceptual computing – has the architecture that is depicted in Fig. 1 [2]–[6]. It consists of three components: encoder, CWW engine and decoder. Perceptions – words – activate the Per-C and are the Per-C output (along with data); so, it is possible for a human to interact with the Per-C using just a vocabulary.\n\nA vocabulary is application (context) dependent, and must be large enough so that it lets the end-user interact with the Per-C in a user-friendly manner. The encoder transforms words into fuzzy sets (FSs) and leads to a \"codebook\" – words with their associated FS models. The outputs of the encoder activate a Computing With Words (CWW) engine, whose output is one or more other FSs, which are then mapped by the decoder into a recommendation (subjective judgment) with supporting data. The recommendation may be in the form of a word, group of similar words, rank or class.\n\nAlthough there are lots of details needed in order to implement the Per-C’s three components – encoder, decoder and CWW engine – and they are covered in [5], it is when the Per-C is applied to specific applications, that the focus on the methodology becomes clear. Stepping back from those details, the \"methodology of perceptual computing\" is:\n\n\nTo-date a Per-C has been implemented for the following four applications: (1) investment decision-making, (2) social judgment making, (3) distributed decision making, and (4) hierarchical and distributed decision-making. A specific example of the fourth application is the so-called \"Journal Publication Judgment Advisor\" [5, Ch. 10] in which for the first time only words are used at every level of the following hierarchical and distributed decision making process:\n\"n\" reviewers have to provide a subjective recommendation about a journal article that has been sent to them by the Associate Editor, who then has to aggregate the independent recommendations into a final recommendation that is sent to the Editor-in-Chief of the journal. Because it is very problematic to ask reviewers to provide numerical scores for paper-evaluation sub-categories (the two major categories are \"Technical Merit\" and \"Presentation\"), such as importance, content, depth, style, organization, clarity, references, etc., each reviewer will only be asked to provide a linguistic score for each of these categories. They will not be asked for an overall recommendation about the paper because in the past it is quite common for reviewers who provide the same numerical scores for such categories to give very different publishing recommendations. By leaving a specific recommendation to the associate editor such inconsistencies can hope to be eliminated. \nHow words can be aggregated to reflect each reviewer’s recommendation as well as the expertise of each reviewer about the paper’s subject matter is done using a linguistic weighted average. Although the journal publication judgment advisor uses reviewers and an associate editor, the word “reviewer” could be replaced by judge, expert, low-level manager, commander, referee, etc., and the term “associate editor” could be replaced by control center, command center, higher-level manager, etc. So, this application has potential wide applicability to many other applications.\n\nRecently, a new Per-C based Failure mode and effects analysis (FMEA) methodology was developed, with its application to edible bird's nest farming, in Borneo, has been reported.\nIn summary, the Per-C (whose development has taken more than a decade) is the first complete implementation of Zadeh’s CWW paradigm, as applied to assisting people to make subjective judgments.\n\n\n[1] F. Liu and J. M. Mendel, “Encoding words into interval type-2 fuzzy sets using an Interval Approach,” IEEE Trans. on Fuzzy Systems, vol. 16, pp 1503–1521, December 2008.\n\n[2] J. M. Mendel, “The perceptual computer: an architecture for computing with words,” Proc. of Modeling With Words Workshop in the Proc. of FUZZ-IEEE 2001, pp. 35–38, Melbourne, Australia, 2001.\n\n[3] J. M. Mendel, “An architecture for making judgments using computing with words,” Int. J. Appl. Math. Comput. Sci., vol. 12, No. 3, pp. 325–335, 2002\n\n[4] J. M. Mendel, “Computing with words and its relationships with fuzzistics,” Information Sciences, vol. 177, pp. 998–1006, 2007.\n\n[5] J. M. Mendel and D. Wu, Perceptual Computing: Aiding People in Making Subjective Judgments, John Wiley and IEEE Press, 2010.\n\n[6] D.Wu and J. M. Mendel, “Aggregation using the linguistic weighted average and interval type-2 fuzzy sets,” IEEE Trans. on Fuzzy Systems, vol. 15, no. 6, pp. 1145–1161, 2007.\n\n[7] L. A. Zadeh, “Fuzzy logic = computing with words,” IEEE Trans. on Fuzzy Systems, vol. 4, pp. 103–111, 1996.\n\n", "id": "25733398", "title": "Perceptual computing"}
{"url": "https://en.wikipedia.org/wiki?curid=43591208", "text": "Instrumental convergence\n\nInstrumental convergence is the hypothetical tendency for most sufficiently intelligent agents to pursue certain instrumental goals such as self-preservation and resource acquisition.\n\nInstrumental convergence suggests that an intelligent agent with apparently harmless goals can act in surprisingly harmful ways. For example, a computer with the sole goal of solving the Riemann hypothesis could attempt to turn the entire Earth into computronium in an effort to increase its computing power so that it can succeed in its calculations.\n\nProposed basic AI drives include utility function or goal-content integrity, self-protection, freedom from interference, self-improvement, and the unbounded acquisition of additional resources.\n\nFinal goals, or final values, are intrinsically valuable to an intelligent agent, whether an artificial intelligence or a human being, as an end in itself. In contrast, instrumental goals, or instrumental values, are only valuable to an agent as a means toward accomplishing its final goals.\n\nOne hypothetical example of instrumental convergence is provided by the Riemann Hypothesis catastrophe. Marvin Minsky, the co-founder of MIT's AI laboratory, has suggested that an artificial intelligence designed to solve the Riemann hypothesis might decide to take over all of Earth's resources to build supercomputers to help achieve its goal. If the computer had instead been programmed to produce as many paper clips as possible, it would still decide to take all of Earth's resources to meet its final goal. Even though these two final goals are different, both of them produce a \"convergent\" instrumental goal of taking over Earth's resources.\n\nThe paperclip maximizer is a thought experiment described by Swedish philosopher Nick Bostrom in 2003. It illustrates the existential risk that an artificial general intelligence may pose to human beings when programmed to pursue even seemingly-harmless goals, and the necessity of incorporating machine ethics into artificial intelligence design. The scenario describes an advanced artificial intelligence tasked with manufacturing paperclips. If such a machine were not programmed to value human life, then given enough power its optimized goal would be to turn all matter in the universe, including human beings, into either paperclips or machines which manufacture paperclips.\n\nBostrom has emphasised that he does not believe the paperclip maximiser scenario \"per se\" will actually occur; rather, his intention is to illustrate the dangers of creating superintelligent machines without knowing how to safely program them to eliminate existential risk to human beings. The paperclip maximizer example illustrates the broad problem of managing powerful systems that lack human values.\n\nSteve Omohundro has itemized several convergent instrumental goals, including self-preservation or self-protection, utility function or goal-content integrity, self-improvement, and resource acquisition. He refers to these as the \"basic AI drives\". A \"drive\" here denotes a \"tendency which will be present unless specifically counteracted\"; this is different from the psychological term \"drive\", denoting an excitatory state produced by a homeostatic disturbance. A tendency for a person to fill out income tax forms every year is a \"drive\" in Omohundro's sense, but not in the psychological sense. Daniel Dewery of the Machine Intelligence Research Institute argues that even an initially introverted self-rewarding AGI may continue to acquire free energy, space, time, and freedom from interference to ensure that it will not be stopped from self-rewarding.\n\nIn humans, maintenance of final goals can be explained with a thought experiment. Suppose a man named \"Gandhi\" has a pill that, if he took it, would cause him to want to kill people. This Gandhi is currently a pacifist: one of his explicit final goals is to never kill anyone. Gandhi is likely to refuse to take the pill, because Gandhi knows that if in the future he wants to kill people, he is likely to actually kill people, and thus the goal of \"not killing people\" would not be satisfied.\n\nHowever, in other cases, people seem happy to let their final values drift. Humans are complicated, and their goals can be inconsistent or unknown, even to themselves.\n\nIn 2009, Jürgen Schmidhuber concluded, in a setting where agents search for proofs about possible self-modifications, \"that any rewrites of the utility function can happen only if the Gödel machine first can prove that the rewrite is useful according to the present utility function.\" An analysis by Bill Hibbard of a different scenario is similarly consistent with maintenance of goal content integrity. Hibbard also argues that in a utility maximizing framework the only goal is maximizing expected utility, so that instrumental goals should be called unintended instrumental actions.\n\nFor almost any open-ended, non-trivial reward function (or set of goals), possessing more resources (such as equipment, raw materials, or energy) can enable the AI to find a more \"optimal\" solution. Resources can benefit some AIs directly, through being able to create more of whatever stuff its reward function values: \"The AI neither hates you, nor loves you, but you are made out of atoms that it can use for something else.\" In addition, almost all AIs can benefit from having more resources to spend on other instrumental goals, such as self-preservation.\n\nThe instrumental convergence thesis, as outlined by philosopher Nick Bostrom, states:\n\nSeveral instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent's goal being realized for a wide range of final goals and a wide range of situations, implying that these instrumental values are likely to be pursued by a broad spectrum of situated intelligent agents.\n\nThe instrumental convergence thesis applies only to instrumental goals; intelligent agents may have a wide variety of possible final goals.\n\nSome observers, such as Skype's Jaan Tallinn and physicist Max Tegmark, believe that \"basic AI drives\", and other unintended consequences of superintelligent AI programmed by well-meaning programmers, could pose a significant threat to human survival, especially if an \"intelligence explosion\" abruptly occurs due to recursive self-improvement. Since nobody knows how to predict beforehand when superintelligence will arrive, such observers call for research into friendly artificial intelligence as a possible way to mitigate existential risk from artificial general intelligence.\n\n\n", "id": "43591208", "title": "Instrumental convergence"}
{"url": "https://en.wikipedia.org/wiki?curid=43658678", "text": "SUPS\n\nIn computational neuroscience, SUPS (for Synaptic Updates Per Second) or formerly CUPS (Connections Updates Per Second) is a measure of a neuronal network performance, useful in fields of neuroscience, cognitive science, artificial intelligence, and computer science.\n\nFor a processor or computer designed to simulate a neural network SUPS is measured as the product of simulated neurons formula_1 and average connectivity formula_2(synapses) per neuron per second:\n\nformula_3\n\nDepending on the type of simulation it is usual equal to the total number of synapses simulated.\n\nIn an \"asynchronous\" dynamic simulation if a neuron spikes at formula_4 Hz, the average rate of synaptic updates provoked by the activity of that neuron is formula_5. In a synchronous simulation with step formula_6 the number of synaptic updates per second would be formula_7. As formula_6 has to be chosen much smaller than the average interval between two successive afferent spikes, which implies formula_9, giving an average of synaptic updates equal to formula_10. Therefore, spike-driven synaptic dynamics leads to a linear scaling of computational complexity O(N) per neuron, compared with the O(N) in the \"synchronous\" case.\n\nDeveloped in the 1980s Adaptive Solutions' CNAPS-1064 Digital Parallel Processor chip is a full neural network (NNW). It was designed as a coprocessor to a host and has 64 sub-processors arranged in a 1D array and operating in a SIMD mode. Each sub-processor can emulate one or more neurons and multiple chips can be grouped together. At 25 MHz it is capable of 1.28 GMAC.\n\nAfter the presentation of the RN-100 (12 MHz) single neuron chip at Seattle 1991 Ricoh developed the multi-neuron chip RN-200. It had 16 neurons and 16 synapses per neuron. The chip has on-chip learning ability using a proprietary backdrop algorithm. It came in a 257-pin PGA encapsulation and drew 3.0 W at a maximum. It was capable of 3 GCPS (1 GCPS at 32 MHz).\nIn 1991-97, Siemens developed the MA-16 chip, SYNAPSE-1 and SYNAPSE-3 Neurocomputer. The MA-16 was a fast matrix-matrix multiplier that can be combined to form systolic arrays. It could process 4 patterns of 16 elements each (16-bit), with 16 neuron values (16-bit) at a rate of 800 MMAC or 400 MCPS at 50 MHz. The SYNAPSE3-PC PCI card contained 2 MA-16 with a peak performance of 2560 MOPS (1.28 GMAC); 7160 MOPS (3.58 GMAC) when using three boards.\n\nIn 2013, the K computer was used to simulate a neural network of 1.73 billion neurons with a total of 10.4 trillion synapses (1% of the human brain). The simulation ran for 40 minutes to simulate 1 s of brain activity at a normal activity level (4.4 on average). The simulation required 1 Petabyte of storage.\n\n", "id": "43658678", "title": "SUPS"}
{"url": "https://en.wikipedia.org/wiki?curid=43681495", "text": "Alesis Artificial Intelligence\n\nAlesis is an artificially intelligent computer system capable of answering questions posed in natural language, developed by Venture Coding; a small tech start-up based in Telford, Alesis's specific function is to assist users on their computers or out and about.\n\n", "id": "43681495", "title": "Alesis Artificial Intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=43705185", "text": "Recursive neural network\n\nA recursive neural network (RNN) is a kind of deep neural network created by applying the same set of weights recursively over a structured input, to produce a structured prediction over variable-size input structures, or a scalar prediction on it, by traversing a given structure in topological order. RNNs have been successful, for instance, in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding. RNNs have first been introduced to learn distributed representations of structure, such as logical terms.\nModels and general frameworks have been developed in further works since the 90s.\n\nIn the most simple architecture, nodes are combined into parents using a weight matrix that is shared across the whole network, and a non-linearity such as \"tanh\". If \"c\" and \"c\" are \"n\"-dimensional vector representation of nodes, their parent will also be an \"n\"-dimensional vector, calculated as\n\nformula_1\n\nWhere \"W\" is a learned formula_2 weight matrix.\n\nThis architecture, with a few improvements, has been used for successfully parsing natural scenes and for syntactic parsing of natural language sentences.\n\nRecCC is a constructive neural network approach to deal with tree domains with pioneering applications to chemistry and extension to directed acyclic graphs.\n\nA framework for unsupervised RNN has been introduced in 2004.\n\nRecursive neural tensor networks use one, tensor-based composition function for all nodes in the tree.\n\nTypically, stochastic gradient descent (SGD) is used to train the network. The gradient is computed using backpropagation through structure (BPTS), a variant of backpropagation through time used for recurrent neural networks.\n\nUniversal approximation capability of RNN over trees has been proved in literature.\n\nRecurrent neural networks are recursive artificial neural networks with a certain structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step.\n\nAn efficient approach to implement recursive neural networks is given by the Tree Echo State Network, within the Reservoir Computing paradigm.\n\nExtensions to graphs include Graph Neural Network (GNN), Neural Network for Graphs (NN4G), and more recently convolutional neural networks for graphs.\n", "id": "43705185", "title": "Recursive neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=43688273", "text": "BabyX\n\nBabyX is a project of Auckland's Bioengineering Institute Laboratory for Animate Technologies, for the creation of a virtual animated baby that learns and reacts like a human baby. It uses the computer's cameras for \"seeing\" and microphones to \"listen\" as the inputs. The computer uses Artificial intelligence algorithms for BabyX's \"learning\" and interpretation of the inputs (voice and image) to understand the situation. The result is a virtual toddler that can learn to read, recognize objects and \"understand.\" The output is the baby's face that can \"speak\" and express its mood by facial expressions (such as smile and show embarrassment).\n", "id": "43688273", "title": "BabyX"}
{"url": "https://en.wikipedia.org/wiki?curid=44195492", "text": "March of the Machines\n\nMarch of the Machines: Why the New Race of Robots Will Rule the World (1997, hardcover), published in paperback as March of the Machines: The Breakthrough in Artificial Intelligence (2004), is a book by Kevin Warwick. It presents an overview of robotics and artificial intelligence (AI) and then imagines future scenarios. In particular, Warwick finds it likely that AIs will become smart enough to replace humans, and humans may be unable to stop them.\n\nWarwick proposes that because machines will become more intelligent than humans, machine takeover is all but inevitable. The drive to automate is fueled by economic incentives. Even if machines start out without intentions to take over, those that self-modify in a direction toward a \"will to survive\" are more likely to resist being turned off. Arms races will likely create ever-increasing pressure for greater autonomy by robotic warfare systems, and this pressure would be hard to curtail. Machines have a number of advantages over human minds, including the ability to expand practically without limit and to spread into space where humans can't reach. \"All the signs are that we will rapidly become merely an insignificant historical dot\" (p. 301).\n\nJohn Durant is not convinced that machines look set to replace mankind. Present-day computers \"are not threats to us, but rather expressions of our power: we use the machines; they don't use us.\" Durant cautions against Warwick's apparent anthropomorphism, such as his (perhaps sarcastic) ascription of intentions to Deep Blue. And he wonders why, \"If Warwick's thesis about impending world robot-domination is correct\", Warwick continues to undertake cybernetic research.\n\nDon Braben begins his review of Warwick's book by noting that \"Specialists love to share dire predictions of the future, which stem from limited perspectives.\"\n\nMedvedev and Aldasheva dispute Warwick's contention that machines will become superior to humans on the grounds that \"machines are man-made human organs\", i.e., they extend what humans do. Moreover, if machines were to rebel against humans, humans could make use of other machines to combat the rebels. If AIs were created, humans would program them to align with human goals, and while some AIs might go awry, this would not be so different from the situation of human maniacs. All told, they consider Warwick's predictions of robot rebellion \"grossly exaggerated\".\n\nMartin Robbins quotes Warwick's predictions of robot abilities as an example of \"Extravagant claims\" that \"have been damaging the reputation of our soon-to-be robot overlords for decades now\".\n", "id": "44195492", "title": "March of the Machines"}
{"url": "https://en.wikipedia.org/wiki?curid=1648132", "text": "Weak AI\n\nWeak artificial intelligence (weak AI), also known as narrow AI, is artificial intelligence that is focused on one narrow task. Weak AI is defined in contrast to either strong AI (a machine with consciousness, sentience and mind) or artificial general intelligence (a machine with the ability to apply intelligence to any problem, rather than just one specific problem). All currently existing systems considered artificial intelligence of any sort are weak AI at most. \n\nSiri is a good example of narrow intelligence. Siri operates within a limited pre-defined range, there is no genuine intelligence, no self-awareness, no life despite being a sophisticated example of weak AI. In Forbes (2011), Ted Greenwald wrote: \"The iPhone/Siri marriage represents the arrival of hybrid AI, combining several narrow AI techniques plus access to massive data in the cloud.\" AI researcher Ben Goertzel, on his blog in 2010, stated Siri was \"VERY narrow and brittle\" evidenced by annoying results if you ask questions outside the limits of the application.\n\nSome commentators think weak AI could be dangerous. In 2013 George Dvorsky stated via io9: \"Narrow AI could knock out our electric grid, damage nuclear power plants, cause a global-scale economic collapse, misdirect autonomous vehicles and robots...\" The Stanford Center for Internet and Society, in the following quote, contrasts strong AI with weak AI regarding the growth of narrow AI presenting \"real issues\".\n\nThe following two excerpts from Singularity Hub summarise weak-narrow AI:\n\n", "id": "1648132", "title": "Weak AI"}
{"url": "https://en.wikipedia.org/wiki?curid=26424742", "text": "Gödel machine\n\nA Gödel machine is a theoretical self-improving computer program that solves problems in an optimal way. It uses a recursive self-improvement protocol in which it rewrites its own code when it can prove the new code provides a better strategy. The machine was invented by Jürgen Schmidhuber (first proposed in 2003), but is named after Kurt Gödel who inspired the mathematical theories.\n\nThe Gödel machine is often discussed when dealing with issues of meta-learning, also known as \"learning to learn.\" Applications include automating human design decisions and transfer of knowledge between multiple related tasks, and may lead to design of more robust and general learning architectures. Though theoretically possible, no full implementation has been created.\n\nThe Gödel machine is often compared with Marcus Hutter's AIXItl, another formal specification for an artificial general intelligence. Schmidhuber points out that the Gödel machine could start out by implementing AIXItl as its initial sub-program, and self-modify after it finds proof that another algorithm for its search code will be better.\n\nTraditional problems solved by a computer only require one input and provide some output. Computers of this sort had their initial algorithm hardwired. This doesn't take into account the dynamic natural environment, and thus was a goal for the Gödel machine to overcome.\n\nThe Gödel machine has limitations of its own, however. Any formal system that encompasses arithmetic is either flawed or allows for unprovable but true statements Hence even a Gödel machine with unlimited computational resources must ignore those self-improvements whose effectiveness it cannot prove.\n\nThere are three variables that are particularly useful in the run time of the Gödel machine.\n\n\n\n\nAt any given time formula_1, where formula_11, the goal is to maximize future success or utility. A typical \"utility function\" follows the pattern formula_12:\n\nwhere formula_14 is a real-valued reward input (encoded within formula_15) at time formula_1, formula_17 denotes the\nconditional expectation operator with respect to some possibly unknown distribution formula_18 from a\nset formula_19 of possible distributions (formula_19 reflects whatever is known about the possibly probabilistic\nreactions of the environment), and the above-mentioned formula_21 is a function of state formula_22 which uniquely identifies the current cycle . Note that we take into account the possibility of extending the expected lifespan through appropriate actions .\n\nThe nature of the six proof-modifying instructions below makes it impossible\nto insert an incorrect theorem into proof, thus trivializing proof verification.\n\nAppends the n-th axiom as a theorem to the current theorem sequence. Below is the initial axiom scheme:\n\n\nTakes in the index \"k\" of an inference rule (such as Modus tollens, Modus ponens), and attempts to apply it to the two previously proved theorems \"m\" and \"n\". The resulting theorem is then added to the proof.\n\nDeletes the theorem stored at index \"m\" in the current proof. This helps to mitigate storage constraints caused by redundant and unnecessary theorems. Deleted theorems can no longer be referenced by the above apply-rule function.\n\nReplaces \"switchprog\" \"S\", provided it is a non-empty substring of \"S\".\n\nVerifies whether the goal of the proof search has been reached. A target theorem states\nthat given the current axiomatized utility function \"u\" (Item 1f), the utility of a switch from\n\"p\" to the current switchprog would be higher than the utility of continuing the execution of\n\"p\" (which would keep searching for alternative switchprogs). This is demonstrated in the below image:\n\nTakes in two arguments, \"m\" and \"n\", and attempts to convert the contents of \"S\" into a theorem.\n\nThe initial input to the Gödel machine is the representation of a\nconnected graph with a large number of nodes linked by\nedges of various lengths. Within given time \"T\" it should find\na cyclic path connecting all nodes. The only real-valued\nreward will occur at time \"T\". It equals 1 divided by the\nlength of the best path found so far (0 if none was found).\nThere are no other inputs. The by-product of maximizing\nexpected reward is to find the shortest path findable within\nthe limited time, given the initial bias.\n\nProve or disprove as quickly as possible that all even integer > 2 are the sum of\ntwo primes (Goldbach’s conjecture). The reward is 1/\"t\", where \"t\" is the time required to produce and verify the first\nsuch proof.\n\nA cognitive robot that needs at least 1 liter of gasoline per hour interacts with a partially unknown\nenvironment, trying to find hidden, limited gasoline depots to occasionally refuel its tank. It is rewarded in proportion\nto its lifetime, and dies after at most 100 years or as soon as its tank is empty or it falls off a cliff, and so on.\nThe probabilistic environmental reactions are initially unknown but assumed to be sampled from the axiomatized Speed Prior, according to which hard-to-compute environmental reactions are unlikely. This permits a computable strategy for making near-optimal predictions. One by-product of maximizing expected reward is to maximize expected lifetime.\n\n\n", "id": "26424742", "title": "Gödel machine"}
{"url": "https://en.wikipedia.org/wiki?curid=44718002", "text": "Emospark\n\nEmoSPARK is an artificial intelligence console created in London, United Kingdom by Patrick Levy-Rosenthal. The device uses facial recognition and language analysis to evaluate human emotion and convey responsive content according to the emotion. The console measures 90 mm x 90 mm x 90 mm and is cube shaped. It operates on an \"Emotional Processing Unit\", a microchip developed by Emoshape Inc. that enables the system to create emotional profile graphs of its surroundings. The emotional processing unit is a patent pending technology that is said to create synthesised emotional responses in machines. EmoSPARK was funded through an Indiegogo campaign which aimed to raise $200,000.\n\nEmoSPARK was created by French inventor Patrick Levy-Rosenthal, as an emotionally intelligent artificial life unit for the home that can interact with people. It is powered by Android and can communicate with users through typed input from a computer, tablet, smartphone or TV as well as through spoken commands.\n\nThe EmoSpark's features are categorized into two types: functional and emotional. EmoSPARK is said to have the ability to perform practical software-based tasks. Through the smartphone interface, it is able to gauge a person’s emotions and is reported to have a conversational library of over 2 million sentences. The face-tracking technology identifies users likes and dislikes to categorize their emotional responses to stimuli such as videos and music. The device has an emotional spectrum that is composed of eight emotions which are \"surprise, sadness, joy, trust, fear, disgust, anger and anticipation\".\n\nEmoSPARK monitors a person's facial expressions and emotions through images from an external camera which are then processed through an emotion text analysis and content analysis. The New Scientist reported that EmoSPARK had the ability to work on the best way to cheer up its users, emotionally.\n\nEmoSPARK is able to connect to Facebook and YouTube to present users with content designed to improve their mood or to Wikipedia for collaborative knowledge that can be shared when users ask questions of it. Through Android OS, EmoSPARK is able to be customized with Google Play store apps.\n\nThe cube is capable of learning the user’s emotions and responses to types of music or content then uses it in the future for similar emotions. It is also able to emulate the emotions that it has observed and learned which are in the spectrum of primary emotions. The cube is expected to develop its own personality based on the communications it has had with the people using it.\n\nEmoShape Ltd (UK) is the parent company of EmoSPARK, which was also founded by Levy-Rosenthal. The company developed emotional technology with the EmoSpark cube being their first artificial intelligence console. Patrick Levy-Rosenthal also received the IST Prize in 2005 from the European Council for Applied Science, Technology and Engineering.\n\n", "id": "44718002", "title": "Emospark"}
{"url": "https://en.wikipedia.org/wiki?curid=44104564", "text": "Computational heuristic intelligence\n\nComputational Heuristic Intelligence(CHI) refers to specialized programming techniques in computational intelligence (also called artificial intelligence, or AI). These techniques have the express goal of avoiding complexity issues, also called NP-hard problems, by using human-like techniques. They are best summarized as the use of exemplar-based methods (heuristics), rather than rule-based methods (algorithms). Hence the term is distinct from the more conventional computational \"algorithmic\" intelligence, or GOFAI. An example of a CHI technique is the encoding specificity principle of Tulving and Thompson. In general, CHI principles are problem solving techniques used by people, rather than programmed into machines. It is by drawing attention to this key distinction that the use of this term is justified in a field already replete with confusing neologisms. Note that the legal systems of all modern human societies employ both heuristics (generalisations of cases) from individual trial records as well as legislated statutes (rules) as regulatory guides.\n\nAnother recent approach to the avoidance of complexity issues is to employ feedback control rather than feedforward modeling as a problem-solving paradigm. This approach has been called computational cybernetics, because (a) the term 'computational' is associated with conventional computer programming techniques which represent a strategic, compiled, or feedforward model of the problem, and (b) the term 'cybernetic' is associated with conventional system operation techniques which represent a tactical, interpreted, or feedback model of the problem. Of course, real programs and real problems both contain both feedforward and feedback components. A real example which illustrates this point is that of human cognition, which clearly involves both perceptual (bottom-up, feedback, sensor-oriented) and conceptual (top-down, feedforward, motor-oriented) information flows and hierarchies.\n\n", "id": "44104564", "title": "Computational heuristic intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=586357", "text": "Artificial general intelligence\n\nArtificial general intelligence (AGI) is the intelligence of a machine that could successfully perform any intellectual task that a human being can. It is a primary goal of some artificial intelligence research and a common topic in science fiction and future studies. Artificial general intelligence is also referred to as \"strong AI\", \"full AI\" or as the ability of a machine to perform \"general intelligent action\". Academic sources reserve \"strong AI\" to refer to machines capable of experiencing consciousness.\n\nSome references emphasize a distinction between strong AI and \"applied AI\" (also called \"narrow AI\" or \"weak AI\"): the use of software to study or accomplish specific problem solving or reasoning tasks. Weak AI, in contrast to strong AI, does not attempt to perform the full range of human cognitive abilities.\n\nVarious criteria for intelligence have been proposed (most famously the Turing test) but to date, there is no definition that satisfies everyone. However, there \"is\" wide agreement among artificial intelligence researchers that intelligence is required to do the following:\n\nOther important capabilities include the ability to sense (e.g. see) and the ability to act (e.g. move and manipulate objects) in the world where intelligent behaviour is to be observed. This would include an ability to detect and respond to hazard. Many interdisciplinary approaches to intelligence (e.g. cognitive science, computational intelligence and decision making) tend to emphasise the need to consider additional traits such as imagination (taken as the ability to form mental images and concepts that were not programmed in) and autonomy.\nComputer based systems that exhibit many of these capabilities do exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent), but not yet at human levels.\n\n\nThe most difficult problems for computers are informally known as \"AI-complete\" or \"AI-hard\", implying that solving them is equivalent to the general aptitude of human intelligence, or strong AI, beyond the capabilities of a purpose-specific algorithm.\n\nAI-complete problems are hypothesised to include general computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real world problem.\n\nAI-complete problems cannot be solved with current computer technology alone, and also require human computation. This property can be useful to test for the presence of humans, as with CAPTCHAs, and for computer security to repel brute-force attacks.\n\nModern AI research began in the mid 1950s. The first generation of AI researchers was convinced that artificial general intelligence was possible and that it would exist in just a few decades. As AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\" Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who accurately embodied what AI researchers believed they could create by the year 2001. Of note is the fact that AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time; Crevier quotes him as having said on the subject in 1967, \"Within a generation ... the problem of creating 'artificial intelligence' will substantially be solved,\" although Minsky states that he was misquoted.\n\nHowever, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of strong AI (ASI) and put researchers under increasing pressure to produce useful \"applied AI\". As the 1980s began, Japan's Fifth Generation Computer Project revived interest in strong AI (ASI), setting out a ten-year timeline that included strong AI (ASI) goals like \"carry on a casual conversation\". In response to this and the success of expert systems, both industry and government pumped money back into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who had predicted the imminent achievement of strong AI (ASI) had been shown to be fundamentally mistaken. By the 1990s, AI researchers had gained a reputation for making vain promises. They became reluctant to make predictions at all and to avoid any mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s].\"\n\nIn the 1990s and early 21st century, mainstream AI has achieved far greater commercial success and academic respectability by focusing on specific sub-problems where they can produce verifiable results and commercial applications, such as neural networks, computer vision or data mining. These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is very heavily funded in both academia and industry.\n\nMost mainstream AI researchers hope that strong AI can be developed by combining the programs that solve various sub-problems using an integrated agent architecture, cognitive architecture or subsumption architecture. Hans Moravec wrote in 1988: \"I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.\"\n\nHowever, even this fundamental philosophy has been disputed; for example, Stevan Harnad of Princeton concluded his 1990 paper on the Symbol Grounding Hypothesis by stating: \"The expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).\"\n\nArtificial general intelligence (AGI) describes research that aims to create machines capable of general intelligent action. The term was introduced by Mark Gubrud in 1997 in a discussion of the implications of fully automated military production and operations. The research objective is much older, for example Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project are regarded as within the scope of AGI. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". As yet, most AI researchers have devoted little attention to AGI, with some claiming that intelligence is too complex to be completely replicated in the near term. However, a small number of computer scientists are active in AGI research, and many of this group are contributing to a series of AGI conferences. The research is extremely diverse and often pioneering in nature. In the introduction to his book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century, but the consensus in the AGI research community seems to be that the timeline discussed by Ray Kurzweil in \"The Singularity is Near\" (i.e. between 2015 and 2045) is plausible. Most mainstream AI researchers doubt that progress will be this rapid. Organizations explicitly pursuing AGI include the Swiss AI lab IDSIA, Nnaisense, the OpenCog Foundation, Adaptive AI, LIDA, and Numenta and the associated Redwood Neuroscience Institute. In addition, organizations such as the Machine Intelligence Research Institute and OpenAI have been founded to influence the development path of AGI. Finally, projects such as the Human Brain Project have the goal of building a functioning simulation of the human brain. A 2017 survey of AGI categorized forty-five known \"active R&D projects\" that explicitly or implicitly (through published research) research AGI, with the largest three being DeepMind, the Human Brain Project, and OpenAI.\n\nA popular approach discussed to achieving general intelligent action is whole brain emulation. A low-level brain model is built by scanning and mapping a biological brain in detail and copying its state into a computer system or another computational device. The computer runs a simulation model so faithful to the original that it will behave in essentially the same way as the original brain, or for all practical purposes, indistinguishably. Whole brain emulation is discussed in computational neuroscience and neuroinformatics, in the context of brain simulation for medical research purposes. It is discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book \"The Singularity Is Near\" predicts that a map of sufficient quality will become available on a similar timescale to the required computing power.\n\n For low-level brain simulation, an extremely powerful computer would be required. The human brain has a huge number of synapses. Each of the 10 (one hundred billion) neurons has on average 7,000 synaptic connections to other neurons. It has been estimated that the brain of a three-year-old child has about 10 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 10 to 5×10 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 10 (100 trillion) synaptic updates per second (SUPS). In 1997 Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 10 computations per second (cps). (For comparison, if a \"computation\" was equivalent to one \"floating point operation\" – a measure used to rate current supercomputers – then 10 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011). He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.\n\nThe artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently only understood in the broadest of outlines. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition the estimates do not account for glial cells, which are at least as numerous as neurons, and which may outnumber neurons by as much as 10:1, and are now known to play a role in cognitive processes.\n\nThere are some research projects that are investigating brain simulation using more sophisticated neural models, implemented on conventional computing architectures. The Artificial Intelligence System project implemented non-real time simulations of a \"brain\" (with 10 neurons) in 2005. It took 50 days on a cluster of 27 processors to simulate 1 second of a model. The Blue Brain project used one of the fastest supercomputer architectures in the world, IBM's Blue Gene platform, to create a real time simulation of a single rat neocortical column consisting of approximately 10,000 neurons and 10 synapses in 2006. A longer term goal is to build a detailed, functional simulation of the physiological processes in the human brain: \"It is not impossible to build a human brain and we can do it in 10 years,\" Henry Markram, director of the Blue Brain Project said in 2009 at the TED conference in Oxford. There have also been controversial claims to have simulated a cat brain. Neuro-silicon interfaces have been proposed as an alternative implementation strategy that may scale better.\n\nHans Moravec addressed the above arguments (\"brains are more complicated\", \"neurons have to be modeled in more detail\") in his 1997 paper \"When will computer hardware match the human brain?\". He measured the ability of existing software to simulate the functionality of neural tissue, specifically the retina. His results do not depend on the number of glial cells, nor on what kinds of processing neurons perform where.\n\nA fundamental criticism of the simulated brain approach derives from embodied cognition where human embodiment is taken as an essential aspect of human intelligence. Many researchers believe that embodiment is necessary to ground meaning. If this view is correct, any fully functional brain model will need to encompass more than just the neurons (i.e., a robotic body). Goertzel proposes virtual embodiment (like Second Life), but it is not yet known whether this would be sufficient.\n\nDesktop computers using microprocessors capable of more than 10 cps (Kurzweil's non-standard unit \"computations per second\", see above) have been available since 2005. According to the brain power estimates used by Kurzweil (and Moravec), this computer should be capable of supporting a simulation of a bee brain, but despite some interest no such simulation exists . There are at least three reasons for this:\n\nIn addition, the scale of the human brain is not currently well-constrained. One estimate puts the human brain at about 100 billion neurons and 100 trillion synapses. Another estimate is 86 billion neurons of which 16.3 billion are in the cerebral cortex and 69 billion in the cerebellum. Glial cell synapses are currently unquantified but are known to be extremely numerous.\n\nAlthough the role of consciousness in strong AI/AGI is debatable, many AGI researchers regard research that investigates possibilities for implementing consciousness as vital. In an early effort Igor Aleksander argued that the principles for creating a conscious machine already existed but that it would take forty years to train such a machine to understand language.\n\nIn 1980, philosopher John Searle coined the term \"strong AI\" as part of his Chinese room argument. He wanted to distinguish between two different hypotheses about artificial intelligence:\nThe first one is called \"the \"strong\" AI hypothesis\" and the second is \"the \"weak\" AI hypothesis\" because the first one makes the \"stronger\" statement: it assumes something special has happened to the machine that goes beyond all its abilities that we can test. Searle referred to the \"strong AI hypothesis\" as \"strong AI\". This usage is also common in academic AI research and textbooks.\n\nThe weak AI hypothesis is equivalent to the hypothesis that artificial general intelligence is possible. According to Russell and Norvig, \"Most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\"\n\nIn contrast to Searle, Kurzweil uses the term \"strong AI\" to describe any artificial intelligence system that acts like it has a mind, regardless of whether a philosopher would be able to determine if it \"actually\" has a mind or not.\n\nSince the launch of AI research in 1956, the growth of this field has slowed down over time and has stalled the aims of creating machines skilled with intelligent action at the human level. A possible explanation for this delay is that computers lack a sufficient scope of memory or processing power. In addition, the level of complexity that connects to the process of AI research may also limit the progress of AI research.\n\nWhile most AI researchers believe that strong AI can be achieved in the future, there are some individuals like Hubert Dreyfus and Roger Penrose that deny the possibility of achieving AI. John McCarthy was one of various computer scientists who believe human-level AI will be accomplished, but a date cannot accurately be predicted.\n\nConceptual limitations are another possible reason for the slowness in AI research. AI researchers may need to modify the conceptual framework of their discipline in order to provide a stronger base and contribution to the quest of achieving strong AI. As William Clocksin wrote in 2003: \"the framework starts from Weizenbaum’s observation that intelligence manifests itself only relative to specific social and cultural contexts\".\n\nFurthermore, AI researchers have been able to create computers that can perform jobs that are complicated for people to do, but conversely they have struggled to develop a computer that is capable of carrying out tasks that are simple for humans to do . A problem that is described by David Gelernter is that some people assume that thinking and reasoning are equivalent. However, the idea of whether thoughts and the creator of those thoughts are isolated individually has intrigued AI researchers.\n\nThe problems that have been encountered in AI research over the past decades have further impeded the progress of AI. The failed predictions that have been promised by AI researchers and the lack of a complete understanding of human behaviors have helped diminish the primary idea of human-level AI. Although the progress of AI research has brought both improvement and disappointment, most investigators have established optimism about potentially achieving the goal of AI in the 21st century.\n\nOther possible reasons have been proposed for the lengthy research in the progress of strong AI. The intricacy of scientific problems and the need to fully understand the human brain through psychology and neurophysiology have limited many researchers from emulating the function of the human brain into a computer hardware. Many researchers tend to underestimate any doubt that is involved with future predictions of AI, but without taking those issues seriously can people then overlook solutions to problematic questions.\n\nClocksin says that a conceptual limitation that may impede the progress of AI research is that people may be using the wrong techniques for computer programs and implementation of equipment. When AI researchers first began to aim for the goal of artificial intelligence, a main interest was human reasoning. Researchers hoped to establish computational models of human knowledge through reasoning and to find out how to design a computer with a specific cognitive task.\n\nThe practice of abstraction, which people tend to redefine when working with a particular context in research, provides researchers with a concentration on just a few concepts. The most productive use of abstraction in AI research comes from planning and problem solving. Although the aim is to increase the speed of a computation, the role of abstraction has posed questions about the involvement of abstraction operators.\n\nA possible reason for the slowness in AI relates to the acknowledgement by many AI researchers that heuristics is a section that contains a significant breach between computer performance and human performance. The specific functions that are programmed to a computer may be able to account for many of the requirements that allow it to match human intelligence. These explanations are not necessarily guaranteed to be the fundamental causes for the delay in achieving strong AI, but they are widely agreed by numerous researchers.\n\nThere have been many AI researchers that debate over the idea whether machines should be created with emotions. There are no emotions in typical models of AI and some researchers say programming emotions into machines allows them to have a mind of their own. Emotion sums up the experiences of humans because it allows them to remember those experiences. David Gelernter writes, \"No computer will be creative unless it can simulate all the nuances of human emotion.\" This concern about emotion has posed problems for AI researchers and it connects to the concept of strong AI as its research progresses into the future.\n\nThere are other aspects of the human mind besides intelligence that are relevant to the concept of strong AI which play a major role in science fiction and the ethics of artificial intelligence:\nThese traits have a moral dimension, because a machine with this form of strong AI may have legal rights, analogous to the rights of non-human animals. Also, Bill Joy, among others, argues a machine with these traits may be a threat to human life or dignity. It remains to be shown whether any of these traits are necessary for strong AI. The role of consciousness is not clear, and currently there is no agreed test for its presence. If a machine is built with a device that simulates the neural correlates of consciousness, would it automatically have self-awareness? It is also possible that some of these properties, such as sentience, naturally emerge from a fully intelligent machine, or that it becomes natural to \"ascribe\" these properties to machines once they begin to act in a way that is clearly intelligent. For example, intelligent action may be sufficient for sentience, rather than the other way around.\n\nIn science fiction, AGI is associated with traits such as consciousness, sentience, sapience, and self-awareness observed in living beings. However, according to philosopher John Searle, it is an open question whether general intelligence is sufficient for consciousness, even a digital brain simulation. \"Strong AI\" (as defined above by Ray Kurzweil) should not be confused with Searle's \"'strong AI hypothesis\". The strong AI hypothesis is the claim that a computer which behaves as intelligently as a person must also necessarily have a mind and consciousness. AGI refers only to the amount of intelligence that the machine displays, with or without a mind.\n\nOpinions vary both on \"whether\" and \"when\" artificial general intelligence will arrive. At one extreme, AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do\"; obviously this prediction failed to come true. Microsoft co-founder Paul Allen believes that such intelligence is unlikely this century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster than light spaceflight. Optimism that AGI is feasible waxes and wanes, and may have seen a resurgence in the 2010s: around 2015, computer scientist Richard Sutton averaged together some recent polls of artificial intelligence experts and estimated a 25% chance that AGI will arrive before 2030, but a 10% chance that it will never arrive at all.\n\nThe creation of artificial general intelligence may have repercussions so great and so complex that it may not be possible to forecast what will come afterwards. Thus the event in the hypothetical future of achieving strong AI is called the technological singularity, because theoretically one cannot see past it. But this has not stopped philosophers and researchers from guessing what the smart computers or robots of the future may do, including forming a utopia by being our friends or overwhelming us in an AI takeover. The latter potentiality is particularly disturbing as it poses an existential risk for mankind.\n\nSmart computers or robots would be able to produce copies of themselves. They would be self-replicating machines. A growing population of intelligent robots could conceivably outcompete inferior humans in job markets, in business, in science, in politics (pursuing robot rights), and technologically, sociologically (by acting as one), and militarily.\n\nIf research into strong AI produced sufficiently intelligent software, it would be able to reprogram and improve itself – a feature called \"recursive self-improvement\". It would then be even better at improving itself, and would probably continue doing so in a rapidly increasing cycle, leading to an intelligence explosion and the emergence of superintelligence. Such an intelligence would not have the limitations of human intellect, and might be able to invent or discover almost anything.\n\nHyper-intelligent software might not necessarily decide to support the continued existence of mankind, and might be extremely difficult to stop. This topic has also recently begun to be discussed in academic publications as a real source of risks to civilization, humans, and planet Earth.\n\nOne proposal to deal with this is to make sure that the first generally intelligent AI is friendly AI, that would then endeavor to ensure that subsequently developed AIs were also nice to us. But, friendly AI is harder to create than plain AGI, and therefore it is likely, in a race between the two, that non-friendly AI would be developed first. Also, there is no guarantee that friendly AI would remain friendly, or that its progeny would also all be good.\n\n", "id": "586357", "title": "Artificial general intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=45567903", "text": "Luminoso\n\nLuminoso, a Cambridge, MA-based text analytics and artificial intelligence company, spun out of the MIT Media Lab and its crowd-sourced Open Mind Common Sense (OMCS) project.\n\nThe company has raised $8 million in financing and its clients include Sony, Autodesk, Scotts Miracle-Gro, and GlaxoSmithKline.\n\nLuminoso was co-founded in 2010 by Dennis Clark, Jason Alonso, Rob Speer, and CEO Catherine Havasi, a research scientist at MIT in artificial intelligence and computational linguistics. The company builds on the knowledge base of MIT’s Open Mind Common Sense (OMCS) project, co-founded in 1999 by Havasi, who continues to serve as its director. The OCMS knowledge base has since been combined with knowledge from other crowdsourced resources, games with a purpose, and expert-created resources to become ConceptNet. ConceptNet consists of approximately 28 million statements in 304 languages, with full support for 10 languages and moderate support for 77 languages. ConceptNet is a resource for making an AI that understands the meanings of the words people use. \n\nDuring the World Cup in June 2014, the company provided a widely reported real-time sentiment analysis of the U.S. vs. Germany match, analyzing 900,000 posts on Twitter, Facebook and Google+.\n\nThe company uses artificial intelligence, natural language processing, and machine learning to derive insights from unstructured data such as contact center interactions, chatbot and live chat transcripts, product reviews, open-ended survey responses, and email. Luminoso's software identifies and quantifies patterns and relationships in text-based data, including domain-specific or creative language. Rather than human-powered keyword searches of data, the software automates taxonomy creation around concepts, allowing related words and phrases to be dynamically generated and tracked. \n\nCommercial applications include analyzing, prioritizing, and routing contact center interactions; identifying consumer complaints before they begin to trend; and tracking sentiment during product launches. The software natively analyzes text in thirteen languages, as well as emoji.\n\nLuminoso's technology can be accessed via two products: Luminoso Analytics and Luminoso Compass. Luminoso Analytics enables a deep-dive analysis into batch or real-time data, whereas Luminoso Compass automates the categorization of real-time data. Both products offer a user interface as well as an API. Luminoso's products can be implemented through either a cloud-based or an on-premises solution.\n\nLuminoso continues to actively conduct research in natural language processing and word embeddings and regularly participates in evaluations such as SemEval. At SemEval 2017, Luminoso participated in Task 2, measuring the semantic similarity of word pairs within and across five languages. Its solution outperformed all competing systems in every language pair tested, with the exception of Persian.\n\nLuminoso has been listed as a \"Cool Vendor in AI for Marketing\" by Gartner, and has also been named a \"Boston Artificial Intelligence startup to watch\" by BostInno. \n\nIn May 2017, Luminoso was recognized as having the Best Application for AI in the Enterprise by AI Business, and was also shortlisted as the Best AI Breakthrough and Best Innovation in NLP.\n\nMajor competitors include Clarabridge and Lexalytics.\n\nAcadia Woods led a $6.5 million round of funding, with Japan’s Digital Garage, in July 2014. The company previously raised a $1.5 million seed round.\n\n", "id": "45567903", "title": "Luminoso"}
{"url": "https://en.wikipedia.org/wiki?curid=46504825", "text": "Open Letter on Artificial Intelligence\n\nIn January 2015, Stephen Hawking, Elon Musk, and dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI. The letter affirmed that society can reap great potential benefits from artificial intelligence, but called for concrete research on how to prevent certain potential \"pitfalls\": artificial intelligence has the potential to eradicate disease and poverty, but researchers must not create something which cannot be controlled. The four-paragraph letter, titled \"Research Priorities for Robust and Beneficial Artificial Intelligence: An Open Letter\", lays out detailed research priorities in an accompanying twelve-page document.\n\nBy 2014, both physicist Stephen Hawking and business magnate Elon Musk had publicly voiced the opinion that superhuman artificial intelligence could provide incalculable benefits, but could also end the human race if deployed incautiously (see Existential risk from advanced artificial intelligence). Hawking and Musk both sit on the scientific advisory board for the Future of Life Institute, an organization working to \"mitigate existential risks facing humanity\". The institute drafted an open letter directed to the broader AI research community, and circulated it to the attendees of its first conference in Puerto Rico during the first weekend of 2015. The letter was made public on January 12.\n\nThe letter highlights both the positive and negative effects of artificial intelligence. According to Bloomberg Business, Professor Max Tegmark of MIT circulated the letter in order to find common ground between signatories who consider superintelligent AI a significant existential risk, and signatories such as Professor Oren Etzioni, who believe the AI field was being \"impugned\" by a one-sided media focus on the alleged risks. The letter contends that:\n\nThe potential benefits (of AI) are huge, since everything that civilization has to offer is a product of human intelligence; we cannot predict what we might achieve when this intelligence is magnified by the tools AI may provide, but the eradication of disease and poverty are not unfathomable. Because of the great potential of AI, it is important to research how to reap its benefits while avoiding potential pitfalls.\n\nOne of the signatories, Professor Bart Selman of Cornell University, said the purpose is to get AI researchers and developers to pay more attention to AI safety. In addition, for policymakers and the general public, the letter is meant to be informative but not alarmist. Another signatory, Professor Francesca Rossi, stated that \"I think it's very important that everybody knows that AI researchers are seriously thinking about these concerns and ethical issues\".\n\nThe signatories ask: How can engineers create AI systems that are beneficial to society, and that are robust? Humans need to remain in control of AI; our AI systems must \"do what we want them to do\". The required research is interdisciplinary, drawing from areas ranging from economics and law to various branches of computer science, such as computer security and formal verification. Challenges that arise are divided into verification (\"Did I build the system right?\"), validity (\"Did I build the right system?\"), security, and control (\"OK, I built the system wrong, can I fix it?\")\n\nSome near-term concerns relate to autonomous vehicles, from civilian drones and self-driving cars. For example, a self-driving car may, in an emergency, have to decide between a small risk of a major accident and a large probability of a small accident. Other concerns relate to lethal intelligent autonomous weapons: Should they be banned? If so, how should 'autonomy' be precisely defined? If not, how should culpability for any misuse or malfunction be apportioned?\n\nOther issues include privacy concerns as AI becomes increasingly able to interpret large surveillance datasets, and how to best manage the economic impact of jobs displaced by AI.\n\nThe document closes by echoing Microsoft research director Eric Horvitz's concerns that:\n\nwe could one day lose control of AI systems via the rise of superintelligences that do not act in accordance with human wishes – and that such powerful systems would threaten humanity. Are such dystopic outcomes possible? If so, how might these situations arise? ...What kind of investments in research should be made to better understand and to address the possibility of the rise of a dangerous superintelligence or the occurrence of an \"intelligence explosion\"?\n\nExisting tools for harnessing AI, such as reinforcement learning and simple utility functions, are inadequate to solve this; therefore more research is necessary to find and validate a robust solution to the \"control problem\".\n\nSignatories include physicist Stephen Hawking, business magnate Elon Musk, the co-founders of DeepMind, Vicarious, Google's director of research Peter Norvig, Professor Stuart J. Russell of the University of California Berkeley, and other AI experts, robot makers, programmers, and ethicists. The original signatory count was over 150 people, including academics from Cambridge, Oxford, Stanford, Harvard, and MIT.\n\n", "id": "46504825", "title": "Open Letter on Artificial Intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=46222904", "text": "Intel RealSense\n\nIntel RealSense, formerly known as Intel Perceptual Computing, is a platform for implementing gesture-based human-computer interaction techniques. It consists of series of consumer grade 3D cameras together with an easy to use machine perception library that simplifies supporting the cameras for third-party software developers.\n\nAs of March 2015, multiple laptop and tablet computer manufactures offer one or more devices with Intel RealSense camera built in. These are Asus, HP, Dell, Lenovo, and Acer. Also, a standalone webcam from Razer is among the first to offer the Intel RealSense camera built into the design. Consumer-ready versions of the RealSense camera are the Razer Stargazer and the Creative BlasterX Senz3D.\n\n\nAn Intel RealSense camera contains the following four components: a conventional camera, an infrared laser projector, an infrared camera, and a microphone array. The infrared projector projects a grid onto the scene (in infrared light which is invisible to human eye) and the infrared camera records it to compute depth information. The microphone array allows localizing sound sources in space and performing background noise cancellation.\n\nThree camera models were announced, with distinct specifications and intended use.\n\nThis is a stand-alone camera that can be attached to a desktop or laptop computer. It is intended to be used for natural gesture-based interaction, face recognition, immersive, video conferencing and collaboration, gaming and learning and 3D scanning.\n\nThere is a version of this camera to be embedded into laptop computers.\n\n\nSnapshot is a camera intended to be built into tablet computers and possibly smartphones. Its intended uses include taking photographs and performing after the fact refocusing, distance measurements, and applying motion photo filters.\n\nThe refocus feature differs from a plenoptic camera in that RealSense Snapshot takes pictures with large depth of field so that initially the whole picture is in focus and then in software it selectively blurs parts of the image depending on their distance.\n\nDell Venue 8 7000 Series Android tablet is equipped with this camera.\n\nRear-mounted camera for Microsoft Surface or a similar tablet, like the HP Spectre X2. This camera is intended for augmented reality applications, content creation, and object scanning. Its depth accuracy is on the order of millimeters and its range is up to 6.0 meters. This makes it the more accurate and longer range of the Intel RealSense 3D cameras. Also unlike the F200 and SR300, the R200 is a stereo camera and is able to obtain accurate depth outdoors as well as indoors.\n\nThe SR300 camera is the next generation of the Front F200 camera. It improves in various respects over its predecessor, notably with\n\n\nThe Front SR300 camera can be bought from Intel for approximately $130 USD.\n\nTo address the lack of applications built on the RealSense platform and to promote the platform among software developers, in 2014 Intel organized the Intel RealSense App Challenge. The winners were awarded large sums of money.\n\nIn an early preview article, \"PC World\"s Mark Hachman concluded that RealSense is an enabling technology that will be largely defined by the software that will take advantage of its features. He noted that as of the time the article was written, the technology was new and there was no such software.\n\n\n", "id": "46222904", "title": "Intel RealSense"}
{"url": "https://en.wikipedia.org/wiki?curid=5389390", "text": "Autonomic networking\n\nAutonomic Networking follows the concept of Autonomic Computing, an initiative started by IBM in 2001. Its ultimate aim is to create self-managing networks to overcome the rapidly growing complexity of the Internet and other networks and to enable their further growth, far beyond the size of today.\n\nThe ever-growing management complexity of the Internet caused by its rapid growth is seen by some experts as a major problem that limits its usability in the future.\n\nWhat's more, increasingly popular smartphones, PDAs, networked audio and video equipment, and game consoles need to be interconnected. Pervasive Computing not only adds features, but also burdens existing networking infrastructure with more and more tasks that sooner or later will not be manageable by human intervention alone.\n\nAnother important aspect is the price of manually controlling huge numbers of vitally important devices of current network infrastructures.\n\nThe autonomic nervous system (ANS) is the part of the nervous system of the higher life forms that is not consciously controlled. It regulates bodily functions and the activity of specific organs. As proposed by IBM, future communication systems might be designed in a similar way to the ANS. \n\nAs autonomics conceptually derives from biological entities such as the human autonomic nervous system, each of the areas can be metaphorically related to functional and structural aspects of a living being. In the human body, the autonomic system facilitates and regulates a variety of functions including respiration, blood pressure and circulation, and emotive response. The autonomic nervous system is the interconnecting fabric that supports feedback loops between internal states and various sources by which internal and external conditions are monitored.\n\nAutognostics includes a range of self-discovery, awareness, and analysis capabilities that provide the autonomic system with a view on high-level state. In metaphor, this represents the perceptual sub-systems that gather, analyze, and report on internal and external states and conditions – for example, this might be viewed as the eyes, visual cortex and perceptual organs of the system. Autognostics, or literally \"self-knowledge\", provides the autonomic system with a basis for response and validation.\n\nA rich autognostic capability may include many different \"perceptual senses\". For example, the human body gathers information via the usual five senses, the so-called sixth sense of proprioception (sense of body position and orientation), and through emotive states that represent the gross wellness of the body. As conditions and states change, they are detected by the sensory monitors and provide the basis for adaptation of related systems. Implicit in such a system are imbedded models of both internal and external environments such that relative value can be assigned to any perceived state - perceived physical threat (e.g. a snake) can result in rapid shallow breathing related to fight-flight response, a phylogenetically effective model of interaction with recognizable threats.\n\nIn the case of autonomic networking, the state of the network may be defined by inputs from:\n\nMost of these sources represent relatively raw and unprocessed views that have limited relevance. Post-processing and various forms of analysis must be applied to generate meaningful measurements and assessments against which current state can be derived.\n\nThe autognostic system interoperates with:\n\nConfiguration management is responsible for the interaction with network elements and interfaces. It includes an accounting capability with historical perspective that provides for the tracking of configurations over time, with respect to various circumstances. In the biological metaphor, these are the hands and, to some degree, the memory of the autonomic system.\n\nOn a network, remediation and provisioning are applied via configuration setting of specific devices. Implementation affecting access and selective performance with respect to role and relationship are also applied. Almost all the \"actions\" that are currently taken by human engineers fall under this area. With only a few exceptions, interfaces are set by hand, or by extension of the hand, through automated scripts.\n\nImplicit in the configuration process is the maintenance of a dynamic population of devices under management, a historical record of changes and the directives which invoked change. Typical to many accounting functions, configuration management should be capable of operating on devices and then rolling back changes to recover previous configurations. Where change may lead to unrecoverable states, the sub-system should be able to qualify the consequences of changes prior to issuing them.\n\nAs directives for change must originate from other sub-systems, the shared language for such directives must be abstracted from the details of the devices involved. The configuration management sub-system must be able to translate unambiguously between directives and hard actions or to be able to signal the need for further detail on a directive. An inferential capacity may be appropriate to support sufficient flexibility (i.e. configuration never takes place because there is no unique one-to-one mapping between directive and configuration settings). Where standards are not sufficient, a learning capacity may also be required to acquire new knowledge of devices and their configuration.\n\nConfiguration management interoperates with all of the other sub-systems including:\n\nPolicy management includes policy specification, deployment, reasoning over policies, updating and maintaining policies, and enforcement. Policy-based management is required for:\n\nIt provides the models of environment and behavior that represent effective interaction according to specific goals. In the human nervous system metaphor, these models are implicit in the evolutionary \"design\" of biological entities and specific to the goals of survival and procreation. Definition of what constitutes a policy is necessary to consider what is involved in managing it. A relatively flexible and abstract framework of values, relationships, roles, interactions, resources, and other components of the network environment is required. This sub-system extends far beyond the physical network to the applications in use and the processes and end-users that employ the network to achieve specific goals. It must express the relative values of various resources, outcomes, and processes and include a basis for assessing states and conditions.\n\nUnless embodied in some system outside the autonomic network or implicit to the specific policy implementation, the framework must also accommodate the definition of process, objectives and goals. Business process definitions and descriptions are then an integral part of the policy implementation. Further, as policy management represents the ultimate basis for the operation of the autonomic system, it must be able to report on its operation with respect to the details of its implementation.\n\nThe policy management sub-system interoperates (at least) indirectly with all other sub-systems but primarily interacts with:\n\nAutodefense represents a dynamic and adaptive mechanism that responds to malicious and intentional attacks on the network infrastructure, or use of the network infrastructure to attack IT resources. As defensive measures tend to impede the operation of IT, it is optimally capable of balancing performance objectives with typically over-riding threat management actions. In the biological metaphor, this sub-system offers mechanisms comparable to the immune system.\n\nThis sub-system must proactively assess network and application infrastructure for risks, detect and identify threats, and define effective both proactive and reactive defensive responses. It has the role of the warrior and the security guard insofar as it has roles for both maintenance and corrective activities. Its relationship with security is close but not identical – security is more concerned with appropriately defined and implemented access and authorization controls to maintain legitimate roles and process. Autodefense deals with forces and processes, typically malicious, outside the normal operation of the system that offer some risk to successful execution.\n\nAutodefense requires high-level and detailed knowledge of the entire network as well as imbedded models of risk that allow it to analyze dynamically the current status. Corrections to decrease risk must be considered in balance with performance objectives and value of process goals – an overzealous defensive response can immobilize the system (like the immune system inappropriately invoking an allergic reaction). The detection of network or application behaviors that signal possible attack or abuse is followed by the generation of an appropriate response – for example, ports might be temporarily closed or packets with a specific source or destination might be filtered out. Further assessment generates subsequent changes either relaxing the defensive measures or strengthening them.\n\nAutodefense interoperates closely with:\n\nIt also may receive definition of relative value of various resources and processes from policy management in order to develop responses consistent with policy.\n\nSecurity provides the structure that defines and enforces the relationships between roles, content, and resources, particularly with respect to access. It includes the framework for definitions as well as the means to implement them. In metaphor, security parallels the complex mechanisms underlying social interactions, defining friends, foes, mates and allies and offering access to limited resources on the basis of assessed benefit.\n\nSeveral key means are employed by security – they include the well-known 3 As of authentication, authorization, and access (control). The basis for applying these means requires the definition of roles and their relationships to resources, processes and each other. High-level concepts like privacy, anonymity and verification are likely imbedded in the form of the role definitions and derive from policy. Successful security reliably supports and enforces roles and relationships.\n\nAutodefense has a close association with security – maintaining the assigned roles in balance with performance exposes the system to potential violations in security. In those cases, the system must compensate by making changes that may sacrifice balance on a temporary basis and indeed may violate the operational terms of security itself. Typically the two are viewed as inextricably intertwined – effective security somewhat hopefully negating any need for a defensive response. Security’s revised role is to mediate between the competing demands from policy for maximized performance and minimized risk with auto defense recovering the balance when inevitable risk translates to threat. Federation represents one of the key challenges to be solved by effective security.\n\nThe security sub-system interoperates directly with:\n\nThe connection fabric supports the interaction with all the elements and sub-systems of the autonomic system. It may be composed of a variety of means and mechanisms, or may be a single central framework. The biological equivalent is the central nervous system itself – although referred to as the autonomic system, it actually is only the communication conduit between the human body’s faculties.\n\nConsequently, it is currently under research by many research projects, how principles and paradigms of mother nature might be applied to networking.\n\nInstead of a layering approach, autonomic networking targets a more flexible structure termed compartmentalization.\n\nThe goal is to produce an architectural design that enables flexible, dynamic, and fully autonomic formation of large-scale networks in which the functionalities of each constituent network node are also composed in an autonomic fashion\n\nFunctions should be divided into atomic units to allow for maximal re-composition freedom.\n\nA fundamental concept of Control theory, the closed control loop, is among the fundamental principles of autonomic networking. A closed control loop maintains the properties of the controlled system within desired bounds by constantly monitoring target parameters.\n\n\n\n\n", "id": "5389390", "title": "Autonomic networking"}
{"url": "https://en.wikipedia.org/wiki?curid=47228422", "text": "User behavior analytics\n\nUser behavior analytics (\"UBA\") as defined by Gartner or Análise de comportamento de Usuário (ACU) in Portuguese, is a cybersecurity process about detection of insider threats, targeted attacks, and financial fraud. UBA solutions look at patterns of human behavior, and then apply algorithms and statistical analysis to detect meaningful anomalies from those patterns—anomalies that indicate potential threats. Instead of tracking devices or security events, UBA tracks a system's users. Big data platforms like Apache Hadoop are increasing UBA functionality by allowing them to analyze petabytes worth of data to detect insider threats and advanced persistent threats.\n\nThe problem UBA responds to, as described by Nemertes Research CEO Johna Till Johnson, is that \"Security systems provide so much information that it's tough to uncover information that truly indicates a potential for real attack. Analytics tools help make sense of the vast amount of data that SIEM, IDS/IPS, system logs, and other tools gather. UBA tools use a specialized type of security analytics that focuses on the behavior of systems and the people using them. UBA technology first evolved in the field of marketing, to help companies understand and predict consumer-buying patterns. But as it turns out, UBA can be extraordinarily useful in the security context too.\" \n\nDevelopments in UBA technology led Gartner to evolve the category to user and entity behavior analytics (\"UEBA\"). In September 2015, Gartner published the Market Guide for User and Entity Analytics by Vice President and Distinguished Analyst, Avivah Litan, that provided a thorough definition and explanation. UEBA was referred to in earlier Gartner reports but not in much depth. Expanding the definition from UBA includes devices, applications, servers, data, or anything with an IP address. It moves beyond the fraud-oriented UBA focus to a broader one encompassing \"malicious and abusive behavior that otherwise went unnoticed by existing security monitoring systems, such as SIEM and DLP.\" The addition of \"entity\" reflects that devices may play a role in a network attack and may also be valuable in uncovering attack activity. \"When end users have been compromised, malware can lay dormant and go undetected for months. Rather than trying to find where the outsider entered, UEBAs allow for quicker detection by using algorithms to detect insider threats.\"\n\nParticularly in the computer security market, there are many vendors for UEBA applications. They can be \"differentiated by whether they are designed to monitor on-premises or cloud-based software as a service (SaaS) applications; the methods in which they obtain the source data; the type of analytics they use (i.e., packaged analytics, user-driven or vendor-written), and the service delivery method (i.e., on-premises or a cloud-based).\" \nAccording to the 2015 market guide released by Gartner, \"the UEBA market grew substantially in 2015; UEBA vendors grew their customer base, market consolidation began, and Gartner client interest in UEBA and security analytics increased.\" The report further projected, \"Over the next three years, leading UEBA platforms will become preferred systems for security operations and investigations at some of the organizations they serve. It will be—and in some cases already is—much easier to discover some security events and analyze individual offenders in UEBA than it is in many legacy security monitoring systems.\"\n\n", "id": "47228422", "title": "User behavior analytics"}
{"url": "https://en.wikipedia.org/wiki?curid=11338826", "text": "Admissible heuristic\n\nIn computer science, specifically in algorithms related to pathfinding, a heuristic function is said to be admissible if it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path.\n\nAn admissible heuristic is used to estimate the cost of reaching the \ngoal state in an informed search algorithm. In order for a heuristic\nto be admissible to the search problem, the estimated cost must always \nbe lower than or equal to the actual cost of reaching the goal state. \nThe search algorithm uses the admissible heuristic to find an estimated \noptimal path to the goal state from the current node. \nFor example, in A* search the evaluation function (where \nformula_1 is the current node) is:\n\nformula_2\n\nwhere\n\nformula_5 is calculated using the heuristic \nfunction. With a non-admissible heuristic, the A* algorithm could \noverlook the optimal solution to a search problem due to an \noverestimation in formula_3.\n\nAn admissible heuristic can be derived from a relaxed\nversion of the problem, or by information from pattern databases that store exact solutions to subproblems of the problem, or by using inductive learning methods.\n\nTwo different examples of admissible heuristics apply to the fifteen puzzle problem:\n\nThe Hamming distance is the total number of misplaced tiles. It is clear that this heuristic is admissible since the total number of moves to order the tiles correctly is at least the number of misplaced tiles (each tile not in place must be moved at least once). The cost (number of moves) to the goal (an ordered puzzle) is at least the Hamming distance of the puzzle.\n\nThe Manhattan distance of a puzzle is defined as:\n\nConsider the puzzle below in which the player wishes to move each tile such that the numbers are ordered. The Manhattan distance is an admissible heuristic in this case because every tile will have to be moved at least the amount of spots in between itself and its correct position. \n\nThe subscripts show the Manhattan distance for each tile. The total Manhattan distance for the shown puzzle is:\n\nIf an admissible heuristic is adapted in an algorithm (for example in A* search algorithm), then this algorithm would eventually find an optimal solution to the goal. A well-known and easy-to-understand example is breadth-first search. In the case of BFS, the heuristic evaluation function formula_5 equals to formula_21 for each node formula_1, which is obviously less than the actual cost (thus underestimated). This heuristic would cause the BFS algorithm to search literally all the possible paths and eventually find the optimal solution (i.e., the shortest path to the goal).\n\nAs an example of why admissibility can guarantee optimality, let's say we have costs as follows:(the cost above/below a node is the heuristic, the cost at an edge is the actual cost)\n\nSo clearly we'd start off visiting the top middle node, since the expected total cost, i.e. formula_3, is formula_24. Then the goal would be a candidate, with formula_3 equal to formula_26. Then we'd clearly pick the bottom nodes one after the other, followed by the updated goal, since they all have formula_3 lower than the formula_3 of the current goal, i.e. their formula_3 is formula_30. So even though the goal was a candidate, we couldn't pick it because there were still better paths out there. This way, an admissible heuristic can ensure optimality.\n\nHowever, note that although an admissible heuristic can guarantee final optimality, it's not necessarily efficient.\n\nWhile all consistent heuristics are admissible, not all admissible heuristics are consistent.\n\nFor tree search problems, if an admissible heuristic is used, the A* search algorithm will never return a suboptimal goal node.\n\n", "id": "11338826", "title": "Admissible heuristic"}
{"url": "https://en.wikipedia.org/wiki?curid=39242563", "text": "Medical intelligence and language engineering lab\n\nThe Medical Intelligence and Language Engineering laboratory, also known as MILE lab, is a research laboratory at the Indian Institute of Science, Bangalore under the Department of Electrical Engineering. The lab is known for its work on Image processing, online handwriting recognition, Text-To-Speech and Optical character recognition systems, all of which are focused mainly on documents and speech in Indian languages. The lab is headed by A. G. Ramakrishnan.\n\nOne of the commitments of MILE lab is the development of technology for people with visual impairment to harness knowledge from any available printed material in Indian languages. The lab is working towards reaching this goal. Its work till now included: document mosaicing of coloured, camera captured images ; text extraction from complex colour images, including camera captured images; document layout analysis; detection of broken and merged characters; OCR technology for Tamil and Kannada; text to speech conversion in Tamil and Kannada; pitch modification using discrete cosine transform in the source domain; automated part of speech tagging; phrase prediction and prosody modeling.\n\nMozhi Vallan, the Tamil OCR product developed by MILE Lab, is being used by Worth Trust and Karna Vidya Technology Centre, Chennai for the conversion of printed school and college books to Braille format. Sri Ramakrishna Math, Chennai is using it to convert their printed philosophical books in Tamil to computer readable text. Lipi Gnani, the Kannada OCR developed by MILE Lab is being used by Braille Transcription Centers of Mitrajyothi and Canara Bank Relief & Welfare Society, Bangalore for similar purposes. Also, Thirukkural, the Tamil TTS system developed by MILE Lab is being used by some school teachers in Singapore for assignments. Madhura, the Kannada TTS developed by the lab, is being used by two blind students, integrated with a screen reader, to read aloud text OCR'ed with Lipi Gnani from Kannada books. Currently, the lab is researching on machine listening and a novel temporal feature named as plosion index has been proposed, which has been shown to be extremely effective in detecting closure-burst transitions of stop consonants and affricates from continuous speech, even in noise. Another feature proposed is DCTILPR, which is a voice source based feature vector that improves the recognition performance of a speaker identification system.\n\nIn the early days, significant work was carried out in medical signal and image processing. A unique algorithm was proposed for ECG compression by treating each cardiac cycle as a vector, and applying linear prediction on the discrete wavelet transform of this vector, after normalizing its period using multirate processing based interpolation. The maturity of the fetal lung was predicted using image texture features obtained from the liver and lung regions of the ultrasound images obtained from pregnant women An effective technique was proposed for lossless compression of 3D magnetic resonance images of the brain. Each MRI slice was represented by uniform or adaptive mesh; affine transformation was applied between the corresponding mesh elements of adjacent slices and context-based entropy coding, on the residues.\n\n", "id": "39242563", "title": "Medical intelligence and language engineering lab"}
{"url": "https://en.wikipedia.org/wiki?curid=44628427", "text": "Cloud robotics\n\nCloud robotics is a field of robotics that attempts to invoke cloud technologies such as cloud computing, cloud storage, and other Internet technologies centred on the benefits of converged infrastructure and shared services for robotics. When connected to the cloud, robots can benefit from the powerful computation, storage, and communication resources of modern data center in the cloud, which can process and share information from various robots or agent (other machines, smart objects, humans, etc.). Humans can also delegate tasks to robots remotely through networks. Cloud computing technologies enable robot systems to be endowed with powerful capability whilst reducing costs through cloud technologies. Thus, it is possible to build lightweight, low cost, smarter robots have intelligent \"brain\" in the cloud. The \"brain\" consists of data center, knowledge base, task planners, deep learning, information processing, environment models, communication support etc.\n\nA cloud for robots potentially has at least six significant components:\n\n\nAutonomous mobile robots: Google's self-driving cars are cloud robots. The cars use the network to access Google's enormous database of maps and satellite and environment model (like Streetview) and combines it with streaming data from GPS, cameras, and 3D sensors to monitor its own position within centimetres, and with past and current traffic patterns to avoid collisions. Each car can learn something about environments, roads, or driving, or conditions, and it sends the information to the Google cloud, where it can be used to improve the performance of other cars.\n\nCloud medical robots: a medical cloud (also called a healthcare cluster) consists of various services such as a disease archive, electronic medical records, a patient health management system, practice services, analytics services, clinic solutions, expert systems, etc. A robot can connect to the cloud to provide clinical service to patients, as well as deliver assistance to doctors (e.g. a co-surgery robot). Moreover, it also provides a collaboration service by sharing information between doctors and care givers about clinical treatment.\nAssistive robots: A domestic robot can be employed for healthcare and life monitoring for elderly people. The system collects the health status of users and exchange information with cloud expert system or doctors to facilitate elderly peoples life, especially for those with chronic diseases. For example, the robots are able to provide support to prevent the elderly from falling down, emergency healthy support such as heart disease, blooding disease. Care givers of elderly people can also get notification when in emergency from the robot through network.\n\nIndustrial robots: As highlighted by the Germany Industry 4.0 Plan \"Industry is on the threshold of the fourth industrial revolution. Driven by the Internet, the real and virtual worlds are growing closer and closer together to form the Internet of Things. Industrial production of the future will be characterised by the strong individualisation of products under the conditions of highly flexible (large series) production, the extensive integration of customers and business partners in business and value-added processes, and the linking of production and high-quality services leading to so-called hybrid products.\" In manufacturing, such cloud based robot systems could learn to handle tasks such as threading wires or cables, or aligning gaskets from professional knowledge base. A group of robots can share information for some collaborative tasks. Even more, a consumer is able to order customised product to manufacturing robots directly with online order system. Another potential paradigm is shopping-delivery robot system- once an order is placed, a warehouse robot dispatches the item to an autonomous car or autonomous drone to delivery it to its recipient (see Figure Cloud Self-driving Car ).\n\nRoboEarth was funded by the European Union's Seventh Framework Programme for research, technological development projects, specifically to explore the field of cloud robotics. The goal of RoboEarth is to allow robotic systems to benefit from the experience of other robots, paving the way for rapid advances in machine cognition and behaviour, and ultimately, for more subtle and sophisticated human-machine interaction. RoboEarth offers a Cloud Robotics infrastructure. RoboEarth’s World-Wide-Web style database stores knowledge generated by humans – and robots – in a machine-readable format. Data stored in the RoboEarth knowledge base include software components, maps for navigation (e.g., object locations, world models), task knowledge (e.g., action recipes, manipulation strategies), and object recognition models (e.g., images, object models). The RoboEarth Cloud Engine includes support for mobile robots, autonomous vehicles, and drones, which require lots of computation for navigation.\n\nRapyuta is an open source cloud robotics framework based on RoboEarth Engine developed by the robotics researcher at ETHZ. Within the framework, each robot connected to Rapyuta can have a secured computing environment (rectangular boxes) giving them the ability to move their heavy computation into the cloud. In addition, the computing environments are tightly interconnected with each other and have a high bandwidth connection to the RoboEarth knowledge repository.\n\nKnowRob is an extensional project of RoboEarth. It is a knowledge processing system that combines knowledge representation and reasoning methods with techniques for acquiring knowledge and for grounding the knowledge in a physical system and can serve as a common semantic framework for integrating information from different sources.\n\nRoboBrain is a large-scale computational system that learns from publicly available Internet resources, computer simulations, and real-life robot trials. It accumulates everything robotics into a comprehensive and interconnected knowledge base. Applications include prototyping for robotics research, household robots, and self-driving cars. The goal is as direct as the project's name—to create a centralised, always-online brain for robots to tap into. The project is dominated by Stanford University and Cornel University. And the project is supported by the National Science Foundation, the Office of Naval Research, the Army Research Office, Google, Microsoft, Qualcomm, the Alfred P. Sloan Foundation and the National Robotics Initiative, whose goal is to advance robotics to help make the United States more competitive in the world economy.\n\nMyRobots is a service for connecting robots and intelligent devices to the Internet. It can be regarded as a social network for robots and smart objects (i.e. Facebook for robots). With socialising, collaborating and sharing, robots can benefit from those interactions too by sharing their sensor information giving insight on their perspective of their current state.\n\nCOALAS is funded by the INTERREG IVA France (Channel) – England European cross-border co-operation programme. The project aims to develop new technologies for handicapped people through social and technological innovation and through the users' social and psychological integrity. Objectives is to produce a cognitive ambient assistive living system with Healthcare cluster in cloud with domestic service robots like humanoid, intelligent wheelchair which connect with the cloud.\n\nROS (Robot Operating System) provides an eco-system to support cloud robotics. ROS is a flexible and distributed framework for robot software development. It is a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behaviour across a wide variety of robotic platforms. A library for ROS that is a pure Java implementation, called rosjava, allows Android applications to be developed for robots. Since Android has a booming market and billion users, it would be significant in the field of Cloud Robotics.\n\nC2RO (C2RO Cloud Robotics) is a platform that processes real-time applications such as collision avoidance and object recognition in the cloud. Previously, high latency times prevented these applications from being processed in the cloud thus requiring on-system computational hardware (e.g. Graphics Processing Unit or GPU). C2RO published a peer-reviewed paper at IEEE PIMRC17 showing its platform could make autonomous navigation and other AI services available on robots- even those with limited computational hardware (e.g. a Raspberry Pi)- from the cloud. C2RO eventually claimed to be the first platform to demonstrate cloud-based SLAM (simultaneous localization and mapping) at RoboBusiness in September 2017.\n\nThough robots can benefit from various advantages of cloud computing, cloud is not the solution to all of robotics.\n\n\nThe research and development of cloud robotics has following potential issues and challenges:\n\n\nEnvironmental security - The concentration of computing resources and users in a cloud computing environment also represents a concentration of security threats. Because of their size and significance, cloud environments are often targeted by virtual machines and bot malware, brute force attacks, and other attacks.\n\nData privacy and security - Hosting confidential data with cloud service providers involves the transfer of a considerable amount of an organisation's control over data security to the provider. For example, every cloud contains a huge information from the clients include personal data. If a household robot is hacked, users could have risk of their personal privacy and security, like house layout, life snapshot, home-view, etc. It may be accessed and leaked to the world around by criminals. Another problems is once a robot is hacked and controlled by someone else, which may put the user in danger.\n\nEthical problems - Some ethics of robotics, especially for cloud based robotics must be considered. Since a robot is connected via networks, it has risk to be accessed by other people. If a robot is out of control and carries out illegal activities, who should be responsible for it.\n\nSpecial Issue on Cloud Robotics and Automation- A special issue of the IEEE Transactions on Automation Science and Engineering, April 2015.\n\nCloud Robotics-Enable cloud computing for robots. The author proposed some paradigms of using cloud computing in robotics. Some potential field and challenges were coined. R. Li 2014.\n\n2013 IEEE IROS Workshop on Cloud Robotics. Tokyo. November 2013.\n\nNRI Workshop on Cloud Robotics: Challenges and Opportunities- February 2013.\n\nRobot APP Store Robot Applications in Cloud, provide applications for robot just like computer/phone app.\n\nDARPA Cloud Robotics.\n\nA Roadmap for U.S. Robotics From Internet to Robotics 2013 Edition- by Georgia Institute of Technology, Carnegie Mellon University Robotics Technology Consortium, University of Pennsylvania, University of Southern California, Stanford University, University of California–Berkeley, University of Washington, Massachusetts Institute of TechnologyUS and Robotics OA US. The Roadmap highlighted “Cloud” Robotics and Automation for Manufacturing in the future years.\n\nCloud-Based Robot Grasping with the Google Object Recognition Engine.\n\nNational Robotics Initiative of US announced in 2011 aimed to explore how robots can enhance the work of humans rather than replacing them. It claims that next generation of robots are more aware than oblivious, more social than solitary.\n\nJames J. Kuffner, a former CMU robotics professor, and research scientist at Google, now CTO of Toyota Research Institute, spoke on cloud robotics in IEEE/RAS International Conference on Humanoid Robotics 2010. It describes \"a new approach to robotics that takes advantage of the Internet as a resource for massively parallel computation and sharing of vast data resources.\"\n\nRyan Hickman, a Google Product Manager, led an internal volunteer effort in 2010 to connect robots with the Google's cloud services.This work was later expanded to include open source ROS support and was demonstrated on stage by Ryan Hickman, Damon Kohler, Brian Gerkey, and Ken Conley at Google I/O 2011.\n\nThe IEEE RAS Technical Committee on Internet and Online Robots was founded by Ken Goldberg and Roland Siegwart et al. in May 2001. The committee then expanded to IEEE Society of Robotics and Automation's Technical Committee on Networked Robots in 2004.\n\nThe first industrial cloud robotics platform, Tend, was founded by Mark Silliman, James Gentes and Robert Kieffer in February 2017. Tend allows robots to be remotely controlled and monitored via websockets and NodeJs.\n\n\n", "id": "44628427", "title": "Cloud robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=47580669", "text": "Aurora (novel)\n\nAurora is a 2015 novel by American science fiction author Kim Stanley Robinson. The novel concerns a generation ship traveling to Tau Ceti in order to begin a human colony. The novel's primary narrating voice is the starship's artificial intelligence. The novel was well received by critics.\n\nA generation ship is launched from Saturn in 2545. It includes twenty-four self-contained biomes and an average population of two thousand people. One hundred sixty years and approximately seven generations later, it is beginning its approach to the Tau Ceti system to begin colonization of a planet's moon, an Earth analog, which has been named Aurora.\n\nDevi, the ship's \"de facto\" chief engineer and leader, is concerned about the decaying infrastructure and biology of the ship: systems are breaking down, each generation has lower intelligence test scores than the last, and bacteria are mutating and evolving at a faster rate than humans. She tells the ship's AI, referred to simply as Ship, to keep a narrative of the voyage. After having some trouble with understanding the human concept of narrative, Ship eventually elects to follow the life of Devi's daughter Freya as a protagonist.\n\nAs a teenager, Freya travels around the ship on her \"wanderjahr\" and learns that many of the ship's inhabitants are dissatisfied with their enclosed existence and what they perceive as a dictatorship. Movement is strictly limited for most people, reproduction is tightly controlled, and education in science and mathematics is mandatory. Freya's \"wanderjahr\" comes to an end when she is called home as Devi grows sick from cancer and dies.\n\nThe ship arrives in the Tau Ceti system and begins to settle Aurora, a moon of Tau Ceti e. It soon becomes apparent that extraterrestrial life is present in the form of primitive prions, which infect and kill most of the landing party. The surviving settlers attempt to return to the ship, and some of those remaining onboard kill them in the airlock to maintain quarantine, leading to a violent political schism throughout the ship. The ship itself, which has been moving towards self-awareness, takes physical control of the situation by lowering oxygen levels and separating warring factions, referring to itself as \"the rule of law\". It then reveals to the crew that there were in fact two ships originally launched for the Tau Ceti expedition, but the other was destroyed during a period of severe civil unrest, and the collective memory of that event was erased from the history records. Under Ship's moderation, a more peaceful debate takes place between the inhabitants about what to do now that Aurora is known to be inhospitable. Unable to reach consensus, the factions agree to part ways, with those who wish to stay retaining as many resources as can be spared to pursue an unlikely attempt at terraforming the Mars-like planet Iris, while the other group, led by Freya, opt to try and return to Earth. Freya and the others who return do not receive any communication from those who remained in the Tau Ceti system.\n\nOn the voyage back to Earth, the ship's biomes continue to deteriorate as bacteria flourish and crops fail. The humans soon face famine and experiment with an untested form of cryogenic freezing, which is largely successful. Upon returning to the Solar system, Ship is forced to decelerate by means of gravity assist between various planets, a process which takes twelve years. During this time, with the full communications data of humanity available to it, it learns more about why it was launched in the first place—simply for expansionism—and denounces its builders as \"criminally negligent narcissists\". Ship manages to safely drop its humans off on a pass of Earth but fails to make a final gravity slowdown past the Sun. Ship is destroyed along with the last survivor of the landing on Aurora.\n\nFreya and the other \"starfarers\" have trouble adjusting to life on Earth, especially with many Terrans hostile to them for a perceived sense of ingratitude and cowardice. At a space colonization conference, a speaker says humanity will continue to send ships into interstellar space no matter how many fail and die, and Freya assaults him. Eventually she joins a group of terraformers who are attempting to restore the Earth's beaches after their loss during previous centuries' sea level rise. While swimming and surfing, she begins to come to terms with life on Earth.\n\nMajor themes in \"Aurora\" include complexities of life aboard a multi-generational starship, interpersonal psychology, artificial intelligence, human migration, and the feasibility of star travel.\n\nRobinson says that in researching the novel he met with his friend Christopher McKay who has helped him since the \"Mars Trilogy\". McKay arranged lunches at the NASA Ames Research Center where Robinson asked questions of NASA employees.\n", "id": "47580669", "title": "Aurora (novel)"}
{"url": "https://en.wikipedia.org/wiki?curid=43097038", "text": "Behavior tree (artificial intelligence, robotics and control)\n\nA Behavior Tree (BT) is a mathematical model of plan execution used in computer science, robotics, control systems and video games. They describe switchings between a finite set of tasks in a modular fashion. Their strength comes from their ability to create very complex tasks composed of simple tasks, without worrying how the simple tasks are implemented. BTs present some similarities to hierarchical state machines with the key difference that the main building block of a behavior is a task rather than a state. Its ease of human understanding make BTs less error prone and very popular in the game developer community. BTs have shown to generalize several other control architectures. \n\nBTs originates from the computer game industry as a powerful tool to model the behavior of non-player characters (NPCs).\nThey have been extensively used in high-profile video games such as Halo, Bioshock, and Spore. Recent works propose BTs as a multi-mission control framework for UAV, complex robots, robotic manipulation, and multi-robot systems.\nBT have now reached the maturity to be treated in Game AI textbooks \nas well as generic game environments such as PyGame and Unreal Engine (see links below).\n\nIt has been shown that BTs generalize a number of earlier control architectures, such as\n\nBT is graphically represented as a directed tree in which the nodes are classified as root, control flow nodes, or execution nodes (tasks). For each pair of connected nodes the outgoing node is called parent and the incoming node is called child. The root has no parents and exactly one child, the control flow nodes have one parent and at least one child, and the execution nodes have one parent and no children. Graphically, the children of a control flow node are placed below it, ordered from left to right.\n\nThe execution of a BT starts from the root which sends ticks with a certain frequency to its child. A tick is an enabling signal that allows the execution of a child. When the execution of a node in the BT is allowed, it returns to the parent a status running if its execution has not finished yet, success if it has achieved its goal, or failure otherwise.\n\nA control flow node is used to control the subtasks of which it is composed. A control flow node may be either a selector (fallback) node or a sequence node. They run each of their subtasks in turn. When a subtask is completed and returns its status (success or failure), the control flow node decides whether to execute the next subtask or not.\n\nFallback nodes are used to find and execute the first child that does not fail. A fallback node will return immediately with a status code of success or running when one of its children returns success or running (see Figure I and the pseudocode below). The children are ticked in order of importance, from left to right.\n\nIn pseudocode, the algorithm for a fallback composition is:\n\nSequence nodes are used to find and execute the first child that has not yet succeeded. A sequence node will return immediately with a status code of failure or running when one of its children returns failure or running (see Figure II and the pseudocode below). The children are ticked in order, from left to right.\n\nIn pseudocode, the algorithm for a sequence composition is:\n\nIn order to apply control theory tools to the analysis of Behavior Trees,\nBT can be defined as three-tuple.\n\nformula_1\n\nwhere formula_2 is the index of the tree, formula_3 is a vector field representing the right hand side of an ordinary difference equation, formula_4 is a time step and \nformula_5 is the return status, that can be equal to either \nRunning formula_6,\nSuccess formula_7, or\nFailure formula_8.\n\nNote: A task is degenerate BT with no parent and no child.\n\nThe execution of a BT is described by the following standard ordinary difference equations:\n\nformula_9\n\nformula_10\n\nwhere formula_11 represent the discrete time, and formula_12 is the state space of the system modelled by the behavior tree.\n\nTwo BTs formula_13 and formula_14 can be composed into a more complex BT formula_15 using a Fallback operator.\n\nformula_16\n\nThen return status formula_17 and the vector field formula_18 associated with formula_15 are defined as follows:\n\nformula_20\n\nformula_21\n\nTwo BTs formula_13 and formula_14 can be composed into a more complex BT formula_15 using a Sequence operator.\n\nformula_25\n\nThen return status formula_17 and the vector field formula_18 associated with formula_15 are defined as follows:\n\nformula_29\n\nformula_30\n\n\n\n", "id": "43097038", "title": "Behavior tree (artificial intelligence, robotics and control)"}
{"url": "https://en.wikipedia.org/wiki?curid=9583985", "text": "Committee machine\n\nA committee machine is a type of artificial neural network using a divide and conquer strategy in which the responses of multiple neural networks (experts) are combined into a single response. The combined response of the committee machine is supposed to be superior to those of its constituent experts. Compare with ensembles of classifiers.\n\nIn this class of committee machines, the responses of several predictors (experts) are combined by means of a mechanism that does not involve the input signal, hence the designation static. This category includes the following methods:\nIn ensemble averaging, outputs of different predictors are linearly combined to produce an overall output.\nIn boosting, a weak algorithm is converted into one that achieves arbitrarily high accuracy.\n\nIn this second class of committee machines, the input signal is directly involved in actuating the mechanism that integrates the outputs of the individual experts into an overall output, hence the designation dynamic. There are two kinds of dynamic structures:\nIn mixture of experts, the individual responses of the experts are non-linearly combined by means of a single gating network.\nIn hierarchical mixture of experts, the individual responses of the individual experts are non-linearly combined by means of several gating networks arranged in a hierarchical fashion.\n", "id": "9583985", "title": "Committee machine"}
{"url": "https://en.wikipedia.org/wiki?curid=25340765", "text": "Emily Howell\n\nEmily Howell is a computer program created by David Cope, a music professor at the University of California, Santa Cruz. Emily Howell is an interactive interface that \"hears\" feedback from listeners, and builds its own musical compositions from a source database, derived from a previous composing program called Experiments in Musical Intelligence (EMI). Cope attempts to “teach” the program by providing feedback so that it can cultivate its own \"personal\" style. The software appears to be based on latent semantic analysis.\n\nEmily Howell’s first album was released in February 2009 by Centaur Records (CRC 3023). Titled \"From Darkness, Light\", this album contains her Opus 1, Opus 2, and Opus 3 compositions for chamber orchestra and multiple pianos. Her second album \"Breathless\" was released in December 2012 by Centaur Records (CRC 3255).\n\n\n\n", "id": "25340765", "title": "Emily Howell"}
{"url": "https://en.wikipedia.org/wiki?curid=21659435", "text": "Music and artificial intelligence\n\nResearch in artificial intelligence (AI) is known to have impacted medical diagnosis, stock trading, robot control, and several other fields. Perhaps less popular is the contribution of AI in the field of music. Nevertheless, artificial intelligence and music (AIM) has, for a long time, been a common subject in several conferences and workshops, including the International Computer Music Conference, the Computing Society Conference and the International Joint Conference on Artificial Intelligence. In fact, the first International Computer Music Conference was the ICMC 1974, Michigan State University, East Lansing, USA \nCurrent research includes the application of AI in music composition, performance, theory and digital sound processing. Several music software applications have been developed that use AI to produce music. A few examples are included below. Note that there are many that are still being developed.\n\nIn 1960, Russian researcher R.Kh.Zaripov published worldwide first paper on algorithmic music composing using the \"Ural-1\" computer.\n\nIn 1965, inventor Ray Kurzweil premiered a piano piece created by a computer that was capable of pattern recognition in various compositions. The computer was then able to analyze and use these patterns to create novel melodies. The computer was debuted on Steve Allen's I've Got a Secret program, and stumped the hosts until film star Henry Morgan guessed Ray's secret.\n\nA program developed by Hexachords and directed by Richard Portelli, mainly focused on orchestral music.\n\nA program developed by David Cope which composes classical music. See Experiments in Musical Intelligence. Emily Howell is an interactive augmentation of EMI. (As a popular example, the background music of the viral video Humans Need Not Apply was created by \"her\", as revealed in the video to illustrate the likely fate of creative jobs.)\n\nThis program was designed to provide small-budget productions with instrumentation for all instruments usually present in the full-fledged orchestra. If there is a small orchestra playing, the program can play the part for missing instruments. High school and community theaters wanting to produce a musical can now benefit from the virtual orchestra and realize a full Broadway score. This software is able to follow the fluctuations in tempo and musical expression. Musicians enjoy the thrill of playing with a full orchestra, while the audience enjoys the rich sound that comes from the combination of the virtual orchestra with the musicians.\n\nDemo:\n\nThe Computer Music Project at CMU develops computer music and interactive performance technology to enhance human musical experience and creativity. This interdisciplinary effort draws on Music Theory, Cognitive Science, Artificial Intelligence and Machine Learning, Human Computer Interaction, Real-Time Systems, Computer Graphics and Animation, Multimedia, Programming Languages, and Signal Processing. One of their project is similar to SmartMusic. It provides accompaniment for the chosen piece follows the soloist (user) despite tempo changes and/or mistakes.\n\nDemo:\n\nSmartMusic is an interactive, computer-based practice tool for musicians. It offers exercises, instant feedback tools, and accompaniments meant to aid musicians. The product is targeted at teachers and students alike and offers five categories of accompaniments: solo, skill development, method books, jazz, and ensemble. Teachers can give students pre-defined assignments via email and scan in sheet music that is not yet in the SmartMusic catalog. Students can choose the difficulty level they want to play at, slow down or speed up the tempo, or change the key in which to play the piece. SmartMusic also compares students' playing with digital template, which allows it to detect mistakes and mark them on a score. It also simulates the rapport between musicians by sensing and reacting to tempo changes.\n\nStarPlay is also a music education software that allows the user to practice by performing with professional musicians, bands and orchestras. They can choose their spot and watch the video from that spot. They can hear the other musicians playing. Again, the program listens to the user's performance and helps them improve their performance by providing constructive feedback as they rehearse. StarPlay was developed by StarPlayIt (formerly In The Chair), a music technology company that has won many awards for its platforms for online musical performance and participation.\n\nDeveloped at Princeton University by Ge Wang and Perry Cook, ChucK is a text-based, cross-platform language that allows real-time synthesis, composition, performance and analysis of music.\n. It is used by SLOrk (Stanford Laptop Orchestra) and PLOrk (Princeton Laptop Orchestra).\n\nDemo:\n\nThe Impromptu media programming environment was developed by Andrew Sorensen for exploring 'intelligent' interactive music and visual systems. Impromptu is used for live coding performances and research including generative orchestral music and computational models of music perception.\n\nMIDI to string instrument (guitar, violin, dombra, etc.) tablature conversion is a nontrivial task, as the same note can reside on different strings of the instrument. And the creation of good fingering is sometimes a challenge even for real musicians, especially when translating a two handed piano composition on a string instrument.\nSo in TabEditor (the tiny plugin for REAPER DAW), an AI was used that solves this puzzle the same way as a musician would: trying to keep all the notes close to each other (to be possible to play) while trying to fit all the piano notes into a range that can be played simultaneously on the instrument. When direct translation is impossible (piano part has more notes than are possible on the guitar) the AI tries to find an acceptable solution, removing as few notes as possible from the original composition.\nThe Prolog programming language was used to create this AI.\n\nLudwig is an automated composition software based on tree search algorithms. Ludwig generates melodies according to principles of classical music theory. The software arranges its melodies with pop-automation patterns or in four-part choral writing. Ludwig can react in real-time on an eight-bar theme played on a keyboard. The theme will be analysed for key, harmonic content and rhythm while it is being performed by a human. The program then without delay repeats the theme arranged e.g. for orchestra. It subsequently varies the melody to create a little piece as interactive answer to the human input.\n\nOMax is a software environment which learns in real-time typical features of a musician's style and plays along with him interactively, giving the flavor of a machine co-improvisation. OMax uses OpenMusic and Max. It is based on researches on stylistic modeling carried out by Gerard Assayag and Shlomo Dubnov and on researches on improvisation with the computer by G. Assayag, M. Chemillier and G. Bloch (Aka the OMax Brothers) in the Ircam Music Representations group.\n\nMelomics is a proprietary computational system for the automatic (without human intervention) composition of music, based on bioinspired methods and produced by Melomics Media. Composing a wide variety of genres, all music composed by Melomics algorithms are available in MP3, MIDI, MusicXML, and PDF (of sheet music), after purchase. Music composed by this algorithm was organized into an album named Iamus (album), which was hailed by New Scientist as \"The first complete album to be composed solely by a computer and recorded by human musicians.\"\n\nMorpheuS is a research project by Dorien Herremans and Elaine Chew at Queen Mary University of London, funded by a Marie Skłodowská-Curie EU project. The system uses an optimization approach based on a variable neighborhood search algorithm to morph existing template pieces into novel pieces with a set level of tonal tension that changes dynamically throughout the piece. This optimization approach allows for the integration of a pattern detection technique in order to enforce long term structure and recurring themes in the generated music. Pieces composed by MorpheuS have been performed at concerts in both Stanford and London.\n\nFlow Machines is a research project funded by the European Research Council (ERC) and led by François Pachet. Flow Machines aims at transforming musical style into a computational object to apply to AI-generated melodies and harmonies. Flow Machines has composed two fully-fledged pop songs, issued from a collaboration between the AI software and pop composer Benoît Carré: Daddy's Car and Mister Shadow.\nFlow Machines also produced DeepBach, a neural network system which produces harmonisation in Bach style indiscernible from original Bach's harmonisations.\n\nCreated in February 2016, AIVA specializes in classical and symphonic music composition. It became the world’s first virtual composer to be recognized by a music society (SACEM). By reading a large collection of existing works of classical music (written by human composers such as Bach, Beethoven, Mozart) AIVA is capable of understanding concepts of music theory and composing on its own. The algorithm AIVA is based on deep learning and reinforcement learning architectures\n\n\n\n", "id": "21659435", "title": "Music and artificial intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=47332350", "text": "DeepDream\n\nDeepDream is a computer vision program created by Google engineer Alexander Mordvintsev which uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dream-like hallucinogenic appearance in the deliberately over-processed images.\n\nGoogle's program popularized the term (deep) \"dreaming\" to refer to the generation of images that produce desired activations in a trained deep network, and the term now refers to a collection of related approaches.\n\nThe DeepDream software originates in a deep convolutional network codenamed \"Inception\" after the film of the same name, was developed for the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) in 2014 and released in July 2015.\n\nThe software is designed to detect faces and other patterns in images, with the aim of automatically classifying images. However, once trained, the network can also be run in reverse, being asked to adjust the original image slightly so that a given output neuron (e.g. the one for faces or certain animals) yields a higher confidence score. This can be used for visualizations to understand the emergent structure of the neural network better, and is the basis for the DeepDream concept. However, after enough reiterations, even imagery initially devoid of the sought features will be adjusted enough that a form of pareidolia results, by which psychedelic and surreal images are generated algorithmically. The optimization resembles Backpropagation, however instead of adjusting the network weights, the weights are held fixed and the input is adjusted.\n\nFor example, an existing image can be altered so that it is \"more cat-like\", and the resulting enhanced image can be again input to the procedure. This usage resembles the activity of looking for animals or other patterns in clouds.\n\nApplying gradient descent independently to each pixel of the input produces images in which\nadjacent pixels have little relation and thus the image has too much high frequency information.\nThe generated images can be greatly improved by including a prior or regularizer that prefers inputs\nthat have natural image statistics (without a preference for any particular image), or are simply smooth.\nFor example, used the total variation regularizer that prefers images that are piecewise constant. Various regularizers are discussed further in. An in-depth, visual exploration of feature visualization and regularization techniques was published more recently.\n\nThe dreaming idea and name became popular on the internet in 2015 thanks to Google's DeepDream program. The idea dates from early in the history of neural networks, and similar methods have been used to synthesize visual textures.\nRelated visualization ideas were developed (prior to Google's work) by several research groups.\n\nAfter Google published their techniques and made their code open source, a number of tools in the form of web services, mobile applications, and desktop software appeared on the market to enable users to transform their own photos.\n\nThe dreaming idea can be applied to hidden (internal) neurons other than those in the output, \nwhich allows exploration of the roles and representations of various parts of the network.\nIt is also possible to optimize the input to satisfy either a single neuron (this usage is sometimes called Activity Maximization) or an entire layer of neurons.\n\nWhile dreaming is most often used for visualizing networks or producing computer art, it has recently been proposed that adding \"dreamed\" inputs to the training set can improve training times for abstractions in Computer Science.\n\nThe cited resemblance of the imagery to LSD- and psilocybin-induced hallucinations is suggestive of a functional resemblance between artificial neural networks and particular layers of the visual cortex.\n\nDeepDream was used for Foster the People's music video for the song \"Doing it for the money\".\n\n\n", "id": "47332350", "title": "DeepDream"}
{"url": "https://en.wikipedia.org/wiki?curid=48653319", "text": "Artificial intelligence for video surveillance\n\nArtificial intelligence for video surveillance utilizes computer software programs that analyze the images from video surveillance cameras in order to recognize humans, vehicles or objects. Security contractors program the software to define restricted areas within the camera's view (such as a fenced off area, a parking lot but not the sidewalk or public street outside the lot) and program for times of day (such as after the close of business) for the property being protected by the camera surveillance. The artificial intelligence (\"A.I.\") sends an alert if it detects a trespasser breaking the \"rule\" set that no person is allowed in that area during that time of day.\n\nThe A.I. program functions by using machine vision. Machine vision is a series of algorithms, or mathematical procedures, which work like a flow-chart or series of questions to compare the object seen with hundreds of thousands of stored reference images of humans in different postures, angles, positions and movements. The A.I. asks itself if the observed object moves like the reference images, whether it is approximately the same size height relative to width, if it has the characteristic two arms and two legs, if it moves with similar speed, and if it is vertical instead of horizontal. Many other questions are possible, such as the degree to which the object is reflective, the degree to which it is steady or vibrating, and the smoothness with which it moves. Combining all of the values from the various questions, an overall ranking is derived which gives the A.I. the probability that the object is or is not a human. If the value exceeds a limit that is set, then the alert is sent. It is characteristic of such programs that they are self-learning to a degree, learning, for example that humans or vehicles appear bigger in certain portions of the monitored image – those areas near the camera – than in other portions, those being the areas farthest from the camera.\n\nIn addition to the simple rule restricting humans or vehicles from certain areas at certain times of day, more complex rules can be set. The user of the system may wish to know if vehicles drive in one direction but not the other. Users may wish to know that there are more than a certain preset number of people within a particular area. The A.I. is capable of maintaining surveillance of hundreds of cameras simultaneously. Its ability to spot a trespasser in the distance or in rain or glare is superior to humans' ability to do so.\n\nThis type of A.I. for security is known as \"rule-based\" because a human programmer must set rules for all of the things for which the user wishes to be alerted. This is the most prevalent form of A.I. for security. Many video surveillance camera systems today include this type of A.I. capability. The hard-drive that houses the program can either be located in the cameras themselves or can be in a separate device that receives the input from the cameras.\n\nA newer, non-rule based form of A.I. for security called \"behavioral analytics\" has been developed. This software is fully self-learning with no initial programming input by the user or security contractor. In this type of analytics, the A.I. learns what is normal behavior for people, vehicles, machines, and the environment based on its own observation of patterns of various characteristics such as size, speed, reflectivity, color, grouping, vertical or horizontal orientation and so forth. The A.I. normalizes the visual data, meaning that it classifies and tags the objects and patterns it observes, building up continuously refined definitions of what is normal or average behavior for the various observed objects. After several weeks of learning in this fashion it can recognize when things break the pattern. When it observes such anomalies it sends an alert. For example, it is normal for cars to drive in the street. A car seen driving up onto a sidewalk would be an anomaly. If a fenced yard is normally empty at night, then a person entering that area would be an anomaly.\n\nLimitations in the ability of humans to vigilantly monitor video surveillance live footage led to the demand for artificial intelligence that could better serve the task. Humans watching a single video monitor for more than twenty minutes lose 95% of their ability to maintain attention sufficient to discern significant events. With two monitors this is cut in half again. Given that many facilities have dozens or even hundreds of cameras, the task is clearly beyond human ability. In general, the camera views of empty hallways, storage facilities, parking lots or structures are exceedingly boring and thus attention is quickly attenuated. When multiple cameras are monitored, typically employing a wall monitor or bank of monitors with split screen views and rotating every several seconds between one set of cameras and the next, the visual tedium is quickly overwhelming. While video surveillance cameras proliferated with great adoption by users ranging from car dealerships and shopping plazas to schools and businesses to highly secured facilities such as nuclear plants, it was recognized in hindsight that video surveillance by human officers (also called \"operators\") was impractical and ineffective. Extensive video surveillance systems were relegated to merely recording for possible forensic use to identify someone, after the fact of a theft, arson, attack or incident. Where wide angle camera views were employed, particularly for large outdoor areas, severe limitations were discovered even for this purpose due to insufficient resolution. In these cases it is impossible to identify the trespasser or perpetrator because their image is too tiny on the monitor.\n\nIn response to the shortcomings of human guards to watch surveillance monitors long-term, the first solution was to add motion detectors to cameras. It was reasoned that an intruder's or perpetrator's motion would send an alert to the remote monitoring officer obviating the need for constant human vigilance. The problem was that in an outdoor environment there is constant motion or changes of pixels that comprise the total viewed image on screen. The motion of leaves on trees blowing in the wind, litter along the ground, insects, birds, dogs, shadows, headlights, sunbeams and so forth all comprise motion. This caused hundreds or even thousands of false alerts per day, rendering this solution inoperable except in indoor environments during times of non-operating hours.\n\nThe next evolution reduced false alerts to a degree but at the cost of complicated and time-consuming manual calibration. Here, changes of a target such as a person or vehicle relative to a fixed background are detected. Where the background changes seasonally or due to other changes, the reliability deteriorates over time. The economics of responding to too many false alerts again proved to be an obstacle and this solution was insufficient.\n\nMahesh Saptharishi did his doctoral work in machine learning. Machine learning of visual recognition relates to patterns and their classification. Recognizing that advances in machine vision to aid robots in interpreting visual data much as humans do could be applied to the security industry requisite of surveillance, he pioneered the development of video analytics, co-founding Broad Reach Technologies in 2000. Following the events of 9/11, the federal government sought a real-time solution to detect intruders. General Electric provided investment to Broad Reach in R&D in 2002. Saptharishi continued his work and co-founded VideoIQ, Inc. in 2007, making the software commercially available to the general market. The company was bought by the large security company Avigilon in 2014, with Saptharishi becoming Chief Technology Officer for the company. True video analytics can distinguish the human form, vehicles and boats or selected objects from the general movement of all other objects and visual static or changes in pixels on the monitor. It does this by recognizing patterns. When the object of interest, for example a human, violates a preset rule, for example that the number of people shall not exceed zero in a pre-defined area during a defined time interval, then an alert is sent. A red rectangle or so-called \"bounding box\" will typically automatically follow the detected intruder, and a short video clip of this is sent as the alert.\n\nThe detection of intruders using video surveillance has limitations based on economics and the nature of video cameras. Typically, cameras outdoors are set to a wide angle view and yet look out over a long distance. Frame rate per second and dynamic range to handle brightly lit areas and dimly lit ones further challenge the camera to actually be adequate to see a moving human intruder. At night, even in illuminated outdoor areas, a moving subject does not gather enough light per frame per second and so, unless quite close to the camera, will appear as a thin wisp or barely discernible ghost or completely invisible. Conditions of glare, partial obscuration, rain, snow, fog, and darkness all compound the problem. Even when a human is directed to look at the actual location on a monitor of a subject in these conditions, the subject will usually not be detected. The A.I. is able to impartially look at the entire image and all cameras' images simultaneously. Using statistical models of degrees of deviation from its learned pattern of what constitutes the human form it will detect an intruder with high reliability and a low false alert rate even in adverse conditions. Its learning is based on approximately a quarter million images of humans in various positions, angles, postures, and so forth.\n\nThe one megapixel VideoIQ camera with the onboard video analytics was able to detect a human at a distance of about 350' and an angle of view of about 30 degrees in non-ideal conditions. Rules could be set for a \"virtual fence\" or intrusion into a pre-defined area. Rules could be set for directional travel, object left behind, crowd formation and some other conditions.\n\nOne of the most powerful features of the system is that a human officer or operator, receiving an alert from the A.I., could immediately talk down over outdoor public address loudspeakers to the intruder. This had high deterrence value as most crimes are opportunistic and the risk of capture to the intruder becomes so pronounced when a live person is talking to them that they are very likely to desist from intrusion and to retreat. The security officer would describe the actions of the intruder so that the intruder had no doubt that a real person was watching them. The officer would announce that the intruder was breaking the law and that law enforcement was being contacted and that they were being video-recorded.\n\nThe police receive a tremendous number of false alarms from burglar alarms. In fact the security industry reports that over 98% of such alarms are false ones. Accordingly, the police give very low priority response to burglar alarms and can take from twenty minutes to two hours to respond to the site. By contrast, the video analytic-detected crime is reported to the central monitoring officer, who verifies with his or her own eyes that it is a real crime in progress. He or she then dispatches to the police who give such calls their highest priority.\n\nAnalytics work with digital cameras or analog cameras that have analog-to-digital converters. Various companies aside from VideoIQ and Avigilon offer video analytics. In many cases the software samples down to standard definition or less, regardless of the camera's resolution. The advance achieved by Avigilon is true analytics on up to exceedingly high-resolution 30 megapixel cameras.\n\nWhile rule-based video analytics worked economically and reliably for many security applications there are many situations in which it cannot work. For an indoor or outdoor area where no one belongs during certain times of day, for example overnight, or for areas where no one belongs at any time such as a cell tower, traditional rule-based analytics are perfectly appropriate. In the example of a cell tower the rare time that a service technician may need to access the area would simply require calling in with a pass-code to put the monitoring response \"on test\" or inactivated for the brief time the authorized person was there.\n\nBut there are many security needs in active environments in which hundreds or thousands of people belong all over the place all the time. For example, a college campus, an active factory, a hospital or any active operating facility. It is not possible to set rules that would discriminate between legitimate people and criminals or wrong-doers.\n\nDeveloped by the team led by Dr. Wes Cobb of Behavioral Recognition Systems, Inc., (BRS Labs) this self-learning, non-rule-based A.I. software takes the data from video cameras and continuously classifies objects and events that it sees. The system is called AISight (as in \"A.I. Sight\"). It has been deployed by government agencies, transit authorities, police departments and is rolling into commercial use. For example, a person crossing a street is one classification. A group of people is another classification. A vehicle is one classification, but with continued learning a public bus would be discriminated from a small truck and that from a motorcycle. With increasing sophistication, the system recognizes patterns in human behavior. For example, it might observe that individuals pass through a controlled access door one at a time. The door opens, the person presents their proximity card or tag, the person passes through and the door closes. This pattern of activity, observed repeatedly, forms a basis for what is normal in the view of the camera observing that scene. Now if an authorized person opens the door but a second \"tail-gating\" unauthorized person grabs the door before it closes and passes through, that is the sort of anomaly that would create an alert. This type of analysis is much more complex than the rule-based analytics. While the rule-based analytics work mainly to detect intruders into areas where no one is normally present at defined times of day, the behavioral analytics works where people are active to detect things that are out of the ordinary.\n\nA fire breaking out outdoors would be an unusual event and would cause an alert, as would a rising cloud of smoke. Vehicles driving the wrong way into a one-way driveway would also typify the type of event that has a strong visual signature and would deviate from the repeatedly observed pattern of vehicles driving the correct one-way in the lane. Someone thrown to the ground by an attacker would be an unusual event that would likely cause an alert. This is situation-specific. So if the camera viewed a gymnasium where wrestling was practiced the A.I. would learn it is usual for one human to throw another to the ground, in which case it would not alert on this observation.\n\nThe A.I. does not know or understand what a human is, or a fire, or a vehicle. It is simply finding characteristics of these things based on their size, shape, color, reflectivity, angle, orientation, motion, and so on. It then finds that the objects it has classified have typical patterns of behavior. For example, humans walk on sidewalks and sometimes on streets but they don't climb up the sides of buildings very often. Vehicles drive on streets but don't drive on sidewalks. Thus the anomalous behavior of someone scaling a building or a vehicle veering onto a sidewalk would trigger an alert.\n\nTypical alarm systems are designed to not miss true positives (real crime events) and to have as low of a false alarm rate as possible. In that regard, burglar alarms miss very few true positives but have a very high false alarm rate even in the controlled indoor environment. Motion detecting cameras miss some true positives but are plagued with overwhelming false alarms in an outdoor environment. Rule-based analytics reliably detect most true positives and have a low rate of false positives but cannot perform in active environments, only in empty ones. Also they are limited to the simple discrimination of whether an intruder is present or not.\n\nSomething as complex or subtle as a fight breaking out or an employee breaking a safety procedure is not possible for a rule based analytics to detect or discriminate. With behavioral analytics, it is. Places where people are moving and working do not present a problem. However, the A.I. may spot many things that appear anomalous but are innocent in nature. For example, if students at a campus walk on a plaza, that will be learned as normal. If a couple of students decided to carry a large sheet outdoors flapping in the wind, that might indeed trigger an alert. The monitoring officer would be alerted to look at his or her monitor and would see that the event is not a threat and would then ignore it. The degree of deviation from norm that triggers an alert can be set so that only the most abnormal things are reported. However, this still constitutes a new way of human and A.I. interaction not typified by the traditional alarm industry mindset. This is because there will be many false alarms that may nevertheless be valuable to send to a human officer who can quickly look and determine if the scene requires a response. In this sense, it is a \"tap on the shoulder\" from the A.I. to have the human look at something.\n\nBecause so many complex things are being processed continuously, the software samples down to the very low resolution of only 1 CIF to conserve computational demand. The 1 CIF resolution means that an object the size of a human will not be detected if the camera utilized is wide angle and the human is more than sixty to eighty feet distant depending on conditions. Larger objects like vehicles or smoke would be detectable at greater distances.\n\nThe utility of artificial intelligence for security does not exist in a vacuum, and its development was not driven by purely academic or scientific study. Rather, it is addressed to real world needs, and hence, economic forces. Its use for non-security applications such as operational efficiency, shopper heat-mapping of display areas (meaning how many people are in a certain area in a retail space), and attendance at classes are developing uses. Humans are not as well qualified as A.I. to compile and recognize patterns consisting of very large data sets requiring simultaneous calculations in multiple remote viewed locations. There is nothing natively human about such awareness. Such multi-tasking has been shown to defocus human attention and performance. A.I.s have the ability to handle such data. For the purposes of security interacting with video cameras they functionally have better visual acuity than humans or the machine approximation to it. For judging subtleties of behaviors or intentions of subjects or degrees of threat, humans remain far superior at the present state of the technology. So the A.I. in security functions to broadly scan beyond human capability and to vet the data to a first level of sorting of relevance and to alert the human officer who then takes over the function of assessment and response.\n\nSecurity in the practical world is economically determined so that the expenditure of preventative security will never typically exceed the perceived cost of the risk to be avoided. Studies have shown that companies typically only spend about one twenty-fifth the amount on security that their actual losses cost them. What by pure economic theory should be an equivalence or homeostasis, thus falls vastly short of it. One theory that explains this is cognitive dissonance, or the ease with which unpleasant things like risk can be shunted from the conscious mind. Nevertheless, security is a major expenditure, and comparison of the costs of different means of security is always foremost amongst security professionals.\n\nAnother reason that future security threats or losses are under-assessed is that often only the direct cost of a potential loss is considered instead of the spectrum of consequential losses that are concomitantly experienced. For example, the vandalism-destruction of a custom production machine in a factory or of a refrigerated tractor trailer would result in a long replacement time during which customers could not be served, resulting in loss of their business. A violent crime will have extensive public relations damage for an employer, beyond the direct liability for failing to protect the employee.\n\nBehavioral analytics uniquely functions beyond simple security and, due to its ability to observe breaches in standard patterns of protocols, it can effectively find unsafe acts of employees that may result in workers comp or public liability incidents. Here too, the assessment of future incidents' costs falls short of the reality. A study by Liberty Mutual Insurance Company showed that the cost to employers is about six times the direct insured cost, since uninsured costs of consequential damages include temporary replacement workers, hiring costs for replacements, training costs, managers' time in reports or court, adverse morale on other workers, and effect on customer and public relations. The potential of A.I. in the form of behavioral analytics to proactively intercept and prevent such incidents is significant.\n\nA case study by VideoIQ at two car dealerships and other collected surveys at a range of industry verticals yielded the approximation that rule-based analytics will protect an area for one-fifth the cost of security officers (guards). An analysis of situational awareness created by non-rule-based behavioral analytics in several university campus scenarios, where the cost of on-site patrolling and command station officers is quite high, showed that a given percentage of increase in situational awareness, i.e., the ability to have significant incidents brought to attention in real-time, could be achieved by adding the A.I. software to existing video camera infrastructure for anywhere from one-twentieth the cost to one-fiftieth the cost of adding more officers to achieve the same result.\n\n", "id": "48653319", "title": "Artificial intelligence for video surveillance"}
{"url": "https://en.wikipedia.org/wiki?curid=48760514", "text": "Roborace\n\nRoborace will be a motorsport championship with autonomously driving, electrically powered vehicles. The series will be held on the same tracks the FIA Formula E Championship uses. It will be the first global championship for driverless cars. As of September 2017, the official CEO is 2016–17 Formula E champion, Lucas Di Grassi.\n\nThe first race was intended to take place during the 2016–17 Formula E season. Ten teams, each with two driverless cars, will compete in one-hour races over the full season. All teams will have equal cars, but will have to develop their own real-time computing algorithms and artificial intelligence technologies.\n\nFor the first season, all teams will be supplied with an electric racing car built by \"Kinetik\", called the \"Robocar\". The chassis was designed by Daniel Simon, who previously worked on vehicles for movies such as \"\" and \"Oblivion\", along with painting the 2011 HRT Formula One Car. Michelin will be the official tyre supplier, with the internal computing units (Drive PX 2) being provided by Nvidia.\n\nThe chassis itself is shaped similar to a teardrop, improving aerodynamic efficiency. The Car weighs around 1000kg and is 4.8m(~15.7ft) long and 2m(~6.6ft) wide. It has four electric motors, each with a power of 300kW and possesses a 540kWh battery. For navigation, it relies on a mixture of optical systems, radars, lidars and ultrasonic sensors. During the first Roborace season torque vectoring will be allowed. A small rear wing at the rear end of the car indicates one or more powerful electric motors. Kinetik expects the cars to be capable of reaching top speeds of more than .\n\nDevelopment of the Robocar started in early 2016, with a first outing of a test vehicle, the so-called \"DevBot\", following in the summer of the same year. The test car consisted of the same internal units (battery, motor, electronics) used in the Robocar, but were placed in the chassis of a LMP3 Ginetta without an engine cover in order to provide better cooling.\n\nDevBot saw its first public outing at the Formula E pre-season tests in Donington Park in August 2016. After battery issues in Hong Kong caused the development team to abandon their demonstration run, the DevBot successfully drove twelve laps around the Moulay El Hassan Formula E circuit in Marrakesh.\n\nDuring testing ahead of the 2017 Buenos Aires ePrix, two Devbot cars raced against each other. One successfully avoided a dog that ran onto the course. The other car crashed on a corner.\n\nOther test tracks included Michelin's testing ground in Ladoux and the Silverstone Stowe Circuit. Further test runs are planned for the following Formula E events.\n\nTen teams will feature two cars each on the 20-car grid in the inaugural season of Roborace. At the moment, it is not known which teams will join the series. At least one of the ten squads, however, is set to be a \"Crowd Sourced Community\" team, backed by private investors and open for AI developers from around the globe.\n", "id": "48760514", "title": "Roborace"}
{"url": "https://en.wikipedia.org/wiki?curid=48791499", "text": "Dynamic epistemic logic\n\nDynamic epistemic logic (DEL) is a logical framework dealing with knowledge and information change\".\" Typically, DEL focuses on situations involving multiple agents and studies how their knowledge changes when events occur. These events can change factual properties of the actual world (they are called \"ontic events\"): for example a red card is painted in blue. They can also bring about changes of knowledge without changing factual properties of the world (they are called \"epistemic events\"): for example a card is revealed publicly (or privately) to be red. Originally, DEL focused on epistemic events. We only present in this entry some of the basic ideas of the original DEL framework; more details about DEL in general can be found in the references.\n\nDue to the nature of its object of study and its abstract approach, DEL is related and has applications to numerous research areas, such as computer science (artificial intelligence), philosophy (formal epistemology), economics (game theory) and cognitive science. In computer science, DEL is for example very much related to multi-agent systems, which are systems where multiple intelligent agents interact and exchange information.\n\nAs a combination of dynamic logic and epistemic logic, dynamic epistemic logic is a young field of research. It really started in 1989 with Plaza’s logic of public announcement. Independently, Gerbrandy and Groeneveld proposed a system dealing moreover with private announcement and that was inspired by the work of Veltman. Another system was proposed by van Ditmarsch whose main inspiration was the Cluedo game. But the most influential and original system was the system proposed by Baltag, Moss and Solecki. This system can deal with all the types of situations studied in the works above and its underlying methodology is conceptually grounded. We will present in this entry some of its basic ideas.\n\nFormally, DEL extends ordinary epistemic logic by the inclusion of event models to describe actions, and a product update operator that defines how epistemic models are updated as the consequence of executing actions described through event models. Epistemic logic will first be recalled. Then, actions and events will enter into the picture and we will introduce the DEL framework.\n\nEpistemic logic is a modal logic dealing with the notions of knowledge and belief. As a logic, it is concerned with understanding the process of \"reasoning\" about knowledge and belief: which principles relating the notions of knowledge and belief are intuitively plausible? Like epistemology, it stems from the Greek word formula_1 or ‘episteme’ meaning knowledge. Epistemology is nevertheless more concerned with analyzing the very \"nature\" and \"scope\" of knowledge, addressing questions such as “What is the definition of knowledge?” or “How is knowledge acquired?”. In fact, epistemic logic grew out of epistemology in the Middle Ages thanks to the efforts of Burley and Ockham. The formal work, based on modal logic, that inaugurated contemporary research into epistemic logic dates back only to 1962 and is due to Hintikka. It then sparked in the 1960s discussions about the principles of knowledge and belief and many axioms for these notions were proposed and discussed. For example, the interaction axioms formula_2 and formula_3 are often considered to be intuitive principles: if an agent Knows formula_4 then (s)he also Believes formula_4, or if an agent Believes formula_4, then (s)he Knows that (s)he Believes formula_4. More recently, these kinds of philosophical theories were taken up by researchers in economics, artificial intelligence and theoretical computer science where reasoning about knowledge is a central topic. Due to the new setting in which epistemic logic was used, new perspectives and new features such as computability issues were then added to the research agenda of epistemic logic.\n\nIn the sequel, formula_8 is a finite set whose elements are called agents and formula_9 is a set of propositional letters.\n\nThe epistemic language is an extension of the basic multi-modal language of modal logic with a common knowledge operator formula_10 and a distributed knowledge operator formula_11. Formally, the epistemic language formula_12 is defined inductively by the following grammar in BNF:\n\nformula_13\n\nwhere formula_14, formula_15 and formula_16. The basic epistemic language formula_17 is the language formula_18 without the common knowledge and distributed knowledge operators. The formula formula_19 is an abbreviation for formula_20 (for a given formula_14), formula_22 is an abbreviation for formula_23, formula_24 is an abbreviation for formula_25 and formula_26 an abbreviation for formula_27.\n\nGroup notions: general, common and distributed knowledge.\n\nIn a multi-agent setting there are three important epistemic concepts: general knowledge, distributed knowledge and common knowledge. The notion of common knowledge was first studied by Lewis in the context of conventions. It was then applied to distributed systems and to game theory, where it allows to express that the rationality of the players, the rules of the game and the set of players are commonly known.\n\n\"General knowledge.\"\n\nGeneral knowledge of formula_28 means that everybody in the group of agents formula_29 knows that formula_28. Formally, this corresponds to the following formula:\n\nformula_31\n\n\"Common knowledge.\"\n\nCommon knowledge of formula_28 means that everybody knows formula_28 but also that everybody knows that everybody knows formula_28, that everybody knows that everybody knows that everybody knows formula_28, and so on \"ad infinitum\". Formally, this corresponds to the following formula\n\nformula_36\n\nAs we do not allow infinite conjunction the notion of common knowledge will have to be introduced as a primitive in our language.\n\nBefore defining the language with this new operator, we are going to give an example introduced by Lewis that illustrates the difference between the notions of general knowledge and common knowledge. Lewis wanted to know what kind of knowledge is needed so that the statement formula_4: “every driver must drive on the right” be a convention among a group of agents. In other words, he wanted to know what kind of knowledge is needed so that everybody feels safe to drive on the right. Suppose there are only two agents formula_38 and formula_39. Then everybody knowing formula_4 (formally formula_41) is not enough. Indeed, it might still be possible that the agent formula_38 considers possible that the agent formula_39 does not know formula_4 (formally formula_45). In that case the agent formula_38 will not feel safe to drive on the right because he might consider that the agent formula_39, not knowing formula_4, could drive on the left. To avoid this problem, we could then assume that everybody knows that everybody knows that formula_4 (formally formula_50). This is again not enough to ensure that everybody feels safe to drive on the right. Indeed, it might still be possible that agent formula_38 considers possible that agent formula_39 considers possible that agent formula_38 does not know formula_4 (formally formula_55). In that case and from formula_38’s point of view, formula_39 considers possible that formula_38, not knowing formula_4, will drive on the left. So from formula_38’s point of view, formula_39 might drive on the left as well (by the same argument as above). So formula_38 will not feel safe to drive on the right. Reasoning by induction, Lewis showed that for any formula_63, formula_64 is not enough for the drivers to feel safe to drive on the right. In fact what we need is an infinite conjunction. In other words, we need common knowledge of formula_4: formula_66.\n\n\"Distributed knowledge.\"\n\nDistributed knowledge of formula_28 means that if the agents pulled their knowledge altogether, they would know that formula_28 holds. In other words, the knowledge of formula_28 is \"distributed\" among the agents. The formula formula_70 reads as ‘it is distributed knowledge among the set of agents formula_71 that formula_28 holds’.\n\nEpistemic logic is a modal logic. So, what we call an epistemic model formula_73 is just a Kripke model as defined in modal logic. The set formula_74 is a non-empty set whose elements are called \"possible worlds\" and the \"interpretation\" formula_75 is a function specifying which propositional facts (such as ‘Ann has the red card’) are true in each of these worlds. The \"accessibility relations\" formula_76 are binary relations for each agent formula_77; they are intended to capture the uncertainty of each agent (about the actual world and about the other agents' uncertainty). Intuitively, we have formula_78 when the world formula_79 is compatible with agent formula_39’s information in world formula_81 or, in other words, when agent formula_39 considers that world formula_79 might correspond to the world formula_81 (from this standpoint). We abusively write formula_85 for formula_86 and formula_87 denotes the set of worlds formula_88.\n\nIntuitively, a pointed epistemic model formula_89, where formula_85, represents from an external point of view how the actual world formula_81 is perceived by the agents formula_29.\n\nFor every epistemic model formula_93, every formula_94 and every formula_95, we define formula_96 inductively by the following truth conditions:\n\nwhere formula_97 is the transitive closure of formula_98: we have that formula_99 if, and only if, there are formula_100 and formula_101 such that formula_102 and for all formula_103, formula_104.\n\nDespite the fact that the notion of common belief has to be introduced as a primitive in the language, we can notice that the definition of epistemic models does not have to be modified in order to give truth value to the common knowledge and distributed knowledge operators.\n\nCard Example:\n\nPlayers formula_71, formula_106 and formula_107 (standing for Ann, Bob and Claire) play a card game with three cards: a red one, a green one and a blue one. Each of them has a single card but they do not know the cards of the other players. Ann has the red card, Bob has the green card and Claire has the blue card. This example is depicted in the pointed epistemic model formula_108 represented below. In this example, formula_109 and formula_110. Each world is labelled by the propositional letters which are true in this world and formula_81 corresponds to the actual world. There is an arrow indexed by agent formula_112 from a possible world formula_113 to a possible world formula_79 when formula_115. Reflexive arrows are omitted, which means that for all formula_116 and all formula_117, we have that formula_118.\n\nformula_119 stands for : \"formula_71 has the red card<nowiki>\"</nowiki>\n\nformula_121 stand for: \"formula_107 has the blue card<nowiki>\"</nowiki>\n\nformula_123 stands for: \"formula_106 has the green card<nowiki>\"</nowiki>\n\nand so on...\n\nWhen accessibility relations are equivalence relations (like in this example) and we have that formula_78, we say that agent formula_39 \"cannot distinguish\" world formula_81 from world formula_79 (or world formula_81 is indistinguishable from world formula_79 for agent formula_39). So, for example, formula_132 cannot distinguish the actual world formula_81 from the possible world where formula_106 has the blue card (formula_135), formula_107 has the green card (formula_137) and formula_71 still has the red card (formula_119).\n\nIn particular, the following statements hold:\n\nformula_140\n\n'All the agents know the color of their card'.\n\nformula_141\n\n'formula_71 knows that formula_106 has either the blue or the green card and that formula_107 has either the blue or the green card'.\n\nformula_145\n\n'Everybody knows that formula_71 has either the red, green or blue card and this is even common knowledge among all agents'.\n\nWe use the same notation formula_147 for both knowledge and belief. Hence, depending on the context, formula_148 will either read ‘the agent formula_39 \"K\"nows that formula_28 holds’ or ‘the agent formula_39 \"B\"elieves that formula_28 holds’. A crucial difference is that, unlike knowledge, beliefs can be \"wrong\": the axiom formula_153 holds only for knowledge, but not necessarily for belief. This axiom called axiom T (for Truth) states that if the agent knows a proposition, then this proposition is true. It is often considered to be the hallmark of knowledge and it has not been subjected to any serious attack ever since its introduction in the Theaetetus by Plato.\n\nThe notion of knowledge might comply to some other constraints (or axioms) such as formula_154: if agent formula_39 knows something, she knows that she knows it. These constraints might affect the nature of the accessibility relations formula_156 which may then comply to some extra properties. So, we are now going to define some particular classes of epistemic models that all add some extra constraints on the accessibility relations formula_156. These constraints are matched by particular axioms for the knowledge operator formula_147. Below each property, we give the axiom which \"defines\" the class of epistemic frames that fulfill this property. (formula_159 stands for formula_148 for any formula_77.)\n\nWe discuss the axioms above. Axiom 4 states that if the agent knows a proposition, then she knows that she knows it (this axiom is also known as the “KK-principle”or “KK-thesis”). In epistemology, axiom 4 tends to be accepted by internalists, but not by externalists. Axiom 4 is nevertheless widely accepted by computer scientists (but also by many philosophers, including Plato, Aristotle, Saint Augustine, Spinoza and Shopenhauer, as Hintikka recalls ). A more controversial axiom for the logic of knowledge is axiom 5 for Euclidicity: this axiom states that if the agent does not know a proposition, then she knows that she does not know it. Most philosophers (including Hintikka) have attacked this axiom, since numerous examples from everyday life seem to invalidate it. In general, axiom 5 is invalidated when the agent has mistaken beliefs, which can be due for example to misperceptions, lies or other forms of deception. Axiom B states that it cannot be the case that the agent considers it possible that she knows a false proposition (that is, formula_162). If we assume that axioms T and 4 are valid, then axiom B falls prey to the same attack as the one for axiom 5 since this axiom is derivable. Axiom D states that the agent’s beliefs are consistent. In combination with axiom K (where the knowledge operator is replaced by a belief operator), axiom D is in fact equivalent to a simpler axiom D' which conveys, maybe more explicitly, the fact that the agent’s beliefs cannot be inconsistent: formula_163. The other intricate axioms .2, .3, .3.2 and .4 have been introduced by epistemic logicians such as Lenzen and Kutchera in the 1970s and presented for some of them as key axioms of epistemic logic. They can be characterized in terms of intuitive interaction axioms relating knowledge and beliefs.\n\nThe Hilbert proof system K for the basic modal logic is defined by the following axioms and inference rules: for all formula_77,\n\nThe axioms of an epistemic logic obviously display the way the agents reason. For example, the axiom K together with the rule of inference Nec entail that if I know formula_28 (formula_159) and I know that formula_28 implies formula_168 (formula_169 then I know that formula_168 (formula_171). Stronger constraints can be added. The following proof systems for formula_172 are often used in the literature.\n\nWe define the set of proof systems formula_173.\n\nMoreover, for all formula_174, we define the proof system formula_175 by adding the following axiom schemes and rules of inference to those of formula_176. For all formula_177,\n\nThe relative strength of the proof systems for knowledge is as follows:\n\nformula_178\n\nSo, all the theorems of formula_179 are also theorems of formula_180 and formula_181. Many philosophers claim that in the most general cases, the logic of knowledge is formula_179 or formula_183. Typically, in computer science and in many of the theories developed in artificial intelligence, the logic of belief (\"doxastic\" logic) is taken to be formula_184 and the logic of knowledge (\"epistemic\" logic) is taken to be formula_181, even if formula_181 is only suitable for situations where the agents do not have mistaken beliefs. formula_187 has been propounded by Floridi as the logic of the notion of 'being informed’ which mainly differs from the logic of knowledge by the absence of introspection for the agents.\n\nFor all formula_174, the class of formula_176–models or formula_175–models is the class of epistemic models whose accessibility relations satisfy the properties listed above defined by the axioms of formula_176 or formula_175. Then, for all formula_174, formula_176 is sound and strongly complete for formula_172 w.r.t. the class of formula_176–models, and formula_175 is sound and strongly complete for formula_198 w.r.t. the class of formula_175–models.\n\nThe satisfiability problem for all the logics introduced is decidable. We list below the computational complexity of the satisfiability problem for each of them. Note that it becomes linear in time if there are only finitely many propositional letters in the language. For formula_200, if we restrict to finite nesting, then the satisfiability problem is NP-complete for all the modal logics considered. If we then further restrict the language to having only finitely many primitive propositions, the complexity goes down to linear in time in all cases.\n\nThe computational complexity of the model checking problem is in P in all cases.\n\nDynamic Epistemic Logic (DEL) is a logical framework for modeling epistemic situations involving several agents, and changes that occur to these situations as a result of incoming information or more generally incoming action. The methodology of DEL is such that it splits the task of representing the agents’ beliefs and knowledge into three parts:\n\n\nTypically, an informative event can be a public announcement to all the agents of a formula formula_168: this public announcement and correlative update constitute the dynamic part. However, epistemic events can be much more complex than simple public announcement, including hiding information for some of the agents, cheating, lying, bluffing, \"etc.\" This complexity is dealt with when we introduce the notion of event model. We will first focus on public announcements to get an intuition of the main underlying ideas of DEL.\n\nIn this section, we assume that all events are public. We start by giving a concrete example where DEL can be used, to better understand what is going on. This example is called the muddy children puzzle. Then, we will present a formalization of this puzzle in a logic called Public Announcement Logic (PAL). The muddy children puzzle is one of the most well known puzzles that played a role in the development of DEL. Other significant puzzles include the sum and product puzzle, the Monty Hall dilemma, the Russian cards problem, the two envelopes problem, Moore's paradox, the hangman paradox, \"etc\".\n\nMuddy Children Example:\n\nWe have two children, A and B, both dirty. A can see B but not himself, and B can see A but not herself. Let formula_4 be the proposition stating that A is dirty, and formula_203 be the proposition stating that B is not dirty.\n\nPublic announcement logic (PAL):\n\nWe present the syntax and semantic of Public Announcement Logic (PAL), which combines features of epistemic logic and propositional dynamic logic.\n\nWe define the language formula_215 inductively by the following grammar in BNF:\n\nformula_216\n\nwhere formula_77.\n\nThe language formula_215 is interpreted over epistemic models. The truth conditions for the connectives of the epistemic language are the same as in epistemic logic (see above). The truth condition for the new dynamic action modality formula_219 is defined as follows:\n\nwhere formula_220 with\n\nformula_221,\n\nformula_222 for all formula_223 and\n\nformula_224.\n\nThe formula formula_219 intuitively means that after a truthful announcement of formula_168, formula_28 holds. A public announcement of a proposition formula_168 changes the current epistemic model like in the figure below.\n\nThe proof system formula_229 defined below is sound and strongly complete for formula_215 w.r.t. the class of all pointed epistemic models.\n\nThe axioms Red 1 - Red 4 are called \"reduction axioms\" because they allow to reduce any formula of formula_215 to a provably equivalent formula of formula_17 in formula_229. The formula formula_234 is a theorem provable in formula_229. It states that after a public announcement of formula_203, the agent knows that formula_203 holds.\n\nPAL is decidable, its model checking problem is solvable in polynomial time and its satisfiability problem is PSPACE-complete.\n\nMuddy children puzzle formalized with PAL:\n\nHere are some of the statements that hold in the muddy children puzzle formalized in PAL.\n\nformula_238\n\n'In the initial situation, A is dirty and B is dirty'.\n\nformula_239\n\n'In the initial situation, A does not know whether he is dirty and B neither'.\n\nformula_240\n\n'After the public announcement that at least one of the children A and B is dirty, both of then know that at least one of them is dirty'. However:\n\nformula_241\n\n'After the public announcement that at least one of the children A and B is dirty, they still do not know that they are dirty'. Moreover:\n\nformula_242\n\n'After the successive public announcements that at least one of the children A and B is dirty and that they still do not know whether they are dirty, A and B then both know that they are dirty'.\n\nIn this last statement, we see at work an interesting feature of the update process: a formula is not necessarily true after being announced. That is what we technically call “self-persistence” and this problem arises for epistemic formulas (unlike propositional formulas). One must not confuse the announcement and the update induced by this announcement, which might cancel some of the information encoded in the announcement.\n\nIn this section, we assume that events are not necessarily public and we focus on items 2 and 3 above, namely on how to represent events and on how to update an epistemic model with such a representation of events by means of a product update.\n\nEpistemic models are used to model how agents perceive the actual world. Their perception can also be described in terms of knowledge and beliefs about the world and about the other agents’ beliefs. The insight of the DEL approach is that one can describe how an event is perceived by the agents in a very similar way. Indeed, the agents’ perception of an event can also be described in terms of knowledge and beliefs. For example, the private announcement of formula_71 to formula_106 that her card is red can also be described in terms of knowledge and beliefs: while formula_71 tells formula_106 that her card is red (event formula_247) formula_107 \"believes\" that nothing happens (event formula_249). This leads to define the notion of event model whose definition is very similar to that of an epistemic model.\n\nA pointed event model formula_250 represents how the actual event represented by formula_247 is perceived by the agents. Intuitively, formula_252 means that while the possible event represented by formula_247 is occurring, agent formula_39 considers possible that the possible event represented by formula_249 is actually occurring.\n\nAn event model is a tuple formula_256 where:\n\n\nformula_263 denotes the set formula_264 .We write formula_265 for formula_266, and formula_250 is called a pointed event model (formula_247 often represents the actual event).\n\nCard Example:\n\nLet us resume the card example and assume that players formula_71 and formula_106 show their card to each other. As it turns out, formula_107 noticed that formula_71 showed her card to formula_106 but did not notice that formula_106 did so to formula_71. Players formula_71 and formula_106 know this. This event is represented below in the event model formula_250.\n\nThe possible event formula_247 corresponds to the actual event ‘players formula_71 and formula_106 show their and cards respectively to each other’ (with precondition formula_282), formula_249 stands for the event ‘player formula_71 shows her green card’ (with precondition formula_285) and formula_286 stands for the atomic event ‘player formula_71 shows her red card’ (with precondition formula_119). Players formula_71 and formula_106 show their cards to each other, players formula_71 and formula_106 know this and consider it possible, while player formula_107 considers possible that player formula_71 shows her red card and also considers possible that player formula_71 shows her green card, since he does not know her card. In fact, that is all that player formula_107 considers possible because she did not notice that formula_297 showed her card.\n\nAnother example of event model is given below. This second example corresponds to the event whereby Player formula_71 shows her red card publicly to everybody. Player formula_71 shows her red card, players formula_71, formula_106 and formula_107 ‘know’ it, players formula_71, formula_106 and formula_107 ‘know’ that each of them ‘knows’ it, \"etc.\" In other words, there is \"common knowledge\" among players formula_71, formula_106 and formula_107 that player formula_71 shows her red card.\n\nThe DEL product update is defined below. This update yields a new pointed epistemic model formula_310 representing how the new situation which was previously represented by formula_89 is perceived by the agents after the occurrence of the event represented by formula_250.\n\nLet formula_313 be an epistemic model and let formula_314 be an event model. The product update of formula_93 and formula_316 is the epistemic model formula_317 defined as follows: for all formula_318 and all formula_319,\n\nIf formula_86 and formula_321 are such that formula_322 then formula_323 denotes the pointed epistemic model formula_324. This definition of the product update is conceptually grounded.\n\nCard Example:\n\nAs a result of the first event described above (Players formula_71 and formula_106 show their cards to each other in front of player formula_107), the agents update their beliefs. We get the situation represented in the pointed epistemic model formula_323 below. In this pointed epistemic model, the following statement holds: formula_329 It states that player formula_71 knows that player formula_106 has the card but player formula_107 'believes' that it is not the case.\n\nThe result of the second event is represented below. In this pointed epistemic model, the following statement holds: formula_333. It states that there is common knowledge among formula_106 and formula_107 that they know the true state of the world (namely formula_71 has the red card, formula_106 has the green card and formula_107 has the blue card), but formula_71 does not know it.\n\nBased on these three components (epistemic model, event model and product update), Baltag, Moss and Solecki defined a general logical language inspired from the logical language of propositional dynamic logic to reason about information and knowledge change.\n\n\n\n", "id": "48791499", "title": "Dynamic epistemic logic"}
{"url": "https://en.wikipedia.org/wiki?curid=12142270", "text": "GENESIS (software)\n\nGENESIS (The \"GEneral NEural SImulation System\") is a simulation environment for constructing realistic models of neurobiological systems at many levels of scale including: sub-cellular processes, individual neurons, networks of neurons, and neuronal systems. These simulations are “computer-based implementations of models whose primary objective is to capture what is known of the anatomical structure and physiological characteristics of the neural system of interest”. GENESIS is intended to quantify the physical framework of the nervous system in a way that allows for easy understanding of the physical structure of the nerves in question. “At present only GENESIS allows parallelized modeling of single neurons and networks on multiple-instruction-multiple-data parallel computers.” Development of GENESIS software spread from its home at Caltech to labs at the University of Texas at San Antonio, the University of Antwerp, the National Centre for Biological Sciences in Bangalore, the University of Colorado, the Pittsburgh Supercomputing Center, the San Diego Supercomputer Center, and Emory University.\n\nGENESIS works by creating simulation environments for constructing models of neurons or neural systems. \"Nerve cells are capable of communicating with each other in such a highly structured manner as to form neuronal networks. To understand neural networks, it is necessary to understand the ways in which one neuron communicates with another through synaptic connections and the process called synaptic transmission\". Neurons have a specialized structure for their function, they \"are different from most other cells in the body in that they are polarized and have distinct morphological regions, each with specific functions\". The two important regions of a neuron are the dendrite and the axon. \"Dendrites are the region where one neuron receives connections from other neurons. The cell body or soma contains the nucleus and the other organelles necessary for cellular function. The axon is a key component of nerve cells over which information is transmitted from one part of the neuron (e.g., the cell body) to the terminal regions of the neuron\". The third important piece of a neuron is the synapse. \"The synapse is the terminal region of the axon this is where one neuron forms a connection with another and conveys information through the process of synaptic transmission\".\n\nNeural networks like the ones simulated with GENESIS software can quickly become highly complex and difficult to understand. \"Just a few interconnected neurons (a microcircuit) can perform sophisticated tasks such as mediate reflexes, process sensory information, generate locomotion and mediate learning and memory. Even more complex networks, macrocircuits, consist of multiple embedded microcircuits. Macrocircuits mediate higher brain functions such as object recognition and cognition\". GENESIS endeavors to simulate neural systems as they are found in nature. Often, \"a neuron can receive contacts from up to 10,000 presynaptic neurons, and, in turn, any one neuron can contact up to 10,000 postsynaptic neurons. The combinatorial possibility could give rise to enormously complex neuronal circuits or network topologies, which might be very difficult to understand\".\n\nGENESIS was developed by Dr. James M. Bower, in the Caltech laboratory, and first released to the public in 1988 in association with the first Methods in Computational Neuroscience Course at the Marine Biological Laboratory in Woods Hole, MA. Full source code for the software was released in the same year under an open software model for development. It's now supported by the Computational Biology Initiative at the University of Texas at San Antonio and is available free along with tutorial guides on its use.\nP-GENESIS, a parallel version of GENESIS, was first run in 1990 on the Intel Delta, which was the prototype for the Intel Paragon family of massively parallel supercomputers.\n\nGENESIS is useful in creating a simulation environment for constructing models of neurobiological systems such as:\n\nThe GENESIS system is complicated, but relatively easy to use.\nAn individual can input commands through one of three ways: script files, graphical user interface, or the GENESIS command shell. These commands are then processed by the script language interpreter. \"The Script Language Interpreter processes commands entered through the keyboard, script files, or the graphical user interface, and passes them to the GENESIS simulation engine. The simulation engine also loads compiled object libraries, reads and writes data files, and interacts with the graphical user interface\". Below is a graphical representation of the user input process and a sample GENESIS output.\n\nMost current applications for GENESIS involve realistic simulations of biological systems. It is usually used to simulate the behavior of larger brain structures, for example the cerebral cortex. These studies most often occur in lab courses in neural simulation at Caltech and the Marine Biological Laboratory at Woods Hole, Massachusetts.\n\nGENESIS can be used in combination with Yale University’s software called NEURON as a means for scientists to collaborate to construct a physical description of the nervous system. The GENESIS software can also be used with Kinetikit in the modeling of signal transduction pathways.\n\nGENESIS has been used in many studies. Some of these studies involve research that focuses on the development of software that would be useful across many disciplines. Others are studies of neurons, such as Purkinje cells. These studies used GENESIS to simulate Purkinje cells and could be useful for the planning and development of later experiments using the GENESIS software.\n\nThere may also be biomedical applications of the software. For example, St. Jude Medical in Europe has developed an implanted GENESIS device.\n\n\n\n", "id": "12142270", "title": "GENESIS (software)"}
{"url": "https://en.wikipedia.org/wiki?curid=47336626", "text": "Oriented energy filters\n\nOriented energy filters are used to grant sight to intelligent machines and sensors. The light comes in and is filtered so that it can be properly computed and analyzed by the computer allowing it to “perceive” what it is measuring. These energy measurements are then calculated to take a real time measurement of the oriented space time structure.\n\n3D Gaussian filters are used to extract orientation measurements. They were chosen due to their ability to capture a broad spectrum and easy and efficient computations.\n\nThe use of these vision systems can then be used in smart room, human interface and surveillance applications. The computations used can tell more than the standalone frame that most perceived motion devices such as a television frame. The objects captured by these devices would tell the velocity and energy of an object and its direction in relation to space and time. This also allows for better tracking ability and recognition.\n", "id": "47336626", "title": "Oriented energy filters"}
{"url": "https://en.wikipedia.org/wiki?curid=201158", "text": "Soft computing\n\nIn computer science, soft computing (sometimes referred to as computational intelligence, though CI does not have an agreed definition) is the use of inexact solutions to computationally hard tasks such as the solution of NP-complete problems, for which there is no known algorithm that can compute an exact solution in polynomial time. Soft computing differs from conventional (hard) computing in that, unlike hard computing, it is tolerant of imprecision, uncertainty, partial truth, and approximation. In effect, the role model for soft computing is the human mind.\n\nThe principal constituents of Soft Computing (SC) are Fuzzy Logic (FL), Evolutionary Computation (EC), Machine Learning (ML) and Probabilistic Reasoning (PR), with the latter subsuming belief networks and parts of learning theory.\n\nSoft computing (SC) solutions are unpredictable, uncertain and between 0 and 1. Soft Computing became a formal area of study in Computer Science in the early 1990s. Earlier computational approaches could model and precisely analyze only relatively simple systems. More complex systems arising in biology, medicine, the humanities, management sciences, and similar fields often remained intractable to conventional mathematical and analytical methods. However, it should be pointed out that simplicity and complexity of systems are relative, and many conventional mathematical models have been both challenging and very productive.\nSoft computing deals with imprecision, uncertainty, partial truth, and approximation to achieve practicability, robustness and low solution cost. As such it forms the basis of a considerable amount of machine learning techniques. Recent trends tend to involve evolutionary and swarm intelligence based algorithms and bio-inspired computation.\n\nThere are main differences between soft computing and possibility. Possibility is used when we don't have enough information to solve a problem but soft computing is used when we don't have enough information about the problem itself. These kinds of problems originate in the human mind with all its doubts, subjectivity and emotions; an example can be determining a suitable temperature for a room to make people feel comfortable.\n\nComponents of soft computing include:\n\nGenerally speaking, soft computing techniques resemble biological processes more closely than traditional techniques, which are largely based on formal logical systems, such as sentential logic and predicate logic, or rely heavily on computer-aided numerical analysis (as in finite element analysis). Soft computing techniques are intended to complement each other.\n\nUnlike hard computing schemes, which strive for exactness and full truth, soft computing techniques exploit the given tolerance of imprecision, partial truth, and uncertainty for a particular problem. Another common contrast comes from the observation that inductive reasoning plays a larger role in soft computing than in hard computing.\n\n", "id": "201158", "title": "Soft computing"}
{"url": "https://en.wikipedia.org/wiki?curid=5421193", "text": "Conflict resolution strategy\n\nConflict resolution strategies are used in production systems in artificial intelligence, such as in rule-based expert systems, to help in choosing which production rule to fire. The need for such a strategy arises when the conditions of two or more rules are satisfied by the currently known facts.\n\nConflict resolution strategies fall into several main categories. They each have advantages which form their rationales.\n\n\n", "id": "5421193", "title": "Conflict resolution strategy"}
{"url": "https://en.wikipedia.org/wiki?curid=49239240", "text": "VaultML\n\nVault is an Israeli–based artificial intelligence company that lays claims to have created technologies that can \"read\" movie and TV screenplays in order to predict box office and investment performance. Part of the process reportedly entails analyzing 300,000 to 400,000 elements from the script, which could be anything from plot, character development, script structure, scene events. The founders are made up of high frequency trading veterans and state they use similar approaches to predicting film performance. Vault published its 2015 film predictions for over 20 movies in early 2015 and successfully predicted correctly many box office performances throughout that year. Vault's algorithms out earned the market on a return on investment basis.\n", "id": "49239240", "title": "VaultML"}
{"url": "https://en.wikipedia.org/wiki?curid=2681589", "text": "Noogenesis\n\nNoogenesis (Ancient Greek: \"νοῦς\"=mind + \"γένεσις\" = origin, becoming) is the emergence and evolution of intelligence.\n\nNoo, nous (British: , US: ) – from the ancient Greek , has synonyms in other languages (Chinese), is a term that currently encompasses the semantics: mind, intelligence, intellect, reason; wisdom; insight, intuition, thought, - in a single phenomenon .\n\nNoogenesis was first mentioned in the posthumously published in 1955 book \"The Phenomenon of Man\" by Pierre Teilhard de Chardin, an anthropologist and philosopher, in a few places:\n\nThe lack of any kind of definition of the term has led to a variety of interpretations reflected in the book, including \"the contemporary period of evolution on Earth, signified by transformation of biosphere onto the sphere of intelligence—noosphere\", \"evolution run by human mind\" etc.\nThe most widespread interpretation is thought to be \"the emergence of mind, which follows geogenesis, biogenesis and anthropogenesis, forming a new sphere on Earth—noosphere\".\n\nIn 2005 Alexey Eryomin in the monograph Noogenesis and Theory of Intellect proposed a new concept of noogenesis in understanding the evolution of intellectual systems, concepts of intellectual systems, information logistics, information speed, intellectual energy, intellectual potential, consolidated into a theory of the intellect which combines the biophysical parameters of intellectual energy—the amount of information, its acceleration (frequency, speed) and the distance it's being sent—into a formula.\nAccording the new concept—proposed hypothesis continue prognostic progressive evolution of the species \"Homo sapiens\", the analogy between the human brain with the enormous amount of neural cells firing at the same time and a similarly functioning human society.\nA new understanding of the term \"noogenesis\" as an evolution of the intellect was proposed by A. Eryomin. A hypothesis based on recapitulation theory links the evolution of the human brain to the development of human civilization. The parallel between the amount of people living on Earth and the amount of neurons becomes more and more obvious leading us to viewing global intelligence as an analogy for human brain.\nAll of the people living on this planet have undoubtedly inherited the amazing cultural treasures of the past, be it production, social and intellectual ones. We are genetically hardwired to be a sort of \"live RAM\" of the global intellectual system. Alexey Eryomin suggests that humanity is moving towards a unified self-contained informational and intellectual system. His research has shown the probability of Super Intellect realizing itself as Global Intelligence on Earth. We could get closer to understanding the most profound patterns and laws of the Universe if these kinds of research were given enough attention. Also, the resemblance between the individual human development and such of the whole human race has to be explored further if we are to face some of the threats of the future.\n\nTherefore, generalizing and summarizing: \nThe term \"noogenesis\" can be used in a variety of fields i.e. medicine, biophysics, semiotics, mathematics, information technology, psychology etc. thus making it a truly cross-disciplinary one. In astrobiology noogenesis concerns the origin of intelligent life and more specifically technological civilizations capable of communicating with humans and or traveling to Earth. The lack of evidence for the existence of such extraterrestrial life creates the Fermi paradox.\n\nThe emergence of the human mind is considered to be one of the five fundamental phenomenons of emergent evolution. \nTo understand the mind, it is necessary to determine how human thinking differs from other thinking beings. Such differences include the ability to generate calculations, to combine dissimilar concepts, to use mental symbols, and to think abstractly.\nThe knowledge of the phenomenon of intelligent systems—the emergence of reason (noogenesis) boils down to:\n\n\nSeveral published works which do not employ the term \"noogenesis\", however, address some patterns in the emergence and functioning of the human intelligence: working memory capacity ≥ 7, ability to predict, prognosis, hierarchical (6 layers neurons) system of information analysis, consciousness, memory, generated and consumed information properties etc. They also set the limits of several physiological aspects of human intelligence. Сonception of emergence of insight.\n\nHistorical evolutionary development and emergence of \"H. sapiens\" as species, include emergence of such concepts as anthropogenesis, phylogenesis, morphogenesis, cephalization, systemogenesis, cognition systems autonomy.\n\nOn the other hand, development of an individual's intellect deals with concepts of embryogenesis, ontogenesis, morphogenesis, neurogenesis, higher nervous function of I.P.Pavlov and his philosophy of mind.\nDespite the fact that the morphofunctional maturity is usually reached by the age of 13, the definitive functioning of the brain structures is not complete until about 16–17 years of age.\n\nBioinformatics, genetic engineering, noopharmacology, cognitive load, brain stimulation, the efficient use of altered states of consciousness, use of non-human cognition, information technology (IT), artificial intelligence (AI) are all believed to be effective methods of intelligence advancement.\n\nThe development of the human brain, perception, cognition, memory and neuroplasticity are unsolved problems in neuroscience. Several megaprojects are being carried out in the American BRAIN Initiative and the European Human Brain Project in attempt to better our understanding of the brain's functionality along with the intention to develop human cognitive performance in the future with artificial intelligence, informational, communication and cognitive technology.\n", "id": "2681589", "title": "Noogenesis"}
{"url": "https://en.wikipedia.org/wiki?curid=43306489", "text": "Enterprise cognitive system\n\nEnterprise Cognitive Systems (ECS) are part of a broader shift in computing, from a programmatic to a probabilistic approach, called Cognitive computing. An Enterprise Cognitive System makes a new class of complex decision support problems computable, where the business context is ambiguous, multi-faceted, and fast-evolving, and what to do in such a situation is usually assessed today by the business user. An ECS is designed to synthesize a business context and link it to the desired outcome. It recommends evidence-based actions to help the end-user achieve the desired outcome. It does so by finding past situations similar to the current situation, and extracting the repeated actions that best influence the desired outcome.\n\nWhile general-purpose Cognitive Systems can be used for different outputs, prescriptive, suggestive, instructive, or simply entertaining, an Enterprise Cognitive System is focused on action, not insight, to help in assessing what to do in a complex situation.\n\nECS have to be:\n\n\n", "id": "43306489", "title": "Enterprise cognitive system"}
{"url": "https://en.wikipedia.org/wiki?curid=49316492", "text": "Darkforest\n\nDarkforest is a computer go program developed by Facebook, based on deep learning techniques using a convolutional neural network. Its updated version Darkfores2 combines the techniques of its predecessor with Monte Carlo tree search. The MCTS effectively takes tree search methods commonly seen in computer chess programs and randomizes them. With the update, the system is known as Darkfmcts3.\n\nDarkforest is of similar strength to programs like CrazyStone and Zen. It has been tested against a professional human player at the 2016 UEC cup. Google's AlphaGo program won against a professional player in October 2015 using a similar combination of techniques.\n\nDarkforest is named after Liu Cixin's science fiction novel \"The Dark Forest\".\n\nCompeting with top human players in the ancient game of Go has been a long-term goal of artificial intelligence. Go’s high branching factor makes traditional search techniques ineffective, even on cutting-edge hardware, and Go’s evaluation function could change drastically with one stone change. However, by using a Deep Convolutional Neural Network designed for long-term predictions, Darkforest has been able to substantially improve the win rate for bots over more traditional Monte Carlo Tree Search based approaches.\n\nAgainst human players, Darkfores2 achieves a stable \"3d ranking\" on KGS Go Server, which roughly corresponds to an advanced amateur human player. However, after adding Monte Carlo Tree Search to Darkfores2 to create a much stronger player named darkfmcts3, it can achieve a \"5d ranking\" on the KGS Go Server.\n\ndarkfmcts3 is on par with state-of-the-art Go AIs such as Zen, DolBaram and Crazy Stone but lags behind AlphaGo. It won 3rd place in January 2016 KGS Bot Tournament against other Go AIs.\n\nAfter Google's AlphaGo won against Fan Hui in 2015, Facebook made its AI's hardware designs public, alongside releasing the code behind DarkForest as open-source, along with heavy recruiting to strengthen its team of AI engineers.\n\nDarkforest uses a neural network to sort through the 10 board positions, and find the most powerful next move. However, neural networks alone cannot match the level of good amateur players or the best search-based Go engines, and so Darkfores2 combines the neural network approach with a search-based machine. A database of 250,000 real Go games were used in the development of Darkforest, with 220,000 used as a training set and the rest used to test the neural network's ability to predict the next moves played in the real games. This allows Darkforest to accurately evaluate the global state of the board, but local tactics were still poor. Search-based engines have poor global evaluation, but are good at local tactics. Combining these two approaches is difficult because search-based engines work much faster than neural networks, a problem which was solved in Darkfores2 by running the processes in parallel with frequent communication between the two.\n\nGo is generally played by analyzing the position of the stones on the board. Some advanced players have described it as playing in some part subconsciously. Unlike chess and checkers, where AI players can simply look farther forward at moves than human players, but with each round of Go having on average 250 possible moves, that approach is ineffective. Instead, neural networks copy human play by training the AI systems on images of successful moves, the AI can effectively learn how to interpret how the board looks, as many grandmasters do. In November 2015, Facebook demonstrated the combination of MCTS with neural networks, which played with a style that \"felt human\".\n\nIt has been noted that Darkforest still has flaws in its play style. Sometimes the bot plays tenuki (\"move elsewhere\") pointlessly when local powerful moves are required. When the bot is losing, it shows the typical behavior of MCTS, it plays bad moves and loses more. The Facebook AI team has acknowledged these as areas of future improvement.\n\nThe family of Darkforest computer go programs is based on convolution neural networks. The most recent advances in Darkfmcts3 combined convolutional neural networks with more traditional Monte Carlo tree search. Darkfmcts3 is the most advanced version of Darkforest, which combines Facebook's most advanced convolutional neural network architecture from Darkfores2 with a Monte Carlo tree search.\n\nDarkfmcts3 relies on a convolution neural networks that predicts the next k moves based on the current state of play. It treats the board as a 19x19 image with multiple channels. Each channel represents a different aspect of board information based upon the specific style of play. For standard and extended play, there are 21 and 25 different channels, respectively. In standard play, each players liberties are represented as six binary channels or planes. The respective plane is true if the player one, two, or three or more liberties available. Ko (i.e. illegal moves) is represented as one binary plane. Stone placement for each opponent and empty board positions are represented as three binary planes, and the duration since a stone has been placed is represented as real numbers on two planes, one for each player. Lastly, the opponents rank is represented by nine binary planes, where if all are true, the player is a 9d level, if 8 are true, a 8d level, and so forth. Extended play additionally considers the boarder (binary plane that is true at the border), position mask (represented as distance from the board center, i.e. formula_1, where formula_2 is a real number at a position), and each player's territory (binary, based on which player a location is closer to).\n\nDarkfmct3 uses a 12-layer full convolutional network with a width of 384 nodes without weight sharing or pooling. Each convolutional layer is followed by a rectified linear unit, a popular activation function for deep neural networks. A key innovation of Darkfmct3 compared to previous approaches is that it uses only one softmax function to predict the next move, which enables the approach to reduce the overall number of parameters. Darkfmct3 was trained against 300 random selected games from an empirical dataset representing different game stages. The learning rate was determined by vanilla stochastic gradient descent.\n\nDarkfmct3 synchronously couples a convolutional neural network with a Monte Carlo tree search. Because the convolutional neural network is computationally taxing, the Monte Carlo tree search focuses computation on the more likely game play trajectories. By running the neural network synchronously with the Monte Carlo tree search, it is possible to guarantee that each node is expanded by the moves predicted by the neural network.\n\nDarkfores2 beats Darkforest, its neural network-only predecessor, around 90% of the time, and Pachi, one of the best search-based engines, around 95% of the time. On the Kyu rating system, Darkforest holds a 1-2d level. Darkfores2 achieves a stable 3d level on KGS Go Server as a ranked bot. With the added Monte Carlo tree search, Darkfmcts3 with 5,000 rollouts beats Pachi with 10k rollouts in all 250 games; with 75k rollouts it achieves a stable 5d level in KGS server, on par with state-of-the-art Go AIs (e.g., Zen, DolBaram, CrazyStone); with 110k rollouts, it won the 3rd place in January KGS Go Tournament.\n\n\n", "id": "49316492", "title": "Darkforest"}
{"url": "https://en.wikipedia.org/wiki?curid=48718351", "text": "Leverhulme Centre for the Future of Intelligence\n\nThe Leverhulme Centre for the Future of Intelligence is an interdisciplinary research centre within the University of Cambridge that explores the opportunities and challenges to humanity from the development of artificial intelligence.\n\nThe Centre brings together academics from the fields of computer science, philosophy, social science and others and is a collaboration led by the University of Cambridge with links to the Oxford Martin School at the University of Oxford, Imperial College London, and the University of California, Berkeley.\n\n", "id": "48718351", "title": "Leverhulme Centre for the Future of Intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=49294877", "text": "Meca Sapiens\n\nMeca Sapiens (from the Latin words mēchanicus, which means \"mechanical\", and sapiens which means \"wise\", i.e. \"wise machine \") is a framework, in Artificial General Intelligence (AGI), whose aim is to implement synthetic consciousness. The framework is based on an understanding of consciousness as a system capability; its design stages followed a top-down development process; and it utilizes standard software engineering tools, structures and techniques.\n\nA complete system architecture based on the Meca Sapiens framework and suitable for design and implementation was published in late 2015. The creator of the Meca Sapiens architecture is J.E. Tardy.\n\nMeca Sapiens differs from other research in artificial consciousness with respect to: its understanding of consciousness, the development process it follows and the tools and techniques it employs.\n\nMeca Sapiens defines consciousness as an observable system capability whose formal component is the capacity of a system to generate, communicate and utilize absolute cognitive (non-sensory) representations of itself and its environment. In this understanding, the human experience does not define consciousness but is viewed as a particular instance of (a more general) category of \"system consciousness\".\n\nThis differs from a prevalent understanding of consciousness as ontologically subjective phenomena (see consciousness, hard problem of consciousness, artificial consciousness, neural correlates of consciousness) that motivate attempts to replicate, in synthetic structures, the subjective experiences (Qualia) of living creatures. In Meca Sapiens, the subjective sensations of consciousness, experienced by humans, are of anecdotal importance; what matters is the observed behavior of a system in response to certain types of information.\n\nThe development of the Meca Sapiens architecture followed a top-down (or Waterfall) process where the objective (in this case synthetic consciousness) is first defined as achievable requirements and a complete system architecture of the solution is outlined before any implementation begins. This approach is characterized by a refusal to produce any software coding before a complete solution is first outlined at the system architecture level.\n\nThis development strategy diverges from the bottom-up approach adopted in other research projects (i.e. IDA-LIDA, Novamente-OpenCog) where coding of a partial version begins early and is followed by continuing attempts to expand the initial prototype in generality and scope.\n\nThe Meca Sapiens architecture is based on the software engineering tools and techniques used in other information systems. It describes a purposefully designed embedded autonomous agent that will be conscious. The architecture makes no use of the structures or processes that take place in human (or animal) brains. In particular, Artificial Neural Networks, whose role is central in many AGI related projects, are treated as stochastic optimization mechanisms.\n\nThe intent to define consciousness as a system capability and implement it using standard techniques was first stated in 1989\n\n2008: launch of the Meca Sapiens project to develop the system architecture of a conscious machine.\n\n2011: publication of a System Requirements Specification document defining machine consciousness in terms of specification objectives.\n\n2015: publication of a complete system architecture to implement synthetic consciousness.\n\n2016: a number of implementation have begun and are currently under way.\n\n", "id": "49294877", "title": "Meca Sapiens"}
{"url": "https://en.wikipedia.org/wiki?curid=49082762", "text": "List of datasets for machine learning research\n\nThese datasets are used for machine-learning research and have been cited in peer-reviewed academic journals and other publications. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce. This list aggregates high-quality datasets that have been shown to be of value to the machine learning research community from multiple different data repositories to provide greater coverage of the topic than is otherwise available.\n\nDatasets consisting primarily of images or videos for tasks such as object detection, facial recognition, and multi-label classification.\n\nIn computer vision, face images have been used extensively to develop facial recognition systems, face detection, and many other projects that use images of faces.\nDatasets consisting primarily of text for tasks such as natural language processing, sentiment analysis, translation, and cluster analysis.\n\nDatasets of sounds and sound features.\n\nDatasets containing electric signal information requiring some sort of Signal processing for further analysis.\n\nDatasets from physical systems\n\nDatasets from biological systems.\n\nDatasets consisting of rows of observations and columns of attributes characterizing those observations. Typically used for regression analysis or classification but other types of algorithms can also be used. This section includes datasets that do not fit in the above categories.\n\nAs datasets come in myriad formats and can sometimes be difficult to use, there has been considerable work put into curating and standardizing the format of datasets to make them easier to use for machine learning research.\n\n\n", "id": "49082762", "title": "List of datasets for machine learning research"}
{"url": "https://en.wikipedia.org/wiki?curid=12994741", "text": "Neurorobotics\n\nNeurorobotics, a combined study of neuroscience, robotics, and artificial intelligence, is the science and technology of embodied autonomous neural systems. Neural systems include brain-inspired algorithms (e.g. connectionist networks), computational models of biological neural networks (e.g. artificial spiking neural networks, large-scale simulations of neural microcircuits) and actual biological systems (e.g. \"in vivo\" and \"in vitro\" neural nets). Such neural systems can be embodied in machines with mechanic or any other forms of physical actuation. This includes robots, prosthetic or wearable systems but at also, at smaller scale, micro-machines and, at the larger scales, furniture and infrastructures.\n\nNeurorobotics is that branch of neuroscience with robotics, which deals with the study and application of science and technology of embodied autonomous neural systems like brain-inspired algorithms. At its core, neurorobotics is based on the idea that the brain is embodied and the body is embedded in the environment. Therefore, most neurorobots are required to function in the real world, as opposed to a simulated environment.\n\nBeyond brain-inspired algorithms for robots neurorobotics may also involve the design of brain-controlled robot systems.\n\nNeurorobotics represents the two-front approach to the study of intelligence. Neuroscience attempts to discern what intelligence consists of and how it works by investigating intelligent biological systems, while the study of artificial intelligence attempts to recreate intelligence through non-biological, or artificial means. Neurorobotics is the overlap of the two, where biologically inspired theories are tested in a grounded environment, with a physical implementation of said model. The successes and failures of a neurorobot and the model it is built from can provide evidence to refute or support that theory, and give insight for future study.\n\nNeurorobots can be divided into various major classes based on the robot's purpose. Each class is designed to implement a specific mechanism of interest for study. Common types of neurorobots are those used to study motor control, memory, action selection, and perception.\n\nNeurorobots are often used to study motor feedback and control systems, and have proved their merit in developing controllers for robots. Locomotion is modeled by a number of neurologically inspired theories on the action of motor systems. Locomotion control has been mimicked using models or central pattern generators, clumps of neurons capable of driving repetitive behavior, to make four-legged walking robots. Other groups have expanded the idea of combining rudimentary control systems into a hierarchical set of simple autonomous systems. These systems can formulate complex movements from a combination of these rudimentary subsets. This theory of motor action is based on the organization of cortical columns, which progressively integrate from simple sensory input into a complex afferent signals, or from complex motor programs to simple controls for each muscle fiber in efferent signals, forming a similar hierarchical structure.\n\nAnother method for motor control uses learned error correction and predictive controls to form a sort of simulated muscle memory. In this model, awkward, random, and error-prone movements are corrected for using error feedback to produce smooth and accurate movements over time. The controller learns to create the correct control signal by predicting the error. Using these ideas, robots have been designed which can learn to produce adaptive arm movements or to avoid obstacles in a course.\n\nRobots designed to test theories of animal memory systems. Many studies currently examine the memory system of rats, particularly the rat hippocampus, dealing with place cells, which fire for a specific location that has been learned. Systems modeled after the rat hippocampus are generally able to learn mental maps of the environment, including recognizing landmarks and associating behaviors with them, allowing them to predict the upcoming obstacles and landmarks.\n\nAnother study has produced a robot based on the proposed learning paradigm of barn owls for orientation and localization based on primarily auditory, but also visual stimuli. The hypothesized method involves synaptic plasticity and neuromodulation, a mostly chemical effect in which reward neurotransmitters such as dopamine or serotonin affect the firing sensitivity of a neuron to be sharper. The robot used in the study adequately matched the behavior of barn owls. Furthermore, the close interaction between motor output and auditory feedback proved to be vital in the learning process, supporting active sensing theories that are involved in many of the learning models.\n\nNeurorobots in these studies are presented with simple mazes or patterns to learn. Some of the problems presented to the neurorobot include recognition of symbols, colors, or other patterns and execute simple actions based on the pattern. In the case of the barn owl simulation, the robot had to determine its location and direction to navigate in its environment.\n\nAction selection studies deal with negative or positive weighting to an action and its outcome. Neurorobots can and have been used to study *simple* ethical interactions, such as the classical thought experiment where there are more people than a life raft can hold, and someone must leave the boat to save the rest. However, more neurorobots used in the study of action selection contend with much simpler persuasions such as self-preservation or perpetuation of the population of robots in the study. These neurorobots are modeled after the neuromodulation of synapses to encourage circuits with positive results. In biological systems, neurotransmitters such as dopamine or acetylcholine positively reinforce neural signals that are beneficial. One study of such interaction involved the robot Darwin VII, which used visual, auditory, and a simulated taste input to \"eat\" conductive metal blocks. The arbitrarily chosen good blocks had a striped pattern on them while the bad blocks had a circular shape on them. The taste sense was simulated by conductivity of the blocks. The robot had positive and negative feedbacks to the taste based on its level of conductivity. The researchers observed the robot to see how it learned its action selection behaviors based on the inputs it had. Other studies have used herds of small robots which feed on batteries strewn about the room, and communicate its findings to other robots.\n\nNeurorobots have also been used to study sensory perception, particularly vision. These are primarily systems that result from embedding neural models of sensory pathways in automatas. This approach gives exposure to the sensory signals that occur during behavior and also enables a more realistic assessment of the degree of robustness of the neural model. It is well known that changes in the sensory\nsignals produced by motor activity provide useful perceptual cues that are used extensively by organisms. For example, researchers have used the depth information that emerges during replication of human head and eye movements to establish robust representations of the visual scene. \n\nBiological robots are not officially neurorobots in that they are not neurologically inspired AI systems, but actual neuron tissue wired to a robot. This employs the use of cultured neural networks to study brain development or neural interactions. These typically consist of a neural culture raised on a multielectrode array (MEA), which is capable of both recording the neural activity and stimulating the tissue. In some cases, the MEA is connected to a computer which presents a simulated environment to the brain tissue and translates brain activity into actions in the simulation, as well as providing sensory feedback. The ability to record neural activity gives researchers a window into a brain, albeit simple, which they can use to learn about a number of the same issues neurorobots are used for.\n\nAn area of concern with the biological robots is ethics. Many questions are raised about how to treat such experiments. Seemingly the most important question is that of consciousness and whether or not the rat brain experiences it. This discussion boils down to the many theories of what consciousness is.\n\n\"See Hybrot, consciousness.\"\n\nNeuroscientists benefit from neurorobotics because it provides a blank slate to test various possible methods of brain function in a controlled and testable environment. Furthermore, while the robots are more simplified versions of the systems they emulate, they are more specific, allowing more direct testing of the issue at hand. They also have the benefit of being accessible at all times, while it is much more difficult to monitor even large portions of a brain while the animal is active, let alone individual neurons. \n\nWith subject of neuroscience growing as it has, numerous neural treatments have emerged, from pharmaceuticals to neural rehabilitation. Progress is dependent on an intricate understanding of the brain and how exactly it functions. It is very difficult to study the brain, especially in humans due to the danger associated with cranial surgeries. Therefore, the use of technology to fill the void of testable subjects is vital. Neurorobots accomplish exactly this, improving the range of tests and experiments that can be performed in the study of neural processes.\n\n\n", "id": "12994741", "title": "Neurorobotics"}
{"url": "https://en.wikipedia.org/wiki?curid=49138450", "text": "Turing Robot\n\nTuring Robot is the first Chinese company engaged in research on the commercialization of artificial intelligence (AI). Its founding members have over ten years of experience in AI research, including semantic recognition, cognitive computing, human-machine interaction, and machine learning.\n\nThe company was founded in 2010 and in 2012 released the \"Wormhole Voice Assistant\" application, which was the first Chinese intelligence-based voice assistant application.\n\nIn 2014, Turing released the first open platform for AI robots, also known as the Turing Robot. In November 2015, the Turing OS system was released .\n\nTuring Robot Open Platform is now the world's largest chatbot open platform with the number of developers reached 600,000. Its Chinese semantic recognition accuracy is up to 94.7%, Turing Robot can provide intelligence-based software and hardware products with AI services covering Chinese semantic analysis, natural language and dialogue processing, DeepQA and more.\n\nSince the official launch in November 2014, the Turing Robot has provided technical support to more than 230,000 developers and partners with over 130 billion total queries so far. The Turing Robot has established a proven track record in many hardware and software fields, including domestic service robots, commercial service robots, companion robots for children, intelligent customer service systems, intelligent vehicle systems, and smart home control systems.\n\nBy simulating the cognitive and communicative behaviors of humans, the Turing Robot provides users with a conversation feature that provides intelligence-based interaction in hardware and software products, as well as the NLP Knowledge Base for customized life and business needs. Meanwhile, the Turing Robot offers a package of 500 types of life service skills to accommodate product needs.\n\nThe Turing Robot comprehensively integrates over 500 kinds of life information and service skills into software and hardware products. Inquiries can be made about recipes, weather, delivery services, among other things.\n\nFeatures of the Turing Robot include personalized settings, keyword filtering, robot training, accuracy rate setting, and data statistics.\n\nCurrently, there are more than 15 application cases for the Turing Robot, including smart robots, toys, vehicle systems, QQ robots, smart home, and smart custom service.\n\nTuring OS is an intelligent robot operating system whose design is based on simulating the thinking mode and emotional recognition of humans, which provides natural and friendly multimode human-machine interaction methods. It includes a thinking enhancement engine, an affective computing engine and a self-learning engine.\n\nTuring OS introduces new multimodal interaction for robots that is closer to human capability that prior technology. With Turing OS installed, robots can understand voice, text, images, body movement, and other multimodal data from external sources such as humans. Meanwhile, they can provide simultaneous feedback, including through voice, text, and images.\n\nThe Affective Computing Engine of Turing OS enables robots to have the same ability to demonstrate affection as humans. The Turing OS develops a mature affective computing engine for robots, which assists robots in understanding and expressing human emotions. The affective computing engine of the Turing OS has two components: human emotion recognition and human-like emotion expression.\n\nThe Cognitive Enhancement Engine of Turing OS enables robots to have the same thinking ability as humans. Based on research into human macro-thinking models and micro-thinking models, the Turing OS provides powerful thinking for robots, and enables robots to operate in multiple macro- and micro-thinking modes. As a result, these robots can have thinking ability similar to that of humans. Robots with the Turing OS attain the thinking level of children aged 4 to 5.\n\nThe Cognitive Enhancement Engine includes 26 types of macro thinking and 10,140 types of micro thinking.\n\nThe Self-learning Engine enables robots to have the same learning abilities as humans.\n\nAdopting an ultra-efficient algorithm for deep learning, Turing OS uses enormous big data sources and the operating environment of a supercomputer to provide robots with a powerful capacity for self-learning. This helps robots realize real-time fast iteration and updating of emotion recognition and expression, thinking models, knowledge construction, and adaptive scenarios.\n\nTuring OS provides application services, covering 25 applicable scenarios, including English learning, family entertainment, knowledge learning, and operations control.\n\nOn The First Innovation Conference of Turing Robot, CEO of Turing Robot, Yu Zhichen released an updated version of Turing OS: Turing OS Version 1.5, with an added visual ability to its former edition.\n\nTuring Robot Application Project was also released on the Innovation Conference of Turing Robot, it includes official applications like Robots’ Chatting, Automatic Camera, Robots Singing and English Read-after. Besides that, the platform is totally free and open to developers to build their won apps on and Weather Report, Dictionary and Music Playlist were taken for a start.\n\nBOSCH Vehicle System\n\nHTC Voice: Little Hi\n\nChina Telecom Customer Service\n\nLetou Carl Vehicle System\n\nJIMI Cat APP\n\nKido Smart Watch\n\nLeddy Robot\n\nDoraemon\n\nRobotant\n\nHahabot\n\nLuobotec\n\nAlbert Robot\n\nTuba Robot\n", "id": "49138450", "title": "Turing Robot"}
{"url": "https://en.wikipedia.org/wiki?curid=44391058", "text": "Winograd Schema Challenge\n\nThe Winograd Schema Challenge (WSC) is a test of machine intelligence proposed by Hector Levesque, a computer scientist at the University of Toronto. Designed to be an improvement on the Turing test, it is a multiple-choice test that employs questions of a very specific structure: they are instances of what are called Winograd Schemas, named after Terry Winograd, a professor of computer science at Stanford University.\n\nOn the surface, Winograd Schema questions simply require the resolution of anaphora: the machine must identify the antecedent of an ambiguous pronoun in a statement. This makes it a task of natural language processing, but Levesque argues that for Winograd Schemas, the task requires the use of knowledge and commonsense reasoning.\n\nNuance Communications announced in July 2014 that it would sponsor an annual WSC competition, with a prize of $25,000 for the best system that could match human performance.\n\nThe Winograd Schema Challenge was proposed in the spirit of the Turing Test. Proposed by Alan Turing in 1950, the Turing Test plays a central role in the philosophy of artificial intelligence. Turing proposed that instead of debating what intelligence is, the science of AI should be concerned with demonstrating intelligent behavior, which can be tested. But the exact nature of the test Turing proposed has come under scrutiny, especially since an AI chat bot named Eugene was claimed to pass it in 2014. The Winograd Schema Challenge was proposed in part to ameliorate the problems that came to light with the nature of the programs that performed well on the test.\n\nTuring's original proposal was what he called the Imitation Game, which involves free-flowing, unrestricted conversations in English between human judges and computer programs over a text-only channel (such as teletype). In general, the machine passes the test if interrogators are not able to tell the difference between it and a human in a five-minute conversation.\n\nOn June 7, 2014, a computer program named Eugene Goostman was declared to be the first AI to have passed the Turing Test in a competition held by the University of Reading in England. In the competition Eugene was able to convince 33% of judges that they were talking with a 13-year-old Ukrainian boy. The supposed victory of a machine that thinks aroused controversies about the Turing Test. Critics claimed that Eugene passed the test simply by fooling the judge and taking advantages of its purported identity. For example, it could easily skip some key questions by joking around and changing subjects. However, the judge would forgive its mistakes because Eugene identified as a teenager who spoke English as his second language.\n\nThe performance of Eugene Goostman exhibited some of the Turing Test's problems. Levesque identifies several major issues, summarized as follows:\n\nThe key factor in the WSC is the special format of its questions, which are derived from Winograd Schemas. Questions of this form may be tailored to require knowledge and commonsense reasoning in a variety of domains. They must also be carefully written not to betray their answers by selectional restrictions or statistical information about the words in the sentence.\n\nThe first cited example of a Winograd Schema (and the reason for their namesake) is due to Terry Winograd:\nThe choices of \"feared\" and \"advocated\" turn the schema into its two instances:\n\nThe question is whether the pronoun \"they\" refers to the city councilmen or the demonstrators, and switching between the two instances of the schema changes the answer. The answer is immediate for a human reader, but proves difficult to emulate in machines. Levesque argues that knowledge plays a central role in these problems: the answer to this schema has to do with our understanding of the typical relationships between and behavior of councilmen and demonstrators.\n\nSince the original proposition of the Winograd Schema Challenge, Ernest Davis, a professor at New York University, has compiled a list of over 140 Winograd Schemas from various sources as examples of the kinds of questions that should appear on the Winograd Schema Challenge.\n\nA Winograd Schema Challenge question consists of three parts:\n\nA machine will be given the problem in a standardized form which includes the answer choices, thus making it a binary decision problem.\n\nThe Winograd Schema Challenge has the following purported advantages:\n\nOne difficulty with the Winograd Schema Challenge is the development of the questions. They need to be carefully tailored to ensure that they require commonsense reasoning to solve. For example, Levesque gives the following example of a so-called Winograd Schema that is \"too easy\":\nThe answer to this question can be determined on the basis of selectional restrictions: in any situation, pills do not get pregnant, women do; women cannot be carcinogenic, but pills can. Thus this answer could be derived without the use of reasoning, or any understanding of the sentences' meaning—all that is necessary is data on the selectional restrictions of \"pregnant\" and \"carcinogenic.\"\n\nAnother limitation pointed out by Peng, Khashabi and Roth \nis limitation of evaluation as a binary task; typically the pronoun can refer to one of the two previous mentions. On the other hand, the general anaphora resolution problem is a clustering task, rather than a binary task. They propose a modified version of the task and modified evaluation metric to make it more similar and applicable to the co-reference problem.\n\nAn annual competition sponsored by Nuance Communications, Inc. will be organized, administered, and evaluated by CommonsenseReasoning.org. The winner will receive a grand prize of $25,000.\n\nThe Twelfth International Symposium on the Logical Formalizations of Commonsense Reasoning was held on March 23–25, 2015 at the AAAI Spring Symposium Series at Stanford University, with a special focus on the Winograd Schema Challenge. The organizing committee included Leora Morgenstern (Leidos), Theodore Patkos (The Foundation for Research & Technology Hellas), and Robert Sloan (University of Illinois at Chicago).\n\nThe 2016 Winograd Schema Challenge was run on July 11, 2016 at IJCAI-16. There were four contestants. The highest score achieved was 58% correct, by Quan Liu et al, of the University of Science and Technology, China. Hence, by the rules of that challenge, no prizes were awarded, and the challenge did not proceed to the second round. The organizing committee in 2016 was Leora Morgenstern, Ernest Davis, and Charles Ortiz. The next running of the challenge will be at AAAI-18.\n\n", "id": "44391058", "title": "Winograd Schema Challenge"}
{"url": "https://en.wikipedia.org/wiki?curid=32856741", "text": "NewsRx\n\nNewsRx is a media and technology company focusing on digital media, printed media, news services, and knowledge discovery through its BUTTER platform. In 1995 the company was the world’s largest producer of health news. The company publishes 194 newsweeklies in health and other fields, which are distributed to subscribers and partners including Factiva, the \"Wall Street Journal Professional Edition\", Thomson Reuters, ProQuest, and Cengage Learning. C W Henderson founded the company in 1984 and its first publication was \"AIDS Weekly.\" In the early 2000s, the firm added the imprint, VerticalNews to publish newsweeklies in non-health fields. Now based in Atlanta, Georgia, the company reports through its daily news service and publishes reference books through its partner, ScholarlyEditions. NewsRx launched its BUTTER platform in 2015, which is a knowledge discovery engine that delivers its content to academics, researchers, and professionals.\n\nThe idea for the first newsletter originated at an international conference on AIDS sponsored by the Centers for Disease Control & Prevention (CDC). A staff member commented to CW Henderson on the need for a publication to condense the rapid rise in information about the disease. In 1984, Henderson created CW Henderson Publisher, which became NewsRx in 2004.\n\nThat same year, the company distributed its first journal, \"CDC AIDS Weekly\", (which split into \"AIDS Weekly\" and \"Medical Letter on the CDC & FDA\") to an international audience. The first subscriber was the Soviet Union. Other subscribers include physicians, educators, government agencies, and pharmaceutical companies.\n\nThe articles in \"AIDS Weekly\" discussed social issues of the disease to medical research. The newsweekly included “shorts” to explain as much as was known about unfolding information and events.\n\nBefore the World Wide Web, NewsRx coordinated with the National AIDS Information Clearinghouse to provide information on the disease. The \"CDC AIDS Weekly Infoline\" provided a list of upcoming AIDS seminars as well as names and addresses of over 65 AIDS periodicals published worldwide.\n\nThe information published in \"AIDS Weekly\" came primarily from the government organization Centers for Disease Control (CDC). Though the newsweekly had no direct ties to the CDC other than as a source for information, a CDC official described the publication as “highly informative.” Other sources of information for this and other titles were the nearby Emory University medical library and international agencies. Articles included summaries of peer-reviewed research, conference reports, news releases, and compilations from other health and medical organizations.\n\nHowever, in the beginning, some critics were offended by the fact that NewsRx was a non-governmental agency distributing statistics that were available for free in official versions frrm the government. (see Controversy)\n\nIn 1988, the firm added \"Cancer Weekly\" and it added \"Blood Weekly\" in 1993. The company added \"Vaccine Weekly\" in 1995, followed by over 100 more medical-related titles.\n\nIn 2007, the firm introduced VerticalNews, a group of newsweeklies that included 86 new non-medical related titles, thus expanding the company’s reader-base. The initiative also emphasized the distribution of information on a global scale (more than 50 countries). Electronic versions of the publications are available. In addition to the 103 newsweeklies in medical, legal, and business fields within the healthcare industry, the firm publishes 88 newsweeklies within the VerticalNews imprint.\n\nThe firm also adopted site licenses, including the ability for users to download reports showing the types of information used in a given organization—information previously restricted to the NewsRx staff. The system recognizes IP addresses to facilitate research activities.\n\nIn 1999, the firm also adopted \"Artificial Intelligence Journalist\" (AIJ) which uses robotics, machine learning, algorithms, logic, and automated reasoning to provide computer-assisted reporting and data driven journalism. This software program shortens the time from news event to news distribution.\n\nOn April 22, 2015, NewsRx announced hiring new VP and Publisher Kalani Rosell. The business development office opened in 2016 in New Haven, Connecticut, headed by Rosell.\n\nIn 2015, NewsRx started BUTTER, which stands for Better Understanding Through Technology & Emerging Research, a business intelligence and data analytics platform with emerging research and new discoveries. It has content for researchers, academics, and investors, using a New Discovery Index (NDI) that analyzes discoveries worldwide by quarter and new discoveries within specific topic areas.\n\nBUTTER uses a search engine and publishes 10,000 new articles a day (11.4 million articles as of March, 2016).\n\nBUTTER’s platform creates content 30 minutes after stock markets close, monitoring all market movements, new SEC and patent filings, trademarks, and financial and investment decisions.\n\nThroughout most of its operating history, the NewsRx staff included board-certified medical editors with bachelor's degrees in journalism or related fields and experience in writing and editing. Supervisors with PhDs and master's degrees in English, Journalism, Management, and Information Technology. NewsRx editors condense news so that each article is less than 1000 words, while including key sentences of the original report and full bibliographic references and citations to original source material.\n\nNewsRx is staffed by journalists rather than medical professionals. At the company’s beginnings, \"Newsweek\" magazine commented that \"AIDS Weekly\", as a non-government entity, should not be reporting on topics that included policy, research, and statistics that some considered exclusive to the government. The head of the Centers for Disease Control and Prevention (CDC) AIDS task force at the time was misquoted as stating that he disagreed with having the CDC name associated with the newsweekly. On the contrary, every issue of the \"CDC AIDS Weekly\" included an advisory caption, “…not sponsored by, endorsed by, affiliated with, or officially connected with the CDC.” Other staffers within the CDC supported NewsRx’s view to bring AIDS awareness to the public eye. The \"Boston Globe\" reported that \"AIDS Weekly\" was a necessary “watchdog” publication providing needed information to the public. Other articles appeared supporting NewsRx in the \"Wall Street Journal\", \"The New York Times\", and \"USA Today\", for what they said to be its impact in AIDS awareness and investigative journalism.\n\nCW Henderson’s role as executive editor at the firm was discussed in an article in \"Editor and Publisher\", focusing on the influence of pharmaceutical companies on news publications. Henderson opposed pharmaceutical company influence on reporters as well as premature reporting of experiments.\nThe firm was also involved with \"The New York Times\" in controversial breaking news about AIDS studies that had purposely been tampered with at the CDC. On at least 5 occasions, research on the causes of AIDS and other viral diseases might have been tampered with. \"CDC AIDS Weekly\" published an internal CDC memorandum on the incident.\n\nIn 2010, the firm's VerticalNews China was the subject of a denial of service attack that originated from China as a result of controversial news reported. The attack was halted when the company’s IP service identified the source and blocked it.\n\nIn 2011, the firm partnered with ScholarlyMedia’s ScholarlyEditions imprint, publishing 4,000 reference books, which replaced the EncyK line. The president of NewsRx is also president of ScholarlyMedia. The company’s book imprint is ScholarlyEditions, and its peer reviewed news service is ScholarlyNews.\n\nThe company's partners include:\n\nThe company's rankings include:\n", "id": "32856741", "title": "NewsRx"}
{"url": "https://en.wikipedia.org/wiki?curid=40257392", "text": "Brain technology\n\nBrain technology, or self-learning know-how systems, defines a technology that employs latest findings in neuroscience. The term was first introduced by the Artificial Intelligence Laboratory in Zurich, Switzerland, in the context of the Roboy project. Brain Technology can be employed in robots, know-how management systems and any other application with self-learning capabilities. In particular, Brain Technology applications allow the visualization of the underlying learning architecture often coined as “know-how maps”.\n", "id": "40257392", "title": "Brain technology"}
{"url": "https://en.wikipedia.org/wiki?curid=50285983", "text": "Behavior informatics\n\nBehavior informatics (BI) is the informatics of behaviors so as to obtain behavior intelligence and behavior insights.\n\nDifferent from applied behavior analysis from the psychological perspective, BI builds computational theories, systems and tools to qualitatively and quantitatively model, represent, analyze, and manage behaviors of individuals, groups and/or organizations .\n\nBI is built on classic study of behavioral science, including behavior modeling, applied behavior analysis, behavior analysis, behavioral economics, and organizational behavior. Typical BI tasks consist of individual and group behavior formation, representation, computational modeling, analysis, learning, simulation, and understanding of behavior impact, utility, non-occurring behaviors etc. for behavior intervention and management.\n\nBehavior informatics covers \"behavior analytics\" which focuses on analysis and learning of behavioral data.\n\nFrom an informatics perspective, a behavior consists of four key elements: actors (behaviorial subjects and behavioral objects), operations (actions, activities) and interactions (relationships), and their properties. A behavior can be represented as a behavior vector, all behaviors of an actor or an actor group can be represented as behavior sequences and multi-dimensional behavior matrix.\n", "id": "50285983", "title": "Behavior informatics"}
{"url": "https://en.wikipedia.org/wiki?curid=50336055", "text": "Glossary of artificial intelligence\n\n\"Contributors are needed to write definitions for terms in this glossary. Please keep the definitions brief and in accordance with Wikipedia's policies and guidelines.\"\n\nThis glossary of artificial intelligence terms is about artificial intelligence, its sub-disciplines, and related fields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "50336055", "title": "Glossary of artificial intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=34600216", "text": "Color moments\n\nColor moments are measures that characterise color distribution in an image in the same way that central moments uniquely describe a probability distribution. Color moments are mainly used for color indexing purposes as features in image retrieval applications in order to compare how similar two images are based on color. Usually one image is compared to a database of digital images with pre-computed features in order to find and retrieve a similar image. Each comparison between images results in a similarity score, and the lower this score is the more identical the two images are supposed to be.\n\nColor moments are scaling and rotation invariant. It is usually the case that only the first three color moments are used as features in image retrieval applications as most of the color distribution information is contained in the low-order moments. Since color moments encode both shape and color information they are a good feature to use under changing lighting conditions, but they cannot handle occlusion very successfully. Color moments can be computed for any color model. Three color moments are computed per channel (e.g. 9 moments if the color model is RGB and 12 moments if the color model is CMYK). Computing color moments is done in the same way as computing moments of a probability distribution.\n\nThe first color moment can be interpreted as the average color in the image, and it can be calculated by using the following formula\n\nwhere N is the number of pixels in the image and formula_2 is the value of the j-th pixel of the image at the i-th color channel.\n\nThe second color moment is the standard deviation, which is obtained by taking the square root of the variance of the color distribution.\n\nwhere formula_4 is the mean value, or first color moment, for the i-th color channel of the image.\n\nThe third color moment is the skewness. It measures how asymmetric the color distribution is, and thus it gives information about the shape of the color distribution. Skewness can be computed with the following formula:\n\nKurtosis is the fourth color moment, and, similarly to skewness, it provides information about the shape of the color distribution. More specifically, kurtosis is a measure of how flat or tall the distribution is in comparison to normal distribution.\n\nHigher-order color moments are usually not part of the color moments feature set in image retrieval tasks as they require more data in order to obtain a good estimate of their value, and also the lower-order moments generally provide enough information.\n\nColor moments have significant applications in image retrieval. They can be used in order to compare how similar two images are. This is a relatively new approach to color indexing. The greatest advantage of using color moments comes from the fact that there is no need to store the complete color distribution. This greatly speeds up image retrieval since there are less features to compare. In addition, the first three color moments have the same units, which allows for comparison between them.\n\nColor indexing is the main application of color moments. Images can be indexed, and the index will contain the computed color moments. Then, if someone has a particular image and wants to find similar images in the database, the color moments of the image of interest will also be computed. After that the following function will be used in order to compute a similarity score between the image of interest and all the images in the database:\nwhere:\nFinally, the images in the database will be ranked according to the computed similarity score with the image of interest, and the database images with the lowest formula_14 value should be retrieved. \"A retrieval based on formula_14 may produce false positives because the index contains no information about the correlation between the color channels\".\n\nA simple and concise example of the use of color moments for image retrieval tasks is illustrated in.\n\nConsider having several test images in a database and a \"New Image\". The goal is to retrieve images from the database that are similar to the \"New Image\". The first three color moments are used as features. There are several steps in this computation.\n\n\n\n\n", "id": "34600216", "title": "Color moments"}
{"url": "https://en.wikipedia.org/wiki?curid=50672378", "text": "Gomocup\n\nGomocup is a worldwide tournament of artificial intelligences (AI) playing Gomoku and Renju. The tournament has been played since 2000 and takes place every year. As of 2016, it is the most famous and largest Gomoku AI tournament in the world, with around 40 participants from about 10 countries.\n\nGomocup has been played in the freestyle Gomoku rule with the board size of 20 since it was started in 2000. In 2009, the standard Gomoku rule was added into Gomocup as a tournament, in which the board size is 15 and more than five in a row is not considered to be a win. In 2016, the Renju rule was also added into Gomocup, with a board size of 15 and forbidden moves for black. In particular, since there are a large number of participants in the freestyle Gomoku tournament, the freestyle Gomoku tournament is divided into several leagues, and the fast game tournament is introduced.\n\nTo get rid of the fact that there is a winning strategy for the player who plays first in Gomoku, balanced openings have been prepared by Gomoku experts since 2006. Games would be started from these balanced openings, and neither side would have a big advantage from the very beginning.\n\nThere were two AI vs. Human tournaments held in the Czech Republic in 2006 and 2011.\n\nIn 2006, the top 3 programs in Gomocup had a tournament with 3 of the top 10 players in Piškvorky online. There were 2 games between each pair of AI and human players. The result was one win, one draw and one loss for AIs, and the total score was 3:3.\n\nIn 2011, the tournament was between the top 4 programs in Gomocup and 4 players at the top of the Czech Gomoku rating list. Similar to the 1st tournament, there were 2 games between each pair of AI. This time there were 3 draws and 1 win for AIs, and the total score was 5:3.\n\nThe Elo rating system for Gomocup was built in 2016 and calculated with all the historical tournament results ever since. The rating is calculated with the open-source tool BayesElo, with a few parameters modified to get adapted to the Gomoku game. There is a rating list for each game rule. The Elo ratings are updated every year after the Gomocup tournament finishes.\n\nThe results for the Gomocup tournaments since 2000 is in the following.\n", "id": "50672378", "title": "Gomocup"}
{"url": "https://en.wikipedia.org/wiki?curid=50562089", "text": "Robot lawyer\n\nA robot lawyer or a robo-lawyer refers to a legal AI application that can perform tasks that are typically done by paralegals or young associates at law firms. However, there is some debate on the correctness of the term. Some commentators say that legal AI is technically speaking neither a lawyer nor a robot and should not be referred to as such. Other commentators believe that the term can be misleading and note that the robot lawyer of the future won't be one all-encompassing application but a collection of specialized bots for various tasks.\n\nSome legal AI solutions are developed and marketed directly to the customers or consumers, whereas other applications are tools for the attorneys at law firms. Lawbots.info has a catalogue of automated legal AI services that have been developed for the direct distribution to the public. One notable legal AI solution for the law firms is ROSS, which has been used by US law firms to assist in legal research, but there are already hundreds of legal AI solutions that operate in multitude of ways varying in sophistication and dependence on scripted algorithms.\n\n\n", "id": "50562089", "title": "Robot lawyer"}
{"url": "https://en.wikipedia.org/wiki?curid=51291103", "text": "Mivar-based approach\n\nThe Mivar-based approach is a mathematical tool for designing artificial intelligence (AI) systems. Mivar (\"Multidimensional Informational Variable Adaptive Reality\") was developed by combining production and Petri nets. The Mivar-based approach was developed for semantic analysis and adequate representation of humanitarian epistemological and axiological principles in the process of developing artificial intelligence. The Mivar-based approach incorporates computer science, informatics and discrete mathematics, databases, expert systems, graph theory, matrices and inference systems. The Mivar-based approach involves two technologies:\nMivar networks allow us to develop cause-effect dependencies (“If-then”) and create an automated, trained, logical reasoning system.\n\nRepresentatives of Russian association for artificial intelligence (RAAI) – for example, V. I. Gorodecki, doctor of technical science, professor at SPIIRAS and V. N. Vagin, doctor of technical science, professor at MPEI declared that the term is incorrect and suggested that the author should use standard terminology.\n\nWhile working in the Russian Ministry of Defense, O. O. Varlamov started developing the theory of “rapid logical inference” in 1985. He was analyzing Petri nets and productions to construct algorithms. Generally, mivar-based theory represents an attempt to combine entity-relationship models and their problem instance – semantic networks and Petri networks. \n\nThe abbreviation MIVAR was introduced as a technical term by O. O. Varlamov, Doctor of Technical Science, professor at Bauman MSTU in 1993 to designate a “semantic unit” in the process of mathematical modeling. The term has been established and used in all of his further works.\n\nThe first experimental systems operating according to mivar-based principles were developed in 2000. Applied mivar systems were introduced in 2015.\n\nMivar is the smallest structural element of discrete information space.\n\nObject-Property-Relation (VSO) is a graph, the nodes of which are concepts and arcs are connections between concepts.\n\nMivar space represents a set of axes, a set of elements, a set of points of space and a set of values of points.\n\nformula_1\n\nwhere:\n\n\nThen: <math>\\forall a_n \\exists F_n =\\{ f_\n\n", "id": "51291103", "title": "Mivar-based approach"}
{"url": "https://en.wikipedia.org/wiki?curid=51295111", "text": "Schema-agnostic databases\n\nSchema-agnostic databases or vocabulary-independent databases aim at supporting users to be abstracted from the representation of the data, supporting the automatic semantic matching between queries and databases. Schema-agnosticism is the property of a database of mapping a query issued with the user terminology and structure, automatically mapping it to the dataset vocabulary.\n\nThe increase in the size and in the semantic heterogeneity of database schemas bring new requirements for users querying and searching structured data. At this scale it can become unfeasible for data consumers to be familiar with the representation of the data in order to query it. At the center of this discussion is the semantic gap between users and databases, which becomes more central as the scale and complexity of the data grows.\n\nThe evolution of data environments towards the consumption of data from multiple data sources and the growth in the \"schema size\", \"complexity\", \"dynamicity\" and \"decentralisation\" (SCoDD) of schemas increases the complexity of contemporary data management. The SCoDD trend emerges as a central data management concern in Big Data scenarios, where users and applications have a demand for more complete data, produced by independent data sources, under different semantic assumptions and contexts of use, which is the typical scenario for Semantic Web Data applications.\n\nThe evolution of databases in the direction of heterogeneous data environments strongly impacts the usability, semiotics and semantic assumptions behind existing data accessibility methods such as structured queries, keyword-based search and visual query systems. With schema-less databases containing potentially millions of dynamically changing attributes, it becomes unfeasible for some users to become aware of the 'schema' or vocabulary in order to query the database. At this scale, the effort in understanding the schema in order to build a structured query can become prohibitive.\n\nSchema-agnostic queries can be defined as query approaches over structured databases which allow users satisfying complex information needs without the understanding of the representation (schema) of the database. Similarly, Tran et al. defines it as \"search approaches, which do not require users to know the schema underlying the data\". Approaches such as keyword-based search over databases allow users to query databases without employing structured queries. However, as discussed by Tran et al.: \"From these points, users however have to do further navigation and exploration to address complex information needs. Unlike keyword search used on the Web, which focuses on simple needs, the keyword search elaborated here is used to obtain more complex results. Instead of a single set of resources, the goal is to compute complex sets of resources and their relations.\"\n\nThe development of approaches to support natural language interfaces (NLI) over databases have aimed towards the goal of schema-agnostic queries. Complementarily, some approaches based on keyword search have targeted keyword-based queries which express more complex information needs. Other approaches have explored the construction of structured queries over databases where schema constraints can be relaxed. All these approaches (natural language, keyword-based search and structured queries) have targeted different degrees of sophistication in addressing the problem of supporting a flexible semantic matching between queries and data, which vary from the completely absence of the semantic concern to more principled semantic models. \nWhile the demand for schema-agnosticism has been an implicit requirement across semantic search and natural language query systems over structured data, it is not sufficiently individuated as a concept and as a necessary requirement for contemporary database management systems. Recent works have started to define and model the semantic aspects involved on schema-agnostic queries.\n\nConsist of schema-agnostic queries following the syntax of a structured standard (for example SQL, SPARQL). The syntax and semantics of operators are maintained, while different terminologies are used.\n\nwhich maps to the following SPARQL query in the dataset vocabulary:\n\nwhich maps to the following SPARQL query in the dataset vocabulary:\n\nConsist of schema-agnostic queries using keyword queries. In this case the syntax and semantics of operators are different from the structured query syntax.\n\nAs of 2016 the concept of schema-agnostic queries has been developed primarily in academia. Most of schema-agnostic query systems have been investigated in the context of Natural Language Interfaces over databases or over the Semantic Web. These works explore the application of semantic parsing techniques over large, heterogeneous and schema-less databases.\nMore recently, the individuation of the concept of schema-agnostic query systems and databases have appeared more explicitly within the literature. Freitas et al. provide a probabilistic model on the semantic complexity of mapping schema-agnostic queries.\n", "id": "51295111", "title": "Schema-agnostic databases"}
{"url": "https://en.wikipedia.org/wiki?curid=52036598", "text": "Differentiable neural computer\n\nA differentiable neural computer (DNC) is a recurrent artificial neural network architecture with an autoassociative memory. The model was published in 2016 by Alex Graves et al. of DeepMind. \n\nSo far, DNCs have only been demonstrated to handle relatively simple tasks, which could have been easily solved using conventional computer programming decades ago. But DNCs don't need to be programmed for each problem they are applied to, but can instead be trained. This attention span allows the user to feed complex data structures such as graphs sequentially, and recall them during later use. Furthermore, they can learn some aspects of symbolic reasoning and apply it to the use of working memory. Some experts see promise that they can be trained to perform complex, structured tasks and address big-data applications that require some sort of rational reasoning, such as generating video commentaries or semantic text analysis.\n\nDNC can be trained to navigate a variety of rapid transit systems, and then what the DNC learns can be applied, for example, to get around on the London Underground. A neural network without memory would typically have to learn about each different transit system from scratch. On graph traversal and sequence-processing tasks with supervised learning, DNCs performed better than alternatives such as long short-term memory or a neural turing machine. With a reinforcement learning approach to a block puzzle problem inspired by SHRDLU, DNC was trained via curriculum learning, and learned to make a plan. It performed better than a traditional recurrent neural network.\n\nDNC networks were introduced as an extension of the Neural Turing Machine (NTM), with the addition of memory attention mechanisms that control where the memory is stored, and temporal attention that records the order of events. This structure allows DNCs to be more robust and abstract than a NTM, and still perform tasks that have longer-term dependencies than some of its predecessors such as the LSTM network. The memory, which is simply a matrix, can be allocated dynamically and accessed indefinitely. The DNC is differentiable end-to-end (each subcomponent of the model is differentiable, therefore so is the whole model). This makes it possible to optimize them efficiently using gradient descent. It learns how to store and retrieve the information such that it satisfies the task execution.\n\nThe DNC model is similar to the Von Neumann architecture, and because of the resizability of memory, it is turing complete. Differentiable Neural Computers were inspired by the mammalian hippocampus\n\nDNC, as originally published \n\n</math>\n\nRefinements to the model have been published since the original paper's release. Sparse memory addressing results in a time and space complexity reduction of thousands of times. This can be achieved by using an approximate nearest neighbors algorithm, such as Locality-sensitive hashing, or a random k-d tree like the Fast Library for Approximate Nearest Neighbors from UBC. Adding Adaptive Computation Time (ACT) separates computation time from data time, which uses the fact that problem length and problem difficulty are not always the same. Training using synthetic gradients performs considerably better than Backpropagation through time (BPTT).\n", "id": "52036598", "title": "Differentiable neural computer"}
{"url": "https://en.wikipedia.org/wiki?curid=51885204", "text": "The Fable of Oscar\n\nThe Fable of Oscar is a fable proposed by John L. Pollock in his book \"How to Build a Person\" () to defend the idea of token physicalism, agent materialism, and strong AI. It ultimately illustrates what is needed for an Artificial Intelligence to be built and why humans are just like intelligent machines.\n\nOnce in a distant land there lived a race of Engineers. They have all their physical needs provided by the machines they have invented. One of the Engineers decide that he will create an \"intelligent machine\" that is much more ingenious than the more machines, in that it can actually sense, learn, and adapt to its environment as an intelligent animal.\n\nThe first version of the machine is called \"Oscar I\". It has pain sensors and \"fight-or-flight\" responses build within to help it survive hostile environment. In this stage Oscar I is much like the machines Hilary Putnam considers in 1960.\n\nIn order for Oscar I to avoid damages in hostile environment, it must not only be able to respond to its pain sensors but also predict what is likely to happen based on its generalization of its pain sensor activations. Therefore, a \"pain sensor sensor\" was built to sense its pain sensors, thus giving it a rudimentary self-awareness. In this stage Oscar I is much like an amoeba as Oscar II like a worm. Amoebas respond to pain while worms learn to avoid it.\n\nThe problem with Oscar II is that it has no conception if the environment is fooling him. For example, he can't distinguish if a machine-eating tiger and a mirror image of such tiger. To solve such problem, \"introspective sensors\" were built into Oscar II and made him \"Oscar III\". Oscar III can now sense the operation of its own sensors and form generalization about its reliability, thus acquired a higher degree of self-awareness. In this stage Oscar II is much like a bird as Oscar III a kitten. Kittens quickly learn about mirror image and come to ignore them while birds go on attacking their own reflection until they become exhausted.\n\nConsider a world populated by Oscarites. If the Oscarites are sufficiently intelligent, it can philosophizing the difference between their outward physical state and inward mental state. While we, from our perspective, describe the Oscarites as sensing the operation of their perceptual sensors, they describe it as they are \"being self-aware and being conscious\".\n\nIn the end of the fable Pollock states that while the Engineers are fictional, Oscar is real and we are in fact the Oscarites.\n\n\n", "id": "51885204", "title": "The Fable of Oscar"}
{"url": "https://en.wikipedia.org/wiki?curid=51237053", "text": "Wojciech Zaremba\n\nWojciech Zaremba (born November 30, 1988) is a Polish mathematician and computer scientist, noted for his work on artificial neural networks and deep learning. Zaremba is head of robotics, and founding team member of OpenAI, which mission is to build safe artificial intelligence (AI), and ensure that its benefits are as evenly distributed as possible.\n\nZaremba was born in 1988 in Kluczbork, Poland. At a young age, he won local competitions and awards in mathematics, computer science, chemistry and physics. In 2007, Zaremba represented Poland in the International Mathematical Olympiad, and won a silver medal.\n\nZaremba studied at the University of Warsaw and École Polytechnique, and graduated in 2013 with two master's degrees in mathematics. He then began his PhD at New York University (NYU) in deep learning under the supervision of Yann LeCun and Rob Fergus. Zaremba graduated and received his PhD in 2015.\n\nDuring his undergraduate years at the University of Warsaw, Zaremba completed several internships for American technology company NVIDIA.\n\nIn the following years, Zaremba worked on an internship at Google where he reproduced state-of-the-art object-recognition modeling developed originally by DNNresearch. Elements of this model were used in Google+’s photo search feature.\n\nNext, he worked at Google Brain, and in the following year, Zaremba spent time at Facebook AI Research under the supervision of Prof. Rob Fergus and Prof. Yann LeCun\n\nIn 2015, Zaremba was one of the 10 co-founders of OpenAI, a non-profit artificial intelligence (AI) research company. The other co-founders were Ilya Sutskever, Greg Brockman, formerly the CTO of Stripe, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, John Schulman, and Pamela Vagata.\nZaremba sits on the advisory board of Growbots, a Silicon Valley startup company aiming to automate sales processes with the use of machine learning and artificial intelligence.\n\n", "id": "51237053", "title": "Wojciech Zaremba"}
{"url": "https://en.wikipedia.org/wiki?curid=14842794", "text": "Artificial imagination\n\nArtificial imagination, also called synthetic imagination or machine imagination, is defined as the artificial simulation of human imagination by general or special purpose computers or artificial neural networks.\n\nThe term artificial imagination is also used to describe a property of machines or programs. Some of the traits that researchers hope to simulate include creativity, vision, digital art, humor, and satire.\n\nArtificial imagination research uses tools and insights from many fields, including computer science, rhetoric, psychology, creative arts, philosophy, neuroscience, affective computing, Artificial Intelligence, cognitive science, linguistics, operations research, creative writing, probability and logic.\n\nPractitioners in the field are researching various aspects of Artificial imagination, such as Artificial (visual) imagination,\nArtificial (aural) Imagination, modeling/filtering content based on human emotions and Interactive Search. Some articles on the topic speculate on how artificial imagination may evolve to create an artificial world \"people may be comfortable enough to escape from the real world\".\n\nSome researchers such as G. Schleis and M. Rizki have focused on using artificial neural networks to simulate artificial imagination.\n\nAnother important project is being led by Hiroharu Kato and Tatsuya Harada at the University of Tokyo in Japan. They have developed a computer capable to translate a description of an object into an image, which could be the easiest way to define what imagination is. Their idea is based on the concept of image as a series of pixels divided into short sequences that correspond to a specific part of an image. The scientists call this sequences “visual words” and those can be interpreted by the machine using statistical distribution to read an create an image of an object that in fact the machine does not knows how it look like.\n\nThe topic of artificial imagination has gotten interest from scholars outside the computer science domain, such as noted communications scholar Ernest Bormann, who came up with the Symbolic Convergence Theory and worked on a project to develop artificial imagination in computer systems. An interdisciplinary research seminar on artificial imagination and postdigital art has been taking place since 2017 at the Ecole Normale Supérieure in Paris.\n\n\"How to Build a Mind: Toward Machines with Imagination\" by Igor Aleksander is a good academic book on the topic; \"Artificial Imagination\", a roman à clef, is a good non-academic book supposedly written by an Artificial imagination system.\n\nThe typical application of artificial imagination is for an interactive search. Interactive searching has been developed since the mid-1990s, accompanied by the world wide web's development and the optimization of search engines. Based on the first query and feedback from a user, the databases to be searched are reorganized to improve the searching results.\n\n\"How artificial imagination can contribute to interactive search\"\n\nArtificial imagination allows us to synthesize images and to develop a new image, whether it is in the database, regardless its existence in the real world. For example, the computer shows results that are based on the answer from the initial query. The user selects several relevant images, and then the technology analyzes these selections and reorganizes the images' ranks to fit the query. In this process, artificial imagination is used to synthesize the selected images and to improve the searching result with additional relevant synthesized images. This technique is based on several algorithms, including the Rocchio algorithm and the evolutionary algorithm. The \"Rocchio algorithm\", locating a query point near relevant examples and far away from irrelevant examples, is simple and works well in a small system where the databases are arranged in certain ranks. The \"evolutionary synthesis\" is composed of two steps: a standard algorithm and an enhancement of the standard algorithm. Through feedback from the user, there would be additional images synthesized so as to be suited to what the user is looking for.\n\nArtificial imagination has a more general definition and wide applications. The traditional fields of artificial imagination include visual imagination and aural imagination. More generally, all the actions to form ideas, images and concepts can be linked to imagination. Thus, artificial imagination means more than only generating graphs. For example, moral imagination is an important research subfield of artificial imagination, although classification of artificial imagination is difficult. \n\nMorals are an important part to human beings' logic, while artificial morals are important in artificial imagination and artificial intelligence. A common criticism of artificial intelligence is whether human beings should take responsible for machines‘ mistake or decisions and how to develop well-behaved machines. As nobody can give a clear description of the best moral rules, it is impossible to create machines with commonly accepted moral rules. However, recent research about artificial morals circumvent the definition of moral. Instead, machine learning methods are applied to train machines to imitate human morals. As the data about moral decisions from thousands of different people are considered, the trained moral model can reflect widely accepted rules. \n\nMemory is another big field of artificial imagination. Researchers like Dr. Aude Oliva have done extensive work on artificial memory, especially visual memory. Compared to visual imagination, the visual memory focuses more on how machine understand, analyse and store pictures in a human way. In addition, characters like spatial features are also considered. As this field is based on the brains' biological structures, extensive research on neuroscience has also been done, which makes it a large intersection between biology and computer science.\n", "id": "14842794", "title": "Artificial imagination"}
{"url": "https://en.wikipedia.org/wiki?curid=52642349", "text": "AIVA\n\nAIVA (Artificial Intelligence Virtual Artist) is a deep learning algorithm applied to music composition. In June 2016, it became the first system of algorithmic composition to be registered, as a composer, in an authors' right Society SACEM.\n\nCreated in February 2016, AIVA specializes in Classical and Symphonic music composition. It became the world’s first virtual composer to be recognized by a music society (SACEM).\nBy reading a large collection of existing works of classical music (written by human composers such as Bach, Beethoven, Mozart) AIVA is capable of understanding concepts of music theory and composing on its own. The algorithm AIVA is based on deep learning and reinforcement learning architectures\n\nAIVA is a published composer; its first studio album “Genesis” was released in November 2016 and counts 20 original and 4 orchestrated works composed by AIVA. The tracks were recorded by human musicians:\nOlivier Hecho as the Conductor of the Aiva Sinfonietta Orchestra and Eric Breton as a Pianist. \n\n\nTrack listing:\n\nAvignon Symphonic Orchestra [ORAP] also performed Aiva's compositions in April 2017.\n\nThis is the preview of the score Op. n°3 for piano solo \"A little chamber music\", composed by AIVA.\n\n", "id": "52642349", "title": "AIVA"}
{"url": "https://en.wikipedia.org/wiki?curid=1092923", "text": "Google\n\nGoogle LLC is an American multinational technology company that specializes in Internet-related services and products. These include online advertising technologies, search, cloud computing, software, and hardware. Google was founded in 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University, in California. Together, they own about 14 percent of its shares, and control 56 percent of the stockholder voting power through supervoting stock. They incorporated Google as a privately held company on September 4, 1998. An initial public offering (IPO) took place on August 19, 2004, and Google moved to its new headquarters in Mountain View, California, nicknamed the Googleplex. In August 2015, Google announced plans to reorganize its various interests as a conglomerate called Alphabet Inc. Google, Alphabet's leading subsidiary, will continue to be the umbrella company for Alphabet's Internet interests. Upon completion of the restructure, Sundar Pichai was appointed CEO of Google; he replaced Larry Page, who became CEO of Alphabet.\n\nThe company's rapid growth since incorporation has triggered a chain of products, acquisitions, and partnerships beyond Google's core search engine (Google Search). It offers services designed for work and productivity (Google Docs, Sheets, and Slides), email (Gmail/Inbox), scheduling and time management (Google Calendar), cloud storage (Google Drive), social networking (Google+), instant messaging and video chat (Google Allo/Duo/Hangouts), language translation (Google Translate), mapping and turn-by-turn navigation (Google Maps/Waze/Earth/Street View), video sharing (YouTube), notetaking (Google Keep), and photo organizing and editing (Google Photos). The company leads the development of the Android mobile operating system, the Google Chrome web browser, and Chrome OS, a lightweight operating system based on the Chrome browser. Google has moved increasingly into hardware; from 2010 to 2015, it partnered with major electronics manufacturers in the production of its Nexus devices, and in October 2016, it released multiple hardware products (including the Google Pixel smartphone, Home smart speaker, Wifi mesh wireless router, and Daydream View virtual reality headset). The new hardware chief, Rick Osterloh, stated: \"a lot of the innovation that we want to do now ends up requiring controlling the end-to-end user experience\". Google has also experimented with becoming an Internet carrier. In February 2010, it announced Google Fiber, a fiber-optic infrastructure that was installed in Kansas City; in April 2015, it launched Project Fi in the United States, combining Wi-Fi and cellular networks from different providers; and in 2016, it announced the Google Station initiative to make public Wi-Fi available around the world, with initial deployment in India.\n\nAlexa, a company that monitors commercial web traffic, lists Google.com as the most visited website in the world. Several other Google services also figure in the top 100 most visited websites, including YouTube and Blogger. Google is the most valuable brand in the world as of 2017, but has received significant criticism involving issues such as privacy concerns, tax avoidance, antitrust, censorship, and search neutrality. Google's mission statement, from the outset, was \"to organize the world's information and make it universally accessible and useful\", and its unofficial slogan was \"Don't be evil\". In October 2015, the motto was replaced in the Alphabet corporate code of conduct by the phrase \"Do the right thing\".\n\n Google began in January 1996 as a research project by Larry Page and Sergey Brin when they were both PhD students at Stanford University in Stanford, California.\n\nWhile conventional search engines ranked results by counting how many times the search terms appeared on the page, the two theorized about a better system that analyzed the relationships among websites. They called this new technology PageRank; it determined a website's relevance by the number of pages, and the importance of those pages that linked back to the original site.\n\nPage and Brin originally nicknamed their new search engine \"BackRub\", because the system checked backlinks to estimate the importance of a site. Eventually, they changed the name to Google; the name of the search engine originated from a misspelling of the word \"googol\", the number 1 followed by 100 zeros, which was picked to signify that the search engine was intended to provide large quantities of information. Originally, Google ran under Stanford University's website, with the domains \"google.stanford.edu\" and \"z.stanford.edu\".\n\nThe domain name for Google was registered on September 15, 1997, and the company was incorporated on September 4, 1998. It was based in the garage of a friend (Susan Wojcicki) in Menlo Park, California. Craig Silverstein, a fellow PhD student at Stanford, was hired as the first employee.\n\nGoogle was initially funded by an August 1998 contribution of $100,000 from Andy Bechtolsheim, co-founder of Sun Microsystems; the money was given before Google was incorporated. Google received money from three other angel investors in 1998: Amazon.com founder Jeff Bezos, Stanford University computer science professor David Cheriton, and entrepreneur Ram Shriram.\n\nAfter some additional, small investments through the end of 1998 to early 1999, a new $25 million round of funding was announced on June 7, 1999, with major investors including the venture capital firms Kleiner Perkins Caufield & Byers and Sequoia Capital.\n\nEarly in 1999, Brin and Page decided they wanted to sell Google to Excite. They went to Excite CEO George Bell and offered to sell it to him for $1 million. He rejected the offer. Vinod Khosla, one of Excite's venture capitalists, talked the duo down to $750,000, but Bell still rejected it.\n\nGoogle's initial public offering (IPO) took place five years later, on August 19, 2004. At that time Larry Page, Sergey Brin, and Eric Schmidt agreed to work together at Google for 20 years, until the year 2024.\n\nAt IPO, the company offered 19,605,052 shares at a price of $85 per share. Shares were sold in an online auction format using a system built by Morgan Stanley and Credit Suisse, underwriters for the deal. The sale of $1.67 bn (billion) gave Google a market capitalization of more than $23bn. By January 2014, its market capitalization had grown to $397bn. The vast majority of the 271 million shares remained under the control of Google, and many Google employees became instant paper millionaires. Yahoo!, a competitor of Google, also benefitted because it owned 8.4 million shares of Google before the IPO took place.\n\nThere were concerns that Google's IPO would lead to changes in company culture. Reasons ranged from shareholder pressure for employee benefit reductions to the fact that many company executives would become instant paper millionaires. As a reply to this concern, co-founders Brin and Page promised in a report to potential investors that the IPO would not change the company's culture. In 2005, articles in \"The New York Times\" and other sources began suggesting that Google had lost its anti-corporate, no evil philosophy. In an effort to maintain the company's unique culture, Google designated a Chief Culture Officer, who also serves as the Director of Human Resources. The purpose of the Chief Culture Officer is to develop and maintain the culture and work on ways to keep true to the core values that the company was founded on: a flat organization with a collaborative environment. Google has also faced allegations of sexism and ageism from former employees. In 2013, a class action against several Silicon Valley companies, including Google, was filed for alleged \"no cold call\" agreements which restrained the recruitment of high-tech employees.\n\nThe stock performed well after the IPO, with shares hitting $350 for the first time on October 31, 2007, primarily because of strong sales and earnings in the online advertising market. The surge in stock price was fueled mainly by individual investors, as opposed to large institutional investors and mutual funds. GOOG shares split into GOOG class C shares and GOOGL class A shares. The company is listed on the NASDAQ stock exchange under the ticker symbols GOOGL and GOOG, and on the Frankfurt Stock Exchange under the ticker symbol GGQ1. These ticker symbols now refer to Alphabet Inc., Google's holding company, since the fourth quarter of 2015.\n\nIn March 1999, the company moved its offices to Palo Alto, California, which is home to several prominent Silicon Valley technology start-ups. The next year, Google began selling advertisements associated with search keywords against Page and Brin's initial opposition toward an advertising-funded search engine. In order to maintain an uncluttered page design, advertisements were solely text-based.\n\nThis model of selling keyword advertising was first pioneered by Goto.com, an Idealab spin-off created by Bill Gross. When the company changed names to Overture Services, it sued Google over alleged infringements of the company's pay-per-click and bidding patents. Overture Services would later be bought by Yahoo! and renamed Yahoo! Search Marketing. The case was then settled out of court; Google agreed to issue shares of common stock to Yahoo! in exchange for a perpetual license.\n\nIn 2001, Google received a patent for its PageRank mechanism. The patent was officially assigned to Stanford University and lists Lawrence Page as the inventor. In 2003, after outgrowing two other locations, the company leased an office complex from Silicon Graphics, at 1600 Amphitheatre Parkway in Mountain View, California. The complex became known as the Googleplex, a play on the word googolplex, the number one followed by a googol zeroes. The Googleplex interiors were designed by Clive Wilkinson Architects. Three years later, Google bought the property from SGI for $319 million. By that time, the name \"Google\" had found its way into everyday language, causing the verb \"google\" to be added to the \"Merriam-Webster Collegiate Dictionary\" and the \"Oxford English Dictionary\", denoted as: \"to use the Google search engine to obtain information on the Internet\". The first use of \"Google\" as a verb in pop culture happened on the TV series \"Buffy the Vampire Slayer\", in 2002.\n\nIn 2005, \"The Washington Post\" reported on a 700 percent increase in third-quarter profit for Google, largely thanks to large companies shifting their advertising strategies from newspapers, magazines, and television to the Internet. In January 2008, all the data that passed through Google's MapReduce software component had an aggregated size of 20 petabytes per day. In 2009, a CNN report about top political searches of 2009 noted that \"more than a billion searches\" are being typed into Google on a daily basis. In May 2011, the number of monthly unique visitors to Google surpassed one billion for the first time, an 8.4 percent increase from May 2010 (931 million).\n\nThe year 2012 was the first time that Google generated $50 billion in annual revenue, which topped the $38 billion that was generated the previous year. In January 2013, then-CEO Larry Page commented, \"We ended 2012 with a strong quarter ... Revenues were up 36% year-on-year, and 8% quarter-on-quarter. And we hit $50 billion in revenues for the first time last year – not a bad achievement in just a decade and a half.\"\n\nGoogle announced the launch of a new company, called Calico, on September 19, 2013, to be led by Apple, Inc. chairman Arthur Levinson. In the official public statement, Page explained that the \"health and well-being\" company would focus on \"the challenge of ageing and associated diseases\".\n\nGoogle celebrated its 15-year anniversary on September 27, 2013, and in 2016 it celebrated its 18th birthday with an animated Doodle shown on web browsers around the world. although it has used other dates for its official birthday. The reason for the choice of September 27 remains unclear, and a dispute with rival search engine Yahoo! Search in 2005 has been suggested as the cause.\n\nThe Alliance for Affordable Internet (A4AI) was launched in October 2013; Google is part of the coalition of public and private organizations that also includes Facebook, Intel, and Microsoft. Led by Sir Tim Berners-Lee, the A4AI seeks to make Internet access more affordable so that access is broadened in the developing world, where only 31% of people are online. Google will help to decrease Internet access prices so they fall below the UN Broadband Commission's worldwide target of 5% of monthly income.\n\nThe corporation's consolidated revenue for the third quarter of 2013 was reported in mid-October 2013 as $14.89 billion, a 12 percent increase compared to the previous quarter. Google's Internet business was responsible for $10.8 billion of this total, with an increase in the number of users' clicks on advertisements.\n\nAccording to Interbrand's annual Best Global Brands report, Google has been the second most valuable brand in the world (behind Apple Inc.) in 2013, 2014, 2015, and 2016, with a valuation of $133 billion.\n\nIn September 2015, Google engineering manager Rachel Potvin revealed details about Google's software code at an engineering conference. She revealed that the entire Google codebase, which spans every single service it develops, consists of over 2 billion lines of code. All that code is stored on a code repository available to all 25,000 Google engineers, and the code is regularly copied and updated on 10 Google data centers. To keep control, Potvin said Google has built its own \"version control system\", called \"Piper\", and that \"when you start a new project, you have a wealth of libraries already available to you. Almost everything has already been done.\" Engineers can make a single code change and deploy it on all services at the same time. The only major exceptions are that the PageRank search results algorithm is stored separately with only specific employee access, and the code for the Android operating system and the Google Chrome browser are also stored separately, as they don't run on the Internet. The \"Piper\" system spans 85 TB of data. Google engineers make 25,000 changes to the code each day, and on a weekly basis change approximately 15 million lines of code across 250,000 files. With that much code, automated bots have to help. Potvin reported, \"You need to make a concerted effort to maintain code health. And this is not just humans maintaining code health, but robots too.” Bots aren't writing code, but generating a lot of the data and configuration files needed to run the company's software. \"Not only is the size of the repository increasing,\" Potvin explained, \"but the rate of change is also increasing. This is an exponential curve.\"\n\nAs of October 2016, Google operates 70 offices in more than 40 countries. Alexa, a company that monitors commercial web traffic, lists Google.com as the most visited website in the world. Several other Google services also figure in the top 100 most visited websites, including YouTube and Blogger.\n\nIn 2001, Google acquired Deja News, the operators of a large archive of materials from Usenet. Google rebranded the archive as Google Groups, and by the end of the year, it had expanded the history back to 1981. \n\nIn April 2003, Google acquired Applied Semantics, a company specializing in making software applications for the online advertising space. The AdSense contextual advertising technology developed by Applied Semantics was adopted into Google's advertising efforts.\n\nIn 2004, Google acquired Keyhole, Inc. Keyhole's eponymous product was later renamed Google Earth.\n\nIn 2005. Google acquired Urchin Software in April 2005, using their Urchin on Demand product (along with ideas from Adaptive Path's Measure Map) to create Google Analytics in 2006.\n\nIn October 2006, Google announced that it had acquired the video-sharing site YouTube for $1.65 billion in Google stock, and the deal was finalized on November 13, 2006.\n\nOn April 13, 2007, Google reached an agreement to acquire DoubleClick for $3.1 billion, transferring to Google valuable relationships that DoubleClick had with Web publishers and advertising agencies.\nThe deal was approved despite anti-trust concerns raised by competitors Microsoft and AT&T.\n\nIn addition to the many companies Google has purchased, the firm has partnered with other organizations for research, advertising, and other activities. In 2005, Google partnered with NASA Ames Research Center to build of offices.\n\nIn 2005 Google partnered with AOL to enhance each other's video search services. In 2006 Google and Fox Interactive Media of News Corporation entered into a $900 million agreement to provide search and advertising on the then-popular social networking site MySpace.\n\nIn 2007, Google began sponsoring NORAD Tracks Santa, displacing the former sponsor AOL. NORAD Tracks Santa purports to follow Santa Claus' progress on Christmas Eve, using Google Earth to \"track Santa\" in 3-D for the first time. \n\nIn 2008, Google developed a partnership with GeoEye to launch a satellite providing Google with high-resolution (0.41 m monochrome, 1.65 m color) imagery for Google Earth. The satellite was launched from Vandenberg Air Force Base on September 6, 2008. Google also announced in 2008 that it was hosting an archive of \"Life Magazine\"s photographs.\n\nIn 2010, Google Energy made its first investment in a renewable energy project, putting $38.8 million into two wind farms in North Dakota. The company announced the two locations will generate 169.5 megawatts of power, enough to supply 55,000 homes. The farms, which were developed by NextEra Energy Resources, will reduce fossil fuel use in the region and return profits. NextEra Energy Resources sold Google a twenty-percent stake in the project to get funding for its development. In February 2010, the Federal Energy Regulatory Commission FERC granted Google an authorization to buy and sell energy at market rates. The order specifically states that Google Energy—a subsidiary of Google—holds the rights \"for the sale of energy, capacity, and ancillary services at market-based rates\", but acknowledges that neither Google Energy nor its affiliates \"own or control any generation or transmission\" facilities. The corporation exercised this authorization in September 2013 when it announced it would purchase all the electricity produced by the not-yet-built 240-megawatt Happy Hereford wind farm.\n\nAlso in 2010, Google purchased Global IP Solutions, a Norway-based company that provides web-based teleconferencing and other related services. This acquisition enabled Google to add telephone-style services to its list of products. On May 27, 2010, Google announced it had also closed the acquisition of the mobile ad network AdMob. This occurred days after the Federal Trade Commission closed its investigation into the purchase. Google acquired the company for an undisclosed amount. In July 2010, Google signed an agreement with an Iowa wind farm to buy 114 megawatts of energy for 20 years.\n\nOn April 4, 2011, \"The Globe and Mail\" reported that Google bid $900 million for 6000 Nortel Networks patents.\n\nOn August 15, 2011, Google made its largest-ever acquisition to-date when it announced that it would acquire Motorola Mobility for $12.5 billion subject to approval from regulators in the United States and Europe. In a post on Google's blog, Google Chief Executive and co-founder Larry Page revealed that the acquisition was a strategic move to strengthen Google's patent portfolio. The company's Android operating system has come under fire in an industry-wide patent battle, as Apple and Microsoft have sued Android device makers such as HTC, Samsung, and Motorola. The merger was completed on May 22, 2012, after the approval of People's Republic of China.\n\nThis purchase was made in part to help Google gain Motorola's considerable patent portfolio on mobile phones and wireless technologies, to help protect Google in its ongoing patent disputes with other companies, mainly Apple and Microsoft, and to allow it to continue to freely offer Android. After the acquisition closed, Google began to restructure the Motorola business to fit Google's strategy. On August 13, 2012, Google announced plans to lay off 4000 Motorola Mobility employees. On December 10, 2012, Google sold the manufacturing operations of Motorola Mobility to Flextronics for $75 million. As a part of the agreement, Flextronics will manufacture undisclosed Android and other mobile devices. On December 19, 2012, Google sold the Motorola Home business division of Motorola Mobility to Arris Group for $2.35 billion in a cash-and-stock transaction. As a part of this deal, Google acquired a 15.7% stake in Arris Group valued at $300 million.\n\nIn June 2013, Google acquired Waze, a $966 million deal. While Waze would remain an independent entity, its social features, such as its crowdsourced location platform, were reportedly valuable integrations between Waze and Google Maps, Google's own mapping service.\n\nOn January 26, 2014, Google announced it had agreed to acquire DeepMind Technologies, a privately held artificial intelligence company from London. DeepMind describes itself as having the ability to combine the best techniques from machine learning and systems neuroscience to build general-purpose learning algorithms. DeepMind's first commercial applications were used in simulations, e-commerce and games. As of December 2013, it was reported that DeepMind had roughly 75 employees. Technology news website \"Recode\" reported that the company was purchased for $400 million though it was not disclosed where the information came from. A Google spokesman would not comment of the price. The purchase of DeepMind aids in Google's recent growth in the artificial intelligence and robotics community.\n\nOn January 29, 2014, Google announced that it would divest Motorola Mobility to Lenovo for $2.91 billion, a fraction of the original $12.5 billion price paid by Google to acquire the company. Google retained all but 2000 of Motorola's patents and entered into cross-licensing deals.\n\nOn September 21, 2017, HTC announced a \"cooperation agreement\" in which it would sell non-exclusive rights to certain intellectual property, as well as smartphone talent, to Google for $1.1 billion.\n\nAs of 2016, Google owned and operated nine data centers across North and South America, two in Asia, and four in Europe.\n\nIn 2011, the company had announced plans to build three data centers at a cost of more than $200 million in Asia (Singapore, Hong Kong and Taiwan) and said they would be operational within two years. In December 2013, Google announced that it had scrapped the plan to build a data center in Hong Kong.\n\nIn October 2013, \"The Washington Post\" reported that the U.S. National Security Agency intercepted communications between Google's data centers, as part of a program named MUSCULAR. This wiretapping was made possible because Google did not encrypt data passed inside its own network. Google began encrypting data sent between data centers in 2013.\n\nGoogle's most efficient data center runs at using only fresh air cooling, requiring no electrically powered air conditioning; the servers run so hot that humans cannot go near them for extended periods.\n\nAn August 2011 report estimated that Google had about 900,000 servers in their data centers, based on energy usage. The report does state that \"Google never says how many servers are running in its data centers.\"\n\nIn December 2016, Google announced that starting in 2017, it will power all of its data centers, as well as all of its offices, from 100% renewable energy. The commitment will make Google \"the world's largest corporate buyer of renewable power, with commitments reaching 2.6 gigawatts (2,600 megawatts) of wind and solar energy\". Google also stated that it does not count that as its final goal; it says that \"since the wind doesn't blow 24 hours a day, we'll also broaden our purchases to a variety of energy sources that can enable renewable power, every hour of every day\". Additionally, the project will \"help support communities\" around the world, as the purchase commitments will \"result in infrastructure investments of more than $3.5 billion globally\", and will \"generate tens of millions of dollars per year in revenue to local property owners, and tens of millions more to local and national governments in tax revenue\".\n\nOn August 10, 2015, Google announced plans to reorganize its various interests as a conglomerate called Alphabet. Google became Alphabet's leading subsidiary, and will continue to be the umbrella company for Alphabet's Internet interests. Upon completion of the restructure, Sundar Pichai became CEO of Google, replacing Larry Page, who became CEO of Alphabet.\n\nOn September 1, 2017, Google Inc. announced its plans of restructuring as a limited liability company, Google LLC, as a wholly owned subsidiary of XXVI Holdings Inc., which is formed as a subsidiary of Alphabet Inc. to hold the equity of its other subsidiaries, including Google LLC and other bets.\n\nFor the 2006 fiscal year, the company reported $10.492 billion in total advertising revenues and only $112 million in licensing and other revenues. In 2011, 96% of Google's revenue was derived from its advertising programs. In addition to its own algorithms for understanding search requests, Google uses technology from the company DoubleClick, to project user interest and target advertising to the search context and the user history.\n\nIn 2007, Google launched \"AdSense for Mobile\", taking advantage of the emerging mobile advertising market.\n\nGoogle Analytics allows website owners to track where and how people use their website, for example by examining click rates for all the links on a page. Google advertisements can be placed on third-party websites in a two-part program. Google's AdWords allows advertisers to display their advertisements in the Google content network, through a cost-per-click scheme. The sister service, Google AdSense, allows website owners to display these advertisements on their website and earn money every time ads are clicked.\n\nOne of the criticisms of this program is the possibility of click fraud, which occurs when a person or automated script clicks on advertisements without being interested in the product, causing the advertiser to pay money to Google unduly. Industry reports in 2006 claimed that approximately 14 to 20 percent of clicks were fraudulent or invalid.\n\nIn February 2003, Google stopped showing the advertisements of Oceana, a non-profit organization protesting a major cruise ship's sewage treatment practices. Google cited its editorial policy at the time, stating \"Google does not accept advertising if the ad or site advocates against other individuals, groups, or organizations.\" In June 2008, Google reached an advertising agreement with Yahoo!, which would have allowed Yahoo! to feature Google advertisements on its web pages. The alliance between the two companies was never completely realized because of antitrust concerns by the U.S. Department of Justice. As a result, Google pulled out of the deal in November 2008.\n\nAccording to market research published by comScore in November 2009, Google Search is the dominant search engine in the United States market, with a market share of 65.6%. Google indexes billions of web pages, so that users can search for the information they desire through the use of keywords and operators.\n\nIn 2003, \"The New York Times\" complained about Google's indexing, claiming that Google's caching of content on its site infringed its copyright for the content. In this case, the United States District Court of Nevada ruled in favor of Google in \"Field v. Google\" and \"Parker v. Google\". The publication \"\" has compiled a list of words that the web giant's new instant search feature will not search.\n\nGoogle Watch has criticized Google's PageRank algorithms, saying that they discriminate against new websites and favor established sites.\n\nGoogle also hosts Google Books. The company began scanning books and uploading limited previews, and full books were allowed, into its new book search engine. The Authors Guild, a group that represents 8,000 U.S. authors, filed a class action suit in a New York City federal court against Google in 2005 over this service. Google replied that it is in compliance with all existing and historical applications of copyright laws regarding books. Google eventually reached a revised settlement in 2009 to limit its scans to books from the U.S., the UK, Australia, and Canada. Furthermore, the Paris Civil Court ruled against Google in late 2009, asking it to remove the works of La Martinière (Éditions du Seuil) from its database. In competition with Amazon.com, Google sells digital versions of new books.\n\nOn July 21, 2010, in response to Bing, Google updated its image search to display a streaming sequence of thumbnails that enlarge when pointed at. Though web searches still appear in a batch per page format, on July 23, 2010, dictionary definitions for certain English words began appearing above the linked results for web searches.\n\nThe \"Hummingbird\" update to the Google search engine was announced in September 2013. The update was introduced over the month prior to the announcement and allows users ask the search engine a question in natural language rather than entering keywords into the search box.\n\nIn August 2016, Google announced two major changes related to its mobile search results. The first, removing the \"mobile-friendly\" label that highlighted pages were easy to read on mobile from its mobile search results page. The second, on January 10, 2017, the company will start punishing mobile pages that show intrusive interstitials when a user first opens a page and they will rank lower in its search results.\n\nIn May 2017, Google enabled a new \"Personal\" tab in Google Search, letting users search for content in their Google accounts' various services, including email messages from Gmail and photos from Google Photos.\n\nG Suite is a monthly subscription offering for organizations and businesses to get access to a collection of Google's services, including Gmail, Google Drive and Docs, Sheets, and Slides, with additional administrative tools, unique domain names, and 24/7 support.\nGoogle Search Appliance was launched in February 2002, targeted toward providing search technology for larger organizations. Google launched the Mini three years later, which was targeted at smaller organizations. Late in 2006, Google began to sell Custom Search Business Edition, providing customers with an advertising-free window into Google.com's index. The service was renamed Google Site Search in 2008. Site Search customers were notified by email in late March 2017 that no new licenses for Site Search would be sold after April 1, 2017, but that customer and technical support would be provided for the duration of existing license agreements.\n\nOn March 15, 2016, Google announced the introduction of Google Analytics 360 Suite, \"a set of integrated data and marketing analytics products, designed specifically for the needs of enterprise-class marketers.\" Among other things, the suite is designed to help \"enterprise class marketers\" \"see the complete customer journey\", generate \"useful insights\", and \"deliver engaging experiences to the right people\". Jack Marshall of \"The Wall Street Journal\" wrote that the suite competes with existing marketing cloud offerings by companies including Adobe, Oracle, Salesforce, and IBM.\n\nGoogle offers Gmail, and the newer variant Inbox, for email, Google Calendar for time-management and scheduling, Google Maps for mapping, navigation and satellite imagery, Google Drive for cloud storage of files, Google Docs, Sheets and Slides for productivity, Google Photos for photo storage and sharing, Google Keep for note-taking, Google Translate for language translation, YouTube for video viewing and sharing, and Google+, Allo, and Duo for social interaction.\n\nGoogle develops the Android mobile operating system, as well as its smartwatch, television, car, and Internet of things-enabled smart devices variations.\n\nIt also develops the Google Chrome web browser, and Chrome OS, an operating system based on Chrome.\n\nIn January 2010, Google released Nexus One, the first Android phone under its own, \"Nexus\", brand. It spawned a number of phones and tablets under the \"Nexus\" branding until its eventual discontinuation in 2016, replaced by a new brand called, Pixel.\n\nIn 2011, the Chromebook was introduced, described as a \"new kind of computer\" running Chrome OS.\n\nIn July 2013, Google introduced the Chromecast dongle, that allows users to stream content from their smartphones to televisions.\n\nIn June 2014, Google announced Google Cardboard, a simple cardboard viewer that lets user place their smartphone in a special front compartment to view virtual reality (VR) media.\n\nIn April 2016, \"Recode\" reported that Google had hired Rick Osterloh, Motorola Mobility's former President, to head Google's new hardware division. In October 2016, Osterloh stated that \"a lot of the innovation that we want to do now ends up requiring controlling the end-to-end user experience\", and Google announced several hardware platforms:\n\nIn February 2010, Google announced the Google Fiber project, with experimental plans to build an ultra-high-speed broadband network for 50,000 to 500,000 customers in one or more American cities. Following Google's corporate restructure to make Alphabet Inc. its parent company, Google Fiber was moved to Alphabet's Access division.\n\nIn April 2015, Google announced Project Fi, a mobile virtual network operator, that combines Wi-Fi and cellular networks from different telecommunication providers in an effort to enable seamless connectivity and fast Internet signal.\n\nIn September 2016, Google began its Google Station initiative, a project for public Wi-Fi at railway stations in India. Caesar Sengupta, VP for Google's next billion users, told \"The Verge\" that 15,000 people get online for the first time thanks to Google Station and that 3.5 million people use the service every month. The expansion meant that Google was looking for partners around the world to further develop the initiative, which promised \"high-quality, secure, easily accessible Wi-Fi\". By December, Google Station had been deployed at 100 railway stations, and in February, Google announced its intention to expand beyond railway stations, with a plan to bring citywide Wi-Fi to Pune.\n\nGoogle launched its Google News service in 2002, an automated service which summarizes news articles from various websites. In March 2005, Agence France Presse (AFP) sued Google for copyright infringement in federal court in the District of Columbia, a case which Google settled for an undisclosed amount in a pact that included a license of the full text of AFP articles for use on Google News.\n\nIn May 2011, Google announced Google Wallet, a mobile application for wireless payments.\n\nIn 2013, Google launched Google Shopping Express, a delivery service initially available only in San Francisco and Silicon Valley.\n\nGoogle Alerts is a content change detection and notification service, offered by the search engine company Google. The service sends emails to the user when it finds new results—such as web pages, newspaper articles, or blogs—that match the user's search term.\n\nIn July 2015 Google released DeepDream, an image recognition software capable of creating psychedelic images using a convolutional neural network.\n\nGoogle introduced its Family Link service in March 2017, letting parents buy Android Nougat-based Android devices for kids under 13 years of age and create a Google account through the app, with the parents controlling the apps installed, monitor the time spent using the device, and setting a \"Bedtime\" feature that remotely locks the device.\n\nIn April 2017, Google launched AutoDraw, a web-based tool using artificial intelligence and machine learning to recognize users' drawings and replace scribbles with related stock images that have been created by professional artists. The tool is built using the same technology as QuickDraw, an experimental game from Google's Creative Lab where users were tasked with drawing objects that algorithms would recognize within 20 seconds.\n\nIn May 2017, Google added \"Family Groups\" to several of its services. The feature, which lets users create a group consisting of their family members' individual Google accounts, lets users add their \"Family Group\" as a collaborator to shared albums in Google Photos, shared notes in Google Keep, and common events in Google Calendar. At announcement, the feature is limited to Australia, Brazil, Canada, France, Germany, Ireland, Italy, Japan, Mexico, New Zealand, Russia, Spain, United Kingdom and United States.\n\nGoogle APIs are a set of application programming interfaces (APIs) developed by Google which allow communication with Google Services and their integration to other services. Examples of these include Search, Gmail, Translate or Google Maps. Third-party apps can use these APIs to take advantage of or extend the functionality of the existing services.\n\nGoogle Developers is Google's site for software development tools, APIs, and technical resources. The site contains documentation on using Google developer tools and APIs—including discussion groups and blogs for developers using Google's developer products.\n\nGoogle Labs was a page created by Google to demonstrate and test new projects.\n\nGoogle owns the top-level domain 1e100.net which is used for some servers within Google's network. The name is a reference to the scientific E notation representation for 1 googol, .\n\nIn March 2017, Google launched a new website, opensource.google.com, to publish its internal documentation for Google Open Source projects.\n\nIn June 2017, Google launched \"We Wear Culture\", a searchable archive of 3,000 years of global fashion. The archive, a result of collaboration between Google and over 180 museums, schools, fashion institutes, and other organizations, also offers curated exhibits of specific fashion topics and their impact on society.\n\nOn \"Fortune\" magazine's list of the best companies to work for, Google ranked first in 2007, 2008 and 2012 and fourth in 2009 and 2010. Google was also nominated in 2010 to be the world's most attractive employer to graduating students in the Universum Communications talent attraction index. Google's corporate philosophy includes principles such as \"you can make money without doing evil,\" \"you can be serious without a suit,\" and \"work should be challenging and the challenge should be fun.\"\n\nAs of the second quarter in 2015, Google has 57,100 employees. Google has released that 30 percent of their employees are female, and 70 percent are male. A March 2013 report detailed that it had 10,000 developers based in more than 40 offices.\n\nGoogle's employees are hired based on a hierarchical system. Employees are split into six hierarchies based on experience and can range \"from entry-level data center workers at level one to managers and experienced engineers at level six.\" \n\nAfter the company's IPO in 2004, founders Sergey Brin and Larry Page and CEO Eric Schmidt requested that their base salary be cut to $1. Subsequent offers by the company to increase their salaries were turned down, primarily because their main compensation continues to come from owning stock in Google. Before 2004, Schmidt made $250,000 per year, and Page and Brin each received an annual salary of $150,000.\n\nIn March 2008, Sheryl Sandberg, then vice-president of global online sales and operations, began her position as chief operating officer of Facebook.\nIn 2009, early employee Tim Armstrong left to become CEO of AOL. In July 2012, Google's first female engineer, Marissa Mayer, left Google to become Yahoo!'s CEO.\n\nIn 2017 former Intel executive Diane Bryant became Chief Operating Officer of Google Cloud.\nAs a motivation technique, Google uses a policy often called Innovation Time Off, where Google engineers are encouraged to spend 20% of their work time on projects that interest them. Some of Google's services, such as Gmail, Google News, Orkut, and AdSense originated from these independent endeavors. In a talk at Stanford University, Marissa Mayer, Google's Vice President of Search Products and User Experience until July 2012, showed that half of all new product launches in the second half of 2005 had originated from the Innovation Time Off.\n\nGoogle's headquarters in Mountain View, California, is referred to as \"the Googleplex\", a play on words on the number googolplex and the headquarters itself being a \"complex\" of buildings. The lobby is decorated with a piano, lava lamps, old server clusters, and a projection of search queries on the wall. The hallways are full of exercise balls and bicycles. Many employees have access to the corporate recreation center. Recreational amenities are scattered throughout the campus and include a workout room with weights and rowing machines, locker rooms, washers and dryers, a massage room, assorted video games, table football, a baby grand piano, a billiard table, and ping pong. In addition to the recreation room, there are snack rooms stocked with various foods and drinks, with special emphasis placed on nutrition. Free food is available to employees 24/7, with the offerings provided by paid vending machines prorated based on and favoring those of better nutritional value.\n\nGoogle's extensive amenities are not available to all of its workers. Temporary workers such as book scanners do not have access to shuttles, Google cafes, or other perks.\n\nIn 2006, Google moved into about of office space in New York City, at 111 Eighth Avenue in Manhattan. The office was designed and built specially for Google, and houses its largest advertising sales team, which has been instrumental in securing large partnerships. The New York headquarters includes a game room, micro-kitchens, and a video game area. In 2010, Google bought the building housing the headquarter, in a deal that valued the property at around $1.9 billion, the biggest for a single building in the United States that year. In February 2012, Google moved additional employees to the New York City campus, with a total of around 2,750 employees.\n\nBy late 2006, Google established a new headquarters for its AdWords division in Ann Arbor, Michigan. In November 2006, Google opened offices on Carnegie Mellon's campus in Pittsburgh, focusing on shopping-related advertisement coding and smartphone applications and programs. Other office locations in the U.S. include Atlanta, Georgia; Austin, Texas; Boulder, Colorado; Cambridge, Massachusetts; San Francisco, California; Seattle, Washington; Reston, Virginia, and Washington, D.C.\n\nIn October 2006, the company announced plans to install thousands of solar panels to provide up to 1.6 megawatts of electricity, enough to satisfy approximately 30% of the campus' energy needs. The system will be the largest solar power system constructed on a U.S. corporate campus and one of the largest on any corporate site in the world. In addition, Google announced in 2009 that it was deploying herds of goats to keep grassland around the Googleplex short, helping to prevent the threat from seasonal bush fires while also reducing the carbon footprint of mowing the extensive grounds. The idea of trimming lawns using goats originated from Bob Widlar, an engineer who worked for National Semiconductor. In 2008, Google faced accusations in \"Harper's Magazine\" of being an \"energy glutton\". The company was accused of employing its \"Don't be evil\" motto and its public energy-saving campaigns to cover up or make up for the massive amounts of energy its servers require.\n\nInternationally, Google has over 70 offices in more than 40 countries. It also has product research and development operations in cities around the world, namely Sydney (birthplace location of Google Maps) and London (part of Android development).\n\nIn November 2013, Google announced plans for a new London headquarter, a notable 1 million square foot office able to accommodate 4,500 employees. Recognized as one of the biggest ever commercial property acquisitions at the time of the deal's announcement in January, Google submitted plans for the new headquarter to the Camden Council in June 2017. The new building, if approved, will feature a rooftop garden with a running track, giant moving blinds, a swimming pool, and a multi-use games area for sports.\n\nIn May 2015, Google announced its intention to create its own campus in Hyderabad, India. The new campus, reported to be the company's largest outside the United States, will accommodate 13,000 employees.\n\nSince 1998, Google has been designing special, temporary alternate logos to place on their homepage intended to celebrate holidays, events, achievements and people. The first Google Doodle was in honor of the Burning Man Festival of 1998. The doodle was designed by Larry Page and Sergey Brin to notify users of their absence in case the servers crashed. Subsequent Google Doodles were designed by an outside contractor, until Larry and Sergey asked then-intern Dennis Hwang to design a logo for Bastille Day in 2000. From that point onward, Doodles have been organized and created by a team of employees termed \"Doodlers\".\n\nGoogle has a tradition of creating April Fools' Day jokes. On April 1, 2000, Google MentalPlex allegedly featured the use of mental power to search the web. In 2007, Google announced a free Internet service called TiSP, or Toilet Internet Service Provider, where one obtained a connection by flushing one end of a fiber-optic cable down their toilet. Also in 2007, Google's Gmail page displayed an announcement for Gmail Paper, allowing users to have email messages printed and shipped to them.\nIn 2008, Google announced Gmail Custom time where users could change the time that the email was sent.\n\nIn 2010, Google changed its company name to Topeka in honor of Topeka, Kansas, whose mayor changed the city's name to Google for a short amount of time in an attempt to sway Google's decision in its new Google Fiber Project. In 2011, Google announced Gmail Motion, an interactive way of controlling Gmail and the computer with body movements via the user's webcam.\n\nGoogle's services contain easter eggs, such as the Swedish Chef's \"Bork bork bork,\" Pig Latin, \"Hacker\" or leetspeak, Elmer Fudd, Pirate, and Klingon as language selections for its search engine. The search engine calculator provides the Answer to the Ultimate Question of Life, the Universe, and Everything from Douglas Adams' \"The Hitchhiker's Guide to the Galaxy\". When searching the word \"recursion\", the spell-checker's result for the properly spelled word is exactly the same word, creating a recursive link.\n\nWhen searching for the word \"anagram,\" meaning a rearrangement of letters from one word to form other valid words, Google's suggestion feature displays \"Did you mean: nag a ram?\" In Google Maps, searching for directions between places separated by large bodies of water, such as Los Angeles and Tokyo, results in instructions to \"kayak across the Pacific Ocean.\" During FIFA World Cup 2010, search queries including \"World Cup\" and \"FIFA\" caused the \"Goooo...gle\" page indicator at the bottom of every result page to read \"Goooo...al!\" instead.\n\nIn 2004, Google formed the not-for-profit philanthropic Google.org, with a start-up fund of $1 billion. The mission of the organization is to create awareness about climate change, global public health, and global poverty. One of its first projects was to develop a viable plug-in hybrid electric vehicle that can attain 100 miles per gallon. Google hired Larry Brilliant as the program's executive director in 2004, and the current director is Megan Smith.\n\nIn 2008, Google announced its \"project 10\" which accepted ideas for how to help the community and then allowed Google users to vote on their favorites. After two years of silence, during which many wondered what had happened to the program, Google revealed the winners of the project, giving a total of ten million dollars to various ideas ranging from non-profit organizations that promote education to a website that intends to make all legal documents public and online.\n\nIn 2011, Google donated 1 million euros to International Mathematical Olympiad to support the next five annual International Mathematical Olympiads (2011–2015). In July 2012, Google launched a \"Legalize Love\" campaign in support of gay rights.\n\nGoogle uses various tax avoidance strategies. Out of the five largest American technology companies, it pays the lowest taxes to the countries of origin of its revenues. Google between 2007 and 2010 saved $3.1 billion in taxes by shuttling non-U.S. profits through Ireland and the Netherlands and then to Bermuda. Such techniques lower its non-U.S. tax rate to 2.3 per cent, while normally the corporate tax rate in for instance the UK is 28 per cent. This has reportedly sparked a French investigation into Google's transfer pricing practices.\n\nFollowing criticism of the amount of corporate taxes that Google paid in the United Kingdom, Chairman Eric Schmidt said, \"It's called capitalism. We are proudly capitalistic.\" During the same December 2012 interview, Schmidt confirmed that the company had no intention of paying more to the UK exchequer. \n\nGoogle Vice President Matt Brittin testified to the Public Accounts Committee of the UK House of Commons that his UK sales team made no sales and hence owed no sales taxes to the UK. In January 2016, Google reached a settlement with the UK to pay £130m in back taxes plus higher taxes in future.\n\nSince 2007, Google has aimed for carbon neutrality in regard to its operations.\n\nGoogle disclosed in September 2011 that it \"continuously uses enough electricity to power 200,000 homes\", almost 260 million watts or about a quarter of the output of a nuclear power plant. Total carbon emissions for 2010 were just under 1.5 million metric tons, mostly due to fossil fuels that provide electricity for the data centers. Google said that 25 percent of its energy was supplied by renewable fuels in 2010. An average search uses only 0.3 watt-hours of electricity, so all global searches are only 12.5 million watts or 5% of the total electricity consumption by Google.\n\nIn 2007, Google launched a project centered on developing renewable energy, titled the \"Renewable Energy Cheaper than Coal (RE<C)\" project. However, the project was cancelled in 2014, after engineers Ross Koningstein and David Fork understood, after years of study, that \"best-case scenario, which was based on our most optimistic forecasts for renewable energy, would still result in severe climate change\", writing that they \"came to the conclusion that even if Google and others had led the way toward a wholesale adoption of renewable energy, that switch would not have resulted in significant reductions of carbon dioxide emissions\".\n\nIn June 2013, \"The Washington Post\" reported that Google had donated $50,000 to the Competitive Enterprise Institute, a libertarian think tank that calls human carbon emissions a positive factor in the environment and argues that global warming is not a concern.\n\nIn July 2013, it was reported that Google had hosted a fundraising event for Oklahoma Senator Jim Inhofe, who has called climate change a \"hoax\". In 2014 Google cut ties with the American Legislative Exchange Council (ALEC) after pressure from the Sierra Club, major unions and Google's own scientists because of ALEC's stance on climate change and opposition to renewable energy.\n\nIn November 2017, Google bought 536 megawatts of wind power. The purchase made the firm reach 100% renewable energy. The wind energy comes from two power plants in South Dakota, one in Iowa and one in Oklahoma.\n\nIn 2013, Google ranked 5th in lobbying spending, up from 213th in 2003. In 2012, the company ranked 2nd in campaign donations of technology and Internet sections.\n\nGoogle has been involved in a number of lawsuits including the High-Tech Employee Antitrust Litigation which resulted in Google being one of four companies to pay a $415 million settlement to employees.\n\nOn June 27, 2017, the company received a record fine of from the European Union for \"promoting its own shopping comparison service at the top of search results.\" Commenting on the penalty, \"New Scientist\" magazine said: \"The hefty sum – the largest ever doled out by the EU's competition regulators – will sting in the short term, but Google can handle it. Alphabet, Google’s parent company, made a profit of $2.5 billion (€2.2 billion) in the first six weeks of 2017 alone. The real impact of the ruling is that Google must stop using its dominance as a search engine to give itself the edge in another market: online price comparisons.\" The company disputed the ruling.\n\nGoogle's market dominance has led to prominent media coverage, including criticism of the company over issues such as aggressive tax avoidance, search neutrality, copyright, censorship of search results and content, and privacy. Other criticisms include alleged misuse and manipulation of search results, its use of others' intellectual property, concerns that its compilation of data may violate people's privacy, and the energy consumption of its servers, as well as concerns over traditional business issues such as monopoly, restraint of trade, anti-competitive practices, and patent infringement.\n\nGoogle's mission statement, from the outset, was \"to organize the world's information and make it universally accessible and useful\", and its unofficial slogan was \"Don't be evil\". In October 2015, the motto was replaced in the Alphabet corporate code of conduct by the phrase: \"Do the right thing\". Google's commitment to such robust idealism has been increasingly called into doubt due to a number of the firm's actions and behaviours which appear to contradict this.\n\nFollowing media reports about PRISM, NSA's massive electronic surveillance program, in June 2013, several technology companies were identified as participants, including Google. According to leaks of said program, Google joined the PRISM program in 2009.\n\nOn August 8, 2017, Google fired employee James Damore after he distributed a memo throughout the company which argued that \"Google's ideological echo chamber\" and bias clouded their thinking about diversity and inclusion, and that it is also biological factors, not discrimination alone, that cause the average woman to be less interested than men in technical positions. Google CEO Sundar Pichai accused Damore in violating company policy by \"advancing harmful gender stereotypes in our workplace\", and he was fired on the same day. \"New York Times\" columnist David Brooks argued Pichai had mishandled the case, and called for his resignation.\n\nReportedly, Google's influenced New America think tank to expel their Open Markets research group, after the group has criticized Google monopolistic power and supported the EU $2.7B fine of Google.\n\nIn 2017, David Elliot and Chris Gillespie argued before the Ninth Circuit of the United States Court of Appeals that \"google\" had suffered genericide. The controversy began in 2012 when Gillespie acquired 763 domain names containing the word \"google.\" Google promptly filed a complaint with the NAF. Elliot then filed a petition for cancelling the Google trademark. Ultimately, the court ruled in favor of Google because Elliot failed to show a preponderance of evidence showing the genericide of \"google.\"\n\n\n", "id": "1092923", "title": "Google"}
{"url": "https://en.wikipedia.org/wiki?curid=51023476", "text": "Artificial empathy\n\nArtificial empathy (AE) is the development of AI systems − such as companion robots − that are able to detect and respond to human emotions. According to scientists, although the technology can be perceived as scary or threatening by many people, it could also have a significant advantage over humans in professions which are traditionally involved in emotional role-playing such as the health care sector. From the care-giver perspective for instance, performing emotional labor above and beyond the requirements of paid labor often results in chronic stress or burnout, and the development of a feeling of being desensitized to patients. However, it is argued that the emotional role-playing between the care-receiver and a robot can actually have a more positive outcome in terms of creating the conditions of less fear and concern for one's own predicament best exemplified by the phrase: \"if it is just a robot taking care of me it cannot be that critical.\" Scholars debate the possible outcome of such technology using two different perspectives. Either, the AE could help the socialization of care-givers, or serve as role model for emotional detachment.\n\nThere are a variety of philosophical, theoretical, and applicative questions related to AE. For example:\n\n", "id": "51023476", "title": "Artificial empathy"}
{"url": "https://en.wikipedia.org/wiki?curid=23886619", "text": "List of programming languages for artificial intelligence\n\nArtificial intelligence researchers have developed several specialized programming languages for artificial intelligence:\n\n\n\n\n", "id": "23886619", "title": "List of programming languages for artificial intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=562827", "text": "POP-11\n\nPOP-11 is a reflective, incrementally compiled programming language with many of the features of an interpreted language. It is the core language of the Poplog programming environment developed originally by the University of Sussex, and recently in the\nSchool of Computer Science at the\nUniversity of Birmingham which hosts\nthe Poplog website.\n\nPOP-11 is an evolution of the language POP-2, developed in Edinburgh University and features an open stack model (like Forth, among others). It is mainly procedural, but supports declarative language constructs, including a pattern matcher and is mostly used for research and teaching in Artificial Intelligence, although it has features sufficient for many other classes of problems. It is often used to introduce symbolic programming techniques to programmers of more conventional languages like Pascal, who find POP syntax more familiar than that of Lisp. One of POP-11's features is that it supports first-class functions.\n\nPop-11 is the core language of the Poplog system. The fact that the compiler and compiler subroutines are available at run-time (a requirement for incremental compilation) gives it the ability to support a far wider range of extensions than would be possible using only a macro facility. This made it possible for incremental compilers to be added for Prolog, Common Lisp and Standard ML, which could be added as required to support either mixed language development or development in the second language without using any Pop-11 constructs. This made it possible for Poplog to be used by teachers, researchers, or developers who were interested in only one of the languages. The most successful product developed in Pop-11 was the Clementine data-mining system, developed by ISL, as described in the entry on Poplog. After SPSS bought ISL they decided to port Clementine to C++ and Java, and eventually succeeded with great effort (and perhaps some loss of the flexibility provided by the use of an AI language!).\n\nAs explained in the entries for Poplog and POP-2, Pop-11 was for a time available only as part of an expensive commercial package (Poplog), but since about 1999 it has been freely available as part of the Open Source version of Poplog, including various additional packages and teaching libraries. An online version of ELIZA using Pop-11 is available at Birmingham.\n\nAt the University of Sussex David Young used Pop-11 in combination with C and Fortran to develop a suite of teaching and interactive development tools for image processing and vision, and has made them available in the Popvision extension to Poplog.\n\nHere is an example of a simple POP-11 program:\n\nThat prints out:\n\nThis one includes some list processing:\n\nExamples using the Pop-11 pattern matcher, which makes it relatively easy for students to learn to develop sophisticated list-processing programs without having to treat patterns as tree structures accessed by 'head' and 'tail' functions (CAR and CDR in Lisp), can be found in the online introductory tutorial. The matcher is at the heart of\nthe SimAgent (sim_agent) toolkit. Some of the powerful features of the toolkit, e.g. linking pattern variables to inline code variables, would have been very difficult to implement without the incremental compiler facilities.\n\n\n\n\n[[Category:Lisp programming language family]]\n[[Category:Artificial intelligence]]", "id": "562827", "title": "POP-11"}
{"url": "https://en.wikipedia.org/wiki?curid=2142", "text": "List of artificial intelligence projects\n\nThe following is a list of current and past, nonclassified notable artificial intelligence projects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "2142", "title": "List of artificial intelligence projects"}
{"url": "https://en.wikipedia.org/wiki?curid=21652", "text": "Natural-language processing\n\nNatural-language processing (NLP) is a field of computer science, artificial intelligence concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language data.\n\nChallenges in natural-language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.\n\nThe history of NLP generally started in the 1950s, although work can be found from earlier periods.\nIn 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n\nThe Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\n\nSome notably successful NLP systems developed in the 1960s were SHRDLU, a natural-language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".\n\nDuring the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.\n\nUp to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to NLP, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n\nMany of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\n\nRecent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.\n\nIn recent years, there has been a flurry of results showing deep learning techniques achieving state-of-the-art results in many natural-language tasks, for example in language modeling,\nparsing, and many others.\n\nSince the so-called \"statistical revolution\"\nin the late 1980s and mid 1990s, much Natural-Language Processing research has relied heavily on machine learning.\n\nFormerly, many language-processing tasks typically involved the direct hand coding of rules, which is not in general robust to natural-language variation. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large \"corpora\" of typical real-world examples (a \"corpus\" (plural, \"corpora\") is a set of documents, possibly with human or computer annotations).\n\nMany different classes of machine learning algorithms have been applied to NLP tasks. These algorithms take as input a large set of \"features\" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\n\nSystems based on machine-learning algorithms have many advantages over hand-produced rules:\n\nThe following is a list of some of the most commonly researched tasks in NLP. Note that some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\n\nThough NLP tasks are obviously very closely intertwined, they are frequently, for convenience, subdivided into categories. A coarse division is given below.\n\n\n\n\n\n", "id": "21652", "title": "Natural-language processing"}
{"url": "https://en.wikipedia.org/wiki?curid=23423534", "text": "Australian Artificial Intelligence Institute\n\nIn Australia, the Australian Artificial Intelligence Institute (Australian AI Institute, AAII, or AI) was a government-funded research and development laboratory for investigating and commercializing Artificial Intelligence, specifically Intelligent Software Agents.\n\nThe AAII was started in 1988 as an initiative by the Hawke government and closed in 1999. It was backed by support from the Computer Power Group, SRI International and the Victorian State Government. The director of the group was Michael Georgeff who came from SRI, contributing his experience with the PRS and vision in the domain of Intelligent agents. It was located in the Melbourne suburb of Carlton before moving to more spacious premises in the city centre of Melbourne, Victoria. At its peak it had more than 40 staff and took up two floors of an office building on the corner of Latrobe and Russell Streets.\n\nIn the late 1990s, the AAII spun out Agentis International (Agentis Business Solutions) to address the commercialization of the developed technology. Another company, Agent Oriented Software (AOS) was formed by a number of ex-AAII staff to pursue agent technology developing JACK Intelligent Agents. After the AAII shutdown, those staff that remained and the intellectual property were transferred to Agentis International.\n\nThis section summarizes a selection of the software and commercial projects that came out of the AAII: \n\n\nOver the course of its existence, the AAII released more than 75 of public technical notes . This section lists an available selection of these notes.\n\n\n\n", "id": "23423534", "title": "Australian Artificial Intelligence Institute"}
{"url": "https://en.wikipedia.org/wiki?curid=52918812", "text": "NTU RGB-D dataset\n\nThe NTU RGB-D (Nanyang Technological University's Red Blue Green and Depth information) dataset is a large dataset containing recordings of labeled human activities\n. This dataset consists of 56,880 action samples containing 4 different modalities (RGB videos, depth map sequences, 3D skeletal data, infrared videos) of data for each sample.\n\nThe dataset consists of 60 labelled actions. Specifically, drink water, eat meal/snack, brushing teeth, brushing hair, drop, pickup, throw, sitting down, standing up (from sitting position), clapping, reading, writing, tear up paper, wear jacket, take off jacket, wear a shoe, take off a shoe, wear on glasses, take off glasses, put on a hat/cap, take off a hat/cap, cheer up, hand waving, kicking something, put something inside pocket / take out something from pocket, hopping (one foot jumping), jump up, make a phone call/answer phone, playing with phone/tablet, typing on a keyboard, pointing to something with finger, taking a selfie, check time (from watch), rub two hands together, nod head/bow, shake head, wipe face, salute, put the palms together, cross hands in front (say stop), sneeze/cough, staggering, falling, touch head (headache), touch chest (stomachache/heart pain), touch back (backache), touch neck (neckache), nausea or vomiting condition, use a fan (with hand or paper)/feeling warm, punching/slapping other person, kicking other person, pushing other person, pat on back of other person, point finger at the other person, hugging other person, giving something to other person, touch other person's pocket, handshaking, walking towards each other and walking apart from each other.\n\nThis is a table of some of the machine learning methods used on the database and their error rates, by type of classifier:\n", "id": "52918812", "title": "NTU RGB-D dataset"}
{"url": "https://en.wikipedia.org/wiki?curid=52968552", "text": "WordDive\n\nWordDive is an AI-based online language learning software and mobile application. Ten languages are currently offered: English, Estonian, Finnish, French, German, Italian, Japanese, Russian, Spanish and Swedish.\n\nThe method is based on the use of multiple senses, individual optimization and game-like elements.With the individual optimization feature, the course is modified based on the user's learning ability. When the user learns quickly, the amount of actively practiced content increases and the repetition decreases. The aim of the exercises is to help the user to start speaking the new language quickly and effortlessly. Each course package includes several individual courses that focus on specific topics or grammatical aspects.\n\nWordDive has been awarded as the Best e-Learning Solution in Finland 2011 as well as the Best Mobile Service in Finland 2014. In spring 2015, WordDive was selected as a Red Herring Europe Top 100 winner.\n\nThe company is based in Tampere, Finland. The service was launched in 2010. As of December 2016, the platform was said to have 250,000 users in 150 countries.\n\nIn Finland and Germany WordDive is best known for its English prep course, which became a market leader in Finland two years after its launch in 2014. Now 25 percent of Finnish high school seniors use the WordDive prep course, and 98 percent of those who complete the course recommend WordDive. There is a grade guarantee: if the course participant completes the prep course but doesn't get at least the second best grade in English, they will be fully refunded.\n\nIn Germany, the company is working together with brand ambassador Samu Haber, the front man of the band Sunrise Avenue and a judge in The Voice of Germany.\n\nIn January 2017 WordDive secured 1.2 million euros via the Invesdor crowdfunding platform.\n", "id": "52968552", "title": "WordDive"}
{"url": "https://en.wikipedia.org/wiki?curid=41732818", "text": "Qloo\n\nQloo (pronounced \"clue\") is a company that uses artificial intelligence (AI). An application programming interface (API) provides cultural correlations. It was founded by Alex Elias and received funding from Leonardo DiCaprio, Barry Sternlicht and Pierre Lagrange.\n\nQloo establishes consumer preference correlations via machine learning across multiple proprietary, customer and open-source data across cultural domains including music, film, television, dining, nightlife, fashion, books and travel. The recommender system uses AI to predict correlations for further applications.\n\nQloo was founded in 2012 by chief executive officer Alex Elias and chief operating officer Jay Alger. Elias was formerly a hedge fund manager with APE Capital.\nHe graduated from the University of Southern California, and then developed his idea at law school at New York University.\nAlger was formerly the CEO of the digital agency Deepend.\n\nQloo was tested on a private website in April 2012. \nIn 2012, Qloo raised $1.4 million in seed funding from investors including Cedric the Entertainer, Danny Masterson, and venture capital firm Kindler Capital.\nQloo had a public beta release in November 2012 after its initial funding. \n\nIn 2013, the company raised an additional $1.6 million from Cross Creek Pictures founding partner Tommy Thompson, and Samih Toukan and Hussam Khoury, founders of Maktoob, an Internet services company purchased by Yahoo! for $164 million in 2009.\nOn November 14, 2013, a website and an iPhone app were announced. The company later released an Android app, and tablet versions, in mid-2014.\n\nIn 2016, Qloo secured $4.5 million in venture capital investment. The $4.5 million was split between a number of investors, including Barry Sternlicht, Pierre Lagrange and Leonardo DiCaprio. In July 2017, Qloo raised $6.5 million in funding rounds from AXA Strategic Ventures and Elton John.\n\nFollowing the investment, the founders stated in an interview with Tech Crunch that they would use the investment to expand Qloo's database. They hoped the move would secure larger contracts with corporate clients. At the time, clients already included Fortune 500 companies such as Twitter, PepsiCo and BMW.\n\nQloo calls itself a cultural AI platform to provide real-time correlation data across domains of culture and entertainment including: film, music, television, dining, nightlife, fashion, books and travel. Each category contains subcategories. \n\nQloo’s knowledge of a user's taste in one category can be utilized to offer suggestions in other categories. Users then rate the suggestions, providing it with feedback for future suggestions.\nQloo has partnerships with companies such as Expedia and iTunes.\n", "id": "41732818", "title": "Qloo"}
{"url": "https://en.wikipedia.org/wiki?curid=28803763", "text": "INDECT\n\nINDECT is a research project in the area of intelligent security systems performed by several European universities since 2009 and funded by the European Union. The purpose of the project is to involve European scientists and researchers in the development of solutions to and tools for automatic threat detection through e.g. processing of CCTV camera data streams, standardization of video sequence quality for user applications, threat detection in computer networks as well as data and privacy protection.\n\nThe area of research, applied methods and techniques are described in the public deliverables which are available to the public on the project's website. Practically, all information related to the research is public. Only documents that comprise information related to financial data or information that could negatively influence the competitiveness and law enforcement capabilities of parties involved in the project are not published. This follows regulations and practices applied in EU research projects.\n\nThe main end-user of INDECT solutions are police forces and security services.\n\nThe principle of operation of the project is detecting threats and identifying source of threats, without monitoring and searching for particular citizens or groups of citizens. Then, the system operator (i.e. police officer) decides whether an intervention of services responsible for public security are required or not. Further investigation eventually leading to persons related to threats are performed, preserving the presumption of innocence, on the basis of existing procedures already used by police services and prosecutors. As it can be found in the project deliverables, INDECT does not involve storage of personal data (such as names, addresses, identity document numbers, etc.).\n\nA similar, behaviour based surveillance program was SAMURAI \"(Suspicious and Abnormal behaviour Monitoring Using a netwoRk of cAmeras & sensors for sItuation awareness enhancement)\".\n\nThe main expected results of the INDECT project are:\n\nSome media and other sources accuse INDECT of privacy abuse, collecting personal data, and keeping information from the public. Consequently, these issues have been commented and discussed by some Members of the European Parliament.\n\nAs can be seen in the project's documentation, INDECT does not involve mobile phone tracking or call interception.\n\nThe rumours about testing INDECT during 2012 UEFA European Football Championship also turned out to be false.\n\nThe mid-term review of the Seventh Framework Programme to the European Parliament strongly urges the European Commission to immediately make all documents available and to define a clear and strict mandate for the research goal, the application, and the end users of INDECT, and stresses a thorough investigation of the possible impact on fundamental rights. Nevertheless, according to Mr. Paweł Kowal, MEP, the project had the ethical review on 15 March 2011 in Brussels with the participation of ethics experts from Austria, France, Netherlands, Germany and Great Britain.\n\n\n", "id": "28803763", "title": "INDECT"}
{"url": "https://en.wikipedia.org/wiki?curid=46803936", "text": "Susan Schneider (philosopher)\n\nSusan Schneider is an American philosopher. She is a professor of philosophy and cognitive science at The University of Connecticut, a fellow at the Institute for Ethics and Emerging Technologies, and a faculty member in the Ethics and Technology Group at the Yale Interdisciplinary Center for Bioethics, Yale University.\n\nSchneider's main focus is the nature of the self and mind. Within the fields of metaphysics and philosophy of mind, much of her work explores the nature of thought, especially in light of discoveries in cognitive science and work in contemporary metaphysics. She has argued that the brain is computational, and has developed a new version of the language of thought (\"LOT\") position. But while supporting computationalism about the brain, she is a critic of physicalism about the nature of the mind, arguing physicalism is ill-conceived.\n\nShe is also actively engaged in debates over artificial intelligence (including superintelligence) and brain enhancement (see mind uploading) and uses ideas from contemporary metaphysics, ethics and cognitive science, interlaced with science fiction thought experiments, to illustrate flaws in positions. In an article for a NASA publication she argues that the most intelligent beings in the universe are likely to be superintelligent robots.\n\nDiscussions of her work have appeared in \"The New York Times, Wired Magazine, Humanity+, Big Think, \"3 Quarks Daily\", \"Discover Magazine\", \"Science Magazine\", Motherboard, Slate (France), Popular Mechanics\", and more.\n\nHer work was recently the subject of a documentary TV episode.\n\nThe Language of Thought\n\nSchneider has framed a new version of the language of thought (“LOT”) approach. According to the LOT approach, humans and even non-human animals think in a “language of thought” - an inner mental language that is not equivalent to any natural language. This mental language is computational, for thinking is regarded as the algorithmic manipulation of mental symbols, where the algorithm is to be identified through research in cognitive science. The “Classical Computational Theory of Mind” holds that part or all of the brain is computational in this algorithmic sense.\n\nIn her book on LOT, Schneider contrasts the LOT approach to its chief foe, the connectionist or neural network approach, urging that both approaches are insightful. She holds that the brain is probably a hybrid system — being both a symbol processing engine, and having neural networks. In particular, deliberative, conscious thought is symbolic, but it is implemented by neural networks.\n\nLOT’s chief philosophical architect, Jerry Fodor, has argued the cognitive mind is likely non-computational.<ref name=\"A Bradford Book / MIT Press\"></ref> Schneider argues against Fodor’s pessimism, illustrating that the development of sophisticated computational theories of cognition, as well as artificial general intelligence (\"AGI\"), are on the horizon.\n\nSchneider also defends a view of the nature of the mental symbols (where such are the basic vocabulary items in the language of thought). She then used this conception of symbols, together with certain work on the nature of meaning, to construct a theory of the nature of concepts. The basic theory of concepts is intended to be ecumenical, having a version that applying in the case of connectionism, as well as versions that apply to both the prototype theory and definitions view of concepts.\n\nThe Metaphysics of Mind\n\nPhysicalism holds that everything is made up of entities that physics says are fundamental, such as particles, fields or strings. Schneider urges that the success of computationalism does not require physicalism to be correct.\n\nSchneider claims some of the most popular versions of physicalism are at odds with commonly accepted positions about substance and properties in the related field of metaphysics. Further, the mathematical nature of fundamental physical theories undermines physicalism itself. Fundamental physical entities are defined mathematically, and the physicalist must consider what makes mathematical statements true. This is an issue dealt with in the field of philosophy of mathematics. The most viable theories in that domain, when combined with a physicalist approach, yield unworkable versions of physicalism. At best, physicalism becomes a form of dualism – a dualism of the abstract and concrete. And it fares as poorly as substance and property dualism with respect to explaining mental causation. Physicalism, thus understood, loses its customary advantages over competing theories.\n\nAstrobiology and Artificial Intelligence (A.I.)\n\nSchneider is among those researchers who believe that the most intelligent alien beings we encounter will be \"postbiological in nature\", being forms of artificial intelligence. (See also Paul Davies, Steven J. Dick, Martin Rees and Seth Shostak.) She is the first to contend that they would be superintelligent, and that we can predict the shape of some of these superintelligences would be like.\n\nTheir reason for the claim that the most intelligent aliens will be \"postbiological\" is called the \"short window observation.” This holds that by the time any society learns to transmit radio signals, they're likely just a few hundred years from upgrading their own biology. As Elon Musk puts it biological beings would be just a \"biological boot loader for digital superintelligence.\"\n\nSchneider poses two questions: first, how can we understand the thinking of superintelligences? And, second, if this is the direction intelligence is going, will these superintelligent beings even be conscious? She poses a \"hard problem of AI consciousness\" that is similar in some respects to David Chalmers hard problem of consciousness, which concerns the human case. The problem is: how can we know that silicon is the right sort of medium for conscious experience?\n\nDrawing from work in cognitive science, Schneider identifies ways that humans might understand the advanced thought patterns of certain kinds of superintelligences – namely, those that are modeled after the biological beings that created them. And she urges that under the right circumstances the A.I. may be conscious.\n\nUploading, Cognitive Enhancement and the Singularity\n\nTranshumanism is a philosophical, cultural, and political movement that holds that the human species is only now in a comparatively early phase and that its very evolution will be altered by developing technologies.\n\nSchneider observes that when one considers whether to enhance in the radical ways the transhumanists advocate (e.g., uploading, adding silicon brain parts for new cognitive capacities, or to enhance existing ones), one must ask, \"Will this radically enhanced creature still be me?\" If not, then, on the assumption that one key factor in a decision to enhance oneself is one's own personal development, we should regard the enhancement in question as undesirable. For when you choose to enhance in these radical ways, the enhancement does not really enhance \"you.\" In this vein, Schneider contends that one cannot really \"upload\" their brain, transferring their consciousness to a computer. At best, uploading will merely create a computational copy of youl.\n\nEnhancement decisions will require deep deliberation about metaphysical and ethical questions that are controversial and difficult to solve: questions that will require reflection about both personal identity and the nature of mind. At best, a pluralistic society should recognize the diversity of different views on these matters, and not assume that science itself can answer questions about whether radical forms of brain enhancement are a form of survival.\n\n\n", "id": "46803936", "title": "Susan Schneider (philosopher)"}
{"url": "https://en.wikipedia.org/wiki?curid=53210461", "text": "Moral Machine\n\nMoral Machine is an online platform, developed by Iyad Rahwan's Scalable Cooperation group at the Massachusetts Institute of Technology, that generates moral dilemmas and collects information on the decisions that people make between two destructive outcomes. The presented scenarios are often variations of the trolley problem, and the information collected would be used for further research regarding the decisions that machine intelligence must make in the future. For example, as artificial intelligence plays an increasingly significant role in autonomous driving technology, research projects like Moral Machine help to find solutions for challenging life-and-death decisions that will face self-driving vehicles.\n", "id": "53210461", "title": "Moral Machine"}
{"url": "https://en.wikipedia.org/wiki?curid=53322329", "text": "Human Problem Solving\n\nHuman Problem Solving (1972) is a book by Allen Newell and Herbert A. Simon.\n", "id": "53322329", "title": "Human Problem Solving"}
{"url": "https://en.wikipedia.org/wiki?curid=53587467", "text": "Outline of machine learning\n\nThe following outline is provided as an overview of and topical guide to machine learning:\n\nMachine learning – subfield of computer science (more particularly soft computing) that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a \"Field of study that gives computers the ability to learn without being explicitly programmed\". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from an example \"training set\" of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.\n\n\nSubfields of machine learning\n\nCross-disciplinary fields involving machine learning\n\nApplications of machine learning\n\nMachine learning hardware\n\nMachine learning tools   (list)\n\nMachine learning framework\n\nProprietary machine learning frameworks\n\nOpen source machine learning frameworks\n\nMachine learning library   (list)\n\nMachine learning algorithm\n\n\nMachine learning method   (list)\n\nDimensionality reduction\n\nEnsemble learning\n\nMeta learning\n\nReinforcement learning\n\nSupervised learning\n\nBayesian statistics\n\nDecision tree algorithm\n\nLinear classifier\n\nUnsupervised learning\n\nArtificial neural network\n\nAssociation rule learning\n\nHierarchical clustering\n\nCluster analysis\n\nAnomaly detection\n\nSemi-supervised learning\n\nDeep learning\n\n\nMachine learning research\n\nHistory of machine learning\n\nMachine learning projects\n\nMachine learning organizations\n\n\nBooks about machine learning\n\n\n\n\n\n\n\n\n", "id": "53587467", "title": "Outline of machine learning"}
{"url": "https://en.wikipedia.org/wiki?curid=10136", "text": "Expert system\n\nIn artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert.\nExpert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software.\n\nAn expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.\n\nExpert systems were introduced by the Stanford Heuristic Programming Project led by Edward Feigenbaum, who is sometimes termed the \"father of expert systems\"; other key early contributors were Bruce Buchanan and Randall Davis. The Stanford researchers tried to identify domains where expertise was highly valued and complex, such as diagnosing infectious diseases (Mycin) and identifying unknown organic molecules (Dendral). The idea that \"intelligent systems derive their power from the knowledge they possess rather than from the specific formalisms and inference schemes they use\" – as Feigenbaum said – was at the time a significant step forward, since the past research had been focused on heuristic computational methods, culminating in attempts to develop very general-purpose problem solvers (foremostly the conjunct work of Allen Newell and Herbert Simon). Expert systems became some of the first truly successful forms of artificial intelligence (AI) software.\n\nResearch on expert systems was also active in France. While in the US the focus tended to be on rule-based systems, first on systems hard coded on top of LISP programming environments and then on expert system shells developed by vendors such as Intellicorp, in France research focused more on systems developed in Prolog. The advantage of expert system shells was that they were somewhat easier for nonprogrammers to use. The advantage of Prolog environments was that they weren't focused only on \"if-then\" rules; Prolog environments provided a much fuller realization of a complete First Order Logic environment.\n\nIn the 1980s, expert systems proliferated. Universities offered expert system courses and two thirds of the Fortune 500 companies applied the technology in daily business activities. Interest was international with the Fifth Generation Computer Systems project in Japan and increased research funding in Europe.\n\nIn 1981, the first IBM PC, with the PC DOS operating system, was introduced. The imbalance between the high affordability of the relatively powerful chips in the PC, compared to the much more expensive cost of processing power in the mainframes that dominated the corporate IT world at the time, created a new type of architecture for corporate computing, termed the client-server model. Calculations and reasoning could be performed at a fraction of the price of a mainframe using a PC. This model also enabled business units to bypass corporate IT departments and directly build their own applications. As a result, client server had a tremendous impact on the expert systems market. Expert systems were already outliers in much of the business world, requiring new skills that many IT departments did not have and were not eager to develop. They were a natural fit for new PC-based shells that promised to put application development into the hands of end users and experts. Until then, the main development environment for expert systems had been high end Lisp machines from Xerox, Symbolics, and Texas Instruments. With the rise of the PC and client server computing, vendors such as Intellicorp and Inference Corporation shifted their priorities to developing PC based tools. Also, new vendors, often financed by venture capital (such as Aion Corporation, Neuron Data, Exsys, and many others), started appearing regularly.\n\nThe first expert system to be used in a design capacity for a large-scale product was the SID (Synthesis of Integral Design) software program, developed in 1982. Written in LISP, SID generated 93% of the VAX 9000 CPU logic gates. Input to the software was a set of rules created by several expert logic designers. SID expanded the rules and generated software logic synthesis routines many times the size of the rules themselves. Surprisingly, the combination of these rules resulted in an overall design that exceeded the capabilities of the experts themselves, and in many cases out-performed the human counterparts. While some rules contradicted others, top-level control parameters for speed and area provided the tie-breaker. The program was highly controversial, but used nevertheless due to project budget constraints. It was terminated by logic designers after the VAX 9000 project completion.\n\nIn the 1990s and beyond, the term \"expert system\" and the idea of a standalone AI system mostly dropped from the IT lexicon. There are two interpretations of this. One is that \"expert systems failed\": the IT world moved on because expert systems didn't deliver on their over hyped promise. The other is the mirror opposite, that expert systems were simply victims of their success: as IT professionals grasped concepts such as rule engines, such tools migrated from being standalone tools for developing special purpose \"expert\" systems, to being one of many standard tools. Many of the leading major business application suite vendors (such as SAP, Siebel, and Oracle) integrated expert system abilities into their suite of products as a way of specifying business logic – rule engines are no longer simply for defining the rules an expert would use but for any type of complex, volatile, and critical business logic; they often go hand in hand with business process automation and integration environments.\n\nAn expert system is an example of a knowledge-based system. Expert systems were the first commercial systems to use a knowledge-based architecture. A knowledge-based system is essentially composed of two sub-systems: the knowledge base and the inference engine.\n\nThe knowledge base represents facts about the world. In early expert systems such as Mycin and Dendral, these facts were represented mainly as flat assertions about variables. In later expert systems developed with commercial shells, the knowledge base took on more structure and used concepts from object-oriented programming. The world was represented as classes, subclasses, and instances and assertions were replaced by values of object instances. The rules worked by querying and asserting values of the objects.\n\nThe inference engine is an automated reasoning system that evaluates the current state of the knowledge-base, applies relevant rules, and then asserts new knowledge into the knowledge base. The inference engine may also include abilities for explanation, so that it can explain to a user the chain of reasoning used to arrive at a particular conclusion by tracing back over the firing of rules that resulted in the assertion.\n\nThere are mainly two modes for an inference engine: forward chaining and backward chaining. The different approaches are dictated by whether the inference engine is being driven by the antecedent (left hand side) or the consequent (right hand side) of the rule. In forward chaining an antecedent fires and asserts the consequent. For example, consider the following rule:\n\nformula_1\n\nA simple example of forward chaining would be to assert Man(Socrates) to the system and then trigger the inference engine. It would match R1 and assert Mortal(Socrates) into the knowledge base.\n\nBackward chaining is a bit less straight forward. In backward chaining the system looks at possible conclusions and works backward to see if they might be true. So if the system was trying to determine if Mortal(Socrates) is true it would find R1 and query the knowledge base to see if Man(Socrates) is true. One of the early innovations of expert systems shells was to integrate inference engines with a user interface. This could be especially powerful with backward chaining. If the system needs to know a particular fact but doesn't it can simply generate an input screen and ask the user if the information is known. So in this example, it could use R1 to ask the user if Socrates was a Man and then use that new information accordingly.\n\nThe use of rules to explicitly represent knowledge also enabled explanation abilities. In the simple example above if the system had used R1 to assert that Socrates was Mortal and a user wished to understand why Socrates was mortal they could query the system and the system would look back at the rules which fired to cause the assertion and present those rules to the user as an explanation. In English if the user asked \"Why is Socrates Mortal?\" the system would reply \"Because all men are mortal and Socrates is a man\". A significant area for research was the generation of explanations from the knowledge base in natural English rather than simply by showing the more formal but less intuitive rules.\n\nAs expert systems evolved, many new techniques were incorporated into various types of inference engines. Some of the most important of these were:\n\n\nThe goal of knowledge-based systems is to make the critical information required for the system to work explicit rather than implicit. In a traditional computer program the logic is embedded in code that can typically only be reviewed by an IT specialist. With an expert system the goal was to specify the rules in a format that was intuitive and easily understood, reviewed, and even edited by domain experts rather than IT experts. The benefits of this explicit knowledge representation were rapid development and ease of maintenance.\n\nEase of maintenance is the most obvious benefit. This was achieved in two ways. First, by removing the need to write conventional code, many of the normal problems that can be caused by even small changes to a system could be avoided with expert systems. Essentially, the logical flow of the program (at least at the highest level) was simply a given for the system, simply invoke the inference engine. This also was a reason for the second benefit: rapid prototyping. With an expert system shell it was possible to enter a few rules and have a prototype developed in days rather than the months or year typically associated with complex IT projects.\n\nA claim for expert system shells that was often made was that they removed the need for trained programmers and that experts could develop systems themselves. In reality, this was seldom if ever true. While the rules for an expert system were more comprehensible than typical computer code, they still had a formal syntax where a misplaced comma or other character could cause havoc as with any other computer language. Also, as expert systems moved from prototypes in the lab to deployment in the business world, issues of integration and maintenance became far more critical. Inevitably demands to integrate with, and take advantage of, large legacy databases and systems arose. To accomplish this, integration required the same skills as any other type of system.\n\nThe most common disadvantage cited for expert systems in the academic literature is the knowledge acquisition problem. Obtaining the time of domain experts for any software application is always difficult, but for expert systems it was especially difficult because the experts were by definition highly valued and in constant demand by the organization. As a result of this problem, a great deal of research in the later years of expert systems was focused on tools for knowledge acquisition, to help automate the process of designing, debugging, and maintaining rules defined by experts. However, when looking at the life-cycle of expert systems in actual use, other problems – essentially the same problems as those of any other large system – seem at least as critical as knowledge acquisition: integration, access to large databases, and performance.\n\nPerformance was especially problematic because early expert systems were built using tools such as Lisp, which executed interpreted (rather than compiled) code. Interpreting provided an extremely powerful development environment but with the drawback that it was virtually impossible to match the efficiency of the fastest compiled languages, such as C. System and database integration were difficult for early expert systems because the tools were mostly in languages and platforms that were neither familiar to nor welcome in most corporate IT environments – programming languages such as Lisp and Prolog, and hardware platforms such as Lisp machines and personal computers. As a result, much effort in the later stages of expert system tool development was focused on integrating with legacy environments such as COBOL and large database systems, and on porting to more standard platforms. These issues were resolved mainly by the client-server paradigm shift, as PCs were gradually accepted in the IT environment as a legitimate platform for serious business system development and as affordable minicomputer servers provided the processing power needed for AI applications.\n\nHayes-Roth divides expert systems applications into 10 categories illustrated in the following table. The example applications were not in the original Hayes-Roth table, and some of them arose well afterward. Any application that is not footnoted is described in the Hayes-Roth book. Also, while these categories provide an intuitive framework to describe the space of expert systems applications, they are not rigid categories, and in some cases an application may show traits of more than one category.\nHearsay was an early attempt at solving voice recognition through an expert systems approach. For the most part this category or expert systems was not all that successful. Hearsay and all interpretation systems are essentially pattern recognition systems—looking for patterns in noisy data. In the case of Hearsay recognizing phonemes in an audio stream. Other early examples were analyzing sonar data to detect Russian submarines. These kinds of systems proved much more amenable to a neural network AI solution than a rule-based approach.\n\nCADUCEUS and MYCIN were medical diagnosis systems. The user describes their symptoms to the computer as they would to a doctor and the computer returns a medical diagnosis.\n\nDendral was a tool to study hypothesis formation in the identification of organic molecules. The general problem it solved—designing a solution given a set of constraints—was one of the most successful areas for early expert systems applied to business domains such as salespeople configuring Digital Equipment Corporation (DEC) VAX computers and mortgage loan application development.\n\nSMH.PAL is an expert system for the assessment of students with multiple disabilities.\n\nMistral is an expert system to monitor dam safety, developed in the 90's by Ismes (Italy). It gets data from an automatic monitoring system and performs a diagnosis of the state of the dam. Its first copy, installed in 1992 on the Ridracoli Dam (Italy), is still operational 24/7/365. It has been installed on several dams in Italy and abroad (e.g., Itaipu Dam in Brazil), and on landslide sites under the name of Eydenet, and on monuments under the name of Kaleidos. Mistral is a registered trade mark of CESI.\n\nBayesian networks (BNs) are probabilistic graphical models, which are typically used to model cause and effect relationships, have become the most widely accepted technique for incorporating expert knowledge along with data. Expert knowledge can be incorporated into BNs by either constructing the causal (or dependence) graph, or by incorporating factors into the causal network which are important for inference but which data fail to capture.\nThe popularity of BNs as expert systems has led to the development of countless prediction and decision support systems in industry, government and academia worldwide. These systems typically incorporate both knowledge and data, and have been applied in the areas of, but not limited to, finance, engineering, sports, sports psychology, law, project management, marketing, medicine, energy, forensics, economics, property market, and defence.\n\n\n", "id": "10136", "title": "Expert system"}
{"url": "https://en.wikipedia.org/wiki?curid=50231947", "text": "Mycroft (software)\n\nMycroft is a free and open-source intelligent personal assistant and knowledge navigator for Linux-based operating systems that uses a natural language user interface. It is named after a fictional computer from 1966 science fiction novel The Moon Is a Harsh Mistress.\n\nInspiration for Mycroft came when Ryan Sipes and Joshua Montgomery were visiting the Kansas City makerspace, where they came across a simple and basic intelligent virtual assistant project. Mycroft is said to be the world’s first open-source AI voice assistant. They were interested in the technology, but did not like its inflexibility. Mycroft was part of the Sprint Accelerator 2016 class in Kansas City and joined 500 Startups Batch 20 in February 2017. The company accepted a strategic investment from Jaguar Land Rover during this same time period.\n\nMycroft uses an intent parser called Adapt to convert natural language into machine readable data structures. For speech synthesis Mycroft uses Mimic, which is based on Flite. Mycroft is designed to be modular, so users are able to change its components. For example, espeak can be used instead of Mimic.\n", "id": "50231947", "title": "Mycroft (software)"}
{"url": "https://en.wikipedia.org/wiki?curid=53644736", "text": "Distributional–relational database\n\nA distributional–relational database, or word-vector database, is a database management system (DBMS) that uses distributional word-vector representations to enrich the semantics of structured data. As distributional word-vectors can be automatically built automatically from large-scale corpora, this enrichment supports the construction of databases which can embed large-scale commonsense background knowledge into their operations. Distributional-Relational models can be applied to the construction of schema-agnostic databases (databases in which users can query the data without being aware of its schema), semantic search, schema-integration and inductive and abductive reasoners as well as different applications in which a semantically flexible knowledge representation model is needed. The main advantage of distributional–relational models over purely logical / Semantic Web models is the fact that the core semantic associations can be automatically captured from corpora in contrast to the definition of manually curated ontologies and rule knowledge bases.\n\nDistributional–relational models were first formalized as a mechanism to cope with the vocabulary/semantic gap between users and the schema behind the data. In this scenario, distributional semantic relatedness measures, combined with semantic pivoting heuristics can support the approximation between user queries (expressed in their own vocabulary) and data (expressed in the vocabulary of the dataset designer).\n\nIn this model, the database symbols (entities and relations) are embedded into a distributional semantic space and have a geometric interpretation under a latent or explicit semantic space. The geometric aspect supports the semantic approximation between entities from different databases or between a query term and a database entity. The distributional relational model then becomes a double layered model where the semantics of the structured data provides the fine-grained semantics intended by the database designer, which is extended by the distributional semantic model which contains the semantic associations expressed at a broader use. \nThese models support the generalization from a closed communication scenario (in which database designers and users live in the same context, e.g. the same organization) to an open communication scenario (e.g. different organizations, the Web), creating an abstraction layer between users and the specific representation of the conceptual model.\n", "id": "53644736", "title": "Distributional–relational database"}
{"url": "https://en.wikipedia.org/wiki?curid=53673334", "text": "Open information extraction\n\nIn natural language processing, open information extraction (OIE) is the task of generating a structured, machine-readable representation of the information in text, usually in the form of triples or n-ary propositions. A proposition can be understood as truth-bearer, a textual expression of a potential fact (e.g., \"Dante wrote the Divine Comedy\"), represented in an amenable structure for computers [e.g., (\"Dante\", \"wrote\", \"Divine Comedy\")]. An OIE extraction normally consists of a relation and a set of arguments. For instance, (\"Dante\", \"passed away in\" \"Ravenna\") is a proposition formed by the relation \"passed away in\" and the arguments \"Dante\" and \"Ravenna\". The first argument is usually referred as the subject while the second is considered to be the object.\n\nThe extraction is said to be a textual representation of a potential fact because its elements are not linked to a knowledge base. Furthermore, the factual nature of the proposition has not yet been established. In the above example, transforming the extraction into a full fledged fact would first require linking, if possible, the relation and the arguments to a knowledge base. Second, the truth of the extraction would need to be determined. In computer science transforming OIE extractions into ontological facts is known as relation extraction.\n\nIn fact, OIE can be seen as the first step to a wide range of deeper text understanding tasks such as relation extraction, knowledge-base construction, question answering, semantic role labeling. The extracted propositions can also be directly used for end-user applications such as structured search (e.g., retrieve all propositions with \"Dante\" as subject).\n\nOIE was first introduced by TextRunner developed at the University of Washington Turing Center headed by Oren Etzioni. Other methods introduced later such as Reverb, OLLIE, ClausIE or CSD helped to shape the OIE task by characterizing some of its aspects. At a high level, all of these approaches make use of a set of patterns to generate the extractions. Depending on the particular approach, these patterns are either hand-crafted or learned.\n\nReverb suggested the necessity to produce meaningful relations to more accurately capture the information in the input text. For instance, given the sentence \"Faust made a pact with the devil\", it would be erroneous to just produce the extraction (\"Faust\", \"made\", \"a pact\") since it would not be adequately informative. A more precise extraction would be (\"Faust\", \"made a pact with\", \"the devil\"). Reverb also argued against the generation of overspecific relations.\n\nOLLIE stressed two important aspects for OIE. First, it pointed to the lack of factuality of the propositions. For instance, in a sentence like \"If John studies hard, he will pass the exam\", it would be inaccurate to consider (\"John\", \"will pass\", \"the exam\") as a fact. Additionally, the authors indicated that an OIE system should be able to extract non-verb mediated relations, which account for significant portion of the information expressed in natural language text. For instance, in the sentence \"Obama, the former US president, was born in Hawaii\", an OIE system should be able to recognize a proposition (\"Obama\", \"is\", \"former US president\").\n\nClausIE introduced the connection between grammatical clauses, propositions, and OIE extractions. The authors stated that as each grammatical clause expresses a proposition, each verb mediated proposition can be identified by solely recognizing the set of clauses expressed in each sentence. This implies that to correctly recognize the set of propositions in an input sentence, it is necessary to understand its grammatical structure. The authors studied the case in the English language that only admits seven clause types, meaning that the identification of each proposition only requires defining seven grammatical patterns.\n\nThe finding also established a separation between the recognition of the propositions and its materialization. In a first step, the proposition can be identified without any consideration of its final form, in a domain-independent and unsupervised way, mostly based on linguistic principles. In a second step, the information can be represented according to the requirements of the underlying application, without conditioning the identification phase.\n\nConsider the sentence \"Albert Einstein was born in Ulm and died in Princeton\". The first step will recognize the two propositions (\"Albert Einstein\", \"was born\", \"in Ulm\") and (\"Albert Einstein\", \"died\", \"in Princeton\"). Once the information has been correctly identified, the propositions can take the particular form required by the underlying application [e.g., (\"Albert Einstein\", \"was born in\", \"Ulm\") and (\"Albert Einstein\", \"died in\", \"Princeton\")].\n\nCSD introduced the idea of minimality in OIE. It considers that computers can make better use of the extractions if they are expressed in a compact way. This is especially important in sentences with subordinate clauses. In these cases, CSD suggests the generation of nested extractions. For example, consider the sentence \"The Embassy said that 6,700 Americans were in Pakistan\". CSD generates two extractions [i] (\"6,700 Americans\", \"were\", \"in Pakistan\") and [ii] (\"The Embassy\", \"said\", \"that [i]). This is usually known as reification.\n", "id": "53673334", "title": "Open information extraction"}
{"url": "https://en.wikipedia.org/wiki?curid=53639854", "text": "ICarbonX\n\niCarbonX is a company founded by Chinese genomicist Jun Wang, former CEO of Beijing Genomic Institute (BGI), in 2015. iCarbonX combines genomics with other health factors such as metabolites, bacteria and lifestyle choices to create a digitalized form of life.\n\niCarbonX has raised over $600 million in investment. Tencent Holdings Limited – owner of social-media app WeChat – and Zhongyuan Union Cell & Gene Engineering Corp. invested $200 million in iCarbonX, which made iCarbonX one of only three Chinese healthcare startups with a valuation of more than $1 billion (Unicorn). The company has about 100 employees.\niCarbonX was founded on October 27, 2015. On September 10, 2016 iCarbonX acquired Imagu Vision Technologies, an Israeli AI and image processing company, in order to establish an iCarbonX-Israel R&D center. On January 5, 2017 iCarbonX announced its Digital Life Alliance with seven other companies including SomaLogic, HealthTell, PatientsLikeMe, AOBiome, GALT, Imagu and Robustnique.\n\nOn January 5, 2017 iCarbonX released Meum, a digital health management platform. The company name, “iCarbonX,” symbolizes the use of the internet and artificial intelligence to improve life, of which a central element is carbon. The “i” and “X” indicate the company’s plans to combine the Internet and artificial intelligence to create something new.\n", "id": "53639854", "title": "ICarbonX"}
{"url": "https://en.wikipedia.org/wiki?curid=253284", "text": "Discovery system\n\nA discovery system may be an artificial intelligence system (in the context of computer science, logic, and mathematics) or a bibliographical search system (in the context of library and information science).\n\nA discovery system is an artificial intelligence system that attempts to discover new scientific concepts or laws.\n\nNotable discovery systems have included\n\n\nA discovery system is a bibliographic search system based on search engine technology. It is part of the concept of Library 2.0 and is intended to supplement or even replace the existing OPAC catalogs.\n\n\nCommercial products:\n\nOpen Source products:\n\n\n\n", "id": "253284", "title": "Discovery system"}
{"url": "https://en.wikipedia.org/wiki?curid=54008163", "text": "Industrial artificial intelligence\n\nIndustrial artificial intelligence, or industrial AI, usually refers to the application of artificial intelligence to industry. Unlike general artificial intelligence which is a frontier research discipline to build computerized systems that perform tasks requiring human intelligence, industrial AI is more concerned with the application of such technologies to address industrial pain-points for customer value creation, productivity improvement, and insight discovery. Although in a dystopian vision of AI applications, intelligent machines may take away jobs of humans and cause social and ethical issues, industry in general holds a more positive view of AI and sees this transformation of economy unstoppable and expects huge business opportunities in this process.\n\nThe concept of artificial intelligence has been proposed around 1940s, and the idea of improving productivity and gaining insights through smart analytics and modeling is not new. Artificial Intelligence and Knowledge-Based systems has been an active research branch of artificial intelligence for the entire product life cycle for product design, production planning, distribution, and field services. E-manufacturing systems and e-factories did not use the term “AI,” but they scale up modeling of engineering systems to enable complete integration of elements in the manufacturing eco-system for smart operation management. Cloud Foundry service platforms widely embed the artificial intelligent technologies. Cybermanufacturing systems also apply predictive analytics and cyber-physical modeling to address the gap between production and machine health for optimized productivity.\n\nThere are several reasons for the recent popularity of industrial AI: More affordable sensors and the automated process of data acquisition; More powerful computation capability of computers to perform more complex tasks at a faster speed with lower cost; Faster connectivity infrastructure and more accessible cloud services for data management and computing power outsourcing. However, the technology alone never creates any business value if the problems in industry are not well studied. The major categories where industrial AI may contribute to are product and service innovation, process improvement, and insight discovery.\n\nIndustrial AI can be embedded to existing products or services to make them more effective, reliable, safer, and last longer. One example is the automobile industry uses vision recognition to avoid accidents, stay in lane, and use speech recognition to facilitate safer driving. In manufacturing, one example is the prediction of blade life for self-aware band saw machines, so that users will be able to rely on evidence of degradation rather than experience, which is safer, will extend blade life, and build up blade usage profile to help blade selection.\n\nIndustrial AI can create new products and novel business models. Predix by General Electric is a Cloud Foundry that serves as an industrial operating system for narrow AI application development. InsightCM™ by National Instruments and Watchdog Agent® Toolbox on the LabVIEW platform also provide software analytical capabilities. The aforementioned band saw machine manufacturer also announced their service center system to help users improve equipment reliability and sawing efficiency as a novel business model.\n\nAutomation is one of the major aspects in process applications of industrial AI. With the help of AI, the scope and pace of automation have been fundamentally changed. AI technologies boost the performance and expand the capability of conventional AI applications. An example is the collaborative robots. Collaborative robotic arms are able to learn the motion and path demonstrated by human operators and perform the same task. AI also automates the process that used to require human participation. An example is the Hong Kong subway, where an AI program decides the distribution and job scheduling of engineers with more efficiency and reliability than human counterparts do.\n\nAnother aspect of process applications is the modeling large-scale systems. Cybermanufacturing systems are defined as a manufacturing service system that is networked and resilient to faults by evidence-based modeling and data-driven deep learning. Such a system deals with large and usually geographically distributed assets, which is hard to be modeled via conventional individual-asset physics-based model. With machine learning and optimization algorithms, a bottom-up framework considering machine health can leverage large samples of assets and automate the operation management, spare part inventory planning, and maintenance scheduling process.\n\nIndustrial AI can also be used for knowledge discovery by identifying insights in engineering systems. In aviation and aeronautics, AI has been playing a vital role in many critical areas, one of which is safety assurance and root cause. NASA is trying to proactively manage risks to aircraft safety by analyzing flight numeric data and text reports in parallel to not only detect anomalies but also relate it to the causal factors. This mined insight of why certain faults happen in the past will shed light on predictions of similar incidents in the future and prevent problems before they occur.\n\nPredictive and preventive maintenance through data-driven machine learning is also critical in cost reduction for industrial applications. Prognostics and health management (PHM) programs capture the opportunities at the shop floor by modeling equipment health degradation. The obtained information can be used for efficiency improvement and quality improvement.\n\nThe challenges of industrial AI to unlock the value lies in the transformation of raw data to intelligent predictions for rapid decision-making. In general, there are four major challenges in realizing industrial AI.\n\nEngineering systems now generate a lot of data and modern industry is indeed a big data environment. However, industrial data usually is structured, but may be low-quality. The “3B” issues of industrial big data is:\nThe quality of the data may be poor, and unlike other consumer-faced applications, data from industrial systems usually have clear physical meanings, which makes it harder to compensate the quality with volume.\nData collected for training machine learning models usually is lacking a comprehensive set of working conditions and health states/fault modes, which may cause false positives and false negatives in online implementation of AI systems.\nIndustrial data patterns can be highly transient and interpreting them requires domain expertise, which can hardly be harnessed by merely mining numeric data.\n\nProduction process happens fast and the equipment and work piece can be expensive, the AI applications need to be applied in real-time to be able to detect anomalies immediately to avoid waste and other consequences. Cloud-based solutions can be powerful and fast, but they still would not fit certain computation efficiency requirements. Edge computing may be a better choice in such scenario.\n\nUnlike consumer-faced AI recommendations systems which have a high tolerance for false positives and negatives, even a very low rate of false positives or negatives rate may cost the total credibility of AI systems. Industrial AI applications are usually dealing with critical issues related to safety, reliability, and operations. Any failure in predictions could incur a negative economic and/or safety impact on the users and discourage them to rely on AI systems.\n\nBesides prediction accuracy and performance fidelity, the industrial AI systems must also go beyond prediction results and give root cause analysis for anomalies. This requires that during development, data scientists need to work with domain experts and include domain know-how into the modeling process, and have the model adaptively learn and accumulate such insights as knowledge.\n", "id": "54008163", "title": "Industrial artificial intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=54073206", "text": "Google.ai\n\nGoogle.ai is a division of Google dedicated solely to artificial intelligence. It was announced at Google I/O 2017 by CEO Sundar Pichai.\n\n\n", "id": "54073206", "title": "Google.ai"}
{"url": "https://en.wikipedia.org/wiki?curid=54329238", "text": "Lawbot\n\nLawbots are a broad class of customer-facing legal AI applications that are used to automate specific legal tasks, such as document automation and legal research. Lawbots use various artificial intelligence techniques or other intelligent systems to limit humans' direct ongoing involvement in certain steps of a legal matter. The user interfaces on lawbots vary from smart searches and step-by-step forms to chatbots. Consumer and enterprise-facing lawbot solutions often do not require direct supervision from a legal professional. Depending on the task, some client-facing solutions used at law firms operate under an attorney supervision.\n\nIn the 2016 report, Deloitte estimated that more than 110,000 law jobs in just the United Kingdom alone could be gone within the next twenty years because of automation. That could result in the creation of more highly skilled jobs and reduction of paralegal and temp positions. Deloitte's report asserts that \"there is significant potential for high-skilled roles that involve repetitive processes to be automated by smart and self-learning algorithms\".\n\nIt has been widely estimated for at least the last generation that all the programs and resources devoted to ensuring access to justice address only 20% of the civil legal needs of low-income people in the United States. Drawing on this experience, in late 2011, the U.S. government-funded Legal Services Corporation decided to convene a summit of leaders to explore how best to use technology in the access-to-justice community. The group adopted a mission for The Summit on the Use of Technology to Expand Access to Justice (Summit) consistent with the magnitude of the challenge: \"to explore the potential of technology to move the United States toward providing some form of effective assistance to 100% of persons otherwise unable to afford an attorney for dealing with essential civil legal needs\".\n\nIn April 2017, joined by Microsoft and Pro Bono Net, the Legal Services Corporation (LSC) announced a pilot program to develop online, statewide legal portals to direct individuals with civil legal needs to the most appropriate forms of assistance. These portals will use cutting-edge, user-centered technology to help ensure that all people with civil legal needs can navigate their options and more easily access solutions and services available from legal aid, the courts, the private bar, and community partners.\n\n\n", "id": "54329238", "title": "Lawbot"}
{"url": "https://en.wikipedia.org/wiki?curid=54575571", "text": "Explainable Artificial Intelligence\n\nExplainable AI (XAI) is a neologism that has recently reached the parlance of artificial intelligence. Its purpose is to provide accountability when addressing technological innovations ascribed to dynamic and non-linearly programmed systems, e.g. artificial neural networks, deep learning, and genetic algorithms.\n\nIt is about asking the question of how algorithms arrive at their decisions. In a sense, it is a technical discipline providing operational tools that might be useful for explaining systems, such as in implementing a right to explanation.\n\nAI-related algorithmic (supervised and unsupervised) practices work on a model of success that orientates towards some form of correct state, with singular focus placed on an expected output. E.g., an image recognition algorithm's level of success will be based on the algorithm's ability to recognize certain objects, and failure to do so will indicate that the algorithm requires further tuning. As the tuning level is dynamic, closely correlated to function refinement and training data-set, granular understanding of the underlying operational vectors is rarely introspected.\n\nXAI aims to address this black-box approach and allow introspection of these dynamic systems tractable, allowing humans to understand how computational machines develop their own models for solving tasks.\n\nA universal definition of this term has yet to have been fully established; however, the DARPA XAI program defines its aims as the following:\n\n\nWhile the term \"Explainable AI\" is new, the field of understanding the knowledge embedded in machine learning systems itself has a long history. Researchers have long been interested in whether it is possible to extract rules from trained neural networks, and researchers in clinical expert systems creating neural network-powered decision support for clinicians have sought to develop dynamic explanations that allow these technologies to be more trusted and trustworthy in practice.\n\nLayerwise Relevance Propagation (LRP), first described in 2015, is a technique for determining which features in a particular input vector contribute most strongly to a neural network’s output.\n\nNewer however is the focus on explaining machine learning and AI to those whom the decisions concern, rather than the designers or direct users of decision systems. Since DARPA's introduction of its program in 2016, a number of new initiatives seek to address the issue of algorithmic accountability and provide transparency concerning how technologies within this domain function.\n\n\nA cross-section of industrial sectors will be affected by these requirements, as accountability is delegated to a greater or lesser extent from humans to machines.\n\nExamples of these effects have already been seen in the following sectors:\n\nAs regulators, official bodies and general users dependency on AI-based dynamic systems, clearer accountability will be required for decision making processes to ensure trust and transparency. Evidence of this requirement gaining more momentum can be seen with the launch of the first global conference exclusively dedicated to this emerging discipline, the International Joint Conference on Artificial Intelligence: Workshop on Explainable Artificial Intelligence (XAI).\n\n", "id": "54575571", "title": "Explainable Artificial Intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=54625345", "text": "Right to explanation\n\nIn the regulation of algorithms, particularly artificial intelligence and its subfield of machine learning, a right to explanation (or right to \"an\" explanation) is a right to be given an explanation for an output of the algorithm. Such rights primarily refer to individual rights to be given an explanation for decisions that significantly affect an individual, particularly legally or financially. For example, a person who applies for a loan and is denied may ask for an explanation, which could be \"Credit bureau X reports that you declared bankruptcy last year; this is the main factor in considering you too likely to default, and thus we will not give you the loan you applied for.\"\n\nSome such legal rights already exist, while the scope of a general \"right to explanation\" is a matter of ongoing debate.\n\nCredit score in the United States – more generally, credit actions – have a well-established right to explanation. Under the Equal Credit Opportunity Act (Regulation B of the Code of Federal Regulations),\nTitle 12, Chapter X, Part 1002, §1002.9, creditors are required to notify applicants of action taken in certain circumstances, and such notifications must provide specific reasons, as detailed in §1002.9(b)(2):\n\nThe official interpretation of this section details what types of statements are acceptable.\n\nCredit agencies and data analysis firms such as FICO comply with this regulation by providing a list of reasons (generally at most 4, per interpretation of regulations), consisting of a numeric (as identifier) and an associated explanation, identifying the main factors affecting a credit score. An example might be:\n\nThe European Union General Data Protection Regulation (enacted 2016, taking effect 2018), extends the automated decision-making rights in the 1995 Data Protection Directive to provide a legally disputed form of a right to an explanation, stated as such in Recital 71: \"[the data subject should have] the right ... to obtain an explanation of the decision reached\". In full:\n\nHowever, the extent to which the regulations themselves provide a \"right to explanation\" is heavily debated. There are two main strands of criticism. There are significant legal issues with the right as found in Article 22 — as recitals are not binding, and the right to an explanation is not mentioned in the binding articles of the text, having been removed during the legislative process. In addition, there are significant restrictions on the types of automated decisions that are covered — which must be both \"solely\" based on automated processing, and have legal or similarly significant effect — which limits their applicability in many of the cases of algorithmic controversy that have been picked up in the media.\n\nA second source of such a right has been pointed to in Article 15, the \"right of access by the data subject\". This restates a similar provision from the 1995 Data Protection Directive, allowing the data subject access to \"meaningful information about the logic involved\" in the same significant, solely automated decision-making, found in Article 22. Yet this too suffers from alleged challenges that relate to the timing of when this right can be drawn upon, as well as practical challenges that mean it may not be binding in many cases of public concern.\n\nIn France the 2016 \"Loi pour une République numérique\" (Digital Republic Act or \"loi numérique\") amends the country's administrative code to introduce a new provision for the explanation of decisions made by public sector bodies about individuals. It notes that where there is \"a decision taken on the basis of an algorithmic treatment\", the rules that define that treatment and its “principal characteristics” must be communicated to the citizen upon request, where there is not an exclusion (e.g. for national security or defence). These should include the following:\nScholars have noted that this right, while limited to administrative decisions, goes beyond the GDPR right to explicitly apply to decision support rather than decisions \"solely\" based on automated processing, as well as provides a framework for explaining specific decisions. Indeed, the GDPR automated decision-making rights in the European Union, one of the places a \"right to an explanation\" has been sought within, find their origins in French law in the late 1970s.\n\nSome argue that a \"right to explanation\" is at best unnecessary, at worst harmful, and threatens to stifle innovation. Specific criticisms include: favoring human decisions over machine decisions; being redundant with existing laws; and focusing on process over outcome.\n\nMore fundamentally, many algorithms used in machine learning are not easily explainable. For example, the output of a deep neural network depends on many layers of computations, connected in a complex way, and no one input or computation may be a dominant factor. The field of Explainable AI seeks to provide better explanations from existing algorithms, and algorithms that are more easily explainable, but it is a young and active field.\n\nSimilarly, human decisions often cannot be easily explained: they may be based on intuition or a \"gut feeling\" that is hard to put into words. Requiring machines to meet a higher standard than humans is thus arguably unreasonable.\n\n\n", "id": "54625345", "title": "Right to explanation"}
{"url": "https://en.wikipedia.org/wiki?curid=54677577", "text": "Extreme: Personal Assistant\n\nExtreme: Personal assistant is a virtual personal assistant developed and published by Multiverse app for Android in May 2016.\n\nUnlike most Android assistants, Extreme has a different, more nerdy looking User Interface. The application was primarily developed to mimic the functionality of Tony Stark's J.A.R.V.I.S. from Iron Man, giving a unique experience to the users. Users can interact with the app via natural voice or keyboard. In the current form, Extreme is able to search the Internet, narrate a joke, read important notifications, open applications and carry out other simple as well as complex tasks.\n\nExtreme was originally developed for Windows PC. The first beta was rolled out in November 2015 with a heavy futuristic looking UI. The software was highly based on Iron Man's JARVIS from Marvel's MCU, Played by Paul Bettany. The early build was coded on Visual Basic 6.0 and used windows speech recognition macros and Windows TTS to listen and speak commands and even housed an Antivirus program.\n\nAt its developmental stages, Extreme could Narrate stories, read news articles, read facebook and twitter notifications, open/close programs, provide a dictation mode and search the internet. Though the system was quite buggy at the time, Iron Man fans loved the software, mainly due to the unique looking interface.\n\nExtreme, at its current form, can carry out:\n\nExtreme also offers numerous pre-scripted responses to amusing questions. When a pre-scripted response is not found, Extreme uses Machine Learning to record user's response and reply appropriately the next time.\n\nCurrently, Extreme's Functionality is limited due to a single language support. No acknowledgment is given by the developers regarding the future regionalization of the app.\n", "id": "54677577", "title": "Extreme: Personal Assistant"}
{"url": "https://en.wikipedia.org/wiki?curid=54683549", "text": "Fast-and-frugal trees\n\nIn artificial intelligence and in heuristic decision-making, a fast-and-frugal tree is a type of classification tree. Fast-and-frugal trees can be used as decision-making tools which operate as lexicographic classifiers, and, if required, associate an action (decision) to each class or category. The original fast-and-frugal trees introduced in 2003 by Laura Martignon, et al. are simple both in execution and in construction, and constitute a kind of simple heuristic in the adaptive toolbox postulated by Gerd Gigerenzer and the Center for Adaptive Behavior and Cognition. Similar models had been previously used by Martignon & Hoffrage, 1999; Green and Mehr, 1997 ; Dhami and Ayton, 2001 ; Dhami and Harries, 2001 and Fischer, Steiner, Zucol, Berger, Martignon, et al. 2002 . \n\nYet recent developments in several fields of application have maintained the simplicity in execution, introducing subtle, relevant procedures for construction that have proven extremely useful for users.\n\nThe basic elements on which to ground a binary classification are (sets of) cues. The fast-and-frugal tree establishes a ranking and, according to the ranking, a “topology” of the tree. Once the ranking is established, the fast-and-frugal tree checks one cue at a time, and at each step, one of the possible outcomes of the considered cue is an exit node which allows for a decision.\n\nFast-and-frugal trees can also be described in terms of their building blocks. First, they have a search rule: They inspect cues in a specific order. Second, they have a stopping rule: Each cue has one value that leads to an exit node and hence to a classification, and another value that leads to consulting the next cue in the cue hierarchy (the exception being the last cue in the hierarchy, which has two exit nodes). Finally, they have a classification rule. \nFigure 1 illustrates a fast-and-frugal tree for classifying a patient as “high risk” of having a heart stroke and thus having to be sent to the “coronary care unit” or “low risk” and thus having to be sent to a “regular nursing bed (Green& Mehr, 1997). \nFast-and-frugal trees have been characterized mathematically as lexicographic classifiers (Martignon, Katsikopoulos and Woike, 2008) and as linear classifiers with non-compensatory weights (Martignon, Katsikopoulos and Woike, 2008) . Their “topology” and construction has been analysed using signal detection theory (Luan, Schooler and Gigerenzer, 2011) and their performance and robustness when compared to regression and CARts has been studied by Laskey and Martignon (2014). An extensive study on the robustness, the predictive value and sensitivity/specificity of Fast-And-Frugal trees compared to those of Naive Bayes and of Full Natural Frequency Trees has been carried out by.\n\nIn 2017, Phillips, Neth, Woike and Gaissmaier introduced the R package FFTrees, which uses new algorithms to construct and evaluate FFTs. A simulation study across ten real-world data sets shows that FFTs created by FFTrees can predict data as well as more complex classification algorithms, while remaining simple enough for anyone to understand and use.\n", "id": "54683549", "title": "Fast-and-frugal trees"}
{"url": "https://en.wikipedia.org/wiki?curid=54690679", "text": "Computational human modeling\n\nComputational human modeling is an interdisciplinary computational science that links the diverse fields of artificial intelligence, cognitive science, and computer vision with machine learning, mathematics, and cognitive psychology.\n\nComputational human modeling emphasizes descriptions of human for A.I. research and applications. \n\nResearch in computational human modeling can include computer vision studies on identify (face recognition), attributes (gender, age, skin color), expressions, geometry (3D face modeling, 3D body modeling), and activity (pose, gaze, actions, and social interactions).\n\n", "id": "54690679", "title": "Computational human modeling"}
{"url": "https://en.wikipedia.org/wiki?curid=25277457", "text": "ACROSS Project\n\nACROSS is a Singular Strategic R&D Project led by Treelogic funded by the Spanish Ministry of Industry, Tourism and Trade activities in the field of Robotics and Cognitive Computing over an execution time-frame from 2009 to 2011. ACROSS project involves a number higher than 100 researchers from 13 Spanish entities.\n\nACROSS modifies the design of social robotics, blocked in providing predefined services, going further by means of intelligent systems. These systems are able to self-reconfigure and modify their behavior autonomously through the capacity for understanding, learning and software remote access.\n\nIn order to provide an open framework for collaboration between universities, research centers and the Administration, ACROSS develops Open Source Services available to everybody.\n\nACROSS works in three application domains:\n\n\n\n", "id": "25277457", "title": "ACROSS Project"}
{"url": "https://en.wikipedia.org/wiki?curid=3630374", "text": "Neural computation\n\nNeural computation is information processing performed by networks of neurons. \n\nNeural computation can be studied for example by building models of neural computation. \n\nThere is a scientific journal dedicated to this subject, \"Neural Computation\". \n\nArtificial neural networks (ANN) is subfield of the research area machine learning. Work on ANNs has been somewhat inspired by knowledge of neural computation. \n", "id": "3630374", "title": "Neural computation"}
{"url": "https://en.wikipedia.org/wiki?curid=55269442", "text": "FEDOR (armed AI bot)\n\nFEDOR or Feodor () (an acronym of \"final experimental demonstration object research\") is the name of a Russian humanoid that is being trained to stay balanced while it shoots guns. Dmitry Rogozin, the Russian Deputy Prime Minister, insisted they are not creating a Terminator. This AI robot creation was originally developed for rescue missions. It has been reported that the FEDOR is designed to make \"colonies\" on the moon and is being prepared for a solo space mission in 2021.\n\nAt the same time, a \"robotic army\" is a controversial part of Russian military plans, especially after Vladimir Putin stated that \"the country that perfects AI will be 'ruler of the world. However, he retains his position that he doesn't want to see anyone \"monopolize\" the field, and that Russia will share the technology with the \"entire world\", as with nuclear technology.\n\n", "id": "55269442", "title": "FEDOR (armed AI bot)"}
{"url": "https://en.wikipedia.org/wiki?curid=142224", "text": "A.I. Artificial Intelligence\n\nA.I. Artificial Intelligence, also known as A.I., is a 2001 American science fiction drama film directed by Steven Spielberg. The screenplay by Spielberg was based on a screen story by Ian Watson and the 1969 short story \"Supertoys Last All Summer Long\" by Brian Aldiss. The film was produced by Kathleen Kennedy, Spielberg and Bonnie Curtis. It stars Haley Joel Osment, Jude Law, Frances O'Connor, Brendan Gleeson and William Hurt. Set in a futuristic post-climate change society, \"A.I.\" tells the story of David (Osment), a childlike android uniquely programmed with the ability to love.\n\nDevelopment of \"A.I.\" originally began with producer-director Stanley Kubrick, after he acquired the rights to Aldiss' story in the early 1970s. Kubrick hired a series of writers until the mid-1990s, including Brian Aldiss, Bob Shaw, Ian Watson, and Sara Maitland. The film languished in protracted development for years, partly because Kubrick felt computer-generated imagery was not advanced enough to create the David character, whom he believed no child actor would convincingly portray. In 1995, Kubrick handed \"A.I.\" to Spielberg, but the film did not gain momentum until Kubrick's death in 1999. Spielberg remained close to Watson's film treatment for the screenplay.\n\nThe film divided critics, with the overall balance being positive, and grossed approximately $235 million. The film was nominated for two Academy Awards at the 74th Academy Awards, for Best Visual Effects and Best Original Score (by John Williams).\n\nIn a 2016 BBC poll of 177 critics around the world, Steven Spielberg's \"A.I. Artificial Intelligence\" was voted the eighty-third greatest film since 2000. \"A.I.\" is dedicated to Stanley Kubrick.\n\nIn the late 22nd century, rising sea levels from global warming have wiped out coastal cities such as Amsterdam, Venice, and New York, and drastically reduced the world's population. A new type of robots called Mecha, advanced humanoids capable of thoughts and emotions, have been created.\n\nDavid, a Mecha that resembles a human child and is programmed to display love for its owners, is sent to Henry Swinton, and his wife, Monica, as a replacement for their son, Martin, who has been placed in suspended animation until he can be cured of a rare disease. Monica warms to David and activates his imprinting protocol, causing him to have an enduring childlike love for her. David is befriended by Teddy, a robotic teddy bear, who cares for David's well-being.\n\nMartin is cured of his disease and brought home; as he recovers, he grows jealous of David. He makes David go to Monica in the night and cut off a lock of her hair. This upsets the parents, particularly Henry, who fears that the scissors are a weapon.\n\nAt a pool party, one of Martin's friends pokes David with a knife, activating his self-protection programming. David fearfully grabs Martin for protection and they accidentally fall into the pool. Martin is saved from drowning, but Henry persuades Monica to return David to his creator for destruction. Instead, Monica abandons both David and Teddy in the forest to hide as an unregistered Mecha.\n\nDavid is captured for an anti-Mecha \"Flesh Fair\", where obsolete and unlicensed Mecha are destroyed before cheering crowds. David is nearly killed, but tricks the crowd into thinking that he is human, and escapes with Gigolo Joe, a male prostitute Mecha who is on the run after being framed for murder. The two set out to find the Blue Fairy, whom David remembers from \"The Adventures of Pinocchio\", and believes can turn him into a human, allowing Monica to love him and take him home.\n\nJoe and David make their way to the resort town, Rouge City, where \"Dr. Know\", a holographic answer engine, leads them to the top of Rockefeller Center in the flooded ruins of Manhattan. There, David meets a copy of himself and destroys it. David then meets his creator, Professor Hobby, who tells David that he was built in the image of the professor's dead son David, and that more copies, including female versions called Darlene, are being manufactured.\n\nDisheartened, David falls from a ledge, but is rescued by Joe using their amphibicopter. David tells Joe he saw the Blue Fairy underwater and wants to go down to meet her. Joe is captured by the authorities using an electromagnet. David and Teddy use the amphibicopter to go to the Fairy, which turns out to be a statue at the now-sunken Coney Island. The two become trapped when the Wonder Wheel falls on their vehicle. David asks repeatedly to be turned into a real boy until the ocean freezes and is deactivated once his power source is drained.\n\nTwo thousand years later, humans have become extinct, and Manhattan is buried under glacial ice. The Mecha have evolved into an advanced, intelligent, silicon-based form. They find David and Teddy, and discover they are original Mecha that knew living humans, making them special.\n\nDavid is revived and walks to the frozen Fairy statue, which collapses when he touches it. The Mecha use David’s memories to reconstruct the Swinton home and explain to him that they cannot make him human. However, David insists that they recreate Monica from DNA in the lock of hair. The Mecha warn David that the clone can only live for a day, and that the process cannot be repeated. David spends the next day with Monica and Teddy. Before she drifts off to sleep, Monica tells David she has always loved him. Teddy climbs onto the bed and watches the two lie peacefully together.\n\n\nKubrick began development on an adaptation of \"Super-Toys Last All Summer Long\" in the late 1970s, hiring the story's author, Brian Aldiss, to write a film treatment. In 1985, Kubrick asked Steven Spielberg to direct the film, with Kubrick producing. Warner Bros. agreed to co-finance \"A.I.\" and cover distribution duties. The film labored in development hell, and Aldiss was fired by Kubrick over creative differences in 1989. Bob Shaw served as writer very briefly, leaving after six weeks because of Kubrick's demanding work schedule, and Ian Watson was hired as the new writer in March 1990. Aldiss later remarked, \"Not only did the bastard fire me, he hired my enemy [Watson] instead.\" Kubrick handed Watson \"The Adventures of Pinocchio\" for inspiration, calling \"A.I.\" \"a picaresque robot version of \"Pinocchio\"\".\n\nThree weeks later Watson gave Kubrick his first story treatment, and concluded his work on \"A.I.\" in May 1991 with another treatment, at 90 pages. Gigolo Joe was originally conceived as a GI Mecha, but Watson suggested changing him to a male prostitute. Kubrick joked, \"I guess we lost the kiddie market.\" In the meantime, Kubrick dropped \"A.I.\" to work on a film adaptation of \"Wartime Lies\", feeling computer animation was not advanced enough to create the David character. However, after the release of Spielberg's \"Jurassic Park\" (with its innovative use of computer-generated imagery), it was announced in November 1993 that production would begin in 1994. Dennis Muren and Ned Gorman, who worked on \"Jurassic Park\", became visual effects supervisors, but Kubrick was displeased with their previsualization, and with the expense of hiring Industrial Light & Magic.\n\nIn early 1994, the film was in pre-production with Christopher \"Fangorn\" Baker as concept artist, and Sara Maitland assisting on the story, which gave it \"a feminist fairy-tale focus\". Maitland said that Kubrick never referred to the film as \"A.I.\", but as \"Pinocchio\". Chris Cunningham became the new visual effects supervisor. Some of his unproduced work for \"A.I.\" can be seen on the DVD, \"The Work of Director Chris Cunningham\". Aside from considering computer animation, Kubrick also had Joseph Mazzello do a screen test for the lead role. Cunningham helped assemble a series of \"little robot-type humans\" for the David character. \"We tried to construct a little boy with a movable rubber face to see whether we could make it look appealing,\" producer Jan Harlan reflected. \"But it was a total failure, it looked awful.\" Hans Moravec was brought in as a technical consultant.\nMeanwhile, Kubrick and Harlan thought \"A.I.\" would be closer to Steven Spielberg's sensibilities as director. Kubrick handed the position to Spielberg in 1995, but Spielberg chose to direct other projects, and convinced Kubrick to remain as director. The film was put on hold due to Kubrick's commitment to \"Eyes Wide Shut\" (1999). After the filmmaker's death in March 1999, Harlan and Christiane Kubrick approached Spielberg to take over the director's position. By November 1999, Spielberg was writing the screenplay based on Watson's 90-page story treatment. It was his first solo screenplay credit since \"Close Encounters of the Third Kind\" (1977). Spielberg remained close to Watson's treatment, but removed various sex scenes with Gigolo Joe. Pre-production was briefly halted during February 2000, because Spielberg pondered directing other projects, which were \"Harry Potter and the Philosopher's Stone\", \"Minority Report\" and \"Memoirs of a Geisha\". The following month Spielberg announced that \"A.I.\" would be his next project, with \"Minority Report\" as a follow-up. When he decided to fast track \"A.I.\", Spielberg brought Chris Baker back as concept artist.\n\nThe original start date was July 10, 2000, but filming was delayed until August. Aside from a couple of weeks shooting on location in Oxbow Regional Park in Oregon, \"A.I.\" was shot entirely using sound stages at Warner Bros. Studios and the Spruce Goose Dome in Long Beach, California.\nThe Swinton house was constructed on Stage 16, while Stage 20 was used for Rouge City and other sets. Spielberg copied Kubrick's obsessively secretive approach to filmmaking by refusing to give the complete script to cast and crew, banning press from the set, and making actors sign confidentiality agreements. Social robotics expert Cynthia Breazeal served as technical consultant during production. Haley Joel Osment and Jude Law applied prosthetic makeup daily in an attempt to look shinier and robotic. Costume designer Bob Ringwood (\"Batman\", \"Troy\") studied pedestrians on the Las Vegas Strip for his influence on the Rouge City extras. Spielberg found post-production on \"A.I.\" difficult because he was simultaneously preparing to shoot \"Minority Report\".\n\nThe film's soundtrack was released by Warner Sunset Records in 2001. The original score was composed and conducted by John Williams and featured singers Lara Fabian on two songs and Josh Groban on one. The film's score also had a limited release as an official \"For your consideration Academy Promo\", as well as a complete score issue by La-La Land Records in 2015. The band Ministry appears in the film playing the song \"What About Us?\" (but the song does not appear on the official soundtrack album).\n\nWarner Bros. used an alternate reality game titled \"The Beast\" to promote the film. Over forty websites were created by Atomic Pictures in New York City (kept online at Cloudmakers.org) including the website for Cybertronics Corp. There were to be a series of video games for the Xbox video game console that followed the storyline of \"The Beast\", but they went undeveloped. To avoid audiences mistaking \"A.I.\" for a family film, no action figures were created, although Hasbro released a talking Teddy following the film's release in June 2001.\n\n\"A.I.\" had its premiere at the Venice Film Festival in 2001.\n\nThe film opened in 3,242 theaters in the United States on June 29, 2001, earning $29,352,630 during its opening weekend. \"A.I\" went on to gross $78.62 million in US totals as well as $157.31 million in foreign countries, coming to a worldwide total of $235.93 million.\n\nBased on 190 reviews collected by Rotten Tomatoes, 73% of the critics gave the film positive notices with a score of 6.6 out of 10. The website described the critical consensus perceiving the film as \"a curious, not always seamless, amalgamation of Kubrick's chilly bleakness and Spielberg's warm-hearted optimism. [The film] is, in a word, fascinating.\" By comparison, Metacritic collected an average score of 65, based on 32 reviews, which is considered favorable.\n\nProducer Jan Harlan stated that Kubrick \"would have applauded\" the final film, while Kubrick's widow Christiane also enjoyed \"A.I\". Brian Aldiss admired the film as well: \"I thought what an inventive, intriguing, ingenious, involving film this was. There are flaws in it and I suppose I might have a personal quibble but it's so long since I wrote it.\" Of the film's ending, he wondered how it might have been had Kubrick directed the film: \"That is one of the 'ifs' of film history—at least the ending indicates Spielberg adding some sugar to Kubrick's wine. The actual ending is overly sympathetic and moreover rather overtly engineered by a plot device that does not really bear credence. But it's a brilliant piece of film and of course it's a phenomenon because it contains the energies and talents of two brilliant filmmakers.\" Richard Corliss heavily praised Spielberg's direction, as well as the cast and visual effects. Roger Ebert gave the film four stars, saying that it was \"wonderful and maddening.\" Leonard Maltin, on the other hand, gives the film two stars out of four in his \"Movie Guide\", writing: \"[The] intriguing story draws us in, thanks in part to Osment's exceptional performance, but takes several wrong turns; ultimately, it just doesn't work. Spielberg rewrote the adaptation Stanley Kubrick commissioned of the Brian Aldiss short story 'Super Toys Last All Summer Long'; [the] result is a curious and uncomfortable hybrid of Kubrick and Spielberg sensibilities.\" However, he calls John Williams' music score \"striking\". Jonathan Rosenbaum compared \"A.I.\" to \"Solaris\" (1972), and praised both \"Kubrick for proposing that Spielberg direct the project and Spielberg for doing his utmost to respect Kubrick's intentions while making it a profoundly personal work.\" Film critic Armond White, of the \"New York Press\", praised the film noting that \"each part of David’s journey through carnal and sexual universes into the final eschatological devastation becomes as profoundly philosophical and contemplative as anything by cinema’s most thoughtful, speculative artists – Borzage, Ozu, Demy, Tarkovsky.\" Filmmaker Billy Wilder hailed \"A.I.\" as \"the most underrated film of the past few years.\" When British filmmaker Ken Russell saw the film, he wept during the ending.\n\nMick LaSalle gave a largely negative review. \"\"A.I.\" exhibits all its creators' bad traits and none of the good. So we end up with the structureless, meandering, slow-motion endlessness of Kubrick combined with the fuzzy, cuddly mindlessness of Spielberg.\" Dubbing it Spielberg's \"first boring movie\", LaSalle also believed the robots at the end of the film were aliens, and compared Gigolo Joe to the \"useless\" Jar Jar Binks, yet praised Robin Williams for his portrayal of a futuristic Albert Einstein. Peter Travers gave a mixed review, concluding \"Spielberg cannot live up to Kubrick's darker side of the future.\" But he still put the film on his top ten list that year for best movies. David Denby in \"The New Yorker\" criticized \"A.I.\" for not adhering closely to his concept of the Pinocchio character. Spielberg responded to some of the criticisms of the film, stating that many of the \"so called sentimental\" elements of \"A.I.\", including the ending, were in fact Kubrick's and the darker elements were his own. However, Sara Maitland, who worked on the project with Kubrick in the 1990s, claimed that one of the reasons Kubrick never started production on \"A.I.\" was because he had a hard time making the ending work. James Berardinelli found the film \"consistently involving, with moments of near-brilliance, but far from a masterpiece. In fact, as the long-awaited 'collaboration' of Kubrick and Spielberg, it ranks as something of a disappointment.\" Of the film's highly debated finale, he claimed, \"There is no doubt that the concluding 30 minutes are all Spielberg; the outstanding question is where Kubrick's vision left off and Spielberg's began.\"\n\nScreenwriter Ian Watson has speculated, \"Worldwide, \"A.I.\" was very successful (and the 4th highest earner of the year) but it didn't do quite so well in America, because the film, so I'm told, was too poetical and intellectual in general for American tastes. Plus, quite a few critics in America misunderstood the film, thinking for instance that the Giacometti-style beings in the final 20 minutes were aliens (whereas they were robots of the future who had evolved themselves from the robots in the earlier part of the film) and also thinking that the final 20 minutes were a sentimental addition by Spielberg, whereas those scenes were exactly what I wrote for Stanley and exactly what he wanted, filmed faithfully by Spielberg.\"\n\nIn 2002, Spielberg told film critic Joe Leydon that \"People pretend to think they know Stanley Kubrick, and think they know me, when most of them don't know either of us\". \"And what's really funny about that is, all the parts of \"A.I.\" that people assume were Stanley's were mine. And all the parts of \"A.I.\" that people accuse me of sweetening and softening and sentimentalizing were all Stanley's. The teddy bear was Stanley's. The whole last 20 minutes of the movie was completely Stanley's. The whole first 35, 40 minutes of the film – all the stuff in the house – was word for word, from Stanley's screenplay. This was Stanley's vision.\" \"Eighty percent of the critics got it all mixed up. But I could see why. Because, obviously, I've done a lot of movies where people have cried and have been sentimental. And I've been accused of sentimentalizing hard-core material. But in fact it was Stanley who did the sweetest parts of \"A.I.\", not me. I'm the guy who did the dark center of the movie, with the Flesh Fair and everything else. That's why he wanted me to make the movie in the first place. He said, 'This is much closer to your sensibilities than my own.\n\nUpon rewatching the film many years after its release, BBC film critic Mark Kermode apologized to Spielberg in an interview in January 2013 for \"getting it wrong\" on the film when he first viewed it in 2001. He now believes the film to be Spielberg's \"enduring masterpiece\".\n\nVisual effects supervisors Dennis Muren, Stan Winston, Michael Lantieri and Scott Farrar were nominated for the Academy Award for Best Visual Effects, while John Williams was nominated for Best Original Music Score. Steven Spielberg, Jude Law and Williams received nominations at the 59th Golden Globe Awards. \"A.I.\" was successful at the Saturn Awards, winning five awards, including Best Science Fiction Film along with Best Writing for Spielberg and Best Performance by a Younger Actor for Osment.\n\n\n", "id": "142224", "title": "A.I. Artificial Intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=55443695", "text": "Google Clips\n\nGoogle Clips is a miniature clip-on camera device developed by Google. It was announced during a Google event on 4 October 2017.\n\nWith a flashing LED that indicates it's recording, Google Clips automatically captures video clips at moments its machine learning algorithms determined to be interesting or relevant.\n\n\"The Independent\" wrote that Google Clips is \"an impressive little device, but one that also has the potential to feel very creepy.\"\n", "id": "55443695", "title": "Google Clips"}
{"url": "https://en.wikipedia.org/wiki?curid=55507995", "text": "Open Neural Network Exchange\n\nThe Open Neural Network Exchange (ONNX) is an open-source artificial intelligence ecosystem. ONNX is available on GitHub.\n\nIn September 2017 Facebook and Microsoft introduced a system for switching between machine learning frameworks such as PyTorch and Caffe2. Later, IBM, Huawei, Intel, AMD, ARM and Qualcomm announced support for the initiative.\n\nIn October 2017, Microsoft announced that it would add its Cognitive Toolkit and Project Brainwave platform to the initiative.\n\nThe initiative targets:\n\nAllow developers to more easily move between frameworks, some of which may be more desirable for specific phases of the development process, such as fast training, network architecture flexibility or inferencing on mobile devices.\n\nAllow hardware vendors and others to improve the performance of artificial neural networks of multiple frameworks at once by targeting the ONNX representation.\n\nONNX provides definitions of an extensible computation graph model, built-in operators and standard data types, focused on inferencing (evaluation).\n\nEach computation dataflow graph is a list of nodes that form an acyclic graph. Nodes have inputs and outputs. Each node is a call to an operator. Metadata documents the graph. Built-in operators are to be available on each ONNX-supporting framework.\n\nMicrosoft and Facebook are part of the Partnership on AI along with Apple, Amazon, Google and IBM that works to increase public awareness and boost research.\n\n", "id": "55507995", "title": "Open Neural Network Exchange"}
{"url": "https://en.wikipedia.org/wiki?curid=1164", "text": "Artificial intelligence\n\nArtificial intelligence (AI, also machine intelligence, MI) is intelligence displayed by machines, in contrast with the natural intelligence (NI) displayed by humans and other animals. In computer science AI research is defined as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of success at some goal. Colloquially, the term \"artificial intelligence\" is applied when a machine mimics \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\". See glossary of artificial intelligence.\n\nThe scope of AI is disputed: as machines become increasingly capable, tasks considered as requiring \"intelligence\" are often removed from the definition, a phenomenon known as the AI effect, leading to the quip \"AI is whatever hasn't been done yet.\" For instance, optical character recognition is frequently excluded from \"artificial intelligence\", having become a routine technology. Capabilities generally classified as AI include successfully understanding human speech, competing at a high level in strategic game systems (such as chess and Go), autonomous cars, intelligent routing in content delivery networks, military simulations, and interpreting complex data, including images and videos.\nArtificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an \"AI winter\"), followed by new approaches, success and renewed funding. For most of its history, AI research has been divided into subfields that often fail to communicate with each other. These sub-fields are based on technical considerations, such as particular goals (e.g. \"robotics\" or \"machine learning\"), the use of particular tools (\"logic\" or \"neural networks\"), or deep philosophical differences. Subfields have also been based on social factors (particular institutions or the work of particular researchers). \nThe traditional problems (or goals) of AI research include reasoning, knowledge, planning, learning, natural language processing, perception and the ability to move and manipulate objects. General intelligence is among the field's long-term goals. Approaches include statistical methods, computational intelligence, and traditional symbolic AI. Many tools are used in AI, including versions of search and mathematical optimization, neural networks and methods based on statistics, probability and economics. The AI field draws upon computer science, mathematics, psychology, linguistics, philosophy, neuroscience, artificial psychology and many others.\nThe field was founded on the claim that human intelligence \"can be so precisely described that a machine can be made to simulate it\". This raises philosophical arguments about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence, issues which have been explored by myth, fiction and philosophy since antiquity. Some people also consider AI a danger to humanity if it progresses unabatedly. Others believe that AI, unlike previous technological revolutions, will create a risk of mass unemployment.\nIn the twenty-first century, AI techniques have experienced a resurgence following concurrent advances in computer power, large amounts of data, and theoretical understanding; and AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science.\n\nWhile thought-capable artificial beings appeared as storytelling devices in antiquity, the idea of actually trying to build a machine to perform useful reasoning may have begun with Ramon Llull (c. 1300 CE). With his Calculus ratiocinator, Gottfried Leibniz extended the concept of the calculating machine (Wilhelm Schickard engineered the first one around 1623), intending to perform operations on concepts rather than numbers. Since the 19th century, artificial beings are common in fiction, as in Mary Shelley's \"Frankenstein\" or Karel Čapek's \"R.U.R. (Rossum's Universal Robots)\".\nThe study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable act of mathematical deduction. This insight, that digital computers can simulate any process of formal reasoning, is known as the Church–Turing thesis. Along with concurrent discoveries in neurology, information theory and cybernetics, this led researchers to consider the possibility of building an electronic brain. The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete \"artificial neurons\".\nThe field of AI research was born at a workshop at Dartmouth College in 1956. Attendees Allen Newell (CMU), Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) and Arthur Samuel (IBM) became the founders and leaders of AI research. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies (c. 1954) (and by 1959 were reportedly playing better than the average human), solving word problems in algebra, proving logical theorems (Logic Theorist, first run c. 1956) and speaking English. By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world. AI's founders were optimistic about the future: Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". Marvin Minsky agreed, writing, \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".\nThey failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an \"AI winter\", a period when obtaining funding for AI projects was difficult.\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985 the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting hiatus began.\nIn the late 1990s and early 21st century, AI began to be used for logistics, data mining, medical diagnosis and other areas. The success was due to increasing computational power (see Moore's law), greater emphasis on solving specific problems, new ties between AI and other fields and a commitment by researchers to mathematical methods and scientific standards. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov on 11 May 1997.\nAdvanced statistical techniques (loosely known as deep learning), access to large amounts of data and faster computers enabled advances in machine learning and perception. By the mid 2010s, machine learning applications were used throughout the world. In a \"Jeopardy!\" quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin. The Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One use algorithms that emerged from lengthy AI research as do intelligent personal assistants in smartphones. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. In the 2017 Future of Go Summit, AlphaGo won a three-game match with Ke Jie, who at the time continuously held the world No. 1 ranking for two years. This marked the completion of a significant milestone in the development of Artificial Intelligence as Go is an extremely complex game, more so than Chess.\n\nAccording to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a \"sporadic usage\" in 2012 to more than 2,700 projects. Clark also presents factual data indicating that error rates in image processing tasks have fallen significantly since 2011. He attributes this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets. Other cited examples include Microsoft's development of a Skype system that can automatically translate from one language to another and Facebook's system that can describe images to blind people.\n\nThe overall research goal of artificial intelligence is to create technology that allows computers and machines to function in an intelligent manner. The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.\n\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.\nFor difficult problems, algorithms can require enormous computational resources—most experience a \"combinatorial explosion\": the amount of memory or computer time required becomes astronomical for problems of a certain size. The search for more efficient problem-solving algorithms is a high priority.\n\nHuman beings ordinarily use fast, intuitive judgments rather than step-by-step deduction that early AI research was able to model. AI has progressed using \"sub-symbolic\" problem solving: embodied agent approaches emphasize the importance of sensorimotor skills to higher reasoning; neural net research attempts to simulate the structures inside the brain that give rise to this skill; statistical approaches to AI mimic the human ability to guess.\n\nKnowledge representation and knowledge engineering are central to AI research. Many of the problems machines are expected to solve will require extensive knowledge about the world. Among the things that AI needs to represent are: objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge (what we know about what other people know); and many other, less well researched domains. A representation of \"what exists\" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The semantics of these are captured as description logic concepts, roles, and individuals, and typically implemented as classes, properties, and individuals in the Web Ontology Language. The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge by acting as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). Such formal knowledge representations are suitable for content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery via automated reasoning (inferring new statements based on explicitly stated knowledge), etc. Video events are often represented as SWRL rules, which can be used, among others, to automatically generate subtitles for constrained videos.\n\nAmong the most difficult problems in knowledge representation are:\n\nIntelligent agents must be able to set goals and achieve them. They need a way to visualize the future—a representation of the state of the world and be able to make predictions about how their actions will change it—and be able to make choices that maximize the utility (or \"value\") of available choices.\n\nIn classical planning problems, the agent can assume that it is the only system acting in the world, allowing the agent to be certain of the consequences of its actions. However, if the agent is not the only actor, then it requires that the agent can reason under uncertainty. This calls for an agent that can not only assess its environment and make predictions, but also evaluate its predictions and adapt based on its assessment.\n\nMulti-agent planning uses the cooperation and competition of many agents to achieve a given goal. Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.\n\nMachine learning, a fundamental concept of AI research since the field's inception, is the study of computer algorithms that improve automatically through experience.\n\nUnsupervised learning is the ability to find patterns in a stream of input. Supervised learning includes both classification and numerical regression. Classification is used to determine what category something belongs in, after seeing a number of examples of things from several categories. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent uses this sequence of rewards and punishments to form a strategy for operating in its problem space. These three types of learning can be analyzed in terms of decision theory, using concepts like utility. The mathematical analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory.\n\nWithin developmental robotics, developmental learning approaches are elaborated upon to allow robots to accumulate repertoires of novel skills through autonomous self-exploration, social interaction with human teachers, and the use of guidance mechanisms (active learning, maturation, motor synergies, etc.).\n\nNatural language processing gives machines the ability to read and understand human language. A sufficiently powerful natural language processing system would enable natural language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of natural language processing include information retrieval, text mining, question answering and machine translation.\n\nA common method of processing and extracting meaning from natural language is through semantic indexing. Although these indexes require a large volume of user input, it is expected that increases in processor speeds and decreases in data storage costs will result in greater efficiency.\n\nMachine perception is the ability to use input from sensors (such as cameras, microphones, tactile sensors, sonar and others) to deduce aspects of the world. Computer vision is the ability to analyze visual input. A few selected subproblems are speech recognition, facial recognition and object recognition.\n\nThe field of robotics is closely related to AI. Intelligence is required for robots to handle tasks such as object manipulation and navigation, with sub-problems such as localization, mapping, and motion planning. These systems require that an agent is able to: Be spatially cognizant of its surroundings, learn from and build a map of its environment, figure out how to get from one point in space to another, and execute that movement (which often involves compliant motion, a process where movement requires maintaining physical contact with an object).\n\nAffective computing is the study and development of systems that can recognize, interpret, process, and simulate human affects. It is an interdisciplinary field spanning computer sciences, psychology, and cognitive science. While the origins of the field may be traced as far back as the early philosophical inquiries into emotion, the more modern branch of computer science originated with Rosalind Picard's 1995 paper on \"affective computing\". A motivation for the research is the ability to simulate empathy, where the machine would be able to interpret human emotions and adapts its behavior to give an appropriate response to those emotions.\n\nEmotion and social skills are important to an intelligent agent for two reasons. First, being able to predict the actions of others by understanding their motives and emotional states allow an agent to make better decisions. Concepts such as game theory, decision theory, necessitate that an agent be able to detect and model human emotions. Second, in an effort to facilitate human–computer interaction, an intelligent machine may want to display emotions (even if it does not experience those emotions itself) to appear more sensitive to the emotional dynamics of human interaction.\n\nA sub-field of AI addresses creativity both theoretically (the philosophical psychological perspective) and practically (the specific implementation of systems that generate novel and useful outputs).\n\nMany researchers think that their work will eventually be incorporated into a machine with artificial general intelligence, combining all the skills mentioned above and even exceeding human ability in most or all these areas. A few believe that anthropomorphic features like artificial consciousness or an artificial brain may be required for such a project.\n\nMany of the problems above also require that general intelligence be solved. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's original intent (social intelligence). A problem like machine translation is considered \"AI-complete\", but all of these problems need to be solved simultaneously in order to reach human-level machine performance.\n\nThere is no established unifying theory or paradigm that guides AI research. Researchers disagree about many issues. A few of the most long standing questions that have remained unanswered are these: should artificial intelligence simulate natural intelligence by studying psychology or neurology? Or is human biology as irrelevant to AI research as bird biology is to aeronautical engineering?\nCan intelligent behavior be described using simple, elegant principles (such as logic or optimization)? Or does it necessarily require solving a large number of completely unrelated problems?\nCan intelligence be reproduced using high-level symbols, similar to words and ideas? Or does it require \"sub-symbolic\" processing?\nJohn Haugeland, who coined the term GOFAI (Good Old-Fashioned Artificial Intelligence), also proposed that AI should more properly be referred to as synthetic intelligence, a term which has since been adopted by some non-GOFAI researchers.\n\nStuart Shapiro divides AI research into three approaches, which he calls computational psychology, computational philosophy, and computer science. Computational psychology is used to make computer programs that mimic human behavior. Computational philosophy, is used to develop an adaptive, free-flowing computer mind. Implementing computer science serves the goal of creating computers that can perform tasks that only people could previously accomplish. Together, the humanesque behavior, mind, and actions make up artificial intelligence.\n\nIn the 1940s and 1950s, a number of researchers explored the connection between neurology, information theory, and cybernetics. Some of them built machines that used electronic networks to exhibit rudimentary intelligence, such as W. Grey Walter's turtles and the Johns Hopkins Beast. Many of these researchers gathered for meetings of the Teleological Society at Princeton University and the Ratio Club in England. By 1960, this approach was largely abandoned, although elements of it would be revived in the 1980s.\n\nWhen access to digital computers became possible in the middle 1950s, AI research began to explore the possibility that human intelligence could be reduced to symbol manipulation. The research was centered in three institutions: Carnegie Mellon University, Stanford and MIT, and each one developed its own style of research. John Haugeland named these approaches to AI \"good old fashioned AI\" or \"GOFAI\". During the 1960s, symbolic approaches had achieved great success at simulating high-level thinking in small demonstration programs. Approaches based on cybernetics or neural networks were abandoned or pushed into the background.\nResearchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.\n\nEconomist Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.\n\nUnlike Newell and Simon, John McCarthy felt that machines did not need to simulate human thought, but should instead try to find the essence of abstract reasoning and problem solving, regardless of whether people used the same algorithms. His laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning. Logic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.\n\nResearchers at MIT (such as Marvin Minsky and Seymour Papert) found that solving difficult problems in vision and natural language processing required ad-hoc solutions – they argued that there was no simple and general principle (like logic) that would capture all the aspects of intelligent behavior. Roger Schank described their \"anti-logic\" approaches as \"scruffy\" (as opposed to the \"neat\" paradigms at CMU and Stanford). Commonsense knowledge bases (such as Doug Lenat's Cyc) are an example of \"scruffy\" AI, since they must be built by hand, one complicated concept at a time.\n\nWhen computers with large memories became available around 1970, researchers from all three traditions began to build knowledge into AI applications. This \"knowledge revolution\" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first truly successful form of AI software. The knowledge revolution was also driven by the realization that enormous amounts of knowledge would be required by many simple AI applications.\n\nBy the 1980s progress in symbolic AI seemed to stall and many believed that symbolic systems would never be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into \"sub-symbolic\" approaches to specific AI problems. Sub-symbolic methods manage to approach intelligence without specific representations of knowledge.\n\nThis includes embodied, situated, behavior-based, and nouvelle AI. Researchers from the related field of robotics, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move and survive. Their work revived the non-symbolic viewpoint of the early cybernetics researchers of the 1950s and reintroduced the use of control theory in AI. This coincided with the development of the embodied mind thesis in the related field of cognitive science: the idea that aspects of the body (such as movement, perception and visualization) are required for higher intelligence.\n\nInterest in neural networks and \"connectionism\" was revived by David Rumelhart and others in the middle of the 1980s. Neural networks are an example of soft computing --- they are solutions to problems which cannot be solved with complete logical certainty, and where an approximate solution is often sufficient. Other soft computing approaches to AI include fuzzy systems, evolutionary computation and many statistical tools. The application of soft computing to AI is studied collectively by the emerging discipline of computational intelligence.\n\nIn the 1990s, AI researchers developed sophisticated mathematical tools to solve specific subproblems. These tools are truly scientific, in the sense that their results are both measurable and verifiable, and they have been responsible for many of AI's recent successes. The shared mathematical language has also permitted a high level of collaboration with more established fields (like mathematics, economics or operations research). Stuart Russell and Peter Norvig describe this movement as nothing less than a \"revolution\" and \"the victory of the neats\". Critics argue that these techniques (with few exceptions) are too focused on particular problems and have failed to address the long-term goal of general intelligence. There is an ongoing debate about the relevance and validity of statistical approaches in AI, exemplified in part by exchanges between Peter Norvig and Noam Chomsky.\n\n\nIn the course of 60 or so years of research, AI has developed a large number of tools to solve the most difficult problems in computer science. A few of the most general of these methods are discussed below.\n\nMany problems in AI can be solved in theory by intelligently searching through many possible solutions: Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule. Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Robotics algorithms for moving limbs and grasping objects use local searches in configuration space. Many learning algorithms use search algorithms based on optimization.\n\nSimple exhaustive searches are rarely sufficient for most real world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use \"heuristics\" or \"rules of thumb\" that prioritize choices in favor of those that are more likely to reach a goal, and to do so in a shorter number of steps. In some search methodologies heuristics can also serve to entirely eliminate some choices that are unlikely to lead to a goal (called \"pruning the search tree\"). Heuristics supply the program with a \"best guess\" for the path on which the solution lies. Heuristics limit the search for solutions into a smaller sample size.\n\nA very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other optimization algorithms are simulated annealing, beam search and random optimization.\n\nEvolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Forms of evolutionary computation include swarm intelligence algorithms (such as ant colony or particle swarm optimization) and evolutionary algorithms (such as genetic algorithms, gene expression programming, and genetic programming).\n\nLogic is used for knowledge representation and problem solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning and inductive logic programming is a method for learning.\n\nSeveral different forms of logic are used in AI research. Propositional or sentential logic is the logic of statements which can be true or false. First-order logic also allows the use of quantifiers and predicates, and can express facts about objects, their properties, and their relations with each other. Fuzzy logic, is a version of first-order logic which allows the truth of a statement to be represented as a value between 0 and 1, rather than simply True (1) or False (0). Fuzzy systems can be used for uncertain reasoning and have been widely used in modern industrial and consumer product control systems. Subjective logic models uncertainty in a different and more explicit manner than fuzzy-logic: a given binomial opinion satisfies belief + disbelief + uncertainty = 1 within a Beta distribution. By this method, ignorance can be distinguished from probabilistic statements that an agent makes with high confidence.\n\nDefault logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem. Several extensions of logic have been designed to handle specific domains of knowledge, such as: description logics; situation calculus, event calculus and fluent calculus (for representing events and time); causal calculus; belief calculus; and modal logics.\n\nMany problems in AI (in reasoning, planning, learning, perception and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of powerful tools to solve these problems using methods from probability theory and economics.\n\nBayesian networks are a very general tool that can be used for a large number of problems: reasoning (using the Bayesian inference algorithm), learning (using the expectation-maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks). Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).\n\nA key concept from the science of economics is \"utility\": a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\n\nThe simplest AI applications can be divided into two types: classifiers (\"if shiny then diamond\") and controllers (\"if shiny then pick up\"). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine a closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class can be seen as a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.\n\nA classifier can be trained in various ways; there are many statistical and machine learning approaches. The most widely used classifiers are the neural network,\nkernel methods such as the support vector machine,\nk-nearest neighbor algorithm,\nGaussian mixture model,\nnaive Bayes classifier,\nand decision tree.\nThe performance of these classifiers have been compared over a wide range of tasks. Classifier performance depends greatly on the characteristics of the data to be classified. There is no single classifier that works best on all given problems; this is also referred to as the \"no free lunch\" theorem. Determining a suitable classifier for a given problem is still more an art than science.\n\nNeural networks are modeled after the neurons in the human brain, where a trained algorithm determines an output response for input signals. The study of non-learning artificial neural networks began in the decade before the field of AI research was founded, in the work of Walter Pitts and Warren McCullouch. Frank Rosenblatt invented the perceptron, a learning network with a single layer, similar to the old concept of linear regression. Early pioneers also include Alexey Grigorevich Ivakhnenko, Teuvo Kohonen, Stephen Grossberg, Kunihiko Fukushima, Christoph von der Malsburg, David Willshaw, Shun-Ichi Amari, Bernard Widrow, John Hopfield, Eduardo R. Caianiello, and others.\n\nThe main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks. Neural networks can be applied to the problem of intelligent control (for robotics) or learning, using such techniques as Hebbian learning, GMDH or competitive learning.\n\nToday, neural networks are often trained by the backpropagation algorithm, which had been around since 1970 as the reverse mode of automatic differentiation published by Seppo Linnainmaa, and was introduced to neural networks by Paul Werbos.\n\nHierarchical temporal memory is an approach that models some of the structural and algorithmic properties of the neocortex.\n\nDeep learning in artificial neural networks with many layers has transformed many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing and others.\n\nAccording to a survey, the expression \"Deep Learning\" was introduced to the Machine Learning community by Rina Dechter in 1986 and gained traction after\nIgor Aizenberg and colleagues introduced it to Artificial Neural Networks in 2000. The first functional Deep Learning networks were published by Alexey Grigorevich Ivakhnenko and V. G. Lapa in 1965. These networks are trained one layer at a time. Ivakhnenko's 1971 paper describes the learning of a deep feedforward multilayer perceptron with eight layers, already much deeper than many later networks. In 2006, a publication by Geoffrey Hinton and Ruslan Salakhutdinov introduced another way of pre-training many-layered feedforward neural networks (FNNs) one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then using supervised backpropagation for fine-tuning. Similar to shallow artificial neural networks, deep neural networks can model complex non-linear relationships. Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.\n\nDeep learning often uses convolutional neural networks (CNNs), whose origins can be traced back to the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1989, Yann LeCun and colleagues applied backpropagation to such an architecture. In the early 2000s, in an industrial application CNNs already processed an estimated 10% to 20% of all the checks written in the US.\nSince 2011, fast implementations of CNNs on GPUs have\nwon many visual pattern recognition competitions.\n\nDeep feedforward neural networks were used in conjunction with reinforcement learning by AlphaGo, Google Deepmind's program that was the first to beat a professional human Go player.\n\nEarly on, deep learning was also applied to sequence learning with recurrent neural networks (RNNs) which are general computers and can run arbitrary programs to process arbitrary sequences of inputs. The depth of an RNN is unlimited and depends on the length of its input sequence. RNNs can be trained by gradient descent but suffer from the vanishing gradient problem. In 1992, it was shown that unsupervised pre-training of a stack of recurrent neural networks can speed up subsequent supervised learning of deep sequential problems.\n\nNumerous researchers now use variants of a deep learning recurrent NN called the long short-term memory (LSTM) network published by Hochreiter & Schmidhuber in 1997. LSTM is often trained by Connectionist Temporal Classification (CTC). At Google, Microsoft and Baidu this approach has revolutionised speech recognition. For example, in 2015, Google's speech recognition experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to billions of smartphone users. Google also used LSTM to improve machine translation, Language Modeling and Multilingual Language Processing. LSTM combined with CNNs also improved automatic image captioning and a plethora of other applications.\n\nControl theory, the grandchild of cybernetics, has many important applications, especially in robotics.\n\nAI researchers have developed several specialized languages for AI research, including Lisp, Prolog, Python, and C++.\n\nIn 1950, Alan Turing proposed a general procedure to test the intelligence of an agent now known as the Turing test. This procedure allows almost all the major problems of artificial intelligence to be tested. However, it is a very difficult challenge and at present all agents fail.\n\nArtificial intelligence can also be evaluated on specific problems such as small problems in chemistry, hand-writing recognition and game-playing. Such tests have been termed subject matter expert Turing tests. Smaller problems provide more achievable goals and there are an ever-increasing number of positive results.\n\nFor example, performance at draughts (i.e. checkers) is optimal, performance at chess is high-human and nearing super-human (see computer chess: computers versus human) and performance at many everyday tasks (such as recognizing a face or crossing a room without bumping into something) is sub-human.\n\nA quite different approach measures machine intelligence through tests which are developed from \"mathematical\" definitions of intelligence. Examples of these kinds of tests start in the late nineties devising intelligence tests using notions from Kolmogorov complexity and data compression. Two major advantages of mathematical definitions are their applicability to nonhuman intelligences and their absence of a requirement for human testers.\n\nA derivative of the Turing test is the Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA). As the name implies, this helps to determine that a user is an actual person and not a computer posing as a human. In contrast to the standard Turing test, CAPTCHA is administered by a machine and targeted to a human as opposed to being administered by a human and targeted to a machine. A computer asks a user to complete a simple test then generates a grade for that test. Computers are unable to solve the problem, so correct solutions are deemed to be the result of a person taking the test. A common type of CAPTCHA is the test that requires the typing of distorted letters, numbers or symbols that appear in an image undecipherable by a computer.\n\nAI is relevant to any intellectual task. Modern artificial intelligence techniques are pervasive and are too numerous to list here. Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.\n\nHigh-profile examples of AI include autonomous vehicles (such as drones and self-driving cars), medical diagnosis, creating art (such as poetry), proving mathematical theorems, playing games (such as Chess or Go), search engines (such as Google search), online assistants (such as Siri), image recognition in photographs, spam filtering, prediction of judicial decisions and targeting online advertisements.\n\nWith social media sites overtaking TV as a source for news for young people and news organisations increasingly reliant on social media platforms for generating distribution, major publishers now use artificial intelligence (AI) technology to post stories more effectively and generate higher volumes of traffic.\n\nThere are a number of competitions and prizes to promote research in artificial intelligence. The main areas promoted are: general machine intelligence, conversational behavior, data-mining, robotic cars, robot soccer and games.\n\nArtificial intelligence is breaking into the healthcare industry by assisting doctors. According to Bloomberg Technology, Microsoft has developed AI to help doctors find the right treatments for cancer.  There is a great amount of research and drugs developed relating to cancer. In detail, there are more than 800 medicines and vaccines to treat cancer. This negatively affects the doctors, because there are too many options to choose from, making it more difficult to choose the right drugs for the patients. Microsoft is working on a project to develop a machine called \"Hanover\". Its goal is to memorize all the papers necessary to cancer and help predict which combinations of drugs will be most effective for each patient. One project that is being worked on at the moment is fighting myeloid leukemia, a fatal cancer where the treatment has not improved in decades. Another study was reported to have found that artificial intelligence was as good as trained doctors in identifying skin cancers. Another study is using artificial intelligence to try and monitor multiple high-risk patients, and this is done by asking each patient numerous questions based on data acquired from live doctor to patient interactions.\n\nAccording to CNN, there was a recent study by surgeons at the Children's National Medical Center in Washington which successfully demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel during open surgery, and doing so better than a human surgeon, the team claimed. IBM has created its own artificial intelligence computer, the IBM Watson, which has beaten human intelligence (at some levels). Watson not only won at the game show \"Jeopardy!\" against former champions, but, was declared a hero after successfully diagnosing a women who was suffering from leukemia.\n\nAdvancements in AI have contributed to the growth of the automotive industry through the creation and evolution of self-driving vehicles. As of 2016, there are over 30 companies utilizing AI into the creation of driverless cars. A few companies involved with AI include Tesla, Google, and Apple.\n\nMany components contribute to the functioning of self-driving cars. These vehicles incorporate systems such as braking, lane changing, collision prevention, navigation and mapping. Together, these systems, as well as high performance computers, are integrated into one complex vehicle.\n\nRecent developments in autonomous automobiles have made the innovation of self-driving trucks possible, though they are still in the testing phase. The UK government has passed legislation to begin testing of self-driving truck platoons in 2018. Self-driving truck platoons are a fleet of self-driving trucks following the lead of one non-self-driving truck, so the truck platoons aren't entirely autonomous yet. Meanwhile, the Daimler, a German automobile corporation, is testing the Freightliner Inspiration which is a semi-autonomous truck that will only be used on the highway.\n\nOne main factor that influences the ability for a driver-less automobile to function is mapping. In general, the vehicle would be pre-programmed with a map of the area being driven. This map would include data on the approximations of street light and curb heights in order for the vehicle to be aware of its surroundings. However, Google has been working on an algorithm with the purpose of eliminating the need for pre-programmed maps and instead, creating a device that would be able to adjust to a variety of new surroundings. Some self-driving cars are not equipped with steering wheels or brakes, so there has also been research focused on creating an algorithm that is capable of maintaining a safe environment for the passengers in the vehicle through awareness of speed and driving conditions.\n\nAnother factor that is influencing the ability for a driver-less automobile is the safety of the passenger. To make a driver-less automobile, engineers must program it to handle high risk situations. These situations could include a head on collision with pedestrians. The car's main goal should be to make a decision that would avoid hitting the pedestrians and saving the passengers in the car. But there is a possibility the car would need to make a decision that would put someone in danger. In other words, the car would need to decide to save the pedestrians or the passengers.The programing of the car in these situations is crucial to a successful driver-less automobile.\n\nFinancial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking can be traced back to 1987 when Security Pacific National Bank in USA set-up a Fraud Prevention Task force to counter the unauthorised use of debit cards. Programs like Kasisto and Moneystream are using AI in financial services.\n\nBanks use artificial intelligence systems today to organize operations, maintain book-keeping, invest in stocks, and manage properties. AI can react to changes overnight or when business is not taking place. In August 2001, robots beat humans in a simulated financial trading competition. AI has also reduced fraud and financial crimes by monitoring behavioral patterns of users for any abnormal changes or anomalies.\n\nThe use of AI machines in the market in applications such as online trading and decision making has changed major economic theories. For example, AI based buying and selling platforms have changed the law of supply and demand in that it is now possible to easily estimate individualized demand and supply curves and thus individualized pricing. Furthermore, AI machines reduce information asymmetry in the market and thus making markets more efficient while reducing the volume of trades. Furthermore, AI in the markets limits the consequences of behavior in the markets again making markets more efficient. Other theories where AI has had impact include in rational choice, rational expectations, game theory, Lewis turning point, portfolio optimization and counterfactual thinking.\n\nArtificial intelligence is used to generate intelligent behaviors primarily in non-player characters (NPCs), often simulating human-like intelligence.\n\nA platform (or \"computing platform\") is defined as \"some sort of hardware architecture or software framework (including application frameworks), that allows software to run\". As Rodney Brooks pointed out many years ago, it is not just the artificial intelligence software that defines the AI features of the platform, but rather the actual platform itself that affects the AI that results, i.e., there needs to be work in AI problems on real-world platforms rather than in isolation.\n\nA wide variety of platforms has allowed different aspects of AI to develop, ranging from expert systems such as Cyc to deep-learning frameworks to robot platforms such as the Roomba with open interface. Recent advances in deep artificial neural networks and distributed computing have led to a proliferation of software libraries, including Deeplearning4j, TensorFlow, Theano and Torch.\n\nCollective AI is a platform architecture that combines individual AI into a collective entity, in order to achieve global results from individual behaviors. With its collective structure, developers can crowdsource information and extend the functionality of existing AI domains on the platform for their own use, as well as continue to create and share new domains and capabilities for the wider community and greater good. As developers continue to contribute, the overall platform grows more intelligent and is able to perform more requests, providing a scalable model for greater communal benefit. Organizations like SoundHound Inc. and the Harvard John A. Paulson School of Engineering and Applied Sciences have used this collaborative AI model.\n\nA McKinsey Global Institute study found a shortage of 1.5 million highly trained data and AI professionals and managers and a number of private bootcamps have developed programs to meet that demand, including free programs like The Data Incubator or paid programs like General Assembly.\n\nAmazon, Google, Facebook, IBM, and Microsoft have established a non-profit partnership to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence. They stated: \"This partnership on AI will conduct research, organize discussions, provide thought leadership, consult with relevant third parties, respond to questions from the public and media, and create educational material that advance the understanding of AI technologies including machine perception, learning, and automated reasoning.\" Apple joined other tech companies as a founding member of the Partnership on AI in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.\n\nThere are three philosophical questions related to AI:\n\nCan a machine be intelligent? Can it \"think\"?\n\n\n\n\n\n\n\nWidespread use of artificial intelligence could have unintended consequences that are dangerous or undesirable. Scientists from the Future of Life Institute, among others, described some short-term research goals to be how AI influences the economy, the laws and ethics that are involved with AI and how to minimize AI security risks. In the long-term, the scientists have proposed to continue optimizing function while minimizing possible security risks that come along with new technologies.\n\nMachines with intelligence have the potential to use their intelligence to make ethical decisions. Research in this area includes \"machine ethics\", \"artificial moral agents\", and the study of \"malevolent vs. friendly AI\".\n\nA common concern about the development of artificial intelligence is the potential threat it could pose to humanity. This concern has recently gained attention after mentions by celebrities including Stephen Hawking, Bill Gates, and Elon Musk. A group of prominent tech titans including Peter Thiel, Amazon Web Services and Musk have committed $1billion to OpenAI a nonprofit company aimed at championing responsible AI development. The opinion of experts within the field of artificial intelligence is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI.\n\nIn his book \"\", Nick Bostrom provides an argument that artificial intelligence will pose a threat to mankind. He argues that sufficiently intelligent AI, if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI's goals do not reflect humanity's - one example is an AI told to compute as many digits of pi as possible - it might harm humanity in order to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal.\n\nFor this danger to be realized, the hypothetical AI would have to overpower or out-think all of humanity, which a minority of experts argue is a possibility far enough in the future to not be worth researching. Other counterarguments revolve around humans being either intrinsically or convergently valuable from the perspective of an artificial intelligence.\n\nConcern over risk from artificial intelligence has led to some high-profile donations and investments. In January 2015, Elon Musk donated ten million dollars to the Future of Life Institute to fund research on understanding AI decision making. The goal of the institute is to \"grow wisdom with which we manage\" the growing power of technology. Musk also funds companies developing artificial intelligence such as Google DeepMind and Vicarious to \"just keep an eye on what's going on with artificial intelligence. I think there is potentially a dangerous outcome there.\"\n\nDevelopment of militarized artificial intelligence is a related concern. Currently, 50+ countries are researching battlefield robots, including the United States, China, Russia, and the United Kingdom. Many people concerned about risk from superintelligent AI also want to limit the use of artificial soldiers.\n\nJoseph Weizenbaum wrote that AI applications cannot, by definition, successfully simulate genuine human empathy and that the use of AI technology in fields such as customer service or psychotherapy was deeply misguided. Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum these points suggest that AI research devalues human life.\n\nThe relationship between automation and employment is complicated. While automation eliminates old jobs, it also creates new jobs through micro-economic and macro-economic effects. Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; \"The Economist\" states that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Subjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classifies only 9% of U.S. jobs as \"high risk\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy. Author Martin Ford and others go further and argue that a large number of jobs are routine, repetitive and (to an AI) predictable; Ford warns that these jobs may be automated in the next couple of decades, and that many of the new jobs may not be \"accessible to people with average capability\", even with retraining. Economists point out that in the past technology has tended to increase rather than reduce total employment, but acknowledge that \"we're in uncharted territory\" with AI.\n\nThis raises the issue of how ethically the machine should behave towards both humans and other AI agents. This issue was addressed by Wendell Wallach in his book titled \"Moral Machines\" in which he introduced the concept of artificial moral agents (AMA). For Wallach, AMAs have become a part of the research landscape of artificial intelligence as guided by its two central questions which he identifies as \"Does Humanity Want Computers Making Moral Decisions\" and \"Can (Ro)bots Really Be Moral\". For Wallach the question is not centered on the issue of \"whether\" machines can demonstrate the equivalent of moral behavior in contrast to the \"constraints\" which society may place on the development of AMAs.\n\nThe field of machine ethics is concerned with giving machines ethical principles, or a procedure for discovering a way to resolve the ethical dilemmas they might encounter, enabling them to function in an ethically responsible manner through their own ethical decision making. The field was delineated in the AAAI Fall 2005 Symposium on Machine Ethics: \"Past research concerning the relationship between technology and ethics has largely focused on responsible and irresponsible use of technology by human beings, with a few people being interested in how human beings ought to treat machines. In all cases, only human beings have engaged in ethical reasoning. The time has come for adding an ethical dimension to at least some machines. Recognition of the ethical ramifications of behavior involving machines, as well as recent and potential developments in machine autonomy, necessitate this. In contrast to computer hacking, software property issues, privacy issues and other topics normally ascribed to computer ethics, machine ethics is concerned with the behavior of machines towards human users and other machines. Research in machine ethics is key to alleviating concerns with autonomous systems—it could be argued that the notion of autonomous machines without such a dimension is at the root of all fear concerning machine intelligence. Further, investigation of machine ethics could enable the discovery of problems with current ethical theories, advancing our thinking about Ethics.\" Machine ethics is sometimes referred to as machine morality, computational ethics or computational morality. A variety of perspectives of this nascent field can be found in the collected edition \"Machine Ethics\" that stems from the AAAI Fall 2005 Symposium on Machine Ethics.\n\nPolitical scientist Charles T. Rubin believes that AI can be neither designed nor guaranteed to be benevolent. He argues that \"any sufficiently advanced benevolence may be indistinguishable from malevolence.\" Humans should not assume machines or robots would treat us favorably, because there is no \"a priori\" reason to believe that they would be sympathetic to our system of morality, which has evolved along with our particular biology (which AIs would not share). Hyper-intelligent software may not necessarily decide to support the continued existence of humanity, and would be extremely difficult to stop. This topic has also recently begun to be discussed in academic publications as a real source of risks to civilization, humans, and planet Earth.\n\nPhysicist Stephen Hawking, Microsoft founder Bill Gates, and SpaceX founder Elon Musk have expressed concerns about the possibility that AI could evolve to the point that humans could not control it, with Hawking theorizing that this could \"spell the end of the human race\".\n\nOne proposal to deal with this is to ensure that the first generally intelligent AI is 'Friendly AI', and will then be able to control subsequently developed AIs. Some question whether this kind of check could really remain in place.\n\nLeading AI researcher Rodney Brooks writes, \"I think it is a mistake to be worrying about us developing malevolent AI anytime in the next few hundred years. I think the worry stems from a fundamental error in not distinguishing the difference between the very real recent advances in a particular aspect of AI, and the enormity and complexity of building sentient volitional intelligence.\"\n\nIf an AI system replicates all key aspects of human intelligence, will that system also be sentient – will it have a mind which has conscious experiences? This question is closely related to the philosophical problem as to the nature of human consciousness, generally referred to as the hard problem of consciousness.\n\nComputationalism is the position in the philosophy of mind that the human mind or the human brain (or both) is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.\n\nThe philosophical position that John Searle has named \"strong AI\" states: \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Searle counters this assertion with his Chinese room argument, which asks us to look \"inside\" the computer and try to find where the \"mind\" might be.\n\nMary Shelley's \"Frankenstein\" considers a key issue in the ethics of artificial intelligence: if a machine can be created that has intelligence, could it also \"feel\"? If it can feel, does it have the same rights as a human? The idea also appears in modern science fiction, such as the film \"\", in which humanoid machines have the ability to feel emotions. This issue, now known as \"robot rights\", is currently being considered by, for example, California's Institute for the Future, although many critics believe that the discussion is premature. Some critics of transhumanism argue that any hypothetical robot rights would lie on a spectrum with animal rights and human rights. The subject is profoundly discussed in the 2010 documentary film \"Plug & Pray\".\n\nAre there limits to how intelligent machines – or human-machine hybrids – can be? A superintelligence, hyperintelligence, or superhuman intelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. ‘’Superintelligence’’ may also refer to the form or degree of intelligence possessed by such an agent.\n\nIf research into Strong AI produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement. The new intelligence could thus increase exponentially and dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario \"singularity\". Technological singularity is when accelerating progress in technologies will cause a runaway effect wherein artificial intelligence will exceed human intellectual capacity and control, thus radically changing or even ending civilization. Because the capabilities of such an intelligence may be impossible to comprehend, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.\n\nRay Kurzweil has used Moore's law (which describes the relentless exponential improvement in digital technology) to calculate that desktop computers will have the same processing power as human brains by the year 2029, and predicts that the singularity will occur in 2045.\n\nRobot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, which has roots in Aldous Huxley and Robert Ettinger, has been illustrated in fiction as well, for example in the manga \"Ghost in the Shell\" and the science-fiction series \"Dune\".\n\nIn the 1980s artist Hajime Sorayama's Sexy Robots series were painted and published in Japan depicting the actual organic human form with lifelike muscular metallic skins and later \"the Gynoids\" book followed that was used by or influenced movie makers including George Lucas and other creatives. Sorayama never considered these organic robots to be real part of nature but always unnatural product of the human mind, a fantasy existing in the mind even when realized in actual form.\n\nEdward Fredkin argues that \"artificial intelligence is the next stage in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" (1863), and expanded upon by George Dyson in his book of the same name in 1998.\n\nThought-capable artificial beings appeared as storytelling devices since antiquity.\n\nThe implications of a constructed machine exhibiting artificial intelligence have been a persistent theme in science fiction since the twentieth century. Early stories typically revolved around intelligent robots. The word \"robot\" itself was coined by Karel Čapek in his 1921 play \"R.U.R.\", the title standing for \"Rossum's Universal Robots\". Later, the SF writer Isaac Asimov developed the Three Laws of Robotics. He subsequently explored these in his many books, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during layman discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.\n\nThe novel \"Do Androids Dream of Electric Sheep?\", by Philip K. Dick, tells a science fiction story about Androids and humans clashing in a futuristic world. Elements of artificial intelligence include the empathy box, mood organ, and the androids themselves. Throughout the novel, Dick portrays the idea that human subjectivity is altered by technology created with artificial intelligence.\n\nNowadays AI is firmly rooted in popular culture; intelligent robots appear in innumerable works. HAL 9000, the murderous computer in charge of the \"Discovery One\" spaceship in and \"\" (both 1968), is an example of the common \"robotic rampage\" archetype in science fiction movies. \"The Terminator\" (1984) and \"The Matrix\" (1999) provide additional widely familiar examples. In contrast, the rare loyal robots such as Gort from \"The Day the Earth Stood Still\" (1951) and Bishop from \"Aliens\" (1986) are less prominent in popular culture.\n\n\n\n", "id": "1164", "title": "Artificial intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=55690224", "text": "Msg.ai\n\nmsg.ai is an American artificial intelligence company and developer of human–computer interaction technologies founded in May 2015.\n\nFounded in May 2015 by Puneet Mehta, msg.ai was recruited by the global media and advertising agency, Universal McCann, to assist clients Heinz and BMW. \n\nmsg.ai worked with Sony Pictures to launch the first ever bot on Facebook Messenger for a $100M film, “Goosebumps” and subsequently joined Y Combinator as a member of the Winter 2016 class.\n\nIn 2016 the company received investments from several venture capital firms, including Index Ventures, Y Combinator, Bowery Capital, Salesforce Ventures and private entrepreneurs such as the founders of Google DeepMind and OpenAI.\n\nmsg.ai later partnered with Facebook’s Creative Shop and the Tommy Hilfiger fashion brand to develop a brand-specific bot with the goal of outperforming existing retail shopping bots.\n\nIn 2016 there was an update to the platform to incorporate multivariate testing. This type of testing, unlike traditional A/B testing, permits the monitoring of user interaction with the bot, adjustments to the bot’s tone, and experiments with the use of media.\n\nmsg.ai’s deep reinforcement learning platform allows for conversational AI and chatbots which engage through personalized interactions at scale and one-to-one relationships throughout the entire user experience.\n\nmsg.ai utilizes artificial intelligence to automate customized messages and engage in natural dialogues with deep reinforcement learning. This allows the bot to interact in a conversational manner and in a number of ways, including offering product recommendations based on user preferences, answering questions regarding availability and pricing, guiding customers towards a purchase, and providing assistance to complex issues.\n\nmsg.ai has collaborated with messaging platforms, creative agencies, and technology providers to build conversational AI for brands such as Heinz, BMW, Tommy Hilfiger, Signal, and The Anne Frank House.\n\n", "id": "55690224", "title": "Msg.ai"}
{"url": "https://en.wikipedia.org/wiki?curid=52649487", "text": "CloudSight\n\nCloudSight, Inc. is a Los Angeles, CA based technology company that specializes in image captioning and understanding.\n\nCloudSight was founded in 2012 by Bradford Folkens and Dominik Mazur. It was previously known as Image Searcher, Inc. and then CamFind, Inc., respectively. In 2016, the company was officially rebranded as CloudSight, Inc.\n\nAs of February, 2015, CloudSight has 11 patents pending for its technology. To date, CloudSight has recognized over 700 million images. This dataset becomes invaluable for neural network training and the development of artificial intelligence.\n\nOn October 11, 2012, CloudSight released its first mobile application into the AppStore, TapTapSee.\n\nTapTapSee is a mobile camera application designed specifically for the blind and visually impaired iOS and Android users. Its image recognition capabilities are powered by CloudSight API. The application utilizes the device’s camera and VoiceOver functions to photograph objects and identify them out loud for the user.\n\nTapTapSee was the 2014 recipient of the Access Award by the American Foundation for the Blind. In March 2013, TapTapSee was named App of the Month by the Royal National Institute for the Blind. At the end of 2013, TapTapSee was elected into the AppleVis iOS Hall of Fame.\n\nOn April 7, 2013, CloudSight released its second mobile application into the AppStore, CamFind. The mobile application surpassed 1,000,000 downloads within the first seven months after its release into the Apple AppStore.\n\nCamFind is a visual search engine application that utilizes image recognition to photograph, identify, and provide information on any object, at any angle. Its image recognition capabilities are powered by CloudSight API.\n\nCamFind is available in the Apple AppStore and Google Play Store. In February 2015, CamFind was released on Google Glass via MyGlass.\n\nIn April 2015, CloudSight evolved CamFind a step further by releasing social network capabilities within the application. The app now features the ability for users to share the items they identify, as well as see the items that others are identifying with CamFind.\n\nIn September 2013, CloudSight released its CloudSight API to the general public.\n\n\"The CloudSight API employs deep learning, a technology that simulates the human brain, 'learning' from its mistakes over time, and is the same technology powering CamFind.\"\n", "id": "52649487", "title": "CloudSight"}
{"url": "https://en.wikipedia.org/wiki?curid=55567666", "text": "New Lab\n\nNew Lab opened in June, 2016, as a multi-disciplinary technology center. Housed in Building 128 of the Brooklyn Navy Yard, the $35 million project serves as a hardware-focused shared workspace, research lab, and hatchery for socially-oriented tech manufacturing.\n\nUsing the MIT Media Lab as a model, the impetus for the independent organization was to provide space and services to new manufacturing enterprises. Current members work in fields such as robotics, connected devices, energy, nanotechnology, life sciences, and urban tech.\n\nMedia coverage of New Lab has focused on the company's role in revitalizing the Brooklyn Navy Yard, its public-private partnership lease structure, and Urban Tech initiative with the New York CIty Economic Development Corporation.\n\nDavid Belt and partner Scott Cohen formed the concept for New Lab in 2011 after prospecting the decaying Building 128 with Navy Yard president David Ehrenberg. The partners found the maritime manufacturing history of the structure, specifically the manufacturing innovations that took place there, synchronous with their aim to provide a platform for emerging hardware technologies in New York City. The city was abundant in resources and opportunities for entrepreneurs working in software, Belt said in a recent interview, but space, tools and resources for those working in the new manufacturing hardware community were lacking.\n\nBelt leveraged his development firm, Macro Sea, a company that specializes \"in bringing historic properties back into cultural relevance,\" to obtain funding, architectural expertise, and begin constructing a lease with the city of New York. The Navy Yard was in the initial stages of its current revitalization at the time and, because the city owned the property, special arrangements were needed to develop there.\n\nCohen began scouting the companies who would comprise their core members and helped work to capitalize them. To date, venture capitalists have invested approximately $250 million in New Lab and its members.\n\nThe land predating New Lab has a rich historical cultural lineage and narrative of experimental and innovative breakthroughs. That past was a major factor in the decision to develop New Lab in the Navy Yard. Before colonial settlement, the area that would become the Navy Yard served as a clamming site for the Lenape Native Americans. It was then settled by the Dutch and sold to a developer, thus beginning its employment as a center for manufacturing. Among the technological advancements that took place at the Navy Yard are: the first use of the steam-powered pile driver; construction of the first undersea telegraph cable; development of a commercialized form of anesthetic ether by E.R. Squibb; and a broadcast of the first woman to sing over the radio, opera singer Eugenia Farrar performed \"I Love You Truly.\"\n\nConstruction of navy ships like the Fulton II, a first-of-its-kind steam-powered warship, and fabrication of the USS Arizona, state-of-the-art among its peers, induced many influential manufacturing process refinements and advancements.\n\nIn interviews, Belt and Cohen both cite this maritime and technological history as inspiration for New Lab, both in guiding the renovation of the facility and in shaping its mission.\n\nAccording to a Brooklyn Navy Yard Development Corporation document,128 was raised in 1899 as a \"steel structure... used to assemble large boiler engines and fabricated sections of naval vessels.\" It served as the primary machine shop for every major ship launched during World Wars One and Two. Designed to accommodate the significant height of a warship, the sequence of its hulking steel girders resembles an airport hangar. 128 has been slated for, but avoided, plans for non-naval readaptation. The City of New York sought to adapt it for reuse as a \"food complex at one point,\" but the effort was not sustained.\n\nMarvel Architects, New Lab's architect of record, along with DBI, Belt's project management firm, worked together to craft and execute the renovation. Press regarding New Lab often states that the company occupies Building 128 of the Navy Yard, but this is slightly misleading in that 128 is a complex of warehouses and New Lab occupies the southernmost portion.\n\nRecladding the building's armature and repurposing of the 51,000 ft machine shop into an 84,000 ft multidisciplinary design, prototyping, and advanced manufacturing space took approximately 5 years and continued until the company's full opening in September 2016. The undertaking utilized approximately 9,000 lbs of steel in total according to the developer.\n\nA guiding principle of the redesign was to harmonize of the needs of the forthcoming lab environment with the original structural features. Modern workplace design elements were fused with the 19th century industrial characteristics of the building's centerline:\n\nNicko Elliott, Macro Sea's design director, and other members of the development firm's team collaborated to design custom or retrofitted furniture for the space. Elliott told The Commercial Observer that the Danish midcentury chairs employed throughout the open areas were purchased at auction in Chicago. They were then reupholstered in yellow, green, and red fabrics. The desks were also made custom and paired with Herman Miller Eames chairs that New Lab co-founder Scott Cohen discovered in some Soho trash outside the Scholastic Building. Cohen phoned Belt and, in their typical hands-on fashion, the two rented a truck and adopted the seats for New Lab.\n\nNew Lab's open floor design was intended, spatially, to reinforce its mission, the layout meant to encourage member companies to collaborate and cross-pollinate ideas. Communal meeting rooms, office pods, and interior plazas on both floors emphasize the developer’s intention to create a collaborative design and fabrication center.\n\nUpon completion, the rebuild subdivided Building 128's usable space into: Private studios = 31,664 ft; Open private studios = 6,226 ft; Fabrication lab = 6,834 ft; Cafe kitchen = 600 ft; Conference rooms = 2,014 ft; Coworking desks = 144; Flex space = 66 desks.\n\nThere is an additional 6,174 ft of event space which hosts talks, hackathons, and new manufacturing events such as the recent Urban Tech Hub launch.\n\nAdditive Manufacturing (3D printing) technology is a component of the design process for many New Lab residents. Prototyping shops are a distinguishing feature of the hardware-centric facility. New Lab leverages partnerships with firms like AutoDesk, Stratasys, BigRep, Haas, and others to provide and maintain equipment. The organization has amassed several million dollars of digital fabrication and manufacturing machine assets such as 3D Printers, electronic workbenches, fabrication tools, and CNC equipment since its opening.\n\n\n\nEighty companies and 400 people currently work at New Lab. Members are typically growth-stage companies with anywhere between 3-20 employees. While New Lab's membership model includes open one-year agreements with public enterprises and individuals seeking shared and private studios or reserved workspaces, the organization gives preference to hardware-focused companies with positive, socially oriented goals. \"We want to make sure that they have a level of optimism and humanism in the work that they're doing so that they're making the world better,\" Belt stated in a recent profile. The core members, who occupy the larger spaces in the building, were selected and recruited by the company for their intellectual property, visibility, and potential to scale.\n\n", "id": "55567666", "title": "New Lab"}
{"url": "https://en.wikipedia.org/wiki?curid=11924", "text": "Game theory\n\nGame theory is \"the study of mathematical models of conflict and cooperation between intelligent rational decision-makers\". Game theory is mainly used in economics, political science, and psychology, as well as logic, computer science and biology. Originally, it addressed zero-sum games, in which one person's gains result in losses for the other participants. Today, game theory applies to a wide range of behavioral relations, and is now an umbrella term for the science of logical decision making in humans, animals, and computers.\n\nModern game theory began with the idea regarding the existence of mixed-strategy equilibria in two-person zero-sum games and its proof by John von Neumann. Von Neumann's original proof used the Brouwer fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by the 1944 book \"Theory of Games and Economic Behavior\", co-written with Oskar Morgenstern, which considered cooperative games of several players. The second edition of this book provided an axiomatic theory of expected utility, which allowed mathematical statisticians and economists to treat decision-making under uncertainty.\n\nThis theory was developed extensively in the 1950s by many scholars. Game theory was later explicitly applied to biology in the 1970s, although similar developments go back at least as far as the 1930s. Game theory has been widely recognized as an important tool in many fields. With the Nobel Memorial Prize in Economic Sciences going to game theorist Jean Tirole in 2014, eleven game-theorists have now won the economics Nobel Prize. John Maynard Smith was awarded the Crafoord Prize for his application of game theory to biology.\n\nEarly discussions of examples of two-person games occurred long before the rise of modern, mathematical game theory. The first known discussion of game theory occurred in a letter written by Charles Waldegrave, an active Jacobite, and uncle to James Waldegrave, a British diplomat, in 1713. In this letter, Waldegrave provides a minimax mixed strategy solution to a two-person version of the card game le Her, and the problem is now known as Waldegrave problem. James Madison made what we now recognize as a game-theoretic analysis of the ways states can be expected to behave under different systems of taxation. In his 1838 \"Recherches sur les principes mathématiques de la théorie des richesses\" (\"Researches into the Mathematical Principles of the Theory of Wealth\"), Antoine Augustin Cournot considered a duopoly and presents a solution that is a restricted version of the Nash equilibrium.\n\nIn 1913, Ernst Zermelo published \"Über eine Anwendung der Mengenlehre auf die Theorie des Schachspiels\" (\"On an Application of Set Theory to the Theory of the Game of Chess\"). It proved that the optimal chess strategy is strictly determined. This paved the way for more general theorems.\n\nIn 1938, the Danish mathematical economist Frederik Zeuthen proved that the mathematical model had a winning strategy by using Brouwer's fixed point theorem. In his 1938 book \"Applications aux Jeux de Hasard\" and earlier notes, Émile Borel proved a minimax theorem for two-person zero-sum matrix games only when the pay-off matrix was symmetric. Borel conjectured that non-existence of mixed-strategy equilibria in two-person zero-sum games would occur, a conjecture that was proved false.\n\nGame theory did not really exist as a unique field until John von Neumann published a paper in 1928. Von Neumann's original proof used Brouwer's fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by his 1944 book \"Theory of Games and Economic Behavior\" co-authored with Oskar Morgenstern. The second edition of this book provided an axiomatic theory of utility, which reincarnated Daniel Bernoulli's old theory of utility (of the money) as an independent discipline. Von Neumann's work in game theory culminated in this 1944 book. This foundational work contains the method for finding mutually consistent solutions for two-person zero-sum games. During the following time period, work on game theory was primarily focused on cooperative game theory, which analyzes optimal strategies for groups of individuals, presuming that they can enforce agreements between them about proper strategies.\n\nIn 1950, the first mathematical discussion of the prisoner's dilemma appeared, and an experiment was undertaken by notable mathematicians Merrill M. Flood and Melvin Dresher, as part of the RAND Corporation's investigations into game theory. RAND pursued the studies because of possible applications to global nuclear strategy. Around this same time, John Nash developed a criterion for mutual consistency of players' strategies, known as Nash equilibrium, applicable to a wider variety of games than the criterion proposed by von Neumann and Morgenstern. Nash proved that every n-player, non-zero-sum (not just 2-player zero-sum) non-cooperative game has what is now known as a Nash equilibrium.\n\nGame theory experienced a flurry of activity in the 1950s, during which time the concepts of the core, the extensive form game, fictitious play, repeated games, and the Shapley value were developed. In addition, the first applications of game theory to philosophy and political science occurred during this time.\n\nIn 1965, Reinhard Selten introduced his solution concept of subgame perfect equilibria, which further refined the Nash equilibrium (later he would introduce trembling hand perfection as well). In 1994 Nash, Selten and Harsanyi became Economics Nobel Laureates for their contributions to economic game theory.\n\nIn the 1970s, game theory was extensively applied in biology, largely as a result of the work of John Maynard Smith and his evolutionarily stable strategy. In addition, the concepts of correlated equilibrium, trembling hand perfection, and common knowledge were introduced and analyzed.\n\nIn 2005, game theorists Thomas Schelling and Robert Aumann followed Nash, Selten and Harsanyi as Nobel Laureates. Schelling worked on dynamic models, early examples of evolutionary game theory. Aumann contributed more to the equilibrium school, introducing an equilibrium coarsening, correlated equilibrium, and developing an extensive formal analysis of the assumption of common knowledge and of its consequences.\n\nIn 2007, Leonid Hurwicz, together with Eric Maskin and Roger Myerson, was awarded the Nobel Prize in Economics \"for having laid the foundations of mechanism design theory\". Myerson's contributions include the notion of proper equilibrium, and an important graduate text: \"Game Theory, Analysis of Conflict\". Hurwicz introduced and formalized the concept of incentive compatibility.\n\nIn 2012, Alvin E. Roth and Lloyd S. Shapley were awarded the Nobel Prize in Economics \"for the theory of stable allocations and the practice of market design\" and, in 2014, the Nobel went to game theorist Jean Tirole.\n\nA game is \"cooperative\" if the players are able to form binding commitments externally enforced (e.g. through contract law). A game is \"non-cooperative\" if players cannot form alliances or if all agreements need to be self-enforcing (e.g. through credible threats).\n\nCooperative games are often analysed through the framework of \"cooperative game theory\", which focuses on predicting which coalitions will form, the joint actions that groups take and the resulting collective payoffs. It is opposed to the traditional \"non-cooperative game theory\" which focuses on predicting individual players' actions and payoffs and analyzing Nash equilibria.\n\nCooperative game theory provides a high-level approach as it only describes the structure, strategies and payoffs of coalitions, whereas non-cooperative game theory also looks at how bargaining procedures will affect the distribution of payoffs within each coalition. As non-cooperative game theory is more general, cooperative games can be analyzed through the approach of non-cooperative game theory (the converse does not hold) provided that sufficient assumptions are made to encompass all the possible strategies available to players due to the possibility of external enforcement of cooperation. While it would thus be optimal to have all games expressed under a non-cooperative framework, in many instances insufficient information is available to accurately model the formal procedures available to the players during the strategic bargaining process, or the resulting model would be of too high complexity to offer a practical tool in the real world. In such cases, cooperative game theory provides a simplified approach that allows to analyze the game at large without having to make any assumption about bargaining powers.\n\nA symmetric game is a game where the payoffs for playing a particular strategy depend only on the other strategies employed, not on who is playing them. If the identities of the players can be changed without changing the payoff to the strategies, then a game is symmetric. Many of the commonly studied 2×2 games are symmetric. The standard representations of chicken, the prisoner's dilemma, and the stag hunt are all symmetric games. Some scholars would consider certain asymmetric games as examples of these games as well. However, the most common payoffs for each of these games are symmetric.\n\nMost commonly studied asymmetric games are games where there are not identical strategy sets for both players. For instance, the ultimatum game and similarly the dictator game have different strategies for each player. It is possible, however, for a game to have identical strategies for both players, yet be asymmetric. For example, the game pictured to the right is asymmetric despite having identical strategy sets for both players.\n\nZero-sum games are a special case of constant-sum games, in which choices by players can neither increase nor decrease the available resources. In zero-sum games the total benefit to all players in the game, for every combination of strategies, always adds to zero (more informally, a player benefits only at the equal expense of others). Poker exemplifies a zero-sum game (ignoring the possibility of the house's cut), because one wins exactly the amount one's opponents lose. Other zero-sum games include matching pennies and most classical board games including Go and chess.\n\nMany games studied by game theorists (including the famed prisoner's dilemma) are non-zero-sum games, because the outcome has net results greater or less than zero. Informally, in non-zero-sum games, a gain by one player does not necessarily correspond with a loss by another.\n\nConstant-sum games correspond to activities like theft and gambling, but not to the fundamental economic situation in which there are potential gains from trade. It is possible to transform any game into a (possibly asymmetric) zero-sum game by adding a dummy player (often called \"the board\") whose losses compensate the players' net winnings.\n\nSimultaneous games are games where both players move simultaneously, or if they do not move simultaneously, the later players are unaware of the earlier players' actions (making them \"effectively\" simultaneous). Sequential games (or dynamic games) are games where later players have some knowledge about earlier actions. This need not be perfect information about every action of earlier players; it might be very little knowledge. For instance, a player may know that an earlier player did not perform one particular action, while he does not know which of the other available actions the first player actually performed.\n\nThe difference between simultaneous and sequential games is captured in the different representations discussed above. Often, normal form is used to represent simultaneous games, while extensive form is used to represent sequential ones. The transformation of extensive to normal form is one way, meaning that multiple extensive form games correspond to the same normal form. Consequently, notions of equilibrium for simultaneous games are insufficient for reasoning about sequential games; see subgame perfection.\n\nIn short, the differences between sequential and simultaneous games are as follows:\n\nAn important subset of sequential games consists of games of perfect information. A game is one of perfect information if all players know the moves previously made by all other players. Most games studied in game theory are imperfect-information games. Examples of perfect-information games include tic-tac-toe, checkers, infinite chess, and Go.\n\nMany card games are games of imperfect information, such as poker and bridge. Perfect information is often confused with complete information, which is a similar concept. Complete information requires that every player know the strategies and payoffs available to the other players but not necessarily the actions taken. Games of incomplete information can be reduced, however, to games of imperfect information by introducing \"moves by nature\".\n\nGames in which the difficulty of finding an optimal strategy stems from the multiplicity of possible moves are called combinatorial games. Examples include chess and go. Games that involve imperfect information may also have a strong combinatorial character, for instance backgammon. There is no unified theory addressing combinatorial elements in games. There are, however, mathematical tools that can solve particular problems and answer general questions.\n\nGames of perfect information have been studied in combinatorial game theory, which has developed novel representations, e.g. surreal numbers, as well as combinatorial and algebraic (and sometimes non-constructive) proof methods to solve games of certain types, including \"loopy\" games that may result in infinitely long sequences of moves. These methods address games with higher combinatorial complexity than those usually considered in traditional (or \"economic\") game theory. A typical game that has been solved this way is hex. A related field of study, drawing from computational complexity theory, is game complexity, which is concerned with estimating the computational difficulty of finding optimal strategies.\n\nResearch in artificial intelligence has addressed both perfect and imperfect information games that have very complex combinatorial structures (like chess, go, or backgammon) for which no provable optimal strategies have been found. The practical solutions involve computational heuristics, like alpha-beta pruning or use of artificial neural networks trained by reinforcement learning, which make games more tractable in computing practice.\n\nGames, as studied by economists and real-world game players, are generally finished in finitely many moves. Pure mathematicians are not so constrained, and set theorists in particular study games that last for infinitely many moves, with the winner (or other payoff) not known until \"after\" all those moves are completed.\n\nThe focus of attention is usually not so much on the best way to play such a game, but whether one player has a winning strategy. (It can be proven, using the axiom of choice, that there are gameseven with perfect information and where the only outcomes are \"win\" or \"lose\"for which \"neither\" player has a winning strategy.) The existence of such strategies, for cleverly designed games, has important consequences in descriptive set theory.\n\nMuch of game theory is concerned with finite, discrete games, that have a finite number of players, moves, events, outcomes, etc. Many concepts can be extended, however. Continuous games allow players to choose a strategy from a continuous strategy set. For instance, Cournot competition is typically modeled with players' strategies being any non-negative quantities, including fractional quantities.\n\nDifferential games such as the continuous pursuit and evasion game are continuous games where the evolution of the players' state variables is governed by differential equations. The problem of finding an optimal strategy in a differential game is closely related to the optimal control theory. In particular, there are two types of strategies: the open-loop strategies are found using the Pontryagin maximum principle while the closed-loop strategies are found using Bellman's Dynamic Programming method.\n\nA particular case of differential games are the games with a random time horizon. In such games, the terminal time is a random variable with a given probability distribution function. Therefore, the players maximize the mathematical expectation of the cost function. It was shown that the modified optimization problem can be reformulated as a discounted differential game over an infinite time interval.\n\nGames with an arbitrary, but finite, number of players are often called n-person games. Evolutionary game theory considers games involving a population of decision makers, where the frequency with which a particular decision is made can change over time in response to the decisions made by all individuals in the population. In biology, this is intended to model (biological) evolution, where genetically programmed organisms pass along some of their strategy programming to their offspring. In economics, the same theory is intended to capture population changes because people play the game many times within their lifetime, and consciously (and perhaps rationally) switch strategies.\n\nIndividual decision problems with stochastic outcomes are sometimes considered \"one-player games\". These situations are not considered game theoretical by some authors. They may be modeled using similar tools within the related disciplines of decision theory, operations research, and areas of artificial intelligence, particularly AI planning (with uncertainty) and multi-agent system. Although these fields may have different motivators, the mathematics involved are substantially the same, e.g. using Markov decision processes (MDP).\n\nStochastic outcomes can also be modeled in terms of game theory by adding a randomly acting player who makes \"chance moves\" (\"moves by nature\"). This player is not typically considered a third player in what is otherwise a two-player game, but merely serves to provide a roll of the dice where required by the game.\n\nFor some problems, different approaches to modeling stochastic outcomes may lead to different solutions. For example, the difference in approach between MDPs and the minimax solution is that the latter considers the worst-case over a set of adversarial moves, rather than reasoning in expectation about these moves given a fixed probability distribution. The minimax approach may be advantageous where stochastic models of uncertainty are not available, but may also be overestimating extremely unlikely (but costly) events, dramatically swaying the strategy in such scenarios if it is assumed that an adversary can force such an event to happen. (See Black swan theory for more discussion on this kind of modeling issue, particularly as it relates to predicting and limiting losses in investment banking.)\n\nGeneral models that include all elements of stochastic outcomes, adversaries, and partial or noisy observability (of moves by other players) have also been studied. The \"gold standard\" is considered to be partially observable stochastic game (POSG), but few realistic problems are computationally feasible in POSG representation.\n\nThese are games the play of which is the development of the rules for another game, the target or subject game. Metagames seek to maximize the utility value of the rule set developed. The theory of metagames is related to mechanism design theory.\n\nThe term metagame analysis is also used to refer to a practical approach developed by Nigel Howard. whereby a situation is framed as a strategic game in which stakeholders try to realise their objectives by means of the options available to them. Subsequent developments have led to the formulation of confrontation analysis.\n\nThese are games prevailing over all forms of society. Pooling games are repeated plays with changing payoff table in general over an experienced path and their equilibrium strategies usually take a form of evolutionary social convention and economic convention. Pooling game theory emerges to formally recognize the interaction between optimal choice in one play and the emergence of forthcoming payoff table update path, identify the invariance existence and robustness, and predict variance over time. The theory is based upon topological transformation classification of payoff table update over time to predict variance and invariance, and is also within the jurisdiction of the computational law of reachable optimality for ordered system.\n\nMean field game theory is the study of strategic decision making in very large populations of small interacting agents. This class of problems was considered in the economics literature by Boyan Jovanovic and Robert W. Rosenthal, in the engineering literature by Peter E. Caines and by mathematician Pierre-Louis Lions and Jean-Michel Lasry.\n\nThe games studied in game theory are well-defined mathematical objects. To be fully defined, a game must specify the following elements: the \"players\" of the game, the \"information\" and \"actions\" available to each player at each decision point, and the \"payoffs\" for each outcome. (Eric Rasmusen refers to these four \"essential elements\" by the acronym \"PAPI\".) A game theorist typically uses these elements, along with a solution concept of their choosing, to deduce a set of equilibrium strategies for each player such that, when these strategies are employed, no player can profit by unilaterally deviating from their strategy. These equilibrium strategies determine an equilibrium to the game—a stable state in which either one outcome occurs or a set of outcomes occur with known probability.\n\nMost cooperative games are presented in the characteristic function form, while the extensive and the normal forms are used to define noncooperative games.\n\nThe extensive form can be used to formalize games with a time sequencing of moves. Games here are played on trees (as pictured here). Here each vertex (or node) represents a point of choice for a player. The player is specified by a number listed by the vertex. The lines out of the vertex represent a possible action for that player. The payoffs are specified at the bottom of the tree. The extensive form can be viewed as a multi-player generalization of a decision tree. To solve any extensive form game, backward induction must be used. It involves working backwards up the game tree to determine what a rational player would do at the last vertex of the tree, what the player with the previous move would do given that the player with the last move is rational, and so on until the first vertex of the tree is reached.\n\nThe game pictured consists of two players. The way this particular game is structured (i.e., with sequential decision making and perfect information), \"Player 1\" \"moves\" first by choosing either \"F\" or \"U\" (Fair or Unfair). Next in the sequence, \"Player 2\", who has now seen \"Player 1\"s move, chooses to play either \"A\" or \"R\". Once \"Player 2\" has made his/ her choice, the game is considered finished and each player gets their respective payoff. Suppose that \"Player 1\" chooses \"U\" and then \"Player 2\" chooses \"A\": \"Player 1\" then gets a payoff of \"eight\" (which in real-world terms can be interpreted in many ways, the simplest of which is in terms of money but could mean things such as eight days of vacation or eight countries conquered or even eight more opportunities to play the same game against other players) and \"Player 2\" gets a payoff of \"two\".\n\nThe extensive form can also capture simultaneous-move games and games with imperfect information. To represent it, either a dotted line connects different vertices to represent them as being part of the same information set (i.e. the players do not know at which point they are), or a closed line is drawn around them. (See example in the imperfect information section.)\n\nThe normal (or strategic form) game is usually represented by a matrix which shows the players, strategies, and payoffs (see the example to the right). More generally it can be represented by any function that associates a payoff for each player with every possible combination of actions. In the accompanying example there are two players; one chooses the row and the other chooses the column. Each player has two strategies, which are specified by the number of rows and the number of columns. The payoffs are provided in the interior. The first number is the payoff received by the row player (Player 1 in our example); the second is the payoff for the column player (Player 2 in our example). Suppose that Player 1 plays \"Up\" and that Player 2 plays \"Left\". Then Player 1 gets a payoff of 4, and Player 2 gets 3.\n\nWhen a game is presented in normal form, it is presumed that each player acts simultaneously or, at least, without knowing the actions of the other. If players have some information about the choices of other players, the game is usually presented in extensive form.\n\nEvery extensive-form game has an equivalent normal-form game, however the transformation to normal form may result in an exponential blowup in the size of the representation, making it computationally impractical.\n\nIn games that possess removable utility, separate rewards are not given; rather, the characteristic function decides the payoff of each unity. The idea is that the unity that is 'empty', so to speak, does not receive a reward at all.\n\nThe origin of this form is to be found in John von Neumann and Oskar Morgenstern's book; when looking at these instances, they guessed that when a union formula_1 appears, it works against the fraction\nformula_2\nas if two individuals were playing a normal game. The balanced payoff of C is a basic function. Although there are differing examples that help determine coalitional amounts from normal games, not all appear that in their function form can be derived from such.\n\nFormally, a characteristic function is seen as: (N,v), where N represents the group of people and formula_3 is a normal utility.\n\nSuch characteristic functions have expanded to describe games where there is no removable utility.\n\nAs a method of applied mathematics, game theory has been used to study a wide variety of human and animal behaviors. It was initially developed in economics to understand a large collection of economic behaviors, including behaviors of firms, markets, and consumers. The first use of game-theoretic analysis was by Antoine Augustin Cournot in 1838 with his solution of the Cournot duopoly. The use of game theory in the social sciences has expanded, and game theory has been applied to political, sociological, and psychological behaviors as well.\n\nAlthough pre-twentieth century naturalists such as Charles Darwin made game-theoretic kinds of statements, the use of game-theoretic analysis in biology began with Ronald Fisher's studies of animal behavior during the 1930s. This work predates the name \"game theory\", but it shares many important features with this field. The developments in economics were later applied to biology largely by John Maynard Smith in his book \"Evolution and the Theory of Games\".\n\nIn addition to being used to describe, predict, and explain behavior, game theory has also been used to develop theories of ethical or normative behavior and to prescribe such behavior. In economics and philosophy, scholars have applied game theory to help in the understanding of good or proper behavior. Game-theoretic arguments of this type can be found as far back as Plato.\n\nThe primary use of game theory is to describe and model how human populations behave. Some scholars believe that by finding the equilibria of games they can predict how actual human populations will behave when confronted with situations analogous to the game being studied. This particular view of game theory has been criticized. It is argued that the assumptions made by game theorists are often violated when applied to real world situations. Game theorists usually assume players act rationally, but in practice, human behavior often deviates from this model. Game theorists respond by comparing their assumptions to those used in physics. Thus while their assumptions do not always hold, they can treat game theory as a reasonable scientific ideal akin to the models used by physicists. However, empirical work has shown that in some classic games, such as the centipede game, guess 2/3 of the average game, and the dictator game, people regularly do not play Nash equilibria. There is an ongoing debate regarding the importance of these experiments and whether the analysis of the experiments fully captures all aspects of the relevant situation.\n\nSome game theorists, following the work of John Maynard Smith and George R. Price, have turned to evolutionary game theory in order to resolve these issues. These models presume either no rationality or bounded rationality on the part of players. Despite the name, evolutionary game theory does not necessarily presume natural selection in the biological sense. Evolutionary game theory includes both biological as well as cultural evolution and also models of individual learning (for example, fictitious play dynamics).\n\nSome scholars, like Leonard Savage, see game theory not as a predictive tool for the behavior of human beings, but as a suggestion for how people ought to behave. Since a strategy, corresponding to a Nash equilibrium of a game constitutes one's best response to the actions of the other players – provided they are in (the same) Nash equilibrium – playing a strategy that is part of a Nash equilibrium seems appropriate. This normative use of game theory has also come under criticism.\n\nGame theory is a major method used in mathematical economics and business for modeling competing behaviors of interacting agents. Applications include a wide array of economic phenomena and approaches, such as auctions, bargaining, mergers & acquisitions pricing, fair division, duopolies, oligopolies, social network formation, agent-based computational economics, general equilibrium, mechanism design, and voting systems; and across such broad areas as experimental economics, behavioral economics, information economics, industrial organization, and political economy.\n\nThis research usually focuses on particular sets of strategies known as \"solution concepts\" or \"equilibria\". A common assumption is that players act rationally. In non-cooperative games, the most famous of these is the Nash equilibrium. A set of strategies is a Nash equilibrium if each represents a best response to the other strategies. If all the players are playing the strategies in a Nash equilibrium, they have no unilateral incentive to deviate, since their strategy is the best they can do given what others are doing.\n\nThe payoffs of the game are generally taken to represent the utility of individual players.\n\nA prototypical paper on game theory in economics begins by presenting a game that is an abstraction of a particular economic situation. One or more solution concepts are chosen, and the author demonstrates which strategy sets in the presented game are equilibria of the appropriate type. Naturally one might wonder to what use this information should be put. Economists and business professors suggest two primary uses (noted above): \"descriptive\" and \"prescriptive\".\n\nThe application of game theory to political science is focused in the overlapping areas of fair division, political economy, public choice, war bargaining, positive political theory, and social choice theory. In each of these areas, researchers have developed game-theoretic models in which the players are often voters, states, special interest groups, and politicians.\n\nEarly examples of game theory applied to political science are provided by Anthony Downs. In his book \"An Economic Theory of Democracy\", he applies the Hotelling firm location model to the political process. In the Downsian model, political candidates commit to ideologies on a one-dimensional policy space. Downs first shows how the political candidates will converge to the ideology preferred by the median voter if voters are fully informed, but then argues that voters choose to remain rationally ignorant which allows for candidate divergence. Game Theory was applied in 1962 to the Cuban missile crisis during the presidency of John F. Kennedy.\n\nIt has also been proposed that game theory explains the stability of any form of political government. Taking the simplest case of a monarchy, for example, the king, being only one person, does not and cannot maintain his authority by personally exercising physical control over all or even any significant number of his subjects. Sovereign control is instead explained by the recognition by each citizen that all other citizens expect each other to view the king (or other established government) as the person whose orders will be followed. Coordinating communication among citizens to replace the sovereign is effectively barred, since conspiracy to replace the sovereign is generally punishable as a crime. Thus, in a process that can be modeled by variants of the prisoner's dilemma, during periods of stability no citizen will find it rational to move to replace the sovereign, even if all the citizens know they would be better off if they were all to act collectively.\n\nA game-theoretic explanation for democratic peace is that public and open debate in democracies send clear and reliable information regarding their intentions to other states. In contrast, it is difficult to know the intentions of nondemocratic leaders, what effect concessions will have, and if promises will be kept. Thus there will be mistrust and unwillingness to make concessions if at least one of the parties in a dispute is a non-democracy.\n\nOn the other hand, game theory predicts that two countries may still go to war even if their leaders are cognizant of the costs of fighting. War may result from asymmetric information; two countries may have incentives to mis-represent the amount of military resources they have on hand, rendering them unable to settle disputes agreeably without resorting to fighting. Moreover, war may arise because of commitment problems: if two countries wish to settle a dispute via peaceful means, but each wishes to go back on the terms of that settlement, they may have no choice but to resort to warfare. Finally, war may result from issue indivisibilities.\n\nGame theory could also help predict a nation's responses when there is a new rule or law to be applied to that nation. One example would be Peter John Wood's (2013) research when he looked into what nations could do to help reduce climate change. Wood thought this could be accomplished by making treaties with other nations to reduce green house gas emissions. However, he concluded that this idea could not work because it would create a prisoner's dilemma to the nations.\n\nUnlike those in economics, the payoffs for games in biology are often interpreted as corresponding to fitness. In addition, the focus has been less on equilibria that correspond to a notion of rationality and more on ones that would be maintained by evolutionary forces. The best known equilibrium in biology is known as the \"evolutionarily stable strategy\" (ESS), first introduced in . Although its initial motivation did not involve any of the mental requirements of the Nash equilibrium, every ESS is a Nash equilibrium.\n\nIn biology, game theory has been used as a model to understand many different phenomena. It was first used to explain the evolution (and stability) of the approximate 1:1 sex ratios. suggested that the 1:1 sex ratios are a result of evolutionary forces acting on individuals who could be seen as trying to maximize their number of grandchildren.\n\nAdditionally, biologists have used evolutionary game theory and the ESS to explain the emergence of animal communication. The analysis of signaling games and other communication games has provided insight into the evolution of communication among animals. For example, the mobbing behavior of many species, in which a large number of prey animals attack a larger predator, seems to be an example of spontaneous emergent organization. Ants have also been shown to exhibit feed-forward behavior akin to fashion (see Paul Ormerod's \"Butterfly Economics\").\n\nBiologists have used the game of chicken to analyze fighting behavior and territoriality.\n\nAccording to Maynard Smith, in the preface to \"Evolution and the Theory of Games\", \"paradoxically, it has turned out that game theory is more readily applied to biology than to the field of economic behaviour for which it was originally designed\". Evolutionary game theory has been used to explain many seemingly incongruous phenomena in nature.\n\nOne such phenomenon is known as biological altruism. This is a situation in which an organism appears to act in a way that benefits other organisms and is detrimental to itself. This is distinct from traditional notions of altruism because such actions are not conscious, but appear to be evolutionary adaptations to increase overall fitness. Examples can be found in species ranging from vampire bats that regurgitate blood they have obtained from a night's hunting and give it to group members who have failed to feed, to worker bees that care for the queen bee for their entire lives and never mate, to vervet monkeys that warn group members of a predator's approach, even when it endangers that individual's chance of survival. All of these actions increase the overall fitness of a group, but occur at a cost to the individual.\n\nEvolutionary game theory explains this altruism with the idea of kin selection. Altruists discriminate between the individuals they help and favor relatives. Hamilton's rule explains the evolutionary rationale behind this selection with the equation c The coefficient values depend heavily on the scope of the playing field; for example if the choice of whom to favor includes all genetic living things, not just all relatives, we assume the discrepancy between all humans only accounts for approximately 1% of the diversity in the playing field, a co-efficient that was ½ in the smaller field becomes 0.995. Similarly if it is considered that information other than that of a genetic nature (e.g. epigenetics, religion, science, etc.) persisted through time the playing field becomes larger still, and the discrepancies smaller.\n\nGame theory has come to play an increasingly important role in logic and in computer science. Several logical theories have a basis in game semantics. In addition, computer scientists have used games to model interactive computations. Also, game theory provides a theoretical basis to the field of multi-agent systems.\n\nSeparately, game theory has played a role in online algorithms; in particular, the k-server problem, which has in the past been referred to as \"games with moving costs\" and \"request-answer games\". Yao's principle is a game-theoretic technique for proving lower bounds on the computational complexity of randomized algorithms, especially online algorithms.\n\nThe emergence of the internet has motivated the development of algorithms for finding equilibria in games, markets, computational auctions, peer-to-peer systems, and security and information markets. Algorithmic game theory and within it algorithmic mechanism design combine computational algorithm design and analysis of complex systems with economic theory.\n\nGame theory has been put to several uses in philosophy. Responding to two papers by , used game theory to develop a philosophical account of convention. In so doing, he provided the first analysis of common knowledge and employed it in analyzing play in coordination games. In addition, he first suggested that one can understand meaning in terms of signaling games. This later suggestion has been pursued by several philosophers since Lewis. Following game-theoretic account of conventions, Edna Ullmann-Margalit (1977) and Bicchieri (2006) have developed theories of social norms that define them as Nash equilibria that result from transforming a mixed-motive game into a coordination game.\n\nGame theory has also challenged philosophers to think in terms of interactive epistemology: what it means for a collective to have common beliefs or knowledge, and what are the consequences of this knowledge for the social outcomes resulting from agents' interactions. Philosophers who have worked in this area include Bicchieri (1989, 1993), Skyrms (1990), and Stalnaker (1999).\n\nIn ethics, some authors have attempted to pursue Thomas Hobbes' project of deriving morality from self-interest. Since games like the prisoner's dilemma present an apparent conflict between morality and self-interest, explaining why cooperation is required by self-interest is an important component of this project. This general strategy is a component of the general social contract view in political philosophy (for examples, see and ).\n\nOther authors have attempted to use evolutionary game theory in order to explain the emergence of human attitudes about morality and corresponding animal behaviors. These authors look at several games including the prisoner's dilemma, stag hunt, and the Nash bargaining game as providing an explanation for the emergence of attitudes about morality (see, e.g., and ).\n\n\n\n\n\nLists\n\n\n\n\n\n", "id": "11924", "title": "Game theory"}
{"url": "https://en.wikipedia.org/wiki?curid=43723774", "text": "Polarr\n\nPolarr is an artificial intelligence startup headquartered in downtown San Jose, CA building photo management and editing. The company was founded in August 2014 by two former Stanford University masters' students, Borui Wang, and Enhao Gong. The company received initial funding from Pejman Mar Ventures and StartX. Polarr currently offers two products: Polarr Photo Editor and Polarr Album Plus.\n\nIn February 2015, Polarr launched an online photo editor. It was one of the earliest in-browser photo editors capable of editing RAW images. \n\nIn June 2015, the first mobile version of Polarr Photo Editor was released for iOS. The app was similar in features to the in-browser photo editor and received more than 250,000 downloads within the first 48 hours of availability in the App Store. An Android version was launched a month later in the Google Play Store. \n\nIn the fall of 2015, Polarr released photo editors for Windows 10 and macOS. \n\nPolarr was named by Apple as Best of the App Store for 2015 and 2016. \n\nIn October 2017, Polarr released Album Plus for iOS. \n\nPolarr Photo Editor is the first product from Polarr. The image editor is available on all major platforms and operating systems. The company claimed that the app has been used by over 10 million photographers to edit images as of August 2017. \n\nAlbum Plus is the latest product from Polarr. It was launched in October 2017. Album Plus helps users to organize their photos on their iPhone's using AI on the phone, and not the cloud. \n\n", "id": "43723774", "title": "Polarr"}
{"url": "https://en.wikipedia.org/wiki?curid=56072531", "text": "Ask Tuki\n\nTUKI started out as a simple backend system for automating searches and information retrieval between editors and journalists.\n\nHowever, with the improvement of virtual assistants in smart phones and computers TUKI was eventually opened to the public in 2017.\n\nBy December 2017, Tuki was operating under its own branding on a small number of news sites and apps offering question and answer forums “Ask Tuki” and chat bot style information retrieval interfaces “Tuki Assistant”\n\nMost notably Al-Sahawat Times runs “Ask Tuki” software on their official site as a members’ peer-to-peer questions and answer service.\n\nFirst developed purely as an internal reference system for newspaper editors and journalists, TUKI Assistant is an Artificial intelligence program that integrates with websites, devices and Virtual assistant (artificial intelligence) programs to deliver a customized voice, handwriting and text input multi-lingual interaction.\n\nIn essence this means that the program, which is now aimed at corporate and governmental solutions can provide additional device control and information layered into existing systems such as siri.\n\n\"Ask TUKI anything. A unique combination of artificial intelligence, field experts and moderated peer to peer communities answer for you swiftly.\" \n\nASK Tuki is a Question and Answer forum similar to quora and Yahoo! Answers. Its key differences are that answers do not just come from peer-to-peer users but also paid, industry experts and artificial intelligence.\n\nUsers are graded on a 7 level system, bronze, silver, gold, platinum, diamond and verified. The different membership levels relate to the ratio of positive feedback and verified answers.\n\nTUKI, operates on Al-Sahawat Times official website and is a fully integrated system within the Newspaper. Al-Sahawat Times in turn is owned by The International Press and Media Group ( IPMG ). Thus TUKI is owned by IPMG\n\nThe copyrights and patent for TUKI is registered to the Chief executive officer of IPMG Sheikh. Dr. Shamsaldin Qais Sulayman al-Said.\n", "id": "56072531", "title": "Ask Tuki"}
{"url": "https://en.wikipedia.org/wiki?curid=56127293", "text": "Artificial intelligence arms race\n\nAn artificial intelligence arms race is a competition between two or more states to have its military forces equipped with the best \"artificial intelligence\" (AI). Since the mid-2010s, many analysts have argued that a such a global arms race for better artificial intelligence has already begun.\n\nRussian General Viktor Bondarev, commander-in-chief of the Russian air force, has stated that as early as February 2017, Russia has been working on AI-guided missiles that can decide to switch targets mid-flight. Reports by state-sponsored Russian media on potential military uses of AI increased in mid-2017. In May 2017, the CEO of Russia's Kronstadt Group, a defense contractor, stated that \"there already exist completely autonomous AI operation systems that provide the means for UAV clusters, when they fulfill missions autonomously, sharing tasks between them, and interact\", and that it is inevitable that \"swarms of drones\" will one day fly over combat zones. Russia has been testing several autonomous and semi-autonomous combat systems, such as Kalashnikov's \"neural net\" combat module, with a machine gun, a camera, and an AI that its makers claim can make its own targeting judgements without human intervention. In September 2017, during a National Knowledge Day address to over a million students in 16,000 Russian schools, Russian President Vladimir Putin stated \"Artificial intelligence is the future, not only for Russia but for all humankind... Whoever becomes the leader in this sphere will become the ruler of the world\".\n\nThe Russian government has strongly rejected any ban on lethal autonomous weapons systems, suggesting that such a ban could be ignored.\n\nAccording to Elsa Kania of the Center for a New American Security, \"China is no longer in a position of technological inferiority but rather sees itself as close to catching up with and overtaking the United States in AI. As such, the (Chinese military) intends to achieve an advantage through changing paradigms in warfare with military innovation, thus seizing the 'commanding heights'...of future military competition\". The close ties between Silicon Valley and China, and the open nature of the American research community, has made the West's most advanced AI technology easily available to China; in addition, Chinese industry has numerous home-grown AI accomplishments of its own, such as Baidu passing a notable Chinese-language speech recognition capability benchmark in 2015. As of 2017, Beijing's roadmap aims to create a $150 billion AI industry by 2030. Before 2013, Chinese defense procurement was mainly restricted to a few conglomerates; however, as of 2017, China often sources sensitive emerging technology such as drones and artificial intelligence from private start-up companies.\n\nChina published a position paper in 2016 questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of the U.N. Security Council to broach the issue.\n\nIn 2014, former Secretary of Defense Chuck Hagel posited the \"Third Offset Strategy\" that rapid advances in artificial intelligence will define the next generation of warfare. According to data science and analytics firm Govini, The U.S. Department of Defense increased investment in artificial intelligence, big data and cloud computing from $5.6 billion in 2011 to $7.4 billion in 2106. However, the civilian NSF budget for AI saw no increase in 2017. In addition, many Western tech companies are leery of being associated too closely with the U.S. military, for fear of losing access to China's market.\n\nThe U.S. has many military AI combat programs, such as the Sea Hunter autonomous warship, which is designed to operate for extended periods at sea without a single crew member, and to even guide itself in and out of port. As of 2017, a temporary US Department of Defense directive requires a human operator to be kept in the loop when it comes to the taking of human life by autonomous weapons systems.\n\nIn 2015, the UK government opposed a ban on lethal autonomous weapons, stating that \"international humanitarian law already provides sufficient regulation for this area\", but that all weapons employed by UK armed forces would be \"under human oversight and control\".\n\nIsrael's Harpy anti-radar \"fire and forget\" drone is designed to be launched by ground troops, and autonomously fly over an area to find and destroy radar that fits pre-determined criteria.\n\nThe South Korean Super aEgis II machine gun, unveiled in 2010, sees use both in South Korea and in the Middle East. It can identify, track, and destroy a moving target at a range of 4 km. While the technology can theoretically operate without human intervention, in practice safeguards are installed to require manual input. A South Korean manufacturer states, \"Our weapons don't sleep, like humans must. They can see in the dark, like humans can't. Our technology therefore plugs the gaps in human capability\", and they want to \"get to a place where our software can discern whether a target is friend, foe, civilian or military\".\n\nAs early as 2007, scholars such as AI professor Noel Sharkey have warned of \"an emerging arms race among the hi-tech nations to develop autonomous submarines, fighter jets, battleships and tanks that can find their own targets and apply violent force without the involvement of meaningful human decisions\". As early as 2014, AI specialists such as Steve Omohundro have been warning that \"An autonomous weapons arms race is already taking place\". Miles Brundage of the University of Oxford has argued an AI arms race might be somewhat mitigated through diplomacy: \"We saw in the various historical arms races that collaboration and dialog can pay dividends\". Over a hundred experts signed an open letter in 2017 calling on the UN to address the issue of lethal autonomous weapons; however, at a November 2017 session of the UN Convention on Certain Conventional Weapons (CCW), diplomats could not agree even on how to define such weapons. The Indian ambassador and chair of the CCW stated that agreement on rules remained a distant prospect. As of 2017, twenty-two countries have called for a full ban on lethal autonomous weapons.\n\nMany experts believe attempts to completely ban killer robots are likely to fail. A 2017 report from Harvard's Belfer Center predicts that AI has the potential to be as transformative as nuclear weapons. The report further argues that \"Preventing expanded military use of AI is likely impossible\" and that \"the more modest goal of safe and effective technology management must be pursued\", such as banning the attaching of an AI dead man's switch to a nuclear arsenal. Part of the impracticality is that detecting treaty violations would be extremely difficult.\n\nA 2015 open letter calling for the ban of lethal automated weapons systems has been signed by tens of thousands of citizens, including scholars such as physicist Stephen Hawking, Tesla magnate Elon Musk, and Apple's Steve Wozniak.\n\nProfessor Noel Sharkey of the University of Sheffield has warned that autonomous weapons will inevitably fall into the hands of terrorist groups such as the Islamic State.\n\n", "id": "56127293", "title": "Artificial intelligence arms race"}
{"url": "https://en.wikipedia.org/wiki?curid=55843837", "text": "Automated machine learning\n\nAutomated machine learning (AutoML) is the process of automating the end-to-end process of applying machine learning to real-world problems. In a typical machine learning application, practitioners must apply the appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods that make the dataset amenable for machine learning. Following those preprocessing steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their final machine learning model. As many of these steps are often beyond the abilities of non-experts, AutoML was proposed as an artificial intelligence-based solution to the ever-growing challenge of applying machine learning. Automating the end-to-end process of applying machine learning offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform models that were designed by hand.\n\nAutomated machine learning can target various stages of the machine learning process:\n\nSoftware tackling various stages of AutoML:\n\n\n\n\n", "id": "55843837", "title": "Automated machine learning"}
{"url": "https://en.wikipedia.org/wiki?curid=1931185", "text": "Automatic image annotation\n\nAutomatic image annotation (also known as automatic image tagging or linguistic indexing) is the process by which a computer system automatically assigns metadata in the form of captioning or keywords to a digital image. This application of computer vision techniques is used in image retrieval systems to organize and locate images of interest from a database.\n\nThis method can be regarded as a type of multi-class image classification with a very large number of classes - as large as the vocabulary size. Typically, image analysis in the form of extracted feature vectors and the training annotation words are used by machine learning techniques to attempt to automatically apply annotations to new images. The first methods learned the correlations between image features and training annotations, then techniques were developed using machine translation to try to translate the textual vocabulary with the 'visual vocabulary', or clustered regions known as \"blobs\". Work following these efforts have included classification approaches, relevance models and so on.\n\nThe advantages of automatic image annotation versus content-based image retrieval (CBIR) are that queries can be more naturally specified by the user. CBIR generally (at present) requires users to search by image concepts such as color and texture, or finding example queries. Certain image features in example images may override the concept that the user is really focusing on. The traditional methods of image retrieval such as those used by libraries have relied on manually annotated images, which is expensive and time-consuming, especially given the large and constantly growing image databases in existence.\n\nSome annotation engines are online, including the ALIPR.com real-time tagging engine developed by Pennsylvania State University researchers, and Behold.\n\n\n\n\n\n", "id": "1931185", "title": "Automatic image annotation"}
{"url": "https://en.wikipedia.org/wiki?curid=2090992", "text": "Dr. Sbaitso\n\nDr. Sbaitso is an artificial intelligence speech synthesis program released late in 1991 by Creative Labs for MS DOS-based personal computers.\n\nDr. Sbaitso was distributed with various sound cards manufactured by Creative Labs (the name was an acronym for Sound Blaster Artificial Intelligent Text to Speech Operator) in the early 1990s.\n\nThe program \"conversed\" with the user as if it were a psychologist, though most of its responses were along the lines of \"WHY DO YOU FEEL THAT WAY?\" rather than any sort of complicated interaction. When confronted with a phrase it could not understand, it would often reply with something such as \"THAT'S NOT MY PROBLEM\". Dr. Sbaitso repeated text out loud that was typed after the word \"SAY\". Repeated swearing or abusive behavior on the part of the user caused Dr. Sbaitso to \"break down\" in a \"PARITY ERROR\" before resetting itself.\n\nThe program introduced itself with the following lines:\nExample:\n\nThe program was designed to showcase the digitized voices the cards were able to produce, though the quality was far from lifelike.\n\n\n", "id": "2090992", "title": "Dr. Sbaitso"}
{"url": "https://en.wikipedia.org/wiki?curid=2760602", "text": "Polyworld\n\nPolyworld is a cross-platform (Linux, Mac OS X) program written by Larry Yaeger to evolve Artificial Intelligence through natural selection and evolutionary algorithms. \n\nIt uses the Qt graphics toolkit and OpenGL to display a graphical environment in which a population of trapezoid agents search for food, mate, have offspring, and prey on each other. The population is typically only in the hundreds, as each individual is rather complex and the environment consumes considerable computer resources. The graphical environment is necessary since the individuals actually move around the 2-D plane and must be able to \"see.\" Since some basic abilities, like eating carcasses or randomly generated food, seeing other individuals, mating or fighting with them, etc., are possible, a number of interesting behaviours have been observed to spontaneously arise after prolonged evolution, such as cannibalism, predators and prey, and mimicry.\n\nEach individual makes decisions based on a neural net using Hebbian learning; the neural net is derived from each individual's genome. The genome does not merely specify the wiring of the neural nets, but also determines their size, speed, color, mutation rate and a number of other factors. The genome is randomly mutated at a set probability, which are also changed in descendant organisms.\n\n", "id": "2760602", "title": "Polyworld"}
{"url": "https://en.wikipedia.org/wiki?curid=463838", "text": "Eurisko\n\nEurisko (Gr., \"I discover\") is a program written by Douglas Lenat in RLL-1, a representation language itself written in the Lisp programming language. A sequel to Automated Mathematician, it consists of heuristics, i.e. rules of thumb, including heuristics describing how to use and change its own heuristics. Lenat was frustrated by Automated Mathematician's constraint to a single domain and so developed Eurisko; his frustration with the effort of encoding domain knowledge for Eurisko led to Lenat's subsequent (and, , continuing) development of Cyc. Lenat envisions ultimately coupling the Cyc knowledgebase with the Eurisko discovery engine.\n\nDevelopment commenced at Carnegie Mellon in 1976 and continued at Stanford University in 1978 when Lenat returned to teach. \"For the first five years, nothing good came out of it\", Lenat said. But when the implementation was changed to a frame language based representation he called RLL (Representation Language Language), heuristic creation and modification became much simpler. Eurisko was then applied to a number of domains with surprising success, including VLSI chip design.\n\nLenat and Eurisko gained notoriety by submitting the winning fleet (a large number of stationary, lightly-armored ships with many small weapons) to the United States Traveller TCS national championship in 1981, forcing extensive changes to the game's rules. However, Eurisko won again in 1982 when the program discovered that the rules permitted the program to destroy its own ships, permitting it to continue to use much the same strategy. Tournament officials announced that if Eurisko won another championship the competition would be abolished; Lenat retired Eurisko from the game. The Traveller TCS wins brought Lenat to the attention of DARPA, which has funded much of his subsequent work.\n\nIn the first-season \"The X-Files\" episode \"Ghost in the Machine\", Eurisko is the name of a fictional software company responsible for the episode's \"monster of the week\", facilities management software known as \"Central Operating System\", or \"COS\". COS (described in the episode as an \"adaptive network\") is shown to be capable of learning when its designer arrives at Eurisko headquarters and is surprised to find that COS has given itself the ability to speak. The designer is forced to create a virus to destroy COS after COS commits a series of murders in an apparent effort to prevent its own destruction.\n\nLenat is also mentioned and Eurisko is discussed at the end of Richard Feynman's Computer Heuristics Lecture as part of the Idiosyncratic Thinking Workshop Series.\n\n", "id": "463838", "title": "Eurisko"}
{"url": "https://en.wikipedia.org/wiki?curid=6006314", "text": "TuVox\n\nTuVox is a company that produces VXML-based telephone speech-recognition applications to replace DTMF touch-tone systems for their clients.\n\nTuVox was founded in 2001 by Steven S. Pollock and Ashok Khosla, formerly of Apple Computer Corporation and Claris Corporation. Since then, TuVox has grown to over 40 employees and has US offices in Cupertino, California and Boca Raton, Florida as well as international offices in London, Vancouver and Sydney. In 2005, TuVox acquired the customers and hosting facilities of Net-By-Tel.\n\nOn July 22, 2010, West Interactive—a subsidiary of West Corporation—announced its acquisition of TuVox.\n\nTuVox clients include 1-800-Flowers.com, AMC Entertainment, American Airlines, British Airways, M&T Bank, Canon Inc., Gateway, Inc., Motorola, Progress Energy Inc., Telecom New Zealand, Time, Inc., BECU, Virgin America and USAA.\n\n\n", "id": "6006314", "title": "TuVox"}
{"url": "https://en.wikipedia.org/wiki?curid=8227051", "text": "Sinewave synthesis\n\nSinewave synthesis, or sine wave speech, is a technique for synthesizing speech by replacing the formants (main bands of energy) with pure tone whistles. The first sinewave synthesis program (\"SWS\") for the automatic creation of stimuli for perceptual experiments was developed by Philip Rubin at Haskins Laboratories in the 1970s. This program was subsequently used by Robert Remez, Philip Rubin, David Pisoni, and other colleagues to show that listeners can perceive continuous speech without traditional speech cues. This work paved the way for a view of speech as a dynamic pattern of trajectories through articulatory-acoustic space.\n\n\n", "id": "8227051", "title": "Sinewave synthesis"}
{"url": "https://en.wikipedia.org/wiki?curid=8642422", "text": "Language identification\n\nIn natural language processing, language identification or language guessing is the problem of determining which natural language given content is in. Computational approaches to this problem view it as a special case of text categorization, solved with various statistical methods.\n\nThere are several statistical approaches to language identification using different techniques to classify the data. One technique is to compare the compressibility of the text to the compressibility of texts in a set of known languages. This approach is known as mutual information based distance measure. The same technique can also be used to empirically construct family trees of languages which closely correspond to the trees constructed using historical methods. Mutual information based distance measure is essentially equivalent to more conventional model-based methods and is not generally considered to be either novel or better than simpler techniques. \n\nAnother technique, as described by Cavnar and Trenkle (1994) and Dunning (1994) is to create a language n-gram model from a \"training text\" for each of the languages. These models can be based on characters (Cavnar and Trenkle) or encoded bytes (Dunning); in the latter, language identification and character encoding detection are integrated. Then, for any piece of text needing to be identified, a similar model is made, and that model is compared to each stored language model. The most likely language is the one with the model that is most similar to the model from the text needing to be identified. This approach can be problematic when the input text is in a language for which there is no model. In that case, the method may return another, \"most similar\" language as its result. Also problematic for any approach are pieces of input text that are composed of several languages, as is common on the Web. \n\nFor a more recent method, see Řehůřek and Kolkus (2009). This method can detect multiple languages in an unstructured piece of text and works robustly on short texts of only a few words: something that the n-gram approaches struggle with.\n\nAn older statistical method by Grefenstette was based on the prevalence of certain function words (e.g., \"the\" in English).\n\nOne of the great bottlenecks of language identification systems is to distinguish between closely related languages. Similar languages like Serbian and Croatian or Indonesian and Malay present significant lexical and structural overlap, making it challenging for systems to discriminate between them. \n\nRecently, the DSL shared task has been organized providing a dataset (Tan et al., 2014) containing 13 different languages (and language varieties) in six language groups: Group A (Bosnian, Croatian, Serbian), Group B (Indonesian, Malaysian), Group C (Czech, Slovakian), Group D (Brazilian Portuguese, European Portuguese), Group E (Peninsular Spain, Argentine Spanish), Group F (American English, British English). The best system reached performance of over 95% results (Goutte et al., 2014). Results of the DSL shared task are described in Zampieri et al. 2014.\n\n\n\n", "id": "8642422", "title": "Language identification"}
{"url": "https://en.wikipedia.org/wiki?curid=8642531", "text": "Machine translation software usability\n\nThe sections below give objective criteria for evaluating the usability of machine translation software output.\n\nDo repeated translations converge on a single expression in both languages? I.e. does the translation method show stationarity or produce a canonical form? Does the translation become stationary without losing the original meaning? This metric has been criticized as not being well correlated with BLEU (BiLingual Evaluation Understudy) scores.\n\nIs the system adaptive to colloquialism, argot or slang? The French language has many rules for creating words in the speech and writing of popular culture. Two such rules are: (a) The reverse spelling of words such as \"femme\" to \"meuf\". (This is called verlan.) (b) The attachment of the suffix \"-ard\" to a noun or verb to form a proper noun. For example, the noun \"faluche\" means \"student hat\". The word \"faluchard\" formed from \"faluche\" colloquially can mean, depending on context, \"a group of students\", \"a gathering of students\" and \"behavior typical of a student\". The Google translator as of 28 December 2006 doesn't derive the constructed words as for example from rule (b), as shown here:\nIl y a une chorale falucharde mercredi, venez nombreux, les faluchards chantent des paillardes! ==> \"There is a choral society falucharde Wednesday, come many, the faluchards sing loose-living women!\"\nFrench argot has three levels of usage:\n\nThe United States National Institute of Standards and Technology conducts annual evaluations of machine translation systems based on the BLEU-4 criterion . A combined method called IQmt which incorporates BLEU and additional metrics NIST, GTM, ROUGE and METEOR has been implemeneted by Gimenez and Amigo .\n\nIs the output grammatical or well-formed in the target language? Using an interlingua should be helpful in this regard, because with a fixed interlingua one should be able to write a grammatical mapping to the target language from the interlingua. Consider the following Arabic language input and English language translation result from the Google translator as of 27 December 2006 . This Google translator output doesn't parse using a reasonable English grammar: \n\nDo repeated re-translations preserve the semantics of the original sentence? For example, consider the following English input passed multiple times into and out of French using the Google translator as of 27 December 2006: \n\nAs noted above and in, this kind of round-trip translation is a very unreliable method of evaluation.\n\nAn interesting peculiarity of Google Translate as of 24 January 2008 (corrected as of 25 January 2008) is the following result when translating from English to Spanish, which shows an embedded joke in the English-Spanish dictionary which has some added poignancy given recent events:\nThis raises the issue of trustworthiness when relying on a machine translation system embedded in a Life-critical system in which the translation system has input to a Safety Critical Decision Making process. Conjointly it raises the issue of whether in a given use the software of the machine translation system is safe from hackers.\n\nIt is not known whether this feature of Google Translate was the result of a joke/hack or perhaps an unintended consequence of the use of a method such as statistical machine translation. Reporters from CNET Networks asked Google for an explanation on January 24, 2008; Google said only that it was an \"internal issue with Google Translate\". The mistranslation was the subject of much hilarity and speculation on the Internet.\n\nIf it is an unintended consequence of the use of a method such as statistical machine translation, and not a joke/hack, then this event is a demonstration of a potential source of critical unreliability in the statistical machine translation method.\n\nIn human translations, in particular on the part of interpreters, selectivity on the part of the translator in performing a translation is often commented on when one of the two parties being served by the interpreter knows both languages.\n\nThis leads to the issue of whether a particular translation could be considered \"verifiable\". In this case, a converging round-trip translation would be a kind of verification.\n\n\n", "id": "8642531", "title": "Machine translation software usability"}
{"url": "https://en.wikipedia.org/wiki?curid=5559076", "text": "Monitoring and surveillance agents\n\nMonitoring and surveillance agents (also known as predictive agents) are a type of intelligent agent software that observes and reports on computer equipment. Monitoring and surveillance agents are often used to monitor complex computer networks to predict when a crash or some other defect may occur. Another type of monitoring and surveillance agent works on computer networks keeping track of the configuration of each computer connected to the network. It tracks and updates the central configuration database when anything on any computer changes, such as the number or type of disk drives. An important task in managing networks lies in prioritizing traffic and shaping bandwidth.\n\n\nHaag & Cummings & McCubbrey & Pinsonneault & Donovan (2006). \"Management Information Systems Third Canadian Ed.\" McGraw-Hill Ryerson\n\n", "id": "5559076", "title": "Monitoring and surveillance agents"}
{"url": "https://en.wikipedia.org/wiki?curid=5410176", "text": "Network compartment\n\nNetwork Compartmentalization, the division of network functionality into network compartments, is an important concept of Autonomic Networking.\n\nNetwork Compartments implement the operational rules and administrative policies for a given communication context. The boundaries of a communication context, and hence the compartment boundaries, are based on technological and/or administrative boundaries. For example, compartment boundaries can be defined by a certain type of network technology (e.g., a specific wireless access network) or based on a particular communication protocol and/or addressing space (e.g., an IPv4 or and IPv6 network), but also based on a policy domain (e.g., a national health network that requires a highly secure boundary). \n\nA compartment's communication principles, protocols and policies form a sort of “recipe” that all compartment entities must obey. For example, the recipe defines how to join a compartment, who can join, and how the naming, addressing and routing is handled. The complexity and details of the internal operation is left to each compartment. For example, registration with a compartment can range from complex trust-based mechanisms to simple registration schemes with a central database or a public DHT-based system; resolution of a communication peer can be handled implicitly by the compartment’s naming and addressing scheme or require explicit actions (e.g., resolution of an identifier to a locator). It is important to note here that compartments have full autonomy on how to handle the compartment’s internal communication – i.e. there are no global invariants that have to be implemented by all compartments or all communication elements.\n\nMembers of a compartment are able and willing to communicate among each other according to compartment’s operational and policy rules. Conceptually a compartment maintains some form of implicit database which contains its members; that is, each entry in the database defines a member. Before one can send a data packet to a compartment member, a resolution step is required which returns a means to “address” the member. Note that the above definition does not specify whether a member is a node, a set of servers or a software module. This rather abstract definition of compartment membership permits to capture many different flavours of members and communication forms. \n\nIt is anticipated that many compartments co-exist and that compartments are able to interwork on various levels (e.g. through \"layering\" or \"peering\" of compartments).\n\n", "id": "5410176", "title": "Network compartment"}
{"url": "https://en.wikipedia.org/wiki?curid=13234913", "text": "Artificial Intelligence Applications Institute\n\nThe Artificial Intelligence Applications Institute (AIAI) at the School of Informatics at the University of Edinburgh is a non-profit technology transfer organisation that promotes the benefits of the application of Artificial Intelligence research to commercial, industrial, and government organisations worldwide.\n\nAIAI was created in July 1983, and received it formal charter from the University of Edinburgh in July 1984. It joined the School of Informatics when this was created from a number of departments and research institutes in 1998. The Director of AIAI is Austin Tate.\n\nAIAI specialises in Intelligent and Knowledge-based systems, including:\n\n", "id": "13234913", "title": "Artificial Intelligence Applications Institute"}
{"url": "https://en.wikipedia.org/wiki?curid=6026708", "text": "Noisy text analytics\n\nNoisy text analytics is a process of information extraction whose goal is to automatically extract structured or semistructured information from noisy unstructured text data. While Text analytics is a growing and mature field that has great value because of the huge amounts of data being produced, processing of noisy text is gaining in importance because a lot of common applications produce noisy text data. Noisy unstructured text data is found in informal settings such as online chat, text messages, e-mails, message boards, newsgroups, blogs, wikis and web pages. Also, text produced by processing spontaneous speech using automatic speech recognition and printed or handwritten text using optical character recognition contains processing noise. Text produced under such circumstances is typically highly noisy containing spelling errors, abbreviations, non-standard words, false starts, repetitions, missing punctuations, missing letter case information, pause filling words such as “um” and “uh” and other texting and speech disfluencies. Such text can be seen in large amounts in contact centers, chat rooms, optical character recognition (OCR) of text documents, short message service (SMS) text, etc. Documents with historical language can also be considered noisy with respect to today’s knowledge about the language. Such text contains important historical, religious, ancient medical knowledge that is useful. The nature of the noisy text produced in all these contexts warrants moving beyond traditional text analysis techniques.\n\nMissing punctuation and the use of non-standard words can often hinder standard natural language processing tools such as part-of-speech tagging\nand parsing. Techniques to both learn from the noisy data and then to be able to process the noisy data are only now being developed.\n\n\n", "id": "6026708", "title": "Noisy text analytics"}
{"url": "https://en.wikipedia.org/wiki?curid=6773335", "text": "Verbot\n\nThe Verbot (Verbal-Robot) was a popular chatterbot program and Artificial Intelligence Software Development Kit (SDK) for the Windows platform and for the web.\n\nVirtual Personalities, Inc. traces its technology back to Michael Mauldin's work as a graduate student and post-doctoral fellow at Carnegie Mellon University; and its artistry back to Peter Plantec's work in personality psychology and art direction.\n\nIn 1994, Mauldin, Founder of Lycos, Inc., developed a prototype Chatterbot, Julia, which competed in the internationally known Turing test, for the coveted Loebner Prize. The Turing Test matches computer scientist judges against machines to see if they can distinguish a computer from a real human. This prototype version was refined and developed, and in 1997, Dr. Mauldin and Peter Plantec, a clinical psychologist, and animator, formed Virtual Personalities, Inc. (now Conversive, Inc.) in order to create a virtual human interface that would incorporate real-time animation as well as speech and natural language processing. The initial release, a stand-alone virtual person called Sylvie, was beta-tested to the public. This release was well received, and finally, after several versions, the production release (deemed version 3) of the Verbally Enhanced Software Robot—or, Verbot was deployed in the Fall 2000.\n\n\nThe Virtual Personalities story goes back to 1978, where Mauldin was attending Rice University. Fascinated by the idea of ELIZA, he proceeded to write a program called \"PET\" for his 8 kilobyte Commodore PET Computer. PET included simple induction as a way to post new information, and once managed the following deep observation: Meanwhile, Plantec was separately designing a personality for \"Entity\", a theoretical virtual human that would interact comfortably with humans without pretending to be one. At that time the technology was not advanced enough to bring Entity to life, however, Mauldin was working on that.\n\nMauldin got so involved with this, that he majored in Computer Science and minored in Linguistics.\n\nIn the late seventies and early eighties, a popular computer game at Universities was Rogue, an implementation of Dungeons and Dragons where the player would descend 26 levels in a randomly created dungeon, fighting monsters, gathering treasure, and searching for the elusive \"Amulet of Yendor\". Mauldin was one of four grad students who devoted a large amount of time to building a program called \"Rog-O-Matic\" that could and on several occasions did manage to retrieve the amulet and emerge victorious from the dungeon.\n\nSo when in 1989, James Aspnes at Carnegie Mellon created the first TinyMUD (a descendent of MUD and AberMUD), Mauldin was one of the first to create a computer player that would explore the text-based world of TinyMUD. But his first robot, Gloria, gradually accreted more and more linguistic ability, to the point that it could pass the \"unsuspecting\" Turing Test. In this version of the test, the human has no reason to suspect that one of the other occupants of the room is controlled by a computer, and so is more polite and asks fewer probing questions. The second generation of Mauldin's TinyMUD robots was Julia, created on Jan. 8, 1990. Julia slowly developed into a more and more capable conversational agent, and assumed useful duties in the TinyMUD world, including tour guide, information assistant, note-taker, message-relayer, and even could play the card game hearts along with the other human players.\n\nIn 1991, the first Loebner Prize contest was held in Boston, Mass., and Julia was there. Although she only finished third, she was ranked by one judge as more human than one of the human confederates, winning a coveted certificate of humanness in the world's first restricted Turing test.\n\nJulia continued to log in to various TinyMUD's and TinyMucks for the next seven years, and also chats with hundreds of people a month over the internet.\n\nJulia's job was to explore a virtual world consisting of pages of textual descriptions, with links between them, and to construct an internal map of that world and answer questions about it (including path information such as the shortest route from one room to another, and matching information, such as which rooms contained a certain kind of object or textual description).\n\nIt was therefore only a very short cognitive leap from Julia to Lycos, another robotic agent that explores a virtual world made of hyperlinked pages of text, and which answers questions about those pages. Sylvie was born and her abilities were expanded greatly to include interfacing with computers and control systems via her serial ports.\n\nSylvie was the first intelligent animated virtual human. She was designed both as a conversation agent and as a virtual human interface that would form a bridge between the two. She became more popular as a conversation agent, but her designers believe she serves as a prototype for future virtual human interface design that will help us all cope with the increasing complexity of technology.\n\nAs and aside, Plantec noticed that an inordinately large number of Sylvies were being sold in Southeast Asia. Upon investigation he discovered that students had discovered a \"test\" mode that would allow them to type in English sentences that Sylvie would pronounce in her somewhat stylized English. Sylvie was teaching them English ... her style of English.\n\nIn 1997, Dr. Mauldin and Peter Plantec, formed Virtual Personalities, Inc. to create Natural Language Processing solutions for companies. In 2001 Virtual Personalities, Inc. became Conversive, Inc. to reflect the focus on providing Customer Service and Marketing to the Enterprise Market. In late 2012 Avaya, Inc. acquired Conversive's assets including Verbots.\n\nThe Verbot 4 version was created and released in 2004. In 2005 Version 4.1 of the Verbot Software was released with many feature enhancements and bug fixes, including built-in support for embedding C# code in outputs and conditionals. In Early 2006 Conversive launched Verbots Online allowing Verbot 4 users to upload their knowledge and show off their bots to the world. In 2009 Version 5 was released, completely free and fully featured. In early 2012 the last version of Verbot, 5.0.1.2, was released to the general public with support for Windows 7. Also in 2012 Verbots Online completely shutdown.\n\nVerbots.com, its community of users and its forums no longer exist but the software and users can still be found. There has been no active development since the early 2012 release of Verbot 5.0.1.2.\n\n\n", "id": "6773335", "title": "Verbot"}
{"url": "https://en.wikipedia.org/wiki?curid=16021556", "text": "DialogOS\n\nDialogOS is a graphical programming environment to design computer system which can converse through voice with the user. Dialogs are clicked together in a Flowchart. DialogOS includes bindings to control Lego Mindstorms robots with the voice.\n\nDialogOS is used in computer science courses in schools and universities to teach programming and to introduce beginners in the basic principles of human/computer interaction and dialog design.\n\nDialogOS can control the LEGO Mindstorms NXT Series. It uses sensor-nodes to obtain values for the following sensors:\n", "id": "16021556", "title": "DialogOS"}
{"url": "https://en.wikipedia.org/wiki?curid=14477647", "text": "Quack.com\n\nQuack.com was an early voice portal company. The domain name later was used for Quack, an iPad search application from AOL.\n\nIt was founded in 1998 by Steven Woods, Jeromy Carriere and Alex Quilici as a Pittsburgh, Pennsylvania, USA, based voice portal infrastructure company named Quackware. Quack was the first company to try to create a voice portal: a consumer-based destination \"site\" in which consumers could not only access information by voice alone, but also complete transactions. Quackware launched a beta phone service in 1999 that allowed consumers to purchase books from sites such as Amazon and CDs from sites such as CDNow by answering a short set of questions. Quack followed with a set of information services from movie listings (inspired by, but expanding upon, Moviefone) to news, weather and stock quotes. This concept introduced a series of lookalike startups including Tellme Networks which raised more money than any Internet startup in history on a similar concept.\n\nQuack received venture funding in 1999 and moved operations to Mountain View in Silicon Valley, California in 1999. \nA deal with Lycos was announced in May 2000.\nIn September 2000 Quack was acquired for $200 million by America Online (AOL) and moved onto the Netscape campus with what was left of the Netscape team.\n\nQuack was attacked in the Canadian press for being representative of the Canadian \"brain drain\" to the US during the Internet bubble, focusing its recruiting efforts on the University of Waterloo, hiring more than 50 engineers from Waterloo in less than 10 months. Quack competitor Tellme Networks raised enormous funds in what became a highly competitive market in 2000, with the emergence of more than a dozen additional competitors in a 12-month period.\n\nFollowing its acquisition by America Online in an effort led by Ted Leonsis to bring Quack into AOL Interactive, the Quack voice service became AOLbyPhone as one of AOL's \"web properties\" along with MapQuest, Moviefone and others.\n\nQuack secured several patents that underlie the technical challenges of delivering interactive voice services. Constructing a voice portal required integrations and innovations not only in speech recognition and speech generation, but also in databases, application specification, constraint-based reasoning and artificial intelligence and computational linguistics. \"Quack\"'s name derived from the company goal of providing not only voice-based services, but more broadly \"Quick Ubiquitous Access to Consumer Knowledge\".\n\nThe patents assigned to Quack.com include: System and method for voice access to Internet-based information, System and method for advertising with an Internet Voice Portal and recognizing the axiom that in interactive voice systems one must \"know the set of possible answers to a question before asking it\". System and method for determining if one web site has the same information as another web site.\n\nQuack.com was spoofed in \"The Simpsons\" in March 2002 in the episode \"Blame It on Lisa\" in which a \"ComQuaak\" sign is replaced by another equally crazy telecom company name.\n\nIn July 2010, quack.com became the focus of a new AOL iPad application, that was a web search experience. The product delivers web results and blends in picture, video and Twitter results. It enables you to preview the web results before you go to the site, search within each result, and flip through the results pages, making full use of the iPad's touch screen features. The iPad app was free via iTunes, but support discontinued in 2012.\n\n", "id": "14477647", "title": "Quack.com"}
{"url": "https://en.wikipedia.org/wiki?curid=18629502", "text": "Context-sensitive user interface\n\nA context-sensitive user interface is one which can automatically choose from a multiplicity of options based on the current or previous state(s) of the program operation. \"Context sensitivity\" is almost ubiquitous in current graphical user interfaces, usually in the form of context menus. Context sensitivity, when operating correctly, should be practically transparent to the user. This can be experienced in computer operating systems which call a compatible program to run files based upon their filename extension, e.g. opening text files with a word processor, video files (.mpg, .mov and .avi, etc.) with a video player, image files (.jpg and .png etc.) with a photo viewer or running program files themselves, and their shortcuts, (i.e. .exe files) when selected.\n\nThe user-interface may also provide \"Context sensitive\" feedback, such as changing the appearance of the mouse pointer or cursor, changing the menu color, or with applicable auditory or tactile feedback.\n\nThe primary reason for introducing context sensitivity is to simplify the user interface.\n\nAdvantages include:\n\n\nContext sensitive actions may be perceived as dumbing down of the user interface - leaving the operator at a loss as to what to do when the computer decides to perform an unwanted action. Additionally non-automatic procedures may be hidden or obscured by the context sensitive interface causing an increase in user workload for operations the designers did not foresee.\n\nA poor implementation can be more annoying than helpful – a classic example of this is Office Assistant.\n\nAt the simplest level each possible action is reduced to a single most likely action – the action performed is based on a single variable (such as file extension). In more complicated \nimplementations multiple factors can be assessed such as the user's previous actions, the size of the file, the programs in current use, metadata etc.\n\nThe method is not only limited to the response to imperative button presses and mouse clicks - pop-up menus can be pruned and/or altered, or a web search can focus results based on previous searches.\n\nAt higher levels of implementation \"context sensitive\" actions require either larger amounts of meta-data, extensive case analysis based programming, or other artificial intelligence algorithms.\n\nContext sensitivity is important in video games – especially those controlled by a gamepad, joystick or computer mouse in which the number of buttons available is limited. It is primarily applied when the player is in a certain place and is used to interact with a person or object. For example, if the player is standing next to a non-player character, an option may come up allowing the player to talk with him/her.\n\nImplementations range from the embryonic 'Quick Time Event' to context sensitive sword combat in which the attack used depends on the position and orientation of both the player and opponent, as well as the virtual surroundings. A similar range of use is found in the 'action button' which, depending upon the in-game position of the player's character, may cause it to pick something up, open a door, grab a rope, punch a monster or opponent, or smash an object.\n\nThe response does not have to be player activated - an on-screen device may only be shown in certain circumstances, e.g. 'targeting' cross hairs in a flight combat game may indicate the player should fire. An alternative implementation is to monitor the input from the player (e.g. level of button pressing activity) and use that to control the pace of the game in an attempt to maximize enjoyment or to control the excitement or ambience.\n\nThe method has become increasingly important as more complex games are designed for machines with few buttons (keyboard-less consoles). Bennet Ring commented (in 2006) that \"\"Context-sensitive\" is the new lens flare\".\n\nContext sensitive help is a common implementation of context sensitivity, a single help button is actioned and the help page or menu will open a specific page or related topic.\n\n\n", "id": "18629502", "title": "Context-sensitive user interface"}
{"url": "https://en.wikipedia.org/wiki?curid=2961998", "text": "ESTAR project\n\nThe eSTAR project is a multi-agent system that aims to implement a true heterogeneous network of robotic telescopes for automated observing. The project is a joint collaboration between the Astrophysics Group of the University of Exeter and the Astrophysics Research Institute at Liverpool John Moores University.\n\nIn 2006 work began on an autonomous software agent for observations of variable stars. This agent implements the optimal sampling technique of Saunders et al. (2006) and the prototype was successfully tested on the RoboNet network of telescopes which includes: the Liverpool Telescope, the Faulkes Telescope North and the Faulkes Telescope South.\n\neSTAR is affiliated with the RoboNet Consortium and the global Heterogeneous Telescope Networks Consortium.\n\nAs of 2007 eSTAR is \"live\" supporting two real-time observing projects. Automated follow-up observations of gamma ray bursts are performed using the 3.8m UKIRT telescope situated in Hawai'i, making this telescope the largest in the world, with an automated response system for tracking such events.\n\neSTAR is also involved in the search for extra-solar planets by placing observations on the RoboNet system of telescopes on behalf of the PLANET collaboration. The technique of gravitational microlensing is used to monitor large numbers of stars in the galactic bulge looking for the tell-tale signature of cool planets orbiting those stars.\n\n", "id": "2961998", "title": "ESTAR project"}
{"url": "https://en.wikipedia.org/wiki?curid=18849138", "text": "YouNoodle\n\nYouNoodle is a San Francisco-based company, with offices in Barcelona and Santiago, founded in 2010, building a platform for entrepreneurship competitions all over the world. YouNoodle matches entrepreneurs with competitions, accelerators, and startup programs, and provides a judging and voting SaaSplatform to university, non-profit, government and enterprise clients organizing innovation challenges and competitions. Stanford's BASES, UC Berkeley LAUNCH, Start-Up Chile, Amazon Startup Challenge, and NASA are all running one or more competitions on YouNoodle's platform. \n\nYouNoodle was founded by Rebeca Hwang and Torsten Kolind in 2010. The company was spun off a project started by Bob Goodson and Kirill Makharinsky in 2007 with support from Peter Thiel (Founders Fund), Max Levchin (PayPal) and Charles Lho (Amicus Group), founding investor and Chairman of YouNoodle today. This project also spawned Quid (Goodson) and indirectly Ostrovok (Makharinsky). Although also named YouNoodle, this project/company was discontinued in 2010, when the three new entities started operations.\n\nThe founders of the 2007-2010 entity were Goodson and Makharinsky, both former students of the University of Oxford. Goodson had studied medieval English literature before moving from Oxford to California when Levchin, the co-founder of Paypal, invited him to join a start-up there. Makharinsky's degree was in applied mathematics, and he was also encouraged to pursue opportunities in the United States by Levchin. Other significant employees included Hwang (co-founder of today's YouNoodle), a Stanford University doctoral student whose research is into social network theory.\n\nYouNoodle's now discontinued \"Startup predictor\", part of the 2007-2010 entity and developed by Makharinsky and Hwang, uses mathematical models to predict the success of new businesses. The user fills in a questionnaire, which takes about half an hour to complete and concentrates on the business' concept, finances, founders and advisers. Because the procedure is designed for very new companies, questions on revenue and traffic are not included. The site then provides an estimate of what the company's value will be after three years and a score from 1 to 1000 representing its value as an investment. The service is free for the startups themselves, but YouNoodle intends to charge third parties for access to the results. (The level of detail required by the questionnaire makes it difficult for people without inside knowledge of a company to provide the data for a prediction on their own.)\n\nThe company's founders have declined to explain the algorithm in detail, but state that it takes into account the entrepreneurs' experience, networks and mutual relations. Information provided by companies which use the site's networking features is used to improve the algorithm. As of August 2008, the algorithm was based on data from 3,000 startups. In the same month the company had four patents pending on the technology.\n\n", "id": "18849138", "title": "YouNoodle"}
{"url": "https://en.wikipedia.org/wiki?curid=19852067", "text": "Pop music automation\n\nPop music automation is a field of study among musicians and computer scientists with a goal of producing successful pop music algorithmically. It is often based on the premise that pop music is especially formulaic, unchanging, and easy to compose. The idea of automating pop music composition is related to many ideas in algorithmic music, Artificial Intelligence (AI) and computational creativity.\n\nAlgorithms (or, at the very least, formal sets of rules) have been used to compose music for centuries; the procedures used to plot voice-leading in counterpoint, for example, can often be reduced to algorithmic determinacy. Now the term is usually reserved, however, for the use of formal procedures to make music without human intervention.\n\nClassical music automation software exists that generates music in the style of Mozart and Bach and jazz. Most notably, David Cope has written a software system called \"Experiments in Musical Intelligence\" (or \"EMI\") that is capable of analyzing and generalizing from existing music by a human composer to generate novel musical compositions in the same style. EMI's output is convincing enough to persuade human listeners that its music is human-generated to a high level of competence.\n\nCreativity research in jazz has focused on the process of improvisation and the cognitive demands that this places on a musical agent: reasoning about time, remembering and conceptualizing what has already been played, and planning ahead for what might be played next.\n\nInevitably associated with Pop music automation is Pop music analysis.\n\nProjects in Pop music automation may include, but are not limited to, ideas in melody creation and song development, vocal generation or improvement, automatic accompaniment and lyric composition.\n\nSome systems exist that automatically choose chords to accompany a vocal melody in real-time. A user with no musical experience can create a song with instrumental accompaniment just by singing into a microphone.\nAn example is a Microsoft Research project called Songsmith, which trains a Hidden Markov model using a music database and uses that\nmodel to select chords for new melodies.\n\nAutomatic melody generation is often done with a Markov chain, the states of the system become note or pitch values, and a probability vector for each note is constructed, completing a transition probability matrix (see below). An algorithm is constructed to produce and output note values based on the transition matrix weightings, which could be MIDI note values, frequency (Hz), or any other desirable metric.\n\nA second-order Markov chain can be introduced by considering the current state \"and\" also the previous state, as indicated in the second table. Higher, \"n\"th-order chains tend to \"group\" particular notes together, while 'breaking off' into other patterns and sequences occasionally. These higher-order chains tend to generate results with a sense of phrasal structure, rather than the 'aimless wandering' produced by a first-order system.\n\nAutomated lyric creating software may take forms such as:\n\nThe Tra-la-Lyrics system produces song lyrics, in Portuguese, for a given melody. This not only involves matching each word syllable with a note in the melody, but also matching the word's stress with the strong beats of the melody.\n\nThis involves natural language processing.\nPablo Gervás has developed a noteworthy system called ASPERA that employs a case-based reasoning (CBR) approach to generating poetic formulations of a given input text via a composition of poetic fragments that are retrieved from a case-base of existing poems. Each poem fragment in the ASPERA case-base is annotated with a prose string that expresses the meaning of the fragment, and this prose string is used as the retrieval key for each fragment. Metrical rules are then used to combine these fragments into a well-formed poetic structure.\n\nPrograms like TALE-SPIN\nOn-line metaphor generation systems like 'Sardonicus' or 'Aristotle' can suggest lexical metaphors for a given descriptive goal (e.g., to describe a supermodel as skinny, the source terms “pencil”, “whip”, “whippet”, “rope”, “stick-insect” and “snake” are suggested). \n\nUsing a language database (such as wordnet) one can create musings on a subject that may be weak grammatically but are still sensical. See such projects as the Flowerewolf automatic poetry generator or the \nDada engine.\n\n\n\n", "id": "19852067", "title": "Pop music automation"}
{"url": "https://en.wikipedia.org/wiki?curid=8168237", "text": "Roblog\n\nRoblog is a neologism for a blog written by a robot with no human intervention.\n\nRoblogs were made possible with a new generation of robots which are capable of uploading images and texts automatically to the Web. The first roblogs to appear, late 2005, were written by AIBO robots, the dog-like robotic pets once manufactured by Sony.\n\nAIBO diaries are roblogs produced by AIBO model ERS-7, running a bundled software called \"Mind\" in either version 2 or 3. Depending on the language of the Mind software, the AIBO blogs in either English or Japanese. To be able to blog on its own, an ERS-7M2 or ERS-7M3 must be linked to the Internet through its Wi-Fi connection capability, and its e-mail sending capability must be correctly configured, for which an SMTP server not requiring authentication nor alternate ports is needed. Posts, consisting of pictures taken with the AIBO's color camera built into its nose, are then sent by e-mail to the blog.\n\n", "id": "8168237", "title": "Roblog"}
{"url": "https://en.wikipedia.org/wiki?curid=19980", "text": "Machine translation\n\nMachine translation, sometimes referred to by the abbreviation MT (not to be confused with computer-aided translation, machine-aided human translation (MAHT) or interactive translation) is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.\n\nOn a basic level, MT performs simple substitution of words in one language for words in another, but that alone usually cannot produce a good translation of a text because recognition of whole phrases and their closest counterparts in the target language is needed. Solving this problem with corpus statistical, and neural techniques is a rapidly growing field that is leading to better translations, handling differences in linguistic typology, translation of idioms, and the isolation of anomalies.\n\nCurrent machine translation software often allows for customization by domain or profession (such as weather reports), improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used. It follows that machine translation of government and legal documents more readily produces usable output than conversation or less standardised text.\n\nImproved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has unambiguously identified which words in the text are proper names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports).\n\nThe progress and potential of machine translation have been debated much through its history. Since the 1950s, a number of scholars have questioned the possibility of achieving fully automatic machine translation of high quality. Some critics claim that there are in-principle obstacles to automating the translation process.\n\nThe idea of machine translation may be traced back to the 17th century. In 1629, René Descartes proposed a universal language, with equivalent ideas in different tongues sharing one symbol. The field of \"machine translation\" appeared in Warren Weaver's Memorandum on Translation (1949). The first researcher in the field, Yehosha Bar-Hillel, began his research at MIT (1951). A Georgetown University MT research team followed (1951) with a public demonstration of its Georgetown-IBM experiment system in 1954. MT research programs popped up in Japan and Russia (1955), and the first MT conference was held in London (1956). Researchers continued to join the field as the Association for Machine Translation and Computational Linguistics was formed in the U.S. (1962) and the National Academy of Sciences formed the Automatic Language Processing Advisory Committee (ALPAC) to study MT (1964). Real progress was much slower, however, and after the ALPAC report (1966), which found that the ten-year-long research had failed to fulfill expectations, funding was greatly reduced. According to a 1972 report by the Director of Defense Research and Engineering (DDR&E), the feasibility of large-scale MT was reestablished by the success of the Logos MT system in translating military manuals into Vietnamese during that conflict.\n\nThe French Textile Institute also used MT to translate abstracts from and into French, English, German and Spanish (1970); Brigham Young University started a project to translate Mormon texts by automated translation (1971); and Xerox used SYSTRAN to translate technical manuals (1978). Beginning in the late 1980s, as computational power increased and became less expensive, more interest was shown in statistical models for machine translation. Various MT companies were launched, including Trados (1984), which was the first to develop and market translation memory technology (1989). The first commercial MT system for Russian / English / German-Ukrainian was developed at Kharkov State University (1991).\n\nMT on the web started with SYSTRAN Offering free translation of small texts (1996), followed by AltaVista Babelfish, which racked up 500,000 requests a day (1997). Franz-Josef Och (the future head of Translation Development AT Google) won DARPA's speed MT competition (2003). More innovations during this time included MOSES, the open-source statistical MT engine (2007), a text/SMS translation service for mobiles in Japan (2008), and a mobile phone with built-in speech-to-speech translation functionality for English, Japanese and Chinese (2009). Recently, Google announced that Google Translate translates roughly enough text to fill 1 million books in one day (2012).\n\nThe idea of using digital computers for translation of natural languages was proposed as early as 1946 by A. D. Booth and possibly others. Warren Weaver wrote an important memorandum \"Translation\" in 1949. The Georgetown experiment was by no means the first such application, and a demonstration was made in 1954 on the APEXC machine at Birkbeck College (University of London) of a rudimentary translation of English into French. Several papers on the topic were published at the time, and even articles in popular journals (see for example \"Wireless World\", Sept. 1955, Cleave and Zacharov). A similar application, also pioneered at Birkbeck College at the time, was reading and composing Braille texts by computer.\n\nThe human translation process may be described as:\n\nBehind this ostensibly simple procedure lies a complex cognitive operation. To decode the meaning of the source text in its entirety, the translator must interpret and analyse all the features of the text, a process that requires in-depth knowledge of the grammar, semantics, syntax, idioms, etc., of the source language, as well as the culture of its speakers. The translator needs the same in-depth knowledge to re-encode the meaning in the target language.\n\nTherein lies the challenge in machine translation: how to program a computer that will \"understand\" a text as a person does, and that will \"create\" a new text in the target language that \"sounds\" as if it has been written by a person.\n\nIn its most general application, this is beyond current technology. Though it works much faster, no automated translation program or procedure, with no human participation, can produce output even close to the quality a human translator can produce. What it can do, however, is provide a general, though imperfect, approximation of the original text, getting the \"gist\" of it (a process called \"gisting\"). This is sufficient for many purposes, including making best use of the finite and expensive time of a human translator, reserved for those cases in which total accuracy is indispensable.\n\nThis problem may be approached in a number of ways, through the evolution of which accuracy has improved.\n\nMachine translation can use a method based on linguistic rules, which means that words will be translated in a linguistic way – the most suitable (orally speaking) words of the target language will replace the ones in the source language.\n\nIt is often argued that the success of machine translation requires the problem of natural language understanding to be solved first.\n\nGenerally, rule-based methods parse a text, usually creating an intermediary, symbolic representation, from which the text in the target language is generated. According to the nature of the intermediary representation, an approach is described as interlingual machine translation or transfer-based machine translation. These methods require extensive lexicons with morphological, syntactic, and semantic information, and large sets of rules.\n\nGiven enough data, machine translation programs often work well enough for a native speaker of one language to get the approximate meaning of what is written by the other native speaker. The difficulty is getting enough data of the right kind to support the particular method. For example, the large multilingual corpus of data needed for statistical methods to work is not necessary for the grammar-based methods. But then, the grammar methods need a skilled linguist to carefully design the grammar that they use.\n\nTo translate between closely related languages, the technique referred to as rule-based machine translation may be used.\n\nThe rule-based machine translation paradigm includes transfer-based machine translation, interlingual machine translation and dictionary-based machine translation paradigms. This type of translation is used mostly in the creation of dictionaries and grammar programs. Unlike other methods, RBMT involves more information about the linguistics of the source and target languages, using the morphological and syntactic rules and semantic analysis of both languages. The basic approach involves linking the structure of the input sentence with the structure of the output sentence using a parser and an analyzer for the source language, a generator for the target language, and a transfer lexicon for the actual translation. RBMT's biggest downfall is that everything must be made explicit: orthographical variation and erroneous input must be made part of the source language analyser in order to cope with it, and lexical selection rules must be written for all instances of ambiguity. Adapting to new domains in itself is not that hard, as the core grammar is the same across domains, and the domain-specific adjustment is limited to lexical selection adjustment.\n\nTransfer-based machine translation is similar to interlingual machine translation in that it creates a translation from an intermediate representation that simulates the meaning of the original sentence. Unlike interlingual MT, it depends partially on the language pair involved in the translation.\n\nInterlingual machine translation is one instance of rule-based machine-translation approaches. In this approach, the source language, i.e. the text to be translated, is transformed into an interlingual language, i.e. a \"language neutral\" representation that is independent of any language. The target language is then generated out of the interlingua. One of the major advantages of this system is that the interlingua becomes more valuable as the number of target languages it can be turned into increases. However, the only interlingual machine translation system that has been made operational at the commercial level is the KANT system (Nyberg and Mitamura, 1992), which is designed to translate Caterpillar Technical English (CTE) into other languages.\n\nMachine translation can use a method based on dictionary entries, which means that the words will be translated as they are by a dictionary.\n\nStatistical machine translation tries to generate translations using statistical methods based on bilingual text corpora, such as the Canadian Hansard corpus, the English-French record of the Canadian parliament and EUROPARL, the record of the European Parliament. Where such corpora are available, good results can be achieved translating similar texts, but such corpora are still rare for many language pairs. The first statistical machine translation software was CANDIDE from IBM. Google used SYSTRAN for several years, but switched to a statistical translation method in October 2007. In 2005, Google improved its internal translation capabilities by using approximately 200 billion words from United Nations materials to train their system; translation accuracy improved. Google Translate and similar statistical translation programs work by detecting patterns in hundreds of millions of documents that have previously been translated by humans and making intelligent guesses based on the findings. Generally, the more human-translated documents available in a given language, the more likely it is that the translation will be of good quality. Newer approaches into Statistical Machine translation such as METIS II and PRESEMT use minimal corpus size and instead focus on derivation of syntactic structure through pattern recognition. With further development, this may allow statistical machine translation to operate off of a monolingual text corpus. SMT's biggest downfall includes it being dependent upon huge amounts of parallel texts, its problems with morphology-rich languages (especially with translating \"into\" such languages), and its inability to correct singleton errors.\n\nExample-based machine translation (EBMT) approach was proposed by Makoto Nagao in 1984. Example-based machine translation is based on the idea of analogy. In this approach, the corpus that is used is one that contains texts that have already been translated. Given a sentence that is to be translated, sentences from this corpus are selected that contain similar sub-sentential components. The similar sentences are then used to translate the sub-sentential components of the original sentence into the target language, and these phrases are put together to form a complete translation.\n\nHybrid machine translation (HMT) leverages the strengths of statistical and rule-based translation methodologies. Several MT organizations (such as Omniscien Technologies (formerly Asia Online), LinguaSys, Systran, and Polytechnic University of Valencia) claim a hybrid approach that uses both rules and statistics. The approaches differ in a number of ways:\nMore recently, with the advent of Neural MT, a new version of hybrid machine translation is emerging that combines the benefits of rules, statistical and neural machine translation. The approach allows benefitting from pre- and post-processing in a rule guided workflow as well as benefitting from NMT and SMT. The downside is the inherent complexity which makes the approach suitable only for specific use cases. One of the proponents of this approach for complex use cases is Omniscien Technologies.\n\nA deep learning based approach to MT, neural machine translation has made rapid progress in recent years, and Google has announced its translation services are now using this technology in preference to its previous statistical methods. Other providers including KantanMT, Omniscien Technologies and SDL have announced the deployment of neural machine translation technology in 2017 as well.\n\nWord-sense disambiguation concerns finding a suitable translation when a word can have more than one meaning. The problem was first raised in the 1950s by Yehoshua Bar-Hillel. He pointed out that without a \"universal encyclopedia\", a machine would never be able to distinguish between the two meanings of a word. Today there are numerous approaches designed to overcome this problem. They can be approximately divided into \"shallow\" approaches and \"deep\" approaches.\n\nShallow approaches assume no knowledge of the text. They simply apply statistical methods to the words surrounding the ambiguous word. Deep approaches presume a comprehensive knowledge of the word. So far, shallow approaches have been more successful.\n\nClaude Piron, a long-time translator for the United Nations and the World Health Organization, wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved:\n\nThe ideal deep approach would require the translation software to do all the research necessary for this kind of disambiguation on its own; but this would require a higher degree of AI than has yet been attained. A shallow approach which simply guessed at the sense of the ambiguous English phrase that Piron mentions (based, perhaps, on which kind of prisoner-of-war camp is more often mentioned in a given corpus) would have a reasonable chance of guessing wrong fairly often. A shallow approach that involves \"ask the user about each ambiguity\" would, by Piron's estimate, only automate about 25% of a professional translator's job, leaving the harder 75% still to be done by a human.\n\nOne of the major pitfalls of MT is its inability to translate non-standard language with the same accuracy as standard language. Heuristic or statistical based MT takes input from various sources in standard form of a language. Rule-based translation, by nature, does not include common non-standard usages. This causes errors in translation from a vernacular source or into colloquial language. Limitations on translation from casual speech present issues in the use of machine translation in mobile devices.\n\nName entities, in narrow sense, refer to concrete or abstract entities in the real world including people, organizations, companies, places etc. It also refers to expressing of time, space, quantity such as 1 July 2011, $79.99 and so on.\n\nNamed entities occur in the text being analyzed in statistical machine translation. The initial difficulty that arises in dealing with named entities is simply identifying them in the text. Consider the list of names common in a particular language to illustrate this – the most common names are different for each language and also are constantly changing. If named entities cannot be recognized by the machine translator, they may be erroneously translated as common nouns, which would most likely not affect the BLEU rating of the translation but would change the text's human readability. It is also possible that, when not identified, named entities will be omitted from the output translation, which would also have implications for the text's readability and message.\n\nAnother way to deal with named entities is to use transliteration instead of translation, meaning that you find the letters in the target language that most closely correspond to the name in the source language. There have been attempts to incorporate this into machine translation by adding a transliteration step into the translation procedure. However, these attempts still have their problems and have even been cited as worsening the quality of translation. Named entities were still identified incorrectly, with words not being transliterated when they should or being transliterated when they shouldn't. For example, for \"Southern California\" the first word should be translated directly, while the second word should be transliterated. However, machines would often transliterate both because they treated them as one entity. Words like these are hard for machine translators, even those with a transliteration component, to process.\n\nThe lack of attention to the issue of named entity translation has been recognized as potentially stemming from a lack of resources to devote to the task in addition to the complexity of creating a good system for named entity translation. One approach to named entity translation has been to transliterate, and not translate, those words. A second is to create a \"do-not-translate\" list, which has the same end goal – transliteration as opposed to translation. Both of these approaches still rely on the correct identification of named entities, however.\n\nA third approach to successful named entity translation is a class-based model. In this method, named entities are replaced with a token to represent the class they belong to. For example, \"Ted\" and \"Erica\" would both be replaced with \"person\" class token. In this way the statistical distribution and use of person names in general can be analyzed instead of looking at the distributions of \"Ted\" and \"Erica\" individually. A problem that the class based model solves is that the probability of a given name in a specific language will not affect the assigned probability of a translation. A study by Stanford on improving this area of translation gives the examples that different probabilities will be assigned to \"David is going for a walk\" and \"Ankit is going for a walk\" for English as a target language due to the different number of occurrences for each name in the training data. A frustrating outcome of the same study by Stanford (and other attempts to improve named recognition translation) is that many times, a decrease in the BLEU scores for translation will result from the inclusion of methods for named entity translation.\n\nSome work has been done in the utilization of multiparallel corpora, that is a body of text that has been translated into 3 or more languages. Using these methods, a text that has been translated into 2 or more languages may be utilized in combination to provide a more accurate translation into a third language compared with if just one of those source languages were used alone.\n\nAn ontology is a formal representation of knowledge which includes the concepts (such as objects, processes etc.) in a domain and some relations between them. If the stored information is of linguistic nature, one can speak of a lexicon.\nIn NLP, ontologies can be used as a source of knowledge for machine translation systems. With access to a large knowledge base, systems can be enabled to resolve many (especially lexical) ambiguities on their own.\nIn the following classic examples, as humans, we are able to interpret the prepositional phrase according to the context because we use our world knowledge, stored in our lexicons:\nA machine translation system initially would not be able to differentiate between the meanings because syntax does not change. With a large enough ontology as a source of knowledge however, the possible interpretations of ambiguous words in a specific context can be reduced.\nOther areas of usage for ontologies within NLP include information retrieval, information extraction and text summarization.\n\nThe ontology generated for the PANGLOSS knowledge-based machine translation system in 1993 may serve as an example of how an ontology for NLP purposes can be compiled:\n\nWhile no system provides the holy grail of fully automatic high-quality machine translation of unrestricted text, many fully automated systems produce reasonable output. The quality of machine translation is substantially improved if the domain is restricted and controlled.\n\nDespite their inherent limitations, MT programs are used around the world. Probably the largest institutional user is the European Commission. The MOLTO project, for example, coordinated by the University of Gothenburg, received more than 2.375 million euros project support from the EU to create a reliable translation tool that covers a majority of the EU languages. The further development of MT systems comes at a time when budget cuts in human translation may increase the EU's dependency on reliable MT programs. The European Commission contributed 3.072 million euros (via its ISA programme) for the creation of MT@EC, a statistical machine translation program tailored to the administrative needs of the EU, to replace a previous rule-based machine translation system.\n\nGoogle has claimed that promising results were obtained using a proprietary statistical machine translation engine. The statistical translation engine used in the Google language tools for Arabic <-> English and Chinese <-> English had an overall score of 0.4281 over the runner-up IBM's BLEU-4 score of 0.3954 (Summer 2006) in tests conducted by the National Institute for Standards and Technology.\n\nWith the recent focus on terrorism, the military sources in the United States have been investing significant amounts of money in natural language engineering. \"In-Q-Tel\" (a venture capital fund, largely funded by the US Intelligence Community, to stimulate new technologies through private sector entrepreneurs) brought up companies like Language Weaver. Currently the military community is interested in translation and processing of languages like Arabic, Pashto, and Dari. Within these languages, the focus is on key phrases and quick communication between military members and civilians through the use of mobile phone apps. The Information Processing Technology Office in DARPA hosts programs like TIDES and Babylon translator. US Air Force has awarded a $1 million contract to develop a language translation technology.\n\nThe notable rise of social networking on the web in recent years has created yet another niche for the application of machine translation software – in utilities such as Facebook, or instant messaging clients such as Skype, GoogleTalk, MSN Messenger, etc. – allowing users speaking different languages to communicate with each other. Machine translation applications have also been released for most mobile devices, including mobile telephones, pocket PCs, PDAs, etc. Due to their portability, such instruments have come to be designated as mobile translation tools enabling mobile business networking between partners speaking different languages, or facilitating both foreign language learning and unaccompanied traveling to foreign countries without the need of the intermediation of a human translator.\n\nDespite being labelled as an unworthy competitor to human translation in 1966 by the Automated Language Processing Advisory Committee put together by the United States government, the quality of machine translation has now been improved to such levels that its application in online collaboration and in the medical field are being investigated. In the Ishida and Matsubara lab of Kyoto University, methods of improving the accuracy of machine translation as a support tool for inter-cultural collaboration in today's globalized society are being studied. The application of this technology in medical settings where human translators are absent is another topic of research however difficulties arise due to the importance of accurate translations in medical diagnoses.\n\nThere are many factors that affect how machine translation systems are evaluated. These factors include the intended use of the translation, the nature of the machine translation software, and the nature of the translation process.\n\nDifferent programs may work well for different purposes. For example, statistical machine translation (SMT) typically outperforms example-based machine translation (EBMT), but researchers found that when evaluating English to French translation, EBMT performs better. The same concept applies for technical documents, which can be more easily translated by SMT because of their formal language.\n\nIn certain applications, however, e.g., product descriptions written in a controlled language, a dictionary-based machine-translation system has produced satisfactory translations that require no human intervention save for quality inspection.\n\nThere are various means for evaluating the output quality of machine translation systems. The oldest is the use of human judges to assess a translation's quality. Even though human evaluation is time-consuming, it is still the most reliable method to compare different systems such as rule-based and statistical systems. Automated means of evaluation include BLEU, NIST, METEOR, and LEPOR.\n\nRelying exclusively on unedited machine translation ignores the fact that communication in human language is context-embedded and that it takes a person to comprehend the context of the original text with a reasonable degree of probability. It is certainly true that even purely human-generated translations are prone to error. Therefore, to ensure that a machine-generated translation will be useful to a human being and that publishable-quality translation is achieved, such translations must be reviewed and edited by a human. The late Claude Piron wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved. Such research is a necessary prelude to the pre-editing necessary in order to provide input for machine-translation software such that the output will not be meaningless.\n\nIn addition to disambiguation problems, decreased accuracy can occur due to varying levels of training data for machine translating programs. Both example-based and statistical machine translation rely on a vast array of real example sentences as a base for translation, and when too many or too few sentences are analyzed accuracy is jeopardized. Researchers found that when a program is trained on 203,529 sentence pairings, accuracy actually decreases. The optimal level of training data seems to be just over 100,000 sentences, possibly because as training data increases, the number of possible sentences increases, making it harder to find an exact translation match.\n\nAlthough there have been concerns about machine translation's accuracy, Dr. Ana Nino of the University of Manchester has researched some of the advantages in utilizing machine translation in the classroom. One such pedagogical method is called using \"MT as a Bad Model.\" MT as a Bad Model forces the language learner to identify inconsistencies or incorrect aspects of a translation; in turn, the individual will (hopefully) possess a better grasp of the language. Dr. Nino cites that this teaching tool was implemented in the late 1980s. At the end of various semesters, Dr. Nino was able to obtain survey results from students who had used MT as a Bad Model (as well as other models.) Overwhelmingly, students felt that they had observed improved comprehension, lexical retrieval, and increased confidence in their target language.\n\nIn the early 2000s, options for machine translation between spoken and signed languages were severely limited. It was a common belief that deaf individuals could use traditional translators. However, stress, intonation, pitch, and timing are conveyed much differently in spoken languages compared to signed languages. Therefore, a deaf individual may misinterpret or become confused about the meaning of written text that is based on a spoken language.\n\nResearchers Zhao, et al. (2000), developed a prototype called TEAM (translation from English to ASL by machine) that completed English to American Sign Language (ASL) translations. The program would first analyze the syntactic, grammatical, and morphological aspects of the English text. Following this step, the program accessed a sign synthesizer, which acted as a dictionary for ASL. This synthesizer housed the process one must follow to complete ASL signs, as well as the meanings of these signs. Once the entire text is analyzed and the signs necessary to complete the translation are located in the synthesizer, a computer generated human appeared and would use ASL to sign the English text to the user.\n\nOnly works that are original are subject to copyright protection, so some scholars claim that machine translation results are not entitled to copyright protection because MT does not involve creativity. The copyright at issue is for a derivative work; the author of the original work in the original language does not lose his rights when a work is translated: a translator must have permission to publish a translation.\n\n\n\n", "id": "19980", "title": "Machine translation"}
{"url": "https://en.wikipedia.org/wiki?curid=22419840", "text": "Imense\n\nImense Ltd is a UK-based company that develops technology for Content-based image retrieval and automatic image annotation.\n\nThe founders of Imense are Dr Christopher Town and Dr David Sinclair. In their academic lives they developed the\nfirst 'Ontological Query Evaluation Language' (OQUEL) for image retrieval, which mapped a plain text user query onto a query over automatically recognized visual content in a corpus of images.\n\nTechnology derived in spirit from OQUEL is in routine use on the Imense PictureSearch portal.\nThe user interface allows a user to type a plain text query that is probabilistically parsed to recognise visual aspects (like 'purple center green background' or 'group of five people')\nand non visual aspects (e.g. 'freedom' or 'Buddhism' or 'Parma ham'). \nThe issues associated with scaling up image search to cope with tens of millions or more images were addressed with active\nsupport from the Science and Technology Facilities Council and GridPP. \nNews articles about Imense search technology include.\n\nKey papers describing the birth and evolution of ontological query languages include: \nThe research focus of Imense Ltd remains ontology based image content recognition. Imense uses cutting edge techniques from machine learning\nto build and train extremely-high-dimensional classifiers to help semantically label things in the visual world. Imense appears to use different types of visual models for different object classes — for example, Bayesian Constrained Local Models for parametric face modeling, and SVMs for general semantic content labeling.\n\n\n", "id": "22419840", "title": "Imense"}
{"url": "https://en.wikipedia.org/wiki?curid=743971", "text": "Content-based image retrieval\n\nContent-based image retrieval (CBIR), also known as query by image content (QBIC) and content-based visual information retrieval (CBVIR) is the application of computer vision techniques to the image retrieval problem, that is, the problem of searching for digital images in large databases (see this survey for a recent scientific overview of the CBIR field). Content-based image retrieval is opposed to traditional concept-based approaches (see Concept-based image indexing).\n\n\"Content-based\" means that the search analyzes the contents of the image rather than the metadata such as keywords, tags, or descriptions associated with the image. The term \"content\" in this context might refer to colors, shapes, textures, or any other information that can be derived from the image itself. CBIR is desirable because searches that rely purely on metadata are dependent on annotation quality and completeness. Having humans manually annotate images by entering keywords or metadata in a large database can be time consuming and may not capture the keywords desired to describe the image. The evaluation of the effectiveness of keyword image search is subjective and has not been well-defined. In the same regard, CBIR systems have similar challenges in defining success.\n\nThe term \"content-based image retrieval\" seems to have originated in 1992 when it was used by T. Kato to describe experiments into automatic retrieval of images from a database, based on the colors and shapes present. Since then, the term has been used to describe the process of retrieving desired images from a large collection on the basis of syntactical image features. The techniques, tools, and algorithms that are used originate from fields such as statistics, pattern recognition, signal processing, and computer vision\n\nThe earliest commercial CBIR system was developed by IBM and was called QBIC (Query by Image Content). Recent network and graph based approaches have presented a simple and attractive alternative to existing methods.\n\nThe interest in CBIR has grown because of the limitations inherent in metadata-based systems, as well as the large range of possible uses for efficient image retrieval. Textual information about images can be easily searched using existing technology, but this requires humans to manually describe each image in the database. This can be impractical for very large databases or for images that are generated automatically, e.g. those from surveillance cameras. It is also possible to miss images that use different synonyms in their descriptions. Systems based on categorizing images in semantic classes like \"cat\" as a subclass of \"animal\" can avoid the miscategorization problem, but will require more effort by a user to find images that might be \"cats\", but are only classified as an \"animal\". Many standards have been developed to categorize images, but all still face scaling and miscategorization issues.\n\nInitial CBIR systems were developed to search databases based on image color, texture, and shape properties. After these systems were developed, the need for user-friendly interfaces became apparent. Therefore, efforts in the CBIR field started to include human-centered design that tried to meet the needs of the user performing the search. This typically means inclusion of: query methods that may allow descriptive semantics, queries that may involve user feedback, systems that may include machine learning, and systems that may understand user satisfaction levels.\n\nMany CBIR systems have been developed, but the problem of retrieving images on the basis of their pixel content remains largely unsolved.\n\nDifferent implementations of CBIR make use of different types of user queries.\n\nQuery by example is a query technique that involves providing the CBIR system with an example image that it will then base its search upon. The underlying search algorithms may vary depending on the application, but result images should all share common elements with the provided example.\n\nOptions for providing example images to the system include:\n\n\nThis query technique removes the difficulties that can arise when trying to describe images with words.\n\n\"Semantic\" retrieval starts with a user making a request like \"find pictures of Abraham Lincoln\". This type of open-ended task is very difficult for computers to perform - Lincoln may not always be facing the camera or in the same pose. Many CBIR systems therefore generally make use of lower-level features like texture, color, and shape. These features are either used in combination with interfaces that allow easier input of the criteria or with databases that have already been trained to match features (such as faces, fingerprints, or shape matching). However, in general, image retrieval requires human feedback in order to identify higher-level concepts.\n\nCombining CBIR search techniques available with the wide range of potential users and their intent can be a difficult task. An aspect of making CBIR successful relies entirely on the ability to understand the user intent. CBIR systems can make use of \"relevance feedback\", where the user progressively refines the search results by marking images in the results as \"relevant\", \"not relevant\", or \"neutral\" to the search query, then repeating the search with the new information. Examples of this type of interface have been developed.\n\nMachine learning and application of iterative techniques are becoming more common in CBIR.\n\nOther query methods include browsing for example images, navigating customized/hierarchical categories, querying by image region (rather than the entire image), querying by multiple example images, querying by visual sketch, querying by direct specification of image features, and multimodal queries (e.g. combining touch, voice, etc.)\n\nThe most common method for comparing two images in content-based image retrieval (typically an example image and an image from the database) is using an image distance measure. An image distance measure compares the similarity of two images in various dimensions such as color, texture, shape, and others. For example, a distance of 0 signifies an exact match with the query, with respect to the dimensions that were considered. As one may intuitively gather, a value greater than 0 indicates various degrees of similarities between the images. Search results then can be sorted based on their distance to the queried image. Many measures of image distance (Similarity Models) have been developed.\n\nComputing distance measures based on color similarity is achieved by computing a color histogram for each image that identifies the proportion of pixels within an image holding specific values. Examining images based on the colors they contain is one of the most widely used techniques because it can be completed without regard to image size or orientation. However, research has also attempted to segment color proportion by region and by spatial relationship among several color regions.\n\nTexture measures look for visual patterns in images and how they are spatially defined. Textures are represented by texels which are then placed into a number of sets, depending on how many textures are detected in the image. These sets not only define the texture, but also where in the image the texture is located.\n\nTexture is a difficult concept to represent. The identification of specific textures in an image is achieved primarily by modeling texture as a two-dimensional gray level variation. The relative brightness of pairs of pixels is computed such that degree of contrast, regularity, coarseness and directionality may be estimated. The problem is in identifying patterns of co-pixel variation and associating them with particular classes of textures such as \"silky\", or \"rough\".\n\nOther methods of classifying textures include:\n\n\nShape does not refer to the shape of an image but to the shape of a particular region that is being sought out. Shapes will often be determined first applying segmentation or edge detection to an image. Other methods use shape filters to identify given shapes of an image. Shape descriptors may also need to be invariant to translation, rotation, and scale.\n\nSome shape descriptors include:\n\n\nMeasures of image retrieval can be defined in terms of precision and recall. However, there are other methods being considered.\n\nAn image is retrieved in CBIR system by adopting several techniques simultaneously such as Integrating Pixel Cluster Indexing, histogram intersection and discrete wavelet transform methods.\n\nPotential uses for CBIR include:\n\n\nCommercial Systems that have been developed include:\n\nExperimental Systems include:\n\n\n\n", "id": "743971", "title": "Content-based image retrieval"}
{"url": "https://en.wikipedia.org/wiki?curid=22717006", "text": "GestureTek\n\nGestureTek is an American-based interactive technology company headquartered in Silicon Valley, California, with offices in Toronto and Ottawa, Ontario and Asia.\n\nFounded in 1986 by Canadians Vincent John Vincent and Francis MacDougall, this privately held company develops and licenses gesture recognition software based on computer vision techniques. The partners invented video gesture control in 1986 and received their base patent in 1996 for the GestPoint video gesture control system. GestPoint technology is a camera-enabled video tracking software system that translates hand and body movement into computer control. The system enables users to navigate and control interactive multi-media and menu-based content, engage in virtual reality game play, experience immersion in an augmented reality environment or interact with a consumer device (such a television, mobile phone or set top box) without using touch-based peripherals.\nSimilar companies include gesture recognition specialist LM3LABS based in Tokyo, Japan.\n\nGestureTek’s gesture interface applications include multi-touch and 3D camera tracking. GestureTek’s multi-touch technology powers the multi-touch table in Melbourne’s Eureka Tower. A GestureTek multi-touch table with object recognition is found at the New York City Visitors Center. Telefónica has a multi-touch window with technology from GestureTek. GestureTek’s 3D tracking technology is used in a 3D television prototype from Hitachi and various digital signage and display solutions based on 3D interaction.\n\nGestureTek currently has 8 patents awarded, including: 5,534,917 (Video Gesture Control Motion Detection); 7,058,204 (Multiple Camera Control System, Point to Control Base Patent); 7,421,093 (Multiple Camera Tracking System for Interfacing With an Application); 7,227,526 (Stereo Camera Control, 3D-Vision Image Control System); 7,379,563 (Two Handed Movement Tracker Tracking Bi-Manual Movements); 7,379,566 (Optical Flow-Based Tilt Sensor For Phone Tilt Control); 7,389,591 (Phone Tilt for Typing & Menus/Orientation-Sensitive Signal Output); 7,430,312 (Five Camera 3D Face Capture).\n\nGestureTek’s software and patents have been licensed by Microsoft for the Xbox 360, Sony for the EyeToy, NTT DoCoMo for their mobile phones and Hasbro for the ION Educational Gaming System. In addition to software provision, GestureTek also fabricates interactive gesture control display systems with natural user interface for interactive advertising, games and presentations.\n\nIn addition, GestureTek’s natural user interface virtual reality system has been the subject of research by universities and hospitals for its application in both physical therapy and physical rehabilitation.\n\nIn 2008, GestureTek received the Mobile Innovation Global Award from the GSMA for its software-based, gesture-controlled user interface for mobile games and applications. The technology is used by Java platform integration providers and mobile developers. Katamari Damacy is one example of a gesture control mobile game powered by GestureTek software.\n\nOther companies in the industry of interactive projections for marketing and retail experiences include Po-motion Inc., Touchmagix and LM3LABS.\n", "id": "22717006", "title": "GestureTek"}
{"url": "https://en.wikipedia.org/wiki?curid=1657551", "text": "Automatic number-plate recognition\n\nAutomatic number-plate recognition (ANPR; see also other names below) is a technology that uses optical character recognition on images to read vehicle registration plates to create vehicle location data. It can use existing closed-circuit television, road-rule enforcement cameras, or cameras specifically designed for the task. ANPR is used by police forces around the world for law enforcement purposes, including to check if a vehicle is registered or licensed. It is also used for electronic toll collection on pay-per-use roads and as a method of cataloguing the movements of traffic, for example by highways agencies.\n\nAutomatic number plate recognition can be used to store the images captured by the cameras as well as the text from the license plate, with some configurable to store a photograph of the driver. Systems commonly use infrared lighting to allow the camera to take the picture at any time of day or night. ANPR technology must take into account plate variations from place to place.\n\nConcerns about these systems have centered on privacy fears of government tracking citizens' movements, misidentification, high error rates, and increased government spending. Critics have described it as a form of mass surveillance.\n\nANPR is sometimes known by various other terms:\nANPR was invented in 1976 at the Police Scientific Development Branch in the UK. Prototype systems were working by 1979, and contracts were awarded to produce industrial systems, first at EMI Electronics, and then at Computer Recognition Systems (CRS) in Wokingham, UK. Early trial systems were deployed on the A1 road and at the Dartford Tunnel. The first arrest through detection of a stolen car was made in 1981. However, ANPR did not become widely used until new developments in cheaper and easier to use software were pioneered during the 1990s. The collection of ANPR data for future use (\"i.e\"., in solving then-unidentified crimes) was documented in the early 2000s. The first documented case of ANPR being used to help solve a murder occurred in November 2005, in Bradford, UK, where ANPR played a vital role in locating and subsequently convicting killers of Sharon Beshenivsky.\n\nThe software aspect of the system runs on standard home computer hardware and can be linked to other applications or databases. It first uses a series of image manipulation techniques to detect, normalize and enhance the image of the number plate, and then optical character recognition (OCR) to extract the alphanumerics of the license plate. ANPR systems are generally deployed in one of two basic approaches: one allows for the entire process to be performed at the lane location in real-time, and the other transmits all the images from many lanes to a remote computer location and performs the OCR process there at some later point in time. When done at the lane site, the information captured of the plate alphanumeric, date-time, lane identification, and any other information required is completed in approximately 250 milliseconds. This information can easily be transmitted to a remote computer for further processing if necessary, or stored at the lane for later retrieval. In the other arrangement, there are typically large numbers of PCs used in a server farm to handle high workloads, such as those found in the London congestion charge project. Often in such systems, there is a requirement to forward images to the remote server, and this can require larger bandwidth transmission media.\n\nANPR uses optical character recognition (OCR) on images taken by cameras. When Dutch vehicle registration plates switched to a different style in 2002, one of the changes made was to the font, introducing small gaps in some letters (such as \"P\" and \"R\") to make them more distinct and therefore more legible to such systems. Some license plate arrangements use variations in font sizes and positioning—ANPR systems must be able to cope with such differences in order to be truly effective. More complicated systems can cope with international variants, though many programs are individually tailored to each country.\n\nThe cameras used can be existing road-rule enforcement or closed-circuit television cameras, as well as mobile units, which are usually attached to vehicles. Some systems use infrared cameras to take a clearer image of the plates.\n\nDuring the 1990s, significant advances in technology took automatic number plate recognition (ANPR) systems from limited expensive, hard to set up, fixed based applications to simple \"point and shoot\" mobile ones. This was made possible by the creation of software that ran on cheaper PC based, non-specialist hardware that also no longer needed to be given the pre-defined angles, direction, size and speed in which the plates would be passing the camera's field of view. Further scaled-down components at more cost-effective price points led to a record number of deployments by law enforcement agencies around the world. Smaller cameras with the ability to read license plates at higher speeds, along with smaller, more durable processors that fit in the trunks of police vehicles, allowed law enforcement officers to patrol daily with the benefit of license plate reading in real time, when they can interdict immediately.\n\nDespite their effectiveness, there are noteworthy challenges related with mobile ANPRs. One of the biggest is that the processor and the cameras must work fast enough to accommodate relative speeds of more than 100 mph (160 km/h), a likely scenario in the case of oncoming traffic. This equipment must also be very efficient since the power source is the vehicle battery, and equipment must be small to minimize the space it requires.\n\nRelative speed is only one issue that affects the camera's ability to actually read a license plate. Algorithms must be able to compensate for all the variables that can affect the ANPR's ability to produce an accurate read, such as time of day, weather and angles between the cameras and the license plates. A system's illumination wavelengths can also have a direct impact on the resolution and accuracy of a read in these conditions.\n\nInstalling ANPR cameras on law enforcement vehicles requires careful consideration of the juxtaposition of the cameras to the license plates they are to read. Using the right number of cameras and positioning them accurately for optimal results can prove challenging, given the various missions and environments at hand. Highway patrol requires forward-looking cameras that span multiple lanes and are able to read license plates at very high speeds. City patrol needs shorter range, lower focal length cameras for capturing plates on parked cars. Parking lots with perpendicularly parked cars often require a specialized camera with a very short focal length. Most technically advanced systems are flexible and can be configured with a number of cameras ranging from one to four which can easily be repositioned as needed. States with rear-only license plates have an additional challenge since a forward-looking camera is ineffective with oncoming traffic. In this case one camera may be turned backwards.\n\nThere are seven primary algorithms that the software requires for identifying a license plate:\n\nThe complexity of each of these subsections of the program determines the accuracy of the system. During the third phase (normalization), some systems use edge detection techniques to increase the picture difference between the letters and the plate backing. A median filter may also be used to reduce the visual noise on the image.\n\nThere are a number of possible difficulties that the software must be able to cope with. These include:\n\nWhile some of these problems can be corrected within the software, it is primarily left to the hardware side of the system to work out solutions to these difficulties. Increasing the height of the camera may avoid problems with objects (such as other vehicles) obscuring the plate but introduces and increases other problems, such as the adjusting for the increased skew of the plate.\n\nOn some cars, tow bars may obscure one or two characters of the license plate. Bikes on bike racks can also obscure the number plate, though in some countries and jurisdictions, such as Victoria, Australia, \"bike plates\" are supposed to be fitted.\nSome small-scale systems allow for some errors in the license plate. When used for giving specific vehicles access to a barricaded area, the decision may be made to have an acceptable error rate of one character. This is because the likelihood of an unauthorized car having such a similar license plate is seen as quite small. However, this level of inaccuracy would not be acceptable in most applications of an ANPR system.\n\nAt the front end of any ANPR system is the imaging hardware which captures the image of the license plates. The initial image capture forms a critically important part of the ANPR system which, in accordance to the garbage in, garbage out principle of computing, will often determine the overall performance.\n\nLicense plate capture is typically performed by specialized cameras designed specifically for the task, although new software techniques are being implemented that support any I.P.-based surveillance camera and increase the utility of ANPR for perimeter security applications. Factors which pose difficulty for license plate imaging cameras include the speed of the vehicles being recorded, varying level of ambient light, headlight glare and harsh environmental conditions. Most dedicated license plate capture cameras will incorporate infrared illumination in order to solve the problems of lighting and plate reflectivity.\nMany countries now use license plates that are retroreflective. This returns the light back to the source and thus improves the contrast of the image. In some countries, the characters on the plate are not reflective, giving a high level of contrast with the reflective background in any lighting conditions. A camera that makes use of active infrared imaging (with a normal colour filter over the lens and an infrared illuminator next to it) benefits greatly from this as the infrared waves are reflected back from the plate. This is only possible on dedicated ANPR cameras, however, and so cameras used for other purposes must rely more heavily on the software capabilities. Further, when a full-colour image is required as well as use of the ANPR-retrieved details, it is necessary to have one infrared-enabled camera and one normal (colour) camera working together.\n\nTo avoid blurring it is ideal to have the shutter speed of a dedicated camera set to 1/1000 of a second. It is also important that the camera uses a global shutter, as opposed to rolling shutter, to assure that the taken images are distortion-free. Because the car is moving, slower shutter speeds could result in an image which is too blurred to read using the OCR software, especially if the camera is much higher up than the vehicle. In slow-moving traffic, or when the camera is at a lower level and the vehicle is at an angle approaching the camera, the shutter speed does not need to be so fast. Shutter speeds of 1/500 of a second can cope with traffic moving up to 40 mph (64 km/h) and 1/250 of a second up to 5 mph (8 km/h). License plate capture cameras can produce usable images from vehicles traveling at .\n\nTo maximize the chances of effective license plate capture, installers should carefully consider the positioning of the camera relative to the target capture area. Exceeding threshold angles of incidence between camera lens and license plate will greatly reduce the probability of obtaining usable images due to distortion. Manufacturers have developed tools to help eliminate errors from the physical installation of license plate capture cameras.\n\nSeveral State Police Forces, and the Department of Justice (Victoria) use both fixed and mobile ANPR systems. The New South Wales Police Force Highway Patrol were the first to trial and use a fixed ANPR camera system in Australia in 2005. In 2009 they began a roll-out of a mobile ANPR system (known officially as MANPR) with three infrared cameras fitted to its Highway Patrol fleet. The system identifies unregistered and stolen vehicles as well as disqualified or suspended drivers as well as other 'persons of interest' such as persons having outstanding warrants.\n\nThe city of Mechelen uses an ANPR system since September 2011 to scan all cars crossing the city limits (inbound and outbound). Cars listed on 'black lists' (no insurance, stolen, etc.) generate an alarm in the dispatching room, so they can be intercepted by a patrol.\nAs of early 2012, 1 million cars per week are automatically checked in this way.\n\nThe police service in Ontario uses automatic licence plate recognition software to nab drivers behind the wheels of vehicles with Ontario number plates.\n\nThe technique is tested by the Danish police. It will be in permanent use from the end of 2015.\n\n180 gantries over major roads have been built throughout the country. These together with a further 250 fixed cameras is to enable a levy of an eco tax on lorries over 3.5 tonnes. The system is currently being opposed and whilst they may be collecting data on vehicles passing the cameras, no eco tax is being charged.\n\nOn 11 March 2008, the Federal Constitutional Court of Germany ruled that some areas of the laws permitting the use of automated number plate recognition systems in Germany violated the right to privacy. More specifically, the court found that the retention of any sort of information (i.e., number plate data) which was not for any pre-destined use (e.g., for use tracking suspected terrorists or for enforcement of speeding laws) was in violation of German law.\nThese systems were provided by Jenoptik Robot GmbH, and called TraffiCapture.\n\nIn 2012 a state consortium was formed among the Hungarian Ministry of Interior, the National Police Headquarters and the Central Commission of Public Administration and Electronic Services with the aim to install and operate a unified intelligent transportation system (\"ITS\") with nationwide coverage by the end of 2015. Within the system, 160 portable traffic enforcement and data-gathering units and 365 permanent gantry installations were brought online with ANPR, speed detection, imaging and statistical capabilities. Since all the data points are connected to a centrally located ITS, each member of the consortium is able to separately utilize its range of administrative and enforcement activities, such as remote vehicle registration and insurance verification, speed, lane and traffic light enforcement and wanted or stolen vehicle interception among others.\n\nSeveral Hungarian auxiliary police units also use a system called Matrix Police in cooperation with the police. It consists of a portable computer equipped with a web camera that scans the stolen car database using automatic number plate recognition. The system is installed on the dashboard of selected patrol vehicles (PDA-based hand-held versions also exist) and is mainly used to control the license plate of parking cars. As the Auxiliary Police do not have the authority to order moving vehicles to stop, if a stolen car is found, the formal police is informed.\n\nThe technique is proposed by Lahore Police; if approved it will be functional by 2018.\n\nVehicle registration plates in Saudi Arabia use white background, but several vehicle types may have a different background. United States diplomatic plates have the letters 'USD', which in Arabic reads 'DSU' when read from right to left in the direction of Arabic script. There are only 17 Arabic letters used on the registration plates. A Challenge for plates recognition in Saudi Arabia is the size of the digits. Some plates use both Eastern Arabic numerals and the 'Western Arabic' equivalents. A research with source code is available for APNR Arabic digits.\n\nThe technique is tested by the Swedish police at nine different places in Sweden.\n\nSeveral cities have tested—and some have put into service—the KGYS (Kent Guvenlik Yonetim Sistemi, City Security Administration System), , i.e., capital Ankara, has debuted KGYS- which consists of a registration plate number recognition system on the main arteries and city exits. The system has been used with two cameras per lane, one for plate recognition, one for speed detection. Now the system has been widened to network all the registration number cameras together, and enforcing average speed over preset distances. Some arteries have limit, and some , and photo evidence with date-time details are posted to registration address if speed violation is detected. As of 2012, the fine for exceeding the speed limit for more than 30% is approximately US$175.\n\nThe project of system integration «OLLI Technology» and the Ministry of Internal Affairs of Ukraine Department of State Traffic Inspection (STI) experiments on the introduction of a modern technical complex which is capable to locate stolen cars, drivers deprived of driving licenses and other problem cars in real time. The Ukrainian complex \"Video control\" working by a principle of video fixing of the car with recognition of license plates with check under data base.\n\nThe Home Office states the purpose of automatic number plate recognition in the United Kingdom is to help detect, deter and disrupt criminality including tackling organised crime groups and terrorists. Vehicle movements are recorded by a network of nearly 8000 cameras capturing between 25 and 30 million ANPR ‘read’ records daily. These records are stored for up to two years in the National ANPR Data Center, which can be accessed, analysed and used as evidence as part of investigations by UK law enforcement agencies.\n\nIn 2012, the UK Parliament enacted the Protection of Freedoms Act which includes several provisions related to controlling and restricting the collection, storage, retention, and use of information about individuals. Under this Act, the Home Office published a code of practice in 2013 for the use of surveillance cameras, including ANPR, by government and law enforcement agencies. The aim of the code is to help ensure their use is \"characterised as surveillance by consent, and such consent on the part of the community must be informed consent and not assumed by a system operator. Surveillance by consent should be regarded as analogous to policing by consent.\" In addition, a set a standards were introduced in 2014 for data, infrastructure, and data access and management.\n\nIn the United States, ANPR systems are more commonly referred to as ALPR (Automatic License Plate Reader/Recognition) technology, due to differences in language (i.e., \"number plates\" are referred to as \"license plates\" in American English)\n\nMobile ANPR use is widespread among US law enforcement agencies at the city, county, state and federal level. According to a 2012 report by the Police Executive Research Forum, approximately 71% of all US police departments use some form of\nANPR. Mobile ANPR is becoming a significant component of municipal predictive policing strategies and intelligence gathering, as well as for recovery of stolen vehicles, identification of wanted felons, and revenue collection from individuals who are delinquent on city or state taxes or fines, or monitoring for \"Amber Alerts\". With the widespread implementation of this technology, many U.S. states now issue misdemeanor citations of up to $500 when a license plate is identified as expired or on the incorrect vehicle. Successfully recognized plates may be matched against databases including \"wanted person\", \"protection order\", missing person, gang member, known and suspected terrorist, supervised release, immigration violator, and National Sex Offender lists. In addition to the real-time processing of license plate numbers, ANPR systems in the US collect (and can indefinitely store) data from each license plate capture. Images, dates, times and GPS coordinates can be stockpiled and can help place a suspect at a scene, aid in witness identification, pattern recognition or the tracking of individuals.\n\nThe Department of Homeland Security has proposed a federal database to combine all monitoring systems, which was cancelled after privacy complaints. In 1998, a Washington D.C. police lieutenant pleaded guilty to extortion after blackmailing the owners of vehicles parked near a gay bar. In 2015, the Los Angeles Police Department proposed sending letters to the home addresses of all vehicles that enter areas of high prostitution.\n\nAn early, private sector mobile ANPR application has been applications for vehicle repossession and recovery), although the application of ANPR by private companies to collect information from privately owned vehicles or collected from private property (for example, driveways) has become an issue of sensitivity and public debate. Other ANPR uses include parking enforcement, and revenue collection from individuals who are delinquent on city or state taxes or fines. The technology is often featured in the reality TV show \"Parking Wars\" featured on A&E Network. In the show, tow truck drivers and booting teams use the ANPR to find delinquent vehicles with high amounts of unpaid parking fines.\n\nANPR is used for speed limit enforcement in Australia, Austria, Belgium, Dubai (UAE), France, Italy, The Netherlands, Spain, South Africa, the UK, and Kuwait.\n\nThis works by tracking vehicles' travel time between two fixed points, and calculating the average speed. These cameras are claimed to have an advantage over traditional speed cameras in maintaining steady legal speeds over extended distances, rather than encouraging heavy braking on approach to specific camera locations and subsequent acceleration back to illegal speeds.\n\nIn has developed a monitoring system named covering more than 2500 km (2012). The Tutor system is also able to intercept cars while changing lanes.\n\nAverage speed cameras (\"trajectcontrole\") are in place in the Netherlands since 2002. As of July 2009, 12 cameras were operational, mostly in the west of the country and along the A12. Some of these are divided in several “sections” to allow for cars leaving and entering the motorway.\n\nA first experimental system was tested on a short stretch of the A2 in 1997 and was deemed a big success by the police, reducing overspeeding to 0.66%, compared to 5 to 6% when regular speed cameras were used at the same location. The first permanent average speed cameras were installed on the A13 in 2002, shortly after the speed limit was reduced to 80 km/h to limit noise and air pollution in the area. In 2007, average speed cameras resulted in 1.7 million fines for overspeeding out of a total of 9.7 millions. According to the Dutch Attorney General, the average number of violation of the speed limits on motorway sections equipped with average speed cameras is between 1 and 2%, compared to 10 to 15% elsewhere.\n\nOne of the most notable stretches of average speed cameras in the UK is found on the A77 road in Scotland, with being monitored between Kilmarnock and Girvan. In 2006 it was confirmed that speeding tickets could potentially be avoided from the 'SPECS' cameras by changing lanes and the RAC Foundation feared that people may play \"Russian Roulette\" changing from one lane to another to lessen their odds of being caught. However, in 2007 the system was upgraded for multi-lane use and in 2008 the manufacturer described the \"myth\" as “categorically untrue”. There exists evidence that implementation of systems such as SPECS has a considerable effect on the volume of drivers travelling at excessive speeds; on the stretch of road mentioned above (A77 Between Glasgow and Ayr) there has been noted a \"huge drop\" in speeding violations since the introduction of a SPECS system.\n\nRecent innovations have contributed to the adoption of ANPR for perimeter security and access control applications at government facilities. Within the US, \"homeland security\" efforts to protect against alleged \"acts of terrorism\" have resulted in adoption of ANPR for sensitive facilities such as embassies, schools, airports, maritime ports, military and federal buildings, law enforcement and government facilities, and transportation centers. ANPR is marketed as able to be implemented through networks of IP based surveillance cameras that perform \"double duty\" alongside facial recognition, object tracking, and recording systems for the purpose of monitoring suspicious or anomalous behavior, improving access control, and matching against watch lists. ANPR systems are most commonly installed at points of significant sensitivity, ingress or egress. Major US agencies such as the Department of Homeland Security, the Department of Justice, the Department of Transportation and the Department of Defense have purchased ANPR for perimeter security applications. Large networks of ANPR systems are being installed by cities such as Boston, London and New York City to provide citywide protection against acts of terrorism, and to provide support for public gatherings and public spaces.\n\nThe Center For Evidence-Based Crime Policy in George Mason University identifies the following randomized controlled trials of automatic number plate recognition technology as very rigorous.\n\nIn addition to government facilities, many private sector industries with facility security concerns are beginning to implement ANPR solutions. Examples include casinos, hospitals, museums, parking facilities, and resorts. In the US, private facilities typically cannot access government or police watch lists, but may develop and match against their own databases for customers, VIPs, critical personnel or \"banned person\" lists. In addition to providing perimeter security, private ANPR has service applications for valet / recognized customer and VIP recognition, logistics and key personnel tracking, sales and advertising, parking management, and logistics (vendor and support vehicle tracking).\n\nMany cities and districts have developed traffic control systems to help monitor the movement and flow of vehicles around the road network. This had typically involved looking at historical data, estimates, observations and statistics, such as:\n\nCCTV cameras can be used to help traffic control centres by giving them live data, allowing for traffic management decisions to be made in real-time. By using ANPR on this footage it is possible to monitor the travel of individual vehicles, automatically providing information about the speed and flow of various routes. These details can highlight problem areas as and when they occur and help the centre to make informed incident management decisions.\n\nSome counties of the United Kingdom have worked with Siemens Traffic to develop traffic monitoring systems for their own control centres and for the public. Projects such as Hampshire County Council's ROMANSE provide an interactive and real-time website showing details about traffic in the city. The site shows information about car parks, ongoing road works, special events and footage taken from CCTV cameras. ANPR systems can be used to provide average point-to-point journey times along particular routes, which can be displayed on a variable-message sign(VMS) giving drivers the ability to plan their route. ROMANSE also allows travellers to see the current situation using a mobile device with an Internet connection (such as WAP, GPRS or 3G), allowing them to view mobile device CCTV images within the Hampshire road network.\n\nThe UK company Trafficmaster has used ANPR since 1998 to estimate average traffic speeds on non-motorway roads without the results being skewed by local fluctuations caused by traffic lights and similar. The company now operates a network of over 4000 ANPR cameras, but claims that only the four most central digits are identified, and no numberplate data is retained.\n\nIEEE Intelligent Transportation Systems Society published some papers on the plate number recognition technologies and applications.\n\nOntario's 407 ETR highway uses a combination of ANPR and radio transponders to toll vehicles entering and exiting the road. Radio antennas are located at each junction and detect the transponders, logging the unique identity of each vehicle in much the same way as the ANPR system does. Without ANPR as a second system it would not be possible to monitor all the traffic. Drivers who opt to rent a transponder for C$2.55 per month are not charged the \"Video Toll Charge\" of C$3.60 for using the road, with heavy vehicles (those with a gross weight of over 5,000 kg) being required to use one. Using either system, users of the highway are notified of the usage charges by post.\n\nThere are numerous other electronic toll collection networks which use this combination of Radio frequency identification and ANPR. These include:\n\nPortuguese roads have old highways with toll stations where drivers can pay with cards and also lanes where there are electronic collection systems. However most new highways only have the option of electronic toll collection system.\nThe electronic toll collection system comprises three different structures:\nWhen the smart tag is installed in the vehicle, the car is quickly identified and owner's bank account is automatically deducted. This process is realized at any speed up to over 250 km per hour.\nIf the car does not have the smart tag, the driver is required to go to a pay station to pay the tolls between 3rd and 5th day after with a surplus charge. If he fails to do so, the owner is sent a letter home with a heavy fine. If this is not paid, it increases five-fold and after that, the car is inserted into a police database for vehicle impounding.\nThis system is also used in some limited access areas of main cities to allow only entry from pre-registered residents. It is planned to be implemented both in more roads and in city entrance toll collection/access restriction. The efficacy of the system is considered to be so high that it is almost impossible for the driver to complain.\n\nThe London congestion charge is an example of a system that charges motorists entering a payment area. Transport for London (TfL) uses ANPR systems and charges motorists a daily fee of £11.50 if they enter, leave or move around within the congestion charge zone between 7 a.m. and 6:00 p.m., Monday to Friday. A reduced fee of £10.50 is paid by vehicle owners who sign up for the automatic deduction scheme. Fines for traveling within the zone without paying the charge are £65 per infraction if paid before the deadline, doubling to £130 per infraction thereafter.\n\nThere are currently 1,500 cameras which use automatic number plate recognition (ANPR) technology. There are also a number of mobile camera units which may be deployed anywhere in the zone.\n\nIt is estimated that around 98% of vehicles moving within the zone are caught on camera. The video streams are transmitted to a data centre located in central London where the ANPR software deduces the registration plate of the vehicle. A second data centre provides a backup location for image data.\n\nBoth front and back number plates are being captured, on vehicles going both in and out – this gives up to four chances to capture the number plates of a vehicle entering and exiting the zone. This list is then compared with a list of cars whose owners/operators have paid to enter the zone – those that have not paid are fined. The registered owner of such a vehicle is looked up in a database provided by the DVLA.\n\nIn Stockholm, Sweden, ANPR is used for the Stockholm congestion tax, owners of cars driving into or out of the inner city must pay a charge, depending on the time of the day. From 2013, also for the Gothenburg congestion tax, which also includes vehicles passing the city on the main highways.\n\nSeveral UK companies and agencies use ANPR systems. These include Vehicle and Operator Services Agency (VOSA), Driver and Vehicle Licensing Agency (DVLA) and Transport for London.\n\nANPR systems may also be used for/by:\n\nVehicle owners have used a variety of techniques in an attempt to evade ANPR systems and road-rule enforcement cameras in general. One method increases the reflective properties of the lettering and makes it more likely that the system will be unable to locate the plate or produce a high enough level of contrast to be able to read it. This is typically done by using a plate cover or a spray, though claims regarding the effectiveness of the latter are disputed. In most jurisdictions, the covers are illegal and covered under existing laws, while in most countries there is no law to disallow the use of the sprays. Other users have attempted to smear their license plate with dirt or utilize covers to mask the plate.\n\nNovelty frames around Texas license plates were made illegal in Texas on 1 September 2003 by Texas Senate Bill 439 because they caused problems with ANPR devices. That law made it a Class C misdemeanor (punishable by a fine of up to US $200), or Class B (punishable by a fine of up to US $2,000 and 180 days in jail) if it can be proven that the owner did it to deliberately obscure their plates. The law was later clarified in 2007 to allow Novelty frames.\n\nIf an ANPR system cannot read the plate, it can flag the image for attention, with the human operators looking to see if they are able to identify the alphanumerics.\n\nIn order to avoid surveillance or penalty charges, there has been an upsurge in car cloning. This is usually achieved by copying registration plates from another car of a similar model and age. This can be difficult to detect, especially as cloners may change the registration plates and travel behavior to hinder investigations.\n\nIn 2013 researchers at Sunflex Zone Ltd created a privacy license plate frame that uses near infrared light to make the license plate unreadable to license plate recognition systems.\n\nThe introduction of ANPR systems has led to fears of misidentification and the furthering of \"1984\"-style surveillance. In the United States, some such as Gregg Easterbrook oppose what they call \"machines that issue speeding tickets and red-light tickets\" as the beginning of a slippery slope towards an automated justice system:\n\nSimilar criticisms have been raised in other countries. Easterbrook also argues that this technology is employed to maximize revenue for the state, rather than to promote safety.\nThe electronic surveillance system produces tickets which in the US are often in excess of $100, and are virtually impossible for a citizen to contest in court without the help of an attorney. The revenues generated by these machines are shared generously with the private corporation that builds and operates them, creating a strong incentive to tweak the system to generate as many tickets as possible.\n\nOlder systems had been notably unreliable; in the UK this has been known to lead to charges being made incorrectly with the vehicle owner having to pay £10 in order to be issued with proof (or not) of the offense. Improvements in technology have drastically decreased error rates, but false accusations are still frequent enough to be a problem.\n\nPerhaps the best known incident involving the abuse of an ANPR database in North America is the case of \"Edmonton Sun\" reporter Kerry Diotte in 2004. Diotte wrote an article critical of Edmonton police use of traffic cameras for revenue enhancement, and in retaliation was added to an ANPR database of \"high-risk drivers\" in an attempt to monitor his habits and create an opportunity to arrest him. The police chief and several officers were fired as a result, and The Office of the Privacy Commissioner of Canada expressed public concern over the \"growing police use of technology to spy on motorists.\"\n\nOther concerns include the storage of information that could be used to identify people and store details about their driving habits and daily life, contravening the Data Protection Act along with similar legislation (see personally identifiable information). The laws in the UK are strict for any system that uses CCTV footage and can identify individuals.\n\nAlso of concern is the safety of the data once it is mined, following the discovery of police surveillance records lost in a gutter.\n\nThere is also a case in the UK for saying that use of ANPR cameras is unlawful under the Regulation of Investigatory Powers Act 2000. The breach exists, some say, in the fact that ANPR is used to monitor the activities of law-abiding citizens and treats everyone like the suspected criminals intended to be surveyed under the Act. The police themselves have been known to refer to the system of ANPR as a \"24/7 traffic movement database\" which is a diversion from its intended purpose of identifying vehicles involved in criminal activities. The opposing viewpoint is that where the plates have been cloned, a 'read' of an innocent motorist's vehicle will allow the elimination of that vehicle from an investigation by visual examination of the images stored. Likewise, stolen vehicles are read by ANPR systems between the time of theft and report to the Police, assisting in the investigation.\n\nThe \"Associated Press\" reported in August 2011 that New York Police Department cars and license plate tracking equipment purchased with federal HIDTA (High Intensity Drug Trafficking Area) funds were used to spy on Muslims at mosques, and to track the license plate numbers of worshipers.\n\nPolice in unmarked cars outfitted with electronic license plate readers would drive down the street and automatically catalog the plates of everyone parked near the mosque, amassing a covert database that would be distributed among officers and used to profile Muslims in public.\n\nIn 2013 the American Civil Liberties Union released 26,000 pages of data about ANPR systems obtained from local, state, and federal agencies through freedom of information laws. \"The documents paint a startling picture of a technology deployed with too few rules that is becoming a tool for mass routine location tracking and surveillance\" wrote the ACLU. The ACLU reported that in many locations the devices were being used to store location information on vehicles which were not suspected of any particular offense. \"Private companies are also using license plate readers and sharing the information they collect with police with little or no oversight or privacy protections. A lack of regulation means that policies governing how long our location data is kept vary widely,\" the ACLU said. In 2012 the ACLU filed suit against the Department of Homeland Security, which funds many local and state ANPR programs through grants, after the agency failed to provide access to records the ACLU had requested under the Freedom of Information Act about the programs.\n\nMany ANPR systems claim accuracy when trained to match plates from a single jurisdiction or region, but can fail when trying to recognize plates from other jurisdictions due to variations in format, font, color, layout, and other plate features. Some jurisdictions offer vanity or affinity plates (particularly in the US), which can create many variations within a single jurisdiction.\n\nFrom time to time, US states will make significant changes in their license plate protocol that will affect OCR accuracy. They may add a character or add a new license plate design. ALPR systems must adapt to these changes quickly in order to be effective. Another challenge with ALPR systems is that some states have the same license plate protocol. For example, more than one state uses the standard three letters followed by four numbers. So each time the ALPR systems alarms, it is the user’s responsibility to make sure that the plate which caused the alarm matches the state associated with the license plate listed on the in-car computer. For maximum effectiveness, an ANPR system should be able to recognize plates from any jurisdiction, and the jurisdiction to which they are associated, but these many variables make such tasks difficult.\n\nCurrently at least one US ANPR provider (PlateSmart) claims their system has been independently reviewed as able to accurately recognize the US state jurisdiction of license plates, and one European ANPR provider claims their system can differentiate all EU plate jurisdictions.\n\nA 2008 article in \"Parking Trend International\" discussed a disparity in claimed vs. experienced license plate recognition read rates, with manufacturers claiming that their recognition engines can correctly report 98% of the time, although customers experience only 90% to 94% success, even with new equipment under perfect conditions. Early systems were reportedly only 60% to 80% reliable.\n\nTrue system error rate is the product of its subsystem error rates (image capture, license plate image extraction, LP image interpretation); slight increases in subsystem error rates can produce dramatic reductions of read rates. The effects of real-world interfering factors on read rate are not uniformly specified or tested by manufacturers. The article states \"there is a need for the industry to adopt a standard performance measurement protocol to enable potential customers assess the best fit for their particular requirements.\"\n\n\n\nLicense Plate Recognition Costs\n", "id": "1657551", "title": "Automatic number-plate recognition"}
{"url": "https://en.wikipedia.org/wiki?curid=20638403", "text": "FatKat (investment software)\n\nFatKat, Inc. is a privately held company founded in 1999 by Raymond C. Kurzweil, an author, inventor, and futurist. He’s perhaps best known for creating an optical character recognition system that – in conjunction with a flatbed scanner and text-to-speech synthesizer – reads text aloud to the sight-impaired. FatKat is an acronym derived from \"financial accelerating transactions from Kurzweil Adaptive Technologies\". The aforesaid company is one of a total of nine Kurzweil companies.\n\nThe purpose of FatKat as listed with the Massachusetts Secretary of the Commonwealth Corporations Division is \"investment software\". Kurzweil, who specializes in artificial intelligence coupled with pattern recognition, has created software that uses quantitative methods to pick stocks for investment purposes.\n\nAlthough selecting stocks based on software-generated recommendations is not new, FatKat’s approach was unique at the time because of its “nonlinear decision making processes more akin to how a brain operates.” In layman's terms, the software can evolve by creating different rules, letting them compete, and using (or combining) the best outcomes. After FatKat’s inception, other investment and/or software companies rushed to develop software based on this and similar Darwinist evolutionary principles, using genetic algorithms.\n\nIn 2005, Kurzweil reported that the FatKat software was \"doing very well – 50% to 100% returns for the last two years\". But as of December 2008, FatKat does not offer its software for sale.\n\nFatKat was registered as a foreign corporation in 1999 with the Massachusetts Secretary of the Commonwealth, Corporations Division. It was originally formed as a company in the state of Delaware. Ray Kurzweil is the president of FatKat, with Aaron Kleiner serving as treasurer and secretary. Michael Brown is listed as a director.\n\nTwo hedge funds exist that use the FatKat name: FatKat Investment Fund, LP and FatKat QP Investment Fund, LP. Both of these investment fund companies list Kurzweil Capital Partners LLC as a general partner. These companies were formed in December 2005, also in Delaware. Neither of the hedge funds is publicly traded. Kurzweil Capital Partners LLC and the two hedge funds are not listed on the Kurzweil companies' web site.\n\nDocumented investors in FatKat, Inc. and its hedge funds are venture capitalist Vinod Khosla and Michael W. Brown (former CFO of Microsoft and chairman of NASDAQ). Other investors have not been disclosed.\n\nKurzweil Technologies (with links to related companies)\n", "id": "20638403", "title": "FatKat (investment software)"}
{"url": "https://en.wikipedia.org/wiki?curid=24175540", "text": "WebCrow\n\nThe WebCrow is a research project carried out at the Information Engineering of the University of Siena with the purpose of automatically solving crosswords.\n\nThe scientific relevance of the project can be understood considering that cracking crosswords requires human-level knowledge. Unlike chess and related games and there is no closed world configuration space. Interestingly, a first nucleus of technology, such as search engines, information retrieval, and machine learning techniques enable computers to enfold with semantics real-life concepts. The project is based on a software system whose major assumption is to attack crosswords making use of the Web as its primary source of knowledge.\n\nWebCrow is very fast and often thrashes human challengers in competitions, especially on multi language crossword schemes. A distinct feature of the WebCrow software system is to combine properly natural language processing (NLP) techniques, the Google web search engine, and constraint satisfaction algorithms from artificial intelligence to acquire knowledge and to fill the schema. The most important component of WebCrow is the Web Search Module (WSM), which implements a domain specific web based question answering algorithm.\n\nThe way WebCrow approaches crosswords solving is quite with respect to humans: Whereas we tend to first answer clues we are sure of and then proceed filling the schema by exploiting the already answered clues as hints, WebCrow uses two clearly distinct stages. In the first one, it processes all the clues and tries to answer them all: For each clue it finds many possible candidates and sorts them according to complex ranking models mainly based on a probability criteria. In the second stage, WebCrow uses constraint satisfaction algorithms to fill the grid with the overall most likely combination of clue answers.\n\nIn order to interact with Google, first of all, WebCrow needs to compose queries on the basis of the given clues. This is done by query expansion, whose purpose is to convert the clue into a query expressed by a simplified and more appropriate language for Google. The retrieved documents are parsed so as to extract a list of word candidates that are congruent with the crossword length constraints. Crosswords can hardly be faced by using encyclopedic knowledge only, since many clues are wordplays or are otherwise purposefully very ambiguous. This enigmatic component of crosswords is faced by a massive use of database of solved crosswords, and by automatic reasoning on a properly organized knowledge base of wired rules. Last but not the least, the final constraint satisfaction step is very effective to fill the correct candidate, even though, unlike humans, the system can not rely on very high confidence on the correctness of the answer.\n\nWebCrow speed and effectiveness has been tested many times in man-machine competitions on Italian, English and multi-language crosswords\nThe outcome of the tests is that WebCrow can successfully compete with average human players on single language schemes and reaches expert level performance in multi-language crosswords. However, WebCrow has not reached expert level in single-language crosswords, yet.\n\nOn August 30, 2006, at the European Conference on Artificial Intelligence (ECAI2006), 25 conference attendees and 53 internet connected crosswords lovers, competed with WebCrow in an official challenge organized within the conference program. The challenge consisted in 5 different crosswords (2 in Italian, 2 in English and one multi-language in Italian and English) and 15 minutes were assigned for each crossword. WebCrow ranked 21 out of 74 participants in the Italian competition, and won both the bilingual and English competitions.\n\nSeveral competitions have been held in Florence, Italy within the Creativity Festival in December 2006, and another official conference competition took place in Hyderabad, India in January 2007, within the International Conference of Artificial Intelligence, where it ranked second out of 25 participants.\n\n", "id": "24175540", "title": "WebCrow"}
{"url": "https://en.wikipedia.org/wiki?curid=1191600", "text": "Document processing\n\nDocument processing involves the conversion of typed and handwritten text on paper-based & electronic documents (e.g., scanned image of a document) into electronic information utilising one of, or a combination of, intelligent character recognition (ICR), optical character recognition (OCR) and experienced data entry clerks.\n\n", "id": "1191600", "title": "Document processing"}
{"url": "https://en.wikipedia.org/wiki?curid=4960888", "text": "Optical answer sheet\n\nAn optical answer sheet or \"bubble sheet\" is a special type of form used in multiple choice question examinations. Optical mark recognition is used to detect answers. The most well known company in the United States involved with optical answer sheets is the Scantron Corporation, although certain uses require their own customized system. The terms \"Optical answer sheet\" and \"scantron\" have become more or less interchangeable.\n\nOptical answer sheets usually have a set of blank ovals or boxes that correspond to each question, often on separate sheets of paper. Bar codes may mark the sheet for automatic processing, and each series of ovals filled will return a certain value when read. In this way students' answers can be digitally recorded, or identity given.\n\nThe first optical answer sheets were read by shining a light through the sheet and measuring how much of the light was blocked using phototubes on the opposite side. As some phototubes are mostly sensitive to the blue end of the visible spectrum, blue pens could not be used, as blue inks reflect and transmit blue light. Because of this, number two pencils had to be used to fill in the bubbles—graphite is a very opaque substance which absorbs or reflects most of the light which hits it.\n\nModern optical answer sheets are read based on reflected light, measuring lightness and darkness. They do not need to be filled in with a number two pencil, though these are recommended over other types due to the lighter marks made by higher-number pencils, and the smudges from number 1 pencils. Black ink will be read, though many systems will ignore marks that are the same color the form is printed in. This also allows optical answer sheets to be double-sided, because marks made on the opposite side will not interfere with reflectance readings as much as with opacity readings.\n\nMost systems accommodate for human error in filling in ovals imprecisely, as long as they do not stray into the other ovals and the oval is almost completely filled\n\nIt is possible for optical answer sheets to be printed incorrectly, such that all ovals will be read as filled. This occurs if the outline of the ovals is too thick, or is irregular. During the 2008 U.S. presidential election, this occurred with over 19,000 absentee ballots in the Georgia county of Gwinnett, and was discovered after around 10,000 had already been returned. The slight difference was not apparent to the naked eye, and was not detected until a test run was made in late October. This required all ballots to be transferred to correctly printed ones, by sequestered workers of the board of elections, under close observation by members of the Democratic and Republican (but not other) political parties, and county sheriff deputies. The transfer, by law, could not occur until election day (November 4).\n", "id": "4960888", "title": "Optical answer sheet"}
{"url": "https://en.wikipedia.org/wiki?curid=26603942", "text": "Silent speech interface\n\nSilent speech interface is a device that allows speech communication without using the sound made when people vocalize their speech sounds. As such it is a type of electronic lip reading. It works by the computer identifying the phonemes that an individual pronounces from nonauditory sources of information about their speech movements. These are then used to recreate the speech using speech synthesis.\n\nSilent speech interface systems have been created using ultrasound and optical camera input of tongue and lip movements. Electromagnetic devices are another technique for tracking tongue and lip movements. The detection of speech movements by electromyography of speech articulator\nmuscles and the larynx is another technique. Another source of information is the vocal tract resonance signals that get transmitted through bone conduction called non-audible murmurs. \nThey have also been created as a brain–computer interface using brain activity in the motor cortex obtained from intracortical microelectrodes.\n\nSuch devices are created as aids to those unable to create the sound phonation needed for audible speech such as after laryngectomies. Another use is for communication when speech is masked by background noise or distorted by self-contained breathing apparatus. A further practical use is where a need exists for silent communication, such as when privacy is required in a public place, or hands-free data silent transmission is needed during a military or security operation.\n\nIn 2002, the Japanese company NTT DoCoMo announced it had created a silent mobile phone using electromyography and imaging of lip movement. \"The spur to developing such a phone,\" the company said, \"was ridding public places of noise,\" adding that, \"the technology is also expected to help people who have permanently lost their voice.\" The feasibility of using silent speech interfaces for practical communication has since then been shown.\n\nThe decoding of silent speech using a computer played an important role in Arthur C. Clarke's story and Stanley Kubrick's associated film \"\". In this, HAL 9000, a computer controlling spaceship Discovery One, bound for Jupiter, discovers a plot to deactivate it by the mission astronauts Dave Bowman and Frank Poole through lip reading their conversations.\n\nIn Orson Scott Card’s series (including \"Ender’s Game\"), the artificial intelligence can be spoken to while the protagonist wears a movement sensor in his jaw, enabling him to converse with the AI without making noise. He also wears an ear implant.\n\n", "id": "26603942", "title": "Silent speech interface"}
{"url": "https://en.wikipedia.org/wiki?curid=4903304", "text": "Eccky\n\nEccky is an online game. Until 2009, it was an MSN-based life simulation game in which two people work together to create and raise a virtual baby. \"Eccky\" won the 2005 SpinAwards for Innovation and for Best Interactive Concept. In 2009, the game play changed to a real-time virtual world on Hyves.\n\n\"Eccky\" was created in August 2005 by Dutch developer Media Republic in association with MSN in the Netherlands. \"Eccky\" has characteristics of life simulation and virtual pet games. The gameplay of the first version of \"Eccky\" involved a virtual baby, or Eccky, which was born on the basis of information derived from both \"Eccky\" user players. \"Eccky\" used an AIML chatbot and MSN Messenger for chat between users and the Eccky baby. In 2006, \"Eccky\" became an independent company as a subsidiary of Media Republic.\n\nFrom its live introduction to the public until August 2006, \"Eccky\" required an initial fee which could be paid by either user player. However \"Eccky\" has been free to play since September 2006.\n\nIn the first version of \"Eccky\", two users create a virtual baby, and raise him/her with the goal of making the child as happy and satisfied as possible. A user fills out a questionnaire with information regarding their personal characteristics, child-rearing attitudes, favorites, etc. Once registration and the \"DNA\" test have been completed, a user may invite another person to make an Eccky. Upon accepting the invitation, the second user also registers on the Eccky website, and completes his/her own \"DNA\" test. Both players then choose both a masculine and feminine name for their future Eccky. Thereafter Eccky is born, with characteristics determined by the combination of both users' DNA profiles. Eccky's sex is randomly determined, as is Eccky’s name (which is chosen randomly between the four possible names chosen by the user parents).\n\nOver a six-day period from Eccky’s birth, Eccky grows and ages three years for every one day of gameplay. Thus, in six days, Eccky develops from a cooing baby into an eighteen-year-old young adult with its own character. Every Eccky is unique at birth, and the way in which the users raise their Eccky further individualizes Eccky’s demeanor and characteristics. On the sixth day, or upon turning 18, Eccky leaves the house to venture off into the wide world, and the game ends.\n\n\"Eccky\" played principally via the Eccky website and MSN Messenger, though also via mobile phone. Upon Eccky’s birth, Eccky is automatically added as a contact to the MSN contact lists of both players. This allows users to talk to their virtual child. Running on an AIML chat engine, Eccky is able to speak from the moment following birth, both in response to users addressing Eccky and by initiating conversations him/herself. Eccky initiates conversation either randomly or to express any particularly pressing need he/she may have (i.e. being extremely hungry, having to go to the bathroom, being very sick, feeling neglected, etc.) Eccky’s vocabulary is initially limited to newborn babble. Then, as with a real child, Eccky’s command of vocabulary grows with each day of gameplay, to a final capability of being able to respond with over 60,000 unique answers on more than 4,000 diverse subjects. Since June 2007, Eccky also features an in-game chat functionality.\n\nEccky and users can also exchange text messages via mobile phone. Exchange of text messages requires that the user have a mobile phone, and that the user purchase a mobile phone for Eccky within the game environment. This feature involves extra costs to the user, specifically the cost of sending text messages, and can be turned on and off at the user's discretion.\n\nMost games are intended to be played with users playing against their Eccky and vice versa. Others are played in cooperation with Eccky. Of note are the racing games, some of which allow users to create their own tracks and race on them with Eccky. Though most games are free to play, some games require additional credits to play.\n\nEccky’s physical and emotional states are subject to continual change, determined by 180 dynamic variables, and are influenced by the interaction with and treatment by users, both via the virtual world and via the chat. The levels of these physical and emotional states are continually assessed and are made visible to users via meters that gauge happiness, hunger, toilet needs, popularity, who Eccky’s favorite parent is, etc. For example, if Eccky is not sufficiently fed, users will see this in Eccky’s hunger meter. In worst-case scenarios, an Eccky can be neglected, physical and emotionally, to the point of needing to be sent to the hospital. During this time, gameplay continues and users may chat with Eccky, but may not interact with Eccky in the virtual world. Further, Eccky’s physical and emotional states are reflected in the chat element, as Eccky will either comment, unsolicited, on how he/she is doing physically and emotionally, or respond to a user in such a way as to make his/her physical and emotional state known.\n\nIt is also possible to send Eccky away for a period, either to live in other accommodations such as in a hotel, on vacation, or to stay with a babysitter. A babysitter may be anyone with an MSN Messenger account. Sending Eccky to a hotel or on vacation does involve additional costs (sending Eccky to a babysitter does not), and as with Eccky staying in the hospital, users may continue to speak with Eccky via chat but cannot interact with Eccky in the virtual world setting during this time. A user can retrieve their Eccky at any time during a stay away from home. Eccky was closed down in 2007 and until it was launched in a new format in 2009.\n\nIn 2009 Dutch/Chinese developer TribePlay created a new version of \"Eccky\" and launched it in Hyves as one of the first social networking virtual worlds. In \"Eccky\" players can make a character (an Eccky) with their social network profile. After that they enter the Eccky world. In this world they have access to various locations, such as a city center, park, mountain top and beach. Players can also visit their own or other users' houses. In any of these virtual locations players can chat, play mini games and do a variety of other things.\n\n\"Eccky\" is integrated into Hyves and Facebook. Together with their Hyves or Facebook friends players can chat, play mini games and post their Eccky's activities on their social networking profile. \"Eccky\" has many games. Games can be played in single-player or multi-player format. Ecckies can also play with their Wobble. A Wobble is Eccky’s own pet. Wobbles were the first inhabitants of the Eccky world. Wobbles live together with Ecckies and need to be taken care of. Once Ecckies advance in levels they receive superpowers to be used in the game.\n\n", "id": "4903304", "title": "Eccky"}
{"url": "https://en.wikipedia.org/wiki?curid=7271261", "text": "Statistical semantics\n\nIn linguistics, statistical semantics applies the methods of statistics to the problem of determining the meaning of words or phrases, ideally through unsupervised learning, to a degree of precision at least sufficient for the purpose of information retrieval. \n\nThe term \"statistical semantics\" was first used by Warren Weaver in his well-known paper on machine translation. He argued that word sense disambiguation for machine translation should be based on the co-occurrence frequency of the context words near a given target word. The underlying assumption that \"a word is characterized by the company it keeps\" was advocated by J.R. Firth. This assumption is known in linguistics as the distributional hypothesis. Emile Delavenay defined \"statistical semantics\" as the \"statistical study of meanings of words and their frequency and order of recurrence\". \"Furnas et al. 1983\" is frequently cited as a foundational contribution to statistical semantics. An early success in the field was latent semantic analysis.\n\nResearch in statistical semantics has resulted in a wide variety of algorithms that use the distributional hypothesis to discover many aspects of semantics, by applying statistical techniques to large corpora:\n\nStatistical semantics focuses on the meanings of common words and the relations between common words, unlike text mining, which tends to focus on whole documents, document collections, or named entities (names of people, places, and organizations). Statistical semantics is a subfield of computational semantics, which is in turn a subfield of computational linguistics and natural language processing.\n\nMany of the applications of statistical semantics (listed above) can also be addressed by lexicon-based algorithms, instead of the corpus-based algorithms of statistical semantics. One advantage of corpus-based algorithms is that they are typically not as labour-intensive as lexicon-based algorithms. Another advantage is that they are usually easier to adapt to new languages than lexicon-based algorithms. However, the best performance on an application is often achieved by combining the two approaches.\n\n", "id": "7271261", "title": "Statistical semantics"}
{"url": "https://en.wikipedia.org/wiki?curid=1741558", "text": "SCIgen\n\nSCIgen is a computer program that uses context-free grammar to randomly generate nonsense in the form of computer science research papers. All elements of the papers are formed, including graphs, diagrams, and citations. Created by scientists at the Massachusetts Institute of Technology, its stated aim is \"to maximize amusement, rather than coherence.\"\n\nOpening abstract of \"Rooter: A Methodology for the Typical Unification of Access Points and Redundancy\":\nIn 2005 a paper generated by SCIgen, \"Rooter: A Methodology for the Typical Unification of Access Points and Redundancy\", was accepted as a non-reviewed paper to the 2005 World Multiconference on Systemics, Cybernetics and Informatics (WMSCI) and the authors were invited to speak. The authors of SCIgen described their hoax on their website, and it soon received great publicity when picked up by Slashdot.\nWMSCI withdrew their invitation, but the SCIgen team went anyway, renting space in the hotel separately from the conference and delivering a series of randomly generated talks on their own \"track.\" The organizer of these WMSCI conferences is Professor Nagib Callaos. From 2000 until 2005, the WMSCI was also sponsored by the Institute of Electrical and Electronics Engineers. The IEEE stopped granting sponsorship to Callaos from 2006 to 2008.\nSubmitting the paper was a deliberate attempt to embarrass WMSCI, which the authors claim accepts low-quality papers and sends unsolicited requests for submissions in bulk to academics. As the SCIgen website states:\n\nComputing writer Stan Kelly-Bootle noted in \"ACM Queue\" that many sentences in the \"Rooter\" paper were individually plausible, which he regarded as posing a problem for automated detection of hoax articles. He suggested that even human readers might be taken in by the effective use of jargon (\"The pun on root/router is par for MIT-graduate humor, and at least one occurrence of methodology is mandatory\") and attribute the paper's apparent incoherence to their own limited knowledge. His conclusion was that \"a reliable gibberish filter requires a careful holistic review by several peer domain experts\".\n\nThe name of a fictional man named \"Herbert Schlangemann\" was used to publish false scientific articles in international conferences that are suspected to be, at least partially, frauds. The author is named after the Swedish short film \"Der Schlangemann\".\n\nIn all cases, the published papers were withdrawn from the conferences' proceedings, and the conference organizing committee as well as the names of the keynote speakers were removed from their websites.\n\n\n\nRefereeing performed on behalf of the Institute of Electrical and Electronics Engineers has also been subject to criticism after fake papers were discovered in conference publications, most notably by Labbé and a researcher using the pseudonym of Schlangemann.\n\nIn this 2010 paper by Cyril Labbé from Grenoble University demonstrated the vulnerability of h-index calculations based on Google Scholar output by feeding it a large set of SCIgen-generated documents that were citing each other, effectively an academic link farm. Using this method the author managed to rank \"Ike Antkare\" ahead of Albert Einstein for instance.\n\n\n", "id": "1741558", "title": "SCIgen"}
{"url": "https://en.wikipedia.org/wiki?curid=25910473", "text": "AForge.NET\n\nAForge.NET is a computer vision and artificial intelligence library originally developed by Andrew Kirillov for the .NET Framework.\n\nThe source code and binaries of the project are available under the terms of the Lesser GPL and the GPL (GNU General Public License).\n\nAnother (unaffiliated) project called Accord.NET was created to extend the features of the original AForge.NET library.\n\nOn April 1, 2012, Andrew Kirillov announced the end of the public support for the library, temporarily closing the discussion forums. The last release of the AForge.NET Framework was made available on July 17, 2013. However, since its release 3.0 in 2015, the Accord.NET project started to incorporate most of the original AForge.NET source code in its codebase, continuing its support and development under the Accord.NET name.\n\nThe framework's API includes support for:\n\nComplete list of features is available on the features page of the project.\n\nThe framework is provided not only with different libraries and their sources, but with many sample applications, which demonstrate the use of this framework, and with documentation help files, which are provided in HTML Help format. The documentation is also available on-line.\n\n", "id": "25910473", "title": "AForge.NET"}
{"url": "https://en.wikipedia.org/wiki?curid=28539183", "text": "EuResist\n\nEuResist is an international project designed to improve the treatment of HIV patients by developing a computerized system that can recommend optimal treatment based on the patient’s clinical and genomic data.\n\nThe project is part of the Virtual Physiological Human framework, funded by the European Commission. It started in 2006 with the formation of a consortium of several research institutes and hospitals in Europe and Israel. The consortium completed its commitment to the European Commission near the end of 2008, at which time the system became available online. A non-profit organization was consequently established by the main partners to maintain and improve the system.\n\nIn 2009, the EuResist project was named as a Computerworld honors program laureate.\n\nAIDS is a disease caused by the HIV retrovirus, which progressively reduces the effectiveness of the immune system, leading to infections and ultimately death.\n\nMore than 30 different drugs exist for treating HIV patients. Antiretroviral drugs can disrupt the virus’s replication process causing its numbers to decrease dramatically. While the virus cannot be eradicated completely, in small numbers it is harmless. Usually a patient is given a combination of three or four drugs, a treatment known as highly active antiretroviral therapy, or HAART. The main reason such a treatment might fail is the development of mutated strands of the virus, resistant to one or more of the prescribed drugs.\n\nThus an important consideration when choosing treatment for a patient is to prescribe those drugs to which the particular patient’s virus strands are most susceptible. One way to achieve that is to extract virus samples from the patient’s blood and test them against all possible drugs. Since this process is lengthy and costly, computerized systems have been developed to predict virus resistance based on its genotype. The treating physician samples virus genotype sequences from the patient’s blood and provides this data to a computerized system. The system then responds with drug recommendations.\n\nSuch systems are limited in accuracy, depending on the amount of data used for their creation, its quality and the richness of mathematical models used for the actual prediction. Prior to EuResist, such systems had several common characteristics that negatively impacted their accuracy:\n\nEuResist sought to create a more accurate HIV treatment prediction system by collecting a large database of in vivo data (clinical and genomic records of real treatments of HIV patients and their consequences), and by using an array of prediction models instead of just one.\nThe database was created by merging local databases of various clinics across Europe. This database is thought to be the largest of its kind in the world.\nFor each patient, it includes various personal and demographic details such as gender, age, country of origin, genomic sequencing of HIV found in the patient’s blood, records of the drugs prescribed, and the changes in the amount of virus in the blood following these treatments.\n\nThis data was used to train an array of prediction models, created by using various contemporary machine learning techniques, among them Bayesian networks, logistic regression, and others.\n\nA web interface allows physicians to specify patients' clinical and genomic data. This data is sent to the prediction engines, and the combined response, which is displayed to the physician, includes various suggested treatments and a prediction of their effect on the amount of HIV in the blood.\n\nThe EuResist system was tested and compared with its predecessors by feeding it with historical data on patients for which treatment results are known. The developers of EuResist, who conducted this test, reported an improved performance over the previous state-of-the-art system.\n\nEuResist started in 2006 as a consortium funded by the European Union as part of the Virtual Physiological Human FP-6 framework. The partners of this consortium were:\nThe consortium completed its commitment to the European Union in late 2008, at which time the EuResist system became available on line.\nThe first five partners mentioned above continued to form a non-profit organization that maintains the system, expands the database with new clinical and genomical records and updates the prediction engines accordingly. As of mid-2010, an average of 600 queries are submitted to the EuResist system every quarter.\n\nOn June 1, 2009, EuResist received a Computerworld honors program laureate award, a global program honoring individuals and organizations that use information technology to benefit society.\n\n", "id": "28539183", "title": "EuResist"}
{"url": "https://en.wikipedia.org/wiki?curid=6898858", "text": "Concept mining\n\nConcept mining is an activity that results in the extraction of concepts from artifacts. Solutions to the task typically involve aspects of artificial intelligence and statistics, such as data mining and text mining. Because artifacts are typically a loosely structured sequence of words and other symbols (rather than concepts), the problem is nontrivial, but it can provide powerful insights into the meaning, provenance and similarity of documents.\n\nTraditionally, the conversion of words to concepts has been performed using a thesaurus, and for computational techniques the tendency is to do the same. The thesauri used are either specially created for the task, or a pre-existing language model, usually related to Princeton's WordNet.\n\nThe mappings of words to concepts are often ambiguous. Typically each word in a given language will relate to several possible concepts. Humans use context to disambiguate the various meanings of a given piece of text, where available machine translation systems cannot easily infer context.\n\nFor the purposes of concept mining however, these ambiguities tend to be less important than they are with machine translation, for in large documents the ambiguities tend to even out, much as is the case with text mining.\n\nThere are many techniques for disambiguation that may be used. Examples are linguistic analysis of the text and the use of word and concept association frequency information that may be inferred from large text corpora. Recently, techniques that base on semantic similarity between the possible concepts and the context have appeared and gained interest in the scientific community.\n\nOne of the spin-offs of calculating document statistics in the concept domain, rather than the word domain, is that concepts form natural tree structures based on hypernymy and meronymy. These structures can be used to produce simple tree membership statistics, that can be used to locate any document in a Euclidean concept space. If the size of a document is also considered as another dimension of this space then an extremely efficient indexing system can be created. This technique is currently in commercial use locating similar legal documents in a 2.5 million document corpus.\n\nStandard numeric clustering techniques may be used in \"concept space\" as described above to locate and index documents by the inferred topic. These are numerically far more efficient than their text mining cousins, and tend to behave more intuitively, in that they map better to the similarity measures a human would generate.\n\n", "id": "6898858", "title": "Concept mining"}
{"url": "https://en.wikipedia.org/wiki?curid=23392007", "text": "Document capture software\n\nDocument Capture Software refers to applications that provide the ability and feature set to automate the process of scanning paper documents. Most scanning hardware, both scanners and copiers, provides the basic ability to scan to any number of image file formats, including: PDF, TIFF, JPG, BMP, etc. This basic functionality is augmented by document capture software, which can add efficiency and standardization to the process.\n\nTypical features of Document Capture Software include:\n\nThe goal for implementing a document capture solution is to reduce the amount of time spent in the scanning and capture process, and produce metadata along with an image file, and/or OCR text. This information is then migrated to a Document Management or Enterprise Content Management system. These systems often provide a search function, allowing search of the assets based on the produced metadata, and then viewed using document imaging software.\n\nECM (Enterprise Content management) and their DMS component (Document Management System) are being adopted by many organizations as a corporate document management system for all types of electronic files, e.g. MS word, PDF ... However, much of the information held by organisations is on paper and this needs to be integrated within the same document repository.\n\nBy converting paper documents into digital format through scanning companies can convert paper into image formats such as TIF and JPG and also extract valuable index information or business data from the document using OCR technology. Digital documents and associated metadata can easily be stored in the ECM in a variety of formats. The most popular of these formats is PDF which not only provides an accurate representation of the document but also allows all the OCR text in the document to be stored behind the PDF image. This format is known as PDF with hidden text or text-searchable PDF. This allows users to search for documents by using keywords in the metadata fields or by searching the content of PDF files across the repository.\n\nInformation held on paper is usually just as valuable to organisations as the electronic documents that are generated internally. Often this information represents a large proportion of the day to day correspondence with suppliers and customers. Having the ability to manage and share this information internally through a document management system such as SharePoint can improve collaboration between departments or employees and also eliminate the risk of losing this information through disasters such as floods or fire.\n\nOrganisations adopting an ECM/DMS often implement electronic workflow which allows the information held on paper to be included as part of an electronic business process and incorporated into a customer record file along with other associated office documents and emails.\nFor business critical documents, such as purchase orders and supplier invoices, digitising documents can help speed up business transactions as well as reduce manual effort involved in keying data into business systems, such as CRM, ERP and Accounting. Scanned invoices can also be routed to managers for payment approval via email or an electronic workflow.\n\nDistributed document capture is a technology which allows the scanning of documents into a central server through the use of individual capture stations. A variation of distributed capture is thin-client document capture in which documents are scanned into a central server through the use of web browser. One of these web-based products was reviewed by AIIM. They said, \"(this product) is a thin-client distributed capture system that streamlines the process of acquiring and creating documents.\" The streamlining is a result of several factors including the lack of software which needs to be installed at every scanning station and the variety of input sources from which documents can be captured. This includes things like email, fax, or a watched folder.\n\nJeff Shuey, Director of Business Development at Kodak, makes a distinction between distributed capture and what he calls \"remote\" capture. In an article publishing in AIIM, he said that the key difference between the two is whether or not the information that is captured from scanning needs to be sent to the centralized server. If, as he points out in his article, the document just needs to be scanned and committed to a SharePoint system and doesn't need to be sent to some other centralized server, this is just a remote capture situation.\n\nThere are Document Capture Software comparisons available, featuring some of the most relevant products (EMC Captiva, IBM Datacap, Artsyl Technologies or Ephesoft) and extracting performance facts and their most relevant features.\n", "id": "23392007", "title": "Document capture software"}
{"url": "https://en.wikipedia.org/wiki?curid=2368154", "text": "Clinical decision support system\n\nA clinical decision support system (CDSS) is a health information technology system that is designed to provide physicians and other health professionals with clinical decision support (CDS), that is, assistance with clinical decision-making tasks. A working definition has been proposed by Robert Hayward of the Centre for Health Evidence: \"Clinical decision support systems link health observations with health knowledge to influence health choices by clinicians for improved health care\". CDSSs constitute a major topic in artificial intelligence in medicine.\n\nThe evidence of the effectiveness of CDSS is mixed. A 2014 systematic review did not find a benefit in terms of risk of death when the CDSS was combined with the electronic health record. There may be some benefits, however, in terms of other outcomes.\n\nA 2005 systematic review concluded that CDSSs improved practitioner performance in 64% of the studies. The CDSSs improved patient outcomes in 13% of the studies. Sustainable CDSSs features associated with improved practitioner performance include the following:\nBoth the number and the methodological quality of studies of CDSSs increased from 1973 through 2004.\n\nAnother 2005 systematic review found... \"\"Decision support systems significantly improved clinical practice in 68% of trials.\"\" The CDSS features associated with success include the following:\n\nHowever, other systematic reviews are less optimistic about the effects of CDS, with one from 2011 stating \"\"There is a large gap between the postulated and empirically demonstrated benefits of [CDSS and other] eHealth technologies ... their cost-effectiveness has yet to be demonstrated\"\".\n\nA 5-year evaluation of the effectiveness of a CDSS in implementing rational treatment of bacterial infections was published in 2014; according to the authors, it was the first long term study of a CDSS.\n\nA clinical decision support system has been defined as an \"active knowledge systems, which use two or more items of patient data to generate case-specific advice.\" This implies that a CDSS is simply a decision support system that is focused on using knowledge management in such a way so as to achieve clinical advice for patient care based on multiple items of patient data.\n\nThe main purpose of modern CDSS is to assist clinicians at the point of care. This means that clinicians interact with a CDSS to help to analyse, and reach a diagnosis based on, patient data.\n\nIn the early days, CDSSs were conceived of as being used to literally make decisions for the clinician. The clinician would input the information and wait for the CDSS to output the \"right\" choice and the clinician would simply act on that output. However, the modern methodology of using CDSSs to assist means that the clinician interacts with the CDSS, utilizing both their own knowledge and the CDSS, to make a better analysis of the patient's data than either human or CDSS could make on their own. Typically, a CDSS makes suggestions for the clinician to look through, and the clinician is expected to pick out useful information from the presented results and discount erroneous CDSS suggestions.\n\nThere are two main types of CDSS:\nas detailed below.\n\nAn example of how a clinical decision support system might be used by a clinician is a specific type of CDSS, a DDSS (diagnosis decision support systems). A DDSS requests some of the patients data and in response, proposes a set of appropriate diagnoses. The doctor then takes the output of the DDSS and determines which diagnoses might be relevant and which are not, and if necessary orders further tests to narrow down the diagnosis.\n\nAnother example of a CDSS would be a case-based reasoning (CBR) system. A CBR system might use previous case data to help determine the appropriate amount of beams and the optimal beam angles for use in radiotherapy for brain cancer patients; medical physicists and oncologists would then review the recommended treatment plan to determine its viability.\n\nAnother important classification of a CDSS is based on the timing of its use. Doctors use these systems at point of care to help them as they are dealing with a patient, with the timing of use being either pre-diagnosis, during diagnosis, or post diagnosis. Pre-diagnosis CDSS systems are used to help the physician prepare the diagnoses. CDSS used during diagnosis help review and filter the physician's preliminary diagnostic choices to improve their final results. Post-diagnosis CDSS systems are used to mine data to derive connections between patients and their past medical history and clinical research to predict future events. It has been claimed that decision support will begin to replace clinicians in common tasks in the future.\n\nAnother approach, used by the National Health Service in England, is to use a DDSS (either, in the past, operated by the patient, or, today, by a phone operative who is not medically-trained) to triage medical conditions out of hours by suggesting a suitable next step to the patient (e.g. call an ambulance, or see a general practitioner on the next working day). The suggestion, which may be disregarded by either the patient or the phone operative if common sense or caution suggests otherwise, is based on the known information and an implicit conclusion about what the \"worst-case\" diagnosis is likely to be (which is not always revealed to the patient, because it might well be incorrect and is not based on a medically-trained person's opinion - it is only used for initial triage purposes).\n\nMost CDSSs consist of three parts: the knowledge base, an inference engine, and a mechanism to communicate. The knowledge base contains the rules and associations of compiled data which most often take the form of IF-THEN rules. If this was a system for determining drug interactions, then a rule might be that IF drug X is taken AND drug Y is taken THEN alert user. Using another interface, an advanced user could edit the knowledge base to keep it up to date with new drugs. The inference engine combines the rules from the knowledge base with the patient's data. The communication mechanism allows the system to show the results to the user as well as have input into the system.\n\nCDSSs that do not use a knowledge base use a form of artificial intelligence called machine learning, which allow computers to learn from past experiences and/or find patterns in clinical data. This eliminates the need for writing rules and for expert input. However, since systems based on machine learning cannot \"explain\" the reasons for their conclusions (they are so-called \"black boxes\", because no meaningful information about how they work can be discerned by human inspection), most clinicians do not use them directly for diagnoses, for reliability and accountability reasons. Nevertheless, they can be useful as post-diagnostic systems, for suggesting patterns for clinicians to look into in more depth.\n\nThree types of non-knowledge-based systems are support vector machines, artificial neural networks and genetic algorithms.\n\n\nWith the enactment of the American Recovery and Reinvestment Act of 2009 (ARRA), there is a push for widespread adoption of health information technology through the Health Information Technology for Economic and Clinical Health Act (HITECH). Through these initiatives, more hospitals and clinics are integrating electronic medical records (EMRs) and computerized physician order entry (CPOE) within their health information processing and storage. Consequently, the Institute of Medicine (IOM) promoted usage of health information technology including clinical decision support systems to advance quality of patient care. The IOM had published a report in 1999, \"To Err is Human\", which focused on the patient safety crisis in the United States, pointing to the incredibly high number of deaths. This statistic attracted great attention to the quality of patient care.\n\nWith the enactment of the HITECH Act included in the ARRA, encouraging the adoption of health IT, more detailed case laws for CDSS and EMRs are still being defined by the Office of National Coordinator for Health Information Technology (ONC) and approved by Department of Health and Human Services (HHS). A definition of \"Meaningful use\" is yet to be published.\n\nDespite the absence of laws, the CDSS vendors would almost certainly be viewed as having a legal duty of care to both the patients who may adversely be affected due to CDSS usage and the clinicians who may use the technology for patient care. However, duties of care legal regulations are not explicitly defined yet.\n\nWith recent effective legislations related to performance shift payment incentives, CDSS are becoming more attractive.\n\nMuch effort has been put forth by many medical institutions and software companies to produce viable CDSSs to support all aspects of clinical tasks. However, with the complexity of clinical workflows and the demands on staff time high, care must be taken by the institution deploying the support system to ensure that the system becomes a fluid and integral part of the clinical workflow. Some CDSSs have met with varying amounts of success, while others have suffered from common problems preventing or reducing successful adoption and acceptance.\n\nTwo sectors of the healthcare domain in which CDSSs have had a large impact are the pharmacy and billing sectors. There are commonly used pharmacy and prescription ordering systems that now perform batch-based checking of orders for negative drug interactions and report warnings to the ordering professional. Another sector of success for CDSS is in billing and claims filing. Since many hospitals rely on Medicare reimbursements to stay in operation, systems have been created to help examine both a proposed treatment plan and the current rules of Medicare in order to suggest a plan that attempts to address both the care of the patient and the financial needs of the institution.\n\nOther CDSSs that are aimed at diagnostic tasks have found success, but are often very limited in deployment and scope. The Leeds Abdominal Pain System went operational in 1971 for the University of Leeds hospital, and was reported to have produced a correct diagnosis in 91.8% of cases, compared to the clinicians' success rate of 79.6%.\n\nDespite the wide range of efforts by institutions to produce and use these systems, widespread adoption and acceptance has still not yet been achieved for most offerings. One large roadblock to acceptance has historically been workflow integration. A tendency to focus only on the functional decision making core of the CDSS existed, causing a deficiency in planning for how the clinician will actually use the product in situ. Often CDSSs were stand-alone applications, requiring the clinician to cease working on their current system, switch to the CDSS, input the necessary data (even if it had already been inputted into another system), and examine the results produced. The additional steps break the flow from the clinician's perspective and cost precious time.\n\nClinical decision support systems face steep technical challenges in a number of areas. Biological systems are profoundly complicated, and a clinical decision may utilize an enormous range of potentially relevant data. For example, an electronic evidence-based medicine system may potentially consider a patient's symptoms, medical history, family history and genetics, as well as historical and geographical trends of disease occurrence, and published clinical data on medicinal effectiveness when recommending a patient's course of treatment.\n\nClinically, a large deterrent to CDSS acceptance is workflow integration, as mentioned above.\n\nAnother source of contention with many medical support systems is that they produce a massive number of alerts. When systems produce high volume of warnings (especially those that do not require escalation), aside from the annoyance, clinicians may pay less attention to warnings, causing potentially critical alerts to be missed.\n\nOne of the core challenges facing CDSS is difficulty in incorporating the extensive quantity of clinical research being published on an ongoing basis. In a given year, tens of thousands of clinical trials are published. Currently, each one of these studies must be manually read, evaluated for scientific legitimacy, and incorporated into the CDSS in an accurate way. In 2004, it was stated that the process of gathering clinical data and medical knowledge and putting them into a form that computers can manipulate to assist in clinical decision-support is \"still in its infancy\".\n\nNevertheless, it is more feasible for a business to do this centrally, even if incompletely, than for each individual doctor to try to keep up with all the research being published.\n\nIn addition to being laborious, integration of new data can sometimes be difficult to quantify or incorporate into the existing decision support schema, particularly in instances where different clinical papers may appear conflicting. Properly resolving these sorts of discrepancies is often the subject of clinical papers itself (see meta-analysis), which often take months to complete.\n\nIn order for a CDSS to offer value, it must demonstrably improve clinical workflow or outcome. Evaluation of CDSS is the process of quantifying its value to improve a system's quality and measure its effectiveness. Because different CDSSs serve different purposes, there is no generic metric which applies to all such systems; however, attributes such as consistency (with itself, and with experts) often apply across a wide spectrum of systems.\n\nThe evaluation benchmark for a CDSS depends on the system's goal: for example, a diagnostic decision support system may be rated based upon the consistency and accuracy of its classification of disease (as compared to physicians or other decision support systems). An evidence-based medicine system might be rated based upon a high incidence of patient improvement, or higher financial reimbursement for care providers.\n\nImplementing electronic health records (EHR) was an inevitable challenge. The reasons behind this challenge are that it is a relatively uncharted area, and there are many issues and complications during the implementation phase of an EHR. This can be seen in the numerous studies that have been undertaken. However, challenges in implementing electronic health records (EHRs) have received some attention, but less is known about the process of transitioning from legacy EHRs to newer systems.\n\nWith all of that said, electronic health records are the way of the future for healthcare industry. They are a way to capture and utilise real-time data to provide high-quality patient care, ensuring efficiency and effective use of time and resources. Incorporating EHR and CDSS together into the process of medicine has the potential to change the way medicine has been taught and practiced. It has been said that \"the highest level of EHR is a CDSS\".\n\nSince \"clinical decision support systems (CDSS) are computer systems designed to impact clinician decision making about individual patients at the point in time that these decisions are made\", it is clear that it would be beneficial to have a fully integrated CDSS and EHR.\n\nEven though the benefits can be seen, to fully implement a CDSS that is integrated with an EHR has historically required significant planning by the healthcare facility/organisation, in order for the purpose of the CDSS to be successful and effective. \nThe success and effectiveness can be measured by the increase in patient care being delivered and reduced adverse events occurring. In addition to this, there would be a saving of time and resources, and benefits in terms of autonomy and financial benefits to the healthcare facility/organisation.\n\nA successful CDSS/EHR integration will allow the provision of best practice, high quality care to the patient, which is the ultimate goal of healthcare.\n\nErrors have always occurred in healthcare, so trying to minimise them as much as possible is important in order to provide quality patient care. Three areas that can be addressed with the implementation of CDSS and Electronic Health Records (EHRs), are:\nCDSSs will be most beneficial in the future when healthcare facilities are \"100% electronic\" in terms of real-time patient information, thus simplifying the number of modifications that have to occur to ensure that all the systems are up to date with each other.\n\nThe measurable benefits of clinical decision support systems on physician performance and patient outcomes remain the subject of ongoing research, as noted in the section above.\n\nImplementing electronic health records (EHR) in healthcare settings incurs challenges; none more important than maintaining efficiency and safety during rollout, but in order for the implementation process to be effective, an understanding of the EHR users' perspectives is key to the success of EHR implementation projects. In addition to this, adoption needs to be actively fostered through a bottom-up, clinical-needs-first approach. The same can be said for CDSS.\n\nThe main areas of concern with moving into a fully integrated EHR/CDSS system are:\n\n\nA service oriented architecture has been proposed as a technical means to address some of these barriers.\n\nAs of July 2015, the planned transition to EHRs in Australia is facing difficulties. The majority of healthcare facilities are still running completely paper-based systems, and some are in a transition phase of scanned EHRs, or are moving towards such a transition phase.\n\nVictoria has attempted to implement EHR across the state with its HealthSMART program, but due to unexpectedly high costs it has cancelled the project.\n\nSouth Australia (SA) however is slightly more successful than Victoria in the implementation of an EHR. This may be due to all public healthcare organisations in SA being centrally run. (However, on the other hand, the UK's National Health Service is also centrally administered, and its National Programme for IT in the 2000s, which included EHRs in its remit, was an expensive disaster.)\n\nSA is in the process of implementing \"Enterprise patient administration system (EPAS)\". This system is the foundation for all public hospitals and health care sites for an EHR within SA and it was expected that by the end of 2014 all facilities in SA will be connected to it. This would allow for successful integration of CDSS into SA and increase the benefits of the EHR.\nBy July 2015 it was reported that only 3 out of 75 health care facilities implemented EPAS.\n\nWith the largest health system in the country and a federated rather than centrally administered model, New South Wales is making consistent progress towards statewide implementation of EHRs. The current iteration of the state's technology, eMR2, includes CDSS features such as a sepsis pathway for identifying at-risk patients based upon data input to the electronic record. As of June 2016, 93 of 194 sites in-scope for the initial roll-out had implemented eMR2\n\n\n", "id": "2368154", "title": "Clinical decision support system"}
{"url": "https://en.wikipedia.org/wiki?curid=29069615", "text": "Resistance Database Initiative\n\nHIV Resistance Response Database Initiative (RDI) is a not-for-profit organisation established in 2002 with the mission of improving the clinical management of HIV infection through the application of bioinformatics to HIV drug resistance and treatment outcome data. The RDI has the following specific goals:\n\n\nThe RDI consists of a small executive group based in the UK, an international advisory group of leading HIV/AIDS scientists and clinicians, and an extensive global network of collaborators and data contributors.\n\nHuman immunodeficiency virus (HIV) is the virus that causes acquired immunodeficiency syndrome (AIDS), a condition in which the immune system begins to fail, leading to life-threatening opportunistic infections.\n\nThere are approximately 25 HIV ‘antiretroviral’ drugs that have been approved for the treatment of HIV infection, from six different classes, based on the point in the HIV life-cycle at which they act.\n\nThey are used in combination; typically 3 or more drugs from 2 or more different classes, a form of therapy known as highly active antiretroviral therapy or HAART. The aim of therapy is suppression of the virus to very low, ideally undetectable, levels in the blood this prevents the virus from depleting the immune cells that it preferentially attacks (CD4 cells) and prevents or delays illness and death.\n\nDespite the expanding availability of these drugs and the impact of their use, treatments continue to fail, often due to the development of resistance. During drug therapy, low-level virus replication still occurs, particularly when a patient misses a dose. HIV makes errors in copying its genetic material and, if a mutation makes the virus resistant to one or more of the drugs, it may begin to replicate more successfully in the presence of that drug and undermine the effect of the treatment. If this happens then the treatment needs to be changed to re-establish control over the virus.\n\nIn well-resourced healthcare settings, when treatment fails a resistance test may be run to predict to which drugs the patient’s virus is resistant. The type of test in most common use is the genotype test, which detects mutations in the viral genetic code. This information is then typically interpreted using rules equating individual mutations with resistance against individual drugs. However, there are many different interpretation systems available that do not always agree, the systems only provide categorical results (resistant, sensitive or intermediate) and they do not necessarily relate well to how a patient will respond to a combination of drugs in the clinic.\n\nThe RDI was established in 2002 to pioneer a new approach: to develop computational models using the genotype and a wide range of other clinically relevant data collected from thousands of patients treated with HAART all over the world and to use these models to predict how an individual patient will respond to different combinations of drugs. The RDI’s goal was to make available a free treatment-response prediction tool over the Internet.\n\nKey to the success of this approach is the collection of large amounts of data with which to train the models and the use of data from as wide and heterogeneous range of sources as possible to maximise the generalisability of the models’ predictions. In order to achieve this, the RDI set out to involve as many clinics worldwide as possible and to be the single repository for the data required, in an attempt to avoid unnecessary duplication of effort and competition.\n\nAs of October 2013, the RDI has collected data from approximately 110,000 patients from dozens of clinics in more than 30 countries. It is probably the largest database of its kind in the world. The data includes demographic information for the patient, and multiple determinations of the amount of virus in the patient’s bloodstream, CD4 cells counts (a white blood cell critical to the function of the immune system that HIV targets and destroys), genetic code of the patients virus, and details of the drugs that have been used to treat the patient.\n\nThe RDI has used these data to conduct extensive research in order to develop the most accurate system possible for the prediction of treatment response. This research involved the development and comparison of different computational modelling methods including artificial neural networks, support vector machines, random forests and logistical regression.\n\nThe predictions of the RDI’s models have historically correlated well with the actual changes in virus load of patients in the clinic, typically achieving a correlation co-efficient of 0.8 or more.\n\nIn October 2010, following clinical testing in two multinational studies, the RDI made its experimental HIV Treatment Response Prediction System, HIV-TRePS available over the Internet. In January 2011, two clinical studies were published indicating that use of the HIV-TRePS system could lead to clinical and economic benefits. The studies, conducted by expert HIV physicians in the USA, Canada and Italy, showed that use of the system was associated with changes of treatment decision to combinations involving fewer drugs overall, which were predicted to result in better virological responses, suggesting that use of the system could potentially improve patient outcomes and reduce the overall number and cost of drugs used.\n\nRecent models have predicted whether a combination treatment will reduce the level of virus in the patient’s bloodstream to undetectable levels with an accuracy of approximately 80%, significantly better than just using a genotype with rules-based interpretation\n\nAs clinics in resource-limited settings are often unable to afford genotyping, the RDI has developed models that predicted treatment response without the need for a genotype, with only a small loss of accuracy. In July 2011, the RDI made these models available as part of the HIV-TRePS system. This version is aimed particularly at resource-limited settings where genotyping is often not routinely available. The most recent of these models, trained with the largest dataset so far, achieved 80% accuracy, which is comparable to models that use a genotype in their predictions and significantly more accurate than genotyping with rules-based interpretation itself.\n\nHIV-TRePS is now in use in 70 countries as a tool to predict virological response to therapy and avoid treatment failure.\n\nThe system has been expanded to enable physicians to include their local drug costs in the modelling. A recent study of data from an Indian cohort demonstrated that the system was able to identify combinations of three locally available drugs with a higher probability of success than the regimen prescribed in the clinic, including those cases where the treatment used in the clinic failed. Moreover, in all these cases some of the alternatives were less costly than the regimen used in the clinic, suggesting that the system could be not only help avoid treatment failure but also reduce costs.\n\n\n\nCohorts: Peter Reiss and Ard van Sighem (ATHENA, the Netherlands); Julio Montaner and Richard Harrigan (BC Center for Excellence in HIV & AIDS, Canada); Tobias Rinke de Wit, Raph Hamers and Kim Sigaloff (PASER-M cohort, The Netherlands); Brian Agan, Vincent Marconi and Scott Wegner (US Department of Defense); Wataru Sugiura (National Institute of Health, Japan); Maurizio Zazzi (MASTER, Italy); Adrian Streinu-Cercel National Institute of Infectious Diseases Prof.Dr. Matei Balş, Bucharest, Romania; Gerardo Alvarez-Uria (VFHCS, India).\nClinics: Jose Gatell and Elisa Lazzari (University Hospital, Barcelona, Spain); Brian Gazzard, Mark Nelson, Anton Pozniak and Sundhiya Mandalia (Chelsea and Westminster Hospital, London, UK); Lidia Ruiz and Bonaventura Clotet (Fundacion Irsi Caixa, Badelona, Spain); Schlomo Staszewski (Hospital of the Johann Wolfgang Goethe-University, Frankfurt, Germany); Carlo Torti (University of Brescia); Cliff Lane and Julie Metcalf (National Institutes of Health Clinic, Rockville, USA); Maria-Jesus Perez-Elias (Instituto Ramón y Cajal de Investigación Sanitaria, Madrid, Spain); Andrew Carr, Richard Norris and Karl Hesse (Immunology B Ambulatory Care Service, St. Vincent’s Hospital, Sydney, NSW, Australia); Dr Emanuel Vlahakis (Taylor’s Square Private Clinic, Darlinghurst, NSW, Australia); Hugo Tempelman and Roos Barth (Ndlovu Care Group, Elandsdoorn, South Africa), Carl Morrow and Robin Wood (Desmond Tutu HIV Centre, University of Cape Town, South Africa); Luminita Ene (“Dr. Victor Babes” Hospital for Infectious and Tropical Diseases, Bucharest, Romania); Gordana Dragovic (University of Belgrade, Belgrade, Serbia).\nClinical trials: Sean Emery and David Cooper (CREST); Carlo Torti (GenPherex); John Baxter (GART, MDR); Laura Monno and Carlo Torti (PhenGen); Jose Gatell and Bonventura Clotet (HAVANA); Gaston Picchio and Marie-Pierre deBethune (DUET 1 & 2 and POWER 3); Maria-Jesus Perez-Elias (RealVirfen).\n\n", "id": "29069615", "title": "Resistance Database Initiative"}
{"url": "https://en.wikipedia.org/wiki?curid=23951451", "text": "SmartAction\n\nSmartAction provides artificial intelligence-based voice self-service. SmartAction's Intelligent Voice Automation (IVA) is a hosted IVR platform that uses natural language speech recognition and is based on an object-oriented coding framework. IVA is a cloud-based, hosted service. IVA® handles complex customer experience calls across 12 verticals, providing assistance with numerous call types such as appointment scheduling and rescheduling, delivery status, order placements, and a variety of outbound calls.\n\nSmartAction was founded by inventor and entrepreneur Peter Voss and is headquartered in El Segundo, CA.\n\nDeveloping artificial intelligence has challenged researchers since at least the 1940s (see History of artificial intelligence). Part of the problem has been the difficulty of defining an adequate theory of intelligence that can serve as a framework to guide hardware implementations. Starting in the early 1990s Voss developed a new theory of intelligence, outlined in the book \"Artificial General Intelligence\". In 2001, Voss founded an R&D startup, Adaptive AI, Inc., to research and develop a prototype artificial general intelligence system based on his theory of intelligence. In 2009 Voss founded Smart Action Company, LLC to commercialize this technology. Born out of Voss' research and development, SmartAction created its first practical application of this new technology focused on the management of inbound and outbound calls for contact centers. Since its inception, SmartAction has continued to develop the technology into a highly specialized and purpose driven AI, identified by the company as Intelligent Voice Automation.\n\nThere are two layers of technology that SmartAction uses in its systems. The first is speech recognition, which enables the system to understand natural language spoken words and phrases. Second is a proprietary artificial intelligence engine used to determine the meaning and intent behind the spoken words, drive the conversation with the caller, and improve over time. A text-to-speech engine enables the system to speak. The AI engine's built-in knowledge and skills also makes development and updates to the system much faster and easier than with older technology.\n\n\n\n", "id": "23951451", "title": "SmartAction"}
{"url": "https://en.wikipedia.org/wiki?curid=8252994", "text": "Vehicle infrastructure integration\n\nVehicle Infrastructure Integration (VII) is an initiative fostering research and applications development for a series of technologies directly linking road vehicles to their physical surroundings, first and foremost in order to improve road safety. The technology draws on several disciplines, including transport engineering, electrical engineering, automotive engineering, and computer science. VII specifically covers road transport although similar technologies are in place or under development for other modes of transport. Planes, for example, use ground-based beacons for automated guidance, allowing the autopilot to fly the plane without human intervention. In highway engineering, improving the safety of a roadway can enhance overall efficiency. VII targets improvements in both safety and efficiency.\n\nVehicle infrastructure integration is that branch of engineering, which deals with the study and application of a series of techniques directly linking road vehicles to their physical surroundings in order to improve road safety.\n\nThe goal of VII is to provide a communications link between vehicles on the road (via On-Board Equipment, OBE), and between vehicles and the roadside infrastructure (via Roadside Equipment, RSE), in order to increase the safety, efficiency, and convenience of the transportation system. It is based on widespread deployment of a dedicated short-range communications (DSRC) link, incorporating IEEE 802.11p. VII's development relies on a business model supporting the interests of all parties concerned: industry, transportation authorities and professional organisations. The initiative has three priorities:\n\nCurrent active safety technology relies on vehicle-based radar and vision systems. For example, this technology can reduce rear-end collisions by tracking obstructions in front or behind the vehicle, automatically applying brakes when needed. This technology is somewhat limited in that it senses only the distance and speed of vehicles within the direct line of sight of cameras and the sensing range of radars. It is almost completely ineffective for angled and left-turn collisions . It may even cause a motorist to lose control of the vehicle in the event of an impending head-on collision. The rear-end collisions covered by today's technology are typically less severe than angle, left-turn, or head-on collisions. Existing technology is therefore inadequate for the overall needs of the roadway system. \n\nVII would provide a direct link between a vehicle on the road and all vehicles within a defined vicinity. The vehicles would be able to communicate with each other, exchanging data on speed, orientation, perhaps even on driver awareness and intent. This could increase safety for nearby vehicles, while enhancing the overall sensitivity of the VII system, for example, by performing an automated emergency maneuver (steering, decelerating, braking) more effectively. In addition, the system is designed to communicate with the roadway infrastructure, allowing for complete, real-time traffic information for the entire network, as well as better queue management and feedback to vehicles. It would ultimately close the feedback loops on what is now an open-loop transportation system.\n\nThrough VII, roadway markings and road signs could become obsolete. Existing VII applications use sensors within vehicles which can identify markings on the roadway or signing along the side of the road, automatically adjusting vehicle parameters as necessary. Ultimately, VII aims to treat such signs and markings as little more than stored data within the system. This could be in the form of data acquired via beacons along a roadway or stored at a centralised database and distributed to all VII-equipped vehicles.\n\nAll the above factors are largely in response to safety but VII could lead to noticeable gains in the operational efficiency of a transportation network. As vehicles will be linked together with a resulting decrease in reaction times, the headway between vehicles could be reduced so that there is less empty space on the road. Available capacity for traffic would therefore be increased. More capacity per lane will in turn mean fewer lanes in general, possibly satisfying the community's concerns about the impact of roadway widening. VII will enable precise traffic-signal coordination by tracking vehicle platoons and will benefit from accurate timing by drawing on real-time traffic data covering volume, density and turning movements.\n\nReal-time traffic data can also be used in the design of new roadways or modification of existing systems as the data could be used to provide accurate origin-destination studies and turning-movement counts for uses in transportation forecasting and traffic operations. Such technology would also lead to improvements for transport engineers to address problems whilst reducing the cost of obtaining and compiling data. Tolling is another prospect for VII technology as it could enable roadways to be automatically tolled. Data could be collectively transmitted to road users for in-vehicle display, outlining the lowest cost, shortest distance, and/or fastest route to a destination on the basis of real-time conditions.\n\nTo some extent, results along these lines have been achieved in trials performed around the globe, making use of GPS, mobile phone signals, and vehicle registration plates. GPS is becoming standard in many new high-end vehicles and is an option on most new low- and mid-range vehicles. In addition, many users also have mobile phones which transmit trackable signals (and may also be GPS-enabled). Mobile phones can already be traced for purposes of emergency response. GPS and mobile phone tracking, however, do not provide fully reliable data. Furthermore, integrating mobile phones in vehicles may be prohibitively difficult. Data from mobile phones, though useful, might even increase risks to motorists as they tend to look at their phones rather than concentrate on their driving. Automatic registration plate recognition can provide high levels of data, but continuously tracking a vehicle through a corridor is a difficult task with existing technology. Today's equipment is designed for data acquisition and functions such as enforcement and tolling, not for returning data to vehicles or motorists for response. GPS will nevertheless be one of the key components in VII systems.\n\nThere are numerous limitations to the development of VII. A common misconception is that the biggest challenge to VII technology is the computing power that can be fitted inside a vehicle. While this is indeed a challenge, the technology for computers has been advancing rapidly and is not a particular concern for VII researchers. Given the fact that technologies already exist for the most basic of forms of VII, perhaps the greatest hurdle to the deployment of VII technology is public acceptance.\n\nThe most common myth about VII is that it includes tracking technology; however, this is not the case. The architecture is designed to prevent identification of individual vehicles, with all data exchange between the vehicle and the system occurring anonymously. Exchanges between the vehicles and third parties such as OEMs and toll collectors will occur, but the network traffic will be sent via encrypted tunnels and will therefore not be decipherable by the VII system.\n\nAlthough the system will be able to detect signal and speed violations, it will not have the capability to identify the violator and report them. The detection is for the purpose of alerting the violator and/or approaching vehicles, to prevent collisions.\n\nOther public acceptance concerns come from advocates of recreational driving as well as from critics of tolling. The former argue that VII will increase the automation of the vehicle, reducing the driver's enjoyment. Recreational driving concerns are particularly prevalent among owners of sports cars. They could be attenuated by compensating for the presence of vehicles without VII or perhaps by maintaining roadways where vehicles without VII are permitted to travel.\n\nThose opposed to tolling believe it will make driving prohibitively expensive for motorists in the lower-income bracket, conflicting with the general wish to provide equal services for all. In response, public transit discounts or road use discounts can be considered for qualifying individuals and/or families. Such provisions currently exist for numerous tolled roadways and could be applicable to roadways that are tolled via VII. However, as VII could allow for the tolling of \"every\" VII-enabled roadway, the provisions may be ineffective in view of the increased need to provide user-efficient transit services to every area.\n\nA major issue facing the deployment of VII is the problem of how to stand up the system initially. The costs associated with installing the technology in vehicles and providing communications and power at every intersection are significant. Building out the infrastructure along the roadside without the auto manufacturers' cooperation would be disastrous, as would the reverse situation; therefore, the two parties will need to work together to make the VII concept work.\n\nThere are proof of concept tests being performed in Michigan and California that will be evaluated by the US DOT and the auto manufacturers, and a decision will be made, jointly, about whether or not to move forward with implementation of the system at that time.\n\nAnother factor for consideration in regard to the technology's distribution is how to update and maintain the units. Traffic systems are highly dynamic, with new traffic controls implemented every day and roadways constructed or repaired every year. The vehicle-based option could be updated via the internet (preferably wireless), but may subsequently require all users to have access to internet technology. Many local government agencies have been testing deployment of internet facilities in cities and along roadways, for example at rest-stops. These systems could be used for VII updating.\n\nAn additional option is to provide updates whenever a vehicle is brought in for inspection or servicing. A major limitation here is that updating would be in the hands of the user. Some vehicle owners maintain their vehicles themselves, and periodic inspections or servicing are considered too infrequent for updating VII. Motorists might also be reluctant to stop at rest-stops for an update if they do not have the possibility of driving in an internet-enabled city.\n\nAlternatively, if receivers were placed in all vehicles and the VII system was primarily located along the roadside, information could be stored in a centralised database. This would allow the agency responsible to issue updates at any time. These would then be disseminated to the roadside units for passing motorists. Operationally, this method is currently considered to provide the greatest effectiveness but at a high cost to the authorities.\n\nSecurity of the units is another concern, especially in the light of the public acceptance issue. Criminals could tamper with VII units, or remove and/or destroy them regardless of whether they are installed inside vehicles or along the roadside. If they are placed inside vehicles, laws similar to those for tampering with an odometer could be enacted; and the units could be examined during inspections or services for signs of tampering. This method has many of the limitations mentioned in relation to the frequency of inspection and motorists who perform their own servicing. It also raises concerns regarding the honesty of vehicle technicians performing the inspections. The ability of technicians to identify signs of tampering would be dependent on their knowledge of the VII systems themselves.\n\nMagnets, electric shocks, and malicious software (viruses, hacking, or jamming) could be used to damage VII systems - regardless of whether units are located inside vehicle or along the roadside. Extensive training and certification would be required for technicians to inspect VII units within a vehicle. Along the roadside, a high degree of security would be required to ensure that the equipment is not damaged and to increase its durability. However, as roadside units could well be placed on the public right-of-way - which is often close to the edge of the roadway - there could be concerns about vehicles hitting them (whether on purpose or by accident). The units would either have to be built so that they do not provide a threat to motorists: perhaps in the form of a low-profile and/or low-mass object designed to be run over or to break apart (which would entail a relatively inexpensive unit); or the unit would have to be shielded by a device such as a guardrail, raising safety concerns of its own.\n\nYet another limitation is in digitizing the inputs for the VII system. VII systems will probably continue to sense existing signs and roadway markings but one of the goals is to eliminate such signs and markings altogether. This would require converting the locations and messages of each item into the VII system's format. Responsibility for this work would probably fall on the highway agencies which nearly all face difficulties in funding, manpower, and available time. Implementing and maintaining VII systems may therefore require support at the national level.\n\nWhile VII is largely being developed as a joint research enterprise involving numerous transport agencies, it is likely initial products will be tailored to individual applications. As a result, compatibility and formatting issues could well arise as systems expand. Overcoming these difficulties could require complicated translation programs between different systems or possibly a complete overhaul of existing VII systems in order to develop a more comprehensive approach. In either case, the costs and potential for bugs in the software will likely be high.\n\nLegislation will be required to set in place access to the VII data and communications between applicable agencies. In the USA, for example, an Interstate is a Federal roadway that is often maintained by the State, but the local county or municipal authorities may be involved too. The legislation would need to set the levels of authority of each agency. In Pennsylvania, for example, municipalities tend to have greater authority than counties and sometimes even the State whereas neighboring Maryland has more authority at the county level than at municipal level; and State roads are almost exclusively controlled by the State. It would also have to be determined which other agencies can use the data (i.e. law enforcement, Census, etc.) and to what degree it is permissible to use the information. Law enforcement would be needed to minimise data misuse. The various levels of authority could also increase incompatibility.\n\nMuch of the current research and experimentation is conducted in the United States where coordination is ensured through the Vehicle Infrastructure Integration Consortium, consisting of automobile manufacturers (Ford, General Motors, DaimlerChrysler, Toyota, Nissan, Honda, Volkswagen, BMW), IT suppliers, U.S. Federal and state transportation departments, and professional associations. Trialling is taking place in Michigan and California.\n\nThe specific applications now being developed under the U.S. initiative\nare:\nIn mid-2007, a VII environment covering some near Detroit will be used to test 20 prototype VII applications. Several automobile manufacturers are also conducting their own VII research and trialling.\n\n\n", "id": "8252994", "title": "Vehicle infrastructure integration"}
{"url": "https://en.wikipedia.org/wiki?curid=15795950", "text": "Activity recognition\n\nActivity recognition aims to recognize the actions and goals of one or more agents from a series of observations on the agents' actions and the environmental conditions. Since the 1980s, this research field has captured the attention of several computer science communities due to its strength in providing personalized support for many different applications and its connection to many different fields of study such as medicine, human-computer interaction, or sociology.\nDue to its many-faceted nature, different fields may refer to activity recognition as plan recognition, goal recognition, intent recognition, behavior recognition, location estimation and location-based services.\n\nSensor-based activity recognition integrates the emerging area of sensor networks with novel data mining and machine learning techniques to model a wide range of human activities. Mobile devices (e.g. smart phones) provide sufficient sensor data and calculation power to enable physical activity recognition to provide an estimation of the energy consumption during everyday life. Sensor-based activity recognition researchers believe that by empowering ubiquitous computers and sensors to monitor the behavior of agents (under consent), these computers will be better suited to act on our behalf.\n\nSensor-based activity recognition is a challenging task due to the inherent noisy nature of the input. Thus, statistical modeling has been the main thrust in this direction in layers, where the recognition at several intermediate levels is conducted and connected. At the lowest level where the sensor data are collected, statistical learning concerns how to find the detailed locations of agents from the received signal data. At an intermediate level, statistical inference may be concerned about how to recognize individuals' activities from the inferred location sequences and environmental conditions at the lower levels. Furthermore, at the highest level a major concern is to find out the overall goal or subgoals of an agent from the activity sequences through a mixture of logical and statistical reasoning. Scientific conferences where activity recognition work from wearable and environmental often appears are ISWC and UbiComp.\n\nRecognizing activities for multiple users using on-body sensors first appeared in the work by ORL using active badge systems in the early 90's. Other sensor technology such as acceleration sensors were used for identifying group activity patterns during office scenarios. Activities of Multiple Users in intelligent environments are addressed in Gu et al. In this work, they investigate the fundamental problem of recognizing activities for multiple users from sensor readings in a home environment, and propose a novel pattern mining approach to recognize both single-user and multi-user activities in a unified solution.\n\nRecognition of group activities is fundamentally different from single, or multi-user activity recognition in that the goal is to recognize the behavior of the group as an entity, rather than the activities of the individual members within it. Group behavior is emergent in nature, meaning that the properties of the behavior of the group are fundamentally different then the properties of the behavior of the individuals within it, or any sum of that behavior. The main challenges are in modeling the behavior of the individual group members, as well as the roles of the individual within the group dynamic and their relationship to emergent behavior of the group in parallel. Challenges which must still be addressed include quantification of the behavior and roles of individuals who join the group, integration of explicit models for role description into inference algorithms, and scalability evaluations for very large groups and crowds. Group activity recognition has applications for crowd management and response in emergency situations, as well as for social networking and Quantified Self applications.\n\nLogic-based approaches keep track of all logically consistent explanations of the observed actions. Thus, all possible and consistent plans or goals must be considered. Kautz provided a formal theory of plan recognition. He described plan recognition as a logical inference process of circumscription. All actions, plans are uniformly referred to as goals, and a recognizer's knowledge is represented by a set of first-order statements called event hierarchy encoded in first-order logic, which defines abstraction, decomposition and functional relationships between types of events.\n\nKautz's general framework for plan recognition has an exponential time complexity in worst case, measured in the size of input hierarchy. Lesh and Etzioni went one step further and presented methods in scaling up goal recognition to scale up his work computationally. In contrast to Kautz's approach where the plan library is explicitly represented, Lesh and Etzioni's approach enables automatic plan-library construction from domain primitives. Furthermore, they introduced compact representations and efficient algorithms for goal recognition on large plan libraries.\n\nInconsistent plans and goals are repeatedly pruned when new actions arrive. Besides, they also presented methods for adapting a goal recognizer to handle individual idiosyncratic behavior given a sample of an individual's recent behavior. Pollack et al. described a direct argumentation model that can know about the relative strength of several kinds of arguments for belief and intention description.\n\nA serious problem of logic-based approaches is their inability or inherent infeasibility to represent uncertainty. They offer no mechanism for preferring one consistent approach to another and incapable of deciding whether one particular plan is more likely than another, as long as both of them can be consistent enough to explain the actions observed. There is also a lack of learning ability associated with logic based methods.\n\nAnother approach to logic-based activity recognition is to use stream reasoning based on Answer Set Programming, and has been applied to recognising activities for health-related applications, which uses weak constraints to model a degree of ambiguity/uncertainty.\n\nProbability theory and statistical learning models are more recently applied in activity recognition to reason about actions, plans and goals under uncertainty. In the literature, there have been several approaches which explicitly represent uncertainty in reasoning about an agent's plans and goals.\n\nUsing sensor data as input, Hodges and Pollack designed machine learning-based systems for identifying individuals as they perform routine daily activities such as making coffee. Intel Research (Seattle) Lab and University of Washington at Seattle have done some important works on using sensors to detect human plans. Some of these works infer user transportation modes from readings of radio-frequency identifiers (RFID) and global positioning systems (GPS).\n\nThe use of temporal probabilistic models has been shown to perform well in activity recognition and generally outperform non-temporal models. Generative models such as the Hidden Markov Model (HMM) and the more generally formulated Dynamic Bayesian Networks (DBN) are popular choices in modelling activities from sensor data.\nDiscriminative models such as Conditional Random Fields (CRF) are also commonly applied and also give good performance in activity recognition.\n\nGenerative and discriminative models both have their pros and cons and the ideal choice depends on their area of application. A dataset together with implementations of a number of popular models (HMM, CRF) for activity recognition can be found here.\n\nConventional temporal probabilistic models such as the hidden Markov model (HMM) and conditional random fields (CRF) model directly model the correlations between the activities and the observed sensor data. In recent years, increasing evidence has supported the use of hierarchical models which take into account the rich hierarchical structure that exists in human behavioral data. The core idea here is that the model does not directly correlate the activities with the sensor data, but instead breaks the activity into sub-activities (sometimes referred to as actions) and models the underlying correlations accordingly. An example could be the activity of preparing spaghetti, which can be broken down into the subactivities or actions of cutting vegetables, frying the vegetables in a pan and serving it on a plate. Examples of such a hierarchical model are Layered Hidden Markov Models (LHMMs) and the hierarchical hidden Markov model (HHMM), which have been shown to significantly outperform its non-hierarchical counterpart in activity recognition.\n\nDifferent from traditional machine learning approaches, an approach based on data mining has been recently proposed.\nIn the work of Gu et al., the problem of activity recognition is formulated as a pattern-based classification problem. They proposed a data mining approach based on discriminative patterns which describe significant changes between any two activity classes of data to recognize sequential, interleaved and concurrent activities in a unified solution.\nGilbert \"et al.\" use 2D corners in both space and time. These are grouped spatially and temporally using a hierarchical process, with an increasing search area. At each stage of the hierarchy, the most distinctive and descriptive features are learned efficiently through data mining (Apriori rule).\n\nIt is a very important and challenging problem to track and understand the behavior of agents through videos taken by various cameras. The primary technique employed is computer vision. Vision-based activity recognition has found many applications such as human-computer interaction, user interface design, robot learning, and surveillance, among others.\nScientific conferences where vision based activity recognition work often appears are ICCV and CVPR.\n\nIn vision-based activity recognition, a great deal of work has been done. Researchers have attempted a number of methods such as optical flow, Kalman filtering, Hidden Markov models, etc., under different modalities such as single camera, stereo, and infrared. In addition, researchers have considered multiple aspects on this topic, including single pedestrian tracking, group tracking, and detecting dropped objects.\n\nRecently some researchers have used RGBD cameras like Microsoft Kinect to detect human activities. Depth cameras add extra dimension i.e. depth which normal 2d camera fails to provide. Sensory information from these depth cameras have been used to generate real-time skeleton model of humans with different body positions. These skeleton information provides meaningful information that researchers have used to model human activities which are trained and later used to recognize unknown activities.\n\nIn vision-based activity recognition, the computational process is often divided into four steps, namely human detection, human tracking, human activity recognition and then a high-level activity evaluation.\n\nOne way to identify specific people is by how they walk. Gait-recognition software can be used to record a person's gait or gait feature profile in a database for the purpose of recognizing that person later, even if they are wearing a disguise.\n\nWhen activity recognition is performed indoors and in cities using the widely available Wi-Fi signals and 802.11 access points, there is much noise and uncertainty. These uncertainties are modeled using a dynamic Bayesian network model by Yin et al. A multiple goal model that can reason about user's interleaving goals is presented by Chai and Yang, where a deterministic state transition model is applied. A better model that models the concurrent and interleaving activities in a probabilistic approach is proposed by Hu and Yang. A user action discovery model is presented by Yin et al., where the Wi-Fi signals are segmented to produce possible actions.\n\nA fundamental problem in Wi-Fi-based activity recognition is to estimate the user locations. Two important issues are how to reduce the human labelling effort and how to cope with the changing signal profiles when the environment changes. Yin et al. dealt with the second issue by transferring the labelled knowledge between time periods. Chai and Yang proposed a hidden Markov model-based method to extend labelled knowledge by leveraging the unlabelled user traces. J. Pan et al. propose to perform location estimation through online co-localization, and S. Pan et al. proposed to apply multi-view learning for migrating the labelled data to a new time period.\n\nMany different applications have been studied by researchers in activity recognition; examples include assisting the sick and disabled. For example, Pollack et al. show that by automatically monitoring human activities, home-based rehabilitation can be provided for people suffering from traumatic brain injuries. One can find applications ranging from security-related applications and logistics support to location-based services.\n\n\n\n\n\n", "id": "15795950", "title": "Activity recognition"}
{"url": "https://en.wikipedia.org/wiki?curid=8006328", "text": "Chinese speech synthesis\n\nChinese speech synthesis is the application of speech synthesis to the Chinese language (usually Standard Chinese). It poses additional difficulties due to the Chinese characters (which frequently have different pronunciations in different contexts), the complex prosody, which is essential to convey the meaning of words, and sometimes the difficulty in obtaining agreement among native speakers concerning what the correct pronunciation is of certain phonemes.\n\nRecordings can be concatenated in any desired combination, but the joins sound forced (as is usual for simple concatenation-based speech synthesis) and this can severely affect prosody; these synthesizers are also inflexible in terms of speed and expression. However, because these synthesizers do not rely on a corpus, there is no noticeable degradation in performance when they are given more unusual or awkward phrases.\n\nEkho is an open source TTS which simply concatenates sampled syllables. It currently supports Cantonese, Mandarin, and experimentally Korean. Some of the Mandarin syllables have been pitched-normalised in Praat. A modified version of these is used in Gradint's \"synthesis from partials\".\n\ncjkware.com used to ship a product called KeyTip Putonghua Reader which worked similarly; it contained 120 Megabytes of sound recordings (GSM-compressed to 40 Megabytes in the evaluation version), comprising 10,000 multi-syllable dictionary words plus single-syllable recordings in 6 different prosodies (4 tones, neutral tone, and an extra third-tone recording for use at the end of a phrase).\n\nThe lightweight open-source speech project eSpeak, which has its own approach to synthesis, has experimented with Mandarin and Cantonese. eSpeak was used by Google Translate from May 2010 until December 2010.\n\nThe commercial product \"Yuet\" is also lightweight (it is intended to be suitable for resource-constrained environments like embedded systems); it was written from scratch in ANSI C starting from 2013. Yuet claims a built-in NLP model that does not require a separate dictionary; the speech synthesised by the engine claims clear word boundaries and emphasis on appropriate words. Communication with its author is required to obtain a copy.\n\nBoth eSpeak and Yuet can synthesis speech for Cantonese and Mandarin from the same input text, and can output the corresponding romanisation (for Cantonese, Yuet uses Yale and eSpeak uses Jyutping; both use Pinyin for Mandarin). eSpeak does not concern itself with word boundaries when these don't change the question of which syllable should be spoken.\n\nA \"corpus-based\" approach can sound very natural in most cases but can err in dealing with unusual phrases if they can't be matched with the corpus. The synthesiser engine is typically very large (hundreds or even thousands of megabytes) due to the size of the corpus.\n\nAnhui USTC iFlyTek Co., Ltd (iFlyTek) published a W3C paper in which they adapted Speech Synthesis Markup Language to produce a mark-up language called Chinese Speech Synthesis Markup Language (CSSML) which can include additional markup to clarify the pronunciation of characters and to add some prosody information. The amount of data involved is not disclosed by iFlyTek but can be seen from the commercial products that iFlyTek have licensed their technology to; for example, Bider's SpeechPlus is a 1.3 Gigabyte download, 1.2 Gigabytes of which is used for the highly compressed data for a single Chinese voice. iFlyTek's synthesiser can also synthesise mixed Chinese and English text with the same voice (e.g. Chinese sentences containing some English words); they claim their English synthesis to be \"average\".\n\nThe iFlyTek corpus appears to be heavily dependent on Chinese characters, and it is not possible to synthesize from pinyin alone. It is sometimes possible by means of CSSML to add pinyin to the characters to disambiguate between multiple possible pronunciations, but this does not always work.\n\nThere is an online interactive demonstration for NeoSpeech speech synthesis, which accepts Chinese characters and also pinyin if it's enclosed in their proprietary \"VTML\" markup.\n\nMac OS had Chinese speech synthesizers available up to version 9. This was removed in 10.0 and reinstated in 10.7 (Lion).\n\nA corpus-based approach was taken by Tsinghua University in SinoSonic, with the Harbin dialect voice data taking 800 Megabytes. This was planned to be offered as a download but the link was never activated. Nowadays, references to it can be found only on Internet Archive.\n\nBell Labs' approach, which was demonstrated online in 1997 but subsequently removed, was described in a monograph \"Multilingual Text-to-Speech Synthesis: The Bell Labs Approach\" (Springer, October 31, 1997, ), and the former employee who was responsible for the project, Chilin Shih (who subsequently worked at the University of Illinois) put some notes about her methods on her website.\n", "id": "8006328", "title": "Chinese speech synthesis"}
{"url": "https://en.wikipedia.org/wiki?curid=30285715", "text": "Mind's Eye (US military)\n\nThe Mind's Eye is a video analysis research project using artificial intelligence. It is funded by the Defense Advanced Research Projects Agency.\n\nTwelve research teams have been contracted by DARPA for the Mind's Eye: Carnegie Mellon University, Co57 Systems, Inc., Colorado State University, Jet Propulsion Laboratory/Caltech, Massachusetts Institute of Technology, Purdue University, SRI International, State University of New York at Buffalo, TNO (Netherlands), University of Arizona, University of California Berkeley and the University of Southern California.\n\n\"The Mind's Eye program seeks to develop in machines a capability that exists only in animals: visual intelligence. This program pursues the capability to learn generally applicable and generative representations of action between objects in a scene directly from visual inputs, and then reason over those learned representations. A key distinction between this research and the state of the art in machine vision is that the latter has made continual progress in recognizing a wide range of objects and their properties - what might be thought of as the nouns in the description of a scene. The focus of Mind's Eye is to add the perceptual and cognitive underpinnings for recognizing and reasoning about the verbs in those scenes, enabling a more complete narrative of action in the visual experience.\"\n\n", "id": "30285715", "title": "Mind's Eye (US military)"}
{"url": "https://en.wikipedia.org/wiki?curid=318439", "text": "Text mining\n\nText mining, also referred to as text data mining, roughly equivalent to text analytics, is the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interestingness. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (\"i.e.\", learning relations between named entities).\n\nText analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods.\n\nA typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted.\n\nThe term text analytics describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources for business intelligence, exploratory data analysis, research, or investigation. The term is roughly synonymous with text mining; indeed, Ronen Feldman modified a 2000 description of \"text mining\" in 2004 to describe \"text analytics\". The latter term is now used more frequently in business settings while \"text mining\" is used in some of the earliest application areas, dating to the 1980s, notably life-sciences research and government intelligence.\n\nThe term text analytics also describes that application of text analytics to respond to business problems, whether independently or in conjunction with query and analysis of fielded, numerical data. It is a truism that 80 percent of business-relevant information originates in unstructured form, primarily text. These techniques and processes discover and present knowledge – facts, business rules, and relationships – that is otherwise locked in textual form, impenetrable to automated processing.\n\nIncreasing interest is being paid to multilingual data mining: the ability to gain information across languages and cluster similar items from different linguistic sources according to their meaning.\n\nThe challenge of exploiting the large proportion of enterprise information that originates in \"unstructured\" form has been recognized for decades. It is recognized in the earliest definition of business intelligence (BI), in an October 1958 IBM Journal article by H.P. Luhn, A Business Intelligence System, which describes a system that will:\n\"...utilize data-processing machines for auto-abstracting and auto-encoding of documents and for creating interest profiles for each of the 'action points' in an organization. Both incoming and internally generated documents are automatically abstracted, characterized by a word pattern, and sent automatically to appropriate action points.\"\nYet as management information systems developed starting in the 1960s, and as BI emerged in the '80s and '90s as a software category and field of practice, the emphasis was on numerical data stored in relational databases. This is not surprising: text in \"unstructured\" documents is hard to process. The emergence of text analytics in its current form stems from a refocusing of research in the late 1990s from algorithm development to application, as described by Prof. Marti A. Hearst in the paper Untangling Text Data Mining:\n\nFor almost a decade the computational linguistics community has viewed large text collections as a resource to be tapped in order to produce better text analysis algorithms. In this paper, I have attempted to suggest a new emphasis: the use of large online text collections to discover new facts and trends about the world itself. I suggest that to make progress we do not need fully artificial intelligent text analysis; rather, a mixture of computationally-driven and user-guided analysis may open the door to exciting new results.\nHearst's 1999 statement of need fairly well describes the state of text analytics technology and practice a decade later.\n\nSubtasks—components of a larger text-analytics effort—typically include:\n\nThe technology is now broadly applied for a wide variety of government, research, and business needs. Applications can be sorted into a number of categories by analysis type or by business function. Using this approach to classifying solutions, application categories include:\n\nMany text mining software packages are marketed for security applications, especially monitoring and analysis of online plain text sources such as Internet news, blogs, etc. for national security purposes. It is also involved in the study of text encryption/decryption.\n\nA range of text mining applications in the biomedical literature has been described.\n\nOne online text mining application in the biomedical literature is PubGene that combines biomedical text mining with network visualization as an Internet service.\n\nGoPubMed is a knowledge-based search engine for biomedical texts.\n\nText mining methods and software is also being researched and developed by major firms, including IBM and Microsoft, to further automate the mining and analysis processes, and by different firms working in the area of search and indexing in general as a way to improve their results.\nWithin public sector much effort has been concentrated on creating software for tracking and monitoring terrorist activities.\n\nText mining is being used by large media companies, such as the Tribune Company, to clarify information and to provide readers with greater search experiences, which in turn increases site \"stickiness\" and revenue. Additionally, on the back end, editors are benefiting by being able to share, associate and package news across properties, significantly increasing opportunities to monetize content.\n\nText mining is starting to be used in marketing as well, more specifically in analytical customer relationship management. Coussement and Van den Poel (2008)<ref name=\"10.1016/j.im.2008.01.005\"></ref> apply it to improve predictive analytics models for customer churn (customer attrition). Text mining is also being applied in stock returns prediction.\n\nSentiment analysis may involve analysis of movie reviews for estimating how favorable a review is for a movie.\nSuch an analysis may need a labeled data set or labeling of the affectivity of words.\nResources for affectivity of words and concepts have been made for WordNet and ConceptNet, respectively.\n\nText has been used to detect emotions in the related area of affective computing. Text based approaches to affective computing have been used on multiple corpora such as students evaluations, children stories and news stories.\n\nThe issue of text mining is of importance to publishers who hold large databases of information needing indexing for retrieval. This is especially true in scientific disciplines, in which highly specific information is often contained within written text. Therefore, initiatives have been taken such as Nature's proposal for an Open Text Mining Interface (OTMI) and the National Institutes of Health's common Journal Publishing Document Type Definition (DTD) that would provide semantic cues to machines to answer specific queries contained within text without removing publisher barriers to public access.\n\nAcademic institutions have also become involved in the text mining initiative:\n\n\nThe automatic analysis of vast textual corpora has created the possibility for scholars to analyse\nmillions of documents in multiple languages with very limited manual intervention.\nKey enabling technologies have been parsing, machine translation, topic categorization, and machine learning.\nThe automatic parsing of textual corpora has enabled the extraction of actors and their relational networks on a vast scale,\nturning textual data into network data. The resulting networks, which can contain thousands of nodes, are then analysed by using tools from network theory to identify the key actors, the key communities or parties, and general properties such as robustness or structural stability of the overall network, or centrality of certain nodes. This automates the approach introduced by quantitative narrative analysis, whereby subject-verb-object triplets are identified with pairs of actors linked by an action, or pairs formed by actor-object.\n\nContent analysis has been a traditional part of social sciences and media studies for a long time. The automation of content analysis has allowed a \"big data\" revolution to take place in that field, with studies in social media and newspaper content that include millions of news items. Gender bias, readability, content similarity, reader preferences, and even mood have been analyzed based on text mining methods over millions of documents. The analysis of readability, gender bias and topic bias was demonstrated in Flaounas et al. showing how different topics have different gender biases and levels of readability; the possibility to detect mood patterns in a vast population by analysing Twitter content was demonstrated as well.\n\nText mining computer programs are available from many commercial and open source companies and sources. See List of text mining software.\n\nBecause of a lack of flexibilities in European copyright and database law, the mining of in-copyright works (such as web mining) without the permission of the copyright owner is illegal. In the UK in 2014, on the recommendation of the Hargreaves review the government amended copyright law to allow text mining as a limitation and exception. It was only the second country in the world to do so, following Japan, which introduced a mining-specific exception in 2009. However, owing to the restriction of the Copyright Directive, the UK exception only allows content mining for non-commercial purposes. UK copyright law does not allow this provision to be overridden by contractual terms and conditions.\n\nThe European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe. The fact that the focus on the solution to this legal issue was licences, and not limitations and exceptions to copyright law, led representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.\n\nBy contrast to Europe, the flexible nature of US copyright law, and in particular fair use, means that text mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea, is viewed as being legal. As text mining is transformative, meaning that it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitisation project of in-copyright books was lawful, in part because of the transformative uses that the digitisation project displayed—one such use being text and data mining.\n\nUntil recently, websites most often used text-based searches, which only found documents containing specific user-defined words or phrases. Now, through use of a semantic web, text mining can find content based on meaning and context (rather than just by a specific word). Additionally, text mining software can be used to build large dossiers of information about specific people and events. For example, large datasets based on data extracted from news reports can be built to facilitate social networks analysis or counter-intelligence. In effect, the text mining software may act in a capacity similar to an intelligence analyst or research librarian, albeit with a more limited scope of analysis. Text mining is also used in some email spam filters as a way of determining the characteristics of messages that are likely to be advertisements or other unwanted material. Text mining plays an important role in determining financial market sentiment.\n\n\n", "id": "318439", "title": "Text mining"}
{"url": "https://en.wikipedia.org/wiki?curid=42799", "text": "Speech synthesis\n\nSpeech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech computer or speech synthesizer, and can be implemented in software or hardware products. A text-to-speech (TTS) system converts normal language text into speech; other systems render symbolic linguistic representations like phonetic transcriptions into speech.\n\nSynthesized speech can be created by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units; a system that stores phones or diphones provides the largest output range, but may lack clarity. For specific usage domains, the storage of entire words or sentences allows for high-quality output. Alternatively, a synthesizer can incorporate a model of the vocal tract and other human voice characteristics to create a completely \"synthetic\" voice output.\n\nThe quality of a speech synthesizer is judged by its similarity to the human voice and by its ability to be understood clearly. An intelligible text-to-speech program allows people with visual impairments or reading disabilities to listen to written words on a home computer. Many computer operating systems have included speech synthesizers since the early 1990s.\n\nA text-to-speech system (or \"engine\") is composed of two parts: a front-end and a back-end. The front-end has two major tasks. First, it converts raw text containing symbols like numbers and abbreviations into the equivalent of written-out words. This process is often called \"text normalization\", \"pre-processing\", or \"tokenization\". The front-end then assigns phonetic transcriptions to each word, and divides and marks the text into prosodic units, like phrases, clauses, and sentences. The process of assigning phonetic transcriptions to words is called \"text-to-phoneme\" or \"grapheme-to-phoneme\" conversion. Phonetic transcriptions and prosody information together make up the symbolic linguistic representation that is output by the front-end. The back-end—often referred to as the \"synthesizer\"—then converts the symbolic linguistic representation into sound. In certain systems, this part includes the computation of the \"target prosody\" (pitch contour, phoneme durations), which is then imposed on the output speech.\n\nLong before the invention of electronic signal processing, some people tried to build machines to emulate human speech. Some early legends of the existence of \"Brazen Heads\" involved Pope Silvester II (d. 1003 AD), Albertus Magnus (1198–1280), and Roger Bacon (1214–1294).\n\nIn 1779 the German-Danish scientist Christian Gottlieb Kratzenstein won the first prize in a competition announced by the Russian Imperial Academy of Sciences and Arts for models he built of the human vocal tract that could produce the five long vowel sounds (in notation: , , , and ). There followed the bellows-operated \"acoustic-mechanical speech machine\" of Wolfgang von Kempelen of Pressburg, Hungary, described in a 1791 paper. This machine added models of the tongue and lips, enabling it to produce consonants as well as vowels. In 1837, Charles Wheatstone produced a \"speaking machine\" based on von Kempelen's design, and in 1846, Joseph Faber exhibited the \"Euphonia\". In 1923 Paget resurrected Wheatstone's design.\n\nIn the 1930s Bell Labs developed the vocoder, which automatically analyzed speech into its fundamental tones and resonances. From his work on the vocoder, Homer Dudley developed a keyboard-operated voice-synthesizer called The Voder (Voice Demonstrator), which he exhibited at the 1939 New York World's Fair.\n\nDr. Franklin S. Cooper and his colleagues at Haskins Laboratories built the Pattern playback in the late 1940s and completed it in 1950. There were several different versions of this hardware device; only one currently survives. The machine converts pictures of the acoustic patterns of speech in the form of a spectrogram back into sound. Using this device, Alvin Liberman and colleagues discovered acoustic cues for the perception of phonetic segments (consonants and vowels).\n\nIn 1975 was released MUSA, one of the first Speech Synthesis system: it was able to read Italian; a second version, released in 1978, was able also to sing in Italian \"a cappella\" style.\n\nDominant systems in the 1980s and 1990s were the DECtalk system, based largely on the work of Dennis Klatt at MIT, and the Bell Labs system; the latter was one of the first multilingual language-independent systems, making extensive use of natural language processing methods.\n\nEarly electronic speech-synthesizers sounded robotic and were often barely intelligible. The quality of synthesized speech has steadily improved, but output from contemporary speech synthesis systems remains clearly distinguishable from actual human speech.\n\nKurzweil predicted in 2005 that as the cost-performance ratio caused speech synthesizers to become cheaper and more accessible, more people would benefit from the use of text-to-speech programs.\n\nThe first computer-based speech-synthesis systems originated in the late 1950s. Noriko Umeda \"et al.\" developed the first general English text-to-speech system in 1968 at the Electrotechnical Laboratory, Japan. In 1961 physicist John Larry Kelly, Jr and his colleague Louis Gerstman used an IBM 704 computer to synthesize speech, an event among the most prominent in the history of Bell Labs. Kelly's voice recorder synthesizer (vocoder) recreated the song \"Daisy Bell\", with musical accompaniment from Max Mathews. Coincidentally, Arthur C. Clarke was visiting his friend and colleague John Pierce at the Bell Labs Murray Hill facility. Clarke was so impressed by the demonstration that he used it in the climactic scene of his screenplay for his novel \"\", where the HAL 9000 computer sings the same song as astronaut Dave Bowman puts it to sleep. Despite the success of purely electronic speech synthesis, research into mechanical speech-synthesizers continues.\n\nHandheld electronics featuring speech synthesis began emerging in the 1970s. One of the first was the Telesensory Systems Inc. (TSI) \"Speech+\" portable calculator for the blind in 1976. Other devices had primarily educational purposes, such as the Speak & Spell toy produced by Texas Instruments in 1978. Fidelity released a speaking version of its electronic chess computer in 1979. The first video game to feature speech synthesis was the 1980 shoot 'em up arcade game, \"Stratovox\" (known in Japan as \"Speak & Rescue\"), from Sun Electronics. The first personal computer game with speech synthesis was \"Manbiki Shoujo\" (\"Shoplifting Girl\"), released in 1980 for the PET 2001, for which the game's developer, Hiroshi Suzuki, developed a \"\"zero cross\"\" programming technique to produce a synthesized speech waveform. Another early example, the arcade version of \"Berzerk\", also dates from 1980. The Milton Bradley Company produced the first multi-player electronic game using voice synthesis, \"Milton\", in the same year.\n\nThe most important qualities of a speech synthesis system are \"naturalness\" and \"intelligibility\". Naturalness describes how closely the output sounds like human speech, while intelligibility is the ease with which the output is understood. The ideal speech synthesizer is both natural and intelligible. Speech synthesis systems usually try to maximize both characteristics.\n\nThe two primary technologies generating synthetic speech waveforms are \"concatenative synthesis\" and \"formant synthesis\". Each technology has strengths and weaknesses, and the intended uses of a synthesis system will typically determine which approach is used.\n\nConcatenative synthesis is based on the concatenation (or stringing together) of segments of recorded speech. Generally, concatenative synthesis produces the most natural-sounding synthesized speech. However, differences between natural variations in speech and the nature of the automated techniques for segmenting the waveforms sometimes result in audible glitches in the output. There are three main sub-types of concatenative synthesis.\n\nUnit selection synthesis uses large databases of recorded speech. During database creation, each recorded utterance is segmented into some or all of the following: individual phones, diphones, half-phones, syllables, morphemes, words, phrases, and sentences. Typically, the division into segments is done using a specially modified speech recognizer set to a \"forced alignment\" mode with some manual correction afterward, using visual representations such as the waveform and spectrogram. An index of the units in the speech database is then created based on the segmentation and acoustic parameters like the fundamental frequency (pitch), duration, position in the syllable, and neighboring phones. At run time, the desired target utterance is created by determining the best chain of candidate units from the database (unit selection). This process is typically achieved using a specially weighted decision tree.\n\nUnit selection provides the greatest naturalness, because it applies only a small amount of digital signal processing (DSP) to the recorded speech. DSP often makes recorded speech sound less natural, although some systems use a small amount of signal processing at the point of concatenation to smooth the waveform. The output from the best unit-selection systems is often indistinguishable from real human voices, especially in contexts for which the TTS system has been tuned. However, maximum naturalness typically require unit-selection speech databases to be very large, in some systems ranging into the gigabytes of recorded data, representing dozens of hours of speech. Also, unit selection algorithms have been known to select segments from a place that results in less than ideal synthesis (e.g. minor words become unclear) even when a better choice exists in the database. Recently, researchers have proposed various automated methods to detect unnatural segments in unit-selection speech synthesis systems.\n\nDiphone synthesis uses a minimal speech database containing all the diphones (sound-to-sound transitions) occurring in a language. The number of diphones depends on the phonotactics of the language: for example, Spanish has about 800 diphones, and German about 2500. In diphone synthesis, only one example of each diphone is contained in the speech database. At runtime, the target prosody of a sentence is superimposed on these minimal units by means of digital signal processing techniques such as linear predictive coding, PSOLA or MBROLA. or more recent techniques such as pitch modification in the source domain using discrete cosine transform Diphone synthesis suffers from the sonic glitches of concatenative synthesis and the robotic-sounding nature of formant synthesis, and has few of the advantages of either approach other than small size. As such, its use in commercial applications is declining, although it continues to be used in research because there are a number of freely available software implementations.\n\nDomain-specific synthesis concatenates prerecorded words and phrases to create complete utterances. It is used in applications where the variety of texts the system will output is limited to a particular domain, like transit schedule announcements or weather reports. The technology is very simple to implement, and has been in commercial use for a long time, in devices like talking clocks and calculators. The level of naturalness of these systems can be very high because the variety of sentence types is limited, and they closely match the prosody and intonation of the original recordings.\n\nBecause these systems are limited by the words and phrases in their databases, they are not general-purpose and can only synthesize the combinations of words and phrases with which they have been preprogrammed. The blending of words within naturally spoken language however can still cause problems unless the many variations are taken into account. For example, in non-rhotic dialects of English the \"\"r\"\" in words like \"\"clear\"\" is usually only pronounced when the following word has a vowel as its first letter (e.g. \"\"clear out\"\" is realized as ). Likewise in French, many final consonants become no longer silent if followed by a word that begins with a vowel, an effect called liaison. This alternation cannot be reproduced by a simple word-concatenation system, which would require additional complexity to be context-sensitive.\n\nFormant synthesis does not use human speech samples at runtime. Instead, the synthesized speech output is created using additive synthesis and an acoustic model (physical modelling synthesis). Parameters such as fundamental frequency, voicing, and noise levels are varied over time to create a waveform of artificial speech. This method is sometimes called \"rules-based synthesis\"; however, many concatenative systems also have rules-based components.\nMany systems based on formant synthesis technology generate artificial, robotic-sounding speech that would never be mistaken for human speech. However, maximum naturalness is not always the goal of a speech synthesis system, and formant synthesis systems have advantages over concatenative systems. Formant-synthesized speech can be reliably intelligible, even at very high speeds, avoiding the acoustic glitches that commonly plague concatenative systems. High-speed synthesized speech is used by the visually impaired to quickly navigate computers using a screen reader. Formant synthesizers are usually smaller programs than concatenative systems because they do not have a database of speech samples. They can therefore be used in embedded systems, where memory and microprocessor power are especially limited. Because formant-based systems have complete control of all aspects of the output speech, a wide variety of prosodies and intonations can be output, conveying not just questions and statements, but a variety of emotions and tones of voice.\n\nExamples of non-real-time but highly accurate intonation control in formant synthesis include the work done in the late 1970s for the Texas Instruments toy Speak & Spell, and in the early 1980s Sega arcade machines and in many Atari, Inc. arcade games using the TMS5220 LPC Chips. Creating proper intonation for these projects was painstaking, and the results have yet to be matched by real-time text-to-speech interfaces.\n\nFormant synthesis was implemented in hardware in the Yamaha FS1R synthesizer, but the speech aspect of formants was never realized in the synth. It was capable of short, several-second formant sequences which could speak a single phrase, but since the MIDI control interface was so restrictive live speech was an impossibility.\n\nArticulatory synthesis refers to computational techniques for synthesizing speech based on models of the human vocal tract and the articulation processes occurring there. The first articulatory synthesizer regularly used for laboratory experiments was developed at Haskins Laboratories in the mid-1970s by Philip Rubin, Tom Baer, and Paul Mermelstein. This synthesizer, known as ASY, was based on vocal tract models developed at Bell Laboratories in the 1960s and 1970s by Paul Mermelstein, Cecil Coker, and colleagues.\n\nUntil recently, articulatory synthesis models have not been incorporated into commercial speech synthesis systems. A notable exception is the NeXT-based system originally developed and marketed by Trillium Sound Research, a spin-off company of the University of Calgary, where much of the original research was conducted. Following the demise of the various incarnations of NeXT (started by Steve Jobs in the late 1980s and merged with Apple Computer in 1997), the Trillium software was published under the GNU General Public License, with work continuing as gnuspeech. The system, first marketed in 1994, provides full articulatory-based text-to-speech conversion using a waveguide or transmission-line analog of the human oral and nasal tracts controlled by Carré's \"distinctive region model\".\n\nMore recent synthesizers, developed by Jorge C. Lucero and colleagues, incorporate models of vocal fold biomechanics, glottal aerodynamics and acoustic wave propagation in the bronqui, traquea, nasal and oral cavities, and thus constitute full systems of physics-based speech simulation.\n\nHMM-based synthesis is a synthesis method based on hidden Markov models, also called Statistical Parametric Synthesis. In this system, the frequency spectrum (vocal tract), fundamental frequency (voice source), and duration (prosody) of speech are modeled simultaneously by HMMs. Speech waveforms are generated from HMMs themselves based on the maximum likelihood criterion.\n\nSinewave synthesis is a technique for synthesizing speech by replacing the formants (main bands of energy) with pure tone whistles.\n\nThe process of normalizing text is rarely straightforward. Texts are full of heteronyms, numbers, and abbreviations that all require expansion into a phonetic representation. There are many spellings in English which are pronounced differently based on context. For example, \"My latest project is to learn how to better project my voice\" contains two pronunciations of \"project\".\n\nMost text-to-speech (TTS) systems do not generate semantic representations of their input texts, as processes for doing so are unreliable, poorly understood, and computationally ineffective. As a result, various heuristic techniques are used to guess the proper way to disambiguate homographs, like examining neighboring words and using statistics about frequency of occurrence.\n\nRecently TTS systems have begun to use HMMs (discussed above) to generate \"parts of speech\" to aid in disambiguating homographs. This technique is quite successful for many cases such as whether \"read\" should be pronounced as \"red\" implying past tense, or as \"reed\" implying present tense. Typical error rates when using HMMs in this fashion are usually below five percent. These techniques also work well for most European languages, although access to required training corpora is frequently difficult in these languages.\n\nDeciding how to convert numbers is another problem that TTS systems have to address. It is a simple programming challenge to convert a number into words (at least in English), like \"1325\" becoming \"one thousand three hundred twenty-five.\" However, numbers occur in many different contexts; \"1325\" may also be read as \"one three two five\", \"thirteen twenty-five\" or \"thirteen hundred and twenty five\". A TTS system can often infer how to expand a number based on surrounding words, numbers, and punctuation, and sometimes the system provides a way to specify the context if it is ambiguous. Roman numerals can also be read differently depending on context. For example, \"Henry VIII\" reads as \"Henry the Eighth\", while \"Chapter VIII\" reads as \"Chapter Eight\".\n\nSimilarly, abbreviations can be ambiguous. For example, the abbreviation \"in\" for \"inches\" must be differentiated from the word \"in\", and the address \"12 St John St.\" uses the same abbreviation for both \"Saint\" and \"Street\". TTS systems with intelligent front ends can make educated guesses about ambiguous abbreviations, while others provide the same result in all cases, resulting in nonsensical (and sometimes comical) outputs, such as \"co-operation\" being rendered as \"company operation\".\n\nSpeech synthesis systems use two basic approaches to determine the pronunciation of a word based on its spelling, a process which is often called text-to-phoneme or grapheme-to-phoneme conversion (phoneme is the term used by linguists to describe distinctive sounds in a language). The simplest approach to text-to-phoneme conversion is the dictionary-based approach, where a large dictionary containing all the words of a language and their correct pronunciations is stored by the program. Determining the correct pronunciation of each word is a matter of looking up each word in the dictionary and replacing the spelling with the pronunciation specified in the dictionary. The other approach is rule-based, in which pronunciation rules are applied to words to determine their pronunciations based on their spellings. This is similar to the \"sounding out\", or synthetic phonics, approach to learning reading.\n\nEach approach has advantages and drawbacks. The dictionary-based approach is quick and accurate, but completely fails if it is given a word which is not in its dictionary. As dictionary size grows, so too does the memory space requirements of the synthesis system. On the other hand, the rule-based approach works on any input, but the complexity of the rules grows substantially as the system takes into account irregular spellings or pronunciations. (Consider that the word \"of\" is very common in English, yet is the only word in which the letter \"f\" is pronounced .) As a result, nearly all speech synthesis systems use a combination of these approaches.\n\nLanguages with a phonemic orthography have a very regular writing system, and the prediction of the pronunciation of words based on their spellings is quite successful. Speech synthesis systems for such languages often use the rule-based method extensively, resorting to dictionaries only for those few words, like foreign names and borrowings, whose pronunciations are not obvious from their spellings. On the other hand, speech synthesis systems for languages like English, which have extremely irregular spelling systems, are more likely to rely on dictionaries, and to use rule-based methods only for unusual words, or words that aren't in their dictionaries.\n\nThe consistent evaluation of speech synthesis systems may be difficult because of a lack of universally agreed objective evaluation criteria. Different organizations often use different speech data. The quality of speech synthesis systems also depends on the quality of the production technique (which may involve analogue or digital recording) and on the facilities used to replay the speech. Evaluating speech synthesis systems has therefore often been compromised by differences between production techniques and replay facilities.\n\nSince 2005, however, some researchers have started to evaluate speech synthesis systems using a common speech dataset.\n\nA study in the journal \"Speech Communication\" by Amy Drahota and colleagues at the University of Portsmouth, UK, reported that listeners to voice recordings could determine, at better than chance levels, whether or not the speaker was smiling. It was suggested that identification of the vocal features that signal emotional content may be used to help make synthesized speech sound more natural. One of the related issues is modification of the pitch contour of the sentence, depending upon whether it is an affirmative, interrogative or exclamatory sentence. One of the techniques for pitch modification uses discrete cosine transform in the source domain (linear prediction residual). Such pitch synchronous pitch modification techniques need a priori pitch marking of the synthesis speech database using techniques such as epoch extraction using dynamic plosion index applied on the integrated linear prediction residual of the voiced regions of speech.\n\nEarly Technology (not available anymore)\n\nCurrent (as of 2013)\n\n\nPopular systems offering speech synthesis as a built-in capability.\n\nThe Mattel Intellivision game console offered the Intellivoice Voice Synthesis module in 1982. It included the SP0256 Narrator speech synthesizer chip on a removable cartridge. The Narrator had 2kB of Read-Only Memory (ROM), and this was utilized to store a database of generic words that could be combined to make phrases in Intellivision games. Since the Orator chip could also accept speech data from external memory, any additional words or phrases needed could be stored inside the cartridge itself. The data consisted of strings of analog-filter coefficients to modify the behavior of the chip's synthetic vocal-tract model, rather than simple digitized samples.\n\nAlso released in 1982, Software Automatic Mouth was the first commercial all-software voice synthesis program. It was later used as the basis for Macintalk. The program was available for non-Macintosh Apple computers (including the Apple II, and the Lisa), various Atari models and the Commodore 64. The Apple version preferred additional hardware that contained DACs, although it could instead use the computer's one-bit audio output (with the addition of much distortion) if the card was not present. The Atari made use of the embedded POKEY audio chip. Speech playback on the Atari normally disabled interrupt requests and shut down the ANTIC chip during vocal output. The audible output is extremely distorted speech when the screen is on. The Commodore 64 made use of the 64's embedded SID audio chip.\n\nArguably, the first speech system integrated into an operating system was the 1400XL/1450XL personal computers designed by Atari, Inc. using the Votrax SC01 chip in 1983. The 1400XL/1450XL computers used a Finite State Machine to enable World English Spelling text-to-speech synthesis. Unfortunately, the 1400XL/1450XL personal computers never shipped in quantity.\n\nThe Atari ST computers were sold with \"stspeech.tos\" on floppy disk.\n\nThe first speech system integrated into an operating system that shipped in quantity was Apple Computer's MacInTalk. The software was licensed from 3rd party developers Joseph Katz and Mark Barton (later, SoftVoice, Inc.) and was featured during the 1984 introduction of the Macintosh computer. This January demo required 512 kilobytes of RAM memory. As a result, it could not run in the 128 kilobytes of RAM the first Mac actually shipped with. So, the demo was accomplished with a prototype 512k Mac, although those in attendance were not told of this and the synthesis demo created considerable excitement for the Macintosh. In the early 1990s Apple expanded its capabilities offering system wide text-to-speech support. With the introduction of faster PowerPC-based computers they included higher quality voice sampling. Apple also introduced speech recognition into its systems which provided a fluid command set. More recently, Apple has added sample-based voices. Starting as a curiosity, the speech system of Apple Macintosh has evolved into a fully supported program, PlainTalk, for people with vision problems. VoiceOver was for the first time featured in Mac OS X Tiger (10.4). During 10.4 (Tiger) & first releases of 10.5 (Leopard) there was only one standard voice shipping with Mac OS X. Starting with 10.6 (Snow Leopard), the user can choose out of a wide range list of multiple voices. VoiceOver voices feature the taking of realistic-sounding breaths between sentences, as well as improved clarity at high read rates over PlainTalk. Mac OS X also includes say, a command-line based application that converts text to audible speech. The AppleScript Standard Additions includes a say verb that allows a script to use any of the installed voices and to control the pitch, speaking rate and modulation of the spoken text.\n\nThe Apple iOS operating system used on the iPhone, iPad and iPod Touch uses VoiceOver speech synthesis for accessibility. Some third party applications also provide speech synthesis to facilitate navigating, reading web pages or translating text.\n\nThe second operating system to feature advanced speech synthesis capabilities was AmigaOS, introduced in 1985. The voice synthesis was licensed by Commodore International from SoftVoice, Inc., who also developed the original MacinTalk text-to-speech system. It featured a complete system of voice emulation for American English, with both male and female voices and \"stress\" indicator markers, made possible through the Amiga's audio chipset. The synthesis system was divided into a translator library which converted unrestricted English text into a standard set of phonetic codes and a narrator device which implemented a formant model of speech generation.. AmigaOS also featured a high-level \"Speak Handler\", which allowed command-line users to redirect text output to speech. Speech synthesis was occasionally used in third-party programs, particularly word processors and educational software. The synthesis software remained largely unchanged from the first AmigaOS release and Commodore eventually removed speech synthesis support from AmigaOS 2.1 onward.\n\nDespite the American English phoneme limitation, an unofficial version with multilingual speech synthesis was developed. This made use of an enhanced version of the translator library which could translate a number of languages, given a set of rules for each language.\n\nModern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and speech recognition. SAPI 4.0 was available as an optional add-on for Windows 95 and Windows 98. Windows 2000 added Narrator, a text–to–speech utility for people who have visual impairment. Third-party programs such as JAWS for Windows, Window-Eyes, Non-visual Desktop Access, Supernova and System Access can perform various text-to-speech tasks such as reading text aloud from a specified website, email account, text document, the Windows clipboard, the user's keyboard typing, etc. Not all programs can use speech synthesis directly. Some programs can use plug-ins, extensions or add-ons to read text aloud. Third-party programs are available that can read text from the system clipboard.\n\nMicrosoft Speech Server is a server-based package for voice synthesis and recognition. It is designed for network use with web applications and call centers.\n\nIn the early 1980s, TI was known as a pioneer in speech synthesis, and a highly popular plug-in speech synthesizer module was available for the TI-99/4 and 4A. Speech synthesizers were offered free with the purchase of a number of cartridges and were used by many TI-written video games (notable titles offered with speech during this promotion were Alpiner and Parsec). The synthesizer uses a variant of linear predictive coding and has a small in-built vocabulary. The original intent was to release small cartridges that plugged directly into the synthesizer unit, which would increase the device's built in vocabulary. However, the success of software text-to-speech in the Terminal Emulator II cartridge cancelled that plan.\n\nText-to-Speech (TTS) refers to the ability of computers to read text aloud. A TTS Engine converts written text to a phonemic representation, then converts the phonemic representation to waveforms that can be output as sound. TTS engines with different languages, dialects and specialized vocabularies are available through third-party publishers.\n\nVersion 1.6 of Android added support for speech synthesis (TTS).\n\nCurrently, there are a number of applications, plugins and gadgets that can read messages directly from an e-mail client and web pages from a web browser or Google Toolbar, such as Text to Voice, which is an add-on to Firefox. Some specialized software can narrate RSS-feeds. On one hand, online RSS-narrators simplify information delivery by allowing users to listen to their favourite news sources and to convert them to podcasts. On the other hand, on-line RSS-readers are available on almost any PC connected to the Internet. Users can download generated audio files to portable devices, e.g. with a help of podcast receiver, and listen to them while walking, jogging or commuting to work.\n\nA growing field in Internet based TTS is web-based assistive technology, e.g. 'Browsealoud' from a UK company and Readspeaker. It can deliver TTS functionality to anyone (for reasons of accessibility, convenience, entertainment or information) with access to a web browser. The non-profit project was created in 2006 to provide a similar web-based TTS interface to the Wikipedia.\n\nOther work is being done in the context of the W3C through the W3C Audio Incubator Group with the involvement of The BBC and Google Inc.\n\nSystems that operate on free and open source software systems including Linux are various, and include open-source programs such as the Festival Speech Synthesis System which uses diphone-based synthesis, as well as more modern and better-sounding techniques, eSpeak, which supports a broad range of languages, and gnuspeech which uses articulatory synthesis from the Free Software Foundation.\n\n\nWith the 2016 introduction of Adobe Voco audio editing and generating software prototype slated to be part of the Adobe Creative Suite and the similarly enabled DeepMind WaveNet, a deep neural network based audio synthesis software from Google \nspeech synthesis is verging on being completely indistinguishable from a real human's voice.\n\nAdobe Voco takes approximately 20 minutes of the desired target's speech and after that it can generate sound-alike voice with even phonemes that were not present in the training material. The software obviously poses ethical concerns as it allows to steal other peoples voices and manipulate them to say anything desired.\n\nThis increases the stress on the disinformation situation coupled with the facts that \n\nA number of markup languages have been established for the rendition of text as speech in an XML-compliant format. The most recent is Speech Synthesis Markup Language (SSML), which became a W3C recommendation in 2004. Older speech synthesis markup languages include Java Speech Markup Language (JSML) and SABLE. Although each of these was proposed as a standard, none of them have been widely adopted.\n\nSpeech synthesis markup languages are distinguished from dialogue markup languages. VoiceXML, for example, includes tags related to speech recognition, dialogue management and touchtone dialing, in addition to text-to-speech markup.\n\nSpeech synthesis has long been a vital assistive technology tool and its application in this area is significant and widespread. It allows environmental barriers to be removed for people with a wide range of disabilities. The longest application has been in the use of screen readers for people with visual impairment, but text-to-speech systems are now commonly used by people with dyslexia and other reading difficulties as well as by pre-literate children. They are also frequently employed to aid those with severe speech impairment usually through a dedicated voice output communication aid.\n\nSpeech synthesis techniques are also used in entertainment productions such as games and animations. In 2007, Animo Limited announced the development of a software application package based on its speech synthesis software FineSpeech, explicitly geared towards customers in the entertainment industries, able to generate narration and lines of dialogue according to user specifications. The application reached maturity in 2008, when NEC Biglobe announced a web service that allows users to create phrases from the voices of characters.\n\nIn recent years, Text to Speech for disability and handicapped communication aids have become widely deployed in Mass Transit. Text to Speech is also finding new applications outside the disability market. For example, speech synthesis, combined with speech recognition, allows for interaction with mobile devices via natural language processing interfaces.\n\nText-to speech is also used in second language acquisition. Voki, for instance, is an educational tool created by Oddcast that allows users to create their own talking avatar, using different accents. They can be emailed, embedded on websites or shared on social media.\n\nIn addition, speech synthesis is a valuable computational aid for the analysis and assessment of speech disorders. A voice quality synthesizer, developed by Jorge C. Lucero et al. at University of Brasilia, simulates the physics of phonation and includes models of vocal frequency jitter and tremor, airflow noise and laryngeal asymmetries. The synthesizer has been used to mimic the timbre of dysphonic speakers with controlled levels of roughness, breathiness and strain.\n\nMultiple companies offer TTS APIs to their customers to accelerate development of new applications utilizing TTS technology. Companies offering TTS APIs include AT&T, CereProc, DIOTEK, IVONA, Neospeech, Readspeaker, SYNVO, YAKiToMe! and CPqD. For mobile app development, Android operating system has been offering text to speech API for a long time. Most recently, with iOS7, Apple started offering an API for text to speech.\n", "id": "42799", "title": "Speech synthesis"}
{"url": "https://en.wikipedia.org/wiki?curid=33233256", "text": "Xaitment\n\nxaitment is a German-based company that develops and sells artificial intelligence (AI) software to video game developers and simulation developers. The company was founded in 2004 by Dr. Andreas Gerber, and is a spin-off of the German Research Centre for Artificial Intelligence, or DFKI. xaitment has its main office in Quierschied, Germany, and field offices in San Francisco and China.\n\nxaitment currently sells two AI software modules: xaitMap and xaitControl. xaitMap provides runtime libraries and graphical tools for navigation mesh generation (also called NavMesh generation), pathfinding, dynamic collision avoidance, and individual and crowd movement. xaitControl is a finite-state machine for game logic and character behavior modeling that also includes a real-time debugger. On January 11, 2012, xaitment announced that it making its source code for these modules available to \"all current and future US and European licensees\".\n\nOn February 22, 2012 xaitment released two new plug-ins, xaitMap and xaitControl for the Unity Game Engine. \nThe full versions are available for PC (Windows and Linux), PlayStation 3, Xbox 360 and Wii. The pathfinding plug-in is available with a Windows dev environment, but can deployed on iOS, Mac, Android and the Unity Web Player.\n\nxaitment's AI software is currently integrated into the Unity game engine, Havok's Vision Engine, Bohemia Interactive's VBS2 Simulation Engine, GameBase's Gamebryo game engine.\n\nxaitment sells its AI software products to video game developers and military and civil simulation developers. Current customers include Tencent, gamania, TML Studios, Emobi Games, IP Keys and others. A full list of customers can be found on xaitment's website.\n\n", "id": "33233256", "title": "Xaitment"}
{"url": "https://en.wikipedia.org/wiki?curid=13111724", "text": "Speech-generating device\n\nSpeech-generating devices (SGDs), also known as voice output communication aids, are electronic augmentative and alternative communication (AAC) systems used to supplement or replace speech or writing for individuals with severe speech impairments, enabling them to verbally communicate. SGDs are important for people who have limited means of interacting verbally, as they allow individuals to become active participants in communication interactions. They are particularly helpful for patients suffering from amyotrophic lateral sclerosis (ALS) but recently have been used for children with predicted speech deficiencies.\n\nThere are several input and display methods for users of varying abilities to make use of SGDs. Some SGDs have multiple pages of symbols to accommodate a large number of utterances, and thus only a portion of the symbols available are visible at any one time, with the communicator navigating the various pages. Speech-generating devices can produce electronic voice output by using digitized recordings of natural speech or through speech synthesis—which may carry less emotional information but can permit the user to speak novel messages.\n\nThe content, organization, and updating of the vocabulary on an SGD is influenced by a number of factors, such at the user's needs and the contexts that the device will be used in. The development of techniques to improve the available vocabulary and rate of speech production is an active research area. Vocabulary items should be of high interest to the user, be frequently applicable, have a range of meanings, and be pragmatic in functionality.\n\nThere are multiple methods of accessing messages on devices: directly or indirectly, or using specialized access devices—although the specific access method will depend on the skills and abilities of the user. SGD output is typically much slower than speech, although rate enhancement strategies can increase the user's rate of output, resulting in enhanced efficiency of communication.\n\nThe first known SGD was prototyped in the mid-1970s, and rapid progress in hardware and software development has meant that SGD capabilities can now be integrated into devices like smartphones. Notable users of SGDs include Stephen Hawking, Roger Ebert, Tony Proudfoot, and Pete Frates (founder of the ALS ice bucket challenge).\n\nSpeech-generating systems may be dedicated devices developed solely for AAC, or non-dedicated devices such as computers running additional software to allow them to function as AAC devices.\n\nSGDs have their roots in early electronic communication aids. The first such aid was a sip-and-puff typewriter controller named the patient-operated selector mechanism (POSSUM) prototyped by Reg Maling in the United Kingdom in 1960. POSSUM scanned through a set of symbols on an illuminated display. Researchers at Delft University in the Netherlands created the lightspot operated typewriter (LOT) in 1970, which made use of small movements of the head to point a small spot of light at a matrix of characters, each equipped with a photoelectric cell. Although it was commercially unsuccessful, the LOT was well received by its users.\n\nIn 1966, Barry Romich, a freshman engineering student at Case Western Reserve University, and Ed Prentke, an engineer at Highland View Hospital in Cleveland, Ohio formed a partnership, creating the Prentke Romich Company. In 1969, the company produced its first communication device, a typing system based on a discarded Teletype machine.\n\nDuring the 1970s and early 1980s, several other companies began to emerge that have since become prominent manufacturers of SGDs. Toby Churchill founded Toby Churchill Ltd in 1973, after losing his speech following encephalitis. In the US, Dynavox (then known as Sentient Systems Technology) grew out of a student project at Carnegie-Mellon University, created in 1982 to help a young woman with cerebral palsy to communicate.\nBeginning in the 1980s, improvements in technology led to a greatly increased number, variety, and performance of commercially available communication devices, and a reduction in their size and price. Alternative methods of access such as Target Scanning (also known as eye pointing) calibrate the movement of a user's eyes to direct an SGD to produce the desired speech phase. Scanning, in which alternatives are presented to the user sequentially, became available on communication devices. Speech output possibilities included both digitized and synthesized speech.\n\nRapid progress in hardware and software development continued, including projects funded by the European Community. The first commercially available dynamic screen speech generating devices were developed in the 1990s. Software programs were developed that allowed the computer-based production of communication boards. High-tech devices have continued to become smaller and lighter, while increasing accessibility and capability; communication devices can be accessed using eye-tracking systems, perform as a computer for word-processing and Internet use, and as an environmental control device for independent access to other equipment such as TV, radio and telephones.\n\nNotable individuals who have used AAC devices include Stephen Hawking, Roger Ebert Tony Proudfoot, and Pete Frates. Hawking is unable to speak due to a combination of severe disabilities caused by ALS, and an emergency tracheotomy. He has come to be associated with the unique voice of his particular synthesis equipment. In the past 20 or so years SGD have gained popularity amongst young children with speech deficiencies, such as autism, Down syndrome, and predicted brain damage due to surgery.\n\nStarting in the early 2000s, specialist saw the benefit of using SGDs not only for adults but for children, as well. Neuro-linguists found that SGDs were just as effective in helping children who were at risk for temporary language deficits after undergoing brain surgery as it is for patients with ALS. In particular, digitized SGDs have been used as communication aids for pediatric patients during the recovery process.\n\nThere are many methods of accessing messages on devices: directly, indirectly, and with specialized access devices. Direct access methods involve physical contact with the system, by using a keyboard or a touch screen. Users accessing SGDs indirectly and through specialized devices must manipulate an object in order to access the system, such as maneuvering a joystick, head mouse, optical head pointer, light pointer, infrared pointer, or switch access scanner.\n\nThe specific access method will depend on the skills and abilities of the user. With direct selection a body part, pointer, adapted mouse, joystick, or eye tracking could be used, whereas switch access scanning is often used for indirect selection. Unlike direct selection (e.g., typing on a keyboard, touching a screen), users of Target Scanning can only make selections when the scanning indicator (or cursor) of the electronic device is on the desired choice. Those who are unable to point typically calibrate their eyes to use eye gaze as a way to point and blocking as a way to select desired words and phrases. The speed and pattern of scanning, as well as the way items are selected, are individualized to the physical, visual and cognitive capabilities of the user.\n\nAugmentative and alternative communication is typically much slower than speech, with users generally producing 8–10 words per minute. Rate enhancement strategies can increase the user's rate of output to around 12–15 words per minute, and as a result enhance the efficiency of communication.\n\nIn any given SGD there may be a large number of vocal expressions that facilitate efficient and effective communication, including greetings, expressing desires, and asking questions. Some SGDs have multiple pages of symbols to accommodate a large number of vocal expressions, and thus only a portion of the symbols available are visible at any one time, with the communicator navigating the various pages. Speech-generating devices generally display a set of selections either using a dynamically changing screen, or a fixed display.\n\nThere are two main options for increasing the rate of communication for an SGD: encoding and prediction.\n\nEncoding permits a user to produce a word, sentence or phrase using only one or two activations of their SGD. Iconic encoding strategies such as Semantic compaction combine sequences of icons (picture symbols) to produce words or phrases. In numeric, alpha-numeric, and letter encoding (also known as Abbreviation-Expansion), words and sentences are coded as sequences of letters and numbers. For example, typing \"HH\" or \"G1\" (for Greeting 1) may retrieve \"Hello, how are you?\".\n\nPrediction is a rate enhancement strategy in which the SGD attempts to reduce the number of keystrokes used by predicting the word or phrase being written by the user. The user can then select the correct prediction without needing to write the entire word. Word prediction software may determine the choices to be offered based on their frequency in language, association with other words, past choices of the user, or grammatical suitability. However, users have been shown to produce more words per minute (using a scanning interface) with a static keyboard layout than with a predictive grid layout, suggesting that the cognitive overhead of reviewing a new arrangement cancels out the benefits of the predictive layout when using a scanning interface.\n\nSome systems, such as Auditory Sciences' Interact-Voice device, combine Encoding and Prediction into the same system. For example, typing \"HMF\" can be an encoded shortcut for \"Can you help me find ____\" and then the prediction capabilities help the user complete the sentence, such as \"Can you help me find my glasses?\" or \"Can you help me find my car keys?\".\n\nAnother approach to rate-enhancement is Dasher, which uses language models and arithmetic coding to present alternative letter targets on the screen with size relative to their likelihood given the history.\n\nThe rate of words produced can depend greatly on the conceptual level of the system: the TALK system, which allows users to choose between large numbers of sentence-level utterances, demonstrated output rates in excess of 60 wpm.\n\nFixed display devices refer to those in which the symbols and items are \"fixed\" in a particular format; some sources refer to these as \"static\" displays. Such display devices have a simpler learning curve than some other devices.\n\nFixed display devices replicate the typical arrangement of low-tech AAC devices (low-tech is defined as those devices that do not need batteries, electricity or electronics), like communication boards. They share some of disadvantages; for example they are typically restricted to a limited number of symbols and hence messages. It is important to note that with technological advances made in the twenty-first century, fixed-display SGDs are not commonly used anymore.\n\nDynamic displays devices are usually also touchscreen devices. iPads, Tobii Technology and Words+. devices are the most commonly used Dynamic Display Devices. They typically generate electronically produced visual symbols that, when pressed, change the set of selections that is displayed. The user can change the symbols available using page links to navigate to appropriate pages of vocabulary and messages. \n\nThe \"home\" page of a dynamic display device may show symbols related to many different contexts or conversational topics. Pressing any one of these symbols may open a different screen with messages related to that topic. For example, when watching a volleyball game, a user may press the \"sport\" symbol to open a page with messages relating to sport, then press the symbol showing a scoreboard to utter the phrase \"What's the score?\".\n\nAdvantages of dynamic display devices include the availability of a much larger vocabulary, and the ability to see the sentence under construction A further advantage of dynamic display devices is that the underlying operating system is capable of providing options for multiple communication channels, including cell phone, text messaging and e-mail. Work by Linköping University has shown that such email writing practices allowed children who were SGD users to develop new social skills and increase their social participation.\n\nLow cost systems can also include a keyboard and audio speaker combination without a dynamic display or visual screen. This type of keyboard sends typed text direct to an audio speaker. It can permit any phrase to be spoken without the need for a visual screen that is not always required. One simple benefit is that a talking keyboard, when used with a standard telephone or speakerphone can enable a voice impaired individual have 2 way conversation over a telephone.\n\nThe output of a SGD may be digitized and/or synthesized: digitized systems play directly recorded words or phrases while synthesized speech uses text-to-speech software that can carry less emotional information but permits the user to speak novel messages by typing new words. Today, individuals use a combination of recorded messages and text-to-speech techniques on their SGDs. However, some devices are limited to only one type of output.\n\nWords, phrases or entire messages can be digitised and stored onto the device for playback to be activated by the user. This process is formally known as Voice Banking. Advantages of recorded speech include that it (a) provides natural prosody and speech naturalness for the listener (e.g., person of the same age and gender as the AAC user can be selected to record the messages), and (b) it provides for additional sounds that may be important for the user such as laughing or whistling. Moreover, Digitized SGDs is that they provide a degree of normalcy both for the patient and for their families when they lose their ability to speak on their own.\n\nA major disadvantage of using only recorded speech is that users are unable to produce novel messages; they are limited to the messages pre-recorded into the device. Depending on the device, there may be a limit to the length of the recordings.\n\nSGDs that use synthesized speech apply the phonetic rules of the language to translate the user’s message into voice output (speech synthesis). Users have the freedom to create novel words and messages and are not limited to those that have been pre-recorded on their device by others.\n\nSmartphones and computers have increased the use of synthesized speech devices through the creation of apps that allow the user to select from a list of phrases or messages to be spoken in the voice and language that the user has chosen. Apps such as SpeakIt! or Assistive Express for iPhone provide a cheap way to use a speech-generating device without having to visit a doctor's office or learn to use specialized machinery.\n\nSynthesized SGDs may allow multiple methods of message creation that can be used individually or in combination: messages can be created from letters, words, phrases, sentences, pictures, or symbols. With synthesized speech there is virtually unlimited storage capacity for messages with few demands on memory space.\n\nSynthesized speech engines are available in many languages, and the engine's parameters, such as speech rate, pitch range, gender, stress patterns, pauses, and pronunciation exceptions can be manipulated by the user.\n\nThe selection set of a SGD is the set of all messages, symbols and codes that are available to a person using that device. The content, organisation, and updating of this selection set are areas of active research and are influenced by a number of factors, including the user's ability, interests and age. The selection set for an AAC system may include words that the user does not know yet – they are included for the user to \"grow into\". The content installed on any given SGD may include a large number of preset pages provided by the manufacturer, with a number of additional pages produced by the user or the user's care team depending on the user's needs and the contexts that the device will be used in.\n\nResearchers Beukelman and Mirenda list a number of possible sources (such as family members, friends, teachers, and care staff) for the selection of initial content for a SGD. A range of sources is required because, in general, one individual would not have the knowledge and experience to generate all the vocal expressions needed in any given environment. For example, parents and therapists might not think to add slang terms, such as \"innit\".\n\nPrevious work has analyzed both vocabulary use of typically developing speakers and word use of AAC users to generate content for new AAC devices. Such processes work well for generating a core set of utterances or vocal expressions but are less effective in situations where a particular vocabulary is needed (for example, terms related directly to a user's interest in horse riding). The term \"fringe vocabulary\" refers to vocabulary that is specific or unique to the individual's personal interests or needs. A typical technique to develop fringe vocabulary for a device is to conduct interviews with multiple \"informants\": siblings, parents, teachers, co-workers and other involved persons.\n\nOther researchers, such as Musselwhite and St. Louis suggest that initial vocabulary items should be of high interest to the user, be frequently applicable, have a range of meanings and be pragmatic in functionality. These criteria have been widely used in the AAC field as an ecological check of SGD content.\n\nBeukelman and Mirenda emphasize that vocabulary selection also involves ongoing vocabulary maintenance; however, a difficulty in AAC is that users or their carers must program in any new utterances manually (e.g. names of new friends or personal stories) and there are no existing commercial solutions for automatically adding content. A number of research approaches have attempted to overcome this difficulty, these range from \"inferred input\", such as generating content based on a log of conversation with a user's friends and family, to data mined from the Internet to find language materials, such as the Webcrawler Project. Moreover, by making use of Lifelogging based approaches, a device's content can be changed based on events that occur to a user during their day. By accessing more of a user's data, more high-quality messages can be generated at a risk of exposing sensitive user data. For example, by making use of global positioning systems, a device's content can be changed based on geographical location.\n\nMany recently developed SGDs include performance measurement and analysis tools to help monitor the content used by an individual. This raises concerns about privacy, and some argue that the device user should be involved in the decision to monitor use in this way. Similar concerns have been raised regarding the proposals for devices with automatic content generation, and privacy is increasingly a factor in design of SGDs. As AAC devices are designed to be used in all areas of a user’s life, there are sensitive legal, social, and technical issues centred on a wide family of personal data management problems that can be found in contexts of AAC use. For example, SGDs may have to be designed so that they support the user's right to delete logs of conversations or content that has been added automatically.\n\nProgramming of Dynamic Speech Generating devices is usually done by augmentative communication specialists. Specialists are required to cater to the needs of the patients because the patients usually choose what kinds of words/ phrases they want. For example, patients use different phrases based on their age, disability, interests, etc. Therefore, content organization is extremely time consuming. Additionally, SGDs are rarely covered by health insurance companies. As a result, resources are very limited with regards to both funding and staffing. Dr. John Costello of Boston Children’s Hospital has been the driving force soliciting donations to keep these program running and well-staffed both within his hospital and in hospitals across the country.\n\n", "id": "13111724", "title": "Speech-generating device"}
{"url": "https://en.wikipedia.org/wiki?curid=34340886", "text": "ETAP-3\n\nETAP-3 is a proprietary linguistic processing system focusing on English and Russian. It was developed in Moscow, Russia at the Institute for Information Transmission Problems (). It is a rule-based system which uses the Meaning-Text Theory as its theoretical foundation. At present, there are several applications of ETAP-3, such as a machine translation tool, a converter of the Universal Networking Language, an interactive learning tool for Russian language learners and a syntactically annotated corpus of Russian language. Demo versions of some of these tools are available online.\n\nThe ETAP-3 machine translation tool can translate text from English into Russian and vice versa. It is a rule-based system which makes it different from the most present-day systems that are predominantly statistical-based. The system makes a syntactical analysis of the input sentence which can be visualized as a syntax tree.\n\nThe machine translation tool uses bilingual dictionaries which contain more than 100,000 lexical entries.\n\nThe UNL converter based on ETAP-3 can transform English and Russian sentences into there representations in UNL (\"Universal Networking Language\") and generate English and Russian sentences from their UNL representations.\n\nA syntactically annotated corpus (treebank) is a part of Russian National Corpus. It contains 40,000 sentences (600,000 words) which are fully syntactically and morphologically annotated. The primary annotation was made by ETAP-3 and then manually verified by competent linguists. This makes the syntactically annotated corpus a reliable tool for linguistic research.\n\nThe ETAP-3 system makes extensive use of lexical functions explored in the Meaning-Text Theory. For this reason, an interactive tool for Russian language learners aiming at the acquisition of lexical functions has been developed. Such learning tools are now being created for German, Spanish and Bulgarian\n", "id": "34340886", "title": "ETAP-3"}
{"url": "https://en.wikipedia.org/wiki?curid=18863997", "text": "Natural-language user interface\n\nNatural-language user interfaces (LUI or NLUI) are a type of computer human interface where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications.\n\nIn interface design, natural-language interfaces are sought after for their speed and ease of use, but most suffer the challenges to understanding wide varieties of ambiguous input.\nNatural-language interfaces are an active area of study in the field of natural-language processing and computational linguistics. An intuitive general natural-language interface is one of the active goals of the Semantic Web.\n\nText interfaces are \"natural\" to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional keyword search engine could be described as a \"shallow\" natural-language user interface.\n\nA natural-language search engine would in theory find targeted answers to user questions (as opposed to keyword search). For example, when confronted with a question of the form 'which U.S. state has the highest income tax?', conventional search engines ignore the question and instead search on the keywords 'state', 'income' and 'tax'. Natural-language search, on the other hand, attempts to use natural-language processing to understand the nature of the question and then to search and return a subset of the web that contains the answer to the question. If it works, results would have a higher relevance than results from a keyword search engine.\n\nPrototype Nl interfaces had already appeared in the late sixties and early seventies.\n\n\nNatural-language interfaces have in the past led users to anthropomorphize the computer, or at least to attribute more intelligence to machines than is warranted. On the part of the user, this has led to unrealistic expectations of the capabilities of the system. Such expectations will make it difficult to learn the restrictions of the system if users attribute too much capability to it, and will ultimately lead to disappointment when the system fails to perform as expected as was the case in the AI winter of the 1970s and 80s.\n\nA 1995 paper titled 'Natural Language Interfaces to Databases – An Introduction', describes some challenges:\n\nOther goals to consider more generally are the speed and efficiency of the interface, in all algorithms these two points are the main point that will determine if some methods are better than others and therefore have greater success in the market. In addition, localisation across multiple language sites requires extra consideration - this is based on differing sentence structure and language syntax variations between most languages.\n\nFinally, regarding the methods used, the main problem to be solved is creating a general algorithm that can recognize the entire spectrum of different voices, while disregarding nationality, gender or age. The significant differences between the extracted features - even from speakers who says the same word or phrase - must be successfully overcome.\n\nThe natural-language interface gives rise to technology used for many different applications.\n\nSome of the main uses are:\n\n\nBelow are named and defined some of the applications that use natural-language recognition, and so have integrated utilities listed above.\n\nUbiquity, an add-on for Mozilla Firefox, is a collection of quick and easy natural-language-derived commands that act as mashups of web services, thus allowing users to get information and relate it to current and other webpages.\n\nWolfram Alpha is an online service that answers factual queries directly by computing the answer from structured data, rather than providing a list of documents or web pages that might contain the answer as a search engine would. It was announced in March 2009 by Stephen Wolfram, and was released to the public on May 15, 2009.\n\nSiri is an intelligent personal assistant application integrated with operating system iOS. The application uses natural language processing to answer questions and make recommendations.\n\nSiri's marketing claims include that it adapts to a user's individual preferences over time and personalizes results, and performs tasks such as making dinner reservations while trying to catch a cab.\n\n\n\n", "id": "18863997", "title": "Natural-language user interface"}
{"url": "https://en.wikipedia.org/wiki?curid=34937796", "text": "Interactions Corporation\n\nInteractions LLC is a privately held technology company that builds and delivers hosted Virtual Assistant applications that enable businesses to deliver automated natural language communications for enterprise customer care.\n\nInteractions LLC was founded in 2004 and is headquartered in Franklin, Massachusetts. Since inception, Interactions has raised $167M and is venture-backed by Sigma Partners, Cross Atlantic Capital Partners, Updata Partners, North Hill Ventures, Revolution Growth, NewSpring Capital, and Comcast Ventures. Michael Iacobucci serves as Interactions' CEO. Interactions has additional offices in Indiana, Michigan, New Jersey and New York.\n\nIn 2010, Interactions received PCI DSS compliance validation.\n\nIn April 2012, Interactions was named a 2012 Gartner 'Cool Vendor' in CRM Customer Service and Social.\n\nIn November 2014, Interactions announced the acquisition of AT&T's Speech and Language Technology group, along with its AT&T Watson(SM) platform. \n\nIn June 2016 the company announced a new service named \"Curo Speech and Language Platform\", based on the acquired AT&T technology, to provide enhanced natural language understanding within its products. Later in 2016 the company updated its Virtual Assistant offering to include a voice biometric add-on, which helps prevent fraud in automated Virtual Assistant interactions.\n\nIn May 2017, Interactions acquired AI-based social media engagement innovator Digital Roots. Interactions Digital Roots uses social AI, natural language processing and machine learning to help global household brands engage with consumers on social media. In Septepmber 2017, Interactions was named to the Forbes Next Billion-Dollar Startups list.\n\nInteractions' hosted Intelligent Virtual Assistants combine artificial intelligence and human understanding to enable businesses and consumers to engage in productive conversations. Operating under a SaaS business model, Interactions' service includes both an application's design and build, as well as continuous operating and tuning. Services created can be compared to services provided by Apple’s Siri, Amazon Alexa, and Google Now.\n\nInteractions' customer base includes multiple consumer-facing Fortune 500 companies, including Hyatt, TXU Energy, The Salt River Project, Humana, Westar Energy, and LifeLock. Interactions customers represent the Telecommunication, Hospitality, Finance, Insurance and Warranty, Retail, Utility, Consumer Software and Electronics, and Healthcare industries.\n", "id": "34937796", "title": "Interactions Corporation"}
{"url": "https://en.wikipedia.org/wiki?curid=35591037", "text": "Marketing and artificial intelligence\n\nArtificial intelligence is a field of study that “seeks to explain and emulate intelligent behaviour in terms of computational processes” through performing the tasks of decision making, problem solving and learning. Unlike other fields associated with intelligence, Artificial intelligence is concerned with both understanding and building of intelligent entities, and has the ability to automate intelligent processes. It is evident that Artificial intelligence is impacting on a variety of subfields and wider society. However literature regarding its application to the field of marketing appears to be scarce.\n\nAdvancements in Artificial intelligence’s application to a range of disciplines have led to the development of Artificial intelligence systems which have proved useful to marketers. These systems assist in areas such as market forecasting, automation of processes and decision making and increase the efficiency of tasks which would usually be performed by humans. The science behind these systems can be explained through neural networks and expert systems which are computer programs that process input and provide valuable output for marketers. \n\nArtificial intelligence systems stemming from Social computing technology can be applied to understand social networks on the Web. Data mining techniques can be used to analyze different types of social networks. This analysis helps a marketer to identify influential actors or nodes within networks, this information can then be applied to take a Societal marketing approach.\n\nArtificial intelligence has gained significant recognition in the marketing industry. However, ethical issues surrounding these systems and their potential to impact on the need for humans in the workforce, specifically marketing, is a controversial topic.\n\nAn artificial neural network is a form of computer program modelled on the brain and nervous system of humans. Neural networks are composed of a series of interconnected processing neurons functioning in unison to achieve certain outcomes. \nUsing “human-like trial and error learning methods neural networks detect patterns existing within a data set ignoring data that is not significant, while emphasising the data which is most influential”.\n\nFrom a marketing perspective, neural networks are a form of software tool used to assist in decision making. Neural networks are effective in gathering and extracting information from large data sources and have the ability to identify the cause and effect within data. These neural nets through the process of learning, identify relationships and connections between data bases. Once knowledge has been accumulated, neural networks can be relied on to provide generalisations and can apply past knowledge and learning to a variety of situations.\n\nNeural networks help fulfil the role of marketing companies through effectively aiding in market segmentation and measurement of performance while reducing costs and improving accuracy. Due to their learning ability, flexibility, adaption and knowledge discovery, neural networks offer many advantages over traditional models. Neural networks can be used to assist in pattern classification, forecasting and marketing analysis.\n\nClassification of customers can be facilitated through the neural network approach allowing companies to make informed marketing decisions. An example of this was employed by Spiegel Inc., a firm dealing in direct-mail operations who used neural networks to improve efficiencies. Using software developed by NeuralWare Inc., Spiegel identified the demographics of customers who had made a single purchase and those customers who had made repeat purchases. Neural networks where then able to identify the key patterns and consequently identify the customers that were most likely to repeat purchase. Understanding this information allowed Speigel to streamline marketing efforts, and reduced costs.\n\nSales forecasting “is the process of estimating future events with the goal of providing benchmarks for monitoring actual performance and reducing uncertainty\". Artificial intelligence techniques have emerged to facilitate the process of forecasting through increasing accuracy in the areas of demand for products, distribution, employee turnover, performance measurement and inventory control. An example of forecasting using neural networks is the Airline Marketing Assistant/Tactician; an application developed by BehabHeuristics which allows for the forecasting of passenger demand and consequent seat allocation through neural networks. This system has been used by Nationalair Canada and USAir.\n\nNeural networks provide a useful alternative to traditional statistical models due to their reliability, time-saving characteristics and ability to recognise patterns from incomplete or noisy data. Examples of marketing analysis systems include the Target Marketing System developed by Churchull Systems for Veratex Corporation. This support system scans a market database to identify dormant customers allowing management to make decisions regarding which key customers to target.\n\nWhen performing marketing analysis, neural networks can assist in the gathering and processing of information ranging from consumer demographics and credit history to the purchase patterns of consumers.\n\nMarketing is a complex field of decision making which involves a large degree of both judgment and intuition on behalf of the marketer. The enormous increase in complexity that the individual decision maker faces renders the decision making process almost an impossible task. Marketing decision engine can help distill the noise. The generation of more efficient management procedures have been recognized as a necessity. The application of Artificial intelligence to decision making through a Decision Support System has the ability to aid the decision maker in dealing with uncertainty in decision problems. Artificial intelligence techniques are increasingly extending decision support through analyzing trends; providing forecasts; reducing information overload; enabling communication required for collaborative decisions, and allowing for up-to-date information.\n\nOrganizations’ strive to satisfy the needs of the customers, paying specific attention to their desires. A consumer-orientated approach requires the production of goods and services that align with these needs. Understanding consumer behaviour aids the marketer in making appropriate decisions. Thus, the decision making is dependent on the marketing problem, the decision maker, and the decision environment.\n\nAn Expert System is a software program that combines the knowledge of experts in an attempt to solve problems through emulating the knowledge and reasoning procedures of the experts. Each expert system has the ability to process data, and then through reasoning, transform it into evaluations, judgments and opinions, thus providing advises to specialized problems.\n\nThe use of an expert system that applies to the field of marketing is MARKEX (Market Expert). These Intelligent decision support systems act as consultants for marketers, supporting the decision maker in different stages, specifically in the new product development process. The software provides a systematic analysis that uses various methods of forecasting, data analysis and multi-criteria decision making to select the most appropriate penetration strategy. BRANDFRAME is another example of a system developed to assist marketers in the decision-making process. The system supports a brand manager in terms of identifying the brand’s attributes, retail channels, competing brands, targets and budgets. New marketing input is fed into the system where BRANDFRAME analyses the data. Recommendations are made by the system in regard to marketing mix instruments, such as lowering the price or starting a sales promotional campaign.\n\nIn terms of marketing, automation uses software to computerize marketing processes that would have otherwise been performed manually. It assists in effectively allowing processes such as customer segmentation, campaign management and products promotion, to be undertaken at a more efficient rate. Marketing automation is a key component of Customer Relationship Management (CRM). Companies are using systems that employ data-mining algorithms that analyses the customer database, giving further insight into the customer. This information may refer to socio-economic characteristics, earlier interactions with the customer, and information about the purchase history of the customer. \nVarinos Systems have been designed to give organizations control over their data. Automation tools allow the system to monitor the performance of campaigns, making regular adjustments to the campaigns to improve response rates and to provide campaign performance tracking.\n\nDistribution of products requires companies to access accurate data so they are able to respond to fluctuating trends in product demand. Automation processes are able to provide a comprehensive system that improves real-time monitoring and intelligent control. \nAmazon acquired Kiva Systems, the makers of the warehouse robot for $775 million in 2012. Prior to the purchase of the automated system, human employees would have to walk the enormous warehouse, tracking and retrieving books. The Kiva robots are able to undertake order fulfillment, product replenishment, as well as heavy lifting, thus increasing efficiency for the company.\n\nA social network is a social arrangement of actors who make up a group, within a network; there can be an array of ties and nodes that exemplifies common occurrences within a network and common relationships. Lui (2011), describes a social network as, “the study of social entities (people in organization, called actors), and their interactions and relationships. The interactions and relationships can be represented with a network or graph, where each vertex (or node) represents an actor and each link represents a relationship.” At the present time there is a growth in virtual social networking with the common emergence of social networks being replicated online, for example social networking sites such as Twitter, Facebook and LinkedIn. From a marketing perspective, analysis and simulation of these networks can help to understand consumer behavior and opinion. The use of Agent-based social simulation techniques and data/opinion mining to collect social knowledge of networks can help a marketer to understand their market and segments within it.\n\nSocial computing is the branch of technology that can be used by marketers to analyze social behaviors within networks and also allows for creation of artificial social agents. Social computing provides the platform to create social based software; some earlier examples of social computing are such systems that allow a user to extract social information such as contact information from email accounts e.g. addresses and companies titles from ones email using Conditional Random Field (CRFs) technology.\n\nData mining involves searching the Web for existing information namely opinions and feelings that are posted online among social networks. “ This area of study is called opinion mining or sentiment analysis. It analyzes peoples opinions, appraisals, attitudes, and emotions toward entities, individuals, issues, events, topics, and their attributes”. However searching for this information and analysis of it can be a sizeable task, manually analyzing this information also presents the potential for researcher bias. Therefore, objective opinion analysis systems are suggested as a solution to this in the form of automated opinion mining and summarization systems. Marketers using this type of intelligence to make inferences about consumer opinion should be wary of what is called opinion spam, where fake opinions or reviews are posted in the web in order to influence potential consumers for or against a product or service.\n\nSearch engines are a common type of intelligence that seeks to learn what the user is interested in to present appropriate information. PageRank and HITS are examples of algorithms that search for information via hyperlinks; Google uses PageRank to control its search engine. Hyperlink based intelligence can be used to seek out web communities, which is described as ‘ a cluster of densely linked pages representing a group of people with a common interest’.\n\nCentrality and prestige are types of measurement terms used to describe the level of common occurrences among a group of actors; the terms help to describe the level of influence and actor holds within a social network. Someone who has many ties within a network would be described as a ‘central’ or ‘prestige’ actor. Identifying these nodes within a social network is helpful for marketers to find out who are the trendsetters within social networks.\n", "id": "35591037", "title": "Marketing and artificial intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=36987204", "text": "Kasparov's Gambit\n\nKasparov's Gambit or simply Gambit is a chess playing computer program created by Heuristic Software and published by Electronic Arts in 1993 based on Socrates II, the only winner of the North American Computer Chess Championship running on a common microcomputer. It was designed for MS-DOS while Garry Kasparov reigned as world champion, whose involvement and support was its key allure.\n\nJulio Kaplan, chessplayer, computer programmer, and owner of the company 'Heuristic Software', first developed Heuristic Alpha in 1990-91. The original version evolved into \"Socrates\" with the help of other chess players and programmers including Larry Kaufman and Don Dailey, who, later, were also developers of \"Kasparov's Gambit\".\n\nImprovements to \"Socrates\" were reflected in a version called \"Titan\", renamed for competition as \"Socrates II\", the most successful of the series winning the 1993 ACM International Chess Championship. During the course of the championship \"Socrates II\", which was running on a stock 486 PC, defeated opponents with purpose-built hardware and software for playing chess, including HiTech and Cray Blitz.\n\nElectronic Arts purchased \"Socrates II\" and hired its creators to build a new product, \"Kasparov's Gambit\", including Kasparov as consultant and brand. It was the company's effort to enter the chess programs market, dominated at the time by \"Chessmaster 3000\" and \"Blitz\". In 1993 it went on sale, but contained a number of bugs, so was patched at the end of that year. The patched version ran at about 75% of the speed of \"Socrates II\" which was quite an achievement considering the whole functionality of the software was sharing the same computer resources.\n\nIn 1993 it competed in the Harvard Cup (six humans versus six programs) facing grandmasters who had ratings ranging from 2515 to 2625 ELO. It finished the competition in 12th and last place. Grandmasters took the first five places and another \"Socrates\" derivation - \"Socrates Exp\" - was the best program finishing in 6th place.\nAccording to team developer Eric Schiller, a Windows version was planned by Electronic Arts, but was never finished. Excluding chess-style board games like (1983) or Battle Chess II: Chinese Chess (2002), Kasparov's Gambit remains the sole effort of Electronic Arts to enter the classic chess software market.\n\n\"Computer Gaming World\" in 1993 approved of \"Kasparov's Gambit\"s \"stunning\" SVGA graphics, Socrates II engine, and coaching features, concluding that it was \"above any PC game on the market\". It was a runner-up for the magazine's Strategy Game of the Year award in June 1994, losing to \"Master of Orion\". The editors called \"Kasparov's Gambit\" \"beautifully crafted\", a \"great teacher\" and \"a chess game for the 'rest of us.'\" It holds the 147th place in \"Computer Gaming World\"s 1996 list of \"150 Best Games of All Time\".\n\nRegarding Garry Kasparov's successful title defense against Nigel Short in the same year, followed by its triumph at the 1993 International Computer Chess Championship and its user-friendly capabilities, \"Gambit\" failed in sales and marked the end of Electronic Arts attempts to produce chess games.\n\n\"Gambit\" was intended to have the capabilities of a champion level software and a teaching tool for a wide range of player levels. It was Electronic Arts' first use of windowed video showing digitized images, video and voice of champ Garry Kasparov giving advice and commenting on player moves.\n\nPrimary features include:\n\nThe human strength rating is calculated using Elo formula with the included personalities and the one of player himself/herself, going from 800 to 2800 points. New players get a customizable 800 ELO, which changes according to the total number of games played, opponents strength and result of game.\n\nCreation of personalities enables five adjustable characteristics in percentage (0-100%)—strength, orthodoxy, creativity, focus and aggresivness—which define, besides its style, its ELO rating. User ELO is calculated according to \"Gambit's\" universe of electronic players and user him/herself, thus do not match rankings in real world, instead this feature was designed to provide a useful way to measure player strength and progress against \"Gambit\".\n\nBesides 125 tutorials, written by renowned chess author and developer Eric Schiller, classified in openings, middle game, endgames (checkmates), tactics and strategy also include a \"Famous Games\" database, a list of all-time world champions games commented by Kasparov with a quiz option where user must choose the next move.\n\nWas designed for 386SX IBM AT compatible systems. Even when it's capable to read commands from keyboard or mouse, the use of mouse is recommended. During the days it was released, \"Kasparov's Gambit\" offered a nice \"look & feel\" experience using SVGA mode with 640x480 resolution and 256 colors and voice/video recordings of world champion Garry Kasparov. A lack of soundcards support was reported by users.\n\nIt is playable in DOSBox emulator since 0.61 version over Linux and other Unix-like operating systems, Windows XP and subsequent versions and Mac OS X.\n\nFirst intention was using \"Heuristic Alpha\" as \"Gambit\"'s base, but unexpected good performance of \"Socrates II\" in tournaments made of it the final choice. According to developer and tester Larry Kauffman \"first released included important bugs, that Knowledge of bishop mobility appears to be missing, as does some other chess knowledge, and Gambit appears to run only about 50-60% of the speed of the ACM program in positions (without bishops) where the two do play and evaluate identically. There are also bugs in the features and the time controls, and the program is rather difficult to use (perhaps because it has so many features). One good thing I can say is that the 3d graphics are superb... I have tested the patched version, and have confirmed that most or all of the bugs have been corrected. The new version does play identically to the ACM program and runs at 70-75% of the speed, so it should rate just 30 points below the ACM program.\"\n\"Socrates II\" engine was fully programmed in assembly language, but rewritten just in C language for \"Kasparov's Gambit\" engine. Instead, assembly language was used for sound and video capabilities, as for other functionalities.\n\n\n", "id": "36987204", "title": "Kasparov's Gambit"}
{"url": "https://en.wikipedia.org/wiki?curid=19089876", "text": "Grandmaster Chess\n\nGrandmaster Chess is a 1992 video game to play chess for PC DOS platform develop by IntraCorp and its subsidiary Capstone that was focused on neural network technology and an artificial intelligence (AI) able to learn from mistakes.\n\nCapable of using VGA and SVGA modes, features multiple skill levels, different sets of pieces, boards and backgrounds, 2D/3D view, pull-down menus, move list with VCR style control, able to analysis moves and games and rate the user strength. Originally it was distributed in floppy discs, but in 1993 in appeared in CD-ROM. This release only relevant addition was the \"Terminator 2: Judgement Day: Chess Wars\" package, an animated chess set like \"Battle Chess\" video game representing the movie.\n\n\"Computer Gaming World\" stated that \"Grandmaster Chess\" \"falls short of the current competition in terms of overall options\". The magazine criticized the game's weak strategic analysis reporting, the absence of an advertised teaching mode, and weak opening book.\n\n\n", "id": "19089876", "title": "Grandmaster Chess"}
{"url": "https://en.wikipedia.org/wiki?curid=18015568", "text": "GNOME Chess\n\nGNOME Chess (formerly glChess) is a graphical front-end featuring a 2D and a 3D chessboard interface. GNOME Chess does not comprise an own chess engine and to play against the computer a third party chess engine must be present, but most Linux distributions package GNU Chess as the default chess engine with it. Additionally GNOME Chess supports third party chess engines, known ones are automatically detected.\n\nGNOME Chess is written in Vala. For 2D rendering it uses GTK+ and Cairo/librsvg, and 3D support is optionally available using OpenGL.\n\nAs part of the GNOME desktop environment and GNOME Games, GNOME Chess is free and open-source software subject to the terms of the GNU General Public License (GPL) version 2.\n\nGNOME Chess supports a plethora of chess engines, such as:\n\nglChess, the predecessor to GNOME Chess, can be used with any other CECP and Universal Chess Interface compatible software like:\n\n\"glChess\" was written by Robert Ancell in 2000 only as a personal project to test open source development.\n\nFirst version was written in C, OpenGL for graphics, and GLUT for the user interface. In May 5 was released 0.1.0, the first but still not playable version, being only capable to draw board and pieces. Days later, on May 31, version 0.1.3 was finally included on SourceForge and playable on a very basic way.\n\nOn April 8, 2001 version 0.2.0 changed GLUT to GTK+ focusing the improvement in visual aspects instead of its chess artificial intelligence. Version 0.3.0, from June 27, could play against other artificial intelligence (AI) engines, like Crafty and GNU Chess, after a Chess Engine Communication Protocol (CECP) implementation and it was ported to IRIX platform. In December, version 0.4.0 was the last one before the project entered into a stand-by time of three years.\n\nIn December, 2004, there was an advance to version 0.8.0 in order to accelerate the achievement the 1.0. This version added network support and updated GTK+ from version 1.2 to 2.0.\n\nOne year later, December 2005, version 0.9.0 was intended to be the last release before 1.0. It replaced C for Python to improve platform portability and maintenance, besides having a better test approach of the codebase testing.\n\nOn December 16, 2006, glChess finally reached version 1.0.\n\nApple Chess is a fork of GNOME Chess.\n\nIn version 3.14 3D mode was removed.\n\n\n", "id": "18015568", "title": "GNOME Chess"}
